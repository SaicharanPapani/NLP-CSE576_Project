{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_Project_model_finetune_10 (1).ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a748e79b5558402d9be020d56e19ed7d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ba103dcce1b44ba0934765c8b54bc4ee","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ef5c08c3839546dfbc6feed274429c39","IPY_MODEL_f2e524921da64e11b1e848cadc75103e","IPY_MODEL_5e142a2bb11145d4825185db35331600"]}},"ba103dcce1b44ba0934765c8b54bc4ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ef5c08c3839546dfbc6feed274429c39":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_1bba2453e7f749c88bca0f1b433302ce","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_55eb2c98357f4be6b341b0c3656b6676"}},"f2e524921da64e11b1e848cadc75103e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_685d750c569b462892037732b3b65208","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":791656,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":791656,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cf4599c007cc4708b1b55f4e062d7878"}},"5e142a2bb11145d4825185db35331600":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b02f1d9bdb2648aebc481bb6766214c6","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 773k/773k [00:00&lt;00:00, 1.36MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_45555d415f8b45b4a2c4e75e86861429"}},"1bba2453e7f749c88bca0f1b433302ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"55eb2c98357f4be6b341b0c3656b6676":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"685d750c569b462892037732b3b65208":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cf4599c007cc4708b1b55f4e062d7878":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b02f1d9bdb2648aebc481bb6766214c6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"45555d415f8b45b4a2c4e75e86861429":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ec8a5be589ec446395028c5d91978237":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4ac61c4404ce4441a86ea8d7e741b6e7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3ab0465bffb648568934865b73dba6f5","IPY_MODEL_9706c0ef85f84b1c9800fef06f141fc4","IPY_MODEL_d56a786adbb0463d9d7c96639ae53a92"]}},"4ac61c4404ce4441a86ea8d7e741b6e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ab0465bffb648568934865b73dba6f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_049556fac8dd46ce98c2090266e1e74f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c0302cd709b44e29845905a252c736a2"}},"9706c0ef85f84b1c9800fef06f141fc4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9a72d7c16ccb44b3bcdc90765b136656","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1389353,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1389353,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_28a22f9a76484ecfa924da746401254d"}},"d56a786adbb0463d9d7c96639ae53a92":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_59a7ad9df6754b788666504faa0ed3eb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.32M/1.32M [00:00&lt;00:00, 7.12MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_900e514338104f6d85500021ddfa73f1"}},"049556fac8dd46ce98c2090266e1e74f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c0302cd709b44e29845905a252c736a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9a72d7c16ccb44b3bcdc90765b136656":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"28a22f9a76484ecfa924da746401254d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"59a7ad9df6754b788666504faa0ed3eb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"900e514338104f6d85500021ddfa73f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"30120f67e39a4011ae57a8aba3e9e438":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b994bae8455e44dfb2f52ca26e54c337","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_86314fdf5f0e460a819baa985a75fc52","IPY_MODEL_65f03238b77a42aba14aa6a71d5960f9","IPY_MODEL_1af54759b88a455786c329311cacb12e"]}},"b994bae8455e44dfb2f52ca26e54c337":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"86314fdf5f0e460a819baa985a75fc52":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_9d03a4f3cb8741498837c0073ee2e023","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_845e22225a5d4d429577e92dbea06ef4"}},"65f03238b77a42aba14aa6a71d5960f9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_30dc9a40244e45a4850a6a523e22a980","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1199,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1199,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7a7badac85c44490a6f6c9bc12ee6ee4"}},"1af54759b88a455786c329311cacb12e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fd652ceed297411099170b9aff2787ac","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.17k/1.17k [00:00&lt;00:00, 22.6kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c8845ee31c00437aa1e248d1c97d5c77"}},"9d03a4f3cb8741498837c0073ee2e023":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"845e22225a5d4d429577e92dbea06ef4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"30dc9a40244e45a4850a6a523e22a980":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7a7badac85c44490a6f6c9bc12ee6ee4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fd652ceed297411099170b9aff2787ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c8845ee31c00437aa1e248d1c97d5c77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e8f7a97d88c14e97a89d7f8859432798":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_dfb0d2a1adf746cd872fcc5428e0fdfd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f909626acc7a4863bb106ece8c277336","IPY_MODEL_707cecbecff84f859e4ee4a67a81e793","IPY_MODEL_6c81dcadc0704085b60cedceea6085d9"]}},"dfb0d2a1adf746cd872fcc5428e0fdfd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f909626acc7a4863bb106ece8c277336":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_63b9898fefb446ed89c0c9f3008ddd08","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_21f49da377464901bcf6c40b24060d97"}},"707cecbecff84f859e4ee4a67a81e793":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_d5041306124048598724d50c0aedf658","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":891691430,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":891691430,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_828de6642bbc4283b2de9b5ec7600bb9"}},"6c81dcadc0704085b60cedceea6085d9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f47e260b43cc49838cc9486cd14f4ed9","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 850M/850M [00:29&lt;00:00, 33.6MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c241bcf8e1f546b2bbf0b56b9d89402a"}},"63b9898fefb446ed89c0c9f3008ddd08":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"21f49da377464901bcf6c40b24060d97":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d5041306124048598724d50c0aedf658":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"828de6642bbc4283b2de9b5ec7600bb9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f47e260b43cc49838cc9486cd14f4ed9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c241bcf8e1f546b2bbf0b56b9d89402a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"qFErrTqq_ybT"},"source":["### Installing neccessary packages:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3OMB-x8_0vK","executionInfo":{"status":"ok","timestamp":1638640008793,"user_tz":420,"elapsed":16538,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"b33f2478-5c9c-41ac-eb79-440a4dcc5d48"},"source":["!pip install transformers\n","# https://huggingface.co/transformers/installation.html\n","!pip install sentencepiece\n","# https://pypi.org/project/sentencepiece/\n","# Python wrapper for SentencePiece. This API will offer the encoding, decoding and training of Sentencepiece.\n","!pip install Cython\n","# https://pypi.org/project/Cython/"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 36.7 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 39.2 MB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 512 kB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 36.0 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 4.6 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (0.29.24)\n"]}]},{"cell_type":"markdown","metadata":{"id":"T-AVcK4gBhW7"},"source":["## Checking the GPU availabilty"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K5BIx7Mj1x9M","executionInfo":{"status":"ok","timestamp":1638640014442,"user_tz":420,"elapsed":5664,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"7ea16dc6-9a45-4eef-ca08-444e51499b77"},"source":["import torch\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\") \n","    print(\"GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"CPU\")"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU\n"]}]},{"cell_type":"code","metadata":{"id":"z1wUPLeYJ6GO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638640141131,"user_tz":420,"elapsed":3,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"5538b130-b449-4111-ea47-8821f371c898"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"bU-UZe2cBPpq"},"source":["## Importing the required packages:"]},{"cell_type":"code","metadata":{"id":"UrGEtltY6SIa","executionInfo":{"status":"ok","timestamp":1638640142833,"user_tz":420,"elapsed":160,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"mlokiVxO7jy0","executionInfo":{"status":"ok","timestamp":1638640143201,"user_tz":420,"elapsed":1,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["import os\n","import sys\n","from transformers.optimization import Adafactor \n","import time\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    get_linear_schedule_with_warmup\n",")\n","import torch\n","import random\n","import re\n","\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/new_run')"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xdoJ8keH8pLP","executionInfo":{"status":"ok","timestamp":1638640143411,"user_tz":420,"elapsed":3,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"4a05191e-33a4-42a4-ec9f-d0974ef88f75"},"source":["import pandas as pd\n","# Reading csv\n","data = pd.read_csv('data_10_full_v1.csv')\n","print(data.head(5))"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["                                              inputs  ... sumlen\n","0  The sum of 1 1000 4 100 0 10 6 and 7 100 2 10 ...  ...      4\n","1  The sum of 1 1000 6 100 1 10 6 and 1 1000 1 10...  ...      4\n","2  The sum of 3 100 7 10 6 and 1 1000 6 100 1 10 ...  ...      4\n","3  The sum of 1 1000 0 100 3 10 1 and 4 100 9 10 ...  ...      4\n","4  The sum of 1 1000 3 100 6 10 3 and 5 100 2 10 ...  ...      4\n","\n","[5 rows x 5 columns]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Uh3S1gzkGt7E","executionInfo":{"status":"ok","timestamp":1638640143548,"user_tz":420,"elapsed":2,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"e3d43edc-0606-4814-f835-93a0e9a56efc"},"source":["data.info"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method DataFrame.info of                                                   inputs  ... sumlen\n","0      The sum of 1 1000 4 100 0 10 6 and 7 100 2 10 ...  ...      4\n","1      The sum of 1 1000 6 100 1 10 6 and 1 1000 1 10...  ...      4\n","2      The sum of 3 100 7 10 6 and 1 1000 6 100 1 10 ...  ...      4\n","3      The sum of 1 1000 0 100 3 10 1 and 4 100 9 10 ...  ...      4\n","4      The sum of 1 1000 3 100 6 10 3 and 5 100 2 10 ...  ...      4\n","...                                                  ...  ...    ...\n","14964       The sum of 9 100 8 10 1 and 6 100 9 10 8 is   ...      4\n","14965  The sum of 1 1000 3 100 0 10 0 and 6 100 5 10 ...  ...      4\n","14966  The sum of 1 1000 6 100 6 10 0 and 8 100 8 10 ...  ...      4\n","14967  The sum of 5 100 7 10 3 and 1 1000 0 100 0 10 ...  ...      4\n","14968  The sum of 4 100 9 10 6 and 1 1000 7 100 3 10 ...  ...      4\n","\n","[14969 rows x 5 columns]>"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"YA_luy6aGn45","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638640143690,"user_tz":420,"elapsed":3,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"43e7abb4-9790-4350-aa9e-f08de55b904e"},"source":["data = data.sample(n = 13000, random_state = 42).reset_index(drop=True)\n","len(data)"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["13000"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"rcgHXMgv8606","executionInfo":{"status":"ok","timestamp":1638640144678,"user_tz":420,"elapsed":335,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","# Test and validation split\n","train, validation = train_test_split(data, test_size=0.2, random_state=42)\n","train, test = train_test_split(train, test_size=0.3, random_state=42)\n","\n","data_train = train.reset_index(drop=True)\n","data_valid = validation.reset_index(drop=True)\n","data_test = test.reset_index(drop=True)"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7YLTIG87LGc","executionInfo":{"status":"ok","timestamp":1638640149570,"user_tz":420,"elapsed":169,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"5a7af54a-f86b-4a31-9eae-e013788a71b6"},"source":["data_train.shape"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7280, 5)"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vohFU6vyHAiq","executionInfo":{"status":"ok","timestamp":1638640150171,"user_tz":420,"elapsed":4,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"bbde79dc-485b-42e7-fb1e-ab73974443b5"},"source":["data_valid.shape"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2600, 5)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Da158J1V-1U7","executionInfo":{"status":"ok","timestamp":1638640150580,"user_tz":420,"elapsed":2,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"0d1edd7a-5f02-4505-8027-01e81ad80d83"},"source":["data_test.shape"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3120, 5)"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"7ls6YKC19R07","executionInfo":{"status":"ok","timestamp":1638640151075,"user_tz":420,"elapsed":2,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["# Initializing Parameters \n","batch_size, num_of_epochs = 32, 25\n","num_of_batches = int(len(data_train)/batch_size)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"ye-gC3y2YI5g","executionInfo":{"status":"ok","timestamp":1638640151617,"user_tz":420,"elapsed":2,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["# Reference\n","# https://huggingface.co/transformers/model_doc/t5.html\n","# https://medium.com/analytics-vidhya/t5-a-detailed-explanation-a0ac9bc53e51\n","# https://towardsdatascience.com/data-to-text-generation-with-t5-building-a-simple-yet-advanced-nlg-model-b5cce5a6df45"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"HngN0xRX1Rk5","executionInfo":{"status":"ok","timestamp":1638640152761,"user_tz":420,"elapsed":521,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"777c5b0f-9a95-4587-e888-462ca4b43e19"},"source":["# get length of all the messages in the train set\n","seq_len = [len(i.split()) for i in data_train['inputs']]\n","\n","pd.Series(seq_len).hist(bins = 30)"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f075d441b10>"]},"metadata":{},"execution_count":19},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"_ad1Lt8c9iDX","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a748e79b5558402d9be020d56e19ed7d","ba103dcce1b44ba0934765c8b54bc4ee","ef5c08c3839546dfbc6feed274429c39","f2e524921da64e11b1e848cadc75103e","5e142a2bb11145d4825185db35331600","1bba2453e7f749c88bca0f1b433302ce","55eb2c98357f4be6b341b0c3656b6676","685d750c569b462892037732b3b65208","cf4599c007cc4708b1b55f4e062d7878","b02f1d9bdb2648aebc481bb6766214c6","45555d415f8b45b4a2c4e75e86861429","ec8a5be589ec446395028c5d91978237","4ac61c4404ce4441a86ea8d7e741b6e7","3ab0465bffb648568934865b73dba6f5","9706c0ef85f84b1c9800fef06f141fc4","d56a786adbb0463d9d7c96639ae53a92","049556fac8dd46ce98c2090266e1e74f","c0302cd709b44e29845905a252c736a2","9a72d7c16ccb44b3bcdc90765b136656","28a22f9a76484ecfa924da746401254d","59a7ad9df6754b788666504faa0ed3eb","900e514338104f6d85500021ddfa73f1","30120f67e39a4011ae57a8aba3e9e438","b994bae8455e44dfb2f52ca26e54c337","86314fdf5f0e460a819baa985a75fc52","65f03238b77a42aba14aa6a71d5960f9","1af54759b88a455786c329311cacb12e","9d03a4f3cb8741498837c0073ee2e023","845e22225a5d4d429577e92dbea06ef4","30dc9a40244e45a4850a6a523e22a980","7a7badac85c44490a6f6c9bc12ee6ee4","fd652ceed297411099170b9aff2787ac","c8845ee31c00437aa1e248d1c97d5c77","e8f7a97d88c14e97a89d7f8859432798","dfb0d2a1adf746cd872fcc5428e0fdfd","f909626acc7a4863bb106ece8c277336","707cecbecff84f859e4ee4a67a81e793","6c81dcadc0704085b60cedceea6085d9","63b9898fefb446ed89c0c9f3008ddd08","21f49da377464901bcf6c40b24060d97","d5041306124048598724d50c0aedf658","828de6642bbc4283b2de9b5ec7600bb9","f47e260b43cc49838cc9486cd14f4ed9","c241bcf8e1f546b2bbf0b56b9d89402a"]},"executionInfo":{"status":"ok","timestamp":1638640201662,"user_tz":420,"elapsed":47710,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"f9bbe4bc-f0d9-4997-ac56-fdb1377130e1"},"source":["# T5-base\n","tokenizer = T5Tokenizer.from_pretrained('t5-base')\n","\n","model = T5ForConditionalGeneration.from_pretrained('t5-base', return_dict=True)\n","# moving the model to device(GPU/CPU)\n","model.to(device)"],"execution_count":20,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a748e79b5558402d9be020d56e19ed7d","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec8a5be589ec446395028c5d91978237","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30120f67e39a4011ae57a8aba3e9e438","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8f7a97d88c14e97a89d7f8859432798","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/850M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","metadata":{"id":"uNRQcyTh3GQf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638640203207,"user_tz":420,"elapsed":1550,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"92c73d64-3a5f-46d9-af21-2da3a5bda469"},"source":["token_lens = []\n","\n","for txt in data_train.inputs:\n","  # doubt\n","  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n","  token_lens.append(len(tokens))\n","\n","max(token_lens)"],"execution_count":21,"outputs":[{"output_type":"execute_result","data":{"text/plain":["24"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Efo0J6kA-yt","executionInfo":{"status":"ok","timestamp":1638640204503,"user_tz":420,"elapsed":1300,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"6fd48839-97b7-49ca-da8e-414acadd4986"},"source":["token_lens_target = []\n","\n","for txt in data_train.target:\n","  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n","  token_lens_target.append(len(tokens))\n","\n","max(token_lens_target)"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["11"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"OT1Us38M2ULV","executionInfo":{"status":"ok","timestamp":1638640205055,"user_tz":420,"elapsed":555,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"2930be25-1ae0-4831-9f5c-a9dd7af24017"},"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(range(1,len(token_lens)+1), token_lens)\n","plt.ylabel('length of tokens')\n","plt.show()\n","\n","MAX_LEN = max(token_lens)\n","print(\"Maximum length is: \", MAX_LEN)\n","# when sample with first 40k and last 40k we got the maximum length is 14"],"execution_count":23,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["Maximum length is:  24\n"]}]},{"cell_type":"code","metadata":{"id":"k6rUVKln1Ff5","executionInfo":{"status":"ok","timestamp":1638640205056,"user_tz":420,"elapsed":6,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["MAX_LEN = 30"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"_FqnJxC4u8XO","executionInfo":{"status":"ok","timestamp":1638640205056,"user_tz":420,"elapsed":5,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["def get_word_embeddings(data, MAX_LEN=45):\n","  input_ids=[]\n","  attention_masks = []\n","  for sent in data:\n","        encoded_sent = tokenizer.encode_plus(\n","            text=sent,  \n","            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n","            max_length=MAX_LEN,                  # Max length to truncate/pad\n","            pad_to_max_length=True,         # Pad sentence to max length\n","            #return_tensors='pt',           # Return PyTorch tensor\n","            return_attention_mask=True      # Return attention mask\n","            )\n","        \n","        # Add the outputs to the lists\n","        input_ids.append(encoded_sent.get('input_ids'))\n","        attention_masks.append(encoded_sent.get('attention_mask'))\n","\n","  # Convert lists to tensors\n","  input_ids = torch.tensor(input_ids)\n","  attention_masks = torch.tensor(attention_masks)\n","\n","  return input_ids, attention_masks"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jAxmAoDrvrWO","executionInfo":{"status":"ok","timestamp":1638640208440,"user_tz":420,"elapsed":3389,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"533dc30a-34a9-4c31-864e-0c9d54e940a7"},"source":["# Run function `preprocessing_for_bert` on the train set and the validation set\n","train_inputs, train_masks = get_word_embeddings(data_train['inputs'])\n","val_inputs, val_masks = get_word_embeddings(data_valid['inputs'])\n","test_inputs, test_masks = get_word_embeddings(data_test['inputs'])"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"id":"XbPJ2deo71p5","executionInfo":{"status":"ok","timestamp":1638640208441,"user_tz":420,"elapsed":9,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["data_train['target_str'] = data_train['target'].astype(str)\n","data_valid['target_str'] = data_valid['target'].astype(str)\n","data_test['target_str'] = data_test['target'].astype(str)\n"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j3dHakeNv66S","executionInfo":{"status":"ok","timestamp":1638640211062,"user_tz":420,"elapsed":2628,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"a00709ba-40c3-4ca0-f029-b6e076d99358"},"source":["#convert lists to tensors\n","train_labels = get_word_embeddings(data_train['target_str'], 20)[0]\n","val_labels = get_word_embeddings(data_valid['target_str'], 20)[0]\n","test_labels = get_word_embeddings(data_test['target_str'], 20)[0]"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EHUnt0aF8GQk","executionInfo":{"status":"ok","timestamp":1638640211062,"user_tz":420,"elapsed":12,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"141815a6-b13f-4b54-906f-ca54d6ae1fde"},"source":["train_labels.shape\n","# doubt"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([7280, 20])"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MgegmxBtoqZV","executionInfo":{"status":"ok","timestamp":1638640211062,"user_tz":420,"elapsed":9,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"c7459325-a220-4e87-9d63-a95dd4de2cdf"},"source":["train_labels"],"execution_count":30,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 505,  910,  204,  ...,    0,    0,    0],\n","        [ 209, 5580,  431,  ...,    0,    0,    0],\n","        [ 204, 5580,  314,  ...,    0,    0,    0],\n","        ...,\n","        [ 209, 5580,  220,  ...,    0,    0,    0],\n","        [ 220, 5580,  305,  ...,    0,    0,    0],\n","        [ 220, 5580,  489,  ...,    0,    0,    0]])"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ezRHf2e7rPL","executionInfo":{"status":"ok","timestamp":1638640211062,"user_tz":420,"elapsed":6,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"38acaaac-2779-4d4c-f072-99129b54f938"},"source":["data_train['target']"],"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0              8 100 2 10 2\n","1       1 1000 6 100 2 10 6\n","2       2 1000 4 100 0 10 5\n","3              1 100 6 10 3\n","4       1 1000 0 100 8 10 7\n","               ...         \n","7275    2 1000 8 100 5 10 9\n","7276    1 1000 6 100 2 10 4\n","7277    1 1000 3 100 3 10 7\n","7278    3 1000 5 100 1 10 8\n","7279    3 1000 7 100 2 10 1\n","Name: target, Length: 7280, dtype: object"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"osOohece1-tJ","executionInfo":{"status":"ok","timestamp":1638640211063,"user_tz":420,"elapsed":5,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"dce78ed8-e595-4926-c0a2-3bb0ec25f43c"},"source":["train_inputs.shape"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([7280, 45])"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","metadata":{"id":"GjDbWMIlv8Iq","executionInfo":{"status":"ok","timestamp":1638640211063,"user_tz":420,"elapsed":4,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","\n","batch_size = 32\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_dataloader = DataLoader(train_data, shuffle = True, batch_size = batch_size)\n","\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_dataloader = DataLoader(val_data, shuffle = True, batch_size = batch_size)\n","\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_dataloader = DataLoader(test_data, shuffle = True, batch_size = batch_size)"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"q88he4D9Lw0L","executionInfo":{"status":"ok","timestamp":1638640211063,"user_tz":420,"elapsed":4,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["#  Optimizer\n","# https://huggingface.co/transformers/model_doc/t5.html#overview\n","optimizer = Adafactor(\n","    model.parameters(),\n","    lr=5e-4, # Initializing the learning Rate as suggested in the T5 official documentation\n","    eps=(1e-8, 1e-3),\n","    clip_threshold=1.0,\n","    decay_rate=-0.3,\n","    beta1=None,\n","    weight_decay=0.0,\n","    relative_step=False,\n","    scale_parameter=False,\n","    warmup_init=False\n",")"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"vQ9Bj35-56kQ","executionInfo":{"status":"ok","timestamp":1638640211248,"user_tz":420,"elapsed":2,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["# Changing the directory to store the model there.\n","# print(os.getcwd())\n","# os.chdir('/content/drive/MyDrive/Colab Notebooks/new_run')\n","# print(os.getcwd())"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"COfO6s0z3kit","executionInfo":{"status":"ok","timestamp":1638640211617,"user_tz":420,"elapsed":371,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"3ee401b8-7c1a-4f65-db3d-6cd1222b1dc4"},"source":["# Loading the configuration file for 't5-base' model\n","!wget https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-12-04 17:50:11--  https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.184.29\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.184.29|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1199 (1.2K) [application/json]\n","Saving to: ‘t5-base-config.json’\n","\n","t5-base-config.json 100%[===================>]   1.17K  --.-KB/s    in 0s      \n","\n","2021-12-04 17:50:11 (49.3 MB/s) - ‘t5-base-config.json’ saved [1199/1199]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"MYMdr0Xu92Yv","executionInfo":{"status":"ok","timestamp":1638640211617,"user_tz":420,"elapsed":3,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["from IPython.display import HTML, display\n","\n","# Setting the progress, with html as UI.\n","def progress(loss, value, max=100):\n","    return HTML(\"\"\" Batch loss :{loss}\n","        <progress\n","            value='{value}'\n","            max='{max}',\n","            style='width: 100%'\n","        >\n","            {value}\n","        </progress>\n","    \"\"\".format(loss=loss,value=value, max=max))"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"_C2w5YpsN7C1","executionInfo":{"status":"ok","timestamp":1638640211617,"user_tz":420,"elapsed":2,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["def evaluate(model, val_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's performance\n","    on our validation set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","\n","    # Tracking variables\n","    v_accuracy = []\n","    v_loss = []\n","\n","    # For each batch in our validation set...\n","    for batch in val_dataloader:\n","        # Load batch to GPU\n","        \n","        v_input_ids, v_attn_mask, v_labels = tuple(t.to(device) for t in batch)\n","\n","        # print(v_input_ids.shape, v_labels.shape)\n","\n","        val_outputs = model.generate(input_ids=v_input_ids, attention_mask=v_attn_mask)\n","\n","        val_preds = [ tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","            for output in val_outputs]\n","\n","        val_labels = [ tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","            for output in v_labels]\n","        \n","        # v_loss.append(val_outputs.loss)\n","\n","        # Get the predictions\n","        # print(val_outputs.logits.shape)\n","        # val_preds = torch.argmax(val_outputs.logits, dim=1).flatten()\n","        # print(val_preds, val_labels)\n","        # Calculate the accuracy rate\n","\n","        val_preds = np.array(val_preds)\n","        val_labels = np.array(val_labels)\n","        accuracy = ((val_preds == val_labels).sum() / len(val_labels)) * 100\n","        v_accuracy.append(accuracy)\n","\n","    # Compute the average accuracy and loss over the validation set.\n","    # v_loss = np.mean(v_loss)\n","    v_accuracy = np.mean(v_accuracy)\n","\n","    return v_accuracy"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wlCbmfAQ-CWJ","outputId":"eb290745-0955-4e41-fbc7-a9d3bc4e6312"},"source":["import gc\n","\n","val_acc = 0\n","train_accuracy = 0\n","\n","# Sets the module in training mode\n","model.train()\n","\n","for epoch in range(1,num_of_epochs+1):\n","  print('Running epoch: {}'.format(epoch))\n","  running_loss=0\n","  # out = display(progress(1, num_of_batches+1), display_id=True)\n","  i =0 \n","  for batch in train_dataloader:\n","    \n","    input_ids, attn_mask, labels = tuple(t.to(device) for t in batch)\n","\n","    # clear out the gradients of all Variables \n","    optimizer.zero_grad()\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    # Forward propogation\n","    # print(model(input_ids=input_ids, attention_mask=attn_mask, labels=labels))\n","    outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n","    \n","    loss = outputs.loss\n","    loss_num=loss.item()\n","    logits = outputs.logits\n","    running_loss+=loss_num\n","    # out.update(progress(loss_num,i, num_of_batches+1))\n","\n","    # calculating the gradients\n","    loss.backward()\n","\n","    # updating the params\n","    optimizer.step()\n","\n","    print(\"Epoch \", epoch, \"Batch \", i, \"/\", len(train_dataloader), \" Training Loss \", loss_num)\n","    i += 1\n","\n","  running_loss = running_loss/len(train_dataloader)\n","  # v_input_ids, v_attn_mask, v_labels = tuple(t.to(device) for t in data_valid)\n","  \n","  curr_accuracy = evaluate(model, val_dataloader)\n","\n","  # print('Epoch: {} , Running loss: {}'.format(epoch,running_loss))\n","  print(f\"{epoch + 1:^7} | {'-':^7} | {running_loss:^12.6f} | {curr_accuracy:^9.6f}\")\n","  print(\"-\"*70)\n","\n","  if curr_accuracy > val_acc:\n","    val_acc = curr_accuracy\n","    # Saving the best model\n","    torch.save(model.state_dict(),'Fine_tune_10based_model_v1.bin')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch  4 Batch  81 / 228  Training Loss  0.008620541542768478\n","Epoch  4 Batch  82 / 228  Training Loss  0.0051237186416983604\n","Epoch  4 Batch  83 / 228  Training Loss  0.0052915168926119804\n","Epoch  4 Batch  84 / 228  Training Loss  0.0056763095781207085\n","Epoch  4 Batch  85 / 228  Training Loss  0.003869482548907399\n","Epoch  4 Batch  86 / 228  Training Loss  0.014065462164580822\n","Epoch  4 Batch  87 / 228  Training Loss  0.006326402071863413\n","Epoch  4 Batch  88 / 228  Training Loss  0.016094278544187546\n","Epoch  4 Batch  89 / 228  Training Loss  0.006378915160894394\n","Epoch  4 Batch  90 / 228  Training Loss  0.003957131411880255\n","Epoch  4 Batch  91 / 228  Training Loss  0.003733821911737323\n","Epoch  4 Batch  92 / 228  Training Loss  0.006186576560139656\n","Epoch  4 Batch  93 / 228  Training Loss  0.005990211386233568\n","Epoch  4 Batch  94 / 228  Training Loss  0.011495775543153286\n","Epoch  4 Batch  95 / 228  Training Loss  0.006447827909141779\n","Epoch  4 Batch  96 / 228  Training Loss  0.01161203347146511\n","Epoch  4 Batch  97 / 228  Training Loss  0.007889951579272747\n","Epoch  4 Batch  98 / 228  Training Loss  0.014175819233059883\n","Epoch  4 Batch  99 / 228  Training Loss  0.010746819898486137\n","Epoch  4 Batch  100 / 228  Training Loss  0.005189330782741308\n","Epoch  4 Batch  101 / 228  Training Loss  0.005364120472222567\n","Epoch  4 Batch  102 / 228  Training Loss  0.006410971283912659\n","Epoch  4 Batch  103 / 228  Training Loss  0.011407962068915367\n","Epoch  4 Batch  104 / 228  Training Loss  0.008251986466348171\n","Epoch  4 Batch  105 / 228  Training Loss  0.009225885383784771\n","Epoch  4 Batch  106 / 228  Training Loss  0.009131667204201221\n","Epoch  4 Batch  107 / 228  Training Loss  0.0029147567693144083\n","Epoch  4 Batch  108 / 228  Training Loss  0.0032520126551389694\n","Epoch  4 Batch  109 / 228  Training Loss  0.004258477594703436\n","Epoch  4 Batch  110 / 228  Training Loss  0.004491020925343037\n","Epoch  4 Batch  111 / 228  Training Loss  0.0024011090863496065\n","Epoch  4 Batch  112 / 228  Training Loss  0.004189138300716877\n","Epoch  4 Batch  113 / 228  Training Loss  0.003200029954314232\n","Epoch  4 Batch  114 / 228  Training Loss  0.0027714765165001154\n","Epoch  4 Batch  115 / 228  Training Loss  0.004320870619267225\n","Epoch  4 Batch  116 / 228  Training Loss  0.004736841656267643\n","Epoch  4 Batch  117 / 228  Training Loss  0.004251166246831417\n","Epoch  4 Batch  118 / 228  Training Loss  0.006775835063308477\n","Epoch  4 Batch  119 / 228  Training Loss  0.006612284574657679\n","Epoch  4 Batch  120 / 228  Training Loss  0.011148006655275822\n","Epoch  4 Batch  121 / 228  Training Loss  0.01189800538122654\n","Epoch  4 Batch  122 / 228  Training Loss  0.01605791226029396\n","Epoch  4 Batch  123 / 228  Training Loss  0.011135951615869999\n","Epoch  4 Batch  124 / 228  Training Loss  0.01647002436220646\n","Epoch  4 Batch  125 / 228  Training Loss  0.00481614051386714\n","Epoch  4 Batch  126 / 228  Training Loss  0.006815464235842228\n","Epoch  4 Batch  127 / 228  Training Loss  0.007148588541895151\n","Epoch  4 Batch  128 / 228  Training Loss  0.00500111747533083\n","Epoch  4 Batch  129 / 228  Training Loss  0.007484446279704571\n","Epoch  4 Batch  130 / 228  Training Loss  0.003088208846747875\n","Epoch  4 Batch  131 / 228  Training Loss  0.0030415132641792297\n","Epoch  4 Batch  132 / 228  Training Loss  0.006625466048717499\n","Epoch  4 Batch  133 / 228  Training Loss  0.004323536530137062\n","Epoch  4 Batch  134 / 228  Training Loss  0.003921338822692633\n","Epoch  4 Batch  135 / 228  Training Loss  0.005805333144962788\n","Epoch  4 Batch  136 / 228  Training Loss  0.007428452372550964\n","Epoch  4 Batch  137 / 228  Training Loss  0.0028347247280180454\n","Epoch  4 Batch  138 / 228  Training Loss  0.004904073663055897\n","Epoch  4 Batch  139 / 228  Training Loss  0.003808275330811739\n","Epoch  4 Batch  140 / 228  Training Loss  0.005718366242945194\n","Epoch  4 Batch  141 / 228  Training Loss  0.01869845762848854\n","Epoch  4 Batch  142 / 228  Training Loss  0.00652819499373436\n","Epoch  4 Batch  143 / 228  Training Loss  0.004641171544790268\n","Epoch  4 Batch  144 / 228  Training Loss  0.003136537503451109\n","Epoch  4 Batch  145 / 228  Training Loss  0.003691843245178461\n","Epoch  4 Batch  146 / 228  Training Loss  0.007036213763058186\n","Epoch  4 Batch  147 / 228  Training Loss  0.008937839418649673\n","Epoch  4 Batch  148 / 228  Training Loss  0.009806583635509014\n","Epoch  4 Batch  149 / 228  Training Loss  0.009850716218352318\n","Epoch  4 Batch  150 / 228  Training Loss  0.008362622000277042\n","Epoch  4 Batch  151 / 228  Training Loss  0.008803759701550007\n","Epoch  4 Batch  152 / 228  Training Loss  0.004691540263593197\n","Epoch  4 Batch  153 / 228  Training Loss  0.00859864056110382\n","Epoch  4 Batch  154 / 228  Training Loss  0.008226546458899975\n","Epoch  4 Batch  155 / 228  Training Loss  0.017723437398672104\n","Epoch  4 Batch  156 / 228  Training Loss  0.01620737463235855\n","Epoch  4 Batch  157 / 228  Training Loss  0.013666307553648949\n","Epoch  4 Batch  158 / 228  Training Loss  0.013116875663399696\n","Epoch  4 Batch  159 / 228  Training Loss  0.007136064115911722\n","Epoch  4 Batch  160 / 228  Training Loss  0.012005853466689587\n","Epoch  4 Batch  161 / 228  Training Loss  0.008943764492869377\n","Epoch  4 Batch  162 / 228  Training Loss  0.009334049187600613\n","Epoch  4 Batch  163 / 228  Training Loss  0.020722921937704086\n","Epoch  4 Batch  164 / 228  Training Loss  0.011416389606893063\n","Epoch  4 Batch  165 / 228  Training Loss  0.019634831696748734\n","Epoch  4 Batch  166 / 228  Training Loss  0.008423568680882454\n","Epoch  4 Batch  167 / 228  Training Loss  0.021402785554528236\n","Epoch  4 Batch  168 / 228  Training Loss  0.006879132241010666\n","Epoch  4 Batch  169 / 228  Training Loss  0.003628769190981984\n","Epoch  4 Batch  170 / 228  Training Loss  0.00896399561315775\n","Epoch  4 Batch  171 / 228  Training Loss  0.0026853340677917004\n","Epoch  4 Batch  172 / 228  Training Loss  0.003402386326342821\n","Epoch  4 Batch  173 / 228  Training Loss  0.004083830863237381\n","Epoch  4 Batch  174 / 228  Training Loss  0.0029436913318932056\n","Epoch  4 Batch  175 / 228  Training Loss  0.003253402654081583\n","Epoch  4 Batch  176 / 228  Training Loss  0.010276708751916885\n","Epoch  4 Batch  177 / 228  Training Loss  0.003095150925219059\n","Epoch  4 Batch  178 / 228  Training Loss  0.0028754512313753366\n","Epoch  4 Batch  179 / 228  Training Loss  0.005187616217881441\n","Epoch  4 Batch  180 / 228  Training Loss  0.007855349220335484\n","Epoch  4 Batch  181 / 228  Training Loss  0.008103307336568832\n","Epoch  4 Batch  182 / 228  Training Loss  0.010205747559666634\n","Epoch  4 Batch  183 / 228  Training Loss  0.0029701380990445614\n","Epoch  4 Batch  184 / 228  Training Loss  0.008841564878821373\n","Epoch  4 Batch  185 / 228  Training Loss  0.009501063264906406\n","Epoch  4 Batch  186 / 228  Training Loss  0.021498382091522217\n","Epoch  4 Batch  187 / 228  Training Loss  0.011335189454257488\n","Epoch  4 Batch  188 / 228  Training Loss  0.005083651281893253\n","Epoch  4 Batch  189 / 228  Training Loss  0.0037764906883239746\n","Epoch  4 Batch  190 / 228  Training Loss  0.0032007109839469194\n","Epoch  4 Batch  191 / 228  Training Loss  0.003611957188695669\n","Epoch  4 Batch  192 / 228  Training Loss  0.0021824759896844625\n","Epoch  4 Batch  193 / 228  Training Loss  0.0021820354741066694\n","Epoch  4 Batch  194 / 228  Training Loss  0.004641951527446508\n","Epoch  4 Batch  195 / 228  Training Loss  0.006540623959153891\n","Epoch  4 Batch  196 / 228  Training Loss  0.008366892114281654\n","Epoch  4 Batch  197 / 228  Training Loss  0.004848438780754805\n","Epoch  4 Batch  198 / 228  Training Loss  0.005951351020485163\n","Epoch  4 Batch  199 / 228  Training Loss  0.007813851349055767\n","Epoch  4 Batch  200 / 228  Training Loss  0.007970036938786507\n","Epoch  4 Batch  201 / 228  Training Loss  0.0062636760994791985\n","Epoch  4 Batch  202 / 228  Training Loss  0.014456799253821373\n","Epoch  4 Batch  203 / 228  Training Loss  0.009353207424283028\n","Epoch  4 Batch  204 / 228  Training Loss  0.016954198479652405\n","Epoch  4 Batch  205 / 228  Training Loss  0.020015966147184372\n","Epoch  4 Batch  206 / 228  Training Loss  0.004637743346393108\n","Epoch  4 Batch  207 / 228  Training Loss  0.005009246990084648\n","Epoch  4 Batch  208 / 228  Training Loss  0.0028499190229922533\n","Epoch  4 Batch  209 / 228  Training Loss  0.004005723167210817\n","Epoch  4 Batch  210 / 228  Training Loss  0.0019519779598340392\n","Epoch  4 Batch  211 / 228  Training Loss  0.0077845132909715176\n","Epoch  4 Batch  212 / 228  Training Loss  0.008237142115831375\n","Epoch  4 Batch  213 / 228  Training Loss  0.002636950695887208\n","Epoch  4 Batch  214 / 228  Training Loss  0.005871860776096582\n","Epoch  4 Batch  215 / 228  Training Loss  0.0023764618672430515\n","Epoch  4 Batch  216 / 228  Training Loss  0.005324042402207851\n","Epoch  4 Batch  217 / 228  Training Loss  0.007883674465119839\n","Epoch  4 Batch  218 / 228  Training Loss  0.010199296288192272\n","Epoch  4 Batch  219 / 228  Training Loss  0.0032374956645071507\n","Epoch  4 Batch  220 / 228  Training Loss  0.0015688203275203705\n","Epoch  4 Batch  221 / 228  Training Loss  0.0035996106453239918\n","Epoch  4 Batch  222 / 228  Training Loss  0.005992811638861895\n","Epoch  4 Batch  223 / 228  Training Loss  0.004581435117870569\n","Epoch  4 Batch  224 / 228  Training Loss  0.00369470939040184\n","Epoch  4 Batch  225 / 228  Training Loss  0.0023506064899265766\n","Epoch  4 Batch  226 / 228  Training Loss  0.0028219406958669424\n","Epoch  4 Batch  227 / 228  Training Loss  0.002936833305284381\n","   5    |    -    |   0.006956   | 96.265244\n","----------------------------------------------------------------------\n","Running epoch: 5\n","Epoch  5 Batch  0 / 228  Training Loss  0.0017473470652475953\n","Epoch  5 Batch  1 / 228  Training Loss  0.002093073446303606\n","Epoch  5 Batch  2 / 228  Training Loss  0.0031109205447137356\n","Epoch  5 Batch  3 / 228  Training Loss  0.0022513088770210743\n","Epoch  5 Batch  4 / 228  Training Loss  0.0021830545738339424\n","Epoch  5 Batch  5 / 228  Training Loss  0.004013299476355314\n","Epoch  5 Batch  6 / 228  Training Loss  0.01393035613000393\n","Epoch  5 Batch  7 / 228  Training Loss  0.009794561192393303\n","Epoch  5 Batch  8 / 228  Training Loss  0.00817280262708664\n","Epoch  5 Batch  9 / 228  Training Loss  0.0028115466702729464\n","Epoch  5 Batch  10 / 228  Training Loss  0.0012975148856639862\n","Epoch  5 Batch  11 / 228  Training Loss  0.003643909003585577\n","Epoch  5 Batch  12 / 228  Training Loss  0.011756316758692265\n","Epoch  5 Batch  13 / 228  Training Loss  0.007997600361704826\n","Epoch  5 Batch  14 / 228  Training Loss  0.0056703886948525906\n","Epoch  5 Batch  15 / 228  Training Loss  0.0020069796591997147\n","Epoch  5 Batch  16 / 228  Training Loss  0.011345344595611095\n","Epoch  5 Batch  17 / 228  Training Loss  0.0015184846706688404\n","Epoch  5 Batch  18 / 228  Training Loss  0.001976403873413801\n","Epoch  5 Batch  19 / 228  Training Loss  0.0045352717861533165\n","Epoch  5 Batch  20 / 228  Training Loss  0.007718848530203104\n","Epoch  5 Batch  21 / 228  Training Loss  0.017424894496798515\n","Epoch  5 Batch  22 / 228  Training Loss  0.004132499452680349\n","Epoch  5 Batch  23 / 228  Training Loss  0.00225380202755332\n","Epoch  5 Batch  24 / 228  Training Loss  0.009936303831636906\n","Epoch  5 Batch  25 / 228  Training Loss  0.004380339290946722\n","Epoch  5 Batch  26 / 228  Training Loss  0.008668988011777401\n","Epoch  5 Batch  27 / 228  Training Loss  0.0077400607988238335\n","Epoch  5 Batch  28 / 228  Training Loss  0.004677769728004932\n","Epoch  5 Batch  29 / 228  Training Loss  0.002973671769723296\n","Epoch  5 Batch  30 / 228  Training Loss  0.003510359674692154\n","Epoch  5 Batch  31 / 228  Training Loss  0.0011726190568879247\n","Epoch  5 Batch  32 / 228  Training Loss  0.0016260234406217933\n","Epoch  5 Batch  33 / 228  Training Loss  0.0028486871160566807\n","Epoch  5 Batch  34 / 228  Training Loss  0.0014902253169566393\n","Epoch  5 Batch  35 / 228  Training Loss  0.00289748958311975\n","Epoch  5 Batch  36 / 228  Training Loss  0.002997691510245204\n","Epoch  5 Batch  37 / 228  Training Loss  0.008133023977279663\n","Epoch  5 Batch  38 / 228  Training Loss  0.0025283116847276688\n","Epoch  5 Batch  39 / 228  Training Loss  0.0017973786452785134\n","Epoch  5 Batch  40 / 228  Training Loss  0.003631263505667448\n","Epoch  5 Batch  41 / 228  Training Loss  0.005241747014224529\n","Epoch  5 Batch  42 / 228  Training Loss  0.005947338417172432\n","Epoch  5 Batch  43 / 228  Training Loss  0.00213673897087574\n","Epoch  5 Batch  44 / 228  Training Loss  0.010228504426777363\n","Epoch  5 Batch  45 / 228  Training Loss  0.008031646721065044\n","Epoch  5 Batch  46 / 228  Training Loss  0.003996917512267828\n","Epoch  5 Batch  47 / 228  Training Loss  0.0025257770903408527\n","Epoch  5 Batch  48 / 228  Training Loss  0.0042878491804003716\n","Epoch  5 Batch  49 / 228  Training Loss  0.0023830104619264603\n","Epoch  5 Batch  50 / 228  Training Loss  0.014617333188652992\n","Epoch  5 Batch  51 / 228  Training Loss  0.007714353501796722\n","Epoch  5 Batch  52 / 228  Training Loss  0.0019576374907046556\n","Epoch  5 Batch  53 / 228  Training Loss  0.0016292175278067589\n","Epoch  5 Batch  54 / 228  Training Loss  0.0011805298272520304\n","Epoch  5 Batch  55 / 228  Training Loss  0.0013174453051760793\n","Epoch  5 Batch  56 / 228  Training Loss  0.00334607670083642\n","Epoch  5 Batch  57 / 228  Training Loss  0.002378851640969515\n","Epoch  5 Batch  58 / 228  Training Loss  0.0039048120379447937\n","Epoch  5 Batch  59 / 228  Training Loss  0.002764409175142646\n","Epoch  5 Batch  60 / 228  Training Loss  0.004523871932178736\n","Epoch  5 Batch  61 / 228  Training Loss  0.0076874056831002235\n","Epoch  5 Batch  62 / 228  Training Loss  0.012576110661029816\n","Epoch  5 Batch  63 / 228  Training Loss  0.014656370505690575\n","Epoch  5 Batch  64 / 228  Training Loss  0.007122155278921127\n","Epoch  5 Batch  65 / 228  Training Loss  0.0024491294752806425\n","Epoch  5 Batch  66 / 228  Training Loss  0.0029430941212922335\n","Epoch  5 Batch  67 / 228  Training Loss  0.0013944536913186312\n","Epoch  5 Batch  68 / 228  Training Loss  0.002608703449368477\n","Epoch  5 Batch  69 / 228  Training Loss  0.0029953080229461193\n","Epoch  5 Batch  70 / 228  Training Loss  0.0010577774373814464\n","Epoch  5 Batch  71 / 228  Training Loss  0.0027048340998589993\n","Epoch  5 Batch  72 / 228  Training Loss  0.0010154006304219365\n","Epoch  5 Batch  73 / 228  Training Loss  0.0006901586311869323\n","Epoch  5 Batch  74 / 228  Training Loss  0.0035409159027040005\n","Epoch  5 Batch  75 / 228  Training Loss  0.0013088762061670423\n","Epoch  5 Batch  76 / 228  Training Loss  0.006885179318487644\n","Epoch  5 Batch  77 / 228  Training Loss  0.005697816610336304\n","Epoch  5 Batch  78 / 228  Training Loss  0.003346644341945648\n","Epoch  5 Batch  79 / 228  Training Loss  0.00629396503791213\n","Epoch  5 Batch  80 / 228  Training Loss  0.003951557911932468\n","Epoch  5 Batch  81 / 228  Training Loss  0.0014473095070570707\n","Epoch  5 Batch  82 / 228  Training Loss  0.002376538934186101\n","Epoch  5 Batch  83 / 228  Training Loss  0.007020936813205481\n","Epoch  5 Batch  84 / 228  Training Loss  0.003020819276571274\n","Epoch  5 Batch  85 / 228  Training Loss  0.0018401469569653273\n","Epoch  5 Batch  86 / 228  Training Loss  0.003102189162746072\n","Epoch  5 Batch  87 / 228  Training Loss  0.005519484169781208\n","Epoch  5 Batch  88 / 228  Training Loss  0.002677047858014703\n","Epoch  5 Batch  89 / 228  Training Loss  0.001232546754181385\n","Epoch  5 Batch  90 / 228  Training Loss  0.0020197140984237194\n","Epoch  5 Batch  91 / 228  Training Loss  0.0015602116473019123\n","Epoch  5 Batch  92 / 228  Training Loss  0.001627907156944275\n","Epoch  5 Batch  93 / 228  Training Loss  0.006566028110682964\n","Epoch  5 Batch  94 / 228  Training Loss  0.00799448974430561\n","Epoch  5 Batch  95 / 228  Training Loss  0.0062827663496136665\n","Epoch  5 Batch  96 / 228  Training Loss  0.008680435828864574\n","Epoch  5 Batch  97 / 228  Training Loss  0.012225499376654625\n","Epoch  5 Batch  98 / 228  Training Loss  0.005336428061127663\n","Epoch  5 Batch  99 / 228  Training Loss  0.0031757198739796877\n","Epoch  5 Batch  100 / 228  Training Loss  0.0036396528594195843\n","Epoch  5 Batch  101 / 228  Training Loss  0.0011849780566990376\n","Epoch  5 Batch  102 / 228  Training Loss  0.006715014576911926\n","Epoch  5 Batch  103 / 228  Training Loss  0.0018220935016870499\n","Epoch  5 Batch  104 / 228  Training Loss  0.0013742061564698815\n","Epoch  5 Batch  105 / 228  Training Loss  0.0014171877410262823\n","Epoch  5 Batch  106 / 228  Training Loss  0.0018367285374552011\n","Epoch  5 Batch  107 / 228  Training Loss  0.0011034180643036962\n","Epoch  5 Batch  108 / 228  Training Loss  0.0015461146831512451\n","Epoch  5 Batch  109 / 228  Training Loss  0.0008200296433642507\n","Epoch  5 Batch  110 / 228  Training Loss  0.001695717335678637\n","Epoch  5 Batch  111 / 228  Training Loss  0.001633480191230774\n","Epoch  5 Batch  112 / 228  Training Loss  0.0009886916959658265\n","Epoch  5 Batch  113 / 228  Training Loss  0.0013401887845247984\n","Epoch  5 Batch  114 / 228  Training Loss  0.0016191421309486032\n","Epoch  5 Batch  115 / 228  Training Loss  0.006481790449470282\n","Epoch  5 Batch  116 / 228  Training Loss  0.0034679423552006483\n","Epoch  5 Batch  117 / 228  Training Loss  0.004436425864696503\n","Epoch  5 Batch  118 / 228  Training Loss  0.0067680091597139835\n","Epoch  5 Batch  119 / 228  Training Loss  0.002980263205245137\n","Epoch  5 Batch  120 / 228  Training Loss  0.001496144337579608\n","Epoch  5 Batch  121 / 228  Training Loss  0.001038282411172986\n","Epoch  5 Batch  122 / 228  Training Loss  0.003408490214496851\n","Epoch  5 Batch  123 / 228  Training Loss  0.0008165263570845127\n","Epoch  5 Batch  124 / 228  Training Loss  0.002869687508791685\n","Epoch  5 Batch  125 / 228  Training Loss  0.006624673958867788\n","Epoch  5 Batch  126 / 228  Training Loss  0.00878964364528656\n","Epoch  5 Batch  127 / 228  Training Loss  0.0040276385843753815\n","Epoch  5 Batch  128 / 228  Training Loss  0.0022335865069180727\n","Epoch  5 Batch  129 / 228  Training Loss  0.0060796719044446945\n","Epoch  5 Batch  130 / 228  Training Loss  0.009348695166409016\n","Epoch  5 Batch  131 / 228  Training Loss  0.0018100751331076026\n","Epoch  5 Batch  132 / 228  Training Loss  0.0010239635594189167\n","Epoch  5 Batch  133 / 228  Training Loss  0.0009966628858819604\n","Epoch  5 Batch  134 / 228  Training Loss  0.003200935199856758\n","Epoch  5 Batch  135 / 228  Training Loss  0.009809778071939945\n","Epoch  5 Batch  136 / 228  Training Loss  0.01097842026501894\n","Epoch  5 Batch  137 / 228  Training Loss  0.00806349515914917\n","Epoch  5 Batch  138 / 228  Training Loss  0.0024122903123497963\n","Epoch  5 Batch  139 / 228  Training Loss  0.006827139761298895\n","Epoch  5 Batch  140 / 228  Training Loss  0.00742859672755003\n","Epoch  5 Batch  141 / 228  Training Loss  0.005300262942910194\n","Epoch  5 Batch  142 / 228  Training Loss  0.004999470431357622\n","Epoch  5 Batch  143 / 228  Training Loss  0.0021028374321758747\n","Epoch  5 Batch  144 / 228  Training Loss  0.0032168577890843153\n","Epoch  5 Batch  145 / 228  Training Loss  0.001418844098225236\n","Epoch  5 Batch  146 / 228  Training Loss  0.004586762748658657\n","Epoch  5 Batch  147 / 228  Training Loss  0.0022735395468771458\n","Epoch  5 Batch  148 / 228  Training Loss  0.0013171774335205555\n","Epoch  5 Batch  149 / 228  Training Loss  0.0014751602429896593\n","Epoch  5 Batch  150 / 228  Training Loss  0.003398455213755369\n","Epoch  5 Batch  151 / 228  Training Loss  0.007396545261144638\n","Epoch  5 Batch  152 / 228  Training Loss  0.012253176420927048\n","Epoch  5 Batch  153 / 228  Training Loss  0.006087617948651314\n","Epoch  5 Batch  154 / 228  Training Loss  0.005643914453685284\n","Epoch  5 Batch  155 / 228  Training Loss  0.001413320773281157\n","Epoch  5 Batch  156 / 228  Training Loss  0.007906263694167137\n","Epoch  5 Batch  157 / 228  Training Loss  0.0035923339892178774\n","Epoch  5 Batch  158 / 228  Training Loss  0.001305042882449925\n","Epoch  5 Batch  159 / 228  Training Loss  0.0019780476577579975\n","Epoch  5 Batch  160 / 228  Training Loss  0.0030992524698376656\n","Epoch  5 Batch  161 / 228  Training Loss  0.0013129806611686945\n","Epoch  5 Batch  162 / 228  Training Loss  0.0024677079636603594\n","Epoch  5 Batch  163 / 228  Training Loss  0.0010782016906887293\n","Epoch  5 Batch  164 / 228  Training Loss  0.0029449001885950565\n","Epoch  5 Batch  165 / 228  Training Loss  0.0025394042022526264\n","Epoch  5 Batch  166 / 228  Training Loss  0.0017302712658420205\n","Epoch  5 Batch  167 / 228  Training Loss  0.0022739230189472437\n","Epoch  5 Batch  168 / 228  Training Loss  0.0011291028931736946\n","Epoch  5 Batch  169 / 228  Training Loss  0.0011945177102461457\n","Epoch  5 Batch  170 / 228  Training Loss  0.001269338303245604\n","Epoch  5 Batch  171 / 228  Training Loss  0.0013022508937865496\n","Epoch  5 Batch  172 / 228  Training Loss  0.00218472839333117\n","Epoch  5 Batch  173 / 228  Training Loss  0.000995426787994802\n","Epoch  5 Batch  174 / 228  Training Loss  0.0010871946578845382\n","Epoch  5 Batch  175 / 228  Training Loss  0.0016594926128163934\n","Epoch  5 Batch  176 / 228  Training Loss  0.0014414279721677303\n","Epoch  5 Batch  177 / 228  Training Loss  0.0049307821318507195\n","Epoch  5 Batch  178 / 228  Training Loss  0.0005400863010436296\n","Epoch  5 Batch  179 / 228  Training Loss  0.00169424619525671\n","Epoch  5 Batch  180 / 228  Training Loss  0.0013766272459179163\n","Epoch  5 Batch  181 / 228  Training Loss  0.0013943170197308064\n","Epoch  5 Batch  182 / 228  Training Loss  0.0009865306783467531\n","Epoch  5 Batch  183 / 228  Training Loss  0.0015231604920700192\n","Epoch  5 Batch  184 / 228  Training Loss  0.0042898328974843025\n","Epoch  5 Batch  185 / 228  Training Loss  0.013956541195511818\n","Epoch  5 Batch  186 / 228  Training Loss  0.008136263117194176\n","Epoch  5 Batch  187 / 228  Training Loss  0.0057647619396448135\n","Epoch  5 Batch  188 / 228  Training Loss  0.004725827369838953\n","Epoch  5 Batch  189 / 228  Training Loss  0.0044727264903485775\n","Epoch  5 Batch  190 / 228  Training Loss  0.012758448719978333\n","Epoch  5 Batch  191 / 228  Training Loss  0.003779323771595955\n","Epoch  5 Batch  192 / 228  Training Loss  0.0018683851230889559\n","Epoch  5 Batch  193 / 228  Training Loss  0.004361937753856182\n","Epoch  5 Batch  194 / 228  Training Loss  0.003497103229165077\n","Epoch  5 Batch  195 / 228  Training Loss  0.0012100707972422242\n","Epoch  5 Batch  196 / 228  Training Loss  0.00328877498395741\n","Epoch  5 Batch  197 / 228  Training Loss  0.0028958686161786318\n","Epoch  5 Batch  198 / 228  Training Loss  0.0015838422114029527\n","Epoch  5 Batch  199 / 228  Training Loss  0.0007626062724739313\n","Epoch  5 Batch  200 / 228  Training Loss  0.0016646754229441285\n","Epoch  5 Batch  201 / 228  Training Loss  0.0013306892942637205\n","Epoch  5 Batch  202 / 228  Training Loss  0.0018201980274170637\n","Epoch  5 Batch  203 / 228  Training Loss  0.0020110481418669224\n","Epoch  5 Batch  204 / 228  Training Loss  0.002755196066573262\n","Epoch  5 Batch  205 / 228  Training Loss  0.001433972385711968\n","Epoch  5 Batch  206 / 228  Training Loss  0.002077760873362422\n","Epoch  5 Batch  207 / 228  Training Loss  0.001338350586593151\n","Epoch  5 Batch  208 / 228  Training Loss  0.0012727024732157588\n","Epoch  5 Batch  209 / 228  Training Loss  0.0016758829588070512\n","Epoch  5 Batch  210 / 228  Training Loss  0.002189663937315345\n","Epoch  5 Batch  211 / 228  Training Loss  0.002391223097220063\n","Epoch  5 Batch  212 / 228  Training Loss  0.0022732499055564404\n","Epoch  5 Batch  213 / 228  Training Loss  0.0014505319995805621\n","Epoch  5 Batch  214 / 228  Training Loss  0.0010763282189145684\n","Epoch  5 Batch  215 / 228  Training Loss  0.001796133117750287\n","Epoch  5 Batch  216 / 228  Training Loss  0.002343726810067892\n","Epoch  5 Batch  217 / 228  Training Loss  0.0007256251992657781\n","Epoch  5 Batch  218 / 228  Training Loss  0.0016454097349196672\n","Epoch  5 Batch  219 / 228  Training Loss  0.0018653462175279856\n","Epoch  5 Batch  220 / 228  Training Loss  0.0007769048679620028\n","Epoch  5 Batch  221 / 228  Training Loss  0.0008599821594543755\n","Epoch  5 Batch  222 / 228  Training Loss  0.0013987008715048432\n","Epoch  5 Batch  223 / 228  Training Loss  0.0009821377461776137\n","Epoch  5 Batch  224 / 228  Training Loss  0.0009209872223436832\n","Epoch  5 Batch  225 / 228  Training Loss  0.00221287296153605\n","Epoch  5 Batch  226 / 228  Training Loss  0.0009585963562130928\n","Epoch  5 Batch  227 / 228  Training Loss  0.0011878389632329345\n","   6    |    -    |   0.003785   | 98.513720\n","----------------------------------------------------------------------\n","Running epoch: 6\n","Epoch  6 Batch  0 / 228  Training Loss  0.0005104143056087196\n","Epoch  6 Batch  1 / 228  Training Loss  0.002043668646365404\n","Epoch  6 Batch  2 / 228  Training Loss  0.0005198850994929671\n","Epoch  6 Batch  3 / 228  Training Loss  0.0009465026669204235\n","Epoch  6 Batch  4 / 228  Training Loss  0.0009134189458563924\n","Epoch  6 Batch  5 / 228  Training Loss  0.00103872106410563\n","Epoch  6 Batch  6 / 228  Training Loss  0.0008464775746688247\n","Epoch  6 Batch  7 / 228  Training Loss  0.0005740708438679576\n","Epoch  6 Batch  8 / 228  Training Loss  0.0005688030505552888\n","Epoch  6 Batch  9 / 228  Training Loss  0.0007811050163581967\n","Epoch  6 Batch  10 / 228  Training Loss  0.0009635469759814441\n","Epoch  6 Batch  11 / 228  Training Loss  0.007908566854894161\n","Epoch  6 Batch  12 / 228  Training Loss  0.0019559403881430626\n","Epoch  6 Batch  13 / 228  Training Loss  0.001151753356680274\n","Epoch  6 Batch  14 / 228  Training Loss  0.0015870071947574615\n","Epoch  6 Batch  15 / 228  Training Loss  0.0019391307141631842\n","Epoch  6 Batch  16 / 228  Training Loss  0.0015575316501781344\n","Epoch  6 Batch  17 / 228  Training Loss  0.005027889274060726\n","Epoch  6 Batch  18 / 228  Training Loss  0.002600181382149458\n","Epoch  6 Batch  19 / 228  Training Loss  0.023359932005405426\n","Epoch  6 Batch  20 / 228  Training Loss  0.012258313596248627\n","Epoch  6 Batch  21 / 228  Training Loss  0.01028046477586031\n","Epoch  6 Batch  22 / 228  Training Loss  0.011826705187559128\n","Epoch  6 Batch  23 / 228  Training Loss  0.001265004393644631\n","Epoch  6 Batch  24 / 228  Training Loss  0.007943484000861645\n","Epoch  6 Batch  25 / 228  Training Loss  0.0011962747666984797\n","Epoch  6 Batch  26 / 228  Training Loss  0.0009291664464399219\n","Epoch  6 Batch  27 / 228  Training Loss  0.0030673067085444927\n","Epoch  6 Batch  28 / 228  Training Loss  0.0010985226836055517\n","Epoch  6 Batch  29 / 228  Training Loss  0.0011387112317606807\n","Epoch  6 Batch  30 / 228  Training Loss  0.0007793998229317367\n","Epoch  6 Batch  31 / 228  Training Loss  0.001157875289209187\n","Epoch  6 Batch  32 / 228  Training Loss  0.0005862180842086673\n","Epoch  6 Batch  33 / 228  Training Loss  0.0005788352573290467\n","Epoch  6 Batch  34 / 228  Training Loss  0.0008022768306545913\n","Epoch  6 Batch  35 / 228  Training Loss  0.0010094098979607224\n","Epoch  6 Batch  36 / 228  Training Loss  0.001363162649795413\n","Epoch  6 Batch  37 / 228  Training Loss  0.0007496889447793365\n","Epoch  6 Batch  38 / 228  Training Loss  0.0010478587355464697\n","Epoch  6 Batch  39 / 228  Training Loss  0.004191241692751646\n","Epoch  6 Batch  40 / 228  Training Loss  0.020424000918865204\n","Epoch  6 Batch  41 / 228  Training Loss  0.007335800677537918\n","Epoch  6 Batch  42 / 228  Training Loss  0.0007071615546010435\n","Epoch  6 Batch  43 / 228  Training Loss  0.0011147321201860905\n","Epoch  6 Batch  44 / 228  Training Loss  0.003138825297355652\n","Epoch  6 Batch  45 / 228  Training Loss  0.0021478720009326935\n","Epoch  6 Batch  46 / 228  Training Loss  0.0011014796327799559\n","Epoch  6 Batch  47 / 228  Training Loss  0.0013252272037789226\n","Epoch  6 Batch  48 / 228  Training Loss  0.0009107129881158471\n","Epoch  6 Batch  49 / 228  Training Loss  0.0012095029233023524\n","Epoch  6 Batch  50 / 228  Training Loss  0.0024350029416382313\n","Epoch  6 Batch  51 / 228  Training Loss  0.0011383288074284792\n","Epoch  6 Batch  52 / 228  Training Loss  0.0003792288771364838\n","Epoch  6 Batch  53 / 228  Training Loss  0.0009635958122089505\n","Epoch  6 Batch  54 / 228  Training Loss  0.0017560943961143494\n","Epoch  6 Batch  55 / 228  Training Loss  0.0005392517196014524\n","Epoch  6 Batch  56 / 228  Training Loss  0.001006278209388256\n","Epoch  6 Batch  57 / 228  Training Loss  0.0017730528488755226\n","Epoch  6 Batch  58 / 228  Training Loss  0.0006443841848522425\n","Epoch  6 Batch  59 / 228  Training Loss  0.0005797230405732989\n","Epoch  6 Batch  60 / 228  Training Loss  0.003436549799516797\n","Epoch  6 Batch  61 / 228  Training Loss  0.002477701287716627\n","Epoch  6 Batch  62 / 228  Training Loss  0.0010121730156242847\n","Epoch  6 Batch  63 / 228  Training Loss  0.0019155803602188826\n","Epoch  6 Batch  64 / 228  Training Loss  0.0011618208372965455\n","Epoch  6 Batch  65 / 228  Training Loss  0.0009398719412274659\n","Epoch  6 Batch  66 / 228  Training Loss  0.004755773581564426\n","Epoch  6 Batch  67 / 228  Training Loss  0.0008518627728335559\n","Epoch  6 Batch  68 / 228  Training Loss  0.002121873665601015\n","Epoch  6 Batch  69 / 228  Training Loss  0.0013728853082284331\n","Epoch  6 Batch  70 / 228  Training Loss  0.0006178741459734738\n","Epoch  6 Batch  71 / 228  Training Loss  0.0006909873918630183\n","Epoch  6 Batch  72 / 228  Training Loss  0.0018942489987239242\n","Epoch  6 Batch  73 / 228  Training Loss  0.0005676466389559209\n","Epoch  6 Batch  74 / 228  Training Loss  0.0015285407425835729\n","Epoch  6 Batch  75 / 228  Training Loss  0.0011442956747487187\n","Epoch  6 Batch  76 / 228  Training Loss  0.000803816132247448\n","Epoch  6 Batch  77 / 228  Training Loss  0.0009244208922609687\n","Epoch  6 Batch  78 / 228  Training Loss  0.0006054495461285114\n","Epoch  6 Batch  79 / 228  Training Loss  0.000857891864143312\n","Epoch  6 Batch  80 / 228  Training Loss  0.00183834508061409\n","Epoch  6 Batch  81 / 228  Training Loss  0.0008248413796536624\n","Epoch  6 Batch  82 / 228  Training Loss  0.0030057658441364765\n","Epoch  6 Batch  83 / 228  Training Loss  0.001436747843399644\n","Epoch  6 Batch  84 / 228  Training Loss  0.0004922029911540449\n","Epoch  6 Batch  85 / 228  Training Loss  0.0009762568515725434\n","Epoch  6 Batch  86 / 228  Training Loss  0.0006192942382767797\n","Epoch  6 Batch  87 / 228  Training Loss  0.0004287350457161665\n","Epoch  6 Batch  88 / 228  Training Loss  0.0005231910035945475\n","Epoch  6 Batch  89 / 228  Training Loss  0.00033848691964522004\n","Epoch  6 Batch  90 / 228  Training Loss  0.0005495453951880336\n","Epoch  6 Batch  91 / 228  Training Loss  0.0006170617998577654\n","Epoch  6 Batch  92 / 228  Training Loss  0.0008872415637597442\n","Epoch  6 Batch  93 / 228  Training Loss  0.0008749892003834248\n","Epoch  6 Batch  94 / 228  Training Loss  0.0004325445624999702\n","Epoch  6 Batch  95 / 228  Training Loss  0.0004912026925012469\n","Epoch  6 Batch  96 / 228  Training Loss  0.0007398569723591208\n","Epoch  6 Batch  97 / 228  Training Loss  0.00028690919862128794\n","Epoch  6 Batch  98 / 228  Training Loss  0.0005118130939081311\n","Epoch  6 Batch  99 / 228  Training Loss  0.0005241369944997132\n","Epoch  6 Batch  100 / 228  Training Loss  0.000395564129576087\n","Epoch  6 Batch  101 / 228  Training Loss  0.0007005817024037242\n","Epoch  6 Batch  102 / 228  Training Loss  0.00048132665688171983\n","Epoch  6 Batch  103 / 228  Training Loss  0.000501565751619637\n","Epoch  6 Batch  104 / 228  Training Loss  0.000655274954624474\n","Epoch  6 Batch  105 / 228  Training Loss  0.0016136184567585588\n","Epoch  6 Batch  106 / 228  Training Loss  0.0006246150005608797\n","Epoch  6 Batch  107 / 228  Training Loss  0.00035256214323453605\n","Epoch  6 Batch  108 / 228  Training Loss  0.00033335620537400246\n","Epoch  6 Batch  109 / 228  Training Loss  0.0005625710473395884\n","Epoch  6 Batch  110 / 228  Training Loss  0.0005106059252284467\n","Epoch  6 Batch  111 / 228  Training Loss  0.0011296632001176476\n","Epoch  6 Batch  112 / 228  Training Loss  0.001373565522953868\n","Epoch  6 Batch  113 / 228  Training Loss  0.0005928660975769162\n","Epoch  6 Batch  114 / 228  Training Loss  0.0004401967744342983\n","Epoch  6 Batch  115 / 228  Training Loss  0.0007686329772695899\n","Epoch  6 Batch  116 / 228  Training Loss  0.0005405667470768094\n","Epoch  6 Batch  117 / 228  Training Loss  0.0024441557470709085\n","Epoch  6 Batch  118 / 228  Training Loss  0.0007014705333858728\n","Epoch  6 Batch  119 / 228  Training Loss  0.006931290961802006\n","Epoch  6 Batch  120 / 228  Training Loss  0.007578366436064243\n","Epoch  6 Batch  121 / 228  Training Loss  0.0019337361445650458\n","Epoch  6 Batch  122 / 228  Training Loss  0.006174120120704174\n","Epoch  6 Batch  123 / 228  Training Loss  0.00040620454819872975\n","Epoch  6 Batch  124 / 228  Training Loss  0.0010626241564750671\n","Epoch  6 Batch  125 / 228  Training Loss  0.00099189265165478\n","Epoch  6 Batch  126 / 228  Training Loss  0.00043418718269094825\n","Epoch  6 Batch  127 / 228  Training Loss  0.0015645228559151292\n","Epoch  6 Batch  128 / 228  Training Loss  0.0011620098957791924\n","Epoch  6 Batch  129 / 228  Training Loss  0.0006190546555444598\n","Epoch  6 Batch  130 / 228  Training Loss  0.0008931032498367131\n","Epoch  6 Batch  131 / 228  Training Loss  0.001391845173202455\n","Epoch  6 Batch  132 / 228  Training Loss  0.0009764048154465854\n","Epoch  6 Batch  133 / 228  Training Loss  0.0006055053090676665\n","Epoch  6 Batch  134 / 228  Training Loss  0.0011269132373854518\n","Epoch  6 Batch  135 / 228  Training Loss  0.017776913940906525\n","Epoch  6 Batch  136 / 228  Training Loss  0.0005591282388195395\n","Epoch  6 Batch  137 / 228  Training Loss  0.005565526429563761\n","Epoch  6 Batch  138 / 228  Training Loss  0.010449441149830818\n","Epoch  6 Batch  139 / 228  Training Loss  0.0006854924140498042\n","Epoch  6 Batch  140 / 228  Training Loss  0.0023565939627587795\n","Epoch  6 Batch  141 / 228  Training Loss  0.0018927784403786063\n","Epoch  6 Batch  142 / 228  Training Loss  0.0006107566296122968\n","Epoch  6 Batch  143 / 228  Training Loss  0.0007932144217193127\n","Epoch  6 Batch  144 / 228  Training Loss  0.0009463033638894558\n","Epoch  6 Batch  145 / 228  Training Loss  0.0025916164740920067\n","Epoch  6 Batch  146 / 228  Training Loss  0.0010380835738033056\n","Epoch  6 Batch  147 / 228  Training Loss  0.0008674285490997136\n","Epoch  6 Batch  148 / 228  Training Loss  0.0003956284490413964\n","Epoch  6 Batch  149 / 228  Training Loss  0.000616838107816875\n","Epoch  6 Batch  150 / 228  Training Loss  0.0007591679459437728\n","Epoch  6 Batch  151 / 228  Training Loss  0.0032114931382238865\n","Epoch  6 Batch  152 / 228  Training Loss  0.00044094849727116525\n","Epoch  6 Batch  153 / 228  Training Loss  0.00040928288944996893\n","Epoch  6 Batch  154 / 228  Training Loss  0.0007590804598294199\n","Epoch  6 Batch  155 / 228  Training Loss  0.0019682415295392275\n","Epoch  6 Batch  156 / 228  Training Loss  0.0007658798131160438\n","Epoch  6 Batch  157 / 228  Training Loss  0.00040903850458562374\n","Epoch  6 Batch  158 / 228  Training Loss  0.001182075822725892\n","Epoch  6 Batch  159 / 228  Training Loss  0.0008561645518057048\n","Epoch  6 Batch  160 / 228  Training Loss  0.000513576262164861\n","Epoch  6 Batch  161 / 228  Training Loss  0.0011639866279438138\n","Epoch  6 Batch  162 / 228  Training Loss  0.000728107406757772\n","Epoch  6 Batch  163 / 228  Training Loss  0.0004484631062950939\n","Epoch  6 Batch  164 / 228  Training Loss  0.00038081349339336157\n","Epoch  6 Batch  165 / 228  Training Loss  0.00039391740574501455\n","Epoch  6 Batch  166 / 228  Training Loss  0.0006713784532621503\n","Epoch  6 Batch  167 / 228  Training Loss  0.0005860610399395227\n","Epoch  6 Batch  168 / 228  Training Loss  0.0007062213844619691\n","Epoch  6 Batch  169 / 228  Training Loss  0.0007606407161802053\n","Epoch  6 Batch  170 / 228  Training Loss  0.0007025198428891599\n","Epoch  6 Batch  171 / 228  Training Loss  0.0003763858403544873\n","Epoch  6 Batch  172 / 228  Training Loss  0.0002907229063566774\n","Epoch  6 Batch  173 / 228  Training Loss  0.0005580770084634423\n","Epoch  6 Batch  174 / 228  Training Loss  0.0006440563593059778\n","Epoch  6 Batch  175 / 228  Training Loss  0.0005434529739432037\n","Epoch  6 Batch  176 / 228  Training Loss  0.0009264649706892669\n","Epoch  6 Batch  177 / 228  Training Loss  0.0005856977077201009\n","Epoch  6 Batch  178 / 228  Training Loss  0.0006273988401517272\n","Epoch  6 Batch  179 / 228  Training Loss  0.0006230910657905042\n","Epoch  6 Batch  180 / 228  Training Loss  0.0003689651202876121\n","Epoch  6 Batch  181 / 228  Training Loss  0.0003363100113347173\n","Epoch  6 Batch  182 / 228  Training Loss  0.0003826601314358413\n","Epoch  6 Batch  183 / 228  Training Loss  0.00033750158036127687\n","Epoch  6 Batch  184 / 228  Training Loss  0.0014323905343189836\n","Epoch  6 Batch  185 / 228  Training Loss  0.0009892403613775969\n","Epoch  6 Batch  186 / 228  Training Loss  0.0009081402095034719\n","Epoch  6 Batch  187 / 228  Training Loss  0.0012353990459814668\n","Epoch  6 Batch  188 / 228  Training Loss  0.002833196660503745\n","Epoch  6 Batch  189 / 228  Training Loss  0.0010171994799748063\n","Epoch  6 Batch  190 / 228  Training Loss  0.0007754479302093387\n","Epoch  6 Batch  191 / 228  Training Loss  0.00047107087448239326\n","Epoch  6 Batch  192 / 228  Training Loss  0.0008008275181055069\n","Epoch  6 Batch  193 / 228  Training Loss  0.0007771583623252809\n","Epoch  6 Batch  194 / 228  Training Loss  0.00037665569107048213\n","Epoch  6 Batch  195 / 228  Training Loss  0.0013847241643816233\n","Epoch  6 Batch  196 / 228  Training Loss  0.0007405588985420763\n","Epoch  6 Batch  197 / 228  Training Loss  0.0005476257647387683\n","Epoch  6 Batch  198 / 228  Training Loss  0.0005366363329812884\n","Epoch  6 Batch  199 / 228  Training Loss  0.000533783808350563\n","Epoch  6 Batch  200 / 228  Training Loss  0.00028141163056716323\n","Epoch  6 Batch  201 / 228  Training Loss  0.0005159461870789528\n","Epoch  6 Batch  202 / 228  Training Loss  0.0002666596556082368\n","Epoch  6 Batch  203 / 228  Training Loss  0.0005468236049637198\n","Epoch  6 Batch  204 / 228  Training Loss  0.00037434339174069464\n","Epoch  6 Batch  205 / 228  Training Loss  0.0005107850884087384\n","Epoch  6 Batch  206 / 228  Training Loss  0.0003319158568046987\n","Epoch  6 Batch  207 / 228  Training Loss  0.0007084671524353325\n","Epoch  6 Batch  208 / 228  Training Loss  0.00036910161725245416\n","Epoch  6 Batch  209 / 228  Training Loss  0.0005220846505835652\n","Epoch  6 Batch  210 / 228  Training Loss  0.0005738533800467849\n","Epoch  6 Batch  211 / 228  Training Loss  0.001770361908711493\n","Epoch  6 Batch  212 / 228  Training Loss  0.003387580858543515\n","Epoch  6 Batch  213 / 228  Training Loss  0.0006543455529026687\n","Epoch  6 Batch  214 / 228  Training Loss  0.00024175961152650416\n","Epoch  6 Batch  215 / 228  Training Loss  0.0006038714782334864\n","Epoch  6 Batch  216 / 228  Training Loss  0.0008504173601977527\n","Epoch  6 Batch  217 / 228  Training Loss  0.0028411801904439926\n","Epoch  6 Batch  218 / 228  Training Loss  0.012913083657622337\n","Epoch  6 Batch  219 / 228  Training Loss  0.006286047399044037\n","Epoch  6 Batch  220 / 228  Training Loss  0.0007460631895810366\n","Epoch  6 Batch  221 / 228  Training Loss  0.002573687583208084\n","Epoch  6 Batch  222 / 228  Training Loss  0.00782076083123684\n","Epoch  6 Batch  223 / 228  Training Loss  0.0033020093105733395\n","Epoch  6 Batch  224 / 228  Training Loss  0.003878713818266988\n","Epoch  6 Batch  225 / 228  Training Loss  0.0007728073978796601\n","Epoch  6 Batch  226 / 228  Training Loss  0.0015050320653244853\n","Epoch  6 Batch  227 / 228  Training Loss  0.00034973054425790906\n","   7    |    -    |   0.001788   | 98.285061\n","----------------------------------------------------------------------\n","Running epoch: 7\n","Epoch  7 Batch  0 / 228  Training Loss  0.000996792339719832\n","Epoch  7 Batch  1 / 228  Training Loss  0.00046249869046732783\n","Epoch  7 Batch  2 / 228  Training Loss  0.015538858249783516\n","Epoch  7 Batch  3 / 228  Training Loss  0.023652715608477592\n","Epoch  7 Batch  4 / 228  Training Loss  0.005693014711141586\n","Epoch  7 Batch  5 / 228  Training Loss  0.011719386093318462\n","Epoch  7 Batch  6 / 228  Training Loss  0.0024077887646853924\n","Epoch  7 Batch  7 / 228  Training Loss  0.0012087608920410275\n","Epoch  7 Batch  8 / 228  Training Loss  0.004073810763657093\n","Epoch  7 Batch  9 / 228  Training Loss  0.0005818247445859015\n","Epoch  7 Batch  10 / 228  Training Loss  0.0007917420007288456\n","Epoch  7 Batch  11 / 228  Training Loss  0.0009289498557336628\n","Epoch  7 Batch  12 / 228  Training Loss  0.00044130985043011606\n","Epoch  7 Batch  13 / 228  Training Loss  0.002915447810664773\n","Epoch  7 Batch  14 / 228  Training Loss  0.000563582347240299\n","Epoch  7 Batch  15 / 228  Training Loss  0.0010695188539102674\n","Epoch  7 Batch  16 / 228  Training Loss  0.004395289812237024\n","Epoch  7 Batch  17 / 228  Training Loss  0.0004892838769592345\n","Epoch  7 Batch  18 / 228  Training Loss  0.0004517983761616051\n","Epoch  7 Batch  19 / 228  Training Loss  0.0008218937437050045\n","Epoch  7 Batch  20 / 228  Training Loss  0.0006647350965067744\n","Epoch  7 Batch  21 / 228  Training Loss  0.0020334566943347454\n","Epoch  7 Batch  22 / 228  Training Loss  0.0018727686256170273\n","Epoch  7 Batch  23 / 228  Training Loss  0.0004266545583959669\n","Epoch  7 Batch  24 / 228  Training Loss  0.0005611818050965667\n","Epoch  7 Batch  25 / 228  Training Loss  0.0007450978737324476\n","Epoch  7 Batch  26 / 228  Training Loss  0.00039285901584662497\n","Epoch  7 Batch  27 / 228  Training Loss  0.0003644044336397201\n","Epoch  7 Batch  28 / 228  Training Loss  0.0003281918470747769\n","Epoch  7 Batch  29 / 228  Training Loss  0.0006586985546164215\n","Epoch  7 Batch  30 / 228  Training Loss  0.0005734172882512212\n","Epoch  7 Batch  31 / 228  Training Loss  0.0005389579455368221\n","Epoch  7 Batch  32 / 228  Training Loss  0.00031787570333108306\n","Epoch  7 Batch  33 / 228  Training Loss  0.0003667893761303276\n","Epoch  7 Batch  34 / 228  Training Loss  0.00038826436502858996\n","Epoch  7 Batch  35 / 228  Training Loss  0.0003353007195983082\n","Epoch  7 Batch  36 / 228  Training Loss  0.000462062016595155\n","Epoch  7 Batch  37 / 228  Training Loss  0.00027720286743715405\n","Epoch  7 Batch  38 / 228  Training Loss  0.0007497965707443655\n","Epoch  7 Batch  39 / 228  Training Loss  0.0010679236147552729\n","Epoch  7 Batch  40 / 228  Training Loss  0.00034997769398614764\n","Epoch  7 Batch  41 / 228  Training Loss  0.0056439791806042194\n","Epoch  7 Batch  42 / 228  Training Loss  0.0003228878485970199\n","Epoch  7 Batch  43 / 228  Training Loss  0.0007446903618983924\n","Epoch  7 Batch  44 / 228  Training Loss  0.0009441602160222828\n","Epoch  7 Batch  45 / 228  Training Loss  0.0009413512307219207\n","Epoch  7 Batch  46 / 228  Training Loss  0.0005774408346042037\n","Epoch  7 Batch  47 / 228  Training Loss  0.0008075133664533496\n","Epoch  7 Batch  48 / 228  Training Loss  0.0004222535062581301\n","Epoch  7 Batch  49 / 228  Training Loss  0.005072011146694422\n","Epoch  7 Batch  50 / 228  Training Loss  0.0006259091896936297\n","Epoch  7 Batch  51 / 228  Training Loss  0.0007269847556017339\n","Epoch  7 Batch  52 / 228  Training Loss  0.0070587038062512875\n","Epoch  7 Batch  53 / 228  Training Loss  0.0023829322308301926\n","Epoch  7 Batch  54 / 228  Training Loss  0.007789536379277706\n","Epoch  7 Batch  55 / 228  Training Loss  0.0037039145827293396\n","Epoch  7 Batch  56 / 228  Training Loss  0.008564077317714691\n","Epoch  7 Batch  57 / 228  Training Loss  0.0014471521135419607\n","Epoch  7 Batch  58 / 228  Training Loss  0.0006869792705401778\n","Epoch  7 Batch  59 / 228  Training Loss  0.0007413477869704366\n","Epoch  7 Batch  60 / 228  Training Loss  0.0026330945547670126\n","Epoch  7 Batch  61 / 228  Training Loss  0.000735321082174778\n","Epoch  7 Batch  62 / 228  Training Loss  0.0009591563721187413\n","Epoch  7 Batch  63 / 228  Training Loss  0.0012772430200129747\n","Epoch  7 Batch  64 / 228  Training Loss  0.0006746099679730833\n","Epoch  7 Batch  65 / 228  Training Loss  0.00446635577827692\n","Epoch  7 Batch  66 / 228  Training Loss  0.0009678573114797473\n","Epoch  7 Batch  67 / 228  Training Loss  0.004222453571856022\n","Epoch  7 Batch  68 / 228  Training Loss  0.0029245621990412474\n","Epoch  7 Batch  69 / 228  Training Loss  0.009144272655248642\n","Epoch  7 Batch  70 / 228  Training Loss  0.0010436475276947021\n","Epoch  7 Batch  71 / 228  Training Loss  0.000565329915843904\n","Epoch  7 Batch  72 / 228  Training Loss  0.0012760787503793836\n","Epoch  7 Batch  73 / 228  Training Loss  0.001387972617521882\n","Epoch  7 Batch  74 / 228  Training Loss  0.009568925015628338\n","Epoch  7 Batch  75 / 228  Training Loss  0.000624983455054462\n","Epoch  7 Batch  76 / 228  Training Loss  0.0006822408176958561\n","Epoch  7 Batch  77 / 228  Training Loss  0.009436286054551601\n","Epoch  7 Batch  78 / 228  Training Loss  0.0006424994207918644\n","Epoch  7 Batch  79 / 228  Training Loss  0.0013665729202330112\n","Epoch  7 Batch  80 / 228  Training Loss  0.0008694657008163631\n","Epoch  7 Batch  81 / 228  Training Loss  0.0011038907105103135\n","Epoch  7 Batch  82 / 228  Training Loss  0.0024614413268864155\n","Epoch  7 Batch  83 / 228  Training Loss  0.0012047654017806053\n","Epoch  7 Batch  84 / 228  Training Loss  0.0018241498619318008\n","Epoch  7 Batch  85 / 228  Training Loss  0.0010433226125314832\n","Epoch  7 Batch  86 / 228  Training Loss  0.000834139995276928\n","Epoch  7 Batch  87 / 228  Training Loss  0.0026406485121697187\n","Epoch  7 Batch  88 / 228  Training Loss  0.0005817884812131524\n","Epoch  7 Batch  89 / 228  Training Loss  0.0004643422143999487\n","Epoch  7 Batch  90 / 228  Training Loss  0.0011607626220211387\n","Epoch  7 Batch  91 / 228  Training Loss  0.000836046994663775\n","Epoch  7 Batch  92 / 228  Training Loss  0.00028879588353447616\n","Epoch  7 Batch  93 / 228  Training Loss  0.0006019818247295916\n","Epoch  7 Batch  94 / 228  Training Loss  0.0006117061711847782\n","Epoch  7 Batch  95 / 228  Training Loss  0.00048323936061933637\n","Epoch  7 Batch  96 / 228  Training Loss  0.0005631805397570133\n","Epoch  7 Batch  97 / 228  Training Loss  0.0012565685901790857\n","Epoch  7 Batch  98 / 228  Training Loss  0.0004263729206286371\n","Epoch  7 Batch  99 / 228  Training Loss  0.00034815905382856727\n","Epoch  7 Batch  100 / 228  Training Loss  0.0004444126388989389\n","Epoch  7 Batch  101 / 228  Training Loss  0.0007519297651015222\n","Epoch  7 Batch  102 / 228  Training Loss  0.00035954179475083947\n","Epoch  7 Batch  103 / 228  Training Loss  0.0005375890177674592\n","Epoch  7 Batch  104 / 228  Training Loss  0.0004602207336574793\n","Epoch  7 Batch  105 / 228  Training Loss  0.00033021773560903966\n","Epoch  7 Batch  106 / 228  Training Loss  0.0011757892789319158\n","Epoch  7 Batch  107 / 228  Training Loss  0.0004061427025590092\n","Epoch  7 Batch  108 / 228  Training Loss  0.00046761552221141756\n","Epoch  7 Batch  109 / 228  Training Loss  0.0004865853115916252\n","Epoch  7 Batch  110 / 228  Training Loss  0.00028905062936246395\n","Epoch  7 Batch  111 / 228  Training Loss  0.00024156176368705928\n","Epoch  7 Batch  112 / 228  Training Loss  0.0007139738881960511\n","Epoch  7 Batch  113 / 228  Training Loss  0.00043407687917351723\n","Epoch  7 Batch  114 / 228  Training Loss  0.0005627588252536952\n","Epoch  7 Batch  115 / 228  Training Loss  0.0006648582639172673\n","Epoch  7 Batch  116 / 228  Training Loss  0.00026448426069691777\n","Epoch  7 Batch  117 / 228  Training Loss  0.0004456659371498972\n","Epoch  7 Batch  118 / 228  Training Loss  0.0004660326521843672\n","Epoch  7 Batch  119 / 228  Training Loss  0.0004950798465870321\n","Epoch  7 Batch  120 / 228  Training Loss  0.00044637249084189534\n","Epoch  7 Batch  121 / 228  Training Loss  0.0002680717152543366\n","Epoch  7 Batch  122 / 228  Training Loss  0.0009307897416874766\n","Epoch  7 Batch  123 / 228  Training Loss  0.00023818392946850508\n","Epoch  7 Batch  124 / 228  Training Loss  0.00033262267243117094\n","Epoch  7 Batch  125 / 228  Training Loss  0.00043876731069758534\n","Epoch  7 Batch  126 / 228  Training Loss  0.00027268187841400504\n","Epoch  7 Batch  127 / 228  Training Loss  0.00041589472675696015\n","Epoch  7 Batch  128 / 228  Training Loss  0.0005549467168748379\n","Epoch  7 Batch  129 / 228  Training Loss  0.0006995760486461222\n","Epoch  7 Batch  130 / 228  Training Loss  0.00036086179898120463\n","Epoch  7 Batch  131 / 228  Training Loss  0.0003442081797402352\n","Epoch  7 Batch  132 / 228  Training Loss  0.0003059337323065847\n","Epoch  7 Batch  133 / 228  Training Loss  0.0003082571201957762\n","Epoch  7 Batch  134 / 228  Training Loss  0.00019926580716855824\n","Epoch  7 Batch  135 / 228  Training Loss  0.00044626794988289475\n","Epoch  7 Batch  136 / 228  Training Loss  0.0003846725740004331\n","Epoch  7 Batch  137 / 228  Training Loss  0.00033696292666718364\n","Epoch  7 Batch  138 / 228  Training Loss  0.00042168452637270093\n","Epoch  7 Batch  139 / 228  Training Loss  0.000582626205869019\n","Epoch  7 Batch  140 / 228  Training Loss  0.0002631272654980421\n","Epoch  7 Batch  141 / 228  Training Loss  0.0003619830822572112\n","Epoch  7 Batch  142 / 228  Training Loss  0.0005346393445506692\n","Epoch  7 Batch  143 / 228  Training Loss  0.00032214209204539657\n","Epoch  7 Batch  144 / 228  Training Loss  0.00024255923926830292\n","Epoch  7 Batch  145 / 228  Training Loss  0.0002848064759746194\n","Epoch  7 Batch  146 / 228  Training Loss  0.00018082224414683878\n","Epoch  7 Batch  147 / 228  Training Loss  0.0007859330507926643\n","Epoch  7 Batch  148 / 228  Training Loss  0.0003639240167103708\n","Epoch  7 Batch  149 / 228  Training Loss  0.00020688418589998037\n","Epoch  7 Batch  150 / 228  Training Loss  0.000296558573609218\n","Epoch  7 Batch  151 / 228  Training Loss  0.0002554470847826451\n","Epoch  7 Batch  152 / 228  Training Loss  0.00020971614867448807\n","Epoch  7 Batch  153 / 228  Training Loss  0.0002815525804180652\n","Epoch  7 Batch  154 / 228  Training Loss  0.000313100143102929\n","Epoch  7 Batch  155 / 228  Training Loss  0.0002057397796306759\n","Epoch  7 Batch  156 / 228  Training Loss  0.0004821338225156069\n","Epoch  7 Batch  157 / 228  Training Loss  0.0002734311274252832\n","Epoch  7 Batch  158 / 228  Training Loss  0.00020195372053422034\n","Epoch  7 Batch  159 / 228  Training Loss  0.0003262110985815525\n","Epoch  7 Batch  160 / 228  Training Loss  0.00025572095182724297\n","Epoch  7 Batch  161 / 228  Training Loss  0.0002986126346513629\n","Epoch  7 Batch  162 / 228  Training Loss  0.0004543523828033358\n","Epoch  7 Batch  163 / 228  Training Loss  0.00018594469293020666\n","Epoch  7 Batch  164 / 228  Training Loss  0.0004208456666674465\n","Epoch  7 Batch  165 / 228  Training Loss  0.0003752821939997375\n","Epoch  7 Batch  166 / 228  Training Loss  0.00020448435680009425\n","Epoch  7 Batch  167 / 228  Training Loss  0.00027154971030540764\n","Epoch  7 Batch  168 / 228  Training Loss  0.0003461872402112931\n","Epoch  7 Batch  169 / 228  Training Loss  0.00020524626597762108\n","Epoch  7 Batch  170 / 228  Training Loss  0.00021657678007613868\n","Epoch  7 Batch  171 / 228  Training Loss  0.00040167089900933206\n","Epoch  7 Batch  172 / 228  Training Loss  0.00023165802122093737\n","Epoch  7 Batch  173 / 228  Training Loss  0.0002861678949557245\n","Epoch  7 Batch  174 / 228  Training Loss  0.00017911822942551225\n","Epoch  7 Batch  175 / 228  Training Loss  0.0002354384632781148\n","Epoch  7 Batch  176 / 228  Training Loss  0.00036479750997386873\n","Epoch  7 Batch  177 / 228  Training Loss  0.000214115105336532\n","Epoch  7 Batch  178 / 228  Training Loss  0.00023682069149799645\n","Epoch  7 Batch  179 / 228  Training Loss  0.00030785638955421746\n","Epoch  7 Batch  180 / 228  Training Loss  0.001218630000948906\n","Epoch  7 Batch  181 / 228  Training Loss  0.00018262714729644358\n","Epoch  7 Batch  182 / 228  Training Loss  0.0005237237201072276\n","Epoch  7 Batch  183 / 228  Training Loss  0.00017261781613342464\n","Epoch  7 Batch  184 / 228  Training Loss  0.000259905937127769\n","Epoch  7 Batch  185 / 228  Training Loss  0.00024107059289235622\n","Epoch  7 Batch  186 / 228  Training Loss  0.00025793726672418416\n","Epoch  7 Batch  187 / 228  Training Loss  0.00022933438594918698\n","Epoch  7 Batch  188 / 228  Training Loss  0.00023413651797454804\n","Epoch  7 Batch  189 / 228  Training Loss  0.0002527042233850807\n","Epoch  7 Batch  190 / 228  Training Loss  0.00020842952653765678\n","Epoch  7 Batch  191 / 228  Training Loss  0.000279078318271786\n","Epoch  7 Batch  192 / 228  Training Loss  0.00026748390519060194\n","Epoch  7 Batch  193 / 228  Training Loss  0.00027608638629317284\n","Epoch  7 Batch  194 / 228  Training Loss  0.00019222477567382157\n","Epoch  7 Batch  195 / 228  Training Loss  0.00017245413619093597\n","Epoch  7 Batch  196 / 228  Training Loss  0.00022438369342125952\n","Epoch  7 Batch  197 / 228  Training Loss  0.0003071413957513869\n","Epoch  7 Batch  198 / 228  Training Loss  0.000218726068851538\n","Epoch  7 Batch  199 / 228  Training Loss  0.00021593044220935553\n","Epoch  7 Batch  200 / 228  Training Loss  0.00016665973816998303\n","Epoch  7 Batch  201 / 228  Training Loss  0.00016668534954078496\n","Epoch  7 Batch  202 / 228  Training Loss  0.0001215623487951234\n","Epoch  7 Batch  203 / 228  Training Loss  0.001274051028303802\n","Epoch  7 Batch  204 / 228  Training Loss  0.00015048126806505024\n","Epoch  7 Batch  205 / 228  Training Loss  0.00023552204947918653\n","Epoch  7 Batch  206 / 228  Training Loss  0.0002789372520055622\n","Epoch  7 Batch  207 / 228  Training Loss  0.0001863873185357079\n","Epoch  7 Batch  208 / 228  Training Loss  0.00016203407722059637\n","Epoch  7 Batch  209 / 228  Training Loss  0.00029379178886301816\n","Epoch  7 Batch  210 / 228  Training Loss  0.00018354732310399413\n","Epoch  7 Batch  211 / 228  Training Loss  0.00021296352497301996\n","Epoch  7 Batch  212 / 228  Training Loss  0.001312754349783063\n","Epoch  7 Batch  213 / 228  Training Loss  0.0005480401450768113\n","Epoch  7 Batch  214 / 228  Training Loss  0.00024130757083185017\n","Epoch  7 Batch  215 / 228  Training Loss  0.00024134290288202465\n","Epoch  7 Batch  216 / 228  Training Loss  0.00016449036775156856\n","Epoch  7 Batch  217 / 228  Training Loss  0.0002545673050917685\n","Epoch  7 Batch  218 / 228  Training Loss  0.00022333700326271355\n","Epoch  7 Batch  219 / 228  Training Loss  0.0002526066964492202\n","Epoch  7 Batch  220 / 228  Training Loss  0.00045852939365431666\n","Epoch  7 Batch  221 / 228  Training Loss  0.00019775886903516948\n","Epoch  7 Batch  222 / 228  Training Loss  0.000471464212751016\n","Epoch  7 Batch  223 / 228  Training Loss  0.00021701701916754246\n","Epoch  7 Batch  224 / 228  Training Loss  0.00013775937259197235\n","Epoch  7 Batch  225 / 228  Training Loss  0.00013695853704120964\n","Epoch  7 Batch  226 / 228  Training Loss  0.00018414539226796478\n","Epoch  7 Batch  227 / 228  Training Loss  0.0001458702317904681\n","   8    |    -    |   0.001145   | 98.971037\n","----------------------------------------------------------------------\n","Running epoch: 8\n","Epoch  8 Batch  0 / 228  Training Loss  0.0002055832592304796\n","Epoch  8 Batch  1 / 228  Training Loss  0.00028046543593518436\n","Epoch  8 Batch  2 / 228  Training Loss  0.0001922569499583915\n","Epoch  8 Batch  3 / 228  Training Loss  0.000216143365832977\n","Epoch  8 Batch  4 / 228  Training Loss  0.0001658699766267091\n","Epoch  8 Batch  5 / 228  Training Loss  0.00015956576680764556\n","Epoch  8 Batch  6 / 228  Training Loss  0.00016857820446603\n","Epoch  8 Batch  7 / 228  Training Loss  0.00014659130829386413\n","Epoch  8 Batch  8 / 228  Training Loss  0.00020853275782428682\n","Epoch  8 Batch  9 / 228  Training Loss  0.00017068532179109752\n","Epoch  8 Batch  10 / 228  Training Loss  0.0001827492524171248\n","Epoch  8 Batch  11 / 228  Training Loss  0.00013273017248138785\n","Epoch  8 Batch  12 / 228  Training Loss  0.0002808016142807901\n","Epoch  8 Batch  13 / 228  Training Loss  0.000153209432028234\n","Epoch  8 Batch  14 / 228  Training Loss  0.0001873494329629466\n","Epoch  8 Batch  15 / 228  Training Loss  0.00034078076714649796\n","Epoch  8 Batch  16 / 228  Training Loss  0.00017520089750178158\n","Epoch  8 Batch  17 / 228  Training Loss  0.0001783228653948754\n","Epoch  8 Batch  18 / 228  Training Loss  0.00018327427096664906\n","Epoch  8 Batch  19 / 228  Training Loss  0.00018947783973999321\n","Epoch  8 Batch  20 / 228  Training Loss  0.00012525868078228086\n","Epoch  8 Batch  21 / 228  Training Loss  0.00017352325085084885\n","Epoch  8 Batch  22 / 228  Training Loss  0.0001626834855414927\n","Epoch  8 Batch  23 / 228  Training Loss  0.00014851571177132428\n","Epoch  8 Batch  24 / 228  Training Loss  0.00013491848949342966\n","Epoch  8 Batch  25 / 228  Training Loss  0.00014496338553726673\n","Epoch  8 Batch  26 / 228  Training Loss  0.0004203528515063226\n","Epoch  8 Batch  27 / 228  Training Loss  9.541535837342963e-05\n","Epoch  8 Batch  28 / 228  Training Loss  0.00022975672618485987\n","Epoch  8 Batch  29 / 228  Training Loss  0.0001432869175914675\n","Epoch  8 Batch  30 / 228  Training Loss  0.00019349389185663313\n","Epoch  8 Batch  31 / 228  Training Loss  0.00012708567373920232\n","Epoch  8 Batch  32 / 228  Training Loss  0.00013603591651190072\n","Epoch  8 Batch  33 / 228  Training Loss  0.0001469165727030486\n","Epoch  8 Batch  34 / 228  Training Loss  0.00014530566113535315\n","Epoch  8 Batch  35 / 228  Training Loss  0.0001112636091420427\n","Epoch  8 Batch  36 / 228  Training Loss  0.00017222265887539834\n","Epoch  8 Batch  37 / 228  Training Loss  0.00017260780441574752\n","Epoch  8 Batch  38 / 228  Training Loss  0.00013232091441750526\n","Epoch  8 Batch  39 / 228  Training Loss  0.00015998425078578293\n","Epoch  8 Batch  40 / 228  Training Loss  0.00012115064600948244\n","Epoch  8 Batch  41 / 228  Training Loss  0.00013337349810171872\n","Epoch  8 Batch  42 / 228  Training Loss  0.00018260990327689797\n","Epoch  8 Batch  43 / 228  Training Loss  0.00015555098070763052\n","Epoch  8 Batch  44 / 228  Training Loss  0.0001309240033151582\n","Epoch  8 Batch  45 / 228  Training Loss  0.00017198028217535466\n","Epoch  8 Batch  46 / 228  Training Loss  0.00017119469703175128\n","Epoch  8 Batch  47 / 228  Training Loss  0.00017410825239494443\n","Epoch  8 Batch  48 / 228  Training Loss  0.00014656668645329773\n","Epoch  8 Batch  49 / 228  Training Loss  0.000142427918035537\n","Epoch  8 Batch  50 / 228  Training Loss  0.000133745968923904\n","Epoch  8 Batch  51 / 228  Training Loss  0.0002269252436235547\n","Epoch  8 Batch  52 / 228  Training Loss  0.00012965223868377507\n","Epoch  8 Batch  53 / 228  Training Loss  0.0001371251419186592\n","Epoch  8 Batch  54 / 228  Training Loss  0.00010767883213702589\n","Epoch  8 Batch  55 / 228  Training Loss  0.0002412078611087054\n","Epoch  8 Batch  56 / 228  Training Loss  0.00015948238433338702\n","Epoch  8 Batch  57 / 228  Training Loss  0.00016330470680259168\n","Epoch  8 Batch  58 / 228  Training Loss  9.793996287044138e-05\n","Epoch  8 Batch  59 / 228  Training Loss  0.00018756452482193708\n","Epoch  8 Batch  60 / 228  Training Loss  0.00014448282308876514\n","Epoch  8 Batch  61 / 228  Training Loss  0.00014638180437032133\n","Epoch  8 Batch  62 / 228  Training Loss  0.00014431276940740645\n","Epoch  8 Batch  63 / 228  Training Loss  0.0001474115124437958\n","Epoch  8 Batch  64 / 228  Training Loss  0.00010880732588702813\n","Epoch  8 Batch  65 / 228  Training Loss  0.00010874731378862634\n","Epoch  8 Batch  66 / 228  Training Loss  0.00012036510452162474\n","Epoch  8 Batch  67 / 228  Training Loss  8.549171616323292e-05\n","Epoch  8 Batch  68 / 228  Training Loss  0.00019655369396787137\n","Epoch  8 Batch  69 / 228  Training Loss  0.00013093030429445207\n","Epoch  8 Batch  70 / 228  Training Loss  0.00020285192294977605\n","Epoch  8 Batch  71 / 228  Training Loss  0.0001827123633120209\n","Epoch  8 Batch  72 / 228  Training Loss  0.00010614241182338446\n","Epoch  8 Batch  73 / 228  Training Loss  0.0001227819302584976\n","Epoch  8 Batch  74 / 228  Training Loss  0.00021334066695999354\n","Epoch  8 Batch  75 / 228  Training Loss  0.00016526127001270652\n","Epoch  8 Batch  76 / 228  Training Loss  0.00022304760932456702\n","Epoch  8 Batch  77 / 228  Training Loss  0.00014409986033570021\n","Epoch  8 Batch  78 / 228  Training Loss  0.00019336803234182298\n","Epoch  8 Batch  79 / 228  Training Loss  9.72254274529405e-05\n","Epoch  8 Batch  80 / 228  Training Loss  0.00011566276953089982\n","Epoch  8 Batch  81 / 228  Training Loss  0.0001609626633580774\n","Epoch  8 Batch  82 / 228  Training Loss  9.163555660052225e-05\n","Epoch  8 Batch  83 / 228  Training Loss  0.00022389073274098337\n","Epoch  8 Batch  84 / 228  Training Loss  0.00012770944158546627\n","Epoch  8 Batch  85 / 228  Training Loss  0.0001359027373837307\n","Epoch  8 Batch  86 / 228  Training Loss  0.0001852943969424814\n","Epoch  8 Batch  87 / 228  Training Loss  8.280707697849721e-05\n","Epoch  8 Batch  88 / 228  Training Loss  0.00018190960690844804\n","Epoch  8 Batch  89 / 228  Training Loss  0.00015586489462293684\n","Epoch  8 Batch  90 / 228  Training Loss  0.00015388382598757744\n","Epoch  8 Batch  91 / 228  Training Loss  0.00016501823847647756\n","Epoch  8 Batch  92 / 228  Training Loss  0.00011924355203518644\n","Epoch  8 Batch  93 / 228  Training Loss  0.00011267579247942194\n","Epoch  8 Batch  94 / 228  Training Loss  0.00011047639418393373\n","Epoch  8 Batch  95 / 228  Training Loss  0.000118918193038553\n","Epoch  8 Batch  96 / 228  Training Loss  0.00012605202209670097\n","Epoch  8 Batch  97 / 228  Training Loss  0.00011454745254013687\n","Epoch  8 Batch  98 / 228  Training Loss  0.0001137775689130649\n","Epoch  8 Batch  99 / 228  Training Loss  0.00011874954361701384\n","Epoch  8 Batch  100 / 228  Training Loss  0.00018435859237797558\n","Epoch  8 Batch  101 / 228  Training Loss  0.00016618573863524944\n","Epoch  8 Batch  102 / 228  Training Loss  0.00013866499648429453\n","Epoch  8 Batch  103 / 228  Training Loss  0.00016145556583069265\n","Epoch  8 Batch  104 / 228  Training Loss  0.0001686194009380415\n","Epoch  8 Batch  105 / 228  Training Loss  8.885357237886637e-05\n","Epoch  8 Batch  106 / 228  Training Loss  0.00013112183660268784\n","Epoch  8 Batch  107 / 228  Training Loss  0.00012680071813520044\n","Epoch  8 Batch  108 / 228  Training Loss  0.00010776879207696766\n","Epoch  8 Batch  109 / 228  Training Loss  0.00014957235543988645\n","Epoch  8 Batch  110 / 228  Training Loss  0.0017743181670084596\n","Epoch  8 Batch  111 / 228  Training Loss  0.00020354066509753466\n","Epoch  8 Batch  112 / 228  Training Loss  9.7781463409774e-05\n","Epoch  8 Batch  113 / 228  Training Loss  0.00026014624745585024\n","Epoch  8 Batch  114 / 228  Training Loss  0.00016847443475853652\n","Epoch  8 Batch  115 / 228  Training Loss  0.0054381610825657845\n","Epoch  8 Batch  116 / 228  Training Loss  0.00014257998554967344\n","Epoch  8 Batch  117 / 228  Training Loss  0.0010693036019802094\n","Epoch  8 Batch  118 / 228  Training Loss  0.000120579767099116\n","Epoch  8 Batch  119 / 228  Training Loss  0.0002550181816332042\n","Epoch  8 Batch  120 / 228  Training Loss  0.0001395534200128168\n","Epoch  8 Batch  121 / 228  Training Loss  0.00013514237070921808\n","Epoch  8 Batch  122 / 228  Training Loss  0.00021823476708959788\n","Epoch  8 Batch  123 / 228  Training Loss  0.00020499150559771806\n","Epoch  8 Batch  124 / 228  Training Loss  0.00010712396760936826\n","Epoch  8 Batch  125 / 228  Training Loss  0.00017360706988256425\n","Epoch  8 Batch  126 / 228  Training Loss  0.00012673331366386265\n","Epoch  8 Batch  127 / 228  Training Loss  0.00018401406123302877\n","Epoch  8 Batch  128 / 228  Training Loss  0.0001434736914234236\n","Epoch  8 Batch  129 / 228  Training Loss  0.00026393315056338906\n","Epoch  8 Batch  130 / 228  Training Loss  0.00013529839634429663\n","Epoch  8 Batch  131 / 228  Training Loss  0.00021986728825140744\n","Epoch  8 Batch  132 / 228  Training Loss  0.0001433326833648607\n","Epoch  8 Batch  133 / 228  Training Loss  0.00018927430210169405\n","Epoch  8 Batch  134 / 228  Training Loss  0.0002395108313066885\n","Epoch  8 Batch  135 / 228  Training Loss  0.00017911785107571632\n","Epoch  8 Batch  136 / 228  Training Loss  0.005575095769017935\n","Epoch  8 Batch  137 / 228  Training Loss  0.00014680692402180284\n","Epoch  8 Batch  138 / 228  Training Loss  0.0003231828159186989\n","Epoch  8 Batch  139 / 228  Training Loss  0.00016034391592256725\n","Epoch  8 Batch  140 / 228  Training Loss  0.00014346114767249674\n","Epoch  8 Batch  141 / 228  Training Loss  0.0002358321798965335\n","Epoch  8 Batch  142 / 228  Training Loss  0.00030876684468239546\n","Epoch  8 Batch  143 / 228  Training Loss  0.00018653928418643773\n","Epoch  8 Batch  144 / 228  Training Loss  0.00010321319859940559\n","Epoch  8 Batch  145 / 228  Training Loss  0.00013216618390288204\n","Epoch  8 Batch  146 / 228  Training Loss  0.00013978326751384884\n","Epoch  8 Batch  147 / 228  Training Loss  0.00016043172217905521\n","Epoch  8 Batch  148 / 228  Training Loss  0.0001598842500243336\n","Epoch  8 Batch  149 / 228  Training Loss  0.00012733796029351652\n","Epoch  8 Batch  150 / 228  Training Loss  0.00010313693201169372\n","Epoch  8 Batch  151 / 228  Training Loss  0.0001525123225292191\n","Epoch  8 Batch  152 / 228  Training Loss  0.00023600032727699727\n","Epoch  8 Batch  153 / 228  Training Loss  0.00013271586794871837\n","Epoch  8 Batch  154 / 228  Training Loss  0.0020039058290421963\n","Epoch  8 Batch  155 / 228  Training Loss  0.00015326097491197288\n","Epoch  8 Batch  156 / 228  Training Loss  0.0006508667138405144\n","Epoch  8 Batch  157 / 228  Training Loss  0.00017694065172690898\n","Epoch  8 Batch  158 / 228  Training Loss  0.0002239160967292264\n","Epoch  8 Batch  159 / 228  Training Loss  0.0006404899177141488\n","Epoch  8 Batch  160 / 228  Training Loss  0.00012850662460550666\n","Epoch  8 Batch  161 / 228  Training Loss  0.00011460542737040669\n","Epoch  8 Batch  162 / 228  Training Loss  9.728075383463874e-05\n","Epoch  8 Batch  163 / 228  Training Loss  0.0001685104361968115\n","Epoch  8 Batch  164 / 228  Training Loss  0.0001499911304563284\n","Epoch  8 Batch  165 / 228  Training Loss  0.00020250510715413839\n","Epoch  8 Batch  166 / 228  Training Loss  0.0001292725355597213\n","Epoch  8 Batch  167 / 228  Training Loss  0.0009106885408982635\n","Epoch  8 Batch  168 / 228  Training Loss  0.00016761766164563596\n","Epoch  8 Batch  169 / 228  Training Loss  0.00012390571646392345\n","Epoch  8 Batch  170 / 228  Training Loss  9.568087989464402e-05\n","Epoch  8 Batch  171 / 228  Training Loss  0.0001391551923006773\n","Epoch  8 Batch  172 / 228  Training Loss  0.000252267811447382\n","Epoch  8 Batch  173 / 228  Training Loss  0.00018540308519732207\n","Epoch  8 Batch  174 / 228  Training Loss  0.0002192609681515023\n","Epoch  8 Batch  175 / 228  Training Loss  0.000132339249830693\n","Epoch  8 Batch  176 / 228  Training Loss  0.0001153134144260548\n","Epoch  8 Batch  177 / 228  Training Loss  0.000760922092013061\n","Epoch  8 Batch  178 / 228  Training Loss  0.00021744539844803512\n","Epoch  8 Batch  179 / 228  Training Loss  0.00023407954722642899\n","Epoch  8 Batch  180 / 228  Training Loss  0.0001475053431931883\n","Epoch  8 Batch  181 / 228  Training Loss  0.00037721270928159356\n","Epoch  8 Batch  182 / 228  Training Loss  0.0001294622488785535\n","Epoch  8 Batch  183 / 228  Training Loss  8.017323852982372e-05\n","Epoch  8 Batch  184 / 228  Training Loss  0.00011520334373926744\n","Epoch  8 Batch  185 / 228  Training Loss  0.00020675019186455756\n","Epoch  8 Batch  186 / 228  Training Loss  9.966643847292289e-05\n","Epoch  8 Batch  187 / 228  Training Loss  0.0001920779177453369\n","Epoch  8 Batch  188 / 228  Training Loss  9.533009142614901e-05\n","Epoch  8 Batch  189 / 228  Training Loss  0.00018051118240691721\n","Epoch  8 Batch  190 / 228  Training Loss  0.00013896888413000852\n","Epoch  8 Batch  191 / 228  Training Loss  0.0001682591682765633\n","Epoch  8 Batch  192 / 228  Training Loss  8.439578959951177e-05\n","Epoch  8 Batch  193 / 228  Training Loss  9.278520883526653e-05\n","Epoch  8 Batch  194 / 228  Training Loss  0.00014257848670240492\n","Epoch  8 Batch  195 / 228  Training Loss  0.00031398978899233043\n","Epoch  8 Batch  196 / 228  Training Loss  0.00010741986625362188\n","Epoch  8 Batch  197 / 228  Training Loss  0.0002044301654677838\n","Epoch  8 Batch  198 / 228  Training Loss  0.00012405301094986498\n","Epoch  8 Batch  199 / 228  Training Loss  0.00035016544279642403\n","Epoch  8 Batch  200 / 228  Training Loss  0.00016317781410180032\n","Epoch  8 Batch  201 / 228  Training Loss  0.000112659101432655\n","Epoch  8 Batch  202 / 228  Training Loss  0.00015679284115321934\n","Epoch  8 Batch  203 / 228  Training Loss  0.00013359516742639244\n","Epoch  8 Batch  204 / 228  Training Loss  0.00011234913108637556\n","Epoch  8 Batch  205 / 228  Training Loss  0.00011408245336497203\n","Epoch  8 Batch  206 / 228  Training Loss  0.0001155603677034378\n","Epoch  8 Batch  207 / 228  Training Loss  0.00011149923375342041\n","Epoch  8 Batch  208 / 228  Training Loss  0.00011892836482729763\n","Epoch  8 Batch  209 / 228  Training Loss  0.00010479253251105547\n","Epoch  8 Batch  210 / 228  Training Loss  0.00011894627095898613\n","Epoch  8 Batch  211 / 228  Training Loss  0.00014029697922524065\n","Epoch  8 Batch  212 / 228  Training Loss  0.00015280788647942245\n","Epoch  8 Batch  213 / 228  Training Loss  0.00014336119056679308\n","Epoch  8 Batch  214 / 228  Training Loss  0.0002357137855142355\n","Epoch  8 Batch  215 / 228  Training Loss  0.00010422695049783215\n","Epoch  8 Batch  216 / 228  Training Loss  0.00011076192458858714\n","Epoch  8 Batch  217 / 228  Training Loss  0.00030477845575660467\n","Epoch  8 Batch  218 / 228  Training Loss  0.00025363118038512766\n","Epoch  8 Batch  219 / 228  Training Loss  0.00014350484707392752\n","Epoch  8 Batch  220 / 228  Training Loss  0.00011517488019308075\n","Epoch  8 Batch  221 / 228  Training Loss  0.00012314094055909663\n","Epoch  8 Batch  222 / 228  Training Loss  0.0005095725646242499\n","Epoch  8 Batch  223 / 228  Training Loss  0.00011957988317590207\n","Epoch  8 Batch  224 / 228  Training Loss  0.00015543345944024622\n","Epoch  8 Batch  225 / 228  Training Loss  0.00010276609100401402\n","Epoch  8 Batch  226 / 228  Training Loss  9.792563650989905e-05\n","Epoch  8 Batch  227 / 228  Training Loss  8.647214417578653e-05\n","   9    |    -    |   0.000238   | 99.352134\n","----------------------------------------------------------------------\n","Running epoch: 9\n","Epoch  9 Batch  0 / 228  Training Loss  0.00010722559818532318\n","Epoch  9 Batch  1 / 228  Training Loss  9.514512203168124e-05\n","Epoch  9 Batch  2 / 228  Training Loss  0.00012124459317419678\n","Epoch  9 Batch  3 / 228  Training Loss  6.512749678222463e-05\n","Epoch  9 Batch  4 / 228  Training Loss  6.598312029382214e-05\n","Epoch  9 Batch  5 / 228  Training Loss  7.594282942591235e-05\n","Epoch  9 Batch  6 / 228  Training Loss  0.0001261680299649015\n","Epoch  9 Batch  7 / 228  Training Loss  0.00028645561542361975\n","Epoch  9 Batch  8 / 228  Training Loss  0.0001397774030920118\n","Epoch  9 Batch  9 / 228  Training Loss  0.00010322347225155681\n","Epoch  9 Batch  10 / 228  Training Loss  0.00018486838962417096\n","Epoch  9 Batch  11 / 228  Training Loss  0.00011030266614397988\n","Epoch  9 Batch  12 / 228  Training Loss  8.576322579756379e-05\n","Epoch  9 Batch  13 / 228  Training Loss  0.0001348849036730826\n","Epoch  9 Batch  14 / 228  Training Loss  0.00010005824879044667\n","Epoch  9 Batch  15 / 228  Training Loss  0.00010815085988724604\n","Epoch  9 Batch  16 / 228  Training Loss  0.00010433587158331648\n","Epoch  9 Batch  17 / 228  Training Loss  9.47600492509082e-05\n","Epoch  9 Batch  18 / 228  Training Loss  0.00012292774044908583\n","Epoch  9 Batch  19 / 228  Training Loss  8.882646943675354e-05\n","Epoch  9 Batch  20 / 228  Training Loss  0.00015827250899747014\n","Epoch  9 Batch  21 / 228  Training Loss  0.00013677470269612968\n","Epoch  9 Batch  22 / 228  Training Loss  0.00011962767166551203\n","Epoch  9 Batch  23 / 228  Training Loss  8.623844769317657e-05\n","Epoch  9 Batch  24 / 228  Training Loss  8.766484825173393e-05\n","Epoch  9 Batch  25 / 228  Training Loss  9.126153599936515e-05\n","Epoch  9 Batch  26 / 228  Training Loss  0.00014228255895432085\n","Epoch  9 Batch  27 / 228  Training Loss  0.0001050309874699451\n","Epoch  9 Batch  28 / 228  Training Loss  0.00012601817434187979\n","Epoch  9 Batch  29 / 228  Training Loss  0.0001450490963179618\n","Epoch  9 Batch  30 / 228  Training Loss  8.061464177444577e-05\n","Epoch  9 Batch  31 / 228  Training Loss  0.0001936816843226552\n","Epoch  9 Batch  32 / 228  Training Loss  0.00010030266275862232\n","Epoch  9 Batch  33 / 228  Training Loss  0.00013317939010448754\n","Epoch  9 Batch  34 / 228  Training Loss  8.923267887439579e-05\n","Epoch  9 Batch  35 / 228  Training Loss  9.742964175529778e-05\n","Epoch  9 Batch  36 / 228  Training Loss  0.00013345589104574174\n","Epoch  9 Batch  37 / 228  Training Loss  0.00017626398766878992\n","Epoch  9 Batch  38 / 228  Training Loss  0.00011739144974853843\n","Epoch  9 Batch  39 / 228  Training Loss  8.990571222966537e-05\n","Epoch  9 Batch  40 / 228  Training Loss  8.177114796126261e-05\n","Epoch  9 Batch  41 / 228  Training Loss  8.768882253207266e-05\n","Epoch  9 Batch  42 / 228  Training Loss  9.622433572076261e-05\n","Epoch  9 Batch  43 / 228  Training Loss  9.648049308452755e-05\n","Epoch  9 Batch  44 / 228  Training Loss  0.000107117673906032\n","Epoch  9 Batch  45 / 228  Training Loss  9.619684715289623e-05\n","Epoch  9 Batch  46 / 228  Training Loss  0.00010231139458483085\n","Epoch  9 Batch  47 / 228  Training Loss  0.00010424623906146735\n","Epoch  9 Batch  48 / 228  Training Loss  8.211795648094267e-05\n","Epoch  9 Batch  49 / 228  Training Loss  7.311198714887723e-05\n","Epoch  9 Batch  50 / 228  Training Loss  9.423313895240426e-05\n","Epoch  9 Batch  51 / 228  Training Loss  0.00013418213347904384\n","Epoch  9 Batch  52 / 228  Training Loss  8.047682058531791e-05\n","Epoch  9 Batch  53 / 228  Training Loss  0.00010669200855772942\n","Epoch  9 Batch  54 / 228  Training Loss  9.279484220314771e-05\n","Epoch  9 Batch  55 / 228  Training Loss  0.00011518954852363095\n","Epoch  9 Batch  56 / 228  Training Loss  0.00010208665480604395\n","Epoch  9 Batch  57 / 228  Training Loss  9.863985178526491e-05\n","Epoch  9 Batch  58 / 228  Training Loss  9.945931378751993e-05\n","Epoch  9 Batch  59 / 228  Training Loss  0.0001279676944250241\n","Epoch  9 Batch  60 / 228  Training Loss  9.528231748845428e-05\n","Epoch  9 Batch  61 / 228  Training Loss  9.566267544869334e-05\n","Epoch  9 Batch  62 / 228  Training Loss  8.555174281354994e-05\n","Epoch  9 Batch  63 / 228  Training Loss  0.00010642032430041581\n","Epoch  9 Batch  64 / 228  Training Loss  5.5318807426374406e-05\n","Epoch  9 Batch  65 / 228  Training Loss  7.783331966493279e-05\n","Epoch  9 Batch  66 / 228  Training Loss  8.222279575420544e-05\n","Epoch  9 Batch  67 / 228  Training Loss  8.79173239809461e-05\n","Epoch  9 Batch  68 / 228  Training Loss  6.732119072694331e-05\n","Epoch  9 Batch  69 / 228  Training Loss  0.00010335589468013495\n","Epoch  9 Batch  70 / 228  Training Loss  7.861165067879483e-05\n","Epoch  9 Batch  71 / 228  Training Loss  0.00011213944526389241\n","Epoch  9 Batch  72 / 228  Training Loss  7.784140325384215e-05\n","Epoch  9 Batch  73 / 228  Training Loss  9.363009303342551e-05\n","Epoch  9 Batch  74 / 228  Training Loss  8.228050137404352e-05\n","Epoch  9 Batch  75 / 228  Training Loss  0.00010001457121688873\n","Epoch  9 Batch  76 / 228  Training Loss  0.00010723226296249777\n","Epoch  9 Batch  77 / 228  Training Loss  8.624632027931511e-05\n","Epoch  9 Batch  78 / 228  Training Loss  7.901262142695487e-05\n","Epoch  9 Batch  79 / 228  Training Loss  6.776629743399099e-05\n","Epoch  9 Batch  80 / 228  Training Loss  9.448243508813903e-05\n","Epoch  9 Batch  81 / 228  Training Loss  0.000136158661916852\n","Epoch  9 Batch  82 / 228  Training Loss  7.088283018674701e-05\n","Epoch  9 Batch  83 / 228  Training Loss  0.0001420723128831014\n","Epoch  9 Batch  84 / 228  Training Loss  9.805717127164826e-05\n","Epoch  9 Batch  85 / 228  Training Loss  7.732167432550341e-05\n","Epoch  9 Batch  86 / 228  Training Loss  7.727996853645891e-05\n","Epoch  9 Batch  87 / 228  Training Loss  6.953698903089389e-05\n","Epoch  9 Batch  88 / 228  Training Loss  7.479738269466907e-05\n","Epoch  9 Batch  89 / 228  Training Loss  0.00010227097664028406\n","Epoch  9 Batch  90 / 228  Training Loss  0.00010950283467536792\n","Epoch  9 Batch  91 / 228  Training Loss  9.294961637351662e-05\n","Epoch  9 Batch  92 / 228  Training Loss  0.00010327735071768984\n","Epoch  9 Batch  93 / 228  Training Loss  8.346876711584628e-05\n","Epoch  9 Batch  94 / 228  Training Loss  0.00011640512093435973\n","Epoch  9 Batch  95 / 228  Training Loss  8.777778566582128e-05\n","Epoch  9 Batch  96 / 228  Training Loss  8.590005745645612e-05\n","Epoch  9 Batch  97 / 228  Training Loss  6.657269841525704e-05\n","Epoch  9 Batch  98 / 228  Training Loss  0.00010568949073785916\n","Epoch  9 Batch  99 / 228  Training Loss  0.00010895540617639199\n","Epoch  9 Batch  100 / 228  Training Loss  0.00010013517749030143\n","Epoch  9 Batch  101 / 228  Training Loss  0.00011376370821380988\n","Epoch  9 Batch  102 / 228  Training Loss  0.00013323243183549494\n","Epoch  9 Batch  103 / 228  Training Loss  6.164958176668733e-05\n","Epoch  9 Batch  104 / 228  Training Loss  8.825231634546071e-05\n","Epoch  9 Batch  105 / 228  Training Loss  9.971144754672423e-05\n","Epoch  9 Batch  106 / 228  Training Loss  9.76692681433633e-05\n","Epoch  9 Batch  107 / 228  Training Loss  6.868640048196539e-05\n","Epoch  9 Batch  108 / 228  Training Loss  6.603017391171306e-05\n","Epoch  9 Batch  109 / 228  Training Loss  6.530156679218635e-05\n","Epoch  9 Batch  110 / 228  Training Loss  8.851858001435176e-05\n","Epoch  9 Batch  111 / 228  Training Loss  6.951292016310617e-05\n","Epoch  9 Batch  112 / 228  Training Loss  8.042286208365113e-05\n","Epoch  9 Batch  113 / 228  Training Loss  0.00011345204256940633\n","Epoch  9 Batch  114 / 228  Training Loss  6.103352643549442e-05\n","Epoch  9 Batch  115 / 228  Training Loss  8.362492371816188e-05\n","Epoch  9 Batch  116 / 228  Training Loss  9.272883471567184e-05\n","Epoch  9 Batch  117 / 228  Training Loss  9.569110989104956e-05\n","Epoch  9 Batch  118 / 228  Training Loss  7.430104597005993e-05\n","Epoch  9 Batch  119 / 228  Training Loss  6.260932423174381e-05\n","Epoch  9 Batch  120 / 228  Training Loss  6.185190432006493e-05\n","Epoch  9 Batch  121 / 228  Training Loss  9.474282705923542e-05\n","Epoch  9 Batch  122 / 228  Training Loss  9.382698044646531e-05\n","Epoch  9 Batch  123 / 228  Training Loss  9.15192358661443e-05\n","Epoch  9 Batch  124 / 228  Training Loss  6.670986476819962e-05\n","Epoch  9 Batch  125 / 228  Training Loss  9.031301306094974e-05\n","Epoch  9 Batch  126 / 228  Training Loss  7.912157161626965e-05\n","Epoch  9 Batch  127 / 228  Training Loss  0.00012468329805415124\n","Epoch  9 Batch  128 / 228  Training Loss  6.47892666165717e-05\n","Epoch  9 Batch  129 / 228  Training Loss  7.31639884179458e-05\n","Epoch  9 Batch  130 / 228  Training Loss  7.366007775999606e-05\n","Epoch  9 Batch  131 / 228  Training Loss  6.546506483573467e-05\n","Epoch  9 Batch  132 / 228  Training Loss  7.176401413744316e-05\n","Epoch  9 Batch  133 / 228  Training Loss  8.890496974345297e-05\n","Epoch  9 Batch  134 / 228  Training Loss  8.16415049484931e-05\n","Epoch  9 Batch  135 / 228  Training Loss  7.853952411096543e-05\n","Epoch  9 Batch  136 / 228  Training Loss  8.138779958244413e-05\n","Epoch  9 Batch  137 / 228  Training Loss  8.026421710383147e-05\n","Epoch  9 Batch  138 / 228  Training Loss  8.393168536713347e-05\n","Epoch  9 Batch  139 / 228  Training Loss  0.00013040649355389178\n","Epoch  9 Batch  140 / 228  Training Loss  0.00016665839939378202\n","Epoch  9 Batch  141 / 228  Training Loss  8.766121754888445e-05\n","Epoch  9 Batch  142 / 228  Training Loss  8.11473437352106e-05\n","Epoch  9 Batch  143 / 228  Training Loss  8.190944936359301e-05\n","Epoch  9 Batch  144 / 228  Training Loss  7.567791908513755e-05\n","Epoch  9 Batch  145 / 228  Training Loss  9.559711907058954e-05\n","Epoch  9 Batch  146 / 228  Training Loss  9.715027408674359e-05\n","Epoch  9 Batch  147 / 228  Training Loss  8.185631304513663e-05\n","Epoch  9 Batch  148 / 228  Training Loss  6.348105671349913e-05\n","Epoch  9 Batch  149 / 228  Training Loss  6.353424396365881e-05\n","Epoch  9 Batch  150 / 228  Training Loss  6.817811663495377e-05\n","Epoch  9 Batch  151 / 228  Training Loss  8.049298048717901e-05\n","Epoch  9 Batch  152 / 228  Training Loss  6.523903721245006e-05\n","Epoch  9 Batch  153 / 228  Training Loss  7.039045158308e-05\n","Epoch  9 Batch  154 / 228  Training Loss  8.171364606823772e-05\n","Epoch  9 Batch  155 / 228  Training Loss  8.534795779269189e-05\n","Epoch  9 Batch  156 / 228  Training Loss  8.602040179539472e-05\n","Epoch  9 Batch  157 / 228  Training Loss  6.502474570879713e-05\n","Epoch  9 Batch  158 / 228  Training Loss  7.520260260207579e-05\n","Epoch  9 Batch  159 / 228  Training Loss  6.479845615103841e-05\n","Epoch  9 Batch  160 / 228  Training Loss  9.193200094159693e-05\n","Epoch  9 Batch  161 / 228  Training Loss  5.6180928368121386e-05\n","Epoch  9 Batch  162 / 228  Training Loss  7.937993359519169e-05\n","Epoch  9 Batch  163 / 228  Training Loss  8.750868437346071e-05\n","Epoch  9 Batch  164 / 228  Training Loss  7.448715041391551e-05\n","Epoch  9 Batch  165 / 228  Training Loss  7.967767305672169e-05\n","Epoch  9 Batch  166 / 228  Training Loss  6.722733087372035e-05\n","Epoch  9 Batch  167 / 228  Training Loss  8.140456338878721e-05\n","Epoch  9 Batch  168 / 228  Training Loss  6.914093683008105e-05\n","Epoch  9 Batch  169 / 228  Training Loss  5.654494816553779e-05\n","Epoch  9 Batch  170 / 228  Training Loss  6.639809726038948e-05\n","Epoch  9 Batch  171 / 228  Training Loss  9.259412036044523e-05\n","Epoch  9 Batch  172 / 228  Training Loss  7.970202568685636e-05\n","Epoch  9 Batch  173 / 228  Training Loss  4.604273271979764e-05\n","Epoch  9 Batch  174 / 228  Training Loss  6.967621447984129e-05\n","Epoch  9 Batch  175 / 228  Training Loss  8.074691868387163e-05\n","Epoch  9 Batch  176 / 228  Training Loss  0.0001159642924903892\n","Epoch  9 Batch  177 / 228  Training Loss  9.289807348977774e-05\n","Epoch  9 Batch  178 / 228  Training Loss  7.551092858193442e-05\n","Epoch  9 Batch  179 / 228  Training Loss  9.678825153969228e-05\n","Epoch  9 Batch  180 / 228  Training Loss  0.0002333434094907716\n","Epoch  9 Batch  181 / 228  Training Loss  6.0334608861012384e-05\n","Epoch  9 Batch  182 / 228  Training Loss  8.2982107414864e-05\n","Epoch  9 Batch  183 / 228  Training Loss  6.78293508826755e-05\n","Epoch  9 Batch  184 / 228  Training Loss  9.78706157184206e-05\n","Epoch  9 Batch  185 / 228  Training Loss  8.279168832814321e-05\n","Epoch  9 Batch  186 / 228  Training Loss  5.144472379470244e-05\n","Epoch  9 Batch  187 / 228  Training Loss  7.566134445369244e-05\n","Epoch  9 Batch  188 / 228  Training Loss  6.03574953856878e-05\n","Epoch  9 Batch  189 / 228  Training Loss  0.0001075131367542781\n","Epoch  9 Batch  190 / 228  Training Loss  7.355672278208658e-05\n","Epoch  9 Batch  191 / 228  Training Loss  6.370509800035506e-05\n","Epoch  9 Batch  192 / 228  Training Loss  7.002486381679773e-05\n","Epoch  9 Batch  193 / 228  Training Loss  6.110173853812739e-05\n","Epoch  9 Batch  194 / 228  Training Loss  9.592386777512729e-05\n","Epoch  9 Batch  195 / 228  Training Loss  9.680528455646709e-05\n","Epoch  9 Batch  196 / 228  Training Loss  6.591805868083611e-05\n","Epoch  9 Batch  197 / 228  Training Loss  9.563991625327617e-05\n","Epoch  9 Batch  198 / 228  Training Loss  8.313355647260323e-05\n","Epoch  9 Batch  199 / 228  Training Loss  9.817838144954294e-05\n","Epoch  9 Batch  200 / 228  Training Loss  8.39222630020231e-05\n","Epoch  9 Batch  201 / 228  Training Loss  9.493510151514784e-05\n","Epoch  9 Batch  202 / 228  Training Loss  6.335201032925397e-05\n","Epoch  9 Batch  203 / 228  Training Loss  5.721321940654889e-05\n","Epoch  9 Batch  204 / 228  Training Loss  8.93741162144579e-05\n","Epoch  9 Batch  205 / 228  Training Loss  7.41588301025331e-05\n","Epoch  9 Batch  206 / 228  Training Loss  6.965432112338021e-05\n","Epoch  9 Batch  207 / 228  Training Loss  0.00011222560715395957\n","Epoch  9 Batch  208 / 228  Training Loss  7.953098247526214e-05\n","Epoch  9 Batch  209 / 228  Training Loss  7.090061262715608e-05\n","Epoch  9 Batch  210 / 228  Training Loss  8.488507592119277e-05\n","Epoch  9 Batch  211 / 228  Training Loss  9.68362728599459e-05\n","Epoch  9 Batch  212 / 228  Training Loss  7.125049160094932e-05\n","Epoch  9 Batch  213 / 228  Training Loss  6.097375808167271e-05\n","Epoch  9 Batch  214 / 228  Training Loss  5.9999740187777206e-05\n","Epoch  9 Batch  215 / 228  Training Loss  6.704144470859319e-05\n","Epoch  9 Batch  216 / 228  Training Loss  7.665855082450435e-05\n","Epoch  9 Batch  217 / 228  Training Loss  6.59660145174712e-05\n","Epoch  9 Batch  218 / 228  Training Loss  5.9520447393879294e-05\n","Epoch  9 Batch  219 / 228  Training Loss  8.397454075748101e-05\n","Epoch  9 Batch  220 / 228  Training Loss  6.833938823547214e-05\n","Epoch  9 Batch  221 / 228  Training Loss  7.771906530251727e-05\n","Epoch  9 Batch  222 / 228  Training Loss  5.903383134864271e-05\n","Epoch  9 Batch  223 / 228  Training Loss  5.159698048373684e-05\n","Epoch  9 Batch  224 / 228  Training Loss  7.237413228722289e-05\n","Epoch  9 Batch  225 / 228  Training Loss  6.915514677530155e-05\n","Epoch  9 Batch  226 / 228  Training Loss  8.954600343713537e-05\n","Epoch  9 Batch  227 / 228  Training Loss  8.695107680978253e-05\n","  10    |    -    |   0.000091   | 99.580793\n","----------------------------------------------------------------------\n","Running epoch: 10\n","Epoch  10 Batch  0 / 228  Training Loss  7.192225893959403e-05\n","Epoch  10 Batch  1 / 228  Training Loss  6.422157457564026e-05\n","Epoch  10 Batch  2 / 228  Training Loss  8.209711813833565e-05\n","Epoch  10 Batch  3 / 228  Training Loss  9.074314584722742e-05\n","Epoch  10 Batch  4 / 228  Training Loss  6.497264257632196e-05\n","Epoch  10 Batch  5 / 228  Training Loss  7.088138227118179e-05\n","Epoch  10 Batch  6 / 228  Training Loss  6.481898890342563e-05\n","Epoch  10 Batch  7 / 228  Training Loss  6.951364048290998e-05\n","Epoch  10 Batch  8 / 228  Training Loss  4.416498268255964e-05\n","Epoch  10 Batch  9 / 228  Training Loss  6.516675784951076e-05\n","Epoch  10 Batch  10 / 228  Training Loss  5.91757707297802e-05\n","Epoch  10 Batch  11 / 228  Training Loss  7.871190609876066e-05\n","Epoch  10 Batch  12 / 228  Training Loss  8.296287705888972e-05\n","Epoch  10 Batch  13 / 228  Training Loss  5.677666558767669e-05\n","Epoch  10 Batch  14 / 228  Training Loss  9.555819269735366e-05\n","Epoch  10 Batch  15 / 228  Training Loss  4.3026357161579654e-05\n","Epoch  10 Batch  16 / 228  Training Loss  7.640635885763913e-05\n","Epoch  10 Batch  17 / 228  Training Loss  7.441837078658864e-05\n","Epoch  10 Batch  18 / 228  Training Loss  9.01300081750378e-05\n","Epoch  10 Batch  19 / 228  Training Loss  7.819941674824804e-05\n","Epoch  10 Batch  20 / 228  Training Loss  4.650365008274093e-05\n","Epoch  10 Batch  21 / 228  Training Loss  5.193047763896175e-05\n","Epoch  10 Batch  22 / 228  Training Loss  5.395475818659179e-05\n","Epoch  10 Batch  23 / 228  Training Loss  7.079062925186008e-05\n","Epoch  10 Batch  24 / 228  Training Loss  7.85424854257144e-05\n","Epoch  10 Batch  25 / 228  Training Loss  7.047978579066694e-05\n","Epoch  10 Batch  26 / 228  Training Loss  7.853902206989005e-05\n","Epoch  10 Batch  27 / 228  Training Loss  5.975384920020588e-05\n","Epoch  10 Batch  28 / 228  Training Loss  5.345707177184522e-05\n","Epoch  10 Batch  29 / 228  Training Loss  9.064029291039333e-05\n","Epoch  10 Batch  30 / 228  Training Loss  5.8121437177760527e-05\n","Epoch  10 Batch  31 / 228  Training Loss  7.481867942260578e-05\n","Epoch  10 Batch  32 / 228  Training Loss  5.8477959100855514e-05\n","Epoch  10 Batch  33 / 228  Training Loss  5.618908835458569e-05\n","Epoch  10 Batch  34 / 228  Training Loss  6.466996273957193e-05\n","Epoch  10 Batch  35 / 228  Training Loss  6.176500028232113e-05\n","Epoch  10 Batch  36 / 228  Training Loss  6.335334910545498e-05\n","Epoch  10 Batch  37 / 228  Training Loss  6.196140748215839e-05\n","Epoch  10 Batch  38 / 228  Training Loss  5.435473212855868e-05\n","Epoch  10 Batch  39 / 228  Training Loss  5.4945714509813115e-05\n","Epoch  10 Batch  40 / 228  Training Loss  7.937416376080364e-05\n","Epoch  10 Batch  41 / 228  Training Loss  5.599541327683255e-05\n","Epoch  10 Batch  42 / 228  Training Loss  7.503181404899806e-05\n","Epoch  10 Batch  43 / 228  Training Loss  9.960801980923861e-05\n","Epoch  10 Batch  44 / 228  Training Loss  8.244429773185402e-05\n","Epoch  10 Batch  45 / 228  Training Loss  7.207484304672107e-05\n","Epoch  10 Batch  46 / 228  Training Loss  7.38937669666484e-05\n","Epoch  10 Batch  47 / 228  Training Loss  7.179750537034124e-05\n","Epoch  10 Batch  48 / 228  Training Loss  4.648078902391717e-05\n","Epoch  10 Batch  49 / 228  Training Loss  5.96414138271939e-05\n","Epoch  10 Batch  50 / 228  Training Loss  5.9440750192152336e-05\n","Epoch  10 Batch  51 / 228  Training Loss  6.91897002980113e-05\n","Epoch  10 Batch  52 / 228  Training Loss  5.7624292821856216e-05\n","Epoch  10 Batch  53 / 228  Training Loss  7.590161840198562e-05\n","Epoch  10 Batch  54 / 228  Training Loss  7.557724165963009e-05\n","Epoch  10 Batch  55 / 228  Training Loss  7.569116132799536e-05\n","Epoch  10 Batch  56 / 228  Training Loss  9.508786024525762e-05\n","Epoch  10 Batch  57 / 228  Training Loss  9.157681779470295e-05\n","Epoch  10 Batch  58 / 228  Training Loss  7.037500472506508e-05\n","Epoch  10 Batch  59 / 228  Training Loss  4.3706244468921795e-05\n","Epoch  10 Batch  60 / 228  Training Loss  7.913973240647465e-05\n","Epoch  10 Batch  61 / 228  Training Loss  6.525777280330658e-05\n","Epoch  10 Batch  62 / 228  Training Loss  7.389209349639714e-05\n","Epoch  10 Batch  63 / 228  Training Loss  6.450991349993274e-05\n","Epoch  10 Batch  64 / 228  Training Loss  4.918021659250371e-05\n","Epoch  10 Batch  65 / 228  Training Loss  5.251747279544361e-05\n","Epoch  10 Batch  66 / 228  Training Loss  8.541247370885685e-05\n","Epoch  10 Batch  67 / 228  Training Loss  7.121126691345125e-05\n","Epoch  10 Batch  68 / 228  Training Loss  4.530525256996043e-05\n","Epoch  10 Batch  69 / 228  Training Loss  5.975222302367911e-05\n","Epoch  10 Batch  70 / 228  Training Loss  6.286900315899402e-05\n","Epoch  10 Batch  71 / 228  Training Loss  7.075863686623052e-05\n","Epoch  10 Batch  72 / 228  Training Loss  7.04249760019593e-05\n","Epoch  10 Batch  73 / 228  Training Loss  7.044179074000567e-05\n","Epoch  10 Batch  74 / 228  Training Loss  5.2267499995650724e-05\n","Epoch  10 Batch  75 / 228  Training Loss  5.71979398955591e-05\n","Epoch  10 Batch  76 / 228  Training Loss  9.267319546779618e-05\n","Epoch  10 Batch  77 / 228  Training Loss  6.469499930972233e-05\n","Epoch  10 Batch  78 / 228  Training Loss  6.923470937181264e-05\n","Epoch  10 Batch  79 / 228  Training Loss  5.9842179325642064e-05\n","Epoch  10 Batch  80 / 228  Training Loss  7.669329352211207e-05\n","Epoch  10 Batch  81 / 228  Training Loss  6.42978266114369e-05\n","Epoch  10 Batch  82 / 228  Training Loss  8.9495544671081e-05\n","Epoch  10 Batch  83 / 228  Training Loss  6.273605686146766e-05\n","Epoch  10 Batch  84 / 228  Training Loss  4.655577504308894e-05\n","Epoch  10 Batch  85 / 228  Training Loss  5.0495709729148075e-05\n","Epoch  10 Batch  86 / 228  Training Loss  4.848343814956024e-05\n","Epoch  10 Batch  87 / 228  Training Loss  4.897165854345076e-05\n","Epoch  10 Batch  88 / 228  Training Loss  5.721052366425283e-05\n","Epoch  10 Batch  89 / 228  Training Loss  7.440631452482194e-05\n","Epoch  10 Batch  90 / 228  Training Loss  6.4991538238246e-05\n","Epoch  10 Batch  91 / 228  Training Loss  7.74741783970967e-05\n","Epoch  10 Batch  92 / 228  Training Loss  6.653909076703712e-05\n","Epoch  10 Batch  93 / 228  Training Loss  6.568186654476449e-05\n","Epoch  10 Batch  94 / 228  Training Loss  5.436989522422664e-05\n","Epoch  10 Batch  95 / 228  Training Loss  5.538347613764927e-05\n","Epoch  10 Batch  96 / 228  Training Loss  5.318133480614051e-05\n","Epoch  10 Batch  97 / 228  Training Loss  7.6063304732088e-05\n","Epoch  10 Batch  98 / 228  Training Loss  6.68231732561253e-05\n","Epoch  10 Batch  99 / 228  Training Loss  6.458695861510932e-05\n","Epoch  10 Batch  100 / 228  Training Loss  6.096661672927439e-05\n","Epoch  10 Batch  101 / 228  Training Loss  6.614032463403419e-05\n","Epoch  10 Batch  102 / 228  Training Loss  5.168468851479702e-05\n","Epoch  10 Batch  103 / 228  Training Loss  7.320360600715503e-05\n","Epoch  10 Batch  104 / 228  Training Loss  4.77657958981581e-05\n","Epoch  10 Batch  105 / 228  Training Loss  4.974113471689634e-05\n","Epoch  10 Batch  106 / 228  Training Loss  5.1084884034935385e-05\n","Epoch  10 Batch  107 / 228  Training Loss  5.794446406071074e-05\n","Epoch  10 Batch  108 / 228  Training Loss  5.114623854751699e-05\n","Epoch  10 Batch  109 / 228  Training Loss  7.441984780598432e-05\n","Epoch  10 Batch  110 / 228  Training Loss  4.272013757145032e-05\n","Epoch  10 Batch  111 / 228  Training Loss  6.101647159084678e-05\n","Epoch  10 Batch  112 / 228  Training Loss  9.249777940567583e-05\n","Epoch  10 Batch  113 / 228  Training Loss  4.771953899762593e-05\n","Epoch  10 Batch  114 / 228  Training Loss  8.278232417069376e-05\n","Epoch  10 Batch  115 / 228  Training Loss  5.888594751013443e-05\n","Epoch  10 Batch  116 / 228  Training Loss  4.490794526645914e-05\n","Epoch  10 Batch  117 / 228  Training Loss  6.19510974502191e-05\n","Epoch  10 Batch  118 / 228  Training Loss  4.6811685024295e-05\n","Epoch  10 Batch  119 / 228  Training Loss  8.175745460903272e-05\n","Epoch  10 Batch  120 / 228  Training Loss  6.56454503769055e-05\n","Epoch  10 Batch  121 / 228  Training Loss  3.9935381209943444e-05\n","Epoch  10 Batch  122 / 228  Training Loss  5.2716448408318684e-05\n","Epoch  10 Batch  123 / 228  Training Loss  4.804577838513069e-05\n","Epoch  10 Batch  124 / 228  Training Loss  4.2737661715364084e-05\n","Epoch  10 Batch  125 / 228  Training Loss  6.376368401106447e-05\n","Epoch  10 Batch  126 / 228  Training Loss  5.0788821681635454e-05\n","Epoch  10 Batch  127 / 228  Training Loss  5.643439362756908e-05\n","Epoch  10 Batch  128 / 228  Training Loss  5.250558751868084e-05\n","Epoch  10 Batch  129 / 228  Training Loss  3.8477730413433164e-05\n","Epoch  10 Batch  130 / 228  Training Loss  6.415104144252837e-05\n","Epoch  10 Batch  131 / 228  Training Loss  6.443688471335918e-05\n","Epoch  10 Batch  132 / 228  Training Loss  4.229725891491398e-05\n","Epoch  10 Batch  133 / 228  Training Loss  6.4122024923563e-05\n","Epoch  10 Batch  134 / 228  Training Loss  3.593669316614978e-05\n","Epoch  10 Batch  135 / 228  Training Loss  5.017471266910434e-05\n","Epoch  10 Batch  136 / 228  Training Loss  4.880528285866603e-05\n","Epoch  10 Batch  137 / 228  Training Loss  4.9462665629107505e-05\n","Epoch  10 Batch  138 / 228  Training Loss  5.320524360286072e-05\n","Epoch  10 Batch  139 / 228  Training Loss  6.338754610624164e-05\n","Epoch  10 Batch  140 / 228  Training Loss  7.219643157441169e-05\n","Epoch  10 Batch  141 / 228  Training Loss  8.918199455365539e-05\n","Epoch  10 Batch  142 / 228  Training Loss  5.513944779522717e-05\n","Epoch  10 Batch  143 / 228  Training Loss  5.85478737775702e-05\n","Epoch  10 Batch  144 / 228  Training Loss  4.3359883420635015e-05\n","Epoch  10 Batch  145 / 228  Training Loss  6.439851131290197e-05\n","Epoch  10 Batch  146 / 228  Training Loss  4.392043047118932e-05\n","Epoch  10 Batch  147 / 228  Training Loss  5.580984361586161e-05\n","Epoch  10 Batch  148 / 228  Training Loss  5.9514994063647464e-05\n","Epoch  10 Batch  149 / 228  Training Loss  4.3627827835734934e-05\n","Epoch  10 Batch  150 / 228  Training Loss  4.980236917617731e-05\n","Epoch  10 Batch  151 / 228  Training Loss  6.830888742115349e-05\n","Epoch  10 Batch  152 / 228  Training Loss  6.052160824765451e-05\n","Epoch  10 Batch  153 / 228  Training Loss  5.758761108154431e-05\n","Epoch  10 Batch  154 / 228  Training Loss  6.309186574071646e-05\n","Epoch  10 Batch  155 / 228  Training Loss  4.810283280676231e-05\n","Epoch  10 Batch  156 / 228  Training Loss  6.840885907877237e-05\n","Epoch  10 Batch  157 / 228  Training Loss  6.949821545276791e-05\n","Epoch  10 Batch  158 / 228  Training Loss  5.451958713820204e-05\n","Epoch  10 Batch  159 / 228  Training Loss  5.507603054866195e-05\n","Epoch  10 Batch  160 / 228  Training Loss  4.9429479986429214e-05\n","Epoch  10 Batch  161 / 228  Training Loss  5.30270735907834e-05\n","Epoch  10 Batch  162 / 228  Training Loss  6.353342178044841e-05\n","Epoch  10 Batch  163 / 228  Training Loss  4.710686698672362e-05\n","Epoch  10 Batch  164 / 228  Training Loss  5.516783494385891e-05\n","Epoch  10 Batch  165 / 228  Training Loss  6.374047370627522e-05\n","Epoch  10 Batch  166 / 228  Training Loss  5.385163240134716e-05\n","Epoch  10 Batch  167 / 228  Training Loss  6.581739580724388e-05\n","Epoch  10 Batch  168 / 228  Training Loss  8.094287477433681e-05\n","Epoch  10 Batch  169 / 228  Training Loss  5.480310210259631e-05\n","Epoch  10 Batch  170 / 228  Training Loss  5.660932947648689e-05\n","Epoch  10 Batch  171 / 228  Training Loss  9.641264477977529e-05\n","Epoch  10 Batch  172 / 228  Training Loss  5.6242337450385094e-05\n","Epoch  10 Batch  173 / 228  Training Loss  5.860285455128178e-05\n","Epoch  10 Batch  174 / 228  Training Loss  3.280926466686651e-05\n","Epoch  10 Batch  175 / 228  Training Loss  6.298172229435295e-05\n","Epoch  10 Batch  176 / 228  Training Loss  5.8068264479516074e-05\n","Epoch  10 Batch  177 / 228  Training Loss  4.697703479905613e-05\n","Epoch  10 Batch  178 / 228  Training Loss  4.921943764202297e-05\n","Epoch  10 Batch  179 / 228  Training Loss  4.7796278522582725e-05\n","Epoch  10 Batch  180 / 228  Training Loss  6.345530709950253e-05\n","Epoch  10 Batch  181 / 228  Training Loss  4.652402640203945e-05\n","Epoch  10 Batch  182 / 228  Training Loss  6.567045056726784e-05\n","Epoch  10 Batch  183 / 228  Training Loss  7.174831262091175e-05\n","Epoch  10 Batch  184 / 228  Training Loss  6.075307464925572e-05\n","Epoch  10 Batch  185 / 228  Training Loss  5.547092223423533e-05\n","Epoch  10 Batch  186 / 228  Training Loss  8.103299478534609e-05\n","Epoch  10 Batch  187 / 228  Training Loss  4.837671804125421e-05\n","Epoch  10 Batch  188 / 228  Training Loss  5.862992111360654e-05\n","Epoch  10 Batch  189 / 228  Training Loss  4.492710650083609e-05\n","Epoch  10 Batch  190 / 228  Training Loss  7.677821849938482e-05\n","Epoch  10 Batch  191 / 228  Training Loss  7.05619459040463e-05\n","Epoch  10 Batch  192 / 228  Training Loss  4.925199391436763e-05\n","Epoch  10 Batch  193 / 228  Training Loss  4.143671685596928e-05\n","Epoch  10 Batch  194 / 228  Training Loss  4.8434649215778336e-05\n","Epoch  10 Batch  195 / 228  Training Loss  5.2381066780071706e-05\n","Epoch  10 Batch  196 / 228  Training Loss  6.210491119418293e-05\n","Epoch  10 Batch  197 / 228  Training Loss  4.597746374201961e-05\n","Epoch  10 Batch  198 / 228  Training Loss  4.4608019379666075e-05\n","Epoch  10 Batch  199 / 228  Training Loss  4.6565255615860224e-05\n","Epoch  10 Batch  200 / 228  Training Loss  6.568771641468629e-05\n","Epoch  10 Batch  201 / 228  Training Loss  4.3239120714133605e-05\n","Epoch  10 Batch  202 / 228  Training Loss  4.898166662314907e-05\n","Epoch  10 Batch  203 / 228  Training Loss  5.8065612392965704e-05\n","Epoch  10 Batch  204 / 228  Training Loss  5.4465053835883737e-05\n","Epoch  10 Batch  205 / 228  Training Loss  6.118629971751943e-05\n","Epoch  10 Batch  206 / 228  Training Loss  6.08858754276298e-05\n","Epoch  10 Batch  207 / 228  Training Loss  4.355475175543688e-05\n","Epoch  10 Batch  208 / 228  Training Loss  6.415217649191618e-05\n","Epoch  10 Batch  209 / 228  Training Loss  7.090522558428347e-05\n","Epoch  10 Batch  210 / 228  Training Loss  5.340068673831411e-05\n","Epoch  10 Batch  211 / 228  Training Loss  7.795928831910715e-05\n","Epoch  10 Batch  212 / 228  Training Loss  6.132847920525819e-05\n","Epoch  10 Batch  213 / 228  Training Loss  4.3712661863537505e-05\n","Epoch  10 Batch  214 / 228  Training Loss  4.4147738663014024e-05\n","Epoch  10 Batch  215 / 228  Training Loss  5.4841941164340824e-05\n","Epoch  10 Batch  216 / 228  Training Loss  5.620324009214528e-05\n","Epoch  10 Batch  217 / 228  Training Loss  5.761606735177338e-05\n","Epoch  10 Batch  218 / 228  Training Loss  5.1871313189622015e-05\n","Epoch  10 Batch  219 / 228  Training Loss  8.058203820837662e-05\n","Epoch  10 Batch  220 / 228  Training Loss  6.0563073930097744e-05\n","Epoch  10 Batch  221 / 228  Training Loss  4.508283018367365e-05\n","Epoch  10 Batch  222 / 228  Training Loss  6.564802606590092e-05\n","Epoch  10 Batch  223 / 228  Training Loss  6.525158823933452e-05\n","Epoch  10 Batch  224 / 228  Training Loss  6.610869604628533e-05\n","Epoch  10 Batch  225 / 228  Training Loss  6.67208805680275e-05\n","Epoch  10 Batch  226 / 228  Training Loss  6.007691263221204e-05\n","Epoch  10 Batch  227 / 228  Training Loss  5.364364551496692e-05\n","  11    |    -    |   0.000062   | 99.504573\n","----------------------------------------------------------------------\n","Running epoch: 11\n","Epoch  11 Batch  0 / 228  Training Loss  6.482606113422662e-05\n","Epoch  11 Batch  1 / 228  Training Loss  5.368816709960811e-05\n","Epoch  11 Batch  2 / 228  Training Loss  5.8939353039022535e-05\n","Epoch  11 Batch  3 / 228  Training Loss  5.4889387683942914e-05\n","Epoch  11 Batch  4 / 228  Training Loss  4.7256340621970594e-05\n","Epoch  11 Batch  5 / 228  Training Loss  5.4612650274066254e-05\n","Epoch  11 Batch  6 / 228  Training Loss  3.634501626947895e-05\n","Epoch  11 Batch  7 / 228  Training Loss  4.6997152821859345e-05\n","Epoch  11 Batch  8 / 228  Training Loss  5.052295819041319e-05\n","Epoch  11 Batch  9 / 228  Training Loss  5.607759885606356e-05\n","Epoch  11 Batch  10 / 228  Training Loss  5.1683902711374685e-05\n","Epoch  11 Batch  11 / 228  Training Loss  5.6242897699121386e-05\n","Epoch  11 Batch  12 / 228  Training Loss  5.450790195027366e-05\n","Epoch  11 Batch  13 / 228  Training Loss  4.3680913222488016e-05\n","Epoch  11 Batch  14 / 228  Training Loss  4.965473272022791e-05\n","Epoch  11 Batch  15 / 228  Training Loss  5.879341551917605e-05\n","Epoch  11 Batch  16 / 228  Training Loss  5.3097308409633115e-05\n","Epoch  11 Batch  17 / 228  Training Loss  3.996703162556514e-05\n","Epoch  11 Batch  18 / 228  Training Loss  3.985574949183501e-05\n","Epoch  11 Batch  19 / 228  Training Loss  6.83759935782291e-05\n","Epoch  11 Batch  20 / 228  Training Loss  4.915831232210621e-05\n","Epoch  11 Batch  21 / 228  Training Loss  4.4528347643790767e-05\n","Epoch  11 Batch  22 / 228  Training Loss  4.346205969341099e-05\n","Epoch  11 Batch  23 / 228  Training Loss  6.295896309893578e-05\n","Epoch  11 Batch  24 / 228  Training Loss  5.821110607939772e-05\n","Epoch  11 Batch  25 / 228  Training Loss  4.6145240048645064e-05\n","Epoch  11 Batch  26 / 228  Training Loss  3.912574175046757e-05\n","Epoch  11 Batch  27 / 228  Training Loss  4.480378265725449e-05\n","Epoch  11 Batch  28 / 228  Training Loss  4.604851346812211e-05\n","Epoch  11 Batch  29 / 228  Training Loss  4.735427864943631e-05\n","Epoch  11 Batch  30 / 228  Training Loss  6.403496081475168e-05\n","Epoch  11 Batch  31 / 228  Training Loss  6.161922647152096e-05\n","Epoch  11 Batch  32 / 228  Training Loss  4.457418617676012e-05\n","Epoch  11 Batch  33 / 228  Training Loss  4.763500328408554e-05\n","Epoch  11 Batch  34 / 228  Training Loss  4.792703111888841e-05\n","Epoch  11 Batch  35 / 228  Training Loss  3.551477857399732e-05\n","Epoch  11 Batch  36 / 228  Training Loss  4.7243844164768234e-05\n","Epoch  11 Batch  37 / 228  Training Loss  4.989007720723748e-05\n","Epoch  11 Batch  38 / 228  Training Loss  4.2532636143732816e-05\n","Epoch  11 Batch  39 / 228  Training Loss  5.164959657122381e-05\n","Epoch  11 Batch  40 / 228  Training Loss  6.040367225068621e-05\n","Epoch  11 Batch  41 / 228  Training Loss  4.8578778660157695e-05\n","Epoch  11 Batch  42 / 228  Training Loss  5.6404463975923136e-05\n","Epoch  11 Batch  43 / 228  Training Loss  4.689937850343995e-05\n","Epoch  11 Batch  44 / 228  Training Loss  4.467522376216948e-05\n","Epoch  11 Batch  45 / 228  Training Loss  5.070316183264367e-05\n","Epoch  11 Batch  46 / 228  Training Loss  4.026345050078817e-05\n","Epoch  11 Batch  47 / 228  Training Loss  4.280414941604249e-05\n","Epoch  11 Batch  48 / 228  Training Loss  4.663534491555765e-05\n","Epoch  11 Batch  49 / 228  Training Loss  4.774264743900858e-05\n","Epoch  11 Batch  50 / 228  Training Loss  4.773248656420037e-05\n","Epoch  11 Batch  51 / 228  Training Loss  4.1979852539952844e-05\n","Epoch  11 Batch  52 / 228  Training Loss  3.8485712138935924e-05\n","Epoch  11 Batch  53 / 228  Training Loss  6.393803050741553e-05\n","Epoch  11 Batch  54 / 228  Training Loss  3.779441613005474e-05\n","Epoch  11 Batch  55 / 228  Training Loss  6.898297579027712e-05\n","Epoch  11 Batch  56 / 228  Training Loss  4.259307752363384e-05\n","Epoch  11 Batch  57 / 228  Training Loss  6.156234303489327e-05\n","Epoch  11 Batch  58 / 228  Training Loss  5.227209112490527e-05\n","Epoch  11 Batch  59 / 228  Training Loss  5.0363240006845444e-05\n","Epoch  11 Batch  60 / 228  Training Loss  3.726475915755145e-05\n","Epoch  11 Batch  61 / 228  Training Loss  4.4703552703140303e-05\n","Epoch  11 Batch  62 / 228  Training Loss  5.112351573188789e-05\n","Epoch  11 Batch  63 / 228  Training Loss  4.495043322094716e-05\n","Epoch  11 Batch  64 / 228  Training Loss  5.3663639846490696e-05\n","Epoch  11 Batch  65 / 228  Training Loss  4.2649135139072314e-05\n","Epoch  11 Batch  66 / 228  Training Loss  6.404171290341765e-05\n","Epoch  11 Batch  67 / 228  Training Loss  5.57288367417641e-05\n","Epoch  11 Batch  68 / 228  Training Loss  4.440454358700663e-05\n","Epoch  11 Batch  69 / 228  Training Loss  4.11147229897324e-05\n","Epoch  11 Batch  70 / 228  Training Loss  5.933919965173118e-05\n","Epoch  11 Batch  71 / 228  Training Loss  5.3928804845782e-05\n","Epoch  11 Batch  72 / 228  Training Loss  6.68807333568111e-05\n","Epoch  11 Batch  73 / 228  Training Loss  6.438799755414948e-05\n","Epoch  11 Batch  74 / 228  Training Loss  5.26043695572298e-05\n","Epoch  11 Batch  75 / 228  Training Loss  4.755245754495263e-05\n","Epoch  11 Batch  76 / 228  Training Loss  4.7356912546092644e-05\n","Epoch  11 Batch  77 / 228  Training Loss  3.9933802327141166e-05\n","Epoch  11 Batch  78 / 228  Training Loss  6.276030762819573e-05\n","Epoch  11 Batch  79 / 228  Training Loss  5.400034206104465e-05\n","Epoch  11 Batch  80 / 228  Training Loss  4.7972152970032766e-05\n","Epoch  11 Batch  81 / 228  Training Loss  3.360846312716603e-05\n","Epoch  11 Batch  82 / 228  Training Loss  4.29940628237091e-05\n","Epoch  11 Batch  83 / 228  Training Loss  4.905490641249344e-05\n","Epoch  11 Batch  84 / 228  Training Loss  6.27248446107842e-05\n","Epoch  11 Batch  85 / 228  Training Loss  6.315716018434614e-05\n","Epoch  11 Batch  86 / 228  Training Loss  5.552845686906949e-05\n","Epoch  11 Batch  87 / 228  Training Loss  6.506984209408984e-05\n","Epoch  11 Batch  88 / 228  Training Loss  4.641394843929447e-05\n","Epoch  11 Batch  89 / 228  Training Loss  4.106682899873704e-05\n","Epoch  11 Batch  90 / 228  Training Loss  3.8490386941703036e-05\n","Epoch  11 Batch  91 / 228  Training Loss  5.2078463340876624e-05\n","Epoch  11 Batch  92 / 228  Training Loss  3.745203866856173e-05\n","Epoch  11 Batch  93 / 228  Training Loss  5.1226146752014756e-05\n","Epoch  11 Batch  94 / 228  Training Loss  5.450770913739689e-05\n","Epoch  11 Batch  95 / 228  Training Loss  5.4914904467295855e-05\n","Epoch  11 Batch  96 / 228  Training Loss  4.556000203592703e-05\n","Epoch  11 Batch  97 / 228  Training Loss  4.641296254703775e-05\n","Epoch  11 Batch  98 / 228  Training Loss  4.674382944358513e-05\n","Epoch  11 Batch  99 / 228  Training Loss  4.7109097067732364e-05\n","Epoch  11 Batch  100 / 228  Training Loss  3.3902448194567114e-05\n","Epoch  11 Batch  101 / 228  Training Loss  5.4888008889975026e-05\n","Epoch  11 Batch  102 / 228  Training Loss  6.25013344688341e-05\n","Epoch  11 Batch  103 / 228  Training Loss  5.168880306882784e-05\n","Epoch  11 Batch  104 / 228  Training Loss  5.324703670339659e-05\n","Epoch  11 Batch  105 / 228  Training Loss  4.700745193986222e-05\n","Epoch  11 Batch  106 / 228  Training Loss  4.0947896195575595e-05\n","Epoch  11 Batch  107 / 228  Training Loss  4.940385042573325e-05\n","Epoch  11 Batch  108 / 228  Training Loss  6.429971836041659e-05\n","Epoch  11 Batch  109 / 228  Training Loss  4.543220711639151e-05\n","Epoch  11 Batch  110 / 228  Training Loss  4.834227729588747e-05\n","Epoch  11 Batch  111 / 228  Training Loss  4.682766302721575e-05\n","Epoch  11 Batch  112 / 228  Training Loss  4.948130663251504e-05\n","Epoch  11 Batch  113 / 228  Training Loss  6.28746856818907e-05\n","Epoch  11 Batch  114 / 228  Training Loss  4.1378018067916855e-05\n","Epoch  11 Batch  115 / 228  Training Loss  6.38450583210215e-05\n","Epoch  11 Batch  116 / 228  Training Loss  2.5740842829691246e-05\n","Epoch  11 Batch  117 / 228  Training Loss  5.8985060604754835e-05\n","Epoch  11 Batch  118 / 228  Training Loss  5.616833732347004e-05\n","Epoch  11 Batch  119 / 228  Training Loss  3.2521216780878603e-05\n","Epoch  11 Batch  120 / 228  Training Loss  3.9229344110935926e-05\n","Epoch  11 Batch  121 / 228  Training Loss  4.769238148583099e-05\n","Epoch  11 Batch  122 / 228  Training Loss  3.508316876832396e-05\n","Epoch  11 Batch  123 / 228  Training Loss  5.2900402806699276e-05\n","Epoch  11 Batch  124 / 228  Training Loss  6.430117355193943e-05\n","Epoch  11 Batch  125 / 228  Training Loss  4.7937854105839506e-05\n","Epoch  11 Batch  126 / 228  Training Loss  4.1504834371153265e-05\n","Epoch  11 Batch  127 / 228  Training Loss  5.415215127868578e-05\n","Epoch  11 Batch  128 / 228  Training Loss  6.844794552307576e-05\n","Epoch  11 Batch  129 / 228  Training Loss  4.6729186578886583e-05\n","Epoch  11 Batch  130 / 228  Training Loss  4.105725383851677e-05\n","Epoch  11 Batch  131 / 228  Training Loss  4.9950718675972894e-05\n","Epoch  11 Batch  132 / 228  Training Loss  5.390203295974061e-05\n","Epoch  11 Batch  133 / 228  Training Loss  5.8718393120216206e-05\n","Epoch  11 Batch  134 / 228  Training Loss  4.6383422159124166e-05\n","Epoch  11 Batch  135 / 228  Training Loss  4.5526332542067394e-05\n","Epoch  11 Batch  136 / 228  Training Loss  4.240808266331442e-05\n","Epoch  11 Batch  137 / 228  Training Loss  5.397567292675376e-05\n","Epoch  11 Batch  138 / 228  Training Loss  5.0099799409508705e-05\n","Epoch  11 Batch  139 / 228  Training Loss  4.664436710299924e-05\n","Epoch  11 Batch  140 / 228  Training Loss  5.1410181185929105e-05\n","Epoch  11 Batch  141 / 228  Training Loss  5.666857396136038e-05\n","Epoch  11 Batch  142 / 228  Training Loss  4.758434442919679e-05\n","Epoch  11 Batch  143 / 228  Training Loss  4.365912900539115e-05\n","Epoch  11 Batch  144 / 228  Training Loss  3.629385901149362e-05\n","Epoch  11 Batch  145 / 228  Training Loss  7.031262794043869e-05\n","Epoch  11 Batch  146 / 228  Training Loss  4.842574708163738e-05\n","Epoch  11 Batch  147 / 228  Training Loss  5.698455424862914e-05\n","Epoch  11 Batch  148 / 228  Training Loss  3.2911582820815966e-05\n","Epoch  11 Batch  149 / 228  Training Loss  5.428634904092178e-05\n","Epoch  11 Batch  150 / 228  Training Loss  6.0227757785469294e-05\n","Epoch  11 Batch  151 / 228  Training Loss  3.7128695112187415e-05\n","Epoch  11 Batch  152 / 228  Training Loss  5.0752230890793726e-05\n","Epoch  11 Batch  153 / 228  Training Loss  3.738527084351517e-05\n","Epoch  11 Batch  154 / 228  Training Loss  5.237761797616258e-05\n","Epoch  11 Batch  155 / 228  Training Loss  4.4698470446746796e-05\n","Epoch  11 Batch  156 / 228  Training Loss  3.898139766533859e-05\n","Epoch  11 Batch  157 / 228  Training Loss  3.242753882659599e-05\n","Epoch  11 Batch  158 / 228  Training Loss  3.979373650508933e-05\n","Epoch  11 Batch  159 / 228  Training Loss  4.7350767999887466e-05\n","Epoch  11 Batch  160 / 228  Training Loss  3.431592267588712e-05\n","Epoch  11 Batch  161 / 228  Training Loss  5.504656655830331e-05\n","Epoch  11 Batch  162 / 228  Training Loss  4.277106927474961e-05\n","Epoch  11 Batch  163 / 228  Training Loss  3.9199185266625136e-05\n","Epoch  11 Batch  164 / 228  Training Loss  4.76683271699585e-05\n","Epoch  11 Batch  165 / 228  Training Loss  5.363621312426403e-05\n","Epoch  11 Batch  166 / 228  Training Loss  4.779220398631878e-05\n","Epoch  11 Batch  167 / 228  Training Loss  3.710680903168395e-05\n","Epoch  11 Batch  168 / 228  Training Loss  5.103034709463827e-05\n","Epoch  11 Batch  169 / 228  Training Loss  3.2866864785319194e-05\n","Epoch  11 Batch  170 / 228  Training Loss  4.996109055355191e-05\n","Epoch  11 Batch  171 / 228  Training Loss  4.2531137296464294e-05\n","Epoch  11 Batch  172 / 228  Training Loss  4.942761006532237e-05\n","Epoch  11 Batch  173 / 228  Training Loss  4.031971548101865e-05\n","Epoch  11 Batch  174 / 228  Training Loss  6.652637966908514e-05\n","Epoch  11 Batch  175 / 228  Training Loss  4.204126526019536e-05\n","Epoch  11 Batch  176 / 228  Training Loss  4.332805474405177e-05\n","Epoch  11 Batch  177 / 228  Training Loss  4.993122274754569e-05\n","Epoch  11 Batch  178 / 228  Training Loss  4.4287626224104315e-05\n","Epoch  11 Batch  179 / 228  Training Loss  4.557947613648139e-05\n","Epoch  11 Batch  180 / 228  Training Loss  4.538009306997992e-05\n","Epoch  11 Batch  181 / 228  Training Loss  4.722074663732201e-05\n","Epoch  11 Batch  182 / 228  Training Loss  4.0563831134932116e-05\n","Epoch  11 Batch  183 / 228  Training Loss  4.752157110488042e-05\n","Epoch  11 Batch  184 / 228  Training Loss  4.7931702283676714e-05\n","Epoch  11 Batch  185 / 228  Training Loss  5.9662397688953206e-05\n","Epoch  11 Batch  186 / 228  Training Loss  3.918957736459561e-05\n","Epoch  11 Batch  187 / 228  Training Loss  4.9102243792731315e-05\n","Epoch  11 Batch  188 / 228  Training Loss  2.7210840926272795e-05\n","Epoch  11 Batch  189 / 228  Training Loss  4.156929935561493e-05\n","Epoch  11 Batch  190 / 228  Training Loss  3.826904503512196e-05\n","Epoch  11 Batch  191 / 228  Training Loss  3.249957808293402e-05\n","Epoch  11 Batch  192 / 228  Training Loss  5.625772610073909e-05\n","Epoch  11 Batch  193 / 228  Training Loss  5.401406087912619e-05\n","Epoch  11 Batch  194 / 228  Training Loss  4.8761146899778396e-05\n","Epoch  11 Batch  195 / 228  Training Loss  4.058818376506679e-05\n","Epoch  11 Batch  196 / 228  Training Loss  5.466529546538368e-05\n","Epoch  11 Batch  197 / 228  Training Loss  4.763137985719368e-05\n","Epoch  11 Batch  198 / 228  Training Loss  3.945260323234834e-05\n","Epoch  11 Batch  199 / 228  Training Loss  3.784213913604617e-05\n","Epoch  11 Batch  200 / 228  Training Loss  5.452659752336331e-05\n","Epoch  11 Batch  201 / 228  Training Loss  5.240926839178428e-05\n","Epoch  11 Batch  202 / 228  Training Loss  5.5676366173429415e-05\n","Epoch  11 Batch  203 / 228  Training Loss  4.546526906779036e-05\n","Epoch  11 Batch  204 / 228  Training Loss  2.714913534873631e-05\n","Epoch  11 Batch  205 / 228  Training Loss  6.913069955771789e-05\n","Epoch  11 Batch  206 / 228  Training Loss  5.7698111049830914e-05\n","Epoch  11 Batch  207 / 228  Training Loss  5.150057404534891e-05\n","Epoch  11 Batch  208 / 228  Training Loss  4.599913881975226e-05\n","Epoch  11 Batch  209 / 228  Training Loss  3.5011409636354074e-05\n","Epoch  11 Batch  210 / 228  Training Loss  5.322365905158222e-05\n","Epoch  11 Batch  211 / 228  Training Loss  3.969846511608921e-05\n","Epoch  11 Batch  212 / 228  Training Loss  4.487239493755624e-05\n","Epoch  11 Batch  213 / 228  Training Loss  5.054736539022997e-05\n","Epoch  11 Batch  214 / 228  Training Loss  6.368984759319574e-05\n","Epoch  11 Batch  215 / 228  Training Loss  3.853414091281593e-05\n","Epoch  11 Batch  216 / 228  Training Loss  3.70794405171182e-05\n","Epoch  11 Batch  217 / 228  Training Loss  3.663232928374782e-05\n","Epoch  11 Batch  218 / 228  Training Loss  3.632075458881445e-05\n","Epoch  11 Batch  219 / 228  Training Loss  3.9501355786342174e-05\n","Epoch  11 Batch  220 / 228  Training Loss  4.667684333981015e-05\n","Epoch  11 Batch  221 / 228  Training Loss  2.9549028113251552e-05\n","Epoch  11 Batch  222 / 228  Training Loss  3.8422713259933516e-05\n","Epoch  11 Batch  223 / 228  Training Loss  4.326651833252981e-05\n","Epoch  11 Batch  224 / 228  Training Loss  3.923059193766676e-05\n","Epoch  11 Batch  225 / 228  Training Loss  5.025671634939499e-05\n","Epoch  11 Batch  226 / 228  Training Loss  3.93137670471333e-05\n","Epoch  11 Batch  227 / 228  Training Loss  4.031898788525723e-05\n","  12    |    -    |   0.000048   | 99.542683\n","----------------------------------------------------------------------\n","Running epoch: 12\n","Epoch  12 Batch  0 / 228  Training Loss  4.3439253204269335e-05\n","Epoch  12 Batch  1 / 228  Training Loss  4.798072041012347e-05\n","Epoch  12 Batch  2 / 228  Training Loss  4.618982347892597e-05\n","Epoch  12 Batch  3 / 228  Training Loss  4.946772241964936e-05\n","Epoch  12 Batch  4 / 228  Training Loss  3.6735862522618845e-05\n","Epoch  12 Batch  5 / 228  Training Loss  4.340069790487178e-05\n","Epoch  12 Batch  6 / 228  Training Loss  4.580617678584531e-05\n","Epoch  12 Batch  7 / 228  Training Loss  3.486430432531051e-05\n","Epoch  12 Batch  8 / 228  Training Loss  3.639334317995235e-05\n","Epoch  12 Batch  9 / 228  Training Loss  3.753149576368742e-05\n","Epoch  12 Batch  10 / 228  Training Loss  5.006411811336875e-05\n","Epoch  12 Batch  11 / 228  Training Loss  3.953728810301982e-05\n","Epoch  12 Batch  12 / 228  Training Loss  3.722321707755327e-05\n","Epoch  12 Batch  13 / 228  Training Loss  4.211878331261687e-05\n","Epoch  12 Batch  14 / 228  Training Loss  4.039332634420134e-05\n","Epoch  12 Batch  15 / 228  Training Loss  3.612056389101781e-05\n","Epoch  12 Batch  16 / 228  Training Loss  3.666580596473068e-05\n","Epoch  12 Batch  17 / 228  Training Loss  4.80344970128499e-05\n","Epoch  12 Batch  18 / 228  Training Loss  3.1687650334788486e-05\n","Epoch  12 Batch  19 / 228  Training Loss  2.8714619475067593e-05\n","Epoch  12 Batch  20 / 228  Training Loss  3.577709139790386e-05\n","Epoch  12 Batch  21 / 228  Training Loss  4.703412196249701e-05\n","Epoch  12 Batch  22 / 228  Training Loss  5.6786600907798856e-05\n","Epoch  12 Batch  23 / 228  Training Loss  3.475557969068177e-05\n","Epoch  12 Batch  24 / 228  Training Loss  4.7074347094167024e-05\n","Epoch  12 Batch  25 / 228  Training Loss  3.316807124065235e-05\n","Epoch  12 Batch  26 / 228  Training Loss  5.494687502505258e-05\n","Epoch  12 Batch  27 / 228  Training Loss  4.239764530211687e-05\n","Epoch  12 Batch  28 / 228  Training Loss  3.597852628445253e-05\n","Epoch  12 Batch  29 / 228  Training Loss  3.698596992762759e-05\n","Epoch  12 Batch  30 / 228  Training Loss  4.61231502413284e-05\n","Epoch  12 Batch  31 / 228  Training Loss  3.9018970710458234e-05\n","Epoch  12 Batch  32 / 228  Training Loss  3.271170498919673e-05\n","Epoch  12 Batch  33 / 228  Training Loss  4.3077943701064214e-05\n","Epoch  12 Batch  34 / 228  Training Loss  5.141554356669076e-05\n","Epoch  12 Batch  35 / 228  Training Loss  4.221601557219401e-05\n","Epoch  12 Batch  36 / 228  Training Loss  3.90279819839634e-05\n","Epoch  12 Batch  37 / 228  Training Loss  4.301680382923223e-05\n","Epoch  12 Batch  38 / 228  Training Loss  3.759854371310212e-05\n","Epoch  12 Batch  39 / 228  Training Loss  3.726067370735109e-05\n","Epoch  12 Batch  40 / 228  Training Loss  3.2557541999267414e-05\n","Epoch  12 Batch  41 / 228  Training Loss  3.2868141715880483e-05\n","Epoch  12 Batch  42 / 228  Training Loss  5.059506656834856e-05\n","Epoch  12 Batch  43 / 228  Training Loss  4.4098538637626916e-05\n","Epoch  12 Batch  44 / 228  Training Loss  4.706155232270248e-05\n","Epoch  12 Batch  45 / 228  Training Loss  4.616929072653875e-05\n","Epoch  12 Batch  46 / 228  Training Loss  3.800438207690604e-05\n","Epoch  12 Batch  47 / 228  Training Loss  4.6771714551141486e-05\n","Epoch  12 Batch  48 / 228  Training Loss  3.48724533978384e-05\n","Epoch  12 Batch  49 / 228  Training Loss  3.57334065483883e-05\n","Epoch  12 Batch  50 / 228  Training Loss  4.452950452105142e-05\n","Epoch  12 Batch  51 / 228  Training Loss  3.290369568276219e-05\n","Epoch  12 Batch  52 / 228  Training Loss  3.360943810548633e-05\n","Epoch  12 Batch  53 / 228  Training Loss  3.458554419921711e-05\n","Epoch  12 Batch  54 / 228  Training Loss  4.7662470024079084e-05\n","Epoch  12 Batch  55 / 228  Training Loss  3.5665503673953936e-05\n","Epoch  12 Batch  56 / 228  Training Loss  3.9322414522757754e-05\n","Epoch  12 Batch  57 / 228  Training Loss  3.874431786243804e-05\n","Epoch  12 Batch  58 / 228  Training Loss  3.880223448504694e-05\n","Epoch  12 Batch  59 / 228  Training Loss  2.3628326744074002e-05\n","Epoch  12 Batch  60 / 228  Training Loss  5.5885604524519295e-05\n","Epoch  12 Batch  61 / 228  Training Loss  3.448261122684926e-05\n","Epoch  12 Batch  62 / 228  Training Loss  4.6378005208680406e-05\n","Epoch  12 Batch  63 / 228  Training Loss  3.7115285522304475e-05\n","Epoch  12 Batch  64 / 228  Training Loss  5.19890381838195e-05\n","Epoch  12 Batch  65 / 228  Training Loss  3.574806032702327e-05\n","Epoch  12 Batch  66 / 228  Training Loss  3.2768919481895864e-05\n","Epoch  12 Batch  67 / 228  Training Loss  3.6455774534260854e-05\n","Epoch  12 Batch  68 / 228  Training Loss  3.9349655708065256e-05\n","Epoch  12 Batch  69 / 228  Training Loss  3.497283978504129e-05\n","Epoch  12 Batch  70 / 228  Training Loss  3.7492369301617146e-05\n","Epoch  12 Batch  71 / 228  Training Loss  4.4239492126507685e-05\n","Epoch  12 Batch  72 / 228  Training Loss  4.757802889798768e-05\n","Epoch  12 Batch  73 / 228  Training Loss  3.1662882975069806e-05\n","Epoch  12 Batch  74 / 228  Training Loss  2.8588623536052182e-05\n","Epoch  12 Batch  75 / 228  Training Loss  3.536804433679208e-05\n","Epoch  12 Batch  76 / 228  Training Loss  3.865850521833636e-05\n","Epoch  12 Batch  77 / 228  Training Loss  2.7117052013636567e-05\n","Epoch  12 Batch  78 / 228  Training Loss  3.4199772926513106e-05\n","Epoch  12 Batch  79 / 228  Training Loss  4.3022915633628145e-05\n","Epoch  12 Batch  80 / 228  Training Loss  4.365399945527315e-05\n","Epoch  12 Batch  81 / 228  Training Loss  3.3995962439803407e-05\n","Epoch  12 Batch  82 / 228  Training Loss  3.522804399835877e-05\n","Epoch  12 Batch  83 / 228  Training Loss  3.9164213376352564e-05\n","Epoch  12 Batch  84 / 228  Training Loss  4.4114400225225836e-05\n","Epoch  12 Batch  85 / 228  Training Loss  4.253375300322659e-05\n","Epoch  12 Batch  86 / 228  Training Loss  4.6476241550408304e-05\n","Epoch  12 Batch  87 / 228  Training Loss  4.4689404603559524e-05\n","Epoch  12 Batch  88 / 228  Training Loss  3.12027805193793e-05\n","Epoch  12 Batch  89 / 228  Training Loss  3.6947465559933335e-05\n","Epoch  12 Batch  90 / 228  Training Loss  3.8630438211839646e-05\n","Epoch  12 Batch  91 / 228  Training Loss  3.0208326279534958e-05\n","Epoch  12 Batch  92 / 228  Training Loss  2.880909050873015e-05\n","Epoch  12 Batch  93 / 228  Training Loss  4.8987549234880134e-05\n","Epoch  12 Batch  94 / 228  Training Loss  5.0475693569751456e-05\n","Epoch  12 Batch  95 / 228  Training Loss  3.584845762816258e-05\n","Epoch  12 Batch  96 / 228  Training Loss  4.918655031360686e-05\n","Epoch  12 Batch  97 / 228  Training Loss  4.512218220042996e-05\n","Epoch  12 Batch  98 / 228  Training Loss  4.1201517888111994e-05\n","Epoch  12 Batch  99 / 228  Training Loss  4.0691236790735275e-05\n","Epoch  12 Batch  100 / 228  Training Loss  4.216434172121808e-05\n","Epoch  12 Batch  101 / 228  Training Loss  3.98727752326522e-05\n","Epoch  12 Batch  102 / 228  Training Loss  5.241872349870391e-05\n","Epoch  12 Batch  103 / 228  Training Loss  4.567020368995145e-05\n","Epoch  12 Batch  104 / 228  Training Loss  4.7329416702268645e-05\n","Epoch  12 Batch  105 / 228  Training Loss  6.021813169354573e-05\n","Epoch  12 Batch  106 / 228  Training Loss  5.054049688624218e-05\n","Epoch  12 Batch  107 / 228  Training Loss  4.5837743527954444e-05\n","Epoch  12 Batch  108 / 228  Training Loss  3.128276875941083e-05\n","Epoch  12 Batch  109 / 228  Training Loss  3.372460923856124e-05\n","Epoch  12 Batch  110 / 228  Training Loss  3.833921073237434e-05\n","Epoch  12 Batch  111 / 228  Training Loss  4.4787760998588055e-05\n","Epoch  12 Batch  112 / 228  Training Loss  3.7345140299294144e-05\n","Epoch  12 Batch  113 / 228  Training Loss  4.965404514223337e-05\n","Epoch  12 Batch  114 / 228  Training Loss  3.401039430173114e-05\n","Epoch  12 Batch  115 / 228  Training Loss  6.375965313054621e-05\n","Epoch  12 Batch  116 / 228  Training Loss  5.301793135004118e-05\n","Epoch  12 Batch  117 / 228  Training Loss  2.8876098440377973e-05\n","Epoch  12 Batch  118 / 228  Training Loss  2.8740561901940964e-05\n","Epoch  12 Batch  119 / 228  Training Loss  4.128686850890517e-05\n","Epoch  12 Batch  120 / 228  Training Loss  4.078474739799276e-05\n","Epoch  12 Batch  121 / 228  Training Loss  4.709291533799842e-05\n","Epoch  12 Batch  122 / 228  Training Loss  4.834973515244201e-05\n","Epoch  12 Batch  123 / 228  Training Loss  4.370372334960848e-05\n","Epoch  12 Batch  124 / 228  Training Loss  4.8398385843029246e-05\n","Epoch  12 Batch  125 / 228  Training Loss  3.474755794741213e-05\n","Epoch  12 Batch  126 / 228  Training Loss  3.081894465140067e-05\n","Epoch  12 Batch  127 / 228  Training Loss  3.854593524010852e-05\n","Epoch  12 Batch  128 / 228  Training Loss  3.67802131222561e-05\n","Epoch  12 Batch  129 / 228  Training Loss  3.857411502394825e-05\n","Epoch  12 Batch  130 / 228  Training Loss  4.478013579500839e-05\n","Epoch  12 Batch  131 / 228  Training Loss  3.758226375794038e-05\n","Epoch  12 Batch  132 / 228  Training Loss  3.6355711927171797e-05\n","Epoch  12 Batch  133 / 228  Training Loss  4.0591476135887206e-05\n","Epoch  12 Batch  134 / 228  Training Loss  2.3443793907063082e-05\n","Epoch  12 Batch  135 / 228  Training Loss  2.7505378966452554e-05\n","Epoch  12 Batch  136 / 228  Training Loss  4.942081795888953e-05\n","Epoch  12 Batch  137 / 228  Training Loss  5.399772271630354e-05\n","Epoch  12 Batch  138 / 228  Training Loss  3.646976620075293e-05\n","Epoch  12 Batch  139 / 228  Training Loss  2.9412733056233265e-05\n","Epoch  12 Batch  140 / 228  Training Loss  3.842507067020051e-05\n","Epoch  12 Batch  141 / 228  Training Loss  3.063162148464471e-05\n","Epoch  12 Batch  142 / 228  Training Loss  3.0442961360677145e-05\n","Epoch  12 Batch  143 / 228  Training Loss  4.115497722523287e-05\n","Epoch  12 Batch  144 / 228  Training Loss  3.648995698313229e-05\n","Epoch  12 Batch  145 / 228  Training Loss  3.769955219468102e-05\n","Epoch  12 Batch  146 / 228  Training Loss  4.4134365452919155e-05\n","Epoch  12 Batch  147 / 228  Training Loss  3.927595025743358e-05\n","Epoch  12 Batch  148 / 228  Training Loss  4.470236672204919e-05\n","Epoch  12 Batch  149 / 228  Training Loss  4.059303682879545e-05\n","Epoch  12 Batch  150 / 228  Training Loss  3.221893712179735e-05\n","Epoch  12 Batch  151 / 228  Training Loss  3.144698348478414e-05\n","Epoch  12 Batch  152 / 228  Training Loss  5.09078599861823e-05\n","Epoch  12 Batch  153 / 228  Training Loss  4.1948605939978734e-05\n","Epoch  12 Batch  154 / 228  Training Loss  3.694455881486647e-05\n","Epoch  12 Batch  155 / 228  Training Loss  3.746482252608985e-05\n","Epoch  12 Batch  156 / 228  Training Loss  2.9639064450748265e-05\n","Epoch  12 Batch  157 / 228  Training Loss  5.6506174587411806e-05\n","Epoch  12 Batch  158 / 228  Training Loss  3.825706517091021e-05\n","Epoch  12 Batch  159 / 228  Training Loss  3.47841669281479e-05\n","Epoch  12 Batch  160 / 228  Training Loss  3.783594002015889e-05\n","Epoch  12 Batch  161 / 228  Training Loss  1.883463301055599e-05\n","Epoch  12 Batch  162 / 228  Training Loss  3.1458650482818484e-05\n","Epoch  12 Batch  163 / 228  Training Loss  5.3207964811008424e-05\n","Epoch  12 Batch  164 / 228  Training Loss  5.0066912081092596e-05\n","Epoch  12 Batch  165 / 228  Training Loss  3.8926678826101124e-05\n","Epoch  12 Batch  166 / 228  Training Loss  3.880033909808844e-05\n","Epoch  12 Batch  167 / 228  Training Loss  3.3764696127036586e-05\n","Epoch  12 Batch  168 / 228  Training Loss  2.4454246158711612e-05\n","Epoch  12 Batch  169 / 228  Training Loss  5.331202919478528e-05\n","Epoch  12 Batch  170 / 228  Training Loss  4.70626400783658e-05\n","Epoch  12 Batch  171 / 228  Training Loss  3.751819531316869e-05\n","Epoch  12 Batch  172 / 228  Training Loss  3.95227107219398e-05\n","Epoch  12 Batch  173 / 228  Training Loss  4.2039420804940164e-05\n","Epoch  12 Batch  174 / 228  Training Loss  3.535593714332208e-05\n","Epoch  12 Batch  175 / 228  Training Loss  3.6035122320754454e-05\n","Epoch  12 Batch  176 / 228  Training Loss  3.639639908215031e-05\n","Epoch  12 Batch  177 / 228  Training Loss  4.09475396736525e-05\n","Epoch  12 Batch  178 / 228  Training Loss  3.3776381314964965e-05\n","Epoch  12 Batch  179 / 228  Training Loss  3.44304608006496e-05\n","Epoch  12 Batch  180 / 228  Training Loss  4.240117050358094e-05\n","Epoch  12 Batch  181 / 228  Training Loss  4.578114749165252e-05\n","Epoch  12 Batch  182 / 228  Training Loss  3.312637272756547e-05\n","Epoch  12 Batch  183 / 228  Training Loss  4.6061741159064695e-05\n","Epoch  12 Batch  184 / 228  Training Loss  2.942827450169716e-05\n","Epoch  12 Batch  185 / 228  Training Loss  2.9620359782711603e-05\n","Epoch  12 Batch  186 / 228  Training Loss  5.565349056269042e-05\n","Epoch  12 Batch  187 / 228  Training Loss  3.663362076622434e-05\n","Epoch  12 Batch  188 / 228  Training Loss  3.7720245018135756e-05\n","Epoch  12 Batch  189 / 228  Training Loss  4.6085770009085536e-05\n","Epoch  12 Batch  190 / 228  Training Loss  3.576651215553284e-05\n","Epoch  12 Batch  191 / 228  Training Loss  4.095393524039537e-05\n","Epoch  12 Batch  192 / 228  Training Loss  3.933832704205997e-05\n","Epoch  12 Batch  193 / 228  Training Loss  3.812328941421583e-05\n","Epoch  12 Batch  194 / 228  Training Loss  3.9703802030999213e-05\n","Epoch  12 Batch  195 / 228  Training Loss  3.424592432565987e-05\n","Epoch  12 Batch  196 / 228  Training Loss  3.397366526769474e-05\n","Epoch  12 Batch  197 / 228  Training Loss  4.478640767047182e-05\n","Epoch  12 Batch  198 / 228  Training Loss  4.220382834319025e-05\n","Epoch  12 Batch  199 / 228  Training Loss  3.109902172582224e-05\n","Epoch  12 Batch  200 / 228  Training Loss  4.0105915104504675e-05\n","Epoch  12 Batch  201 / 228  Training Loss  2.962163125630468e-05\n","Epoch  12 Batch  202 / 228  Training Loss  2.7303645765641704e-05\n","Epoch  12 Batch  203 / 228  Training Loss  4.1288505599368364e-05\n","Epoch  12 Batch  204 / 228  Training Loss  4.4701962906401604e-05\n","Epoch  12 Batch  205 / 228  Training Loss  3.5860743082594126e-05\n","Epoch  12 Batch  206 / 228  Training Loss  4.379099846119061e-05\n","Epoch  12 Batch  207 / 228  Training Loss  3.8613226934103295e-05\n","Epoch  12 Batch  208 / 228  Training Loss  4.6249977458501235e-05\n","Epoch  12 Batch  209 / 228  Training Loss  4.264352173777297e-05\n","Epoch  12 Batch  210 / 228  Training Loss  5.032512854086235e-05\n","Epoch  12 Batch  211 / 228  Training Loss  3.8082820537965745e-05\n","Epoch  12 Batch  212 / 228  Training Loss  6.708976434310898e-05\n","Epoch  12 Batch  213 / 228  Training Loss  3.6950492358300835e-05\n","Epoch  12 Batch  214 / 228  Training Loss  4.403601633384824e-05\n","Epoch  12 Batch  215 / 228  Training Loss  3.829262641374953e-05\n","Epoch  12 Batch  216 / 228  Training Loss  3.9472433854825795e-05\n","Epoch  12 Batch  217 / 228  Training Loss  4.362757317721844e-05\n","Epoch  12 Batch  218 / 228  Training Loss  3.3083106245612726e-05\n","Epoch  12 Batch  219 / 228  Training Loss  3.855298928101547e-05\n","Epoch  12 Batch  220 / 228  Training Loss  3.991592166130431e-05\n","Epoch  12 Batch  221 / 228  Training Loss  3.422816371312365e-05\n","Epoch  12 Batch  222 / 228  Training Loss  3.562194251571782e-05\n","Epoch  12 Batch  223 / 228  Training Loss  3.41238992405124e-05\n","Epoch  12 Batch  224 / 228  Training Loss  2.826476156769786e-05\n","Epoch  12 Batch  225 / 228  Training Loss  3.962419577874243e-05\n","Epoch  12 Batch  226 / 228  Training Loss  3.914960689144209e-05\n","Epoch  12 Batch  227 / 228  Training Loss  3.7353158404584974e-05\n","  13    |    -    |   0.000040   | 99.542683\n","----------------------------------------------------------------------\n","Running epoch: 13\n","Epoch  13 Batch  0 / 228  Training Loss  5.0236849347129464e-05\n","Epoch  13 Batch  1 / 228  Training Loss  2.3472150132874958e-05\n","Epoch  13 Batch  2 / 228  Training Loss  3.624823148129508e-05\n","Epoch  13 Batch  3 / 228  Training Loss  3.641050716396421e-05\n","Epoch  13 Batch  4 / 228  Training Loss  4.268399061402306e-05\n","Epoch  13 Batch  5 / 228  Training Loss  3.765113069675863e-05\n","Epoch  13 Batch  6 / 228  Training Loss  2.9820632335031405e-05\n","Epoch  13 Batch  7 / 228  Training Loss  3.301401011412963e-05\n","Epoch  13 Batch  8 / 228  Training Loss  3.36407647409942e-05\n","Epoch  13 Batch  9 / 228  Training Loss  3.0172795959515497e-05\n","Epoch  13 Batch  10 / 228  Training Loss  5.061348929302767e-05\n","Epoch  13 Batch  11 / 228  Training Loss  3.199369530193508e-05\n","Epoch  13 Batch  12 / 228  Training Loss  3.934575215680525e-05\n","Epoch  13 Batch  13 / 228  Training Loss  3.0462126233032905e-05\n","Epoch  13 Batch  14 / 228  Training Loss  4.25266262027435e-05\n","Epoch  13 Batch  15 / 228  Training Loss  3.144158108625561e-05\n","Epoch  13 Batch  16 / 228  Training Loss  3.07512964354828e-05\n","Epoch  13 Batch  17 / 228  Training Loss  5.8946858189301565e-05\n","Epoch  13 Batch  18 / 228  Training Loss  4.40539006376639e-05\n","Epoch  13 Batch  19 / 228  Training Loss  4.399719909997657e-05\n","Epoch  13 Batch  20 / 228  Training Loss  3.9596736314706504e-05\n","Epoch  13 Batch  21 / 228  Training Loss  2.722020508372225e-05\n","Epoch  13 Batch  22 / 228  Training Loss  3.1117429898586124e-05\n","Epoch  13 Batch  23 / 228  Training Loss  3.516074502840638e-05\n","Epoch  13 Batch  24 / 228  Training Loss  3.154875957989134e-05\n","Epoch  13 Batch  25 / 228  Training Loss  3.2609441404929385e-05\n","Epoch  13 Batch  26 / 228  Training Loss  2.5314660888398066e-05\n","Epoch  13 Batch  27 / 228  Training Loss  4.373394403955899e-05\n","Epoch  13 Batch  28 / 228  Training Loss  3.3155483833979815e-05\n","Epoch  13 Batch  29 / 228  Training Loss  3.19095779559575e-05\n","Epoch  13 Batch  30 / 228  Training Loss  4.573395199258812e-05\n","Epoch  13 Batch  31 / 228  Training Loss  3.591875793063082e-05\n","Epoch  13 Batch  32 / 228  Training Loss  3.5744331398746e-05\n","Epoch  13 Batch  33 / 228  Training Loss  3.987594027421437e-05\n","Epoch  13 Batch  34 / 228  Training Loss  2.9699698643526062e-05\n","Epoch  13 Batch  35 / 228  Training Loss  3.110749457846396e-05\n","Epoch  13 Batch  36 / 228  Training Loss  3.264850238338113e-05\n","Epoch  13 Batch  37 / 228  Training Loss  3.479676161077805e-05\n","Epoch  13 Batch  38 / 228  Training Loss  4.014980368083343e-05\n","Epoch  13 Batch  39 / 228  Training Loss  3.623001612140797e-05\n","Epoch  13 Batch  40 / 228  Training Loss  3.0165489079081453e-05\n","Epoch  13 Batch  41 / 228  Training Loss  2.7912767109228298e-05\n","Epoch  13 Batch  42 / 228  Training Loss  3.570175977074541e-05\n","Epoch  13 Batch  43 / 228  Training Loss  3.1745519663672894e-05\n","Epoch  13 Batch  44 / 228  Training Loss  3.3219599572476e-05\n","Epoch  13 Batch  45 / 228  Training Loss  2.9698818252654746e-05\n","Epoch  13 Batch  46 / 228  Training Loss  3.428996569709852e-05\n","Epoch  13 Batch  47 / 228  Training Loss  3.2805070077301934e-05\n","Epoch  13 Batch  48 / 228  Training Loss  3.36888842866756e-05\n","Epoch  13 Batch  49 / 228  Training Loss  4.9463829782325774e-05\n","Epoch  13 Batch  50 / 228  Training Loss  2.8237554943189025e-05\n","Epoch  13 Batch  51 / 228  Training Loss  3.053051477763802e-05\n","Epoch  13 Batch  52 / 228  Training Loss  2.6927722501568496e-05\n","Epoch  13 Batch  53 / 228  Training Loss  3.306766302557662e-05\n","Epoch  13 Batch  54 / 228  Training Loss  3.818541154032573e-05\n","Epoch  13 Batch  55 / 228  Training Loss  3.2889154681470245e-05\n","Epoch  13 Batch  56 / 228  Training Loss  3.910898885806091e-05\n","Epoch  13 Batch  57 / 228  Training Loss  3.819548146566376e-05\n","Epoch  13 Batch  58 / 228  Training Loss  3.3271116990363225e-05\n","Epoch  13 Batch  59 / 228  Training Loss  2.933770520030521e-05\n","Epoch  13 Batch  60 / 228  Training Loss  3.54549047187902e-05\n","Epoch  13 Batch  61 / 228  Training Loss  3.4111861168639734e-05\n","Epoch  13 Batch  62 / 228  Training Loss  3.08864182443358e-05\n","Epoch  13 Batch  63 / 228  Training Loss  2.2538664779858664e-05\n","Epoch  13 Batch  64 / 228  Training Loss  3.988034586654976e-05\n","Epoch  13 Batch  65 / 228  Training Loss  2.475496512488462e-05\n","Epoch  13 Batch  66 / 228  Training Loss  3.5358381865080446e-05\n","Epoch  13 Batch  67 / 228  Training Loss  2.8812046366510913e-05\n","Epoch  13 Batch  68 / 228  Training Loss  4.2964071326423436e-05\n","Epoch  13 Batch  69 / 228  Training Loss  5.0826947699533775e-05\n","Epoch  13 Batch  70 / 228  Training Loss  4.45761761511676e-05\n","Epoch  13 Batch  71 / 228  Training Loss  3.387296601431444e-05\n","Epoch  13 Batch  72 / 228  Training Loss  3.462576205492951e-05\n","Epoch  13 Batch  73 / 228  Training Loss  4.9912516260519624e-05\n","Epoch  13 Batch  74 / 228  Training Loss  3.9517879486083984e-05\n","Epoch  13 Batch  75 / 228  Training Loss  2.806865086313337e-05\n","Epoch  13 Batch  76 / 228  Training Loss  3.249549263273366e-05\n","Epoch  13 Batch  77 / 228  Training Loss  2.3693195544183254e-05\n","Epoch  13 Batch  78 / 228  Training Loss  3.534552888595499e-05\n","Epoch  13 Batch  79 / 228  Training Loss  3.2384330552304164e-05\n","Epoch  13 Batch  80 / 228  Training Loss  2.832810059771873e-05\n","Epoch  13 Batch  81 / 228  Training Loss  2.8577473131008446e-05\n","Epoch  13 Batch  82 / 228  Training Loss  2.7224163204664364e-05\n","Epoch  13 Batch  83 / 228  Training Loss  3.27561137964949e-05\n","Epoch  13 Batch  84 / 228  Training Loss  3.221639053663239e-05\n","Epoch  13 Batch  85 / 228  Training Loss  3.155823651468381e-05\n","Epoch  13 Batch  86 / 228  Training Loss  3.175709207425825e-05\n","Epoch  13 Batch  87 / 228  Training Loss  3.282266698079184e-05\n","Epoch  13 Batch  88 / 228  Training Loss  2.8373082386679016e-05\n","Epoch  13 Batch  89 / 228  Training Loss  3.3837965020211414e-05\n","Epoch  13 Batch  90 / 228  Training Loss  5.295029404805973e-05\n","Epoch  13 Batch  91 / 228  Training Loss  4.77761568618007e-05\n","Epoch  13 Batch  92 / 228  Training Loss  4.0569248085375875e-05\n","Epoch  13 Batch  93 / 228  Training Loss  3.614582237787545e-05\n","Epoch  13 Batch  94 / 228  Training Loss  2.80052081507165e-05\n","Epoch  13 Batch  95 / 228  Training Loss  3.1152165320236236e-05\n","Epoch  13 Batch  96 / 228  Training Loss  3.538890086929314e-05\n","Epoch  13 Batch  97 / 228  Training Loss  2.787734592857305e-05\n","Epoch  13 Batch  98 / 228  Training Loss  3.650528742582537e-05\n","Epoch  13 Batch  99 / 228  Training Loss  3.370649574208073e-05\n","Epoch  13 Batch  100 / 228  Training Loss  4.892943979939446e-05\n","Epoch  13 Batch  101 / 228  Training Loss  3.087212462560274e-05\n","Epoch  13 Batch  102 / 228  Training Loss  3.031210508197546e-05\n","Epoch  13 Batch  103 / 228  Training Loss  2.136412149411626e-05\n","Epoch  13 Batch  104 / 228  Training Loss  4.273287413525395e-05\n","Epoch  13 Batch  105 / 228  Training Loss  3.069610465900041e-05\n","Epoch  13 Batch  106 / 228  Training Loss  3.3440715924371034e-05\n","Epoch  13 Batch  107 / 228  Training Loss  2.7639707695925608e-05\n","Epoch  13 Batch  108 / 228  Training Loss  3.40298174705822e-05\n","Epoch  13 Batch  109 / 228  Training Loss  3.3485346648376435e-05\n","Epoch  13 Batch  110 / 228  Training Loss  2.7223301003687084e-05\n","Epoch  13 Batch  111 / 228  Training Loss  3.6365108826430514e-05\n","Epoch  13 Batch  112 / 228  Training Loss  4.4442287617130205e-05\n","Epoch  13 Batch  113 / 228  Training Loss  3.3379044907633215e-05\n","Epoch  13 Batch  114 / 228  Training Loss  3.528541492414661e-05\n","Epoch  13 Batch  115 / 228  Training Loss  3.036282396351453e-05\n","Epoch  13 Batch  116 / 228  Training Loss  2.865503483917564e-05\n","Epoch  13 Batch  117 / 228  Training Loss  4.610633914126083e-05\n","Epoch  13 Batch  118 / 228  Training Loss  3.280392775195651e-05\n","Epoch  13 Batch  119 / 228  Training Loss  2.8014983399771154e-05\n","Epoch  13 Batch  120 / 228  Training Loss  4.0871978853829205e-05\n","Epoch  13 Batch  121 / 228  Training Loss  4.109977453481406e-05\n","Epoch  13 Batch  122 / 228  Training Loss  6.148743705125526e-05\n","Epoch  13 Batch  123 / 228  Training Loss  4.101115700905211e-05\n","Epoch  13 Batch  124 / 228  Training Loss  4.6143111831042916e-05\n","Epoch  13 Batch  125 / 228  Training Loss  2.3525781216449104e-05\n","Epoch  13 Batch  126 / 228  Training Loss  3.408072007005103e-05\n","Epoch  13 Batch  127 / 228  Training Loss  2.6238853024551645e-05\n","Epoch  13 Batch  128 / 228  Training Loss  3.137784369755536e-05\n","Epoch  13 Batch  129 / 228  Training Loss  4.033836739836261e-05\n","Epoch  13 Batch  130 / 228  Training Loss  2.939105615951121e-05\n","Epoch  13 Batch  131 / 228  Training Loss  2.904050961660687e-05\n","Epoch  13 Batch  132 / 228  Training Loss  2.422208126517944e-05\n","Epoch  13 Batch  133 / 228  Training Loss  2.7228359613218345e-05\n","Epoch  13 Batch  134 / 228  Training Loss  3.1147199479164556e-05\n","Epoch  13 Batch  135 / 228  Training Loss  2.884070636355318e-05\n","Epoch  13 Batch  136 / 228  Training Loss  3.3263149816775694e-05\n","Epoch  13 Batch  137 / 228  Training Loss  2.7982265237369575e-05\n","Epoch  13 Batch  138 / 228  Training Loss  4.238735346007161e-05\n","Epoch  13 Batch  139 / 228  Training Loss  2.9124837965355255e-05\n","Epoch  13 Batch  140 / 228  Training Loss  3.8246776966843754e-05\n","Epoch  13 Batch  141 / 228  Training Loss  5.11702019139193e-05\n","Epoch  13 Batch  142 / 228  Training Loss  3.415466926526278e-05\n","Epoch  13 Batch  143 / 228  Training Loss  2.590644362499006e-05\n","Epoch  13 Batch  144 / 228  Training Loss  2.9560746042989194e-05\n","Epoch  13 Batch  145 / 228  Training Loss  3.29842405335512e-05\n","Epoch  13 Batch  146 / 228  Training Loss  2.719910662563052e-05\n","Epoch  13 Batch  147 / 228  Training Loss  2.9254753826535307e-05\n","Epoch  13 Batch  148 / 228  Training Loss  3.428072886890732e-05\n","Epoch  13 Batch  149 / 228  Training Loss  3.4236498322570696e-05\n","Epoch  13 Batch  150 / 228  Training Loss  2.1776802896056324e-05\n","Epoch  13 Batch  151 / 228  Training Loss  3.315204958198592e-05\n","Epoch  13 Batch  152 / 228  Training Loss  2.88859100692207e-05\n","Epoch  13 Batch  153 / 228  Training Loss  4.1406412492506206e-05\n","Epoch  13 Batch  154 / 228  Training Loss  2.8275147997192107e-05\n","Epoch  13 Batch  155 / 228  Training Loss  2.541878529882524e-05\n","Epoch  13 Batch  156 / 228  Training Loss  4.0294213249580935e-05\n","Epoch  13 Batch  157 / 228  Training Loss  3.9559810829814523e-05\n","Epoch  13 Batch  158 / 228  Training Loss  2.70285272563342e-05\n","Epoch  13 Batch  159 / 228  Training Loss  2.6107107260031626e-05\n","Epoch  13 Batch  160 / 228  Training Loss  2.4711718651815318e-05\n","Epoch  13 Batch  161 / 228  Training Loss  2.7977290301350877e-05\n","Epoch  13 Batch  162 / 228  Training Loss  3.5158165701432154e-05\n","Epoch  13 Batch  163 / 228  Training Loss  3.4987977414857596e-05\n","Epoch  13 Batch  164 / 228  Training Loss  2.6025449187727645e-05\n","Epoch  13 Batch  165 / 228  Training Loss  3.8316728023346514e-05\n","Epoch  13 Batch  166 / 228  Training Loss  2.989323365909513e-05\n","Epoch  13 Batch  167 / 228  Training Loss  3.145216032862663e-05\n","Epoch  13 Batch  168 / 228  Training Loss  2.9096674552420154e-05\n","Epoch  13 Batch  169 / 228  Training Loss  3.121250483673066e-05\n","Epoch  13 Batch  170 / 228  Training Loss  3.146130984532647e-05\n","Epoch  13 Batch  171 / 228  Training Loss  4.488205740926787e-05\n","Epoch  13 Batch  172 / 228  Training Loss  2.9820328563801013e-05\n","Epoch  13 Batch  173 / 228  Training Loss  3.163325527566485e-05\n","Epoch  13 Batch  174 / 228  Training Loss  3.6931076465407386e-05\n","Epoch  13 Batch  175 / 228  Training Loss  2.4104820113279857e-05\n","Epoch  13 Batch  176 / 228  Training Loss  2.163450153602753e-05\n","Epoch  13 Batch  177 / 228  Training Loss  3.511814065859653e-05\n","Epoch  13 Batch  178 / 228  Training Loss  2.6152967620873824e-05\n","Epoch  13 Batch  179 / 228  Training Loss  2.9563865609816276e-05\n","Epoch  13 Batch  180 / 228  Training Loss  2.8080758056603372e-05\n","Epoch  13 Batch  181 / 228  Training Loss  2.961662903544493e-05\n","Epoch  13 Batch  182 / 228  Training Loss  3.049847509828396e-05\n","Epoch  13 Batch  183 / 228  Training Loss  3.58127654180862e-05\n","Epoch  13 Batch  184 / 228  Training Loss  4.90874626848381e-05\n","Epoch  13 Batch  185 / 228  Training Loss  3.515062417136505e-05\n","Epoch  13 Batch  186 / 228  Training Loss  4.354811244411394e-05\n","Epoch  13 Batch  187 / 228  Training Loss  3.440511500230059e-05\n","Epoch  13 Batch  188 / 228  Training Loss  2.4883800506358966e-05\n","Epoch  13 Batch  189 / 228  Training Loss  5.2000057621626183e-05\n","Epoch  13 Batch  190 / 228  Training Loss  3.13701675622724e-05\n","Epoch  13 Batch  191 / 228  Training Loss  4.0647697460372e-05\n","Epoch  13 Batch  192 / 228  Training Loss  3.5918899811804295e-05\n","Epoch  13 Batch  193 / 228  Training Loss  3.619388007791713e-05\n","Epoch  13 Batch  194 / 228  Training Loss  3.533011840772815e-05\n","Epoch  13 Batch  195 / 228  Training Loss  3.4026299545075744e-05\n","Epoch  13 Batch  196 / 228  Training Loss  4.0531671402277425e-05\n","Epoch  13 Batch  197 / 228  Training Loss  2.878501618397422e-05\n","Epoch  13 Batch  198 / 228  Training Loss  2.9141603590687737e-05\n","Epoch  13 Batch  199 / 228  Training Loss  2.455025060044136e-05\n","Epoch  13 Batch  200 / 228  Training Loss  2.8717058739857748e-05\n","Epoch  13 Batch  201 / 228  Training Loss  3.496241333778016e-05\n","Epoch  13 Batch  202 / 228  Training Loss  3.714209015015513e-05\n","Epoch  13 Batch  203 / 228  Training Loss  2.708495048864279e-05\n","Epoch  13 Batch  204 / 228  Training Loss  3.3274693123530596e-05\n","Epoch  13 Batch  205 / 228  Training Loss  3.9831655158195645e-05\n","Epoch  13 Batch  206 / 228  Training Loss  3.9790713344700634e-05\n","Epoch  13 Batch  207 / 228  Training Loss  2.9829247068846598e-05\n","Epoch  13 Batch  208 / 228  Training Loss  3.5642762668430805e-05\n","Epoch  13 Batch  209 / 228  Training Loss  3.3732263545971364e-05\n","Epoch  13 Batch  210 / 228  Training Loss  3.526254295138642e-05\n","Epoch  13 Batch  211 / 228  Training Loss  2.920312363130506e-05\n","Epoch  13 Batch  212 / 228  Training Loss  4.560099478112534e-05\n","Epoch  13 Batch  213 / 228  Training Loss  2.9793704015901312e-05\n","Epoch  13 Batch  214 / 228  Training Loss  3.755982470465824e-05\n","Epoch  13 Batch  215 / 228  Training Loss  2.2580592485610396e-05\n","Epoch  13 Batch  216 / 228  Training Loss  2.466777732479386e-05\n","Epoch  13 Batch  217 / 228  Training Loss  3.2272360840579495e-05\n","Epoch  13 Batch  218 / 228  Training Loss  3.318360177217983e-05\n","Epoch  13 Batch  219 / 228  Training Loss  3.256576746935025e-05\n","Epoch  13 Batch  220 / 228  Training Loss  2.4241278879344463e-05\n","Epoch  13 Batch  221 / 228  Training Loss  2.3336524463957176e-05\n","Epoch  13 Batch  222 / 228  Training Loss  2.8296137315919623e-05\n","Epoch  13 Batch  223 / 228  Training Loss  2.9644657843164168e-05\n","Epoch  13 Batch  224 / 228  Training Loss  3.468633440206759e-05\n","Epoch  13 Batch  225 / 228  Training Loss  3.404229209991172e-05\n","Epoch  13 Batch  226 / 228  Training Loss  3.30549810314551e-05\n","Epoch  13 Batch  227 / 228  Training Loss  5.413269900600426e-05\n","  14    |    -    |   0.000034   | 99.580793\n","----------------------------------------------------------------------\n","Running epoch: 14\n","Epoch  14 Batch  0 / 228  Training Loss  2.183783180953469e-05\n","Epoch  14 Batch  1 / 228  Training Loss  3.464539986453019e-05\n","Epoch  14 Batch  2 / 228  Training Loss  3.0816525395493954e-05\n","Epoch  14 Batch  3 / 228  Training Loss  3.7036203138995916e-05\n","Epoch  14 Batch  4 / 228  Training Loss  2.3135775336413644e-05\n","Epoch  14 Batch  5 / 228  Training Loss  3.095019928878173e-05\n","Epoch  14 Batch  6 / 228  Training Loss  2.6100888135260902e-05\n","Epoch  14 Batch  7 / 228  Training Loss  2.716009839787148e-05\n","Epoch  14 Batch  8 / 228  Training Loss  3.910614759661257e-05\n","Epoch  14 Batch  9 / 228  Training Loss  2.397170101176016e-05\n","Epoch  14 Batch  10 / 228  Training Loss  2.7266290999250486e-05\n","Epoch  14 Batch  11 / 228  Training Loss  2.967539148812648e-05\n","Epoch  14 Batch  12 / 228  Training Loss  3.4429278457537293e-05\n","Epoch  14 Batch  13 / 228  Training Loss  2.865316128008999e-05\n","Epoch  14 Batch  14 / 228  Training Loss  3.3026277378667146e-05\n","Epoch  14 Batch  15 / 228  Training Loss  3.841144643956795e-05\n","Epoch  14 Batch  16 / 228  Training Loss  2.9561899282271042e-05\n","Epoch  14 Batch  17 / 228  Training Loss  2.814582512655761e-05\n","Epoch  14 Batch  18 / 228  Training Loss  3.580295742722228e-05\n","Epoch  14 Batch  19 / 228  Training Loss  2.5881285182549618e-05\n","Epoch  14 Batch  20 / 228  Training Loss  2.7608417440205812e-05\n","Epoch  14 Batch  21 / 228  Training Loss  3.2490755984326825e-05\n","Epoch  14 Batch  22 / 228  Training Loss  2.8664351702900603e-05\n","Epoch  14 Batch  23 / 228  Training Loss  2.631089046190027e-05\n","Epoch  14 Batch  24 / 228  Training Loss  2.5785961042856798e-05\n","Epoch  14 Batch  25 / 228  Training Loss  3.238658973714337e-05\n","Epoch  14 Batch  26 / 228  Training Loss  4.760033334605396e-05\n","Epoch  14 Batch  27 / 228  Training Loss  3.769425529753789e-05\n","Epoch  14 Batch  28 / 228  Training Loss  1.988184521906078e-05\n","Epoch  14 Batch  29 / 228  Training Loss  3.929023660020903e-05\n","Epoch  14 Batch  30 / 228  Training Loss  2.9348204407142475e-05\n","Epoch  14 Batch  31 / 228  Training Loss  3.191592622897588e-05\n","Epoch  14 Batch  32 / 228  Training Loss  2.219376983703114e-05\n","Epoch  14 Batch  33 / 228  Training Loss  3.693003236548975e-05\n","Epoch  14 Batch  34 / 228  Training Loss  4.046590402140282e-05\n","Epoch  14 Batch  35 / 228  Training Loss  4.210434417473152e-05\n","Epoch  14 Batch  36 / 228  Training Loss  2.8980712158954702e-05\n","Epoch  14 Batch  37 / 228  Training Loss  2.3275377316167578e-05\n","Epoch  14 Batch  38 / 228  Training Loss  3.5643424780573696e-05\n","Epoch  14 Batch  39 / 228  Training Loss  2.3877579224063084e-05\n","Epoch  14 Batch  40 / 228  Training Loss  3.0657534807687625e-05\n","Epoch  14 Batch  41 / 228  Training Loss  3.3080679713748395e-05\n","Epoch  14 Batch  42 / 228  Training Loss  2.347055487916805e-05\n","Epoch  14 Batch  43 / 228  Training Loss  2.2680163965560496e-05\n","Epoch  14 Batch  44 / 228  Training Loss  3.5354929423192516e-05\n","Epoch  14 Batch  45 / 228  Training Loss  2.670725189091172e-05\n","Epoch  14 Batch  46 / 228  Training Loss  3.598932744353078e-05\n","Epoch  14 Batch  47 / 228  Training Loss  2.7498526833369397e-05\n","Epoch  14 Batch  48 / 228  Training Loss  2.620658415253274e-05\n","Epoch  14 Batch  49 / 228  Training Loss  3.2069714507088065e-05\n","Epoch  14 Batch  50 / 228  Training Loss  2.5637593353167176e-05\n","Epoch  14 Batch  51 / 228  Training Loss  4.205647564958781e-05\n","Epoch  14 Batch  52 / 228  Training Loss  2.9139549951651134e-05\n","Epoch  14 Batch  53 / 228  Training Loss  2.8380993171595037e-05\n","Epoch  14 Batch  54 / 228  Training Loss  2.9423690648400225e-05\n","Epoch  14 Batch  55 / 228  Training Loss  2.9819628252880648e-05\n","Epoch  14 Batch  56 / 228  Training Loss  2.6287427317583933e-05\n","Epoch  14 Batch  57 / 228  Training Loss  3.4427794162184e-05\n","Epoch  14 Batch  58 / 228  Training Loss  3.9153303077910095e-05\n","Epoch  14 Batch  59 / 228  Training Loss  3.4620225051185116e-05\n","Epoch  14 Batch  60 / 228  Training Loss  3.71329952031374e-05\n","Epoch  14 Batch  61 / 228  Training Loss  1.750090450514108e-05\n","Epoch  14 Batch  62 / 228  Training Loss  2.8472688427427784e-05\n","Epoch  14 Batch  63 / 228  Training Loss  2.6410145437694155e-05\n","Epoch  14 Batch  64 / 228  Training Loss  3.313697379780933e-05\n","Epoch  14 Batch  65 / 228  Training Loss  2.2552489099325612e-05\n","Epoch  14 Batch  66 / 228  Training Loss  2.4554148694733158e-05\n","Epoch  14 Batch  67 / 228  Training Loss  3.367563840583898e-05\n","Epoch  14 Batch  68 / 228  Training Loss  2.1047064365120605e-05\n","Epoch  14 Batch  69 / 228  Training Loss  3.160729829687625e-05\n","Epoch  14 Batch  70 / 228  Training Loss  2.9687187634408474e-05\n","Epoch  14 Batch  71 / 228  Training Loss  2.6881132725975476e-05\n","Epoch  14 Batch  72 / 228  Training Loss  3.882453529513441e-05\n","Epoch  14 Batch  73 / 228  Training Loss  2.5131559596047737e-05\n","Epoch  14 Batch  74 / 228  Training Loss  2.040798972302582e-05\n","Epoch  14 Batch  75 / 228  Training Loss  3.0768289434490725e-05\n","Epoch  14 Batch  76 / 228  Training Loss  3.0662300559924915e-05\n","Epoch  14 Batch  77 / 228  Training Loss  2.8108752303523943e-05\n","Epoch  14 Batch  78 / 228  Training Loss  3.695216219057329e-05\n","Epoch  14 Batch  79 / 228  Training Loss  3.00060237350408e-05\n","Epoch  14 Batch  80 / 228  Training Loss  3.2020128855947405e-05\n","Epoch  14 Batch  81 / 228  Training Loss  2.6497888029552996e-05\n","Epoch  14 Batch  82 / 228  Training Loss  3.604921221267432e-05\n","Epoch  14 Batch  83 / 228  Training Loss  2.7996178687317297e-05\n","Epoch  14 Batch  84 / 228  Training Loss  2.8515301892184652e-05\n","Epoch  14 Batch  85 / 228  Training Loss  3.335123983561061e-05\n","Epoch  14 Batch  86 / 228  Training Loss  2.7244974262430333e-05\n","Epoch  14 Batch  87 / 228  Training Loss  3.549032408045605e-05\n","Epoch  14 Batch  88 / 228  Training Loss  3.1293657229980454e-05\n","Epoch  14 Batch  89 / 228  Training Loss  3.1996325560612604e-05\n","Epoch  14 Batch  90 / 228  Training Loss  3.517549339449033e-05\n","Epoch  14 Batch  91 / 228  Training Loss  2.8325393941486254e-05\n","Epoch  14 Batch  92 / 228  Training Loss  2.825232149916701e-05\n","Epoch  14 Batch  93 / 228  Training Loss  2.954697993118316e-05\n","Epoch  14 Batch  94 / 228  Training Loss  2.982424666697625e-05\n","Epoch  14 Batch  95 / 228  Training Loss  3.351029954501428e-05\n","Epoch  14 Batch  96 / 228  Training Loss  2.5381687009939924e-05\n","Epoch  14 Batch  97 / 228  Training Loss  3.271412060712464e-05\n","Epoch  14 Batch  98 / 228  Training Loss  3.204605309292674e-05\n","Epoch  14 Batch  99 / 228  Training Loss  1.9555833205231465e-05\n","Epoch  14 Batch  100 / 228  Training Loss  2.760807728918735e-05\n","Epoch  14 Batch  101 / 228  Training Loss  2.7469466658658348e-05\n","Epoch  14 Batch  102 / 228  Training Loss  3.0096411137492396e-05\n","Epoch  14 Batch  103 / 228  Training Loss  2.331687937839888e-05\n","Epoch  14 Batch  104 / 228  Training Loss  3.8186968595255166e-05\n","Epoch  14 Batch  105 / 228  Training Loss  2.6197425540885888e-05\n","Epoch  14 Batch  106 / 228  Training Loss  2.6820978746400215e-05\n","Epoch  14 Batch  107 / 228  Training Loss  3.2028423447627574e-05\n","Epoch  14 Batch  108 / 228  Training Loss  3.306564758531749e-05\n","Epoch  14 Batch  109 / 228  Training Loss  4.2245192162226886e-05\n","Epoch  14 Batch  110 / 228  Training Loss  3.358945104992017e-05\n","Epoch  14 Batch  111 / 228  Training Loss  4.149576125200838e-05\n","Epoch  14 Batch  112 / 228  Training Loss  3.102336631854996e-05\n","Epoch  14 Batch  113 / 228  Training Loss  3.0411416446440853e-05\n","Epoch  14 Batch  114 / 228  Training Loss  2.882745502574835e-05\n","Epoch  14 Batch  115 / 228  Training Loss  2.7776803108281456e-05\n","Epoch  14 Batch  116 / 228  Training Loss  3.0277728001237847e-05\n","Epoch  14 Batch  117 / 228  Training Loss  2.8626576749957167e-05\n","Epoch  14 Batch  118 / 228  Training Loss  1.4924537026672624e-05\n","Epoch  14 Batch  119 / 228  Training Loss  2.892799057008233e-05\n","Epoch  14 Batch  120 / 228  Training Loss  2.625512752274517e-05\n","Epoch  14 Batch  121 / 228  Training Loss  3.769355680560693e-05\n","Epoch  14 Batch  122 / 228  Training Loss  2.868552110157907e-05\n","Epoch  14 Batch  123 / 228  Training Loss  2.876796861528419e-05\n","Epoch  14 Batch  124 / 228  Training Loss  2.1327701688278466e-05\n","Epoch  14 Batch  125 / 228  Training Loss  2.53498692472931e-05\n","Epoch  14 Batch  126 / 228  Training Loss  2.51724268309772e-05\n","Epoch  14 Batch  127 / 228  Training Loss  2.769904131127987e-05\n","Epoch  14 Batch  128 / 228  Training Loss  3.17400335916318e-05\n","Epoch  14 Batch  129 / 228  Training Loss  3.3765449188649654e-05\n","Epoch  14 Batch  130 / 228  Training Loss  2.7949305149377324e-05\n","Epoch  14 Batch  131 / 228  Training Loss  3.1102641514735296e-05\n","Epoch  14 Batch  132 / 228  Training Loss  2.8999149435549043e-05\n","Epoch  14 Batch  133 / 228  Training Loss  3.224738611606881e-05\n","Epoch  14 Batch  134 / 228  Training Loss  3.220628423150629e-05\n","Epoch  14 Batch  135 / 228  Training Loss  3.300283424323425e-05\n","Epoch  14 Batch  136 / 228  Training Loss  3.166008900734596e-05\n","Epoch  14 Batch  137 / 228  Training Loss  2.373079041717574e-05\n","Epoch  14 Batch  138 / 228  Training Loss  2.61861459875945e-05\n","Epoch  14 Batch  139 / 228  Training Loss  3.480870509520173e-05\n","Epoch  14 Batch  140 / 228  Training Loss  2.5059267500182614e-05\n","Epoch  14 Batch  141 / 228  Training Loss  2.6024925318779424e-05\n","Epoch  14 Batch  142 / 228  Training Loss  2.4524666514480487e-05\n","Epoch  14 Batch  143 / 228  Training Loss  3.619946801336482e-05\n","Epoch  14 Batch  144 / 228  Training Loss  2.5164388716802932e-05\n","Epoch  14 Batch  145 / 228  Training Loss  2.453328670526389e-05\n","Epoch  14 Batch  146 / 228  Training Loss  3.9200687751872465e-05\n","Epoch  14 Batch  147 / 228  Training Loss  3.1945375667419285e-05\n","Epoch  14 Batch  148 / 228  Training Loss  4.345833440311253e-05\n","Epoch  14 Batch  149 / 228  Training Loss  3.0013306968612596e-05\n","Epoch  14 Batch  150 / 228  Training Loss  2.8746862881234847e-05\n","Epoch  14 Batch  151 / 228  Training Loss  2.721041710174177e-05\n","Epoch  14 Batch  152 / 228  Training Loss  3.954101339331828e-05\n","Epoch  14 Batch  153 / 228  Training Loss  4.0516348235541955e-05\n","Epoch  14 Batch  154 / 228  Training Loss  2.9050792363705114e-05\n","Epoch  14 Batch  155 / 228  Training Loss  2.8950535124749877e-05\n","Epoch  14 Batch  156 / 228  Training Loss  3.4585827961564064e-05\n","Epoch  14 Batch  157 / 228  Training Loss  2.7638347091851756e-05\n","Epoch  14 Batch  158 / 228  Training Loss  2.7727790438802913e-05\n","Epoch  14 Batch  159 / 228  Training Loss  3.250277222832665e-05\n","Epoch  14 Batch  160 / 228  Training Loss  2.4444485461572185e-05\n","Epoch  14 Batch  161 / 228  Training Loss  2.566124385339208e-05\n","Epoch  14 Batch  162 / 228  Training Loss  3.5127181035932153e-05\n","Epoch  14 Batch  163 / 228  Training Loss  2.4552393369958736e-05\n","Epoch  14 Batch  164 / 228  Training Loss  2.9303046176210046e-05\n","Epoch  14 Batch  165 / 228  Training Loss  2.5860261303023435e-05\n","Epoch  14 Batch  166 / 228  Training Loss  3.347112942719832e-05\n","Epoch  14 Batch  167 / 228  Training Loss  2.3288370357477106e-05\n","Epoch  14 Batch  168 / 228  Training Loss  2.853637488442473e-05\n","Epoch  14 Batch  169 / 228  Training Loss  3.1085830414667726e-05\n","Epoch  14 Batch  170 / 228  Training Loss  3.511761678964831e-05\n","Epoch  14 Batch  171 / 228  Training Loss  3.127942909486592e-05\n","Epoch  14 Batch  172 / 228  Training Loss  2.470713297952898e-05\n","Epoch  14 Batch  173 / 228  Training Loss  2.5064582587219775e-05\n","Epoch  14 Batch  174 / 228  Training Loss  2.3001495719654486e-05\n","Epoch  14 Batch  175 / 228  Training Loss  3.286044375272468e-05\n","Epoch  14 Batch  176 / 228  Training Loss  3.05163466691738e-05\n","Epoch  14 Batch  177 / 228  Training Loss  3.375468440935947e-05\n","Epoch  14 Batch  178 / 228  Training Loss  3.3355376217514277e-05\n","Epoch  14 Batch  179 / 228  Training Loss  3.070534876314923e-05\n","Epoch  14 Batch  180 / 228  Training Loss  2.1058269339846447e-05\n","Epoch  14 Batch  181 / 228  Training Loss  2.8440821552067064e-05\n","Epoch  14 Batch  182 / 228  Training Loss  2.7346823117113672e-05\n","Epoch  14 Batch  183 / 228  Training Loss  3.219713835278526e-05\n","Epoch  14 Batch  184 / 228  Training Loss  3.291042594355531e-05\n","Epoch  14 Batch  185 / 228  Training Loss  2.9731259928666987e-05\n","Epoch  14 Batch  186 / 228  Training Loss  2.5064417059184052e-05\n","Epoch  14 Batch  187 / 228  Training Loss  2.4602075427537784e-05\n","Epoch  14 Batch  188 / 228  Training Loss  2.4113562176353298e-05\n","Epoch  14 Batch  189 / 228  Training Loss  2.120774661307223e-05\n","Epoch  14 Batch  190 / 228  Training Loss  2.864389171008952e-05\n","Epoch  14 Batch  191 / 228  Training Loss  2.4676442990312353e-05\n","Epoch  14 Batch  192 / 228  Training Loss  2.867224429792259e-05\n","Epoch  14 Batch  193 / 228  Training Loss  2.154804315068759e-05\n","Epoch  14 Batch  194 / 228  Training Loss  2.8114862288930453e-05\n","Epoch  14 Batch  195 / 228  Training Loss  3.1387775379698724e-05\n","Epoch  14 Batch  196 / 228  Training Loss  2.8982714866288006e-05\n","Epoch  14 Batch  197 / 228  Training Loss  3.763544009416364e-05\n","Epoch  14 Batch  198 / 228  Training Loss  2.4108483557938598e-05\n","Epoch  14 Batch  199 / 228  Training Loss  3.940588067052886e-05\n","Epoch  14 Batch  200 / 228  Training Loss  2.3104392312234268e-05\n","Epoch  14 Batch  201 / 228  Training Loss  2.6039560907520354e-05\n","Epoch  14 Batch  202 / 228  Training Loss  2.6065928977914155e-05\n","Epoch  14 Batch  203 / 228  Training Loss  2.0203753592795692e-05\n","Epoch  14 Batch  204 / 228  Training Loss  2.695368675631471e-05\n","Epoch  14 Batch  205 / 228  Training Loss  2.7256441171630286e-05\n","Epoch  14 Batch  206 / 228  Training Loss  3.432279481785372e-05\n","Epoch  14 Batch  207 / 228  Training Loss  2.8201257009641267e-05\n","Epoch  14 Batch  208 / 228  Training Loss  3.0420482289628126e-05\n","Epoch  14 Batch  209 / 228  Training Loss  3.1212359317578375e-05\n","Epoch  14 Batch  210 / 228  Training Loss  2.8498092433437705e-05\n","Epoch  14 Batch  211 / 228  Training Loss  2.5056991944438778e-05\n","Epoch  14 Batch  212 / 228  Training Loss  2.9126658773748204e-05\n","Epoch  14 Batch  213 / 228  Training Loss  2.6465137125342153e-05\n","Epoch  14 Batch  214 / 228  Training Loss  2.594167563074734e-05\n","Epoch  14 Batch  215 / 228  Training Loss  2.3857934138504788e-05\n","Epoch  14 Batch  216 / 228  Training Loss  4.0588311094325036e-05\n","Epoch  14 Batch  217 / 228  Training Loss  2.8608683351194486e-05\n","Epoch  14 Batch  218 / 228  Training Loss  2.313483491889201e-05\n","Epoch  14 Batch  219 / 228  Training Loss  2.47802963713184e-05\n","Epoch  14 Batch  220 / 228  Training Loss  1.7791471691452898e-05\n","Epoch  14 Batch  221 / 228  Training Loss  2.048687201749999e-05\n","Epoch  14 Batch  222 / 228  Training Loss  3.1851355743128806e-05\n","Epoch  14 Batch  223 / 228  Training Loss  2.7139996745972894e-05\n","Epoch  14 Batch  224 / 228  Training Loss  4.0038605220615864e-05\n","Epoch  14 Batch  225 / 228  Training Loss  2.7426498490967788e-05\n","Epoch  14 Batch  226 / 228  Training Loss  2.2382040697266348e-05\n","Epoch  14 Batch  227 / 228  Training Loss  2.4589167878730223e-05\n","  15    |    -    |   0.000030   | 99.542683\n","----------------------------------------------------------------------\n","Running epoch: 15\n","Epoch  15 Batch  0 / 228  Training Loss  2.6409843485453166e-05\n","Epoch  15 Batch  1 / 228  Training Loss  2.4449236661894247e-05\n","Epoch  15 Batch  2 / 228  Training Loss  2.8871514587081037e-05\n","Epoch  15 Batch  3 / 228  Training Loss  2.0407340343808755e-05\n","Epoch  15 Batch  4 / 228  Training Loss  2.0249277440598235e-05\n","Epoch  15 Batch  5 / 228  Training Loss  2.7157864678883925e-05\n","Epoch  15 Batch  6 / 228  Training Loss  3.199175262125209e-05\n","Epoch  15 Batch  7 / 228  Training Loss  2.6431302103446797e-05\n","Epoch  15 Batch  8 / 228  Training Loss  3.2827687391545624e-05\n","Epoch  15 Batch  9 / 228  Training Loss  2.7836678782477975e-05\n","Epoch  15 Batch  10 / 228  Training Loss  3.056582499993965e-05\n","Epoch  15 Batch  11 / 228  Training Loss  3.0967021302785724e-05\n","Epoch  15 Batch  12 / 228  Training Loss  2.8998410925851204e-05\n","Epoch  15 Batch  13 / 228  Training Loss  3.4969791158800945e-05\n","Epoch  15 Batch  14 / 228  Training Loss  2.782922820188105e-05\n","Epoch  15 Batch  15 / 228  Training Loss  2.5621699023758993e-05\n","Epoch  15 Batch  16 / 228  Training Loss  2.392535134276841e-05\n","Epoch  15 Batch  17 / 228  Training Loss  1.9434064597589895e-05\n","Epoch  15 Batch  18 / 228  Training Loss  2.3203034288599156e-05\n","Epoch  15 Batch  19 / 228  Training Loss  2.6697885914472863e-05\n","Epoch  15 Batch  20 / 228  Training Loss  2.0887271602987312e-05\n","Epoch  15 Batch  21 / 228  Training Loss  2.7613834390649572e-05\n","Epoch  15 Batch  22 / 228  Training Loss  2.329402559553273e-05\n","Epoch  15 Batch  23 / 228  Training Loss  2.2524682208313607e-05\n","Epoch  15 Batch  24 / 228  Training Loss  2.6076460926560685e-05\n","Epoch  15 Batch  25 / 228  Training Loss  3.2010095310397446e-05\n","Epoch  15 Batch  26 / 228  Training Loss  3.0516142942360602e-05\n","Epoch  15 Batch  27 / 228  Training Loss  2.4760558517300524e-05\n","Epoch  15 Batch  28 / 228  Training Loss  2.731295899138786e-05\n","Epoch  15 Batch  29 / 228  Training Loss  3.265401755925268e-05\n","Epoch  15 Batch  30 / 228  Training Loss  2.0872232198598795e-05\n","Epoch  15 Batch  31 / 228  Training Loss  2.081427737721242e-05\n","Epoch  15 Batch  32 / 228  Training Loss  3.501245373627171e-05\n","Epoch  15 Batch  33 / 228  Training Loss  2.813905848597642e-05\n","Epoch  15 Batch  34 / 228  Training Loss  3.1935454899212345e-05\n","Epoch  15 Batch  35 / 228  Training Loss  2.1465324607561342e-05\n","Epoch  15 Batch  36 / 228  Training Loss  3.4312604839215055e-05\n","Epoch  15 Batch  37 / 228  Training Loss  2.6760364562505856e-05\n","Epoch  15 Batch  38 / 228  Training Loss  2.7509036954143085e-05\n","Epoch  15 Batch  39 / 228  Training Loss  2.427139042993076e-05\n","Epoch  15 Batch  40 / 228  Training Loss  2.652352486620657e-05\n","Epoch  15 Batch  41 / 228  Training Loss  2.1989812012179755e-05\n","Epoch  15 Batch  42 / 228  Training Loss  3.050180930586066e-05\n","Epoch  15 Batch  43 / 228  Training Loss  2.456091533531435e-05\n","Epoch  15 Batch  44 / 228  Training Loss  3.80000019504223e-05\n","Epoch  15 Batch  45 / 228  Training Loss  2.7874557417817414e-05\n","Epoch  15 Batch  46 / 228  Training Loss  4.205781442578882e-05\n","Epoch  15 Batch  47 / 228  Training Loss  2.766285979305394e-05\n","Epoch  15 Batch  48 / 228  Training Loss  2.609812690934632e-05\n","Epoch  15 Batch  49 / 228  Training Loss  2.073284485959448e-05\n","Epoch  15 Batch  50 / 228  Training Loss  2.569167918409221e-05\n","Epoch  15 Batch  51 / 228  Training Loss  2.579313331807498e-05\n","Epoch  15 Batch  52 / 228  Training Loss  3.5955286875832826e-05\n","Epoch  15 Batch  53 / 228  Training Loss  2.6576692107482813e-05\n","Epoch  15 Batch  54 / 228  Training Loss  2.8721135095111094e-05\n","Epoch  15 Batch  55 / 228  Training Loss  2.4467133698635735e-05\n","Epoch  15 Batch  56 / 228  Training Loss  3.195065073668957e-05\n","Epoch  15 Batch  57 / 228  Training Loss  2.275173392263241e-05\n","Epoch  15 Batch  58 / 228  Training Loss  3.154291698592715e-05\n","Epoch  15 Batch  59 / 228  Training Loss  4.129585067857988e-05\n","Epoch  15 Batch  60 / 228  Training Loss  1.8844712030841038e-05\n","Epoch  15 Batch  61 / 228  Training Loss  2.4591518013039604e-05\n","Epoch  15 Batch  62 / 228  Training Loss  3.186269896104932e-05\n","Epoch  15 Batch  63 / 228  Training Loss  3.0685492674820125e-05\n","Epoch  15 Batch  64 / 228  Training Loss  2.824222610797733e-05\n","Epoch  15 Batch  65 / 228  Training Loss  1.5309111404349096e-05\n","Epoch  15 Batch  66 / 228  Training Loss  2.3359034457826056e-05\n","Epoch  15 Batch  67 / 228  Training Loss  2.432629480608739e-05\n","Epoch  15 Batch  68 / 228  Training Loss  3.3307955163763836e-05\n","Epoch  15 Batch  69 / 228  Training Loss  3.5144516004947945e-05\n","Epoch  15 Batch  70 / 228  Training Loss  3.07380614685826e-05\n","Epoch  15 Batch  71 / 228  Training Loss  2.5171562811010517e-05\n","Epoch  15 Batch  72 / 228  Training Loss  2.04619445867138e-05\n","Epoch  15 Batch  73 / 228  Training Loss  3.1132512958720326e-05\n","Epoch  15 Batch  74 / 228  Training Loss  2.82395430986071e-05\n","Epoch  15 Batch  75 / 228  Training Loss  2.3925433197291568e-05\n","Epoch  15 Batch  76 / 228  Training Loss  2.3994860384846106e-05\n","Epoch  15 Batch  77 / 228  Training Loss  2.4524013497284614e-05\n","Epoch  15 Batch  78 / 228  Training Loss  3.5772707633441314e-05\n","Epoch  15 Batch  79 / 228  Training Loss  2.9734292184002697e-05\n","Epoch  15 Batch  80 / 228  Training Loss  2.628596848808229e-05\n","Epoch  15 Batch  81 / 228  Training Loss  1.957369750016369e-05\n","Epoch  15 Batch  82 / 228  Training Loss  2.7948550268774852e-05\n","Epoch  15 Batch  83 / 228  Training Loss  2.1793130144942552e-05\n","Epoch  15 Batch  84 / 228  Training Loss  3.453389945207164e-05\n","Epoch  15 Batch  85 / 228  Training Loss  2.990615575981792e-05\n","Epoch  15 Batch  86 / 228  Training Loss  2.4623112039989792e-05\n","Epoch  15 Batch  87 / 228  Training Loss  2.649688394740224e-05\n","Epoch  15 Batch  88 / 228  Training Loss  1.9632099792943336e-05\n","Epoch  15 Batch  89 / 228  Training Loss  3.122985071968287e-05\n","Epoch  15 Batch  90 / 228  Training Loss  2.4396233129664324e-05\n","Epoch  15 Batch  91 / 228  Training Loss  2.8237405786057934e-05\n","Epoch  15 Batch  92 / 228  Training Loss  1.8331058527110144e-05\n","Epoch  15 Batch  93 / 228  Training Loss  2.3354852601187304e-05\n","Epoch  15 Batch  94 / 228  Training Loss  2.0020059309899807e-05\n","Epoch  15 Batch  95 / 228  Training Loss  1.8868204278987832e-05\n","Epoch  15 Batch  96 / 228  Training Loss  2.1229821868473664e-05\n","Epoch  15 Batch  97 / 228  Training Loss  2.2652742700302042e-05\n","Epoch  15 Batch  98 / 228  Training Loss  2.1434414520626888e-05\n","Epoch  15 Batch  99 / 228  Training Loss  2.7118867365061305e-05\n","Epoch  15 Batch  100 / 228  Training Loss  2.0581612261594273e-05\n","Epoch  15 Batch  101 / 228  Training Loss  2.7662657885230146e-05\n","Epoch  15 Batch  102 / 228  Training Loss  4.3124313378939405e-05\n","Epoch  15 Batch  103 / 228  Training Loss  3.2652886147843674e-05\n","Epoch  15 Batch  104 / 228  Training Loss  2.1431622371892445e-05\n","Epoch  15 Batch  105 / 228  Training Loss  1.694237653282471e-05\n","Epoch  15 Batch  106 / 228  Training Loss  3.12158081214875e-05\n","Epoch  15 Batch  107 / 228  Training Loss  2.4258977646240965e-05\n","Epoch  15 Batch  108 / 228  Training Loss  2.627216963446699e-05\n","Epoch  15 Batch  109 / 228  Training Loss  2.210450838902034e-05\n","Epoch  15 Batch  110 / 228  Training Loss  1.6298627087962814e-05\n","Epoch  15 Batch  111 / 228  Training Loss  2.5717086828080937e-05\n","Epoch  15 Batch  112 / 228  Training Loss  2.9787499443045817e-05\n","Epoch  15 Batch  113 / 228  Training Loss  2.384955223533325e-05\n","Epoch  15 Batch  114 / 228  Training Loss  2.8044025384588167e-05\n","Epoch  15 Batch  115 / 228  Training Loss  2.6461528250365518e-05\n","Epoch  15 Batch  116 / 228  Training Loss  1.9254413928138092e-05\n","Epoch  15 Batch  117 / 228  Training Loss  2.9002942028455436e-05\n","Epoch  15 Batch  118 / 228  Training Loss  2.6030193112092093e-05\n","Epoch  15 Batch  119 / 228  Training Loss  2.641052196850069e-05\n","Epoch  15 Batch  120 / 228  Training Loss  2.0893890905426815e-05\n","Epoch  15 Batch  121 / 228  Training Loss  3.536460644681938e-05\n","Epoch  15 Batch  122 / 228  Training Loss  2.3076356228557415e-05\n","Epoch  15 Batch  123 / 228  Training Loss  3.0350667657330632e-05\n","Epoch  15 Batch  124 / 228  Training Loss  2.5209455998265184e-05\n","Epoch  15 Batch  125 / 228  Training Loss  2.1961368474876508e-05\n","Epoch  15 Batch  126 / 228  Training Loss  3.376226231921464e-05\n","Epoch  15 Batch  127 / 228  Training Loss  3.9011589251458645e-05\n","Epoch  15 Batch  128 / 228  Training Loss  2.5011180696310475e-05\n","Epoch  15 Batch  129 / 228  Training Loss  1.9870418327627704e-05\n","Epoch  15 Batch  130 / 228  Training Loss  2.5790013751247898e-05\n","Epoch  15 Batch  131 / 228  Training Loss  2.426088394713588e-05\n","Epoch  15 Batch  132 / 228  Training Loss  1.9421466276980937e-05\n","Epoch  15 Batch  133 / 228  Training Loss  2.3548087483504787e-05\n","Epoch  15 Batch  134 / 228  Training Loss  3.5328073863638565e-05\n","Epoch  15 Batch  135 / 228  Training Loss  2.2636533685727045e-05\n","Epoch  15 Batch  136 / 228  Training Loss  2.4537750505260192e-05\n","Epoch  15 Batch  137 / 228  Training Loss  3.0167351724230684e-05\n","Epoch  15 Batch  138 / 228  Training Loss  2.7655894882627763e-05\n","Epoch  15 Batch  139 / 228  Training Loss  2.0237161152181216e-05\n","Epoch  15 Batch  140 / 228  Training Loss  1.6659532775520347e-05\n","Epoch  15 Batch  141 / 228  Training Loss  2.9386010282905772e-05\n","Epoch  15 Batch  142 / 228  Training Loss  2.8685899451375008e-05\n","Epoch  15 Batch  143 / 228  Training Loss  2.4217742975451984e-05\n","Epoch  15 Batch  144 / 228  Training Loss  2.7503174351295456e-05\n","Epoch  15 Batch  145 / 228  Training Loss  2.2196310965227894e-05\n","Epoch  15 Batch  146 / 228  Training Loss  1.9948183762608096e-05\n","Epoch  15 Batch  147 / 228  Training Loss  2.3354048607870936e-05\n","Epoch  15 Batch  148 / 228  Training Loss  2.536427382437978e-05\n","Epoch  15 Batch  149 / 228  Training Loss  3.4386182960588485e-05\n","Epoch  15 Batch  150 / 228  Training Loss  2.6252349925925955e-05\n","Epoch  15 Batch  151 / 228  Training Loss  3.708813528646715e-05\n","Epoch  15 Batch  152 / 228  Training Loss  2.5739322154549882e-05\n","Epoch  15 Batch  153 / 228  Training Loss  2.5538611225783825e-05\n","Epoch  15 Batch  154 / 228  Training Loss  2.968971966765821e-05\n","Epoch  15 Batch  155 / 228  Training Loss  2.64495665760478e-05\n","Epoch  15 Batch  156 / 228  Training Loss  2.85278947558254e-05\n","Epoch  15 Batch  157 / 228  Training Loss  2.5170334993163124e-05\n","Epoch  15 Batch  158 / 228  Training Loss  1.8213175280834548e-05\n","Epoch  15 Batch  159 / 228  Training Loss  3.195610042894259e-05\n","Epoch  15 Batch  160 / 228  Training Loss  2.4485818357788958e-05\n","Epoch  15 Batch  161 / 228  Training Loss  2.9443142921081744e-05\n","Epoch  15 Batch  162 / 228  Training Loss  2.8324266168056056e-05\n","Epoch  15 Batch  163 / 228  Training Loss  2.710721491894219e-05\n","Epoch  15 Batch  164 / 228  Training Loss  2.4619483156129718e-05\n","Epoch  15 Batch  165 / 228  Training Loss  2.3753014829708263e-05\n","Epoch  15 Batch  166 / 228  Training Loss  2.4271965230582282e-05\n","Epoch  15 Batch  167 / 228  Training Loss  2.2629772502114065e-05\n","Epoch  15 Batch  168 / 228  Training Loss  2.298162507941015e-05\n","Epoch  15 Batch  169 / 228  Training Loss  3.373964864294976e-05\n","Epoch  15 Batch  170 / 228  Training Loss  2.5129727873718366e-05\n","Epoch  15 Batch  171 / 228  Training Loss  2.6840250939130783e-05\n","Epoch  15 Batch  172 / 228  Training Loss  2.734551344474312e-05\n","Epoch  15 Batch  173 / 228  Training Loss  2.2246455046115443e-05\n","Epoch  15 Batch  174 / 228  Training Loss  2.8338250558590516e-05\n","Epoch  15 Batch  175 / 228  Training Loss  2.4038014089455828e-05\n","Epoch  15 Batch  176 / 228  Training Loss  2.9743323466391303e-05\n","Epoch  15 Batch  177 / 228  Training Loss  2.8890000976389274e-05\n","Epoch  15 Batch  178 / 228  Training Loss  2.679395220184233e-05\n","Epoch  15 Batch  179 / 228  Training Loss  2.4767305148998275e-05\n","Epoch  15 Batch  180 / 228  Training Loss  2.8523307264549658e-05\n","Epoch  15 Batch  181 / 228  Training Loss  2.935939482995309e-05\n","Epoch  15 Batch  182 / 228  Training Loss  2.356891127419658e-05\n","Epoch  15 Batch  183 / 228  Training Loss  2.081280035781674e-05\n","Epoch  15 Batch  184 / 228  Training Loss  2.7500911528477445e-05\n","Epoch  15 Batch  185 / 228  Training Loss  2.9842896765330806e-05\n","Epoch  15 Batch  186 / 228  Training Loss  1.872155371529516e-05\n","Epoch  15 Batch  187 / 228  Training Loss  2.434027919662185e-05\n","Epoch  15 Batch  188 / 228  Training Loss  2.810680598486215e-05\n","Epoch  15 Batch  189 / 228  Training Loss  2.3722976038698107e-05\n","Epoch  15 Batch  190 / 228  Training Loss  2.4915983885875903e-05\n","Epoch  15 Batch  191 / 228  Training Loss  2.3028682335279882e-05\n","Epoch  15 Batch  192 / 228  Training Loss  2.208593286923133e-05\n","Epoch  15 Batch  193 / 228  Training Loss  2.083382787532173e-05\n","Epoch  15 Batch  194 / 228  Training Loss  2.9705604902119376e-05\n","Epoch  15 Batch  195 / 228  Training Loss  3.1016166758490726e-05\n","Epoch  15 Batch  196 / 228  Training Loss  2.2563570382772014e-05\n","Epoch  15 Batch  197 / 228  Training Loss  1.850748958531767e-05\n","Epoch  15 Batch  198 / 228  Training Loss  3.0733375751879066e-05\n","Epoch  15 Batch  199 / 228  Training Loss  2.7637634048005566e-05\n","Epoch  15 Batch  200 / 228  Training Loss  2.3146159946918488e-05\n","Epoch  15 Batch  201 / 228  Training Loss  2.2509742848342285e-05\n","Epoch  15 Batch  202 / 228  Training Loss  2.6874244213104248e-05\n","Epoch  15 Batch  203 / 228  Training Loss  2.7687379770213738e-05\n","Epoch  15 Batch  204 / 228  Training Loss  2.0677143766079098e-05\n","Epoch  15 Batch  205 / 228  Training Loss  3.4466891520423815e-05\n","Epoch  15 Batch  206 / 228  Training Loss  1.4443998225033283e-05\n","Epoch  15 Batch  207 / 228  Training Loss  2.107674299622886e-05\n","Epoch  15 Batch  208 / 228  Training Loss  2.4948163627414033e-05\n","Epoch  15 Batch  209 / 228  Training Loss  2.8047221348970197e-05\n","Epoch  15 Batch  210 / 228  Training Loss  2.382796083111316e-05\n","Epoch  15 Batch  211 / 228  Training Loss  2.307584509253502e-05\n","Epoch  15 Batch  212 / 228  Training Loss  2.6355250156484544e-05\n","Epoch  15 Batch  213 / 228  Training Loss  2.0722316548926756e-05\n","Epoch  15 Batch  214 / 228  Training Loss  2.3752067136229016e-05\n","Epoch  15 Batch  215 / 228  Training Loss  3.457148704910651e-05\n","Epoch  15 Batch  216 / 228  Training Loss  2.1327747163013555e-05\n","Epoch  15 Batch  217 / 228  Training Loss  1.7953929273062386e-05\n","Epoch  15 Batch  218 / 228  Training Loss  2.795854197756853e-05\n","Epoch  15 Batch  219 / 228  Training Loss  2.290984775754623e-05\n","Epoch  15 Batch  220 / 228  Training Loss  2.556529398134444e-05\n","Epoch  15 Batch  221 / 228  Training Loss  2.3163414880400524e-05\n","Epoch  15 Batch  222 / 228  Training Loss  3.173942241119221e-05\n","Epoch  15 Batch  223 / 228  Training Loss  2.5715853553265333e-05\n","Epoch  15 Batch  224 / 228  Training Loss  2.5102932340814732e-05\n","Epoch  15 Batch  225 / 228  Training Loss  2.3323449568124488e-05\n","Epoch  15 Batch  226 / 228  Training Loss  2.3238681023940444e-05\n","Epoch  15 Batch  227 / 228  Training Loss  2.0655790649470873e-05\n","  16    |    -    |   0.000026   | 99.580793\n","----------------------------------------------------------------------\n","Running epoch: 16\n","Epoch  16 Batch  0 / 228  Training Loss  2.579474494268652e-05\n","Epoch  16 Batch  1 / 228  Training Loss  2.5584609829820693e-05\n","Epoch  16 Batch  2 / 228  Training Loss  2.0837902411585674e-05\n","Epoch  16 Batch  3 / 228  Training Loss  3.16008081426844e-05\n","Epoch  16 Batch  4 / 228  Training Loss  2.704990401980467e-05\n","Epoch  16 Batch  5 / 228  Training Loss  3.84400445909705e-05\n","Epoch  16 Batch  6 / 228  Training Loss  2.4544468033127487e-05\n","Epoch  16 Batch  7 / 228  Training Loss  1.9548750060494058e-05\n","Epoch  16 Batch  8 / 228  Training Loss  1.8947199350805022e-05\n","Epoch  16 Batch  9 / 228  Training Loss  3.727933290065266e-05\n","Epoch  16 Batch  10 / 228  Training Loss  2.401895835646428e-05\n","Epoch  16 Batch  11 / 228  Training Loss  3.792980714933947e-05\n","Epoch  16 Batch  12 / 228  Training Loss  1.952410275407601e-05\n","Epoch  16 Batch  13 / 228  Training Loss  1.6404745110776275e-05\n","Epoch  16 Batch  14 / 228  Training Loss  2.4960454538813792e-05\n","Epoch  16 Batch  15 / 228  Training Loss  2.56403764069546e-05\n","Epoch  16 Batch  16 / 228  Training Loss  2.79811501968652e-05\n","Epoch  16 Batch  17 / 228  Training Loss  2.4631019186927006e-05\n","Epoch  16 Batch  18 / 228  Training Loss  2.3318661988014355e-05\n","Epoch  16 Batch  19 / 228  Training Loss  2.8423097319318913e-05\n","Epoch  16 Batch  20 / 228  Training Loss  2.7945838155574165e-05\n","Epoch  16 Batch  21 / 228  Training Loss  1.9646584405563772e-05\n","Epoch  16 Batch  22 / 228  Training Loss  2.0445531845325604e-05\n","Epoch  16 Batch  23 / 228  Training Loss  1.903934207803104e-05\n","Epoch  16 Batch  24 / 228  Training Loss  2.6119194444618188e-05\n","Epoch  16 Batch  25 / 228  Training Loss  2.0379524357849732e-05\n","Epoch  16 Batch  26 / 228  Training Loss  2.2699241526424885e-05\n","Epoch  16 Batch  27 / 228  Training Loss  2.7107995265396312e-05\n","Epoch  16 Batch  28 / 228  Training Loss  2.1339905288186856e-05\n","Epoch  16 Batch  29 / 228  Training Loss  2.840905472112354e-05\n","Epoch  16 Batch  30 / 228  Training Loss  2.2972624719841406e-05\n","Epoch  16 Batch  31 / 228  Training Loss  2.956647404062096e-05\n","Epoch  16 Batch  32 / 228  Training Loss  2.330715869902633e-05\n","Epoch  16 Batch  33 / 228  Training Loss  2.5082204956561327e-05\n","Epoch  16 Batch  34 / 228  Training Loss  1.8067226847051643e-05\n","Epoch  16 Batch  35 / 228  Training Loss  2.8243250199011527e-05\n","Epoch  16 Batch  36 / 228  Training Loss  3.091203689109534e-05\n","Epoch  16 Batch  37 / 228  Training Loss  2.0493685951805674e-05\n","Epoch  16 Batch  38 / 228  Training Loss  1.760994018695783e-05\n","Epoch  16 Batch  39 / 228  Training Loss  2.0130977645749226e-05\n","Epoch  16 Batch  40 / 228  Training Loss  2.8381642550812103e-05\n","Epoch  16 Batch  41 / 228  Training Loss  2.4168210075004026e-05\n","Epoch  16 Batch  42 / 228  Training Loss  2.472111737006344e-05\n","Epoch  16 Batch  43 / 228  Training Loss  1.767971843946725e-05\n","Epoch  16 Batch  44 / 228  Training Loss  1.9542501831892878e-05\n","Epoch  16 Batch  45 / 228  Training Loss  1.7270507669309154e-05\n","Epoch  16 Batch  46 / 228  Training Loss  2.106694591930136e-05\n","Epoch  16 Batch  47 / 228  Training Loss  2.2069934857427143e-05\n","Epoch  16 Batch  48 / 228  Training Loss  2.3650627554161474e-05\n","Epoch  16 Batch  49 / 228  Training Loss  3.337279486004263e-05\n","Epoch  16 Batch  50 / 228  Training Loss  2.2577385607291944e-05\n","Epoch  16 Batch  51 / 228  Training Loss  2.714645233936608e-05\n","Epoch  16 Batch  52 / 228  Training Loss  3.566680243238807e-05\n","Epoch  16 Batch  53 / 228  Training Loss  2.010702883126214e-05\n","Epoch  16 Batch  54 / 228  Training Loss  1.9085984604316764e-05\n","Epoch  16 Batch  55 / 228  Training Loss  2.001205757551361e-05\n","Epoch  16 Batch  56 / 228  Training Loss  1.8589518731459975e-05\n","Epoch  16 Batch  57 / 228  Training Loss  2.3723425329080783e-05\n","Epoch  16 Batch  58 / 228  Training Loss  2.5232267944375053e-05\n","Epoch  16 Batch  59 / 228  Training Loss  2.0201689039822668e-05\n","Epoch  16 Batch  60 / 228  Training Loss  2.817754830175545e-05\n","Epoch  16 Batch  61 / 228  Training Loss  2.2463249479187652e-05\n","Epoch  16 Batch  62 / 228  Training Loss  1.8127213479601778e-05\n","Epoch  16 Batch  63 / 228  Training Loss  2.222111652372405e-05\n","Epoch  16 Batch  64 / 228  Training Loss  2.8228547307662666e-05\n","Epoch  16 Batch  65 / 228  Training Loss  2.3218992282636464e-05\n","Epoch  16 Batch  66 / 228  Training Loss  1.735304613248445e-05\n","Epoch  16 Batch  67 / 228  Training Loss  2.126547769876197e-05\n","Epoch  16 Batch  68 / 228  Training Loss  2.1477164409589022e-05\n","Epoch  16 Batch  69 / 228  Training Loss  2.137082083208952e-05\n","Epoch  16 Batch  70 / 228  Training Loss  2.4806311557767913e-05\n","Epoch  16 Batch  71 / 228  Training Loss  2.0086623408133164e-05\n","Epoch  16 Batch  72 / 228  Training Loss  2.5932691642083228e-05\n","Epoch  16 Batch  73 / 228  Training Loss  2.789248901535757e-05\n","Epoch  16 Batch  74 / 228  Training Loss  1.9811317542917095e-05\n","Epoch  16 Batch  75 / 228  Training Loss  2.7651705750031397e-05\n","Epoch  16 Batch  76 / 228  Training Loss  2.1574833226623014e-05\n","Epoch  16 Batch  77 / 228  Training Loss  2.3190505089587532e-05\n","Epoch  16 Batch  78 / 228  Training Loss  2.91248616122175e-05\n","Epoch  16 Batch  79 / 228  Training Loss  2.028566086664796e-05\n","Epoch  16 Batch  80 / 228  Training Loss  3.1424398912349716e-05\n","Epoch  16 Batch  81 / 228  Training Loss  2.1006850147387013e-05\n","Epoch  16 Batch  82 / 228  Training Loss  2.6343148419982754e-05\n","Epoch  16 Batch  83 / 228  Training Loss  3.033147004316561e-05\n","Epoch  16 Batch  84 / 228  Training Loss  2.5062448912649415e-05\n","Epoch  16 Batch  85 / 228  Training Loss  2.426261744403746e-05\n","Epoch  16 Batch  86 / 228  Training Loss  1.9809074728982523e-05\n","Epoch  16 Batch  87 / 228  Training Loss  2.8311143978498876e-05\n","Epoch  16 Batch  88 / 228  Training Loss  2.5257953893742524e-05\n","Epoch  16 Batch  89 / 228  Training Loss  2.2114854800747707e-05\n","Epoch  16 Batch  90 / 228  Training Loss  2.4461327484459616e-05\n","Epoch  16 Batch  91 / 228  Training Loss  2.760487586783711e-05\n","Epoch  16 Batch  92 / 228  Training Loss  1.9707256797119044e-05\n","Epoch  16 Batch  93 / 228  Training Loss  2.3968825189513154e-05\n","Epoch  16 Batch  94 / 228  Training Loss  1.971843266801443e-05\n","Epoch  16 Batch  95 / 228  Training Loss  2.2920323317521252e-05\n","Epoch  16 Batch  96 / 228  Training Loss  1.8074506442644633e-05\n","Epoch  16 Batch  97 / 228  Training Loss  1.9514494852046482e-05\n","Epoch  16 Batch  98 / 228  Training Loss  2.7373380362405442e-05\n","Epoch  16 Batch  99 / 228  Training Loss  3.436447514104657e-05\n","Epoch  16 Batch  100 / 228  Training Loss  1.833203532441985e-05\n","Epoch  16 Batch  101 / 228  Training Loss  1.9764405806199647e-05\n","Epoch  16 Batch  102 / 228  Training Loss  2.2519083358929493e-05\n","Epoch  16 Batch  103 / 228  Training Loss  2.971046524180565e-05\n","Epoch  16 Batch  104 / 228  Training Loss  1.54351037053857e-05\n","Epoch  16 Batch  105 / 228  Training Loss  2.177352325816173e-05\n","Epoch  16 Batch  106 / 228  Training Loss  2.0230952941346914e-05\n","Epoch  16 Batch  107 / 228  Training Loss  2.9435908800223842e-05\n","Epoch  16 Batch  108 / 228  Training Loss  2.2662086848868057e-05\n","Epoch  16 Batch  109 / 228  Training Loss  2.6615889510139823e-05\n","Epoch  16 Batch  110 / 228  Training Loss  2.3684988263994455e-05\n","Epoch  16 Batch  111 / 228  Training Loss  2.0899960873066448e-05\n","Epoch  16 Batch  112 / 228  Training Loss  2.3803144358680584e-05\n","Epoch  16 Batch  113 / 228  Training Loss  2.4059674615273252e-05\n","Epoch  16 Batch  114 / 228  Training Loss  2.167160164390225e-05\n","Epoch  16 Batch  115 / 228  Training Loss  2.309736009920016e-05\n","Epoch  16 Batch  116 / 228  Training Loss  2.150922045984771e-05\n","Epoch  16 Batch  117 / 228  Training Loss  2.6519197490415536e-05\n","Epoch  16 Batch  118 / 228  Training Loss  2.1466994439833798e-05\n","Epoch  16 Batch  119 / 228  Training Loss  2.0499124730122276e-05\n","Epoch  16 Batch  120 / 228  Training Loss  2.2825523046776652e-05\n","Epoch  16 Batch  121 / 228  Training Loss  1.641890048631467e-05\n","Epoch  16 Batch  122 / 228  Training Loss  1.900317147374153e-05\n","Epoch  16 Batch  123 / 228  Training Loss  3.7621211959049106e-05\n","Epoch  16 Batch  124 / 228  Training Loss  2.1645286324201152e-05\n","Epoch  16 Batch  125 / 228  Training Loss  2.1967975044390187e-05\n","Epoch  16 Batch  126 / 228  Training Loss  1.9598990547819994e-05\n","Epoch  16 Batch  127 / 228  Training Loss  1.8774149793898687e-05\n","Epoch  16 Batch  128 / 228  Training Loss  2.655615026014857e-05\n","Epoch  16 Batch  129 / 228  Training Loss  2.194504668295849e-05\n","Epoch  16 Batch  130 / 228  Training Loss  2.4339122319361195e-05\n","Epoch  16 Batch  131 / 228  Training Loss  2.8757169275195338e-05\n","Epoch  16 Batch  132 / 228  Training Loss  1.984813206945546e-05\n","Epoch  16 Batch  133 / 228  Training Loss  2.8794569516321644e-05\n","Epoch  16 Batch  134 / 228  Training Loss  2.343480082345195e-05\n","Epoch  16 Batch  135 / 228  Training Loss  2.4170054530259222e-05\n","Epoch  16 Batch  136 / 228  Training Loss  2.2579930373467505e-05\n","Epoch  16 Batch  137 / 228  Training Loss  2.4815299184410833e-05\n","Epoch  16 Batch  138 / 228  Training Loss  2.1404490325949155e-05\n","Epoch  16 Batch  139 / 228  Training Loss  2.039117316599004e-05\n","Epoch  16 Batch  140 / 228  Training Loss  3.480083614704199e-05\n","Epoch  16 Batch  141 / 228  Training Loss  2.3820313799660653e-05\n","Epoch  16 Batch  142 / 228  Training Loss  2.650588248798158e-05\n","Epoch  16 Batch  143 / 228  Training Loss  2.1307638235157356e-05\n","Epoch  16 Batch  144 / 228  Training Loss  2.2283405996859074e-05\n","Epoch  16 Batch  145 / 228  Training Loss  1.922334558912553e-05\n","Epoch  16 Batch  146 / 228  Training Loss  2.5122961233137175e-05\n","Epoch  16 Batch  147 / 228  Training Loss  2.660745485627558e-05\n","Epoch  16 Batch  148 / 228  Training Loss  2.280931585119106e-05\n","Epoch  16 Batch  149 / 228  Training Loss  3.4407734347041696e-05\n","Epoch  16 Batch  150 / 228  Training Loss  2.169953950215131e-05\n","Epoch  16 Batch  151 / 228  Training Loss  2.7285055693937466e-05\n","Epoch  16 Batch  152 / 228  Training Loss  2.319560371688567e-05\n","Epoch  16 Batch  153 / 228  Training Loss  1.7210617443197407e-05\n","Epoch  16 Batch  154 / 228  Training Loss  2.374802170379553e-05\n","Epoch  16 Batch  155 / 228  Training Loss  2.001680877583567e-05\n","Epoch  16 Batch  156 / 228  Training Loss  2.9576453016488813e-05\n","Epoch  16 Batch  157 / 228  Training Loss  1.993958358070813e-05\n","Epoch  16 Batch  158 / 228  Training Loss  1.8392285710433498e-05\n","Epoch  16 Batch  159 / 228  Training Loss  2.6516641810303554e-05\n","Epoch  16 Batch  160 / 228  Training Loss  2.237830813101027e-05\n","Epoch  16 Batch  161 / 228  Training Loss  1.5317969882744364e-05\n","Epoch  16 Batch  162 / 228  Training Loss  2.3823604351491667e-05\n","Epoch  16 Batch  163 / 228  Training Loss  2.0123245121794753e-05\n","Epoch  16 Batch  164 / 228  Training Loss  2.3741416953271255e-05\n","Epoch  16 Batch  165 / 228  Training Loss  2.0050365492352284e-05\n","Epoch  16 Batch  166 / 228  Training Loss  3.339326212881133e-05\n","Epoch  16 Batch  167 / 228  Training Loss  2.4548276996938512e-05\n","Epoch  16 Batch  168 / 228  Training Loss  1.7836326151154935e-05\n","Epoch  16 Batch  169 / 228  Training Loss  2.1798030502395704e-05\n","Epoch  16 Batch  170 / 228  Training Loss  2.0338389731477946e-05\n","Epoch  16 Batch  171 / 228  Training Loss  2.0450373995117843e-05\n","Epoch  16 Batch  172 / 228  Training Loss  1.915891516546253e-05\n","Epoch  16 Batch  173 / 228  Training Loss  2.1596188162220642e-05\n","Epoch  16 Batch  174 / 228  Training Loss  2.0745888832607307e-05\n","Epoch  16 Batch  175 / 228  Training Loss  2.6378354959888384e-05\n","Epoch  16 Batch  176 / 228  Training Loss  2.1635094526573084e-05\n","Epoch  16 Batch  177 / 228  Training Loss  2.4629927793284878e-05\n","Epoch  16 Batch  178 / 228  Training Loss  2.329242488485761e-05\n","Epoch  16 Batch  179 / 228  Training Loss  2.6172729121753946e-05\n","Epoch  16 Batch  180 / 228  Training Loss  1.9412022083997726e-05\n","Epoch  16 Batch  181 / 228  Training Loss  1.9936540411436e-05\n","Epoch  16 Batch  182 / 228  Training Loss  2.4430311896139756e-05\n","Epoch  16 Batch  183 / 228  Training Loss  2.266830051667057e-05\n","Epoch  16 Batch  184 / 228  Training Loss  2.082775426970329e-05\n","Epoch  16 Batch  185 / 228  Training Loss  2.7919615604332648e-05\n","Epoch  16 Batch  186 / 228  Training Loss  2.202765244874172e-05\n","Epoch  16 Batch  187 / 228  Training Loss  1.7186868717544712e-05\n","Epoch  16 Batch  188 / 228  Training Loss  2.4713323000469245e-05\n","Epoch  16 Batch  189 / 228  Training Loss  2.4597011361038312e-05\n","Epoch  16 Batch  190 / 228  Training Loss  2.6558944227872416e-05\n","Epoch  16 Batch  191 / 228  Training Loss  2.4614357243990526e-05\n","Epoch  16 Batch  192 / 228  Training Loss  1.932718623720575e-05\n","Epoch  16 Batch  193 / 228  Training Loss  2.1278592612361535e-05\n","Epoch  16 Batch  194 / 228  Training Loss  1.986795905395411e-05\n","Epoch  16 Batch  195 / 228  Training Loss  2.441332435410004e-05\n","Epoch  16 Batch  196 / 228  Training Loss  2.611560012155678e-05\n","Epoch  16 Batch  197 / 228  Training Loss  2.5985655156546272e-05\n","Epoch  16 Batch  198 / 228  Training Loss  2.698500975384377e-05\n","Epoch  16 Batch  199 / 228  Training Loss  1.5324090782087296e-05\n","Epoch  16 Batch  200 / 228  Training Loss  2.079034129565116e-05\n","Epoch  16 Batch  201 / 228  Training Loss  1.5207927390292753e-05\n","Epoch  16 Batch  202 / 228  Training Loss  3.272683170507662e-05\n","Epoch  16 Batch  203 / 228  Training Loss  2.2679507310385816e-05\n","Epoch  16 Batch  204 / 228  Training Loss  2.784415482892655e-05\n","Epoch  16 Batch  205 / 228  Training Loss  1.829889151849784e-05\n","Epoch  16 Batch  206 / 228  Training Loss  1.9361230442882515e-05\n","Epoch  16 Batch  207 / 228  Training Loss  1.4393670426215976e-05\n","Epoch  16 Batch  208 / 228  Training Loss  3.2131920306710526e-05\n","Epoch  16 Batch  209 / 228  Training Loss  2.7857846362167038e-05\n","Epoch  16 Batch  210 / 228  Training Loss  2.63951897068182e-05\n","Epoch  16 Batch  211 / 228  Training Loss  2.266101546410937e-05\n","Epoch  16 Batch  212 / 228  Training Loss  2.559976746852044e-05\n","Epoch  16 Batch  213 / 228  Training Loss  3.0252349461079575e-05\n","Epoch  16 Batch  214 / 228  Training Loss  2.726906313910149e-05\n","Epoch  16 Batch  215 / 228  Training Loss  1.3902276805310976e-05\n","Epoch  16 Batch  216 / 228  Training Loss  2.150730142602697e-05\n","Epoch  16 Batch  217 / 228  Training Loss  1.8314167391508818e-05\n","Epoch  16 Batch  218 / 228  Training Loss  1.7715958165354095e-05\n","Epoch  16 Batch  219 / 228  Training Loss  2.0467046851990744e-05\n","Epoch  16 Batch  220 / 228  Training Loss  2.1212948922766373e-05\n","Epoch  16 Batch  221 / 228  Training Loss  3.4075997973559424e-05\n","Epoch  16 Batch  222 / 228  Training Loss  1.4885384189256001e-05\n","Epoch  16 Batch  223 / 228  Training Loss  2.048813439614605e-05\n","Epoch  16 Batch  224 / 228  Training Loss  1.9924123989767395e-05\n","Epoch  16 Batch  225 / 228  Training Loss  3.1861312891123816e-05\n","Epoch  16 Batch  226 / 228  Training Loss  2.4144666895153932e-05\n","Epoch  16 Batch  227 / 228  Training Loss  1.5713665561634116e-05\n","  17    |    -    |   0.000023   | 99.580793\n","----------------------------------------------------------------------\n","Running epoch: 17\n","Epoch  17 Batch  0 / 228  Training Loss  1.803718987503089e-05\n","Epoch  17 Batch  1 / 228  Training Loss  2.4039854906732216e-05\n","Epoch  17 Batch  2 / 228  Training Loss  1.511476693849545e-05\n","Epoch  17 Batch  3 / 228  Training Loss  1.739699291647412e-05\n","Epoch  17 Batch  4 / 228  Training Loss  2.5118686608038843e-05\n","Epoch  17 Batch  5 / 228  Training Loss  2.216292341472581e-05\n","Epoch  17 Batch  6 / 228  Training Loss  2.2392638129531406e-05\n","Epoch  17 Batch  7 / 228  Training Loss  2.265801595058292e-05\n","Epoch  17 Batch  8 / 228  Training Loss  1.6347319615306333e-05\n","Epoch  17 Batch  9 / 228  Training Loss  1.8350017853663303e-05\n","Epoch  17 Batch  10 / 228  Training Loss  2.0792425857507624e-05\n","Epoch  17 Batch  11 / 228  Training Loss  2.1904903405811638e-05\n","Epoch  17 Batch  12 / 228  Training Loss  1.9741586584132165e-05\n","Epoch  17 Batch  13 / 228  Training Loss  2.7322821551933885e-05\n","Epoch  17 Batch  14 / 228  Training Loss  1.7416839909856208e-05\n","Epoch  17 Batch  15 / 228  Training Loss  1.6438072634628043e-05\n","Epoch  17 Batch  16 / 228  Training Loss  2.202388714067638e-05\n","Epoch  17 Batch  17 / 228  Training Loss  2.5428991648368537e-05\n","Epoch  17 Batch  18 / 228  Training Loss  1.999657615670003e-05\n","Epoch  17 Batch  19 / 228  Training Loss  1.9459399482002482e-05\n","Epoch  17 Batch  20 / 228  Training Loss  2.0674324332503602e-05\n","Epoch  17 Batch  21 / 228  Training Loss  2.5496115995338187e-05\n","Epoch  17 Batch  22 / 228  Training Loss  1.7955304429051466e-05\n","Epoch  17 Batch  23 / 228  Training Loss  2.1488302081706934e-05\n","Epoch  17 Batch  24 / 228  Training Loss  2.546838004491292e-05\n","Epoch  17 Batch  25 / 228  Training Loss  1.332724241365213e-05\n","Epoch  17 Batch  26 / 228  Training Loss  2.2268959583016112e-05\n","Epoch  17 Batch  27 / 228  Training Loss  2.03839190362487e-05\n","Epoch  17 Batch  28 / 228  Training Loss  2.481004412402399e-05\n","Epoch  17 Batch  29 / 228  Training Loss  2.1618641767418012e-05\n","Epoch  17 Batch  30 / 228  Training Loss  2.0727082301164046e-05\n","Epoch  17 Batch  31 / 228  Training Loss  2.3618249542778358e-05\n","Epoch  17 Batch  32 / 228  Training Loss  2.6227065973216668e-05\n","Epoch  17 Batch  33 / 228  Training Loss  1.9901188352378085e-05\n","Epoch  17 Batch  34 / 228  Training Loss  2.4293567548738793e-05\n","Epoch  17 Batch  35 / 228  Training Loss  1.951313424797263e-05\n","Epoch  17 Batch  36 / 228  Training Loss  1.8639262634678744e-05\n","Epoch  17 Batch  37 / 228  Training Loss  1.8196769815403968e-05\n","Epoch  17 Batch  38 / 228  Training Loss  1.819115823309403e-05\n","Epoch  17 Batch  39 / 228  Training Loss  1.7275704522035085e-05\n","Epoch  17 Batch  40 / 228  Training Loss  2.2600817828788422e-05\n","Epoch  17 Batch  41 / 228  Training Loss  3.156396996928379e-05\n","Epoch  17 Batch  42 / 228  Training Loss  1.8153692508349195e-05\n","Epoch  17 Batch  43 / 228  Training Loss  1.8342743715038523e-05\n","Epoch  17 Batch  44 / 228  Training Loss  2.3332071577897295e-05\n","Epoch  17 Batch  45 / 228  Training Loss  2.2334101231535897e-05\n","Epoch  17 Batch  46 / 228  Training Loss  2.2736223399988376e-05\n","Epoch  17 Batch  47 / 228  Training Loss  2.5405126507394016e-05\n","Epoch  17 Batch  48 / 228  Training Loss  2.0810453861486167e-05\n","Epoch  17 Batch  49 / 228  Training Loss  1.409822107234504e-05\n","Epoch  17 Batch  50 / 228  Training Loss  3.088813537033275e-05\n","Epoch  17 Batch  51 / 228  Training Loss  1.9887420421582647e-05\n","Epoch  17 Batch  52 / 228  Training Loss  2.974647213704884e-05\n","Epoch  17 Batch  53 / 228  Training Loss  2.1389931134763174e-05\n","Epoch  17 Batch  54 / 228  Training Loss  1.9192015315638855e-05\n","Epoch  17 Batch  55 / 228  Training Loss  2.7063924790127203e-05\n","Epoch  17 Batch  56 / 228  Training Loss  2.344104359508492e-05\n","Epoch  17 Batch  57 / 228  Training Loss  2.05560791073367e-05\n","Epoch  17 Batch  58 / 228  Training Loss  2.7434269213699736e-05\n","Epoch  17 Batch  59 / 228  Training Loss  2.0049341401318088e-05\n","Epoch  17 Batch  60 / 228  Training Loss  1.7774091247702017e-05\n","Epoch  17 Batch  61 / 228  Training Loss  1.5762585462653078e-05\n","Epoch  17 Batch  62 / 228  Training Loss  2.056386074400507e-05\n","Epoch  17 Batch  63 / 228  Training Loss  1.4062925401958637e-05\n","Epoch  17 Batch  64 / 228  Training Loss  1.883891127363313e-05\n","Epoch  17 Batch  65 / 228  Training Loss  1.9614550183177926e-05\n","Epoch  17 Batch  66 / 228  Training Loss  2.0972316633560695e-05\n","Epoch  17 Batch  67 / 228  Training Loss  2.7196478185942397e-05\n","Epoch  17 Batch  68 / 228  Training Loss  2.164818943128921e-05\n","Epoch  17 Batch  69 / 228  Training Loss  2.2107386030256748e-05\n","Epoch  17 Batch  70 / 228  Training Loss  1.5471576261916198e-05\n","Epoch  17 Batch  71 / 228  Training Loss  1.9526254618540406e-05\n","Epoch  17 Batch  72 / 228  Training Loss  2.236317959614098e-05\n","Epoch  17 Batch  73 / 228  Training Loss  2.1016014216002077e-05\n","Epoch  17 Batch  74 / 228  Training Loss  2.6499139494262636e-05\n","Epoch  17 Batch  75 / 228  Training Loss  1.950507066794671e-05\n","Epoch  17 Batch  76 / 228  Training Loss  2.5268711397075094e-05\n","Epoch  17 Batch  77 / 228  Training Loss  2.720062548178248e-05\n","Epoch  17 Batch  78 / 228  Training Loss  1.5980594980646856e-05\n","Epoch  17 Batch  79 / 228  Training Loss  1.575731766934041e-05\n","Epoch  17 Batch  80 / 228  Training Loss  2.5264074793085456e-05\n","Epoch  17 Batch  81 / 228  Training Loss  2.7450088964542374e-05\n","Epoch  17 Batch  82 / 228  Training Loss  2.4797787773422897e-05\n","Epoch  17 Batch  83 / 228  Training Loss  2.2341451767715625e-05\n","Epoch  17 Batch  84 / 228  Training Loss  1.8328873920836486e-05\n","Epoch  17 Batch  85 / 228  Training Loss  1.9007040464202873e-05\n","Epoch  17 Batch  86 / 228  Training Loss  1.4615220607083756e-05\n","Epoch  17 Batch  87 / 228  Training Loss  1.8741036910796538e-05\n","Epoch  17 Batch  88 / 228  Training Loss  2.4372291591134854e-05\n","Epoch  17 Batch  89 / 228  Training Loss  1.841571975091938e-05\n","Epoch  17 Batch  90 / 228  Training Loss  2.501595372450538e-05\n","Epoch  17 Batch  91 / 228  Training Loss  1.448113198421197e-05\n","Epoch  17 Batch  92 / 228  Training Loss  1.8263124729855917e-05\n","Epoch  17 Batch  93 / 228  Training Loss  2.609834700706415e-05\n","Epoch  17 Batch  94 / 228  Training Loss  1.7781811038730666e-05\n","Epoch  17 Batch  95 / 228  Training Loss  1.7296664736932144e-05\n","Epoch  17 Batch  96 / 228  Training Loss  2.6075480491272174e-05\n","Epoch  17 Batch  97 / 228  Training Loss  1.8482869563740678e-05\n","Epoch  17 Batch  98 / 228  Training Loss  1.6799009245005436e-05\n","Epoch  17 Batch  99 / 228  Training Loss  1.9466424419078976e-05\n","Epoch  17 Batch  100 / 228  Training Loss  2.2187396098161116e-05\n","Epoch  17 Batch  101 / 228  Training Loss  2.692609814403113e-05\n","Epoch  17 Batch  102 / 228  Training Loss  2.519864574423991e-05\n","Epoch  17 Batch  103 / 228  Training Loss  2.3324755602516234e-05\n","Epoch  17 Batch  104 / 228  Training Loss  2.145418693544343e-05\n","Epoch  17 Batch  105 / 228  Training Loss  2.092603790515568e-05\n","Epoch  17 Batch  106 / 228  Training Loss  1.9647304725367576e-05\n","Epoch  17 Batch  107 / 228  Training Loss  2.1514757463592105e-05\n","Epoch  17 Batch  108 / 228  Training Loss  2.227811455668416e-05\n","Epoch  17 Batch  109 / 228  Training Loss  1.6700874766684137e-05\n","Epoch  17 Batch  110 / 228  Training Loss  2.903525273723062e-05\n","Epoch  17 Batch  111 / 228  Training Loss  2.1754876797785982e-05\n","Epoch  17 Batch  112 / 228  Training Loss  2.200838025601115e-05\n","Epoch  17 Batch  113 / 228  Training Loss  2.3268148652277887e-05\n","Epoch  17 Batch  114 / 228  Training Loss  2.2133079255581833e-05\n","Epoch  17 Batch  115 / 228  Training Loss  2.2160187654662877e-05\n","Epoch  17 Batch  116 / 228  Training Loss  2.6120522306882776e-05\n","Epoch  17 Batch  117 / 228  Training Loss  2.013117409660481e-05\n","Epoch  17 Batch  118 / 228  Training Loss  2.153096102119889e-05\n","Epoch  17 Batch  119 / 228  Training Loss  2.008426599786617e-05\n","Epoch  17 Batch  120 / 228  Training Loss  2.3750060790916905e-05\n","Epoch  17 Batch  121 / 228  Training Loss  3.266221028752625e-05\n","Epoch  17 Batch  122 / 228  Training Loss  2.623110412969254e-05\n","Epoch  17 Batch  123 / 228  Training Loss  2.3473552573705092e-05\n","Epoch  17 Batch  124 / 228  Training Loss  1.6188309018616565e-05\n","Epoch  17 Batch  125 / 228  Training Loss  1.7350514099234715e-05\n","Epoch  17 Batch  126 / 228  Training Loss  2.3040758605930023e-05\n","Epoch  17 Batch  127 / 228  Training Loss  2.073863652185537e-05\n","Epoch  17 Batch  128 / 228  Training Loss  1.9038179743802175e-05\n","Epoch  17 Batch  129 / 228  Training Loss  2.446987127768807e-05\n","Epoch  17 Batch  130 / 228  Training Loss  1.555258859298192e-05\n","Epoch  17 Batch  131 / 228  Training Loss  1.7718661183607765e-05\n","Epoch  17 Batch  132 / 228  Training Loss  2.546514770074282e-05\n","Epoch  17 Batch  133 / 228  Training Loss  2.215893255197443e-05\n","Epoch  17 Batch  134 / 228  Training Loss  2.6073295884998515e-05\n","Epoch  17 Batch  135 / 228  Training Loss  2.310982017661445e-05\n","Epoch  17 Batch  136 / 228  Training Loss  3.114293213002384e-05\n","Epoch  17 Batch  137 / 228  Training Loss  1.7006594134727493e-05\n","Epoch  17 Batch  138 / 228  Training Loss  2.6433830498717725e-05\n","Epoch  17 Batch  139 / 228  Training Loss  2.278291321999859e-05\n","Epoch  17 Batch  140 / 228  Training Loss  2.4195658625103533e-05\n","Epoch  17 Batch  141 / 228  Training Loss  2.7732865419238806e-05\n","Epoch  17 Batch  142 / 228  Training Loss  3.128451498923823e-05\n","Epoch  17 Batch  143 / 228  Training Loss  1.8625782104209065e-05\n","Epoch  17 Batch  144 / 228  Training Loss  2.4114257030305453e-05\n","Epoch  17 Batch  145 / 228  Training Loss  1.99384903680766e-05\n","Epoch  17 Batch  146 / 228  Training Loss  2.3321454136748798e-05\n","Epoch  17 Batch  147 / 228  Training Loss  2.278992542414926e-05\n","Epoch  17 Batch  148 / 228  Training Loss  1.5908955901977606e-05\n","Epoch  17 Batch  149 / 228  Training Loss  1.7252441466553137e-05\n","Epoch  17 Batch  150 / 228  Training Loss  2.467856029397808e-05\n","Epoch  17 Batch  151 / 228  Training Loss  2.596047670522239e-05\n","Epoch  17 Batch  152 / 228  Training Loss  2.506932105461601e-05\n","Epoch  17 Batch  153 / 228  Training Loss  2.0184492314001545e-05\n","Epoch  17 Batch  154 / 228  Training Loss  2.7814990971819498e-05\n","Epoch  17 Batch  155 / 228  Training Loss  1.889925442810636e-05\n","Epoch  17 Batch  156 / 228  Training Loss  1.688982047198806e-05\n","Epoch  17 Batch  157 / 228  Training Loss  2.1155201466172002e-05\n","Epoch  17 Batch  158 / 228  Training Loss  1.782632534741424e-05\n","Epoch  17 Batch  159 / 228  Training Loss  2.1056510377093218e-05\n","Epoch  17 Batch  160 / 228  Training Loss  2.3950376998982392e-05\n","Epoch  17 Batch  161 / 228  Training Loss  2.112321453751065e-05\n","Epoch  17 Batch  162 / 228  Training Loss  3.044655022677034e-05\n","Epoch  17 Batch  163 / 228  Training Loss  2.4385675715166144e-05\n","Epoch  17 Batch  164 / 228  Training Loss  2.1117852156748995e-05\n","Epoch  17 Batch  165 / 228  Training Loss  2.1569087039097212e-05\n","Epoch  17 Batch  166 / 228  Training Loss  1.859734402387403e-05\n","Epoch  17 Batch  167 / 228  Training Loss  1.8512862880015746e-05\n","Epoch  17 Batch  168 / 228  Training Loss  1.146269823948387e-05\n","Epoch  17 Batch  169 / 228  Training Loss  2.1827399905305356e-05\n","Epoch  17 Batch  170 / 228  Training Loss  1.702125337033067e-05\n","Epoch  17 Batch  171 / 228  Training Loss  2.0319594113971107e-05\n","Epoch  17 Batch  172 / 228  Training Loss  2.9167180400690995e-05\n","Epoch  17 Batch  173 / 228  Training Loss  1.861598684627097e-05\n","Epoch  17 Batch  174 / 228  Training Loss  1.7330374248558655e-05\n","Epoch  17 Batch  175 / 228  Training Loss  2.0162729924777523e-05\n","Epoch  17 Batch  176 / 228  Training Loss  1.5221262401610147e-05\n","Epoch  17 Batch  177 / 228  Training Loss  1.8420347259962e-05\n","Epoch  17 Batch  178 / 228  Training Loss  2.2431029719882645e-05\n","Epoch  17 Batch  179 / 228  Training Loss  2.378564386162907e-05\n","Epoch  17 Batch  180 / 228  Training Loss  2.2992338926997036e-05\n","Epoch  17 Batch  181 / 228  Training Loss  1.4691744581796229e-05\n","Epoch  17 Batch  182 / 228  Training Loss  1.867665196186863e-05\n","Epoch  17 Batch  183 / 228  Training Loss  2.2576350602321327e-05\n","Epoch  17 Batch  184 / 228  Training Loss  1.5457862900802866e-05\n","Epoch  17 Batch  185 / 228  Training Loss  1.4869723599986173e-05\n","Epoch  17 Batch  186 / 228  Training Loss  2.0016494090668857e-05\n","Epoch  17 Batch  187 / 228  Training Loss  2.4189663236029446e-05\n","Epoch  17 Batch  188 / 228  Training Loss  1.9896926460205577e-05\n","Epoch  17 Batch  189 / 228  Training Loss  1.857518873293884e-05\n","Epoch  17 Batch  190 / 228  Training Loss  2.490042788849678e-05\n","Epoch  17 Batch  191 / 228  Training Loss  2.119181226589717e-05\n","Epoch  17 Batch  192 / 228  Training Loss  1.7244638002011925e-05\n","Epoch  17 Batch  193 / 228  Training Loss  2.195021625084337e-05\n","Epoch  17 Batch  194 / 228  Training Loss  2.2639938833890483e-05\n","Epoch  17 Batch  195 / 228  Training Loss  2.6476132916286588e-05\n","Epoch  17 Batch  196 / 228  Training Loss  2.4827775632729754e-05\n","Epoch  17 Batch  197 / 228  Training Loss  2.2284752049017698e-05\n","Epoch  17 Batch  198 / 228  Training Loss  2.6401225113659166e-05\n","Epoch  17 Batch  199 / 228  Training Loss  2.719153962971177e-05\n","Epoch  17 Batch  200 / 228  Training Loss  2.1387746528489515e-05\n","Epoch  17 Batch  201 / 228  Training Loss  1.5786998119438067e-05\n","Epoch  17 Batch  202 / 228  Training Loss  2.4434988517896272e-05\n","Epoch  17 Batch  203 / 228  Training Loss  1.7322128769592382e-05\n","Epoch  17 Batch  204 / 228  Training Loss  1.9821227397187613e-05\n","Epoch  17 Batch  205 / 228  Training Loss  2.1413350623333827e-05\n","Epoch  17 Batch  206 / 228  Training Loss  1.862852332124021e-05\n","Epoch  17 Batch  207 / 228  Training Loss  1.5446781617356464e-05\n","Epoch  17 Batch  208 / 228  Training Loss  2.9249084036564454e-05\n","Epoch  17 Batch  209 / 228  Training Loss  1.4771178030059673e-05\n","Epoch  17 Batch  210 / 228  Training Loss  1.626040830160491e-05\n","Epoch  17 Batch  211 / 228  Training Loss  2.312059405085165e-05\n","Epoch  17 Batch  212 / 228  Training Loss  2.0149045667494647e-05\n","Epoch  17 Batch  213 / 228  Training Loss  1.8164664652431384e-05\n","Epoch  17 Batch  214 / 228  Training Loss  2.1782083422294818e-05\n","Epoch  17 Batch  215 / 228  Training Loss  2.1809391910210252e-05\n","Epoch  17 Batch  216 / 228  Training Loss  1.910437276819721e-05\n","Epoch  17 Batch  217 / 228  Training Loss  1.2791341760021169e-05\n","Epoch  17 Batch  218 / 228  Training Loss  1.9344070096849464e-05\n","Epoch  17 Batch  219 / 228  Training Loss  2.3523261916125193e-05\n","Epoch  17 Batch  220 / 228  Training Loss  1.5738611182314344e-05\n","Epoch  17 Batch  221 / 228  Training Loss  2.2088897821959108e-05\n","Epoch  17 Batch  222 / 228  Training Loss  2.5316354367532767e-05\n","Epoch  17 Batch  223 / 228  Training Loss  2.0852134184679016e-05\n","Epoch  17 Batch  224 / 228  Training Loss  1.9200982933398336e-05\n","Epoch  17 Batch  225 / 228  Training Loss  2.144605605280958e-05\n","Epoch  17 Batch  226 / 228  Training Loss  1.6894244254217483e-05\n","Epoch  17 Batch  227 / 228  Training Loss  1.5229110431391746e-05\n","  18    |    -    |   0.000021   | 99.580793\n","----------------------------------------------------------------------\n","Running epoch: 18\n","Epoch  18 Batch  0 / 228  Training Loss  1.584478741278872e-05\n","Epoch  18 Batch  1 / 228  Training Loss  1.8033930246019736e-05\n","Epoch  18 Batch  2 / 228  Training Loss  1.965730552910827e-05\n","Epoch  18 Batch  3 / 228  Training Loss  1.546186103951186e-05\n","Epoch  18 Batch  4 / 228  Training Loss  1.7676611605565995e-05\n","Epoch  18 Batch  5 / 228  Training Loss  1.5959130905685015e-05\n","Epoch  18 Batch  6 / 228  Training Loss  2.1331954485503957e-05\n","Epoch  18 Batch  7 / 228  Training Loss  1.962174974323716e-05\n","Epoch  18 Batch  8 / 228  Training Loss  1.5576903024339117e-05\n","Epoch  18 Batch  9 / 228  Training Loss  2.8538541300804354e-05\n","Epoch  18 Batch  10 / 228  Training Loss  2.3971379050635733e-05\n","Epoch  18 Batch  11 / 228  Training Loss  2.2405693016480654e-05\n","Epoch  18 Batch  12 / 228  Training Loss  2.388973007327877e-05\n","Epoch  18 Batch  13 / 228  Training Loss  2.0899416995234787e-05\n","Epoch  18 Batch  14 / 228  Training Loss  1.9037575839320198e-05\n","Epoch  18 Batch  15 / 228  Training Loss  1.7733389540808275e-05\n","Epoch  18 Batch  16 / 228  Training Loss  1.9972980226157233e-05\n","Epoch  18 Batch  17 / 228  Training Loss  1.5065333172969986e-05\n","Epoch  18 Batch  18 / 228  Training Loss  2.1609823306789622e-05\n","Epoch  18 Batch  19 / 228  Training Loss  2.313691220479086e-05\n","Epoch  18 Batch  20 / 228  Training Loss  1.5827763490960933e-05\n","Epoch  18 Batch  21 / 228  Training Loss  1.946952033904381e-05\n","Epoch  18 Batch  22 / 228  Training Loss  1.9521114154485986e-05\n","Epoch  18 Batch  23 / 228  Training Loss  1.726479604258202e-05\n","Epoch  18 Batch  24 / 228  Training Loss  1.4752373317605816e-05\n","Epoch  18 Batch  25 / 228  Training Loss  1.967904063349124e-05\n","Epoch  18 Batch  26 / 228  Training Loss  2.4217250029323623e-05\n","Epoch  18 Batch  27 / 228  Training Loss  1.6698646504664794e-05\n","Epoch  18 Batch  28 / 228  Training Loss  2.47940952249337e-05\n","Epoch  18 Batch  29 / 228  Training Loss  2.2403979528462514e-05\n","Epoch  18 Batch  30 / 228  Training Loss  1.9343589883646928e-05\n","Epoch  18 Batch  31 / 228  Training Loss  2.0984305592719465e-05\n","Epoch  18 Batch  32 / 228  Training Loss  1.835501825553365e-05\n","Epoch  18 Batch  33 / 228  Training Loss  1.4976171769376379e-05\n","Epoch  18 Batch  34 / 228  Training Loss  1.7383767044520937e-05\n","Epoch  18 Batch  35 / 228  Training Loss  1.9385279301786795e-05\n","Epoch  18 Batch  36 / 228  Training Loss  1.8032678781310096e-05\n","Epoch  18 Batch  37 / 228  Training Loss  1.938437890203204e-05\n","Epoch  18 Batch  38 / 228  Training Loss  3.185947207384743e-05\n","Epoch  18 Batch  39 / 228  Training Loss  1.445186626369832e-05\n","Epoch  18 Batch  40 / 228  Training Loss  2.035461329796817e-05\n","Epoch  18 Batch  41 / 228  Training Loss  2.0476749341469258e-05\n","Epoch  18 Batch  42 / 228  Training Loss  2.332895564904902e-05\n","Epoch  18 Batch  43 / 228  Training Loss  1.5913963579805568e-05\n","Epoch  18 Batch  44 / 228  Training Loss  1.6826303181005642e-05\n","Epoch  18 Batch  45 / 228  Training Loss  2.1137951989658177e-05\n","Epoch  18 Batch  46 / 228  Training Loss  1.800341487978585e-05\n","Epoch  18 Batch  47 / 228  Training Loss  2.076535383821465e-05\n","Epoch  18 Batch  48 / 228  Training Loss  2.6486970455152914e-05\n","Epoch  18 Batch  49 / 228  Training Loss  2.087263055727817e-05\n","Epoch  18 Batch  50 / 228  Training Loss  1.683164737187326e-05\n","Epoch  18 Batch  51 / 228  Training Loss  2.339044658583589e-05\n","Epoch  18 Batch  52 / 228  Training Loss  2.061248778773006e-05\n","Epoch  18 Batch  53 / 228  Training Loss  2.4618148017907515e-05\n","Epoch  18 Batch  54 / 228  Training Loss  1.5032444025564473e-05\n","Epoch  18 Batch  55 / 228  Training Loss  2.328911614313256e-05\n","Epoch  18 Batch  56 / 228  Training Loss  1.4423472748603672e-05\n","Epoch  18 Batch  57 / 228  Training Loss  2.124181810359005e-05\n","Epoch  18 Batch  58 / 228  Training Loss  1.5415182133438066e-05\n","Epoch  18 Batch  59 / 228  Training Loss  1.9634098862297833e-05\n","Epoch  18 Batch  60 / 228  Training Loss  2.231406506325584e-05\n","Epoch  18 Batch  61 / 228  Training Loss  1.8484182874090038e-05\n","Epoch  18 Batch  62 / 228  Training Loss  2.5520168492221273e-05\n","Epoch  18 Batch  63 / 228  Training Loss  2.7589601813815534e-05\n","Epoch  18 Batch  64 / 228  Training Loss  1.6232506823143922e-05\n","Epoch  18 Batch  65 / 228  Training Loss  1.8278888092027046e-05\n","Epoch  18 Batch  66 / 228  Training Loss  1.8531949535827152e-05\n","Epoch  18 Batch  67 / 228  Training Loss  3.0148536097840406e-05\n","Epoch  18 Batch  68 / 228  Training Loss  1.4367722542374395e-05\n","Epoch  18 Batch  69 / 228  Training Loss  1.7597663827473298e-05\n","Epoch  18 Batch  70 / 228  Training Loss  1.539927689009346e-05\n","Epoch  18 Batch  71 / 228  Training Loss  1.6168360161827877e-05\n","Epoch  18 Batch  72 / 228  Training Loss  2.3836684704292566e-05\n","Epoch  18 Batch  73 / 228  Training Loss  1.501089172961656e-05\n","Epoch  18 Batch  74 / 228  Training Loss  2.1989315428072587e-05\n","Epoch  18 Batch  75 / 228  Training Loss  2.2437743609771132e-05\n","Epoch  18 Batch  76 / 228  Training Loss  1.97903900698293e-05\n","Epoch  18 Batch  77 / 228  Training Loss  2.19206322071841e-05\n","Epoch  18 Batch  78 / 228  Training Loss  2.2512234863825142e-05\n","Epoch  18 Batch  79 / 228  Training Loss  2.242355731141288e-05\n","Epoch  18 Batch  80 / 228  Training Loss  1.6897953173611313e-05\n","Epoch  18 Batch  81 / 228  Training Loss  2.613999276945833e-05\n","Epoch  18 Batch  82 / 228  Training Loss  2.3137017706176266e-05\n","Epoch  18 Batch  83 / 228  Training Loss  1.8225115127279423e-05\n","Epoch  18 Batch  84 / 228  Training Loss  1.8336430002818815e-05\n","Epoch  18 Batch  85 / 228  Training Loss  1.8661552530829795e-05\n","Epoch  18 Batch  86 / 228  Training Loss  1.5931860616547056e-05\n","Epoch  18 Batch  87 / 228  Training Loss  2.2537331460625865e-05\n","Epoch  18 Batch  88 / 228  Training Loss  2.382186721661128e-05\n","Epoch  18 Batch  89 / 228  Training Loss  1.6218833479797468e-05\n","Epoch  18 Batch  90 / 228  Training Loss  1.8421338609186932e-05\n","Epoch  18 Batch  91 / 228  Training Loss  2.269281139888335e-05\n","Epoch  18 Batch  92 / 228  Training Loss  1.4126478163234424e-05\n","Epoch  18 Batch  93 / 228  Training Loss  2.528666300349869e-05\n","Epoch  18 Batch  94 / 228  Training Loss  2.1761157768196426e-05\n","Epoch  18 Batch  95 / 228  Training Loss  1.7376416508341208e-05\n","Epoch  18 Batch  96 / 228  Training Loss  1.3159595255274326e-05\n","Epoch  18 Batch  97 / 228  Training Loss  1.6066149328253232e-05\n","Epoch  18 Batch  98 / 228  Training Loss  2.1836571249878034e-05\n","Epoch  18 Batch  99 / 228  Training Loss  1.5165026525210124e-05\n","Epoch  18 Batch  100 / 228  Training Loss  2.0153302102698945e-05\n","Epoch  18 Batch  101 / 228  Training Loss  1.693756530585233e-05\n","Epoch  18 Batch  102 / 228  Training Loss  1.7976042727241293e-05\n","Epoch  18 Batch  103 / 228  Training Loss  2.047554517048411e-05\n","Epoch  18 Batch  104 / 228  Training Loss  2.1287543859216385e-05\n","Epoch  18 Batch  105 / 228  Training Loss  1.9657411030493677e-05\n","Epoch  18 Batch  106 / 228  Training Loss  2.485906043148134e-05\n","Epoch  18 Batch  107 / 228  Training Loss  1.981950299523305e-05\n","Epoch  18 Batch  108 / 228  Training Loss  1.5261935914168134e-05\n","Epoch  18 Batch  109 / 228  Training Loss  2.036955083895009e-05\n","Epoch  18 Batch  110 / 228  Training Loss  1.526758751424495e-05\n","Epoch  18 Batch  111 / 228  Training Loss  1.820070974645205e-05\n","Epoch  18 Batch  112 / 228  Training Loss  1.5665023965993896e-05\n","Epoch  18 Batch  113 / 228  Training Loss  2.2775759134674445e-05\n","Epoch  18 Batch  114 / 228  Training Loss  1.779591002559755e-05\n","Epoch  18 Batch  115 / 228  Training Loss  1.715423059067689e-05\n","Epoch  18 Batch  116 / 228  Training Loss  1.8131555407308042e-05\n","Epoch  18 Batch  117 / 228  Training Loss  2.099812809319701e-05\n","Epoch  18 Batch  118 / 228  Training Loss  1.4828469829808455e-05\n","Epoch  18 Batch  119 / 228  Training Loss  2.2981268557487056e-05\n","Epoch  18 Batch  120 / 228  Training Loss  1.6322939700330608e-05\n","Epoch  18 Batch  121 / 228  Training Loss  1.8625898519530892e-05\n","Epoch  18 Batch  122 / 228  Training Loss  2.9858076231903397e-05\n","Epoch  18 Batch  123 / 228  Training Loss  1.7434371329727583e-05\n","Epoch  18 Batch  124 / 228  Training Loss  2.553098056523595e-05\n","Epoch  18 Batch  125 / 228  Training Loss  2.3455819246009924e-05\n","Epoch  18 Batch  126 / 228  Training Loss  2.0747560483869165e-05\n","Epoch  18 Batch  127 / 228  Training Loss  1.5946687199175358e-05\n","Epoch  18 Batch  128 / 228  Training Loss  2.2345369870890863e-05\n","Epoch  18 Batch  129 / 228  Training Loss  2.1403278879006393e-05\n","Epoch  18 Batch  130 / 228  Training Loss  2.8733667932101525e-05\n","Epoch  18 Batch  131 / 228  Training Loss  2.1274548998917453e-05\n","Epoch  18 Batch  132 / 228  Training Loss  1.588799204910174e-05\n","Epoch  18 Batch  133 / 228  Training Loss  1.2727939974865876e-05\n","Epoch  18 Batch  134 / 228  Training Loss  1.56927781063132e-05\n","Epoch  18 Batch  135 / 228  Training Loss  1.4669935808342416e-05\n","Epoch  18 Batch  136 / 228  Training Loss  1.7096741430577822e-05\n","Epoch  18 Batch  137 / 228  Training Loss  2.0943403796991333e-05\n","Epoch  18 Batch  138 / 228  Training Loss  1.6720607163733803e-05\n","Epoch  18 Batch  139 / 228  Training Loss  1.5489582438021898e-05\n","Epoch  18 Batch  140 / 228  Training Loss  1.7215388652402908e-05\n","Epoch  18 Batch  141 / 228  Training Loss  2.133579801011365e-05\n","Epoch  18 Batch  142 / 228  Training Loss  1.7873036995297298e-05\n","Epoch  18 Batch  143 / 228  Training Loss  1.5899235222605057e-05\n","Epoch  18 Batch  144 / 228  Training Loss  2.6137469831155613e-05\n","Epoch  18 Batch  145 / 228  Training Loss  2.2903375793248415e-05\n","Epoch  18 Batch  146 / 228  Training Loss  1.7309832401224412e-05\n","Epoch  18 Batch  147 / 228  Training Loss  2.4528349968022667e-05\n","Epoch  18 Batch  148 / 228  Training Loss  1.7902948457049206e-05\n","Epoch  18 Batch  149 / 228  Training Loss  1.5263854947988875e-05\n","Epoch  18 Batch  150 / 228  Training Loss  1.760696977726184e-05\n","Epoch  18 Batch  151 / 228  Training Loss  1.4857991118333302e-05\n","Epoch  18 Batch  152 / 228  Training Loss  1.926204640767537e-05\n","Epoch  18 Batch  153 / 228  Training Loss  1.839930882852059e-05\n","Epoch  18 Batch  154 / 228  Training Loss  1.454529137845384e-05\n","Epoch  18 Batch  155 / 228  Training Loss  2.1104127881699242e-05\n","Epoch  18 Batch  156 / 228  Training Loss  2.540697096264921e-05\n","Epoch  18 Batch  157 / 228  Training Loss  2.1068914065835997e-05\n","Epoch  18 Batch  158 / 228  Training Loss  2.2404990886570886e-05\n","Epoch  18 Batch  159 / 228  Training Loss  1.5880639693932608e-05\n","Epoch  18 Batch  160 / 228  Training Loss  2.454899913573172e-05\n","Epoch  18 Batch  161 / 228  Training Loss  1.6306481484207325e-05\n","Epoch  18 Batch  162 / 228  Training Loss  3.065045893890783e-05\n","Epoch  18 Batch  163 / 228  Training Loss  1.709593561827205e-05\n","Epoch  18 Batch  164 / 228  Training Loss  2.040519393631257e-05\n","Epoch  18 Batch  165 / 228  Training Loss  1.7859882063930854e-05\n","Epoch  18 Batch  166 / 228  Training Loss  2.147296254406683e-05\n","Epoch  18 Batch  167 / 228  Training Loss  1.883698678284418e-05\n","Epoch  18 Batch  168 / 228  Training Loss  1.556719325890299e-05\n","Epoch  18 Batch  169 / 228  Training Loss  2.19189914787421e-05\n","Epoch  18 Batch  170 / 228  Training Loss  2.1983774786349386e-05\n","Epoch  18 Batch  171 / 228  Training Loss  1.6672283891239204e-05\n","Epoch  18 Batch  172 / 228  Training Loss  1.9372411770746112e-05\n","Epoch  18 Batch  173 / 228  Training Loss  1.5293326214305125e-05\n","Epoch  18 Batch  174 / 228  Training Loss  1.744852852425538e-05\n","Epoch  18 Batch  175 / 228  Training Loss  1.71899919223506e-05\n","Epoch  18 Batch  176 / 228  Training Loss  2.1140247554285452e-05\n","Epoch  18 Batch  177 / 228  Training Loss  1.4276307410909794e-05\n","Epoch  18 Batch  178 / 228  Training Loss  1.442157918063458e-05\n","Epoch  18 Batch  179 / 228  Training Loss  1.753379183355719e-05\n","Epoch  18 Batch  180 / 228  Training Loss  2.132832378265448e-05\n","Epoch  18 Batch  181 / 228  Training Loss  1.722176602925174e-05\n","Epoch  18 Batch  182 / 228  Training Loss  1.1754444130929187e-05\n","Epoch  18 Batch  183 / 228  Training Loss  1.9940389393013902e-05\n","Epoch  18 Batch  184 / 228  Training Loss  1.7650234440225177e-05\n","Epoch  18 Batch  185 / 228  Training Loss  2.5212468244717456e-05\n","Epoch  18 Batch  186 / 228  Training Loss  1.3479996596288402e-05\n","Epoch  18 Batch  187 / 228  Training Loss  1.498687379353214e-05\n","Epoch  18 Batch  188 / 228  Training Loss  1.7716212823870592e-05\n","Epoch  18 Batch  189 / 228  Training Loss  2.0132501958869398e-05\n","Epoch  18 Batch  190 / 228  Training Loss  2.1004445443395525e-05\n","Epoch  18 Batch  191 / 228  Training Loss  2.6221392545267008e-05\n","Epoch  18 Batch  192 / 228  Training Loss  1.704274291114416e-05\n","Epoch  18 Batch  193 / 228  Training Loss  1.7076448784791864e-05\n","Epoch  18 Batch  194 / 228  Training Loss  2.4682964067324065e-05\n","Epoch  18 Batch  195 / 228  Training Loss  1.584706478752196e-05\n","Epoch  18 Batch  196 / 228  Training Loss  1.610273466212675e-05\n","Epoch  18 Batch  197 / 228  Training Loss  2.6564852305455133e-05\n","Epoch  18 Batch  198 / 228  Training Loss  2.9788483516313136e-05\n","Epoch  18 Batch  199 / 228  Training Loss  1.852341665653512e-05\n","Epoch  18 Batch  200 / 228  Training Loss  2.0098750610486604e-05\n","Epoch  18 Batch  201 / 228  Training Loss  2.1191746782278642e-05\n","Epoch  18 Batch  202 / 228  Training Loss  2.0103951101191342e-05\n","Epoch  18 Batch  203 / 228  Training Loss  2.2801879822509363e-05\n","Epoch  18 Batch  204 / 228  Training Loss  2.0340266928542405e-05\n","Epoch  18 Batch  205 / 228  Training Loss  1.7960115656023845e-05\n","Epoch  18 Batch  206 / 228  Training Loss  1.6033449355745688e-05\n","Epoch  18 Batch  207 / 228  Training Loss  1.5242059816955589e-05\n","Epoch  18 Batch  208 / 228  Training Loss  1.334338139713509e-05\n","Epoch  18 Batch  209 / 228  Training Loss  2.237814260297455e-05\n","Epoch  18 Batch  210 / 228  Training Loss  1.962730493687559e-05\n","Epoch  18 Batch  211 / 228  Training Loss  1.6179506928892806e-05\n","Epoch  18 Batch  212 / 228  Training Loss  1.9359958969289437e-05\n","Epoch  18 Batch  213 / 228  Training Loss  1.700114444247447e-05\n","Epoch  18 Batch  214 / 228  Training Loss  2.173310895159375e-05\n","Epoch  18 Batch  215 / 228  Training Loss  1.6031914128689095e-05\n","Epoch  18 Batch  216 / 228  Training Loss  1.6875259461812675e-05\n","Epoch  18 Batch  217 / 228  Training Loss  1.7398380805389024e-05\n","Epoch  18 Batch  218 / 228  Training Loss  1.979030275833793e-05\n","Epoch  18 Batch  219 / 228  Training Loss  1.6324107491527684e-05\n","Epoch  18 Batch  220 / 228  Training Loss  2.3874901671661064e-05\n","Epoch  18 Batch  221 / 228  Training Loss  1.7287253285758197e-05\n","Epoch  18 Batch  222 / 228  Training Loss  2.2712600184604526e-05\n","Epoch  18 Batch  223 / 228  Training Loss  1.872971552074887e-05\n","Epoch  18 Batch  224 / 228  Training Loss  1.5458703273907304e-05\n","Epoch  18 Batch  225 / 228  Training Loss  1.624577635084279e-05\n","Epoch  18 Batch  226 / 228  Training Loss  2.332605072297156e-05\n","Epoch  18 Batch  227 / 228  Training Loss  1.6223693819483742e-05\n","  19    |    -    |   0.000019   | 99.580793\n","----------------------------------------------------------------------\n","Running epoch: 19\n","Epoch  19 Batch  0 / 228  Training Loss  1.2410737326717936e-05\n","Epoch  19 Batch  1 / 228  Training Loss  1.3841772670275532e-05\n","Epoch  19 Batch  2 / 228  Training Loss  2.6694102416513488e-05\n","Epoch  19 Batch  3 / 228  Training Loss  2.2733267542207614e-05\n","Epoch  19 Batch  4 / 228  Training Loss  2.0943380150129087e-05\n","Epoch  19 Batch  5 / 228  Training Loss  1.5019491911516525e-05\n","Epoch  19 Batch  6 / 228  Training Loss  1.702544977888465e-05\n","Epoch  19 Batch  7 / 228  Training Loss  1.4693403500132263e-05\n","Epoch  19 Batch  8 / 228  Training Loss  1.7482434486737475e-05\n","Epoch  19 Batch  9 / 228  Training Loss  1.7770196791389026e-05\n","Epoch  19 Batch  10 / 228  Training Loss  2.057106394204311e-05\n","Epoch  19 Batch  11 / 228  Training Loss  2.208353362220805e-05\n","Epoch  19 Batch  12 / 228  Training Loss  1.9712413632078096e-05\n","Epoch  19 Batch  13 / 228  Training Loss  1.3076432878733613e-05\n","Epoch  19 Batch  14 / 228  Training Loss  1.598047310835682e-05\n","Epoch  19 Batch  15 / 228  Training Loss  2.370398215134628e-05\n","Epoch  19 Batch  16 / 228  Training Loss  1.9638697267509997e-05\n","Epoch  19 Batch  17 / 228  Training Loss  1.4773929251532536e-05\n","Epoch  19 Batch  18 / 228  Training Loss  1.3475693776854314e-05\n","Epoch  19 Batch  19 / 228  Training Loss  2.2687638193019666e-05\n","Epoch  19 Batch  20 / 228  Training Loss  2.4887645849958062e-05\n","Epoch  19 Batch  21 / 228  Training Loss  2.2820266167400405e-05\n","Epoch  19 Batch  22 / 228  Training Loss  1.8742994143394753e-05\n","Epoch  19 Batch  23 / 228  Training Loss  1.5219286069623195e-05\n","Epoch  19 Batch  24 / 228  Training Loss  1.9237249944126233e-05\n","Epoch  19 Batch  25 / 228  Training Loss  2.0039935407112353e-05\n","Epoch  19 Batch  26 / 228  Training Loss  1.753519245539792e-05\n","Epoch  19 Batch  27 / 228  Training Loss  1.8936581909656525e-05\n","Epoch  19 Batch  28 / 228  Training Loss  2.0838630007347092e-05\n","Epoch  19 Batch  29 / 228  Training Loss  1.7398915588273667e-05\n","Epoch  19 Batch  30 / 228  Training Loss  1.4832633496553171e-05\n","Epoch  19 Batch  31 / 228  Training Loss  2.5022420231834985e-05\n","Epoch  19 Batch  32 / 228  Training Loss  1.619875183678232e-05\n","Epoch  19 Batch  33 / 228  Training Loss  2.1814466890646145e-05\n","Epoch  19 Batch  34 / 228  Training Loss  1.732435339363292e-05\n","Epoch  19 Batch  35 / 228  Training Loss  1.8550812455941923e-05\n","Epoch  19 Batch  36 / 228  Training Loss  2.0714267520816065e-05\n","Epoch  19 Batch  37 / 228  Training Loss  2.7812249754788354e-05\n","Epoch  19 Batch  38 / 228  Training Loss  1.4368450138135813e-05\n","Epoch  19 Batch  39 / 228  Training Loss  1.5493320461246185e-05\n","Epoch  19 Batch  40 / 228  Training Loss  1.9741484720725566e-05\n","Epoch  19 Batch  41 / 228  Training Loss  2.007922375923954e-05\n","Epoch  19 Batch  42 / 228  Training Loss  2.1705473045585677e-05\n","Epoch  19 Batch  43 / 228  Training Loss  1.6828551451908424e-05\n","Epoch  19 Batch  44 / 228  Training Loss  2.046873669314664e-05\n","Epoch  19 Batch  45 / 228  Training Loss  1.737491948006209e-05\n","Epoch  19 Batch  46 / 228  Training Loss  2.360058897465933e-05\n","Epoch  19 Batch  47 / 228  Training Loss  2.0361381757538766e-05\n","Epoch  19 Batch  48 / 228  Training Loss  1.8843869838747196e-05\n","Epoch  19 Batch  49 / 228  Training Loss  2.2127549527795054e-05\n","Epoch  19 Batch  50 / 228  Training Loss  1.6485679225297645e-05\n","Epoch  19 Batch  51 / 228  Training Loss  1.853502544690855e-05\n","Epoch  19 Batch  52 / 228  Training Loss  1.8620363334775902e-05\n","Epoch  19 Batch  53 / 228  Training Loss  1.76796929736156e-05\n","Epoch  19 Batch  54 / 228  Training Loss  1.7374171875417233e-05\n","Epoch  19 Batch  55 / 228  Training Loss  1.6893302017706446e-05\n","Epoch  19 Batch  56 / 228  Training Loss  2.1449997802847065e-05\n","Epoch  19 Batch  57 / 228  Training Loss  1.6012250853236765e-05\n","Epoch  19 Batch  58 / 228  Training Loss  1.850062471930869e-05\n","Epoch  19 Batch  59 / 228  Training Loss  1.7537258827360347e-05\n","Epoch  19 Batch  60 / 228  Training Loss  1.6721489373594522e-05\n","Epoch  19 Batch  61 / 228  Training Loss  1.4645238479715772e-05\n","Epoch  19 Batch  62 / 228  Training Loss  1.729111318127252e-05\n","Epoch  19 Batch  63 / 228  Training Loss  2.0633102394640446e-05\n","Epoch  19 Batch  64 / 228  Training Loss  1.5636280295439065e-05\n","Epoch  19 Batch  65 / 228  Training Loss  1.8591508705867454e-05\n","Epoch  19 Batch  66 / 228  Training Loss  1.7495447536930442e-05\n","Epoch  19 Batch  67 / 228  Training Loss  1.8355704014538787e-05\n","Epoch  19 Batch  68 / 228  Training Loss  2.0613191736629233e-05\n","Epoch  19 Batch  69 / 228  Training Loss  1.870382038759999e-05\n","Epoch  19 Batch  70 / 228  Training Loss  2.1002802895964123e-05\n","Epoch  19 Batch  71 / 228  Training Loss  1.6042318748077378e-05\n","Epoch  19 Batch  72 / 228  Training Loss  2.046351437456906e-05\n","Epoch  19 Batch  73 / 228  Training Loss  1.588112354511395e-05\n","Epoch  19 Batch  74 / 228  Training Loss  2.3156248062150553e-05\n","Epoch  19 Batch  75 / 228  Training Loss  1.4407996786758304e-05\n","Epoch  19 Batch  76 / 228  Training Loss  1.5256652659445535e-05\n","Epoch  19 Batch  77 / 228  Training Loss  1.537946445751004e-05\n","Epoch  19 Batch  78 / 228  Training Loss  1.4639114851888735e-05\n","Epoch  19 Batch  79 / 228  Training Loss  2.6157524189329706e-05\n","Epoch  19 Batch  80 / 228  Training Loss  1.8311369785806164e-05\n","Epoch  19 Batch  81 / 228  Training Loss  1.7808644770411775e-05\n","Epoch  19 Batch  82 / 228  Training Loss  2.1368430680013262e-05\n","Epoch  19 Batch  83 / 228  Training Loss  2.2226387955015525e-05\n","Epoch  19 Batch  84 / 228  Training Loss  1.872055509011261e-05\n","Epoch  19 Batch  85 / 228  Training Loss  1.7353242583340034e-05\n","Epoch  19 Batch  86 / 228  Training Loss  1.5607956811436452e-05\n","Epoch  19 Batch  87 / 228  Training Loss  1.3534599929698743e-05\n","Epoch  19 Batch  88 / 228  Training Loss  1.6343885363312438e-05\n","Epoch  19 Batch  89 / 228  Training Loss  1.8486131011741236e-05\n","Epoch  19 Batch  90 / 228  Training Loss  1.5110262211237568e-05\n","Epoch  19 Batch  91 / 228  Training Loss  2.095120544254314e-05\n","Epoch  19 Batch  92 / 228  Training Loss  1.7027396097546443e-05\n","Epoch  19 Batch  93 / 228  Training Loss  2.5277515305788256e-05\n","Epoch  19 Batch  94 / 228  Training Loss  1.812800473999232e-05\n","Epoch  19 Batch  95 / 228  Training Loss  2.2621694370172918e-05\n","Epoch  19 Batch  96 / 228  Training Loss  2.130118264176417e-05\n","Epoch  19 Batch  97 / 228  Training Loss  2.2165171685628593e-05\n","Epoch  19 Batch  98 / 228  Training Loss  1.6815927665447816e-05\n","Epoch  19 Batch  99 / 228  Training Loss  1.6865240468177944e-05\n","Epoch  19 Batch  100 / 228  Training Loss  1.7343183571938425e-05\n","Epoch  19 Batch  101 / 228  Training Loss  1.5314184565795586e-05\n","Epoch  19 Batch  102 / 228  Training Loss  1.7719945390126668e-05\n","Epoch  19 Batch  103 / 228  Training Loss  1.9925566448364407e-05\n","Epoch  19 Batch  104 / 228  Training Loss  1.4027564247953705e-05\n","Epoch  19 Batch  105 / 228  Training Loss  1.4335377272800542e-05\n","Epoch  19 Batch  106 / 228  Training Loss  1.532752503408119e-05\n","Epoch  19 Batch  107 / 228  Training Loss  1.50315345308627e-05\n","Epoch  19 Batch  108 / 228  Training Loss  1.8123646441381425e-05\n","Epoch  19 Batch  109 / 228  Training Loss  1.3223887435742654e-05\n","Epoch  19 Batch  110 / 228  Training Loss  1.4788647604291327e-05\n","Epoch  19 Batch  111 / 228  Training Loss  1.6777339624240994e-05\n","Epoch  19 Batch  112 / 228  Training Loss  2.0403138478286564e-05\n","Epoch  19 Batch  113 / 228  Training Loss  1.2503800462582149e-05\n","Epoch  19 Batch  114 / 228  Training Loss  1.7315865989075974e-05\n","Epoch  19 Batch  115 / 228  Training Loss  1.8131291653844528e-05\n","Epoch  19 Batch  116 / 228  Training Loss  1.8072400052915327e-05\n","Epoch  19 Batch  117 / 228  Training Loss  1.6263971701846458e-05\n","Epoch  19 Batch  118 / 228  Training Loss  1.737110324029345e-05\n","Epoch  19 Batch  119 / 228  Training Loss  1.9335546312504448e-05\n","Epoch  19 Batch  120 / 228  Training Loss  1.6350533769582398e-05\n","Epoch  19 Batch  121 / 228  Training Loss  2.0210802176734433e-05\n","Epoch  19 Batch  122 / 228  Training Loss  2.1974685296299867e-05\n","Epoch  19 Batch  123 / 228  Training Loss  1.599494498805143e-05\n","Epoch  19 Batch  124 / 228  Training Loss  1.809913919714745e-05\n","Epoch  19 Batch  125 / 228  Training Loss  1.5670939319534227e-05\n","Epoch  19 Batch  126 / 228  Training Loss  1.4768197615921963e-05\n","Epoch  19 Batch  127 / 228  Training Loss  1.99128407984972e-05\n","Epoch  19 Batch  128 / 228  Training Loss  1.3710852726944722e-05\n","Epoch  19 Batch  129 / 228  Training Loss  2.1171736079850234e-05\n","Epoch  19 Batch  130 / 228  Training Loss  1.4704157365486026e-05\n","Epoch  19 Batch  131 / 228  Training Loss  1.1777778126997873e-05\n","Epoch  19 Batch  132 / 228  Training Loss  1.8056081898976117e-05\n","Epoch  19 Batch  133 / 228  Training Loss  1.7007430869853124e-05\n","Epoch  19 Batch  134 / 228  Training Loss  2.3568270989926532e-05\n","Epoch  19 Batch  135 / 228  Training Loss  2.1299561922205612e-05\n","Epoch  19 Batch  136 / 228  Training Loss  1.7622516679693945e-05\n","Epoch  19 Batch  137 / 228  Training Loss  1.3745613614446484e-05\n","Epoch  19 Batch  138 / 228  Training Loss  1.9860050088027492e-05\n","Epoch  19 Batch  139 / 228  Training Loss  1.2427815818227828e-05\n","Epoch  19 Batch  140 / 228  Training Loss  1.4969322364777327e-05\n","Epoch  19 Batch  141 / 228  Training Loss  2.3213182430481538e-05\n","Epoch  19 Batch  142 / 228  Training Loss  1.174705357698258e-05\n","Epoch  19 Batch  143 / 228  Training Loss  1.7242437024833634e-05\n","Epoch  19 Batch  144 / 228  Training Loss  1.6193860574276187e-05\n","Epoch  19 Batch  145 / 228  Training Loss  2.0422292436705902e-05\n","Epoch  19 Batch  146 / 228  Training Loss  1.0494230991753284e-05\n","Epoch  19 Batch  147 / 228  Training Loss  1.935821637744084e-05\n","Epoch  19 Batch  148 / 228  Training Loss  1.919688656926155e-05\n","Epoch  19 Batch  149 / 228  Training Loss  1.5006744433776475e-05\n","Epoch  19 Batch  150 / 228  Training Loss  1.842914571170695e-05\n","Epoch  19 Batch  151 / 228  Training Loss  1.6978421626845375e-05\n","Epoch  19 Batch  152 / 228  Training Loss  1.6163568943738937e-05\n","Epoch  19 Batch  153 / 228  Training Loss  1.5366134903160855e-05\n","Epoch  19 Batch  154 / 228  Training Loss  2.0577084796968848e-05\n","Epoch  19 Batch  155 / 228  Training Loss  2.1139741875231266e-05\n","Epoch  19 Batch  156 / 228  Training Loss  1.3348520951694809e-05\n","Epoch  19 Batch  157 / 228  Training Loss  1.7601554645807482e-05\n","Epoch  19 Batch  158 / 228  Training Loss  1.6538948329980485e-05\n","Epoch  19 Batch  159 / 228  Training Loss  1.8091875972459093e-05\n","Epoch  19 Batch  160 / 228  Training Loss  2.3876251361798495e-05\n","Epoch  19 Batch  161 / 228  Training Loss  1.88258654816309e-05\n","Epoch  19 Batch  162 / 228  Training Loss  1.6207737644435838e-05\n","Epoch  19 Batch  163 / 228  Training Loss  1.5395708032883704e-05\n","Epoch  19 Batch  164 / 228  Training Loss  1.8138081941287965e-05\n","Epoch  19 Batch  165 / 228  Training Loss  1.7351308997604065e-05\n","Epoch  19 Batch  166 / 228  Training Loss  1.2867382793047e-05\n","Epoch  19 Batch  167 / 228  Training Loss  1.4268616723711602e-05\n","Epoch  19 Batch  168 / 228  Training Loss  2.0543575374176726e-05\n","Epoch  19 Batch  169 / 228  Training Loss  1.773779877112247e-05\n","Epoch  19 Batch  170 / 228  Training Loss  2.2328575141727924e-05\n","Epoch  19 Batch  171 / 228  Training Loss  1.6882420823094435e-05\n","Epoch  19 Batch  172 / 228  Training Loss  1.5088893633219413e-05\n","Epoch  19 Batch  173 / 228  Training Loss  2.38089487538673e-05\n","Epoch  19 Batch  174 / 228  Training Loss  2.7430021873442456e-05\n","Epoch  19 Batch  175 / 228  Training Loss  1.4520100194204133e-05\n","Epoch  19 Batch  176 / 228  Training Loss  2.1920865037827753e-05\n","Epoch  19 Batch  177 / 228  Training Loss  1.772666655597277e-05\n","Epoch  19 Batch  178 / 228  Training Loss  1.72134350577835e-05\n","Epoch  19 Batch  179 / 228  Training Loss  1.3498455700755585e-05\n","Epoch  19 Batch  180 / 228  Training Loss  1.7388689229846932e-05\n","Epoch  19 Batch  181 / 228  Training Loss  1.804114981496241e-05\n","Epoch  19 Batch  182 / 228  Training Loss  1.595760477357544e-05\n","Epoch  19 Batch  183 / 228  Training Loss  1.6649082681396976e-05\n","Epoch  19 Batch  184 / 228  Training Loss  1.2427450201357715e-05\n","Epoch  19 Batch  185 / 228  Training Loss  1.3851829862687737e-05\n","Epoch  19 Batch  186 / 228  Training Loss  1.2518605217337608e-05\n","Epoch  19 Batch  187 / 228  Training Loss  1.2221474207763094e-05\n","Epoch  19 Batch  188 / 228  Training Loss  1.6418696759501472e-05\n","Epoch  19 Batch  189 / 228  Training Loss  1.678886474110186e-05\n","Epoch  19 Batch  190 / 228  Training Loss  1.934354440891184e-05\n","Epoch  19 Batch  191 / 228  Training Loss  2.256251718790736e-05\n","Epoch  19 Batch  192 / 228  Training Loss  1.5105641978152562e-05\n","Epoch  19 Batch  193 / 228  Training Loss  1.9411814719205722e-05\n","Epoch  19 Batch  194 / 228  Training Loss  1.762614192557521e-05\n","Epoch  19 Batch  195 / 228  Training Loss  1.8528822693042457e-05\n","Epoch  19 Batch  196 / 228  Training Loss  1.4848512364551425e-05\n","Epoch  19 Batch  197 / 228  Training Loss  1.84316322702216e-05\n","Epoch  19 Batch  198 / 228  Training Loss  1.6234149370575324e-05\n","Epoch  19 Batch  199 / 228  Training Loss  1.456459540349897e-05\n","Epoch  19 Batch  200 / 228  Training Loss  1.3973118257126771e-05\n","Epoch  19 Batch  201 / 228  Training Loss  1.9153347238898277e-05\n","Epoch  19 Batch  202 / 228  Training Loss  2.352017691009678e-05\n","Epoch  19 Batch  203 / 228  Training Loss  2.1406263840617612e-05\n","Epoch  19 Batch  204 / 228  Training Loss  1.780766251613386e-05\n","Epoch  19 Batch  205 / 228  Training Loss  2.172035601688549e-05\n","Epoch  19 Batch  206 / 228  Training Loss  1.940494985319674e-05\n","Epoch  19 Batch  207 / 228  Training Loss  1.5165986951615196e-05\n","Epoch  19 Batch  208 / 228  Training Loss  1.70522544067353e-05\n","Epoch  19 Batch  209 / 228  Training Loss  1.8301676391274668e-05\n","Epoch  19 Batch  210 / 228  Training Loss  1.144346060755197e-05\n","Epoch  19 Batch  211 / 228  Training Loss  1.9776798581006005e-05\n","Epoch  19 Batch  212 / 228  Training Loss  1.6953448721324094e-05\n","Epoch  19 Batch  213 / 228  Training Loss  1.1201458619325422e-05\n","Epoch  19 Batch  214 / 228  Training Loss  1.4342163922265172e-05\n","Epoch  19 Batch  215 / 228  Training Loss  2.4400080292252824e-05\n","Epoch  19 Batch  216 / 228  Training Loss  1.4460461898124777e-05\n","Epoch  19 Batch  217 / 228  Training Loss  1.4730647308169864e-05\n","Epoch  19 Batch  218 / 228  Training Loss  1.6896692613954656e-05\n","Epoch  19 Batch  219 / 228  Training Loss  1.4500263205263764e-05\n","Epoch  19 Batch  220 / 228  Training Loss  1.5801526387804188e-05\n","Epoch  19 Batch  221 / 228  Training Loss  2.2141883164294995e-05\n","Epoch  19 Batch  222 / 228  Training Loss  2.068737376248464e-05\n","Epoch  19 Batch  223 / 228  Training Loss  1.7037153156707063e-05\n","Epoch  19 Batch  224 / 228  Training Loss  1.511707068857504e-05\n","Epoch  19 Batch  225 / 228  Training Loss  1.1090914085798431e-05\n","Epoch  19 Batch  226 / 228  Training Loss  2.7791018510470167e-05\n","Epoch  19 Batch  227 / 228  Training Loss  2.014754863921553e-05\n","  20    |    -    |   0.000018   | 99.466463\n","----------------------------------------------------------------------\n","Running epoch: 20\n","Epoch  20 Batch  0 / 228  Training Loss  1.912447260110639e-05\n","Epoch  20 Batch  1 / 228  Training Loss  2.0918478185194544e-05\n","Epoch  20 Batch  2 / 228  Training Loss  1.8557348084868863e-05\n","Epoch  20 Batch  3 / 228  Training Loss  1.961630732694175e-05\n","Epoch  20 Batch  4 / 228  Training Loss  1.5133849046833348e-05\n","Epoch  20 Batch  5 / 228  Training Loss  1.7132597349700518e-05\n","Epoch  20 Batch  6 / 228  Training Loss  2.0809657144127414e-05\n","Epoch  20 Batch  7 / 228  Training Loss  2.013851553783752e-05\n","Epoch  20 Batch  8 / 228  Training Loss  1.1840035767818335e-05\n","Epoch  20 Batch  9 / 228  Training Loss  1.2308324585319497e-05\n","Epoch  20 Batch  10 / 228  Training Loss  1.6470858099637553e-05\n","Epoch  20 Batch  11 / 228  Training Loss  1.425397476850776e-05\n","Epoch  20 Batch  12 / 228  Training Loss  2.043917811533902e-05\n","Epoch  20 Batch  13 / 228  Training Loss  1.2019459973089397e-05\n","Epoch  20 Batch  14 / 228  Training Loss  1.6564305042265914e-05\n","Epoch  20 Batch  15 / 228  Training Loss  1.1019908924936317e-05\n","Epoch  20 Batch  16 / 228  Training Loss  1.5565850844723172e-05\n","Epoch  20 Batch  17 / 228  Training Loss  1.5727568097645417e-05\n","Epoch  20 Batch  18 / 228  Training Loss  2.1787092919112183e-05\n","Epoch  20 Batch  19 / 228  Training Loss  1.9323244487168267e-05\n","Epoch  20 Batch  20 / 228  Training Loss  2.179344664909877e-05\n","Epoch  20 Batch  21 / 228  Training Loss  2.1944808395346627e-05\n","Epoch  20 Batch  22 / 228  Training Loss  1.3237838174973149e-05\n","Epoch  20 Batch  23 / 228  Training Loss  1.6289404811686836e-05\n","Epoch  20 Batch  24 / 228  Training Loss  1.1691352483467199e-05\n","Epoch  20 Batch  25 / 228  Training Loss  1.7460140952607617e-05\n","Epoch  20 Batch  26 / 228  Training Loss  1.9448736566118896e-05\n","Epoch  20 Batch  27 / 228  Training Loss  1.786729808372911e-05\n","Epoch  20 Batch  28 / 228  Training Loss  2.043175118160434e-05\n","Epoch  20 Batch  29 / 228  Training Loss  2.0096787920920178e-05\n","Epoch  20 Batch  30 / 228  Training Loss  1.44943314808188e-05\n","Epoch  20 Batch  31 / 228  Training Loss  1.8549337255535647e-05\n","Epoch  20 Batch  32 / 228  Training Loss  1.6531252185814083e-05\n","Epoch  20 Batch  33 / 228  Training Loss  1.7565860616741702e-05\n","Epoch  20 Batch  34 / 228  Training Loss  1.673616498010233e-05\n","Epoch  20 Batch  35 / 228  Training Loss  1.6020203474909067e-05\n","Epoch  20 Batch  36 / 228  Training Loss  1.6643620256218128e-05\n","Epoch  20 Batch  37 / 228  Training Loss  1.561573299113661e-05\n","Epoch  20 Batch  38 / 228  Training Loss  1.6506472093169577e-05\n","Epoch  20 Batch  39 / 228  Training Loss  2.0508721718215384e-05\n","Epoch  20 Batch  40 / 228  Training Loss  1.7341937564196996e-05\n","Epoch  20 Batch  41 / 228  Training Loss  1.4866658602841198e-05\n","Epoch  20 Batch  42 / 228  Training Loss  1.8437600374454632e-05\n","Epoch  20 Batch  43 / 228  Training Loss  1.800131576601416e-05\n","Epoch  20 Batch  44 / 228  Training Loss  1.4924425158824306e-05\n","Epoch  20 Batch  45 / 228  Training Loss  1.5064630133565515e-05\n","Epoch  20 Batch  46 / 228  Training Loss  1.4142254258331377e-05\n","Epoch  20 Batch  47 / 228  Training Loss  1.9576758859329857e-05\n","Epoch  20 Batch  48 / 228  Training Loss  1.1871462447743397e-05\n","Epoch  20 Batch  49 / 228  Training Loss  1.699271160759963e-05\n","Epoch  20 Batch  50 / 228  Training Loss  1.7229662262252532e-05\n","Epoch  20 Batch  51 / 228  Training Loss  1.765663364494685e-05\n","Epoch  20 Batch  52 / 228  Training Loss  1.865495869424194e-05\n","Epoch  20 Batch  53 / 228  Training Loss  1.1043007361877244e-05\n","Epoch  20 Batch  54 / 228  Training Loss  1.9627932488219813e-05\n","Epoch  20 Batch  55 / 228  Training Loss  2.184289405704476e-05\n","Epoch  20 Batch  56 / 228  Training Loss  1.9273596990387887e-05\n","Epoch  20 Batch  57 / 228  Training Loss  2.1422460122266784e-05\n","Epoch  20 Batch  58 / 228  Training Loss  1.23237668958609e-05\n","Epoch  20 Batch  59 / 228  Training Loss  1.981106834136881e-05\n","Epoch  20 Batch  60 / 228  Training Loss  1.6450565453851596e-05\n","Epoch  20 Batch  61 / 228  Training Loss  1.779548983904533e-05\n","Epoch  20 Batch  62 / 228  Training Loss  1.5282372260116972e-05\n","Epoch  20 Batch  63 / 228  Training Loss  1.718858584354166e-05\n","Epoch  20 Batch  64 / 228  Training Loss  1.3373610272537917e-05\n","Epoch  20 Batch  65 / 228  Training Loss  1.561704630148597e-05\n","Epoch  20 Batch  66 / 228  Training Loss  1.3791835044685286e-05\n","Epoch  20 Batch  67 / 228  Training Loss  1.8243752492708154e-05\n","Epoch  20 Batch  68 / 228  Training Loss  1.4614461179007776e-05\n","Epoch  20 Batch  69 / 228  Training Loss  1.7294198187300935e-05\n","Epoch  20 Batch  70 / 228  Training Loss  1.450769468647195e-05\n","Epoch  20 Batch  71 / 228  Training Loss  1.8047989215119742e-05\n","Epoch  20 Batch  72 / 228  Training Loss  1.4493064554699231e-05\n","Epoch  20 Batch  73 / 228  Training Loss  1.388702821714105e-05\n","Epoch  20 Batch  74 / 228  Training Loss  1.5709360013715923e-05\n","Epoch  20 Batch  75 / 228  Training Loss  1.6645295545458794e-05\n","Epoch  20 Batch  76 / 228  Training Loss  2.2211628674995154e-05\n","Epoch  20 Batch  77 / 228  Training Loss  1.72377513081301e-05\n","Epoch  20 Batch  78 / 228  Training Loss  1.7349673726130277e-05\n","Epoch  20 Batch  79 / 228  Training Loss  2.040714207396377e-05\n","Epoch  20 Batch  80 / 228  Training Loss  1.9229406461818144e-05\n","Epoch  20 Batch  81 / 228  Training Loss  1.661833084654063e-05\n","Epoch  20 Batch  82 / 228  Training Loss  1.649736259423662e-05\n","Epoch  20 Batch  83 / 228  Training Loss  2.026638867391739e-05\n","Epoch  20 Batch  84 / 228  Training Loss  1.9368544599274173e-05\n","Epoch  20 Batch  85 / 228  Training Loss  1.266815252165543e-05\n","Epoch  20 Batch  86 / 228  Training Loss  1.812598384276498e-05\n","Epoch  20 Batch  87 / 228  Training Loss  1.834219801821746e-05\n","Epoch  20 Batch  88 / 228  Training Loss  1.5351992260548286e-05\n","Epoch  20 Batch  89 / 228  Training Loss  1.3394914276432246e-05\n","Epoch  20 Batch  90 / 228  Training Loss  1.467845868319273e-05\n","Epoch  20 Batch  91 / 228  Training Loss  1.6722246073186398e-05\n","Epoch  20 Batch  92 / 228  Training Loss  1.564264493936207e-05\n","Epoch  20 Batch  93 / 228  Training Loss  1.9060247723245993e-05\n","Epoch  20 Batch  94 / 228  Training Loss  1.5407589671667665e-05\n","Epoch  20 Batch  95 / 228  Training Loss  1.83816173375817e-05\n","Epoch  20 Batch  96 / 228  Training Loss  1.282656467083143e-05\n","Epoch  20 Batch  97 / 228  Training Loss  1.625408731342759e-05\n","Epoch  20 Batch  98 / 228  Training Loss  1.580996649863664e-05\n","Epoch  20 Batch  99 / 228  Training Loss  1.3710955499846023e-05\n","Epoch  20 Batch  100 / 228  Training Loss  1.6193147530429997e-05\n","Epoch  20 Batch  101 / 228  Training Loss  1.6353620594600216e-05\n","Epoch  20 Batch  102 / 228  Training Loss  1.625879122002516e-05\n","Epoch  20 Batch  103 / 228  Training Loss  1.3479415429173969e-05\n","Epoch  20 Batch  104 / 228  Training Loss  1.6003981727408245e-05\n","Epoch  20 Batch  105 / 228  Training Loss  2.024554123636335e-05\n","Epoch  20 Batch  106 / 228  Training Loss  1.2402371794451028e-05\n","Epoch  20 Batch  107 / 228  Training Loss  2.1598913008347154e-05\n","Epoch  20 Batch  108 / 228  Training Loss  1.4278792150435038e-05\n","Epoch  20 Batch  109 / 228  Training Loss  2.1854355509276502e-05\n","Epoch  20 Batch  110 / 228  Training Loss  1.8025255485554226e-05\n","Epoch  20 Batch  111 / 228  Training Loss  1.5625821106368676e-05\n","Epoch  20 Batch  112 / 228  Training Loss  1.4652678146376275e-05\n","Epoch  20 Batch  113 / 228  Training Loss  1.2873992091044784e-05\n","Epoch  20 Batch  114 / 228  Training Loss  1.606143450771924e-05\n","Epoch  20 Batch  115 / 228  Training Loss  1.316190628131153e-05\n","Epoch  20 Batch  116 / 228  Training Loss  1.1731644008250441e-05\n","Epoch  20 Batch  117 / 228  Training Loss  1.228905330208363e-05\n","Epoch  20 Batch  118 / 228  Training Loss  1.47194823512109e-05\n","Epoch  20 Batch  119 / 228  Training Loss  1.3958886484033428e-05\n","Epoch  20 Batch  120 / 228  Training Loss  1.8227598047815263e-05\n","Epoch  20 Batch  121 / 228  Training Loss  1.6548276107641868e-05\n","Epoch  20 Batch  122 / 228  Training Loss  1.7552954886923544e-05\n","Epoch  20 Batch  123 / 228  Training Loss  1.9613984477473423e-05\n","Epoch  20 Batch  124 / 228  Training Loss  1.4138419828668702e-05\n","Epoch  20 Batch  125 / 228  Training Loss  1.5572177289868705e-05\n","Epoch  20 Batch  126 / 228  Training Loss  1.0325149560230784e-05\n","Epoch  20 Batch  127 / 228  Training Loss  1.6174832126125693e-05\n","Epoch  20 Batch  128 / 228  Training Loss  1.6851878172019497e-05\n","Epoch  20 Batch  129 / 228  Training Loss  1.5864057786529884e-05\n","Epoch  20 Batch  130 / 228  Training Loss  2.2033189452486113e-05\n","Epoch  20 Batch  131 / 228  Training Loss  2.0115821826038882e-05\n","Epoch  20 Batch  132 / 228  Training Loss  1.2632257494260557e-05\n","Epoch  20 Batch  133 / 228  Training Loss  1.538501601316966e-05\n","Epoch  20 Batch  134 / 228  Training Loss  1.2443968444131315e-05\n","Epoch  20 Batch  135 / 228  Training Loss  1.3949624189990573e-05\n","Epoch  20 Batch  136 / 228  Training Loss  1.8994140191352926e-05\n","Epoch  20 Batch  137 / 228  Training Loss  1.203344527311856e-05\n","Epoch  20 Batch  138 / 228  Training Loss  1.9079781850450672e-05\n","Epoch  20 Batch  139 / 228  Training Loss  1.7033382391673513e-05\n","Epoch  20 Batch  140 / 228  Training Loss  1.4518795069307089e-05\n","Epoch  20 Batch  141 / 228  Training Loss  1.373165832774248e-05\n","Epoch  20 Batch  142 / 228  Training Loss  1.5334589988924563e-05\n","Epoch  20 Batch  143 / 228  Training Loss  1.850542685133405e-05\n","Epoch  20 Batch  144 / 228  Training Loss  2.291097553097643e-05\n","Epoch  20 Batch  145 / 228  Training Loss  2.1892881704843603e-05\n","Epoch  20 Batch  146 / 228  Training Loss  1.7100743207265623e-05\n","Epoch  20 Batch  147 / 228  Training Loss  1.7078715245588683e-05\n","Epoch  20 Batch  148 / 228  Training Loss  1.9555674953153357e-05\n","Epoch  20 Batch  149 / 228  Training Loss  1.6101732398965396e-05\n","Epoch  20 Batch  150 / 228  Training Loss  1.554668415337801e-05\n","Epoch  20 Batch  151 / 228  Training Loss  2.355299875489436e-05\n","Epoch  20 Batch  152 / 228  Training Loss  1.7070718968170695e-05\n","Epoch  20 Batch  153 / 228  Training Loss  1.3755287909589242e-05\n","Epoch  20 Batch  154 / 228  Training Loss  1.6840192984091118e-05\n","Epoch  20 Batch  155 / 228  Training Loss  2.1119072698638774e-05\n","Epoch  20 Batch  156 / 228  Training Loss  1.2218632036820054e-05\n","Epoch  20 Batch  157 / 228  Training Loss  2.2158463252708316e-05\n","Epoch  20 Batch  158 / 228  Training Loss  1.2247331142134499e-05\n","Epoch  20 Batch  159 / 228  Training Loss  1.3563255379267503e-05\n","Epoch  20 Batch  160 / 228  Training Loss  1.1935166185139678e-05\n","Epoch  20 Batch  161 / 228  Training Loss  1.2497289390012156e-05\n","Epoch  20 Batch  162 / 228  Training Loss  1.5545128917437978e-05\n","Epoch  20 Batch  163 / 228  Training Loss  2.066993693006225e-05\n","Epoch  20 Batch  164 / 228  Training Loss  2.1601390471914783e-05\n","Epoch  20 Batch  165 / 228  Training Loss  1.8519993318477646e-05\n","Epoch  20 Batch  166 / 228  Training Loss  1.7640888472669758e-05\n","Epoch  20 Batch  167 / 228  Training Loss  1.24523739941651e-05\n","Epoch  20 Batch  168 / 228  Training Loss  1.4208172615326475e-05\n","Epoch  20 Batch  169 / 228  Training Loss  1.95551838260144e-05\n","Epoch  20 Batch  170 / 228  Training Loss  1.8520879166317172e-05\n","Epoch  20 Batch  171 / 228  Training Loss  2.057226447504945e-05\n","Epoch  20 Batch  172 / 228  Training Loss  1.821865953388624e-05\n","Epoch  20 Batch  173 / 228  Training Loss  1.4426361303776503e-05\n","Epoch  20 Batch  174 / 228  Training Loss  2.086552558466792e-05\n","Epoch  20 Batch  175 / 228  Training Loss  1.1732077837223187e-05\n","Epoch  20 Batch  176 / 228  Training Loss  1.8587881640996784e-05\n","Epoch  20 Batch  177 / 228  Training Loss  1.6804824554128572e-05\n","Epoch  20 Batch  178 / 228  Training Loss  1.5837031241972e-05\n","Epoch  20 Batch  179 / 228  Training Loss  1.3062723155599087e-05\n","Epoch  20 Batch  180 / 228  Training Loss  1.8405626178719103e-05\n","Epoch  20 Batch  181 / 228  Training Loss  2.116466930601746e-05\n","Epoch  20 Batch  182 / 228  Training Loss  2.732764187385328e-05\n","Epoch  20 Batch  183 / 228  Training Loss  2.301416316186078e-05\n","Epoch  20 Batch  184 / 228  Training Loss  1.5012819858384319e-05\n","Epoch  20 Batch  185 / 228  Training Loss  1.5599551261402667e-05\n","Epoch  20 Batch  186 / 228  Training Loss  1.3659504475072026e-05\n","Epoch  20 Batch  187 / 228  Training Loss  1.7796492102206685e-05\n","Epoch  20 Batch  188 / 228  Training Loss  1.4146085959509946e-05\n","Epoch  20 Batch  189 / 228  Training Loss  9.598458746040706e-06\n","Epoch  20 Batch  190 / 228  Training Loss  1.7457221474614926e-05\n","Epoch  20 Batch  191 / 228  Training Loss  1.638871617615223e-05\n","Epoch  20 Batch  192 / 228  Training Loss  1.249978959094733e-05\n","Epoch  20 Batch  193 / 228  Training Loss  1.2741137652483303e-05\n","Epoch  20 Batch  194 / 228  Training Loss  1.1664811609080061e-05\n","Epoch  20 Batch  195 / 228  Training Loss  1.3403558114077896e-05\n","Epoch  20 Batch  196 / 228  Training Loss  1.4137299331196118e-05\n","Epoch  20 Batch  197 / 228  Training Loss  1.7299425962846726e-05\n","Epoch  20 Batch  198 / 228  Training Loss  1.5998564776964486e-05\n","Epoch  20 Batch  199 / 228  Training Loss  2.3865457478677854e-05\n","Epoch  20 Batch  200 / 228  Training Loss  1.0998511243087705e-05\n","Epoch  20 Batch  201 / 228  Training Loss  2.0523399143712595e-05\n","Epoch  20 Batch  202 / 228  Training Loss  2.460655741742812e-05\n","Epoch  20 Batch  203 / 228  Training Loss  1.519600937172072e-05\n","Epoch  20 Batch  204 / 228  Training Loss  1.766775494616013e-05\n","Epoch  20 Batch  205 / 228  Training Loss  1.409729702572804e-05\n","Epoch  20 Batch  206 / 228  Training Loss  1.301791962760035e-05\n","Epoch  20 Batch  207 / 228  Training Loss  1.767161302268505e-05\n","Epoch  20 Batch  208 / 228  Training Loss  1.1748494216590188e-05\n","Epoch  20 Batch  209 / 228  Training Loss  1.4428563190449495e-05\n","Epoch  20 Batch  210 / 228  Training Loss  1.354532469122205e-05\n","Epoch  20 Batch  211 / 228  Training Loss  1.6716472600819543e-05\n","Epoch  20 Batch  212 / 228  Training Loss  1.605172838026192e-05\n","Epoch  20 Batch  213 / 228  Training Loss  2.0998766558477655e-05\n","Epoch  20 Batch  214 / 228  Training Loss  1.1480086868687067e-05\n","Epoch  20 Batch  215 / 228  Training Loss  1.532307760498952e-05\n","Epoch  20 Batch  216 / 228  Training Loss  1.8854774680221453e-05\n","Epoch  20 Batch  217 / 228  Training Loss  1.6001155017875135e-05\n","Epoch  20 Batch  218 / 228  Training Loss  1.5447581972694024e-05\n","Epoch  20 Batch  219 / 228  Training Loss  1.6746877008699812e-05\n","Epoch  20 Batch  220 / 228  Training Loss  1.5342244296334684e-05\n","Epoch  20 Batch  221 / 228  Training Loss  1.7283655324717984e-05\n","Epoch  20 Batch  222 / 228  Training Loss  1.9505530872265808e-05\n","Epoch  20 Batch  223 / 228  Training Loss  2.2078009351389483e-05\n","Epoch  20 Batch  224 / 228  Training Loss  9.296059033658821e-06\n","Epoch  20 Batch  225 / 228  Training Loss  1.1136666216771118e-05\n","Epoch  20 Batch  226 / 228  Training Loss  1.3314803254615981e-05\n","Epoch  20 Batch  227 / 228  Training Loss  1.377447733830195e-05\n","  21    |    -    |   0.000017   | 99.580793\n","----------------------------------------------------------------------\n","Running epoch: 21\n","Epoch  21 Batch  0 / 228  Training Loss  1.3101936019666027e-05\n","Epoch  21 Batch  1 / 228  Training Loss  1.6915508240344934e-05\n","Epoch  21 Batch  2 / 228  Training Loss  1.3224002032075077e-05\n","Epoch  21 Batch  3 / 228  Training Loss  1.5468020137632266e-05\n","Epoch  21 Batch  4 / 228  Training Loss  1.3900731573812664e-05\n","Epoch  21 Batch  5 / 228  Training Loss  1.603372766112443e-05\n","Epoch  21 Batch  6 / 228  Training Loss  1.409236756444443e-05\n","Epoch  21 Batch  7 / 228  Training Loss  1.3458986359182745e-05\n","Epoch  21 Batch  8 / 228  Training Loss  1.2884302122984082e-05\n","Epoch  21 Batch  9 / 228  Training Loss  9.832254363573156e-06\n","Epoch  21 Batch  10 / 228  Training Loss  1.403437363478588e-05\n","Epoch  21 Batch  11 / 228  Training Loss  1.260231056221528e-05\n","Epoch  21 Batch  12 / 228  Training Loss  1.3431299521471374e-05\n","Epoch  21 Batch  13 / 228  Training Loss  1.4100316548137926e-05\n","Epoch  21 Batch  14 / 228  Training Loss  1.275782869925024e-05\n","Epoch  21 Batch  15 / 228  Training Loss  1.3116796253598295e-05\n","Epoch  21 Batch  16 / 228  Training Loss  1.5140714822337031e-05\n","Epoch  21 Batch  17 / 228  Training Loss  9.832993782765698e-06\n","Epoch  21 Batch  18 / 228  Training Loss  1.7368720364174806e-05\n","Epoch  21 Batch  19 / 228  Training Loss  1.4896798347763252e-05\n","Epoch  21 Batch  20 / 228  Training Loss  1.4833787645329721e-05\n","Epoch  21 Batch  21 / 228  Training Loss  1.670191704761237e-05\n","Epoch  21 Batch  22 / 228  Training Loss  1.6419051462435164e-05\n","Epoch  21 Batch  23 / 228  Training Loss  1.2951668395544402e-05\n","Epoch  21 Batch  24 / 228  Training Loss  1.8308912331121974e-05\n","Epoch  21 Batch  25 / 228  Training Loss  1.735534351610113e-05\n","Epoch  21 Batch  26 / 228  Training Loss  1.785184576874599e-05\n","Epoch  21 Batch  27 / 228  Training Loss  1.590463398315478e-05\n","Epoch  21 Batch  28 / 228  Training Loss  1.3582857718574814e-05\n","Epoch  21 Batch  29 / 228  Training Loss  1.1318753422528971e-05\n","Epoch  21 Batch  30 / 228  Training Loss  1.617260932107456e-05\n","Epoch  21 Batch  31 / 228  Training Loss  1.1912499758182094e-05\n","Epoch  21 Batch  32 / 228  Training Loss  1.592601984157227e-05\n","Epoch  21 Batch  33 / 228  Training Loss  1.4194677532941569e-05\n","Epoch  21 Batch  34 / 228  Training Loss  1.3429259524855297e-05\n","Epoch  21 Batch  35 / 228  Training Loss  1.5313544281525537e-05\n","Epoch  21 Batch  36 / 228  Training Loss  1.1358179108356126e-05\n","Epoch  21 Batch  37 / 228  Training Loss  1.654402876738459e-05\n","Epoch  21 Batch  38 / 228  Training Loss  1.739700383041054e-05\n","Epoch  21 Batch  39 / 228  Training Loss  1.5821104170754552e-05\n","Epoch  21 Batch  40 / 228  Training Loss  1.1946745871682651e-05\n","Epoch  21 Batch  41 / 228  Training Loss  1.799274468794465e-05\n","Epoch  21 Batch  42 / 228  Training Loss  1.4799557902733795e-05\n","Epoch  21 Batch  43 / 228  Training Loss  1.641487324377522e-05\n","Epoch  21 Batch  44 / 228  Training Loss  1.31792812680942e-05\n","Epoch  21 Batch  45 / 228  Training Loss  2.0506784494500607e-05\n","Epoch  21 Batch  46 / 228  Training Loss  1.5961661119945347e-05\n","Epoch  21 Batch  47 / 228  Training Loss  1.583313860464841e-05\n","Epoch  21 Batch  48 / 228  Training Loss  1.4387825103767682e-05\n","Epoch  21 Batch  49 / 228  Training Loss  1.4993791410233825e-05\n","Epoch  21 Batch  50 / 228  Training Loss  1.3241407941677608e-05\n","Epoch  21 Batch  51 / 228  Training Loss  1.2666690054174978e-05\n","Epoch  21 Batch  52 / 228  Training Loss  1.4404457942873705e-05\n","Epoch  21 Batch  53 / 228  Training Loss  1.7052501789294183e-05\n","Epoch  21 Batch  54 / 228  Training Loss  1.9454224457149394e-05\n","Epoch  21 Batch  55 / 228  Training Loss  1.5876687029958703e-05\n","Epoch  21 Batch  56 / 228  Training Loss  1.6737994883442298e-05\n","Epoch  21 Batch  57 / 228  Training Loss  1.415969200024847e-05\n","Epoch  21 Batch  58 / 228  Training Loss  1.1700790309987497e-05\n","Epoch  21 Batch  59 / 228  Training Loss  2.0971341655240394e-05\n","Epoch  21 Batch  60 / 228  Training Loss  1.773758231138345e-05\n","Epoch  21 Batch  61 / 228  Training Loss  1.3417617083177902e-05\n","Epoch  21 Batch  62 / 228  Training Loss  1.068606161425123e-05\n","Epoch  21 Batch  63 / 228  Training Loss  1.895914647320751e-05\n","Epoch  21 Batch  64 / 228  Training Loss  1.2810774023819249e-05\n","Epoch  21 Batch  65 / 228  Training Loss  1.9909180991817266e-05\n","Epoch  21 Batch  66 / 228  Training Loss  1.564250123919919e-05\n","Epoch  21 Batch  67 / 228  Training Loss  2.148055864381604e-05\n","Epoch  21 Batch  68 / 228  Training Loss  1.3402345757640433e-05\n","Epoch  21 Batch  69 / 228  Training Loss  2.117784424626734e-05\n","Epoch  21 Batch  70 / 228  Training Loss  1.2883382623840589e-05\n","Epoch  21 Batch  71 / 228  Training Loss  1.49484812936862e-05\n","Epoch  21 Batch  72 / 228  Training Loss  1.402778707415564e-05\n","Epoch  21 Batch  73 / 228  Training Loss  9.50400408328278e-06\n","Epoch  21 Batch  74 / 228  Training Loss  1.046818488248391e-05\n","Epoch  21 Batch  75 / 228  Training Loss  1.7134863810497336e-05\n","Epoch  21 Batch  76 / 228  Training Loss  2.1533087419811636e-05\n","Epoch  21 Batch  77 / 228  Training Loss  1.3413924534688704e-05\n","Epoch  21 Batch  78 / 228  Training Loss  1.5349232853623107e-05\n","Epoch  21 Batch  79 / 228  Training Loss  1.787839937605895e-05\n","Epoch  21 Batch  80 / 228  Training Loss  9.70748715189984e-06\n","Epoch  21 Batch  81 / 228  Training Loss  1.3063016012893058e-05\n","Epoch  21 Batch  82 / 228  Training Loss  1.6811167370178737e-05\n","Epoch  21 Batch  83 / 228  Training Loss  1.8880480638472363e-05\n","Epoch  21 Batch  84 / 228  Training Loss  1.5147616977628786e-05\n","Epoch  21 Batch  85 / 228  Training Loss  1.6096226318040863e-05\n","Epoch  21 Batch  86 / 228  Training Loss  1.1985309356532525e-05\n","Epoch  21 Batch  87 / 228  Training Loss  1.4001622730575036e-05\n","Epoch  21 Batch  88 / 228  Training Loss  2.1969415683997795e-05\n","Epoch  21 Batch  89 / 228  Training Loss  1.5456096662092023e-05\n","Epoch  21 Batch  90 / 228  Training Loss  2.0651914383051917e-05\n","Epoch  21 Batch  91 / 228  Training Loss  1.4950422155379783e-05\n","Epoch  21 Batch  92 / 228  Training Loss  1.1269887181697413e-05\n","Epoch  21 Batch  93 / 228  Training Loss  1.5801295376149938e-05\n","Epoch  21 Batch  94 / 228  Training Loss  1.4411431038752198e-05\n","Epoch  21 Batch  95 / 228  Training Loss  1.6777705241111107e-05\n","Epoch  21 Batch  96 / 228  Training Loss  1.255570077773882e-05\n","Epoch  21 Batch  97 / 228  Training Loss  1.8303941033082083e-05\n","Epoch  21 Batch  98 / 228  Training Loss  1.687300937192049e-05\n","Epoch  21 Batch  99 / 228  Training Loss  1.4884584743413143e-05\n","Epoch  21 Batch  100 / 228  Training Loss  2.2629003069596365e-05\n","Epoch  21 Batch  101 / 228  Training Loss  1.306044123339234e-05\n","Epoch  21 Batch  102 / 228  Training Loss  1.5094834452611394e-05\n","Epoch  21 Batch  103 / 228  Training Loss  1.907906334963627e-05\n","Epoch  21 Batch  104 / 228  Training Loss  1.5777779481140897e-05\n","Epoch  21 Batch  105 / 228  Training Loss  1.3206898074713536e-05\n","Epoch  21 Batch  106 / 228  Training Loss  1.6793290342320688e-05\n","Epoch  21 Batch  107 / 228  Training Loss  1.070329835783923e-05\n","Epoch  21 Batch  108 / 228  Training Loss  1.5375040675280616e-05\n","Epoch  21 Batch  109 / 228  Training Loss  1.4396379810932558e-05\n","Epoch  21 Batch  110 / 228  Training Loss  1.4969125913921744e-05\n","Epoch  21 Batch  111 / 228  Training Loss  1.667567266849801e-05\n","Epoch  21 Batch  112 / 228  Training Loss  1.6528258129255846e-05\n","Epoch  21 Batch  113 / 228  Training Loss  2.2308688130578957e-05\n","Epoch  21 Batch  114 / 228  Training Loss  1.4266867765400093e-05\n","Epoch  21 Batch  115 / 228  Training Loss  1.6759051504777744e-05\n","Epoch  21 Batch  116 / 228  Training Loss  1.3774710168945603e-05\n","Epoch  21 Batch  117 / 228  Training Loss  2.0551848137984052e-05\n","Epoch  21 Batch  118 / 228  Training Loss  1.4054615348868538e-05\n","Epoch  21 Batch  119 / 228  Training Loss  1.5733534382889047e-05\n","Epoch  21 Batch  120 / 228  Training Loss  1.230869565915782e-05\n","Epoch  21 Batch  121 / 228  Training Loss  1.591713044035714e-05\n","Epoch  21 Batch  122 / 228  Training Loss  1.8970671590068378e-05\n","Epoch  21 Batch  123 / 228  Training Loss  1.6288629922200926e-05\n","Epoch  21 Batch  124 / 228  Training Loss  1.1096371054009069e-05\n","Epoch  21 Batch  125 / 228  Training Loss  1.2689500181295443e-05\n","Epoch  21 Batch  126 / 228  Training Loss  1.3763647075393237e-05\n","Epoch  21 Batch  127 / 228  Training Loss  1.926272307173349e-05\n","Epoch  21 Batch  128 / 228  Training Loss  1.6736812540329993e-05\n","Epoch  21 Batch  129 / 228  Training Loss  1.7664397091721185e-05\n","Epoch  21 Batch  130 / 228  Training Loss  1.6399610103690065e-05\n","Epoch  21 Batch  131 / 228  Training Loss  1.2825361409340985e-05\n","Epoch  21 Batch  132 / 228  Training Loss  1.8931617887574248e-05\n","Epoch  21 Batch  133 / 228  Training Loss  1.4694485798827372e-05\n","Epoch  21 Batch  134 / 228  Training Loss  1.3900607882533222e-05\n","Epoch  21 Batch  135 / 228  Training Loss  1.6606836652499624e-05\n","Epoch  21 Batch  136 / 228  Training Loss  2.1265403120196424e-05\n","Epoch  21 Batch  137 / 228  Training Loss  1.957106542249676e-05\n","Epoch  21 Batch  138 / 228  Training Loss  1.7879956430988386e-05\n","Epoch  21 Batch  139 / 228  Training Loss  1.5175019143498503e-05\n","Epoch  21 Batch  140 / 228  Training Loss  1.8155053112423047e-05\n","Epoch  21 Batch  141 / 228  Training Loss  2.5155488401651382e-05\n","Epoch  21 Batch  142 / 228  Training Loss  1.7259628293686546e-05\n","Epoch  21 Batch  143 / 228  Training Loss  1.6899368347367272e-05\n","Epoch  21 Batch  144 / 228  Training Loss  1.1305514817649964e-05\n","Epoch  21 Batch  145 / 228  Training Loss  1.879120281955693e-05\n","Epoch  21 Batch  146 / 228  Training Loss  1.0982932508341037e-05\n","Epoch  21 Batch  147 / 228  Training Loss  1.4502237718261313e-05\n","Epoch  21 Batch  148 / 228  Training Loss  1.4540610209223814e-05\n","Epoch  21 Batch  149 / 228  Training Loss  1.2738213627017103e-05\n","Epoch  21 Batch  150 / 228  Training Loss  1.9094284652965143e-05\n","Epoch  21 Batch  151 / 228  Training Loss  1.0135957381862681e-05\n","Epoch  21 Batch  152 / 228  Training Loss  1.9329234419274144e-05\n","Epoch  21 Batch  153 / 228  Training Loss  1.2092934412066825e-05\n","Epoch  21 Batch  154 / 228  Training Loss  1.750056617311202e-05\n","Epoch  21 Batch  155 / 228  Training Loss  1.3966445294499863e-05\n","Epoch  21 Batch  156 / 228  Training Loss  2.2714897568221204e-05\n","Epoch  21 Batch  157 / 228  Training Loss  1.343767962680431e-05\n","Epoch  21 Batch  158 / 228  Training Loss  1.347534271189943e-05\n","Epoch  21 Batch  159 / 228  Training Loss  1.7778476831153966e-05\n","Epoch  21 Batch  160 / 228  Training Loss  1.6462250641779974e-05\n","Epoch  21 Batch  161 / 228  Training Loss  1.6811531168059446e-05\n","Epoch  21 Batch  162 / 228  Training Loss  1.3769555152975954e-05\n","Epoch  21 Batch  163 / 228  Training Loss  1.9830204109894112e-05\n","Epoch  21 Batch  164 / 228  Training Loss  1.5030049326014705e-05\n","Epoch  21 Batch  165 / 228  Training Loss  1.5210214769467711e-05\n","Epoch  21 Batch  166 / 228  Training Loss  1.2645260540011805e-05\n","Epoch  21 Batch  167 / 228  Training Loss  1.640914888412226e-05\n","Epoch  21 Batch  168 / 228  Training Loss  1.2988801245228387e-05\n","Epoch  21 Batch  169 / 228  Training Loss  2.2138101485325024e-05\n","Epoch  21 Batch  170 / 228  Training Loss  1.2936015082232188e-05\n","Epoch  21 Batch  171 / 228  Training Loss  1.7291704352828674e-05\n","Epoch  21 Batch  172 / 228  Training Loss  2.277411476825364e-05\n","Epoch  21 Batch  173 / 228  Training Loss  1.7965923689189367e-05\n","Epoch  21 Batch  174 / 228  Training Loss  1.907964724523481e-05\n","Epoch  21 Batch  175 / 228  Training Loss  1.4655057384516113e-05\n","Epoch  21 Batch  176 / 228  Training Loss  1.3677372407983057e-05\n","Epoch  21 Batch  177 / 228  Training Loss  1.4223138350644149e-05\n","Epoch  21 Batch  178 / 228  Training Loss  1.7094876966439188e-05\n","Epoch  21 Batch  179 / 228  Training Loss  1.4255032510845922e-05\n","Epoch  21 Batch  180 / 228  Training Loss  1.5058513781696092e-05\n","Epoch  21 Batch  181 / 228  Training Loss  1.6248959582298994e-05\n","Epoch  21 Batch  182 / 228  Training Loss  1.0481338904355653e-05\n","Epoch  21 Batch  183 / 228  Training Loss  1.6200856407522224e-05\n","Epoch  21 Batch  184 / 228  Training Loss  1.0122399544343352e-05\n","Epoch  21 Batch  185 / 228  Training Loss  1.2800701369997114e-05\n","Epoch  21 Batch  186 / 228  Training Loss  1.0198084055446088e-05\n","Epoch  21 Batch  187 / 228  Training Loss  1.5337105651269667e-05\n","Epoch  21 Batch  188 / 228  Training Loss  1.9215567590435967e-05\n","Epoch  21 Batch  189 / 228  Training Loss  1.3224200301920064e-05\n","Epoch  21 Batch  190 / 228  Training Loss  1.243048427568283e-05\n","Epoch  21 Batch  191 / 228  Training Loss  1.6115434846142307e-05\n","Epoch  21 Batch  192 / 228  Training Loss  1.563113255542703e-05\n","Epoch  21 Batch  193 / 228  Training Loss  2.1647878384101205e-05\n","Epoch  21 Batch  194 / 228  Training Loss  1.2094867997802794e-05\n","Epoch  21 Batch  195 / 228  Training Loss  1.4644453585788142e-05\n","Epoch  21 Batch  196 / 228  Training Loss  1.491337206971366e-05\n","Epoch  21 Batch  197 / 228  Training Loss  1.4842208656773437e-05\n","Epoch  21 Batch  198 / 228  Training Loss  1.4386952898348682e-05\n","Epoch  21 Batch  199 / 228  Training Loss  1.3020476217207033e-05\n","Epoch  21 Batch  200 / 228  Training Loss  1.7424708858015947e-05\n","Epoch  21 Batch  201 / 228  Training Loss  1.6105292161228135e-05\n","Epoch  21 Batch  202 / 228  Training Loss  1.4054172424948774e-05\n","Epoch  21 Batch  203 / 228  Training Loss  1.1156914297316689e-05\n","Epoch  21 Batch  204 / 228  Training Loss  1.2653023986786138e-05\n","Epoch  21 Batch  205 / 228  Training Loss  1.4481744074146263e-05\n","Epoch  21 Batch  206 / 228  Training Loss  1.6357040294678882e-05\n","Epoch  21 Batch  207 / 228  Training Loss  1.4705436115036719e-05\n","Epoch  21 Batch  208 / 228  Training Loss  2.1519863366847858e-05\n","Epoch  21 Batch  209 / 228  Training Loss  1.3898757060815115e-05\n","Epoch  21 Batch  210 / 228  Training Loss  1.6510990462847985e-05\n","Epoch  21 Batch  211 / 228  Training Loss  1.418826559529407e-05\n","Epoch  21 Batch  212 / 228  Training Loss  1.2609622899617534e-05\n","Epoch  21 Batch  213 / 228  Training Loss  1.3622835467685945e-05\n","Epoch  21 Batch  214 / 228  Training Loss  1.8736060155788437e-05\n","Epoch  21 Batch  215 / 228  Training Loss  1.3154638509149663e-05\n","Epoch  21 Batch  216 / 228  Training Loss  2.002911060117185e-05\n","Epoch  21 Batch  217 / 228  Training Loss  1.8541955796536058e-05\n","Epoch  21 Batch  218 / 228  Training Loss  1.0856748303922359e-05\n","Epoch  21 Batch  219 / 228  Training Loss  1.6075469829957e-05\n","Epoch  21 Batch  220 / 228  Training Loss  1.432908720744308e-05\n","Epoch  21 Batch  221 / 228  Training Loss  1.5008369700808544e-05\n","Epoch  21 Batch  222 / 228  Training Loss  1.4018622096045874e-05\n","Epoch  21 Batch  223 / 228  Training Loss  1.2121004147047643e-05\n","Epoch  21 Batch  224 / 228  Training Loss  1.4730336260981858e-05\n","Epoch  21 Batch  225 / 228  Training Loss  1.8251601431984454e-05\n","Epoch  21 Batch  226 / 228  Training Loss  1.3550195944844745e-05\n","Epoch  21 Batch  227 / 228  Training Loss  9.457538908463903e-06\n","  22    |    -    |   0.000015   | 99.580793\n","----------------------------------------------------------------------\n","Running epoch: 22\n","Epoch  22 Batch  0 / 228  Training Loss  1.265502032765653e-05\n","Epoch  22 Batch  1 / 228  Training Loss  1.7536211089463905e-05\n","Epoch  22 Batch  2 / 228  Training Loss  1.2077410247002263e-05\n","Epoch  22 Batch  3 / 228  Training Loss  1.1492804333101958e-05\n","Epoch  22 Batch  4 / 228  Training Loss  1.7132817447418347e-05\n","Epoch  22 Batch  5 / 228  Training Loss  1.399818756908644e-05\n","Epoch  22 Batch  6 / 228  Training Loss  1.6984782632789575e-05\n","Epoch  22 Batch  7 / 228  Training Loss  1.9961766156484373e-05\n","Epoch  22 Batch  8 / 228  Training Loss  1.1591723705350887e-05\n","Epoch  22 Batch  9 / 228  Training Loss  1.3183023838791996e-05\n","Epoch  22 Batch  10 / 228  Training Loss  1.7952959751710296e-05\n","Epoch  22 Batch  11 / 228  Training Loss  1.441511540178908e-05\n","Epoch  22 Batch  12 / 228  Training Loss  1.9039625840377994e-05\n","Epoch  22 Batch  13 / 228  Training Loss  1.5053026800160296e-05\n","Epoch  22 Batch  14 / 228  Training Loss  1.5636289390386082e-05\n","Epoch  22 Batch  15 / 228  Training Loss  1.5923071259749122e-05\n","Epoch  22 Batch  16 / 228  Training Loss  1.9116943803965114e-05\n","Epoch  22 Batch  17 / 228  Training Loss  1.29859863591264e-05\n","Epoch  22 Batch  18 / 228  Training Loss  1.6173868061741814e-05\n","Epoch  22 Batch  19 / 228  Training Loss  1.8639804693521e-05\n","Epoch  22 Batch  20 / 228  Training Loss  8.831782906781882e-06\n","Epoch  22 Batch  21 / 228  Training Loss  1.3472144019033294e-05\n","Epoch  22 Batch  22 / 228  Training Loss  2.0457124264794402e-05\n","Epoch  22 Batch  23 / 228  Training Loss  1.3636361472890712e-05\n","Epoch  22 Batch  24 / 228  Training Loss  1.3541658518079203e-05\n","Epoch  22 Batch  25 / 228  Training Loss  1.627417623240035e-05\n","Epoch  22 Batch  26 / 228  Training Loss  1.5948471627780236e-05\n","Epoch  22 Batch  27 / 228  Training Loss  1.354395499220118e-05\n","Epoch  22 Batch  28 / 228  Training Loss  1.4278581147664227e-05\n","Epoch  22 Batch  29 / 228  Training Loss  2.1356145225581713e-05\n","Epoch  22 Batch  30 / 228  Training Loss  1.5044603969727177e-05\n","Epoch  22 Batch  31 / 228  Training Loss  1.4301057490229141e-05\n","Epoch  22 Batch  32 / 228  Training Loss  1.6494359442731366e-05\n","Epoch  22 Batch  33 / 228  Training Loss  1.462330692447722e-05\n","Epoch  22 Batch  34 / 228  Training Loss  1.209665151691297e-05\n","Epoch  22 Batch  35 / 228  Training Loss  9.870231224340387e-06\n","Epoch  22 Batch  36 / 228  Training Loss  1.3555089935834985e-05\n","Epoch  22 Batch  37 / 228  Training Loss  1.3577891877503134e-05\n","Epoch  22 Batch  38 / 228  Training Loss  1.1716378139681183e-05\n","Epoch  22 Batch  39 / 228  Training Loss  1.2715696357190609e-05\n","Epoch  22 Batch  40 / 228  Training Loss  1.4868312064209022e-05\n","Epoch  22 Batch  41 / 228  Training Loss  1.2754548151860945e-05\n","Epoch  22 Batch  42 / 228  Training Loss  1.583940684213303e-05\n","Epoch  22 Batch  43 / 228  Training Loss  2.2047073798603378e-05\n","Epoch  22 Batch  44 / 228  Training Loss  1.2003693882434163e-05\n","Epoch  22 Batch  45 / 228  Training Loss  1.1726562661351636e-05\n","Epoch  22 Batch  46 / 228  Training Loss  1.2414506272762083e-05\n","Epoch  22 Batch  47 / 228  Training Loss  1.453683216823265e-05\n","Epoch  22 Batch  48 / 228  Training Loss  1.7676200513960794e-05\n","Epoch  22 Batch  49 / 228  Training Loss  1.4561015632352792e-05\n","Epoch  22 Batch  50 / 228  Training Loss  1.3567085261456668e-05\n","Epoch  22 Batch  51 / 228  Training Loss  1.3919379853177816e-05\n","Epoch  22 Batch  52 / 228  Training Loss  1.4069034477870446e-05\n","Epoch  22 Batch  53 / 228  Training Loss  1.2490597327996511e-05\n","Epoch  22 Batch  54 / 228  Training Loss  1.0442447091918439e-05\n","Epoch  22 Batch  55 / 228  Training Loss  1.4120565538178198e-05\n","Epoch  22 Batch  56 / 228  Training Loss  1.4445143278862815e-05\n","Epoch  22 Batch  57 / 228  Training Loss  1.1844292203022633e-05\n","Epoch  22 Batch  58 / 228  Training Loss  1.298148708883673e-05\n","Epoch  22 Batch  59 / 228  Training Loss  1.1954510227951687e-05\n","Epoch  22 Batch  60 / 228  Training Loss  2.018019040406216e-05\n","Epoch  22 Batch  61 / 228  Training Loss  1.478639205743093e-05\n","Epoch  22 Batch  62 / 228  Training Loss  1.3929148735769559e-05\n","Epoch  22 Batch  63 / 228  Training Loss  1.0887995813391171e-05\n","Epoch  22 Batch  64 / 228  Training Loss  1.80898277903907e-05\n","Epoch  22 Batch  65 / 228  Training Loss  1.5083214748301543e-05\n","Epoch  22 Batch  66 / 228  Training Loss  1.1488575182738714e-05\n","Epoch  22 Batch  67 / 228  Training Loss  1.750643968989607e-05\n","Epoch  22 Batch  68 / 228  Training Loss  1.5440204151673242e-05\n","Epoch  22 Batch  69 / 228  Training Loss  1.2349167263892014e-05\n","Epoch  22 Batch  70 / 228  Training Loss  1.0895039849856403e-05\n","Epoch  22 Batch  71 / 228  Training Loss  1.1476749932626262e-05\n","Epoch  22 Batch  72 / 228  Training Loss  1.4644932889495976e-05\n","Epoch  22 Batch  73 / 228  Training Loss  1.2989653441763949e-05\n","Epoch  22 Batch  74 / 228  Training Loss  1.548739237478003e-05\n","Epoch  22 Batch  75 / 228  Training Loss  1.0604764611343853e-05\n","Epoch  22 Batch  76 / 228  Training Loss  1.4955387086956762e-05\n","Epoch  22 Batch  77 / 228  Training Loss  1.465917557652574e-05\n","Epoch  22 Batch  78 / 228  Training Loss  1.6833326299092732e-05\n","Epoch  22 Batch  79 / 228  Training Loss  1.5701312804594636e-05\n","Epoch  22 Batch  80 / 228  Training Loss  1.8962267859023996e-05\n","Epoch  22 Batch  81 / 228  Training Loss  1.1761958376155235e-05\n","Epoch  22 Batch  82 / 228  Training Loss  1.920223076012917e-05\n","Epoch  22 Batch  83 / 228  Training Loss  1.2642631190828979e-05\n","Epoch  22 Batch  84 / 228  Training Loss  1.1705099495884497e-05\n","Epoch  22 Batch  85 / 228  Training Loss  1.2365048860374372e-05\n","Epoch  22 Batch  86 / 228  Training Loss  1.4116607417236082e-05\n","Epoch  22 Batch  87 / 228  Training Loss  2.048072565230541e-05\n","Epoch  22 Batch  88 / 228  Training Loss  1.8225653548142873e-05\n","Epoch  22 Batch  89 / 228  Training Loss  1.30265125335427e-05\n","Epoch  22 Batch  90 / 228  Training Loss  1.4356506653712131e-05\n","Epoch  22 Batch  91 / 228  Training Loss  1.4496954463538714e-05\n","Epoch  22 Batch  92 / 228  Training Loss  1.5125246136449277e-05\n","Epoch  22 Batch  93 / 228  Training Loss  1.3246555681689642e-05\n","Epoch  22 Batch  94 / 228  Training Loss  1.3602459148387425e-05\n","Epoch  22 Batch  95 / 228  Training Loss  9.100056558963843e-06\n","Epoch  22 Batch  96 / 228  Training Loss  1.2165507541794796e-05\n","Epoch  22 Batch  97 / 228  Training Loss  1.2740046258841176e-05\n","Epoch  22 Batch  98 / 228  Training Loss  2.0947827579220757e-05\n","Epoch  22 Batch  99 / 228  Training Loss  1.2597722161444835e-05\n","Epoch  22 Batch  100 / 228  Training Loss  1.2398430044413544e-05\n","Epoch  22 Batch  101 / 228  Training Loss  1.4274866771302186e-05\n","Epoch  22 Batch  102 / 228  Training Loss  1.3679787116416264e-05\n","Epoch  22 Batch  103 / 228  Training Loss  1.0475455383129884e-05\n","Epoch  22 Batch  104 / 228  Training Loss  1.3927859072282445e-05\n","Epoch  22 Batch  105 / 228  Training Loss  1.4650960110884625e-05\n","Epoch  22 Batch  106 / 228  Training Loss  1.5432058717124164e-05\n","Epoch  22 Batch  107 / 228  Training Loss  9.51277070271317e-06\n","Epoch  22 Batch  108 / 228  Training Loss  1.4327122698887251e-05\n","Epoch  22 Batch  109 / 228  Training Loss  1.5291736417566426e-05\n","Epoch  22 Batch  110 / 228  Training Loss  1.6615671484032646e-05\n","Epoch  22 Batch  111 / 228  Training Loss  1.5530280506936833e-05\n","Epoch  22 Batch  112 / 228  Training Loss  1.8648610421223566e-05\n","Epoch  22 Batch  113 / 228  Training Loss  1.420757507730741e-05\n","Epoch  22 Batch  114 / 228  Training Loss  1.9728289771592245e-05\n","Epoch  22 Batch  115 / 228  Training Loss  1.4112793905951548e-05\n","Epoch  22 Batch  116 / 228  Training Loss  1.272576173505513e-05\n","Epoch  22 Batch  117 / 228  Training Loss  1.1214350706723053e-05\n","Epoch  22 Batch  118 / 228  Training Loss  1.2301346941967495e-05\n","Epoch  22 Batch  119 / 228  Training Loss  1.7293603377765976e-05\n","Epoch  22 Batch  120 / 228  Training Loss  1.6103711459436454e-05\n","Epoch  22 Batch  121 / 228  Training Loss  1.459305622120155e-05\n","Epoch  22 Batch  122 / 228  Training Loss  1.3980579751660116e-05\n","Epoch  22 Batch  123 / 228  Training Loss  1.5293260730686598e-05\n","Epoch  22 Batch  124 / 228  Training Loss  1.5948962754919194e-05\n","Epoch  22 Batch  125 / 228  Training Loss  2.089127156068571e-05\n","Epoch  22 Batch  126 / 228  Training Loss  1.205894113809336e-05\n","Epoch  22 Batch  127 / 228  Training Loss  1.9913428332074545e-05\n","Epoch  22 Batch  128 / 228  Training Loss  1.6925050658755936e-05\n","Epoch  22 Batch  129 / 228  Training Loss  1.528928623883985e-05\n","Epoch  22 Batch  130 / 228  Training Loss  1.7965447113965638e-05\n","Epoch  22 Batch  131 / 228  Training Loss  1.3388734259933699e-05\n","Epoch  22 Batch  132 / 228  Training Loss  1.3857442354492377e-05\n","Epoch  22 Batch  133 / 228  Training Loss  1.2136241821281146e-05\n","Epoch  22 Batch  134 / 228  Training Loss  1.1975637789873872e-05\n","Epoch  22 Batch  135 / 228  Training Loss  1.7108486645156518e-05\n","Epoch  22 Batch  136 / 228  Training Loss  1.321124545938801e-05\n","Epoch  22 Batch  137 / 228  Training Loss  1.3846944966644514e-05\n","Epoch  22 Batch  138 / 228  Training Loss  1.2209080523462035e-05\n","Epoch  22 Batch  139 / 228  Training Loss  1.1620754776231479e-05\n","Epoch  22 Batch  140 / 228  Training Loss  1.263924605154898e-05\n","Epoch  22 Batch  141 / 228  Training Loss  1.6322808733093552e-05\n","Epoch  22 Batch  142 / 228  Training Loss  1.6013025742722675e-05\n","Epoch  22 Batch  143 / 228  Training Loss  1.181589141197037e-05\n","Epoch  22 Batch  144 / 228  Training Loss  1.5660743883927353e-05\n","Epoch  22 Batch  145 / 228  Training Loss  1.4195973562891595e-05\n","Epoch  22 Batch  146 / 228  Training Loss  8.326666829816531e-06\n","Epoch  22 Batch  147 / 228  Training Loss  1.29635682242224e-05\n","Epoch  22 Batch  148 / 228  Training Loss  1.5032310329843313e-05\n","Epoch  22 Batch  149 / 228  Training Loss  1.4972712961025536e-05\n","Epoch  22 Batch  150 / 228  Training Loss  1.4394694517250173e-05\n","Epoch  22 Batch  151 / 228  Training Loss  1.6802590835141018e-05\n","Epoch  22 Batch  152 / 228  Training Loss  1.4106271919445135e-05\n","Epoch  22 Batch  153 / 228  Training Loss  1.4730905604665168e-05\n","Epoch  22 Batch  154 / 228  Training Loss  1.1132789950352162e-05\n","Epoch  22 Batch  155 / 228  Training Loss  1.237173728441121e-05\n","Epoch  22 Batch  156 / 228  Training Loss  1.751478703226894e-05\n","Epoch  22 Batch  157 / 228  Training Loss  1.0702466170187108e-05\n","Epoch  22 Batch  158 / 228  Training Loss  1.1413254469516687e-05\n","Epoch  22 Batch  159 / 228  Training Loss  1.1255503522988874e-05\n","Epoch  22 Batch  160 / 228  Training Loss  1.0969759387080558e-05\n","Epoch  22 Batch  161 / 228  Training Loss  1.0740614015958272e-05\n","Epoch  22 Batch  162 / 228  Training Loss  1.9884182620444335e-05\n","Epoch  22 Batch  163 / 228  Training Loss  1.7725898942444474e-05\n","Epoch  22 Batch  164 / 228  Training Loss  1.253731443284778e-05\n","Epoch  22 Batch  165 / 228  Training Loss  1.7556489183334634e-05\n","Epoch  22 Batch  166 / 228  Training Loss  1.2637459803954698e-05\n","Epoch  22 Batch  167 / 228  Training Loss  1.1582582374103367e-05\n","Epoch  22 Batch  168 / 228  Training Loss  1.7195892723975703e-05\n","Epoch  22 Batch  169 / 228  Training Loss  1.7093510905397125e-05\n","Epoch  22 Batch  170 / 228  Training Loss  1.1409409125917591e-05\n","Epoch  22 Batch  171 / 228  Training Loss  1.771901224856265e-05\n","Epoch  22 Batch  172 / 228  Training Loss  1.7553364159539342e-05\n","Epoch  22 Batch  173 / 228  Training Loss  1.4479713172477204e-05\n","Epoch  22 Batch  174 / 228  Training Loss  1.0924259186140262e-05\n","Epoch  22 Batch  175 / 228  Training Loss  1.2002207768091466e-05\n","Epoch  22 Batch  176 / 228  Training Loss  1.183287713502068e-05\n","Epoch  22 Batch  177 / 228  Training Loss  2.0099356333957985e-05\n","Epoch  22 Batch  178 / 228  Training Loss  1.3337798009160906e-05\n","Epoch  22 Batch  179 / 228  Training Loss  1.1288459063507617e-05\n","Epoch  22 Batch  180 / 228  Training Loss  1.6366740965167992e-05\n","Epoch  22 Batch  181 / 228  Training Loss  1.5684936442994513e-05\n","Epoch  22 Batch  182 / 228  Training Loss  9.002071237773634e-06\n","Epoch  22 Batch  183 / 228  Training Loss  1.7737913367454894e-05\n","Epoch  22 Batch  184 / 228  Training Loss  1.3434623724606354e-05\n","Epoch  22 Batch  185 / 228  Training Loss  1.1099454241048079e-05\n","Epoch  22 Batch  186 / 228  Training Loss  2.4465585738653317e-05\n","Epoch  22 Batch  187 / 228  Training Loss  1.3770606528851204e-05\n","Epoch  22 Batch  188 / 228  Training Loss  1.2827624232158996e-05\n","Epoch  22 Batch  189 / 228  Training Loss  1.7189526261063293e-05\n","Epoch  22 Batch  190 / 228  Training Loss  1.1388146958779544e-05\n","Epoch  22 Batch  191 / 228  Training Loss  1.7620619473746046e-05\n","Epoch  22 Batch  192 / 228  Training Loss  1.589473322383128e-05\n","Epoch  22 Batch  193 / 228  Training Loss  2.134314490831457e-05\n","Epoch  22 Batch  194 / 228  Training Loss  1.4773313523619436e-05\n","Epoch  22 Batch  195 / 228  Training Loss  1.446362330170814e-05\n","Epoch  22 Batch  196 / 228  Training Loss  1.802489350666292e-05\n","Epoch  22 Batch  197 / 228  Training Loss  1.4677670151286293e-05\n","Epoch  22 Batch  198 / 228  Training Loss  1.3854942153557204e-05\n","Epoch  22 Batch  199 / 228  Training Loss  9.294405572290998e-06\n","Epoch  22 Batch  200 / 228  Training Loss  1.2257603884791024e-05\n","Epoch  22 Batch  201 / 228  Training Loss  1.1247926522628404e-05\n","Epoch  22 Batch  202 / 228  Training Loss  1.0483298865437973e-05\n","Epoch  22 Batch  203 / 228  Training Loss  1.4357596228364855e-05\n","Epoch  22 Batch  204 / 228  Training Loss  1.1455330422904808e-05\n","Epoch  22 Batch  205 / 228  Training Loss  1.8583594282972626e-05\n","Epoch  22 Batch  206 / 228  Training Loss  8.67196013132343e-06\n","Epoch  22 Batch  207 / 228  Training Loss  9.632387445890345e-06\n","Epoch  22 Batch  208 / 228  Training Loss  1.4166185792419128e-05\n","Epoch  22 Batch  209 / 228  Training Loss  1.564188460179139e-05\n","Epoch  22 Batch  210 / 228  Training Loss  1.7110260159824975e-05\n","Epoch  22 Batch  211 / 228  Training Loss  1.2122372027079109e-05\n","Epoch  22 Batch  212 / 228  Training Loss  1.0806988029798958e-05\n","Epoch  22 Batch  213 / 228  Training Loss  1.3966595361125655e-05\n","Epoch  22 Batch  214 / 228  Training Loss  1.4904620002198499e-05\n","Epoch  22 Batch  215 / 228  Training Loss  1.8248196283821017e-05\n","Epoch  22 Batch  216 / 228  Training Loss  1.589111161592882e-05\n","Epoch  22 Batch  217 / 228  Training Loss  1.58915117935976e-05\n","Epoch  22 Batch  218 / 228  Training Loss  1.5870300558162853e-05\n","Epoch  22 Batch  219 / 228  Training Loss  1.4514354006678332e-05\n","Epoch  22 Batch  220 / 228  Training Loss  1.3055981071374845e-05\n","Epoch  22 Batch  221 / 228  Training Loss  1.3547275557357352e-05\n","Epoch  22 Batch  222 / 228  Training Loss  1.3650736946146935e-05\n","Epoch  22 Batch  223 / 228  Training Loss  1.2264043107279576e-05\n","Epoch  22 Batch  224 / 228  Training Loss  1.1708268175425474e-05\n","Epoch  22 Batch  225 / 228  Training Loss  1.2239848729223013e-05\n","Epoch  22 Batch  226 / 228  Training Loss  1.6508503904333338e-05\n","Epoch  22 Batch  227 / 228  Training Loss  9.808111826714594e-06\n","  23    |    -    |   0.000014   | 99.580793\n","----------------------------------------------------------------------\n","Running epoch: 23\n","Epoch  23 Batch  0 / 228  Training Loss  1.0705762178986333e-05\n","Epoch  23 Batch  1 / 228  Training Loss  1.1829817594843917e-05\n","Epoch  23 Batch  2 / 228  Training Loss  1.3482267604558729e-05\n","Epoch  23 Batch  3 / 228  Training Loss  1.183431322715478e-05\n","Epoch  23 Batch  4 / 228  Training Loss  1.1163709132233635e-05\n","Epoch  23 Batch  5 / 228  Training Loss  1.2146126209700014e-05\n","Epoch  23 Batch  6 / 228  Training Loss  1.1768171134463046e-05\n","Epoch  23 Batch  7 / 228  Training Loss  8.928634997573681e-06\n","Epoch  23 Batch  8 / 228  Training Loss  1.115212307922775e-05\n","Epoch  23 Batch  9 / 228  Training Loss  1.1973730579484254e-05\n","Epoch  23 Batch  10 / 228  Training Loss  1.0720018508436624e-05\n","Epoch  23 Batch  11 / 228  Training Loss  1.4252631444833241e-05\n","Epoch  23 Batch  12 / 228  Training Loss  9.897217751131393e-06\n","Epoch  23 Batch  13 / 228  Training Loss  1.5255076505127363e-05\n","Epoch  23 Batch  14 / 228  Training Loss  1.3344503713597078e-05\n","Epoch  23 Batch  15 / 228  Training Loss  2.0481800675042905e-05\n","Epoch  23 Batch  16 / 228  Training Loss  1.2886617696494795e-05\n","Epoch  23 Batch  17 / 228  Training Loss  1.0033918442786671e-05\n","Epoch  23 Batch  18 / 228  Training Loss  1.1874228221131489e-05\n","Epoch  23 Batch  19 / 228  Training Loss  1.2746728316415101e-05\n","Epoch  23 Batch  20 / 228  Training Loss  1.3034441508352757e-05\n","Epoch  23 Batch  21 / 228  Training Loss  1.4513435417029541e-05\n","Epoch  23 Batch  22 / 228  Training Loss  1.536696254333947e-05\n","Epoch  23 Batch  23 / 228  Training Loss  1.2081693057552911e-05\n","Epoch  23 Batch  24 / 228  Training Loss  2.0499221136560664e-05\n","Epoch  23 Batch  25 / 228  Training Loss  1.7834992831922136e-05\n","Epoch  23 Batch  26 / 228  Training Loss  9.601976671547163e-06\n","Epoch  23 Batch  27 / 228  Training Loss  1.303977296629455e-05\n","Epoch  23 Batch  28 / 228  Training Loss  1.4340145753521938e-05\n","Epoch  23 Batch  29 / 228  Training Loss  9.345929356641136e-06\n","Epoch  23 Batch  30 / 228  Training Loss  1.1646094208117574e-05\n","Epoch  23 Batch  31 / 228  Training Loss  1.0762954843812622e-05\n","Epoch  23 Batch  32 / 228  Training Loss  1.634707768971566e-05\n","Epoch  23 Batch  33 / 228  Training Loss  1.2046190931869205e-05\n","Epoch  23 Batch  34 / 228  Training Loss  1.4352757716551423e-05\n","Epoch  23 Batch  35 / 228  Training Loss  1.8043512682197616e-05\n","Epoch  23 Batch  36 / 228  Training Loss  1.6788641005405225e-05\n","Epoch  23 Batch  37 / 228  Training Loss  1.1903507584065665e-05\n","Epoch  23 Batch  38 / 228  Training Loss  1.2641110515687615e-05\n","Epoch  23 Batch  39 / 228  Training Loss  1.7055268472176977e-05\n","Epoch  23 Batch  40 / 228  Training Loss  1.8986635041073896e-05\n","Epoch  23 Batch  41 / 228  Training Loss  1.3308900634001475e-05\n","Epoch  23 Batch  42 / 228  Training Loss  1.4752027709619142e-05\n","Epoch  23 Batch  43 / 228  Training Loss  1.2033455277560279e-05\n","Epoch  23 Batch  44 / 228  Training Loss  1.2461355254345108e-05\n","Epoch  23 Batch  45 / 228  Training Loss  1.3459592992148828e-05\n","Epoch  23 Batch  46 / 228  Training Loss  1.3018087884120177e-05\n","Epoch  23 Batch  47 / 228  Training Loss  1.1067515515605919e-05\n","Epoch  23 Batch  48 / 228  Training Loss  1.4225942322809715e-05\n","Epoch  23 Batch  49 / 228  Training Loss  1.649190926400479e-05\n","Epoch  23 Batch  50 / 228  Training Loss  1.1325088962621521e-05\n","Epoch  23 Batch  51 / 228  Training Loss  1.0954787285299972e-05\n","Epoch  23 Batch  52 / 228  Training Loss  1.3397890143096447e-05\n","Epoch  23 Batch  53 / 228  Training Loss  1.5392970453831367e-05\n","Epoch  23 Batch  54 / 228  Training Loss  1.707132105366327e-05\n","Epoch  23 Batch  55 / 228  Training Loss  1.5546193026239052e-05\n","Epoch  23 Batch  56 / 228  Training Loss  1.2319552297412883e-05\n","Epoch  23 Batch  57 / 228  Training Loss  1.3135047993273474e-05\n","Epoch  23 Batch  58 / 228  Training Loss  9.97541610558983e-06\n","Epoch  23 Batch  59 / 228  Training Loss  1.4784569430048577e-05\n","Epoch  23 Batch  60 / 228  Training Loss  1.3824072084389627e-05\n","Epoch  23 Batch  61 / 228  Training Loss  1.1887927030329593e-05\n","Epoch  23 Batch  62 / 228  Training Loss  1.622992567718029e-05\n","Epoch  23 Batch  63 / 228  Training Loss  1.7339136320515536e-05\n","Epoch  23 Batch  64 / 228  Training Loss  1.0917755389527883e-05\n","Epoch  23 Batch  65 / 228  Training Loss  2.184689583373256e-05\n","Epoch  23 Batch  66 / 228  Training Loss  1.2896150110464077e-05\n","Epoch  23 Batch  67 / 228  Training Loss  1.3220605978858657e-05\n","Epoch  23 Batch  68 / 228  Training Loss  9.683182724984363e-06\n","Epoch  23 Batch  69 / 228  Training Loss  1.4599410860682838e-05\n","Epoch  23 Batch  70 / 228  Training Loss  1.6565772966714576e-05\n","Epoch  23 Batch  71 / 228  Training Loss  1.2286649507586844e-05\n","Epoch  23 Batch  72 / 228  Training Loss  1.3231484444986563e-05\n","Epoch  23 Batch  73 / 228  Training Loss  1.614776010683272e-05\n","Epoch  23 Batch  74 / 228  Training Loss  9.603636499377899e-06\n","Epoch  23 Batch  75 / 228  Training Loss  1.5055586118251085e-05\n","Epoch  23 Batch  76 / 228  Training Loss  1.4681265383842401e-05\n","Epoch  23 Batch  77 / 228  Training Loss  1.4137845937511884e-05\n","Epoch  23 Batch  78 / 228  Training Loss  9.489585863775574e-06\n","Epoch  23 Batch  79 / 228  Training Loss  1.449534192943247e-05\n","Epoch  23 Batch  80 / 228  Training Loss  1.2361796507320832e-05\n","Epoch  23 Batch  81 / 228  Training Loss  1.0845721590158064e-05\n","Epoch  23 Batch  82 / 228  Training Loss  1.6152585885720327e-05\n","Epoch  23 Batch  83 / 228  Training Loss  1.3992890671943314e-05\n","Epoch  23 Batch  84 / 228  Training Loss  1.4395050129678566e-05\n","Epoch  23 Batch  85 / 228  Training Loss  1.3232165656518191e-05\n","Epoch  23 Batch  86 / 228  Training Loss  1.3361775017983746e-05\n","Epoch  23 Batch  87 / 228  Training Loss  2.1946931155980565e-05\n","Epoch  23 Batch  88 / 228  Training Loss  1.601817348273471e-05\n","Epoch  23 Batch  89 / 228  Training Loss  1.590814281371422e-05\n","Epoch  23 Batch  90 / 228  Training Loss  1.0706628017942421e-05\n","Epoch  23 Batch  91 / 228  Training Loss  1.0821433534147218e-05\n","Epoch  23 Batch  92 / 228  Training Loss  1.2516289643826894e-05\n","Epoch  23 Batch  93 / 228  Training Loss  1.3789725016977172e-05\n","Epoch  23 Batch  94 / 228  Training Loss  1.4043102964933496e-05\n","Epoch  23 Batch  95 / 228  Training Loss  1.2472725757106673e-05\n","Epoch  23 Batch  96 / 228  Training Loss  1.501688348071184e-05\n","Epoch  23 Batch  97 / 228  Training Loss  1.126764345826814e-05\n","Epoch  23 Batch  98 / 228  Training Loss  1.7528951502754353e-05\n","Epoch  23 Batch  99 / 228  Training Loss  1.3493304322764743e-05\n","Epoch  23 Batch  100 / 228  Training Loss  1.4453357835009228e-05\n","Epoch  23 Batch  101 / 228  Training Loss  1.0759674296423327e-05\n","Epoch  23 Batch  102 / 228  Training Loss  1.1586186701606493e-05\n","Epoch  23 Batch  103 / 228  Training Loss  1.2378138308122288e-05\n","Epoch  23 Batch  104 / 228  Training Loss  1.1601443475228734e-05\n","Epoch  23 Batch  105 / 228  Training Loss  1.8366798030911013e-05\n","Epoch  23 Batch  106 / 228  Training Loss  1.9115776012768038e-05\n","Epoch  23 Batch  107 / 228  Training Loss  1.6354442777810618e-05\n","Epoch  23 Batch  108 / 228  Training Loss  1.0168354492634535e-05\n","Epoch  23 Batch  109 / 228  Training Loss  1.0942409971903544e-05\n","Epoch  23 Batch  110 / 228  Training Loss  1.510272250015987e-05\n","Epoch  23 Batch  111 / 228  Training Loss  1.3031926755502354e-05\n","Epoch  23 Batch  112 / 228  Training Loss  1.533268005005084e-05\n","Epoch  23 Batch  113 / 228  Training Loss  1.561660974402912e-05\n","Epoch  23 Batch  114 / 228  Training Loss  1.6732987205614336e-05\n","Epoch  23 Batch  115 / 228  Training Loss  1.7931901311385445e-05\n","Epoch  23 Batch  116 / 228  Training Loss  2.1212215870036744e-05\n","Epoch  23 Batch  117 / 228  Training Loss  1.7507782104075886e-05\n","Epoch  23 Batch  118 / 228  Training Loss  1.6949166820268147e-05\n","Epoch  23 Batch  119 / 228  Training Loss  1.1690836799971294e-05\n","Epoch  23 Batch  120 / 228  Training Loss  1.6643409253447317e-05\n","Epoch  23 Batch  121 / 228  Training Loss  1.233665352629032e-05\n","Epoch  23 Batch  122 / 228  Training Loss  1.3298672456585336e-05\n","Epoch  23 Batch  123 / 228  Training Loss  1.0703071893658489e-05\n","Epoch  23 Batch  124 / 228  Training Loss  1.1157173503306694e-05\n","Epoch  23 Batch  125 / 228  Training Loss  1.0608270713419188e-05\n","Epoch  23 Batch  126 / 228  Training Loss  1.565617094456684e-05\n","Epoch  23 Batch  127 / 228  Training Loss  1.013696692098165e-05\n","Epoch  23 Batch  128 / 228  Training Loss  1.3429802493192255e-05\n","Epoch  23 Batch  129 / 228  Training Loss  1.2489248547353782e-05\n","Epoch  23 Batch  130 / 228  Training Loss  9.031400622916408e-06\n","Epoch  23 Batch  131 / 228  Training Loss  2.272735582664609e-05\n","Epoch  23 Batch  132 / 228  Training Loss  1.1086087397416122e-05\n","Epoch  23 Batch  133 / 228  Training Loss  1.683146729192231e-05\n","Epoch  23 Batch  134 / 228  Training Loss  9.991022125177551e-06\n","Epoch  23 Batch  135 / 228  Training Loss  1.1227028153371066e-05\n","Epoch  23 Batch  136 / 228  Training Loss  1.2197568139526993e-05\n","Epoch  23 Batch  137 / 228  Training Loss  1.3448074241750874e-05\n","Epoch  23 Batch  138 / 228  Training Loss  1.83732554432936e-05\n","Epoch  23 Batch  139 / 228  Training Loss  1.3711716746911407e-05\n","Epoch  23 Batch  140 / 228  Training Loss  1.7317250240012072e-05\n","Epoch  23 Batch  141 / 228  Training Loss  1.3500537534127943e-05\n","Epoch  23 Batch  142 / 228  Training Loss  2.1888732590014115e-05\n","Epoch  23 Batch  143 / 228  Training Loss  1.3932588444731664e-05\n","Epoch  23 Batch  144 / 228  Training Loss  1.5057237760629505e-05\n","Epoch  23 Batch  145 / 228  Training Loss  1.5316878489102237e-05\n","Epoch  23 Batch  146 / 228  Training Loss  1.3014820979151409e-05\n","Epoch  23 Batch  147 / 228  Training Loss  1.0673056749510579e-05\n","Epoch  23 Batch  148 / 228  Training Loss  1.4546401871484704e-05\n","Epoch  23 Batch  149 / 228  Training Loss  1.1124561751785222e-05\n","Epoch  23 Batch  150 / 228  Training Loss  1.5569859897368588e-05\n","Epoch  23 Batch  151 / 228  Training Loss  1.3558521459344774e-05\n","Epoch  23 Batch  152 / 228  Training Loss  1.4652481695520692e-05\n","Epoch  23 Batch  153 / 228  Training Loss  1.6794898328953423e-05\n","Epoch  23 Batch  154 / 228  Training Loss  1.594695459061768e-05\n","Epoch  23 Batch  155 / 228  Training Loss  1.0698697224142961e-05\n","Epoch  23 Batch  156 / 228  Training Loss  1.4557905160472728e-05\n","Epoch  23 Batch  157 / 228  Training Loss  1.3933386071585119e-05\n","Epoch  23 Batch  158 / 228  Training Loss  1.5304945918614976e-05\n","Epoch  23 Batch  159 / 228  Training Loss  1.190431612485554e-05\n","Epoch  23 Batch  160 / 228  Training Loss  1.702349618426524e-05\n","Epoch  23 Batch  161 / 228  Training Loss  1.601397161721252e-05\n","Epoch  23 Batch  162 / 228  Training Loss  1.196268385683652e-05\n","Epoch  23 Batch  163 / 228  Training Loss  1.253446225746302e-05\n","Epoch  23 Batch  164 / 228  Training Loss  1.362725470244186e-05\n","Epoch  23 Batch  165 / 228  Training Loss  1.4632324564445298e-05\n","Epoch  23 Batch  166 / 228  Training Loss  1.2125130524509586e-05\n","Epoch  23 Batch  167 / 228  Training Loss  1.2059730579494499e-05\n","Epoch  23 Batch  168 / 228  Training Loss  1.1485475624795072e-05\n","Epoch  23 Batch  169 / 228  Training Loss  9.819663318921812e-06\n","Epoch  23 Batch  170 / 228  Training Loss  1.2461723599699326e-05\n","Epoch  23 Batch  171 / 228  Training Loss  1.1060663382522762e-05\n","Epoch  23 Batch  172 / 228  Training Loss  1.4145363820716739e-05\n","Epoch  23 Batch  173 / 228  Training Loss  8.955088560469449e-06\n","Epoch  23 Batch  174 / 228  Training Loss  1.7101756384363398e-05\n","Epoch  23 Batch  175 / 228  Training Loss  1.3728330486628693e-05\n","Epoch  23 Batch  176 / 228  Training Loss  1.3685281373909675e-05\n","Epoch  23 Batch  177 / 228  Training Loss  1.241299196408363e-05\n","Epoch  23 Batch  178 / 228  Training Loss  1.2466880434658378e-05\n","Epoch  23 Batch  179 / 228  Training Loss  1.0979654689435847e-05\n","Epoch  23 Batch  180 / 228  Training Loss  1.3834401215717662e-05\n","Epoch  23 Batch  181 / 228  Training Loss  1.2397702448652126e-05\n","Epoch  23 Batch  182 / 228  Training Loss  1.1040506251447368e-05\n","Epoch  23 Batch  183 / 228  Training Loss  1.0289068086422049e-05\n","Epoch  23 Batch  184 / 228  Training Loss  8.819397407933138e-06\n","Epoch  23 Batch  185 / 228  Training Loss  1.490526665293146e-05\n","Epoch  23 Batch  186 / 228  Training Loss  1.7671398381935433e-05\n","Epoch  23 Batch  187 / 228  Training Loss  1.7177528206957504e-05\n","Epoch  23 Batch  188 / 228  Training Loss  1.5057022210385185e-05\n","Epoch  23 Batch  189 / 228  Training Loss  1.3601007594843395e-05\n","Epoch  23 Batch  190 / 228  Training Loss  9.277173376176506e-06\n","Epoch  23 Batch  191 / 228  Training Loss  1.0799547453643754e-05\n","Epoch  23 Batch  192 / 228  Training Loss  1.0675209523469675e-05\n","Epoch  23 Batch  193 / 228  Training Loss  1.3326229236554354e-05\n","Epoch  23 Batch  194 / 228  Training Loss  1.0131110684596933e-05\n","Epoch  23 Batch  195 / 228  Training Loss  1.0923065929091536e-05\n","Epoch  23 Batch  196 / 228  Training Loss  1.1829785762529355e-05\n","Epoch  23 Batch  197 / 228  Training Loss  1.4667287359770853e-05\n","Epoch  23 Batch  198 / 228  Training Loss  1.1271981747995596e-05\n","Epoch  23 Batch  199 / 228  Training Loss  1.1511854609125294e-05\n","Epoch  23 Batch  200 / 228  Training Loss  1.1332895155646838e-05\n","Epoch  23 Batch  201 / 228  Training Loss  1.217415865539806e-05\n","Epoch  23 Batch  202 / 228  Training Loss  1.3950208085589111e-05\n","Epoch  23 Batch  203 / 228  Training Loss  1.515966869192198e-05\n","Epoch  23 Batch  204 / 228  Training Loss  1.4423104403249454e-05\n","Epoch  23 Batch  205 / 228  Training Loss  1.4349889170262031e-05\n","Epoch  23 Batch  206 / 228  Training Loss  1.4014517546456773e-05\n","Epoch  23 Batch  207 / 228  Training Loss  8.609014003013726e-06\n","Epoch  23 Batch  208 / 228  Training Loss  1.2030720426992048e-05\n","Epoch  23 Batch  209 / 228  Training Loss  1.1910282410099171e-05\n","Epoch  23 Batch  210 / 228  Training Loss  1.678353874012828e-05\n","Epoch  23 Batch  211 / 228  Training Loss  1.4835610272712074e-05\n","Epoch  23 Batch  212 / 228  Training Loss  1.7059064703062177e-05\n","Epoch  23 Batch  213 / 228  Training Loss  1.571916618559044e-05\n","Epoch  23 Batch  214 / 228  Training Loss  1.1536938473000191e-05\n","Epoch  23 Batch  215 / 228  Training Loss  1.6384359696530737e-05\n","Epoch  23 Batch  216 / 228  Training Loss  1.2322707334533334e-05\n","Epoch  23 Batch  217 / 228  Training Loss  9.27036307984963e-06\n","Epoch  23 Batch  218 / 228  Training Loss  1.0865804142667912e-05\n","Epoch  23 Batch  219 / 228  Training Loss  1.3901619240641594e-05\n","Epoch  23 Batch  220 / 228  Training Loss  1.3496470273821615e-05\n","Epoch  23 Batch  221 / 228  Training Loss  1.1427518074924592e-05\n","Epoch  23 Batch  222 / 228  Training Loss  1.3990963452670258e-05\n","Epoch  23 Batch  223 / 228  Training Loss  1.2794576832675375e-05\n","Epoch  23 Batch  224 / 228  Training Loss  8.818229616736062e-06\n","Epoch  23 Batch  225 / 228  Training Loss  1.2367175258987118e-05\n","Epoch  23 Batch  226 / 228  Training Loss  1.2027368029521313e-05\n","Epoch  23 Batch  227 / 228  Training Loss  1.1527375136211049e-05\n","  24    |    -    |   0.000014   | 99.580793\n","----------------------------------------------------------------------\n","Running epoch: 24\n","Epoch  24 Batch  0 / 228  Training Loss  1.2157104720245115e-05\n","Epoch  24 Batch  1 / 228  Training Loss  1.2192127542220987e-05\n","Epoch  24 Batch  2 / 228  Training Loss  1.0321997251594439e-05\n","Epoch  24 Batch  3 / 228  Training Loss  1.6055408195825294e-05\n","Epoch  24 Batch  4 / 228  Training Loss  1.2355236322036944e-05\n","Epoch  24 Batch  5 / 228  Training Loss  1.0593781553325243e-05\n","Epoch  24 Batch  6 / 228  Training Loss  1.1326728781568818e-05\n","Epoch  24 Batch  7 / 228  Training Loss  1.126980350818485e-05\n","Epoch  24 Batch  8 / 228  Training Loss  1.4043374903849326e-05\n","Epoch  24 Batch  9 / 228  Training Loss  1.4259092495194636e-05\n","Epoch  24 Batch  10 / 228  Training Loss  1.103443810279714e-05\n","Epoch  24 Batch  11 / 228  Training Loss  1.3715213754039723e-05\n","Epoch  24 Batch  12 / 228  Training Loss  1.4287521480582654e-05\n","Epoch  24 Batch  13 / 228  Training Loss  1.6105092072393745e-05\n","Epoch  24 Batch  14 / 228  Training Loss  9.049495929502882e-06\n","Epoch  24 Batch  15 / 228  Training Loss  1.2249745850567706e-05\n","Epoch  24 Batch  16 / 228  Training Loss  1.1254734999965876e-05\n","Epoch  24 Batch  17 / 228  Training Loss  1.2848419828515034e-05\n","Epoch  24 Batch  18 / 228  Training Loss  1.2304379197303206e-05\n","Epoch  24 Batch  19 / 228  Training Loss  1.6784646504675038e-05\n","Epoch  24 Batch  20 / 228  Training Loss  1.0617833140713628e-05\n","Epoch  24 Batch  21 / 228  Training Loss  1.716371116344817e-05\n","Epoch  24 Batch  22 / 228  Training Loss  1.2044644790876191e-05\n","Epoch  24 Batch  23 / 228  Training Loss  1.2567354133352637e-05\n","Epoch  24 Batch  24 / 228  Training Loss  1.0983520041918382e-05\n","Epoch  24 Batch  25 / 228  Training Loss  1.3301049875735771e-05\n","Epoch  24 Batch  26 / 228  Training Loss  1.3440658221952617e-05\n","Epoch  24 Batch  27 / 228  Training Loss  1.3577344361692667e-05\n","Epoch  24 Batch  28 / 228  Training Loss  1.3490664059645496e-05\n","Epoch  24 Batch  29 / 228  Training Loss  9.744087947183289e-06\n","Epoch  24 Batch  30 / 228  Training Loss  1.1669199011521414e-05\n","Epoch  24 Batch  31 / 228  Training Loss  1.4680486856377684e-05\n","Epoch  24 Batch  32 / 228  Training Loss  1.1381749573047273e-05\n","Epoch  24 Batch  33 / 228  Training Loss  9.264944310416467e-06\n","Epoch  24 Batch  34 / 228  Training Loss  2.3396165488520637e-05\n","Epoch  24 Batch  35 / 228  Training Loss  1.0047251635114662e-05\n","Epoch  24 Batch  36 / 228  Training Loss  1.5254912796081044e-05\n","Epoch  24 Batch  37 / 228  Training Loss  1.3113723071001004e-05\n","Epoch  24 Batch  38 / 228  Training Loss  1.0023154572991189e-05\n","Epoch  24 Batch  39 / 228  Training Loss  1.51282847582479e-05\n","Epoch  24 Batch  40 / 228  Training Loss  1.553151014377363e-05\n","Epoch  24 Batch  41 / 228  Training Loss  1.0490346539882012e-05\n","Epoch  24 Batch  42 / 228  Training Loss  1.0287916666129604e-05\n","Epoch  24 Batch  43 / 228  Training Loss  1.1792305485869292e-05\n","Epoch  24 Batch  44 / 228  Training Loss  1.1387788617867045e-05\n","Epoch  24 Batch  45 / 228  Training Loss  1.3860610124538653e-05\n","Epoch  24 Batch  46 / 228  Training Loss  1.2285587217775173e-05\n","Epoch  24 Batch  47 / 228  Training Loss  1.4123301298241131e-05\n","Epoch  24 Batch  48 / 228  Training Loss  1.1252318472543266e-05\n","Epoch  24 Batch  49 / 228  Training Loss  1.2621659152500797e-05\n","Epoch  24 Batch  50 / 228  Training Loss  1.8013106455327943e-05\n","Epoch  24 Batch  51 / 228  Training Loss  1.1939069736399688e-05\n","Epoch  24 Batch  52 / 228  Training Loss  1.0152458344236948e-05\n","Epoch  24 Batch  53 / 228  Training Loss  1.4774083865631837e-05\n","Epoch  24 Batch  54 / 228  Training Loss  1.2058691027050372e-05\n","Epoch  24 Batch  55 / 228  Training Loss  1.504805823060451e-05\n","Epoch  24 Batch  56 / 228  Training Loss  1.1265318789810408e-05\n","Epoch  24 Batch  57 / 228  Training Loss  1.1308389730402268e-05\n","Epoch  24 Batch  58 / 228  Training Loss  1.4181406186253298e-05\n","Epoch  24 Batch  59 / 228  Training Loss  1.2567562407639343e-05\n","Epoch  24 Batch  60 / 228  Training Loss  9.08006950339768e-06\n","Epoch  24 Batch  61 / 228  Training Loss  6.33103172731353e-06\n","Epoch  24 Batch  62 / 228  Training Loss  1.471022824262036e-05\n","Epoch  24 Batch  63 / 228  Training Loss  9.535972822050098e-06\n","Epoch  24 Batch  64 / 228  Training Loss  1.0747588930826169e-05\n","Epoch  24 Batch  65 / 228  Training Loss  1.6842739569256082e-05\n","Epoch  24 Batch  66 / 228  Training Loss  9.440750545763876e-06\n","Epoch  24 Batch  67 / 228  Training Loss  1.541758501844015e-05\n","Epoch  24 Batch  68 / 228  Training Loss  1.5651374269509688e-05\n","Epoch  24 Batch  69 / 228  Training Loss  1.4434476725000422e-05\n","Epoch  24 Batch  70 / 228  Training Loss  1.2589503967319615e-05\n","Epoch  24 Batch  71 / 228  Training Loss  9.295010386267677e-06\n","Epoch  24 Batch  72 / 228  Training Loss  1.4750601621926762e-05\n","Epoch  24 Batch  73 / 228  Training Loss  1.3405918252828997e-05\n","Epoch  24 Batch  74 / 228  Training Loss  1.3140526789356954e-05\n","Epoch  24 Batch  75 / 228  Training Loss  1.0965989531541709e-05\n","Epoch  24 Batch  76 / 228  Training Loss  1.0945776921289507e-05\n","Epoch  24 Batch  77 / 228  Training Loss  1.4782834114157595e-05\n","Epoch  24 Batch  78 / 228  Training Loss  1.1097521564806812e-05\n","Epoch  24 Batch  79 / 228  Training Loss  1.503295516158687e-05\n","Epoch  24 Batch  80 / 228  Training Loss  1.04664868558757e-05\n","Epoch  24 Batch  81 / 228  Training Loss  1.3876584489480592e-05\n","Epoch  24 Batch  82 / 228  Training Loss  8.938595783547498e-06\n","Epoch  24 Batch  83 / 228  Training Loss  1.0616228792059701e-05\n","Epoch  24 Batch  84 / 228  Training Loss  1.1734902727766894e-05\n","Epoch  24 Batch  85 / 228  Training Loss  1.329440146946581e-05\n","Epoch  24 Batch  86 / 228  Training Loss  1.630973201827146e-05\n","Epoch  24 Batch  87 / 228  Training Loss  8.91101353772683e-06\n","Epoch  24 Batch  88 / 228  Training Loss  1.2990503819310106e-05\n","Epoch  24 Batch  89 / 228  Training Loss  1.0485198799869977e-05\n","Epoch  24 Batch  90 / 228  Training Loss  1.3138092072040308e-05\n","Epoch  24 Batch  91 / 228  Training Loss  1.143686404248001e-05\n","Epoch  24 Batch  92 / 228  Training Loss  1.3749691788689233e-05\n","Epoch  24 Batch  93 / 228  Training Loss  9.413832231075503e-06\n","Epoch  24 Batch  94 / 228  Training Loss  1.3339353245100938e-05\n","Epoch  24 Batch  95 / 228  Training Loss  1.5948351574479602e-05\n","Epoch  24 Batch  96 / 228  Training Loss  1.6875719666131772e-05\n","Epoch  24 Batch  97 / 228  Training Loss  1.0466299499967135e-05\n","Epoch  24 Batch  98 / 228  Training Loss  1.1755345440178644e-05\n","Epoch  24 Batch  99 / 228  Training Loss  1.1861042366945185e-05\n","Epoch  24 Batch  100 / 228  Training Loss  1.2488289030443411e-05\n","Epoch  24 Batch  101 / 228  Training Loss  1.5096093193278648e-05\n","Epoch  24 Batch  102 / 228  Training Loss  1.4082155757932924e-05\n","Epoch  24 Batch  103 / 228  Training Loss  9.531097020953894e-06\n","Epoch  24 Batch  104 / 228  Training Loss  1.8867822291213088e-05\n","Epoch  24 Batch  105 / 228  Training Loss  1.390983561577741e-05\n","Epoch  24 Batch  106 / 228  Training Loss  1.5058671124279499e-05\n","Epoch  24 Batch  107 / 228  Training Loss  1.4245454622141551e-05\n","Epoch  24 Batch  108 / 228  Training Loss  1.5618408724549226e-05\n","Epoch  24 Batch  109 / 228  Training Loss  8.699556929059327e-06\n","Epoch  24 Batch  110 / 228  Training Loss  1.1401662959542591e-05\n","Epoch  24 Batch  111 / 228  Training Loss  1.4511211702483706e-05\n","Epoch  24 Batch  112 / 228  Training Loss  1.2180835256003775e-05\n","Epoch  24 Batch  113 / 228  Training Loss  1.4107540664554108e-05\n","Epoch  24 Batch  114 / 228  Training Loss  1.3408631275524385e-05\n","Epoch  24 Batch  115 / 228  Training Loss  1.5096297829586547e-05\n","Epoch  24 Batch  116 / 228  Training Loss  9.188990588882007e-06\n","Epoch  24 Batch  117 / 228  Training Loss  1.2371770935715176e-05\n","Epoch  24 Batch  118 / 228  Training Loss  1.190949205920333e-05\n","Epoch  24 Batch  119 / 228  Training Loss  1.622384115762543e-05\n","Epoch  24 Batch  120 / 228  Training Loss  1.0612554433464538e-05\n","Epoch  24 Batch  121 / 228  Training Loss  9.317815056419931e-06\n","Epoch  24 Batch  122 / 228  Training Loss  2.1502075469470583e-05\n","Epoch  24 Batch  123 / 228  Training Loss  1.7335709344479255e-05\n","Epoch  24 Batch  124 / 228  Training Loss  1.5173953215708025e-05\n","Epoch  24 Batch  125 / 228  Training Loss  1.3489887351170182e-05\n","Epoch  24 Batch  126 / 228  Training Loss  1.7453116015531123e-05\n","Epoch  24 Batch  127 / 228  Training Loss  1.1959289622609504e-05\n","Epoch  24 Batch  128 / 228  Training Loss  1.0130348528036848e-05\n","Epoch  24 Batch  129 / 228  Training Loss  1.1862511200888548e-05\n","Epoch  24 Batch  130 / 228  Training Loss  1.0858707355509978e-05\n","Epoch  24 Batch  131 / 228  Training Loss  1.1605619874899276e-05\n","Epoch  24 Batch  132 / 228  Training Loss  1.409780998073984e-05\n","Epoch  24 Batch  133 / 228  Training Loss  1.4815345821261872e-05\n","Epoch  24 Batch  134 / 228  Training Loss  1.5108792467799503e-05\n","Epoch  24 Batch  135 / 228  Training Loss  8.879534107109066e-06\n","Epoch  24 Batch  136 / 228  Training Loss  1.4617020497098565e-05\n","Epoch  24 Batch  137 / 228  Training Loss  1.0915233360719867e-05\n","Epoch  24 Batch  138 / 228  Training Loss  1.171271378552774e-05\n","Epoch  24 Batch  139 / 228  Training Loss  1.4718491001985967e-05\n","Epoch  24 Batch  140 / 228  Training Loss  1.0012672646553256e-05\n","Epoch  24 Batch  141 / 228  Training Loss  1.346785302303033e-05\n","Epoch  24 Batch  142 / 228  Training Loss  1.4511577319353819e-05\n","Epoch  24 Batch  143 / 228  Training Loss  1.2923112080898136e-05\n","Epoch  24 Batch  144 / 228  Training Loss  1.6165762644959614e-05\n","Epoch  24 Batch  145 / 228  Training Loss  1.2729159607260954e-05\n","Epoch  24 Batch  146 / 228  Training Loss  1.2410046110744588e-05\n","Epoch  24 Batch  147 / 228  Training Loss  1.3302435036166571e-05\n","Epoch  24 Batch  148 / 228  Training Loss  1.3897095413994975e-05\n","Epoch  24 Batch  149 / 228  Training Loss  1.6349957149941474e-05\n","Epoch  24 Batch  150 / 228  Training Loss  1.285481994273141e-05\n","Epoch  24 Batch  151 / 228  Training Loss  9.69962275121361e-06\n","Epoch  24 Batch  152 / 228  Training Loss  1.3672130080522038e-05\n","Epoch  24 Batch  153 / 228  Training Loss  1.3808404219162185e-05\n","Epoch  24 Batch  154 / 228  Training Loss  1.391159003105713e-05\n","Epoch  24 Batch  155 / 228  Training Loss  1.3936170034867246e-05\n","Epoch  24 Batch  156 / 228  Training Loss  9.934660738508683e-06\n","Epoch  24 Batch  157 / 228  Training Loss  1.7843991372501478e-05\n","Epoch  24 Batch  158 / 228  Training Loss  1.679308115853928e-05\n","Epoch  24 Batch  159 / 228  Training Loss  1.1296671800664626e-05\n","Epoch  24 Batch  160 / 228  Training Loss  8.132989933073986e-06\n","Epoch  24 Batch  161 / 228  Training Loss  9.750302524480503e-06\n","Epoch  24 Batch  162 / 228  Training Loss  1.0763791578938253e-05\n","Epoch  24 Batch  163 / 228  Training Loss  1.0021400157711469e-05\n","Epoch  24 Batch  164 / 228  Training Loss  1.0200354154221714e-05\n","Epoch  24 Batch  165 / 228  Training Loss  9.372424756293185e-06\n","Epoch  24 Batch  166 / 228  Training Loss  1.271866221941309e-05\n","Epoch  24 Batch  167 / 228  Training Loss  1.156817415903788e-05\n","Epoch  24 Batch  168 / 228  Training Loss  1.2939989574078936e-05\n","Epoch  24 Batch  169 / 228  Training Loss  1.4114770237938501e-05\n","Epoch  24 Batch  170 / 228  Training Loss  9.667208360042423e-06\n","Epoch  24 Batch  171 / 228  Training Loss  1.1001269740518183e-05\n","Epoch  24 Batch  172 / 228  Training Loss  1.2710933333437424e-05\n","Epoch  24 Batch  173 / 228  Training Loss  1.4043214832781814e-05\n","Epoch  24 Batch  174 / 228  Training Loss  1.1207204806851223e-05\n","Epoch  24 Batch  175 / 228  Training Loss  1.6629455785732716e-05\n","Epoch  24 Batch  176 / 228  Training Loss  1.2503674952313304e-05\n","Epoch  24 Batch  177 / 228  Training Loss  1.4675259990326595e-05\n","Epoch  24 Batch  178 / 228  Training Loss  1.5339444871642627e-05\n","Epoch  24 Batch  179 / 228  Training Loss  1.0348425348638557e-05\n","Epoch  24 Batch  180 / 228  Training Loss  1.1313221875752788e-05\n","Epoch  24 Batch  181 / 228  Training Loss  1.4144103261060081e-05\n","Epoch  24 Batch  182 / 228  Training Loss  1.0010829100792762e-05\n","Epoch  24 Batch  183 / 228  Training Loss  1.1613777132879477e-05\n","Epoch  24 Batch  184 / 228  Training Loss  1.3631171896122396e-05\n","Epoch  24 Batch  185 / 228  Training Loss  1.0376799764344469e-05\n","Epoch  24 Batch  186 / 228  Training Loss  1.194504238810623e-05\n","Epoch  24 Batch  187 / 228  Training Loss  1.2151221199019346e-05\n","Epoch  24 Batch  188 / 228  Training Loss  1.2985069588467013e-05\n","Epoch  24 Batch  189 / 228  Training Loss  1.3060163837508298e-05\n","Epoch  24 Batch  190 / 228  Training Loss  1.3539790415961761e-05\n","Epoch  24 Batch  191 / 228  Training Loss  1.1093106877524406e-05\n","Epoch  24 Batch  192 / 228  Training Loss  1.028494989441242e-05\n","Epoch  24 Batch  193 / 228  Training Loss  8.468065061606467e-06\n","Epoch  24 Batch  194 / 228  Training Loss  1.376055479340721e-05\n","Epoch  24 Batch  195 / 228  Training Loss  1.6939531633397564e-05\n","Epoch  24 Batch  196 / 228  Training Loss  1.0568846846581437e-05\n","Epoch  24 Batch  197 / 228  Training Loss  1.4990003364800941e-05\n","Epoch  24 Batch  198 / 228  Training Loss  1.3867659617972095e-05\n","Epoch  24 Batch  199 / 228  Training Loss  1.265658465854358e-05\n","Epoch  24 Batch  200 / 228  Training Loss  9.659597708377987e-06\n","Epoch  24 Batch  201 / 228  Training Loss  1.0261434908898082e-05\n","Epoch  24 Batch  202 / 228  Training Loss  1.407735726388637e-05\n","Epoch  24 Batch  203 / 228  Training Loss  1.4113955330685712e-05\n","Epoch  24 Batch  204 / 228  Training Loss  1.0657849998096935e-05\n","Epoch  24 Batch  205 / 228  Training Loss  1.8480763174011372e-05\n","Epoch  24 Batch  206 / 228  Training Loss  1.0663150533218868e-05\n","Epoch  24 Batch  207 / 228  Training Loss  1.0894239494518843e-05\n","Epoch  24 Batch  208 / 228  Training Loss  1.4235966773412656e-05\n","Epoch  24 Batch  209 / 228  Training Loss  9.577533091942314e-06\n","Epoch  24 Batch  210 / 228  Training Loss  1.5298513972084038e-05\n","Epoch  24 Batch  211 / 228  Training Loss  8.144887033267878e-06\n","Epoch  24 Batch  212 / 228  Training Loss  1.4186853150022216e-05\n","Epoch  24 Batch  213 / 228  Training Loss  1.3917138858232647e-05\n","Epoch  24 Batch  214 / 228  Training Loss  1.2382637578411959e-05\n","Epoch  24 Batch  215 / 228  Training Loss  1.544810220366344e-05\n","Epoch  24 Batch  216 / 228  Training Loss  1.267371408175677e-05\n","Epoch  24 Batch  217 / 228  Training Loss  1.2406531823216937e-05\n","Epoch  24 Batch  218 / 228  Training Loss  1.2726204658974893e-05\n","Epoch  24 Batch  219 / 228  Training Loss  1.2209822671138681e-05\n","Epoch  24 Batch  220 / 228  Training Loss  1.0470079359947704e-05\n","Epoch  24 Batch  221 / 228  Training Loss  1.3383275472733658e-05\n","Epoch  24 Batch  222 / 228  Training Loss  9.33046430873219e-06\n","Epoch  24 Batch  223 / 228  Training Loss  1.1814069694082718e-05\n","Epoch  24 Batch  224 / 228  Training Loss  1.6523667000001296e-05\n","Epoch  24 Batch  225 / 228  Training Loss  1.1394795365049504e-05\n","Epoch  24 Batch  226 / 228  Training Loss  1.3951599612482823e-05\n","Epoch  24 Batch  227 / 228  Training Loss  1.2357148989394773e-05\n","  25    |    -    |   0.000013   | 99.580793\n","----------------------------------------------------------------------\n","Running epoch: 25\n","Epoch  25 Batch  0 / 228  Training Loss  1.0671146810636856e-05\n","Epoch  25 Batch  1 / 228  Training Loss  1.2716778655885719e-05\n","Epoch  25 Batch  2 / 228  Training Loss  1.5555566278635524e-05\n","Epoch  25 Batch  3 / 228  Training Loss  1.1207378520339262e-05\n","Epoch  25 Batch  4 / 228  Training Loss  1.1618526514212135e-05\n","Epoch  25 Batch  5 / 228  Training Loss  9.768320524017327e-06\n","Epoch  25 Batch  6 / 228  Training Loss  1.2030578545818571e-05\n","Epoch  25 Batch  7 / 228  Training Loss  1.4302886484074406e-05\n","Epoch  25 Batch  8 / 228  Training Loss  1.0485697202966549e-05\n","Epoch  25 Batch  9 / 228  Training Loss  1.293887362407986e-05\n","Epoch  25 Batch  10 / 228  Training Loss  1.0990561349899508e-05\n","Epoch  25 Batch  11 / 228  Training Loss  1.290876571147237e-05\n","Epoch  25 Batch  12 / 228  Training Loss  8.930879630497657e-06\n","Epoch  25 Batch  13 / 228  Training Loss  1.329084079770837e-05\n","Epoch  25 Batch  14 / 228  Training Loss  1.4868048310745507e-05\n","Epoch  25 Batch  15 / 228  Training Loss  1.0064904927276075e-05\n","Epoch  25 Batch  16 / 228  Training Loss  1.3199261047702748e-05\n","Epoch  25 Batch  17 / 228  Training Loss  8.906885341275483e-06\n","Epoch  25 Batch  18 / 228  Training Loss  1.0370574273110833e-05\n","Epoch  25 Batch  19 / 228  Training Loss  1.35370401039836e-05\n","Epoch  25 Batch  20 / 228  Training Loss  7.585056209791219e-06\n","Epoch  25 Batch  21 / 228  Training Loss  1.678179978625849e-05\n","Epoch  25 Batch  22 / 228  Training Loss  7.086741334205726e-06\n","Epoch  25 Batch  23 / 228  Training Loss  1.2428322406776715e-05\n","Epoch  25 Batch  24 / 228  Training Loss  1.1181053196196444e-05\n","Epoch  25 Batch  25 / 228  Training Loss  1.0544518772803713e-05\n","Epoch  25 Batch  26 / 228  Training Loss  1.4042196198715828e-05\n","Epoch  25 Batch  27 / 228  Training Loss  1.2590107871801592e-05\n","Epoch  25 Batch  28 / 228  Training Loss  1.3887282875657547e-05\n","Epoch  25 Batch  29 / 228  Training Loss  7.827009540051222e-06\n","Epoch  25 Batch  30 / 228  Training Loss  1.4497255506285e-05\n","Epoch  25 Batch  31 / 228  Training Loss  1.4627384189225268e-05\n","Epoch  25 Batch  32 / 228  Training Loss  1.4644295333710033e-05\n","Epoch  25 Batch  33 / 228  Training Loss  8.946595698944293e-06\n","Epoch  25 Batch  34 / 228  Training Loss  7.512834599765483e-06\n","Epoch  25 Batch  35 / 228  Training Loss  8.424589395872317e-06\n","Epoch  25 Batch  36 / 228  Training Loss  1.3717764886678196e-05\n","Epoch  25 Batch  37 / 228  Training Loss  1.7539354303153232e-05\n","Epoch  25 Batch  38 / 228  Training Loss  1.2016063919872977e-05\n","Epoch  25 Batch  39 / 228  Training Loss  1.3228986063040793e-05\n","Epoch  25 Batch  40 / 228  Training Loss  9.267459972761571e-06\n","Epoch  25 Batch  41 / 228  Training Loss  1.409910782967927e-05\n","Epoch  25 Batch  42 / 228  Training Loss  8.35776336316485e-06\n","Epoch  25 Batch  43 / 228  Training Loss  9.820902050705627e-06\n","Epoch  25 Batch  44 / 228  Training Loss  1.0748709428298753e-05\n","Epoch  25 Batch  45 / 228  Training Loss  9.568850146024488e-06\n","Epoch  25 Batch  46 / 228  Training Loss  8.10358687886037e-06\n","Epoch  25 Batch  47 / 228  Training Loss  1.5414592780871317e-05\n","Epoch  25 Batch  48 / 228  Training Loss  9.73222358879866e-06\n","Epoch  25 Batch  49 / 228  Training Loss  1.6076175597845577e-05\n","Epoch  25 Batch  50 / 228  Training Loss  1.730778967612423e-05\n","Epoch  25 Batch  51 / 228  Training Loss  1.2510399756138213e-05\n","Epoch  25 Batch  52 / 228  Training Loss  1.2287043318792712e-05\n","Epoch  25 Batch  53 / 228  Training Loss  1.657590109971352e-05\n","Epoch  25 Batch  54 / 228  Training Loss  1.2623514521692414e-05\n","Epoch  25 Batch  55 / 228  Training Loss  9.422723451280035e-06\n","Epoch  25 Batch  56 / 228  Training Loss  1.1476904546725564e-05\n","Epoch  25 Batch  57 / 228  Training Loss  1.6439280443591997e-05\n","Epoch  25 Batch  58 / 228  Training Loss  1.2840592717111576e-05\n","Epoch  25 Batch  59 / 228  Training Loss  1.3846290130459238e-05\n","Epoch  25 Batch  60 / 228  Training Loss  1.1410877959860954e-05\n","Epoch  25 Batch  61 / 228  Training Loss  1.183303538709879e-05\n","Epoch  25 Batch  62 / 228  Training Loss  1.138544666900998e-05\n","Epoch  25 Batch  63 / 228  Training Loss  1.1194870239705779e-05\n","Epoch  25 Batch  64 / 228  Training Loss  1.3318953278940171e-05\n","Epoch  25 Batch  65 / 228  Training Loss  1.2292189239815343e-05\n","Epoch  25 Batch  66 / 228  Training Loss  1.534357033960987e-05\n","Epoch  25 Batch  67 / 228  Training Loss  7.315308721445035e-06\n","Epoch  25 Batch  68 / 228  Training Loss  1.3879920516046695e-05\n","Epoch  25 Batch  69 / 228  Training Loss  1.2525985766842496e-05\n","Epoch  25 Batch  70 / 228  Training Loss  1.2938010513607878e-05\n","Epoch  25 Batch  71 / 228  Training Loss  1.450867966923397e-05\n","Epoch  25 Batch  72 / 228  Training Loss  1.1547126632649451e-05\n","Epoch  25 Batch  73 / 228  Training Loss  1.1975422239629552e-05\n","Epoch  25 Batch  74 / 228  Training Loss  1.0458287761139218e-05\n","Epoch  25 Batch  75 / 228  Training Loss  1.1553034710232168e-05\n","Epoch  25 Batch  76 / 228  Training Loss  1.4728226233273745e-05\n","Epoch  25 Batch  77 / 228  Training Loss  1.0232596650894266e-05\n","Epoch  25 Batch  78 / 228  Training Loss  1.0399451639386825e-05\n","Epoch  25 Batch  79 / 228  Training Loss  9.849556590779684e-06\n","Epoch  25 Batch  80 / 228  Training Loss  1.7623377061681822e-05\n","Epoch  25 Batch  81 / 228  Training Loss  1.3916183888795786e-05\n","Epoch  25 Batch  82 / 228  Training Loss  1.0679640581656713e-05\n","Epoch  25 Batch  83 / 228  Training Loss  8.35864102555206e-06\n","Epoch  25 Batch  84 / 228  Training Loss  1.3241731721791439e-05\n","Epoch  25 Batch  85 / 228  Training Loss  9.720924026623834e-06\n","Epoch  25 Batch  86 / 228  Training Loss  1.4887370525684673e-05\n","Epoch  25 Batch  87 / 228  Training Loss  1.3722134099225514e-05\n","Epoch  25 Batch  88 / 228  Training Loss  1.2863123629358597e-05\n","Epoch  25 Batch  89 / 228  Training Loss  1.1520767657202668e-05\n","Epoch  25 Batch  90 / 228  Training Loss  1.2353857528069057e-05\n","Epoch  25 Batch  91 / 228  Training Loss  1.3024271538597532e-05\n","Epoch  25 Batch  92 / 228  Training Loss  1.3382328688749112e-05\n","Epoch  25 Batch  93 / 228  Training Loss  1.2069938748027198e-05\n","Epoch  25 Batch  94 / 228  Training Loss  1.121408786275424e-05\n","Epoch  25 Batch  95 / 228  Training Loss  8.71177780936705e-06\n","Epoch  25 Batch  96 / 228  Training Loss  1.200360475195339e-05\n","Epoch  25 Batch  97 / 228  Training Loss  1.3217378182162065e-05\n","Epoch  25 Batch  98 / 228  Training Loss  1.0792087778099813e-05\n","Epoch  25 Batch  99 / 228  Training Loss  1.0658715837053023e-05\n","Epoch  25 Batch  100 / 228  Training Loss  1.421445358573692e-05\n","Epoch  25 Batch  101 / 228  Training Loss  1.4920151443220675e-05\n","Epoch  25 Batch  102 / 228  Training Loss  1.2056368177582044e-05\n","Epoch  25 Batch  103 / 228  Training Loss  1.2596392480190843e-05\n","Epoch  25 Batch  104 / 228  Training Loss  1.2886494914710056e-05\n","Epoch  25 Batch  105 / 228  Training Loss  1.0670186384231783e-05\n","Epoch  25 Batch  106 / 228  Training Loss  1.057661484082928e-05\n","Epoch  25 Batch  107 / 228  Training Loss  1.050012360792607e-05\n","Epoch  25 Batch  108 / 228  Training Loss  9.925237463903613e-06\n","Epoch  25 Batch  109 / 228  Training Loss  1.3860330909665208e-05\n","Epoch  25 Batch  110 / 228  Training Loss  1.1586629625526257e-05\n","Epoch  25 Batch  111 / 228  Training Loss  1.1040139725082554e-05\n","Epoch  25 Batch  112 / 228  Training Loss  1.3275353012431879e-05\n","Epoch  25 Batch  113 / 228  Training Loss  1.4087803720030934e-05\n","Epoch  25 Batch  114 / 228  Training Loss  9.355036127089988e-06\n","Epoch  25 Batch  115 / 228  Training Loss  7.495616500818869e-06\n","Epoch  25 Batch  116 / 228  Training Loss  1.2226353646838106e-05\n","Epoch  25 Batch  117 / 228  Training Loss  8.21063258626964e-06\n","Epoch  25 Batch  118 / 228  Training Loss  8.727092790650204e-06\n","Epoch  25 Batch  119 / 228  Training Loss  1.4412064047064632e-05\n","Epoch  25 Batch  120 / 228  Training Loss  8.847060598782264e-06\n","Epoch  25 Batch  121 / 228  Training Loss  1.2113935554225463e-05\n","Epoch  25 Batch  122 / 228  Training Loss  1.242310281668324e-05\n","Epoch  25 Batch  123 / 228  Training Loss  1.2881549992016517e-05\n","Epoch  25 Batch  124 / 228  Training Loss  1.2181676538602915e-05\n","Epoch  25 Batch  125 / 228  Training Loss  9.373312423122115e-06\n","Epoch  25 Batch  126 / 228  Training Loss  1.0325406947231386e-05\n","Epoch  25 Batch  127 / 228  Training Loss  1.9848308511427604e-05\n","Epoch  25 Batch  128 / 228  Training Loss  1.442766824766295e-05\n","Epoch  25 Batch  129 / 228  Training Loss  1.2534571396827232e-05\n","Epoch  25 Batch  130 / 228  Training Loss  1.1375526810297742e-05\n","Epoch  25 Batch  131 / 228  Training Loss  1.4547435057465918e-05\n","Epoch  25 Batch  132 / 228  Training Loss  1.237804644915741e-05\n","Epoch  25 Batch  133 / 228  Training Loss  9.897498784994241e-06\n","Epoch  25 Batch  134 / 228  Training Loss  1.0913374353549443e-05\n","Epoch  25 Batch  135 / 228  Training Loss  1.0523812306928448e-05\n","Epoch  25 Batch  136 / 228  Training Loss  1.3076099094178062e-05\n","Epoch  25 Batch  137 / 228  Training Loss  9.239121027349029e-06\n","Epoch  25 Batch  138 / 228  Training Loss  1.1947962775593624e-05\n","Epoch  25 Batch  139 / 228  Training Loss  1.1373276720405556e-05\n","Epoch  25 Batch  140 / 228  Training Loss  1.0714615200413391e-05\n","Epoch  25 Batch  141 / 228  Training Loss  1.257517851627199e-05\n","Epoch  25 Batch  142 / 228  Training Loss  1.4315734915726352e-05\n","Epoch  25 Batch  143 / 228  Training Loss  1.5708155842730775e-05\n","Epoch  25 Batch  144 / 228  Training Loss  1.498219535278622e-05\n","Epoch  25 Batch  145 / 228  Training Loss  1.2120129213144537e-05\n","Epoch  25 Batch  146 / 228  Training Loss  1.085942130885087e-05\n","Epoch  25 Batch  147 / 228  Training Loss  1.4730023394804448e-05\n","Epoch  25 Batch  148 / 228  Training Loss  1.2605902156792581e-05\n","Epoch  25 Batch  149 / 228  Training Loss  1.5246468137775082e-05\n","Epoch  25 Batch  150 / 228  Training Loss  1.0204104910371825e-05\n","Epoch  25 Batch  151 / 228  Training Loss  1.0521706826693844e-05\n","Epoch  25 Batch  152 / 228  Training Loss  9.931175554811489e-06\n","Epoch  25 Batch  153 / 228  Training Loss  1.3668885003426112e-05\n","Epoch  25 Batch  154 / 228  Training Loss  8.781894393905532e-06\n","Epoch  25 Batch  155 / 228  Training Loss  1.3585869965027086e-05\n","Epoch  25 Batch  156 / 228  Training Loss  1.0527883205213584e-05\n","Epoch  25 Batch  157 / 228  Training Loss  1.4831767657597084e-05\n","Epoch  25 Batch  158 / 228  Training Loss  1.2246404367033392e-05\n","Epoch  25 Batch  159 / 228  Training Loss  1.2551981853903271e-05\n","Epoch  25 Batch  160 / 228  Training Loss  1.2810812222596724e-05\n","Epoch  25 Batch  161 / 228  Training Loss  1.1743535651476122e-05\n","Epoch  25 Batch  162 / 228  Training Loss  1.8225469830213115e-05\n","Epoch  25 Batch  163 / 228  Training Loss  1.3285583918332122e-05\n","Epoch  25 Batch  164 / 228  Training Loss  8.771236934990156e-06\n","Epoch  25 Batch  165 / 228  Training Loss  1.473619613534538e-05\n","Epoch  25 Batch  166 / 228  Training Loss  1.7067435692297295e-05\n","Epoch  25 Batch  167 / 228  Training Loss  9.876542208075989e-06\n","Epoch  25 Batch  168 / 228  Training Loss  1.3808855328534264e-05\n","Epoch  25 Batch  169 / 228  Training Loss  1.1275177712377626e-05\n","Epoch  25 Batch  170 / 228  Training Loss  1.1117808753624558e-05\n","Epoch  25 Batch  171 / 228  Training Loss  1.1840708793897647e-05\n","Epoch  25 Batch  172 / 228  Training Loss  6.970575668674428e-06\n","Epoch  25 Batch  173 / 228  Training Loss  1.453386175853666e-05\n","Epoch  25 Batch  174 / 228  Training Loss  1.3036359632678796e-05\n","Epoch  25 Batch  175 / 228  Training Loss  9.444088391319383e-06\n","Epoch  25 Batch  176 / 228  Training Loss  8.409487236349378e-06\n","Epoch  25 Batch  177 / 228  Training Loss  9.991420483856928e-06\n","Epoch  25 Batch  178 / 228  Training Loss  9.35952084546443e-06\n","Epoch  25 Batch  179 / 228  Training Loss  1.1845861081383191e-05\n","Epoch  25 Batch  180 / 228  Training Loss  1.279518437513616e-05\n","Epoch  25 Batch  181 / 228  Training Loss  9.996682820201386e-06\n","Epoch  25 Batch  182 / 228  Training Loss  1.4994434422987979e-05\n","Epoch  25 Batch  183 / 228  Training Loss  1.2332420737948269e-05\n","Epoch  25 Batch  184 / 228  Training Loss  1.4152605217532255e-05\n","Epoch  25 Batch  185 / 228  Training Loss  8.87772512214724e-06\n","Epoch  25 Batch  186 / 228  Training Loss  1.2336126928857993e-05\n","Epoch  25 Batch  187 / 228  Training Loss  1.2135447832406498e-05\n","Epoch  25 Batch  188 / 228  Training Loss  1.350469119643094e-05\n","Epoch  25 Batch  189 / 228  Training Loss  1.0240017218166031e-05\n","Epoch  25 Batch  190 / 228  Training Loss  1.3119775758241303e-05\n","Epoch  25 Batch  191 / 228  Training Loss  1.3862714695278555e-05\n","Epoch  25 Batch  192 / 228  Training Loss  1.4470660062215757e-05\n","Epoch  25 Batch  193 / 228  Training Loss  1.068510209734086e-05\n","Epoch  25 Batch  194 / 228  Training Loss  9.120960385189392e-06\n","Epoch  25 Batch  195 / 228  Training Loss  1.1280752005404793e-05\n","Epoch  25 Batch  196 / 228  Training Loss  1.013214478007285e-05\n","Epoch  25 Batch  197 / 228  Training Loss  1.1343241567374207e-05\n","Epoch  25 Batch  198 / 228  Training Loss  1.338190213573398e-05\n","Epoch  25 Batch  199 / 228  Training Loss  1.4923702110536397e-05\n","Epoch  25 Batch  200 / 228  Training Loss  8.641864042147063e-06\n","Epoch  25 Batch  201 / 228  Training Loss  1.4633349564974196e-05\n","Epoch  25 Batch  202 / 228  Training Loss  9.233024684363045e-06\n","Epoch  25 Batch  203 / 228  Training Loss  1.4761823877051938e-05\n","Epoch  25 Batch  204 / 228  Training Loss  1.1077641829615459e-05\n","Epoch  25 Batch  205 / 228  Training Loss  1.4387999726750422e-05\n","Epoch  25 Batch  206 / 228  Training Loss  1.2957309081684798e-05\n","Epoch  25 Batch  207 / 228  Training Loss  1.0737638149294071e-05\n","Epoch  25 Batch  208 / 228  Training Loss  1.1840111255878583e-05\n","Epoch  25 Batch  209 / 228  Training Loss  9.896639312501065e-06\n","Epoch  25 Batch  210 / 228  Training Loss  1.487470854044659e-05\n","Epoch  25 Batch  211 / 228  Training Loss  1.2369411706458777e-05\n","Epoch  25 Batch  212 / 228  Training Loss  1.2540066563815344e-05\n","Epoch  25 Batch  213 / 228  Training Loss  1.2008020348730497e-05\n","Epoch  25 Batch  214 / 228  Training Loss  1.2497410352807492e-05\n","Epoch  25 Batch  215 / 228  Training Loss  1.56889309437247e-05\n","Epoch  25 Batch  216 / 228  Training Loss  1.2154754585935734e-05\n","Epoch  25 Batch  217 / 228  Training Loss  9.749823220772669e-06\n","Epoch  25 Batch  218 / 228  Training Loss  1.14369613584131e-05\n","Epoch  25 Batch  219 / 228  Training Loss  1.5577270460198633e-05\n","Epoch  25 Batch  220 / 228  Training Loss  9.744588169269264e-06\n","Epoch  25 Batch  221 / 228  Training Loss  8.581895599490963e-06\n","Epoch  25 Batch  222 / 228  Training Loss  1.009100833471166e-05\n","Epoch  25 Batch  223 / 228  Training Loss  1.2490610060922336e-05\n","Epoch  25 Batch  224 / 228  Training Loss  1.0443076462252066e-05\n","Epoch  25 Batch  225 / 228  Training Loss  9.86325085250428e-06\n","Epoch  25 Batch  226 / 228  Training Loss  1.2878283087047748e-05\n","Epoch  25 Batch  227 / 228  Training Loss  8.845530828693882e-06\n","  26    |    -    |   0.000012   | 99.580793\n","----------------------------------------------------------------------\n"]}]},{"cell_type":"code","metadata":{"id":"iTiNbM081LlD"},"source":["# Changing the directory to store the model there.\n","# print(os.getcwd())\n","# os.chdir('/content/drive/My Drive/Colab Notebooks/new/')\n","# print(os.getcwd())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c3bq66KD1eg7"},"source":["#### Saving the Model (creating checkpoint)"]},{"cell_type":"code","metadata":{"id":"-fJcvYcG1nd3"},"source":["# PATH = \"fine_tune_10e_25eph.pt\"\n","# torch.save({\n","#             'epoch': num_of_epochs,\n","#             'model_state_dict': model.state_dict(),\n","#             'optimizer_state_dict': optimizer.state_dict(),\n","#             'loss': running_loss,\n","#             }, PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AuyVxTpoGouA","executionInfo":{"status":"ok","timestamp":1638640281882,"user_tz":420,"elapsed":19767,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["model_load = T5ForConditionalGeneration.from_pretrained('Fine_tune_10based_model_v1.bin', return_dict=True, config='t5-base-config.json')"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"elq45fvZvxKR","executionInfo":{"status":"ok","timestamp":1638640303508,"user_tz":420,"elapsed":303,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"74b779b7-b471-43ec-ca5a-22a81539ce1b"},"source":["model_load.to('cpu')\n","# model.to(device)\n","# evaluate(model_load, test_dataloader)"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"vM-_2K5uyV6V","executionInfo":{"status":"ok","timestamp":1638640304734,"user_tz":420,"elapsed":137,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["# model_load.to(device)"],"execution_count":48,"outputs":[]},{"cell_type":"code","metadata":{"id":"lWLx5DywGtHI","executionInfo":{"status":"ok","timestamp":1638640305926,"user_tz":420,"elapsed":2,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["# Function to generate sentences from symptoms on the test dataset\n","def generateText(text):\n","  model_load.eval()\n","  input_ids = tokenizer.encode(text, return_tensors=\"pt\")  # Batch size 1\n","  # s = time.time()\n","  outputs = model_load.generate(input_ids)\n","  prediction=tokenizer.decode(outputs[0]).replace('<pad>','').replace('</s>','')\n","  return prediction"],"execution_count":49,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H8ZwnGO_xplA","executionInfo":{"status":"ok","timestamp":1638640307666,"user_tz":420,"elapsed":230,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"e03a4805-4535-4863-a8ee-ad1b1719bb59"},"source":["model_load.device"],"execution_count":50,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"KllLHDqnJEUV","executionInfo":{"status":"ok","timestamp":1638640309407,"user_tz":420,"elapsed":181,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"1500914e-36d8-4e72-dfb4-0940b7c1484a"},"source":["data_valid\n","# testing on this for now"],"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>inputs</th>\n","      <th>target</th>\n","      <th>maxlen</th>\n","      <th>minlen</th>\n","      <th>sumlen</th>\n","      <th>target_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The sum of 1 1000 2 100 7 10 9 and 1 1000 8 10...</td>\n","      <td>3 1000 1 100 6 10 9</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>3 1000 1 100 6 10 9</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The sum of 4 100 8 10 0 and 1 1000 8 100 1 10 ...</td>\n","      <td>2 1000 2 100 9 10 0</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>2 1000 2 100 9 10 0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The sum of 3 100 2 10 7 and 1 1000 2 100 1 10 ...</td>\n","      <td>1 1000 5 100 4 10 1</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1 1000 5 100 4 10 1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The sum of 1 1000 4 100 6 10 0 and 4 100 0 10 ...</td>\n","      <td>1 1000 8 100 6 10 8</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1 1000 8 100 6 10 8</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The sum of 1 1000 8 100 1 10 8 and 1 1000 1 10...</td>\n","      <td>2 1000 9 100 3 10 1</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2 1000 9 100 3 10 1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2595</th>\n","      <td>The sum of 1 1000 0 100 0 10 7 and 8 100 1 10 ...</td>\n","      <td>1 1000 8 100 2 10 0</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1 1000 8 100 2 10 0</td>\n","    </tr>\n","    <tr>\n","      <th>2596</th>\n","      <td>The sum of 2 100 6 10 3 and 5 100 6 10 6 is</td>\n","      <td>8 100 2 10 9</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>8 100 2 10 9</td>\n","    </tr>\n","    <tr>\n","      <th>2597</th>\n","      <td>The sum of 1 1000 9 100 8 10 7 and 1 100 9 10 ...</td>\n","      <td>2 1000 1 100 8 10 1</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>2 1000 1 100 8 10 1</td>\n","    </tr>\n","    <tr>\n","      <th>2598</th>\n","      <td>The sum of 1 1000 5 100 7 10 6 and 5 100 8 10 ...</td>\n","      <td>2 1000 1 100 5 10 6</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>2 1000 1 100 5 10 6</td>\n","    </tr>\n","    <tr>\n","      <th>2599</th>\n","      <td>The sum of 3 100 5 10 2 and 1 1000 4 100 3 10 ...</td>\n","      <td>1 1000 7 100 8 10 8</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1 1000 7 100 8 10 8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2600 rows × 6 columns</p>\n","</div>"],"text/plain":["                                                 inputs  ...           target_str\n","0     The sum of 1 1000 2 100 7 10 9 and 1 1000 8 10...  ...  3 1000 1 100 6 10 9\n","1     The sum of 4 100 8 10 0 and 1 1000 8 100 1 10 ...  ...  2 1000 2 100 9 10 0\n","2     The sum of 3 100 2 10 7 and 1 1000 2 100 1 10 ...  ...  1 1000 5 100 4 10 1\n","3     The sum of 1 1000 4 100 6 10 0 and 4 100 0 10 ...  ...  1 1000 8 100 6 10 8\n","4     The sum of 1 1000 8 100 1 10 8 and 1 1000 1 10...  ...  2 1000 9 100 3 10 1\n","...                                                 ...  ...                  ...\n","2595  The sum of 1 1000 0 100 0 10 7 and 8 100 1 10 ...  ...  1 1000 8 100 2 10 0\n","2596       The sum of 2 100 6 10 3 and 5 100 6 10 6 is   ...         8 100 2 10 9\n","2597  The sum of 1 1000 9 100 8 10 7 and 1 100 9 10 ...  ...  2 1000 1 100 8 10 1\n","2598  The sum of 1 1000 5 100 7 10 6 and 5 100 8 10 ...  ...  2 1000 1 100 5 10 6\n","2599  The sum of 3 100 5 10 2 and 1 1000 4 100 3 10 ...  ...  1 1000 7 100 8 10 8\n","\n","[2600 rows x 6 columns]"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"mMFAn0nSGy2y","executionInfo":{"status":"ok","timestamp":1638640312245,"user_tz":420,"elapsed":897,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"30f8898e-ec90-4e21-b197-619a581879d7"},"source":["generateText(data_valid.inputs[0])"],"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' 3 1000 1 100 6 10 9'"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"f5HnhuzcrudM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638640393731,"user_tz":420,"elapsed":134,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"1800ffc0-faf6-440a-f619-81e7cdb51d55"},"source":["data_ood = pd.read_csv('data_10_full_ood_v1.csv')\n","print(data_ood.head(5))"],"execution_count":54,"outputs":[{"output_type":"stream","name":"stdout","text":["                                              inputs  ... sumlen\n","0  The sum of 2 1000 4 100 2 10 7 and 2 1000 1 10...  ...      4\n","1  The sum of 3 1000 5 100 5 10 8 and 2 1000 3 10...  ...      4\n","2  The sum of 3 1000 9 100 3 10 7 and 3 1000 5 10...  ...      4\n","3  The sum of 2 1000 6 100 8 10 1 and 3 1000 9 10...  ...      4\n","4  The sum of 2 1000 0 100 4 10 1 and 2 1000 4 10...  ...      4\n","\n","[5 rows x 5 columns]\n"]}]},{"cell_type":"code","metadata":{"id":"blh9aBAb3Zq0","executionInfo":{"status":"ok","timestamp":1638640396565,"user_tz":420,"elapsed":137,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["data_ood['type'] = \"OOD\""],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"id":"g6POXTzo3mEl","executionInfo":{"status":"ok","timestamp":1638640398875,"user_tz":420,"elapsed":116,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["data_test['type'] = 'iid'\n","data_ood['target_str'] = data_ood['target'].astype(str)"],"execution_count":56,"outputs":[]},{"cell_type":"code","metadata":{"id":"x53-zStY3rVU","executionInfo":{"status":"ok","timestamp":1638640400240,"user_tz":420,"elapsed":131,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["data_full = data_ood + data_test\n"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"id":"_jzqgm7k3ysN","executionInfo":{"status":"ok","timestamp":1638640401681,"user_tz":420,"elapsed":132,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["bigdata = data_test.append(data_ood, ignore_index=True)"],"execution_count":58,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"SACmT5aL4CuX","executionInfo":{"status":"ok","timestamp":1638640406109,"user_tz":420,"elapsed":133,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"cebe5174-e609-4f9c-9f44-cac4e7d4b693"},"source":["bigdata"],"execution_count":59,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>inputs</th>\n","      <th>target</th>\n","      <th>maxlen</th>\n","      <th>minlen</th>\n","      <th>sumlen</th>\n","      <th>target_str</th>\n","      <th>type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The sum of 1 1000 3 100 4 10 5 and 1 1000 2 10...</td>\n","      <td>2 1000 6 100 0 10 6</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2 1000 6 100 0 10 6</td>\n","      <td>iid</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The sum of 1 1000 5 100 2 10 0 and 1 1000 5 10...</td>\n","      <td>3 1000 1 100 0 10 7</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>3 1000 1 100 0 10 7</td>\n","      <td>iid</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The sum of 1 10 2 and 1 1000 5 100 2 10 3 is</td>\n","      <td>1 1000 5 100 3 10 5</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1 1000 5 100 3 10 5</td>\n","      <td>iid</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The sum of 3 100 3 10 7 and 4 100 1 10 3 is</td>\n","      <td>7 100 5 10 0</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>7 100 5 10 0</td>\n","      <td>iid</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The sum of 2 100 5 10 2 and 1 1000 1 100 2 10 ...</td>\n","      <td>1 1000 3 100 8 10 0</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1 1000 3 100 8 10 0</td>\n","      <td>iid</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>18098</th>\n","      <td>The sum of 3 1000 8 100 3 10 8 and 2 1000 2 10...</td>\n","      <td>6 1000 1 100 0 10 6</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>6 1000 1 100 0 10 6</td>\n","      <td>OOD</td>\n","    </tr>\n","    <tr>\n","      <th>18099</th>\n","      <td>The sum of 2 1000 5 100 1 10 0 and 3 1000 8 10...</td>\n","      <td>6 1000 3 100 1 10 2</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>6 1000 3 100 1 10 2</td>\n","      <td>OOD</td>\n","    </tr>\n","    <tr>\n","      <th>18100</th>\n","      <td>The sum of 3 1000 3 100 6 10 7 and 2 1000 6 10...</td>\n","      <td>6 1000 0 100 5 10 4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>6 1000 0 100 5 10 4</td>\n","      <td>OOD</td>\n","    </tr>\n","    <tr>\n","      <th>18101</th>\n","      <td>The sum of 2 1000 3 100 9 10 0 and 2 1000 7 10...</td>\n","      <td>5 1000 1 100 8 10 3</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>5 1000 1 100 8 10 3</td>\n","      <td>OOD</td>\n","    </tr>\n","    <tr>\n","      <th>18102</th>\n","      <td>The sum of 2 1000 7 100 6 10 5 and 2 1000 0 10...</td>\n","      <td>4 1000 8 100 5 10 0</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4 1000 8 100 5 10 0</td>\n","      <td>OOD</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>18103 rows × 7 columns</p>\n","</div>"],"text/plain":["                                                  inputs  ... type\n","0      The sum of 1 1000 3 100 4 10 5 and 1 1000 2 10...  ...  iid\n","1      The sum of 1 1000 5 100 2 10 0 and 1 1000 5 10...  ...  iid\n","2          The sum of 1 10 2 and 1 1000 5 100 2 10 3 is   ...  iid\n","3           The sum of 3 100 3 10 7 and 4 100 1 10 3 is   ...  iid\n","4      The sum of 2 100 5 10 2 and 1 1000 1 100 2 10 ...  ...  iid\n","...                                                  ...  ...  ...\n","18098  The sum of 3 1000 8 100 3 10 8 and 2 1000 2 10...  ...  OOD\n","18099  The sum of 2 1000 5 100 1 10 0 and 3 1000 8 10...  ...  OOD\n","18100  The sum of 3 1000 3 100 6 10 7 and 2 1000 6 10...  ...  OOD\n","18101  The sum of 2 1000 3 100 9 10 0 and 2 1000 7 10...  ...  OOD\n","18102  The sum of 2 1000 7 100 6 10 5 and 2 1000 0 10...  ...  OOD\n","\n","[18103 rows x 7 columns]"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","metadata":{"id":"qj7E2DPhfwpf","colab":{"base_uri":"https://localhost:8080/","height":300},"outputId":"77cb4dd8-3ff8-43f9-9161-69ffed824467"},"source":["bigdata.describe()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>maxlen</th>\n","      <th>minlen</th>\n","      <th>sumlen</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>11072.000000</td>\n","      <td>11072.000000</td>\n","      <td>11072.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>3.205744</td>\n","      <td>2.902186</td>\n","      <td>3.612446</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.422401</td>\n","      <td>0.506044</td>\n","      <td>0.494574</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>1.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>3.000000</td>\n","      <td>3.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["             maxlen        minlen        sumlen\n","count  11072.000000  11072.000000  11072.000000\n","mean       3.205744      2.902186      3.612446\n","std        0.422401      0.506044      0.494574\n","min        1.000000      1.000000      2.000000\n","25%        3.000000      3.000000      3.000000\n","50%        3.000000      3.000000      4.000000\n","75%        3.000000      3.000000      4.000000\n","max        4.000000      4.000000      4.000000"]},"metadata":{},"execution_count":107}]},{"cell_type":"code","metadata":{"id":"iyRvS_WAlToo","executionInfo":{"status":"ok","timestamp":1638640525197,"user_tz":420,"elapsed":130,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["data_ood.describe(), data_ood.shape\n","data_ood = data_ood.sample(n = 6000, random_state = 42).reset_index(drop=True)"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dBYx73Ckli2S","executionInfo":{"status":"ok","timestamp":1638640534284,"user_tz":420,"elapsed":204,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}},"outputId":"adaa26ad-6716-4255-8c0d-2b21bc2d7782"},"source":["data_ood.describe(), data_ood.shape"],"execution_count":62,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(       maxlen  minlen  sumlen\n"," count  6000.0  6000.0  6000.0\n"," mean      4.0     4.0     4.0\n"," std       0.0     0.0     0.0\n"," min       4.0     4.0     4.0\n"," 25%       4.0     4.0     4.0\n"," 50%       4.0     4.0     4.0\n"," 75%       4.0     4.0     4.0\n"," max       4.0     4.0     4.0, (6000, 7))"]},"metadata":{},"execution_count":62}]},{"cell_type":"code","metadata":{"id":"GbXnMdzJ5PjT","executionInfo":{"status":"ok","timestamp":1638644522496,"user_tz":420,"elapsed":3953761,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"}}},"source":["data_ood['inputs']\n","data_ood['predictions'] = data_ood.apply(lambda x: generateText(x['inputs']), axis=1)\n","data_ood.to_csv('ood_4digits_v1.csv', index = False, header=None)"],"execution_count":63,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kp5fCMtn6ave"},"source":["Don't execute from below."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":528},"id":"fxX7Javx6Zoj","outputId":"98485e02-f225-4988-b436-5cddb3abf6fc"},"source":["bigdata"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>inputs</th>\n","      <th>target</th>\n","      <th>maxlen</th>\n","      <th>minlen</th>\n","      <th>sumlen</th>\n","      <th>target_str</th>\n","      <th>type</th>\n","      <th>predictions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The sum of 1 1000 3 100 4 10 5 and 1 1000 2 10...</td>\n","      <td>2 1000 6 100 0 10 6</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2 1000 6 100 0 10 6</td>\n","      <td>iid</td>\n","      <td>2 1000 6 100 0 10 6</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The sum of 1 1000 5 100 2 10 0 and 1 1000 5 10...</td>\n","      <td>3 1000 1 100 0 10 7</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>3 1000 1 100 0 10 7</td>\n","      <td>iid</td>\n","      <td>3 1000 1 100 0 10 7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The sum of 1 10 2 and 1 1000 5 100 2 10 3 is</td>\n","      <td>1 1000 5 100 3 10 5</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1 1000 5 100 3 10 5</td>\n","      <td>iid</td>\n","      <td>1 1000 5 100 3 10 5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The sum of 3 100 3 10 7 and 4 100 1 10 3 is</td>\n","      <td>7 100 5 10 0</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>7 100 5 10 0</td>\n","      <td>iid</td>\n","      <td>7 100 5 10 0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The sum of 2 100 5 10 2 and 1 1000 1 100 2 10 ...</td>\n","      <td>1 1000 3 100 8 10 0</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1 1000 3 100 8 10 0</td>\n","      <td>iid</td>\n","      <td>1 1000 3 100 8 10 0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11067</th>\n","      <td>The sum of 5 100 8 10 1 and 5 100 1 10 1 is</td>\n","      <td>1 1000 0 100 9 10 2</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1 1000 0 100 9 10 2</td>\n","      <td>OOD</td>\n","      <td>1 1000 0 100 9 10 2</td>\n","    </tr>\n","    <tr>\n","      <th>11068</th>\n","      <td>The sum of 1 100 9 10 5 and 6 100 7 10 3 is</td>\n","      <td>8 100 6 10 8</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>8 100 6 10 8</td>\n","      <td>OOD</td>\n","      <td>8 100 6 10 8</td>\n","    </tr>\n","    <tr>\n","      <th>11069</th>\n","      <td>The sum of 1 100 8 10 4 and 2 100 3 10 6 is</td>\n","      <td>4 100 2 10 0</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>4 100 2 10 0</td>\n","      <td>OOD</td>\n","      <td>4 100 2 10 0</td>\n","    </tr>\n","    <tr>\n","      <th>11070</th>\n","      <td>The sum of 3 100 4 10 2 and 9 100 0 10 7 is</td>\n","      <td>1 1000 2 100 4 10 9</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1 1000 2 100 4 10 9</td>\n","      <td>OOD</td>\n","      <td>1 1000 2 100 4 10 9</td>\n","    </tr>\n","    <tr>\n","      <th>11071</th>\n","      <td>The sum of 3 100 4 10 0 and 3 100 2 10 7 is</td>\n","      <td>6 100 6 10 7</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>6 100 6 10 7</td>\n","      <td>OOD</td>\n","      <td>6 100 6 10 7</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11072 rows × 8 columns</p>\n","</div>"],"text/plain":["                                                  inputs  ...           predictions\n","0      The sum of 1 1000 3 100 4 10 5 and 1 1000 2 10...  ...   2 1000 6 100 0 10 6\n","1      The sum of 1 1000 5 100 2 10 0 and 1 1000 5 10...  ...   3 1000 1 100 0 10 7\n","2          The sum of 1 10 2 and 1 1000 5 100 2 10 3 is   ...   1 1000 5 100 3 10 5\n","3           The sum of 3 100 3 10 7 and 4 100 1 10 3 is   ...          7 100 5 10 0\n","4      The sum of 2 100 5 10 2 and 1 1000 1 100 2 10 ...  ...   1 1000 3 100 8 10 0\n","...                                                  ...  ...                   ...\n","11067       The sum of 5 100 8 10 1 and 5 100 1 10 1 is   ...   1 1000 0 100 9 10 2\n","11068       The sum of 1 100 9 10 5 and 6 100 7 10 3 is   ...          8 100 6 10 8\n","11069       The sum of 1 100 8 10 4 and 2 100 3 10 6 is   ...          4 100 2 10 0\n","11070       The sum of 3 100 4 10 2 and 9 100 0 10 7 is   ...   1 1000 2 100 4 10 9\n","11071       The sum of 3 100 4 10 0 and 3 100 2 10 7 is   ...          6 100 6 10 7\n","\n","[11072 rows x 8 columns]"]},"metadata":{},"execution_count":114}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pT50mdfFoepk","outputId":"9f474419-98c9-4d3c-8f67-00b4244855b4"},"source":["lest = []\n","same_ = []\n","for i, j, isood in zip(bigdata['target_str'], bigdata['predictions'], bigdata['type']):\n","  if i.strip() != j.strip():\n","    lest.append((i, j, isood))\n","  else:\n","    same_.append((i, j, isood))\n","print(len(lest), len(same_))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["57 11015\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGlB3l6rprOV","outputId":"73450709-b3b6-479f-8a82-6d98aa1cb8e7"},"source":["lest"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('1 1000 1 100 9 10 9', ' 1 1000 2 100 9 10 9', 'iid'),\n"," ('5 100 0 10 0', ' 4 100 0 10 0', 'iid'),\n"," ('1 1000 9 100 2 10 9', ' 1 1000 9 100 3 10 9', 'iid'),\n"," ('8 100 9 10 8', ' 9 100 9 10 8', 'iid'),\n"," ('2 1000 3 100 9 10 9', ' 2 1000 4 100 9 10 9', 'iid'),\n"," ('1 1000 8 100 5 10 7', ' 1 1000 8 100 5 10 8', 'iid'),\n"," ('8 100 9 10 1', ' 9 100 9 10 1', 'OOD'),\n"," ('3 100 0 10 1', ' 2 100 0 10 1', 'OOD'),\n"," ('5 10 6', ' 6 10 6', 'OOD'),\n"," ('8 100 9 10 9', ' 9 100 9 10 9', 'OOD'),\n"," ('9 100 9 10 5', ' 1 1000 0 100 9 10 5', 'OOD'),\n"," ('1 100 4 10 5', ' 1 1000 4 10 5', 'OOD'),\n"," ('6 100 0 10 7', ' 5 100 0 10 7', 'OOD'),\n"," ('8 10 7', ' 9 10 7', 'OOD'),\n"," ('3 100 0 10 1', ' 2 100 0 10 1', 'OOD'),\n"," ('4 100 0 10 0', ' 3 100 0 10 0', 'OOD'),\n"," ('8 10 9', ' 9 10 9', 'OOD'),\n"," ('2 100 1 10 3', ' 2 100 0 10 3', 'OOD'),\n"," ('8 100 9 10 7', ' 9 100 9 10 7', 'OOD'),\n"," ('8 100 9 10 8', ' 9 100 9 10 8', 'OOD'),\n"," ('5 10 7', ' 6 10 7', 'OOD'),\n"," ('1 100 2 10 2', ' 1 1000 2 10 2', 'OOD'),\n"," ('8 100 9 10 9', ' 9 100 9 10 9', 'OOD'),\n"," ('7 100 9 10 7', ' 8 100 9 10 7', 'OOD'),\n"," ('1 10 2', ' 1 and 2', 'OOD'),\n"," ('3 100 0 10 0', ' 2 100 0 10 0', 'OOD'),\n"," ('3 100 2 10 0', ' 3 100 1 10 0', 'OOD'),\n"," ('3 100 0 10 5', ' 2 100 0 10 5', 'OOD'),\n"," ('3 100 0 10 9', ' 2 100 0 10 9', 'OOD'),\n"," ('9 100 0 10 1', ' 8 100 0 10 1', 'OOD'),\n"," ('8 100 4 10 9', ' 8 100 5 10 9', 'OOD'),\n"," ('6 100 9 10 8', ' 7 100 9 10 8', 'OOD'),\n"," ('8 100 7 10 5', ' 8 100 7 10 7', 'OOD'),\n"," ('1 1000 0 100 0 10 9', ' 1 1000 0 100 1 10 9', 'OOD'),\n"," ('1 100 2 10 2', ' 1 1000 2 10 2', 'OOD'),\n"," ('4 100 1 10 1', ' 4 100 0 10 1', 'OOD'),\n"," ('8 100 1 10 1', ' 8 100 0 10 1', 'OOD'),\n"," ('8 100 0 10 1', ' 7 100 0 10 1', 'OOD'),\n"," ('5 100 0 10 5', ' 4 100 0 10 5', 'OOD'),\n"," ('2 100 1 10 2', ' 2 100 0 10 2', 'OOD'),\n"," ('6 100 6 10 2', ' 6 100 6 10 4', 'OOD'),\n"," ('7 100 1 10 3', ' 7 100 0 10 3', 'OOD'),\n"," ('7 100 9 10 7', ' 8 100 9 10 7', 'OOD'),\n"," ('1 100 1 10 4', ' 1 1000 1 10 4', 'OOD'),\n"," ('3 100 8 10 5', ' 3 100 8 10 8', 'OOD'),\n"," ('2 10 4', ' 1 10 4', 'OOD'),\n"," ('2 10 0', ' 1 10 0', 'OOD'),\n"," ('8 100 9 10 5', ' 9 100 9 10 5', 'OOD'),\n"," ('1 1000 0 100 1 10 0', ' 1 1000 1 100 1 10 0', 'OOD'),\n"," ('8 100 9 10 1', ' 9 100 9 10 1', 'OOD'),\n"," ('5 100 7 10 1', ' 6 100 7 10 1', 'OOD'),\n"," ('7 100 6 10 0', ' 7 100 5 10 0', 'OOD'),\n"," ('5 100 9 10 4', ' 6 100 9 10 4', 'OOD'),\n"," ('3 100 0 10 4', ' 2 100 0 10 4', 'OOD'),\n"," ('6 100 0 10 0', ' 5 100 0 10 0', 'OOD'),\n"," ('5 100 9 10 9', ' 6 100 9 10 9', 'OOD'),\n"," ('7 100 1 10 0', ' 7 100 0 10 1', 'OOD')]"]},"metadata":{},"execution_count":121}]},{"cell_type":"code","metadata":{"id":"m-qnPQqYmxBz"},"source":["def convert_to_10ebased(number: str, split_type: str, invert_number: bool) -> str:\n","    signal = None\n","    if number[0] == '-':\n","        signal = '-'\n","        number = number[1:]\n","\n","    output = []\n","    for i, digit in enumerate(number[::-1]):\n","        if split_type is None:\n","            output.append('10e' + str(i))\n","        elif split_type == 'underscore':\n","            output.append('10e' + '_'.join(str(i)))\n","        elif split_type == 'character':\n","            output.append(' '.join('D' + str(i) + 'E'))\n","        else:\n","            raise Exception(f'Wrong split_type: {split_type}')\n","        output.append(digit)\n","\n","    if signal:\n","        output.append(signal)\n","\n","    # The output is already inverted. If we want it to _not_ be inverted, then we invert it.\n","    if not invert_number:\n","        output = output[::-1]\n","\n","    return ' '.join(output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DkPBZikmluAT"},"source":["def convert_to_10based(number: str, invert_number: bool) -> str:\n","    signal = None\n","    if number[0] == '-':\n","        signal = '-'\n","        number = number[1:]\n","\n","    output = []\n","    for i, digit in enumerate(number[::-1]):\n","        if i > 0:\n","            output.append('1' + i * '0')\n","        output.append(digit)\n","\n","    if signal:\n","        output.append(signal)\n","\n","    # The output is already inverted. If we want it to not be inverted, then we invert it.\n","    if not invert_number:\n","        output = output[::-1]\n","\n","    return ' '.join(output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"LSKVJpXRrqj3","outputId":"e918f7d4-f24c-474a-9525-196742aff430"},"source":["convert_to_10ebased(\n","                \"932\", split_type=None, invert_number=False)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'9 10e2 3 10e1 2 10e0'"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","metadata":{"id":"QdFMBWGPYcLx"},"source":["Testing with OOD"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"id":"n1aW3PfjYmf9","outputId":"4322b8e8-a108-49a5-e1ae-186ee0c362e9"},"source":["seq_len = [len(i.split()) for i in data_ood['inputs']]\n","\n","pd.Series(seq_len).hist(bins = 30)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7fa662e2aa90>"]},"metadata":{},"execution_count":40},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NnqaqPnAY7pC","outputId":"0be258c4-c95b-44bd-b5bf-101ad329ac35"},"source":["token_lens_ood = []\n","\n","for txt in data_ood.inputs:\n","  # doubt\n","  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n","  token_lens_ood.append(len(tokens))\n","\n","max(token_lens_ood)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["46"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ALXua3mtZJ2B","outputId":"211b44f7-b5ab-4267-de5f-1328b457877e"},"source":["token_lens_target_ood = []\n","\n","for txt in data_ood.target:\n","  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n","  token_lens_target_ood.append(len(tokens))\n","\n","max(token_lens_target_ood)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["24"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ZyxTIlsZW2v","outputId":"59a5297f-74a3-4223-f041-7e0433416747"},"source":["ood_inputs, ood_masks = get_word_embeddings(data_ood['inputs'], 55)\n","data_ood['target_str'] = data_ood['target'].astype(str)\n","ood_labels = get_word_embeddings(data_ood['target_str'], 30)[0]"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"id":"Fd0mq9gLaBPF"},"source":["ood_data = TensorDataset(ood_inputs, ood_masks, ood_labels)\n","ood_dataloader = DataLoader(ood_data, shuffle = True, batch_size = batch_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plFASDdCaiqR","outputId":"79a9edc7-d374-4ed6-add5-a5069def879e"},"source":["model_load.to(device)\n","evaluate(model_load, ood_dataloader)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.6103567625133121"]},"metadata":{},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"MlpL5uElayw6"},"source":[""],"execution_count":null,"outputs":[]}]}