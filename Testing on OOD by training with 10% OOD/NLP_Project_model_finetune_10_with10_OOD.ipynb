{"cells":[{"cell_type":"markdown","metadata":{"id":"qFErrTqq_ybT"},"source":["### Installing neccessary packages:"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17322,"status":"ok","timestamp":1638663889094,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"q3OMB-x8_0vK","outputId":"679d4df4-803a-4ee5-b0f1-2aee987c035c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting transformers\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 11.8 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 328 kB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 40.0 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 28.1 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 36.5 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 11.2 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (0.29.24)\n"]}],"source":["!pip install transformers\n","# https://huggingface.co/transformers/installation.html\n","!pip install sentencepiece\n","# https://pypi.org/project/sentencepiece/\n","# Python wrapper for SentencePiece. This API will offer the encoding, decoding and training of Sentencepiece.\n","!pip install Cython\n","# https://pypi.org/project/Cython/"]},{"cell_type":"markdown","metadata":{"id":"T-AVcK4gBhW7"},"source":["## Checking the GPU availabilty"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5452,"status":"ok","timestamp":1638663894524,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"K5BIx7Mj1x9M","outputId":"6acd3a70-fee5-434e-8650-16ec78ed318b"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU\n"]}],"source":["import torch\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\") \n","    print(\"GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"CPU\")"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":528,"status":"ok","timestamp":1638663949436,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"z1wUPLeYJ6GO","outputId":"34d2ecf7-beb8-4d76-ec66-77f233ec80d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"bU-UZe2cBPpq"},"source":["## Importing the required packages:"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1638663950183,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"UrGEtltY6SIa"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":447,"status":"ok","timestamp":1638664044229,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"mlokiVxO7jy0"},"outputs":[],"source":["import os\n","import sys\n","from transformers.optimization import Adafactor \n","import time\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    get_linear_schedule_with_warmup\n",")\n","import torch\n","import random\n","import re\n","\n","os.chdir('/content/drive/MyDrive/Colab Notebooks/new_run_10_odd')"]},{"cell_type":"code","execution_count":78,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":404,"status":"ok","timestamp":1638664948108,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"xdoJ8keH8pLP","outputId":"15204b4a-92d9-4ca2-afa9-8d2947b36f98"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                              inputs  ... sumlen\n","0  The sum of 1 1000 4 100 0 10 6 and 7 100 2 10 ...  ...      4\n","1  The sum of 1 1000 6 100 1 10 6 and 1 1000 1 10...  ...      4\n","2  The sum of 3 100 7 10 6 and 1 1000 6 100 1 10 ...  ...      4\n","3  The sum of 1 1000 0 100 3 10 1 and 4 100 9 10 ...  ...      4\n","4  The sum of 1 1000 3 100 6 10 3 and 5 100 2 10 ...  ...      4\n","\n","[5 rows x 5 columns]\n","                                              inputs  ... sumlen\n","0  The sum of 1 10000 8 1000 9 100 7 10 6 and 1 1...  ...      5\n","1  The sum of 2 10000 8 1000 6 100 2 10 6 and 1 1...  ...      5\n","2  The sum of 2 10000 6 1000 2 100 7 10 8 and 2 1...  ...      5\n","3  The sum of 1 10000 8 1000 7 100 2 10 3 and 1 1...  ...      5\n","4  The sum of 2 10000 9 1000 4 100 3 10 6 and 3 1...  ...      5\n","\n","[5 rows x 5 columns]\n"]}],"source":["import pandas as pd\n","# Reading csv\n","data = pd.read_csv('data_10_full_v1.csv')\n","print(data.head(5))\n","data_ood = pd.read_csv('10_5digits_ood_v1.csv')\n","print(data_ood.head(5))"]},{"cell_type":"code","execution_count":79,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64,"status":"ok","timestamp":1638664948501,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"Uh3S1gzkGt7E","outputId":"8abe6437-a467-4a5e-925c-91f368956d2b"},"outputs":[{"data":{"text/plain":["<bound method DataFrame.info of                                                   inputs  ... sumlen\n","0      The sum of 1 1000 4 100 0 10 6 and 7 100 2 10 ...  ...      4\n","1      The sum of 1 1000 6 100 1 10 6 and 1 1000 1 10...  ...      4\n","2      The sum of 3 100 7 10 6 and 1 1000 6 100 1 10 ...  ...      4\n","3      The sum of 1 1000 0 100 3 10 1 and 4 100 9 10 ...  ...      4\n","4      The sum of 1 1000 3 100 6 10 3 and 5 100 2 10 ...  ...      4\n","...                                                  ...  ...    ...\n","14964       The sum of 9 100 8 10 1 and 6 100 9 10 8 is   ...      4\n","14965  The sum of 1 1000 3 100 0 10 0 and 6 100 5 10 ...  ...      4\n","14966  The sum of 1 1000 6 100 6 10 0 and 8 100 8 10 ...  ...      4\n","14967  The sum of 5 100 7 10 3 and 1 1000 0 100 0 10 ...  ...      4\n","14968  The sum of 4 100 9 10 6 and 1 1000 7 100 3 10 ...  ...      4\n","\n","[14969 rows x 5 columns]>"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["data.info"]},{"cell_type":"code","execution_count":80,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1638664949076,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"YA_luy6aGn45","outputId":"846fcb6b-0fe9-47e7-c1bb-30edc234e55f"},"outputs":[{"data":{"text/plain":["1300"]},"execution_count":80,"metadata":{},"output_type":"execute_result"}],"source":["data = data.sample(n = 11700, random_state = 42).reset_index(drop=True)\n","len(data)\n","data_extra_ood = data_ood.sample(n = 1300, random_state = 42).reset_index(drop=True)\n","len(data_extra_ood)"]},{"cell_type":"code","execution_count":81,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1638664949379,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"DNj1WxcFDApB"},"outputs":[],"source":["data = data.append(data_extra_ood, ignore_index=True)"]},{"cell_type":"code","execution_count":82,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1638664949380,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"rcgHXMgv8606"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","\n","# Test and validation split\n","train, validation = train_test_split(data, test_size=0.2, random_state=42)\n","train, test = train_test_split(train, test_size=0.3, random_state=42)\n","\n","data_train = train.reset_index(drop=True)\n","data_valid = validation.reset_index(drop=True)\n","data_test = test.reset_index(drop=True)"]},{"cell_type":"code","execution_count":83,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1638664950091,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"y7YLTIG87LGc","outputId":"42490271-336f-4215-f8d4-6fe262296193"},"outputs":[{"data":{"text/plain":["(7280, 5)"]},"execution_count":83,"metadata":{},"output_type":"execute_result"}],"source":["data_train.shape"]},{"cell_type":"code","execution_count":84,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1638664950856,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"vohFU6vyHAiq","outputId":"582cfbf5-c470-4267-8fee-60d7abdb16b0"},"outputs":[{"data":{"text/plain":["(2600, 5)"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["data_valid.shape"]},{"cell_type":"code","execution_count":85,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1638664950857,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"Da158J1V-1U7","outputId":"3f588701-d615-42ca-faed-98f33bb981ba"},"outputs":[{"data":{"text/plain":["(3120, 5)"]},"execution_count":85,"metadata":{},"output_type":"execute_result"}],"source":["data_test.shape"]},{"cell_type":"code","execution_count":86,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1638664950857,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"7ls6YKC19R07"},"outputs":[],"source":["# Initializing Parameters \n","batch_size, num_of_epochs = 32, 25\n","num_of_batches = int(len(data_train)/batch_size)"]},{"cell_type":"code","execution_count":87,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1638664951515,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"ye-gC3y2YI5g"},"outputs":[],"source":["# Reference\n","# https://huggingface.co/transformers/model_doc/t5.html\n","# https://medium.com/analytics-vidhya/t5-a-detailed-explanation-a0ac9bc53e51\n","# https://towardsdatascience.com/data-to-text-generation-with-t5-building-a-simple-yet-advanced-nlg-model-b5cce5a6df45"]},{"cell_type":"code","execution_count":88,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1638664951895,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"HngN0xRX1Rk5","outputId":"55dd7d9a-fe08-4cbc-d2a5-c7945977c86e"},"outputs":[{"data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7f5a6cc2bb10>"]},"execution_count":88,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# get length of all the messages in the train set\n","seq_len = [len(i.split()) for i in data_train['inputs']]\n","\n","pd.Series(seq_len).hist(bins = 30)"]},{"cell_type":"code","execution_count":89,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8190,"status":"ok","timestamp":1638664960082,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"_ad1Lt8c9iDX","outputId":"6c83e9d8-d2ea-4f21-e720-00f0904bc3b1"},"outputs":[{"data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"execution_count":89,"metadata":{},"output_type":"execute_result"}],"source":["# T5-base\n","tokenizer = T5Tokenizer.from_pretrained('t5-base')\n","\n","model = T5ForConditionalGeneration.from_pretrained('t5-base', return_dict=True)\n","# moving the model to device(GPU/CPU)\n","model.to(device)"]},{"cell_type":"code","execution_count":90,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1409,"status":"ok","timestamp":1638664961486,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"uNRQcyTh3GQf","outputId":"d145082e-ee6c-41f5-b66d-7ab857747532"},"outputs":[{"data":{"text/plain":["30"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["token_lens = []\n","\n","for txt in data_train.inputs:\n","  # doubt\n","  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n","  token_lens.append(len(tokens))\n","\n","max(token_lens)"]},{"cell_type":"code","execution_count":91,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":913,"status":"ok","timestamp":1638664963253,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"9Efo0J6kA-yt","outputId":"a54046e5-d486-4110-f4aa-a2e8c012f9d1"},"outputs":[{"data":{"text/plain":["13"]},"execution_count":91,"metadata":{},"output_type":"execute_result"}],"source":["token_lens_target = []\n","\n","for txt in data_train.target:\n","  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n","  token_lens_target.append(len(tokens))\n","\n","max(token_lens_target)"]},{"cell_type":"code","execution_count":92,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"elapsed":353,"status":"ok","timestamp":1638664963604,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"OT1Us38M2ULV","outputId":"37b69dde-e0a7-48ba-be8f-f01a03969856"},"outputs":[{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Maximum length is:  30\n"]}],"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(range(1,len(token_lens)+1), token_lens)\n","plt.ylabel('length of tokens')\n","plt.show()\n","\n","MAX_LEN = max(token_lens)\n","print(\"Maximum length is: \", MAX_LEN)\n","# when sample with first 40k and last 40k we got the maximum length is 14"]},{"cell_type":"code","execution_count":93,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1638664963605,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"k6rUVKln1Ff5"},"outputs":[],"source":["MAX_LEN = 30"]},{"cell_type":"code","execution_count":94,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1638664963605,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"_FqnJxC4u8XO"},"outputs":[],"source":["def get_word_embeddings(data, MAX_LEN=45):\n","  input_ids=[]\n","  attention_masks = []\n","  for sent in data:\n","        encoded_sent = tokenizer.encode_plus(\n","            text=sent,  \n","            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n","            max_length=MAX_LEN,                  # Max length to truncate/pad\n","            pad_to_max_length=True,         # Pad sentence to max length\n","            #return_tensors='pt',           # Return PyTorch tensor\n","            return_attention_mask=True      # Return attention mask\n","            )\n","        \n","        # Add the outputs to the lists\n","        input_ids.append(encoded_sent.get('input_ids'))\n","        attention_masks.append(encoded_sent.get('attention_mask'))\n","\n","  # Convert lists to tensors\n","  input_ids = torch.tensor(input_ids)\n","  attention_masks = torch.tensor(attention_masks)\n","\n","  return input_ids, attention_masks"]},{"cell_type":"code","execution_count":95,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3364,"status":"ok","timestamp":1638664966962,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"jAxmAoDrvrWO","outputId":"36ce3533-1c02-4394-8d1a-dd232352ab0b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["# Run function `preprocessing_for_bert` on the train set and the validation set\n","train_inputs, train_masks = get_word_embeddings(data_train['inputs'])\n","val_inputs, val_masks = get_word_embeddings(data_valid['inputs'])\n","test_inputs, test_masks = get_word_embeddings(data_test['inputs'])"]},{"cell_type":"code","execution_count":96,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1638664966962,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"XbPJ2deo71p5"},"outputs":[],"source":["data_train['target_str'] = data_train['target'].astype(str)\n","data_valid['target_str'] = data_valid['target'].astype(str)\n","data_test['target_str'] = data_test['target'].astype(str)\n"]},{"cell_type":"code","execution_count":97,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2681,"status":"ok","timestamp":1638664969640,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"j3dHakeNv66S","outputId":"f9b036b3-b164-4f66-c9ec-2c1f3194db22"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["#convert lists to tensors\n","train_labels = get_word_embeddings(data_train['target_str'], 20)[0]\n","val_labels = get_word_embeddings(data_valid['target_str'], 20)[0]\n","test_labels = get_word_embeddings(data_test['target_str'], 20)[0]"]},{"cell_type":"code","execution_count":98,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1638664969641,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"EHUnt0aF8GQk","outputId":"29188383-f981-47ba-d8da-53770133fb2a"},"outputs":[{"data":{"text/plain":["torch.Size([7280, 20])"]},"execution_count":98,"metadata":{},"output_type":"execute_result"}],"source":["train_labels.shape\n","# doubt"]},{"cell_type":"code","execution_count":99,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1638664969641,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"MgegmxBtoqZV","outputId":"133e6e3c-26cf-4944-835e-d9490cfca74b"},"outputs":[{"data":{"text/plain":["tensor([[ 505,  910,  204,  ...,    0,    0,    0],\n","        [ 209, 5580,  431,  ...,    0,    0,    0],\n","        [ 204, 5580,  314,  ...,    0,    0,    0],\n","        ...,\n","        [ 209, 5580,  220,  ...,    0,    0,    0],\n","        [ 220, 5580,  305,  ...,    0,    0,    0],\n","        [ 220, 5580,  489,  ...,    0,    0,    0]])"]},"execution_count":99,"metadata":{},"output_type":"execute_result"}],"source":["train_labels"]},{"cell_type":"code","execution_count":100,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1638664969641,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"9ezRHf2e7rPL","outputId":"388bd55d-4c82-4b9c-8706-bca4d2831391"},"outputs":[{"data":{"text/plain":["0              8 100 2 10 2\n","1       1 1000 6 100 2 10 6\n","2       2 1000 4 100 0 10 5\n","3              1 100 6 10 3\n","4       1 1000 0 100 8 10 7\n","               ...         \n","7275    2 1000 8 100 5 10 9\n","7276    1 1000 6 100 2 10 4\n","7277    1 1000 3 100 3 10 7\n","7278    3 1000 5 100 1 10 8\n","7279    3 1000 7 100 2 10 1\n","Name: target, Length: 7280, dtype: object"]},"execution_count":100,"metadata":{},"output_type":"execute_result"}],"source":["data_train['target']"]},{"cell_type":"code","execution_count":101,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1638664969641,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"osOohece1-tJ","outputId":"87da1031-0e02-488f-f5e5-5fc29a04133d"},"outputs":[{"data":{"text/plain":["torch.Size([7280, 45])"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["train_inputs.shape"]},{"cell_type":"code","execution_count":102,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1638664969642,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"GjDbWMIlv8Iq"},"outputs":[],"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","\n","batch_size = 32\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_dataloader = DataLoader(train_data, shuffle = True, batch_size = batch_size)\n","\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_dataloader = DataLoader(val_data, shuffle = True, batch_size = batch_size)\n","\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_dataloader = DataLoader(test_data, shuffle = True, batch_size = batch_size)"]},{"cell_type":"code","execution_count":103,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1638664969642,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"q88he4D9Lw0L"},"outputs":[],"source":["#  Optimizer\n","# https://huggingface.co/transformers/model_doc/t5.html#overview\n","optimizer = Adafactor(\n","    model.parameters(),\n","    lr=5e-4, # Initializing the learning Rate as suggested in the T5 official documentation\n","    eps=(1e-8, 1e-3),\n","    clip_threshold=1.0,\n","    decay_rate=-0.3,\n","    beta1=None,\n","    weight_decay=0.0,\n","    relative_step=False,\n","    scale_parameter=False,\n","    warmup_init=False\n",")"]},{"cell_type":"code","execution_count":104,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1638664969642,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"vQ9Bj35-56kQ"},"outputs":[],"source":["# Changing the directory to store the model there.\n","# print(os.getcwd())\n","# os.chdir('/content/drive/MyDrive/Colab Notebooks/new_run')\n","# print(os.getcwd())"]},{"cell_type":"code","execution_count":105,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":582,"status":"ok","timestamp":1638664970218,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"COfO6s0z3kit","outputId":"464ba0e6-1310-4d89-dd8b-505e724b1b7b"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2021-12-05 00:42:49--  https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.42.214\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.42.214|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1199 (1.2K) [application/json]\n","Saving to: ‘t5-base-config.json.2’\n","\n","t5-base-config.json 100%[===================>]   1.17K  --.-KB/s    in 0s      \n","\n","2021-12-05 00:42:49 (54.5 MB/s) - ‘t5-base-config.json.2’ saved [1199/1199]\n","\n"]}],"source":["# Loading the configuration file for 't5-base' model\n","!wget https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1638664550885,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"MYMdr0Xu92Yv"},"outputs":[],"source":["from IPython.display import HTML, display\n","\n","# Setting the progress, with html as UI.\n","def progress(loss, value, max=100):\n","    return HTML(\"\"\" Batch loss :{loss}\n","        <progress\n","            value='{value}'\n","            max='{max}',\n","            style='width: 100%'\n","        >\n","            {value}\n","        </progress>\n","    \"\"\".format(loss=loss,value=value, max=max))"]},{"cell_type":"code","execution_count":55,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1638664552959,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"_C2w5YpsN7C1"},"outputs":[],"source":["def evaluate(model, val_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's performance\n","    on our validation set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","\n","    # Tracking variables\n","    v_accuracy = []\n","    v_loss = []\n","\n","    # For each batch in our validation set...\n","    for batch in val_dataloader:\n","        # Load batch to GPU\n","        \n","        v_input_ids, v_attn_mask, v_labels = tuple(t.to(device) for t in batch)\n","\n","        # print(v_input_ids.shape, v_labels.shape)\n","\n","        val_outputs = model.generate(input_ids=v_input_ids, attention_mask=v_attn_mask)\n","\n","        val_preds = [ tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","            for output in val_outputs]\n","\n","        val_labels = [ tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","            for output in v_labels]\n","        \n","        # v_loss.append(val_outputs.loss)\n","\n","        # Get the predictions\n","        # print(val_outputs.logits.shape)\n","        # val_preds = torch.argmax(val_outputs.logits, dim=1).flatten()\n","        # print(val_preds, val_labels)\n","        # Calculate the accuracy rate\n","\n","        val_preds = np.array(val_preds)\n","        val_labels = np.array(val_labels)\n","        accuracy = ((val_preds == val_labels).sum() / len(val_labels)) * 100\n","        v_accuracy.append(accuracy)\n","\n","    # Compute the average accuracy and loss over the validation set.\n","    # v_loss = np.mean(v_loss)\n","    v_accuracy = np.mean(v_accuracy)\n","\n","    return v_accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9646207,"status":"ok","timestamp":1638658159957,"user":{"displayName":"avinashd dhulipa","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02529447251242284337"},"user_tz":420},"id":"wlCbmfAQ-CWJ","outputId":"46533f76-67a6-465e-ed61-28547765efa3"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch  4 Batch  81 / 228  Training Loss  0.006206437945365906\n","Epoch  4 Batch  82 / 228  Training Loss  0.007809885777533054\n","Epoch  4 Batch  83 / 228  Training Loss  0.0061338008381426334\n","Epoch  4 Batch  84 / 228  Training Loss  0.005070612300187349\n","Epoch  4 Batch  85 / 228  Training Loss  0.008318006061017513\n","Epoch  4 Batch  86 / 228  Training Loss  0.016389505937695503\n","Epoch  4 Batch  87 / 228  Training Loss  0.009210653603076935\n","Epoch  4 Batch  88 / 228  Training Loss  0.007397035602480173\n","Epoch  4 Batch  89 / 228  Training Loss  0.023871934041380882\n","Epoch  4 Batch  90 / 228  Training Loss  0.02265647053718567\n","Epoch  4 Batch  91 / 228  Training Loss  0.004307321272790432\n","Epoch  4 Batch  92 / 228  Training Loss  0.004547972232103348\n","Epoch  4 Batch  93 / 228  Training Loss  0.0026716955471783876\n","Epoch  4 Batch  94 / 228  Training Loss  0.0089536402374506\n","Epoch  4 Batch  95 / 228  Training Loss  0.008177082054316998\n","Epoch  4 Batch  96 / 228  Training Loss  0.003622357966378331\n","Epoch  4 Batch  97 / 228  Training Loss  0.010488653555512428\n","Epoch  4 Batch  98 / 228  Training Loss  0.007413791958242655\n","Epoch  4 Batch  99 / 228  Training Loss  0.012798388488590717\n","Epoch  4 Batch  100 / 228  Training Loss  0.006914076860994101\n","Epoch  4 Batch  101 / 228  Training Loss  0.008485298603773117\n","Epoch  4 Batch  102 / 228  Training Loss  0.00436682952567935\n","Epoch  4 Batch  103 / 228  Training Loss  0.005733267404139042\n","Epoch  4 Batch  104 / 228  Training Loss  0.006795988418161869\n","Epoch  4 Batch  105 / 228  Training Loss  0.003833279013633728\n","Epoch  4 Batch  106 / 228  Training Loss  0.00730012496933341\n","Epoch  4 Batch  107 / 228  Training Loss  0.006433798465877771\n","Epoch  4 Batch  108 / 228  Training Loss  0.007220146711915731\n","Epoch  4 Batch  109 / 228  Training Loss  0.01399400644004345\n","Epoch  4 Batch  110 / 228  Training Loss  0.00406885938718915\n","Epoch  4 Batch  111 / 228  Training Loss  0.005693927872925997\n","Epoch  4 Batch  112 / 228  Training Loss  0.002701111836358905\n","Epoch  4 Batch  113 / 228  Training Loss  0.0026395178865641356\n","Epoch  4 Batch  114 / 228  Training Loss  0.0032085131388157606\n","Epoch  4 Batch  115 / 228  Training Loss  0.004000575747340918\n","Epoch  4 Batch  116 / 228  Training Loss  0.00262650940567255\n","Epoch  4 Batch  117 / 228  Training Loss  0.00406945776194334\n","Epoch  4 Batch  118 / 228  Training Loss  0.002862181980162859\n","Epoch  4 Batch  119 / 228  Training Loss  0.006609483156353235\n","Epoch  4 Batch  120 / 228  Training Loss  0.005984680261462927\n","Epoch  4 Batch  121 / 228  Training Loss  0.00790575984865427\n","Epoch  4 Batch  122 / 228  Training Loss  0.0068146465346217155\n","Epoch  4 Batch  123 / 228  Training Loss  0.0036823966074734926\n","Epoch  4 Batch  124 / 228  Training Loss  0.0025901624467223883\n","Epoch  4 Batch  125 / 228  Training Loss  0.006051491480320692\n","Epoch  4 Batch  126 / 228  Training Loss  0.007662190590053797\n","Epoch  4 Batch  127 / 228  Training Loss  0.018559688702225685\n","Epoch  4 Batch  128 / 228  Training Loss  0.035836488008499146\n","Epoch  4 Batch  129 / 228  Training Loss  0.020592328161001205\n","Epoch  4 Batch  130 / 228  Training Loss  0.003923377953469753\n","Epoch  4 Batch  131 / 228  Training Loss  0.012519000098109245\n","Epoch  4 Batch  132 / 228  Training Loss  0.010073678568005562\n","Epoch  4 Batch  133 / 228  Training Loss  0.01265154778957367\n","Epoch  4 Batch  134 / 228  Training Loss  0.012108654715120792\n","Epoch  4 Batch  135 / 228  Training Loss  0.008106594905257225\n","Epoch  4 Batch  136 / 228  Training Loss  0.018415292724967003\n","Epoch  4 Batch  137 / 228  Training Loss  0.014876892790198326\n","Epoch  4 Batch  138 / 228  Training Loss  0.012431839480996132\n","Epoch  4 Batch  139 / 228  Training Loss  0.007743289228528738\n","Epoch  4 Batch  140 / 228  Training Loss  0.006062632892280817\n","Epoch  4 Batch  141 / 228  Training Loss  0.004361093044281006\n","Epoch  4 Batch  142 / 228  Training Loss  0.0031724299769848585\n","Epoch  4 Batch  143 / 228  Training Loss  0.008587251417338848\n","Epoch  4 Batch  144 / 228  Training Loss  0.015720102936029434\n","Epoch  4 Batch  145 / 228  Training Loss  0.003740863176062703\n","Epoch  4 Batch  146 / 228  Training Loss  0.0037255887873470783\n","Epoch  4 Batch  147 / 228  Training Loss  0.0038940105587244034\n","Epoch  4 Batch  148 / 228  Training Loss  0.0033609415404498577\n","Epoch  4 Batch  149 / 228  Training Loss  0.003464728593826294\n","Epoch  4 Batch  150 / 228  Training Loss  0.006484656129032373\n","Epoch  4 Batch  151 / 228  Training Loss  0.023793818429112434\n","Epoch  4 Batch  152 / 228  Training Loss  0.012643551453948021\n","Epoch  4 Batch  153 / 228  Training Loss  0.00391546031460166\n","Epoch  4 Batch  154 / 228  Training Loss  0.007440116256475449\n","Epoch  4 Batch  155 / 228  Training Loss  0.003826210740953684\n","Epoch  4 Batch  156 / 228  Training Loss  0.007678057998418808\n","Epoch  4 Batch  157 / 228  Training Loss  0.007471530232578516\n","Epoch  4 Batch  158 / 228  Training Loss  0.008734842762351036\n","Epoch  4 Batch  159 / 228  Training Loss  0.002778157126158476\n","Epoch  4 Batch  160 / 228  Training Loss  0.00547358114272356\n","Epoch  4 Batch  161 / 228  Training Loss  0.0026440841611474752\n","Epoch  4 Batch  162 / 228  Training Loss  0.005239059217274189\n","Epoch  4 Batch  163 / 228  Training Loss  0.004426433704793453\n","Epoch  4 Batch  164 / 228  Training Loss  0.003750443458557129\n","Epoch  4 Batch  165 / 228  Training Loss  0.0023144097067415714\n","Epoch  4 Batch  166 / 228  Training Loss  0.0020652704406529665\n","Epoch  4 Batch  167 / 228  Training Loss  0.002782453317195177\n","Epoch  4 Batch  168 / 228  Training Loss  0.0024887514300644398\n","Epoch  4 Batch  169 / 228  Training Loss  0.004221600480377674\n","Epoch  4 Batch  170 / 228  Training Loss  0.005007931496948004\n","Epoch  4 Batch  171 / 228  Training Loss  0.004935289733111858\n","Epoch  4 Batch  172 / 228  Training Loss  0.006169397383928299\n","Epoch  4 Batch  173 / 228  Training Loss  0.010548989288508892\n","Epoch  4 Batch  174 / 228  Training Loss  0.0035608720500022173\n","Epoch  4 Batch  175 / 228  Training Loss  0.004508006386458874\n","Epoch  4 Batch  176 / 228  Training Loss  0.006133240181952715\n","Epoch  4 Batch  177 / 228  Training Loss  0.005627779755741358\n","Epoch  4 Batch  178 / 228  Training Loss  0.008681646548211575\n","Epoch  4 Batch  179 / 228  Training Loss  0.00242496095597744\n","Epoch  4 Batch  180 / 228  Training Loss  0.0033454825170338154\n","Epoch  4 Batch  181 / 228  Training Loss  0.005067633930593729\n","Epoch  4 Batch  182 / 228  Training Loss  0.011017820797860622\n","Epoch  4 Batch  183 / 228  Training Loss  0.005204641260206699\n","Epoch  4 Batch  184 / 228  Training Loss  0.002212722320109606\n","Epoch  4 Batch  185 / 228  Training Loss  0.015191343612968922\n","Epoch  4 Batch  186 / 228  Training Loss  0.00942778866738081\n","Epoch  4 Batch  187 / 228  Training Loss  0.005707474425435066\n","Epoch  4 Batch  188 / 228  Training Loss  0.00644251424819231\n","Epoch  4 Batch  189 / 228  Training Loss  0.019097870215773582\n","Epoch  4 Batch  190 / 228  Training Loss  0.00459090294316411\n","Epoch  4 Batch  191 / 228  Training Loss  0.0063996827229857445\n","Epoch  4 Batch  192 / 228  Training Loss  0.0077028824016451836\n","Epoch  4 Batch  193 / 228  Training Loss  0.007020214106887579\n","Epoch  4 Batch  194 / 228  Training Loss  0.011326701380312443\n","Epoch  4 Batch  195 / 228  Training Loss  0.019658179953694344\n","Epoch  4 Batch  196 / 228  Training Loss  0.004423367790877819\n","Epoch  4 Batch  197 / 228  Training Loss  0.011389029212296009\n","Epoch  4 Batch  198 / 228  Training Loss  0.0042017363011837006\n","Epoch  4 Batch  199 / 228  Training Loss  0.007443474140018225\n","Epoch  4 Batch  200 / 228  Training Loss  0.0035571125335991383\n","Epoch  4 Batch  201 / 228  Training Loss  0.006750862114131451\n","Epoch  4 Batch  202 / 228  Training Loss  0.0090045016258955\n","Epoch  4 Batch  203 / 228  Training Loss  0.0022325520403683186\n","Epoch  4 Batch  204 / 228  Training Loss  0.004637598060071468\n","Epoch  4 Batch  205 / 228  Training Loss  0.017490198835730553\n","Epoch  4 Batch  206 / 228  Training Loss  0.01669716276228428\n","Epoch  4 Batch  207 / 228  Training Loss  0.018877647817134857\n","Epoch  4 Batch  208 / 228  Training Loss  0.0066765123046934605\n","Epoch  4 Batch  209 / 228  Training Loss  0.00397476926445961\n","Epoch  4 Batch  210 / 228  Training Loss  0.008219532668590546\n","Epoch  4 Batch  211 / 228  Training Loss  0.01394713670015335\n","Epoch  4 Batch  212 / 228  Training Loss  0.007303810212761164\n","Epoch  4 Batch  213 / 228  Training Loss  0.012516707181930542\n","Epoch  4 Batch  214 / 228  Training Loss  0.004944055341184139\n","Epoch  4 Batch  215 / 228  Training Loss  0.004060620907694101\n","Epoch  4 Batch  216 / 228  Training Loss  0.002618317725136876\n","Epoch  4 Batch  217 / 228  Training Loss  0.009583644568920135\n","Epoch  4 Batch  218 / 228  Training Loss  0.003270485205575824\n","Epoch  4 Batch  219 / 228  Training Loss  0.00224054092541337\n","Epoch  4 Batch  220 / 228  Training Loss  0.01167299784719944\n","Epoch  4 Batch  221 / 228  Training Loss  0.003032765118405223\n","Epoch  4 Batch  222 / 228  Training Loss  0.0025952092837542295\n","Epoch  4 Batch  223 / 228  Training Loss  0.012196527794003487\n","Epoch  4 Batch  224 / 228  Training Loss  0.012797720730304718\n","Epoch  4 Batch  225 / 228  Training Loss  0.007605064660310745\n","Epoch  4 Batch  226 / 228  Training Loss  0.0070459493435919285\n","Epoch  4 Batch  227 / 228  Training Loss  0.0050959051586687565\n","   5    |    -    |   0.007790   | 95.160061\n","----------------------------------------------------------------------\n","Running epoch: 5\n","Epoch  5 Batch  0 / 228  Training Loss  0.003101613838225603\n","Epoch  5 Batch  1 / 228  Training Loss  0.004477364011108875\n","Epoch  5 Batch  2 / 228  Training Loss  0.002097722142934799\n","Epoch  5 Batch  3 / 228  Training Loss  0.0074928090907633305\n","Epoch  5 Batch  4 / 228  Training Loss  0.0019706340972334146\n","Epoch  5 Batch  5 / 228  Training Loss  0.009208230301737785\n","Epoch  5 Batch  6 / 228  Training Loss  0.009512251242995262\n","Epoch  5 Batch  7 / 228  Training Loss  0.004206118639558554\n","Epoch  5 Batch  8 / 228  Training Loss  0.003129614982753992\n","Epoch  5 Batch  9 / 228  Training Loss  0.006146729923784733\n","Epoch  5 Batch  10 / 228  Training Loss  0.0033952626399695873\n","Epoch  5 Batch  11 / 228  Training Loss  0.005032560788094997\n","Epoch  5 Batch  12 / 228  Training Loss  0.003441770328208804\n","Epoch  5 Batch  13 / 228  Training Loss  0.0024776863865554333\n","Epoch  5 Batch  14 / 228  Training Loss  0.0018400810658931732\n","Epoch  5 Batch  15 / 228  Training Loss  0.0036418226081877947\n","Epoch  5 Batch  16 / 228  Training Loss  0.0037994454614818096\n","Epoch  5 Batch  17 / 228  Training Loss  0.0033909815829247236\n","Epoch  5 Batch  18 / 228  Training Loss  0.0051592555828392506\n","Epoch  5 Batch  19 / 228  Training Loss  0.022360695526003838\n","Epoch  5 Batch  20 / 228  Training Loss  0.013818186707794666\n","Epoch  5 Batch  21 / 228  Training Loss  0.006325156893581152\n","Epoch  5 Batch  22 / 228  Training Loss  0.008278129622340202\n","Epoch  5 Batch  23 / 228  Training Loss  0.0068846652284264565\n","Epoch  5 Batch  24 / 228  Training Loss  0.008958885446190834\n","Epoch  5 Batch  25 / 228  Training Loss  0.006949850358068943\n","Epoch  5 Batch  26 / 228  Training Loss  0.004821344278752804\n","Epoch  5 Batch  27 / 228  Training Loss  0.00730733061209321\n","Epoch  5 Batch  28 / 228  Training Loss  0.013575857505202293\n","Epoch  5 Batch  29 / 228  Training Loss  0.0073669953271746635\n","Epoch  5 Batch  30 / 228  Training Loss  0.010129297152161598\n","Epoch  5 Batch  31 / 228  Training Loss  0.008356974460184574\n","Epoch  5 Batch  32 / 228  Training Loss  0.0030267820693552494\n","Epoch  5 Batch  33 / 228  Training Loss  0.006756233517080545\n","Epoch  5 Batch  34 / 228  Training Loss  0.002541152061894536\n","Epoch  5 Batch  35 / 228  Training Loss  0.001282422337681055\n","Epoch  5 Batch  36 / 228  Training Loss  0.002096120733767748\n","Epoch  5 Batch  37 / 228  Training Loss  0.0036402158439159393\n","Epoch  5 Batch  38 / 228  Training Loss  0.003995855804532766\n","Epoch  5 Batch  39 / 228  Training Loss  0.0013250545598566532\n","Epoch  5 Batch  40 / 228  Training Loss  0.0030124112963676453\n","Epoch  5 Batch  41 / 228  Training Loss  0.0018356669461354613\n","Epoch  5 Batch  42 / 228  Training Loss  0.012012621387839317\n","Epoch  5 Batch  43 / 228  Training Loss  0.0028358264826238155\n","Epoch  5 Batch  44 / 228  Training Loss  0.00215392978861928\n","Epoch  5 Batch  45 / 228  Training Loss  0.003909970633685589\n","Epoch  5 Batch  46 / 228  Training Loss  0.0035345167852938175\n","Epoch  5 Batch  47 / 228  Training Loss  0.0014368396950885653\n","Epoch  5 Batch  48 / 228  Training Loss  0.007964048534631729\n","Epoch  5 Batch  49 / 228  Training Loss  0.004598063882440329\n","Epoch  5 Batch  50 / 228  Training Loss  0.01212758757174015\n","Epoch  5 Batch  51 / 228  Training Loss  0.0030078799463808537\n","Epoch  5 Batch  52 / 228  Training Loss  0.0025698852259665728\n","Epoch  5 Batch  53 / 228  Training Loss  0.002557970816269517\n","Epoch  5 Batch  54 / 228  Training Loss  0.007629703730344772\n","Epoch  5 Batch  55 / 228  Training Loss  0.005730430595576763\n","Epoch  5 Batch  56 / 228  Training Loss  0.0043684253469109535\n","Epoch  5 Batch  57 / 228  Training Loss  0.008258383721113205\n","Epoch  5 Batch  58 / 228  Training Loss  0.00513320229947567\n","Epoch  5 Batch  59 / 228  Training Loss  0.016035929322242737\n","Epoch  5 Batch  60 / 228  Training Loss  0.00520674791187048\n","Epoch  5 Batch  61 / 228  Training Loss  0.003887633327394724\n","Epoch  5 Batch  62 / 228  Training Loss  0.0023997430689632893\n","Epoch  5 Batch  63 / 228  Training Loss  0.0032674376852810383\n","Epoch  5 Batch  64 / 228  Training Loss  0.006996517535299063\n","Epoch  5 Batch  65 / 228  Training Loss  0.00270425109192729\n","Epoch  5 Batch  66 / 228  Training Loss  0.0019169090082868934\n","Epoch  5 Batch  67 / 228  Training Loss  0.0046011945232748985\n","Epoch  5 Batch  68 / 228  Training Loss  0.01654583401978016\n","Epoch  5 Batch  69 / 228  Training Loss  0.0068993233144283295\n","Epoch  5 Batch  70 / 228  Training Loss  0.005122976843267679\n","Epoch  5 Batch  71 / 228  Training Loss  0.000972243957221508\n","Epoch  5 Batch  72 / 228  Training Loss  0.0037822718732059\n","Epoch  5 Batch  73 / 228  Training Loss  0.004270755220204592\n","Epoch  5 Batch  74 / 228  Training Loss  0.0057494197972118855\n","Epoch  5 Batch  75 / 228  Training Loss  0.00285329925827682\n","Epoch  5 Batch  76 / 228  Training Loss  0.002671109978109598\n","Epoch  5 Batch  77 / 228  Training Loss  0.0021595011930912733\n","Epoch  5 Batch  78 / 228  Training Loss  0.005491375923156738\n","Epoch  5 Batch  79 / 228  Training Loss  0.004815266467630863\n","Epoch  5 Batch  80 / 228  Training Loss  0.0026860549114644527\n","Epoch  5 Batch  81 / 228  Training Loss  0.003639801172539592\n","Epoch  5 Batch  82 / 228  Training Loss  0.002472812542691827\n","Epoch  5 Batch  83 / 228  Training Loss  0.0020395980682224035\n","Epoch  5 Batch  84 / 228  Training Loss  0.0018494479591026902\n","Epoch  5 Batch  85 / 228  Training Loss  0.001701932749710977\n","Epoch  5 Batch  86 / 228  Training Loss  0.003681804286316037\n","Epoch  5 Batch  87 / 228  Training Loss  0.002132256282493472\n","Epoch  5 Batch  88 / 228  Training Loss  0.001356927095912397\n","Epoch  5 Batch  89 / 228  Training Loss  0.0015061855083331466\n","Epoch  5 Batch  90 / 228  Training Loss  0.0008737021125853062\n","Epoch  5 Batch  91 / 228  Training Loss  0.0018190589034929872\n","Epoch  5 Batch  92 / 228  Training Loss  0.002290181117132306\n","Epoch  5 Batch  93 / 228  Training Loss  0.0011183430906385183\n","Epoch  5 Batch  94 / 228  Training Loss  0.0032335843425244093\n","Epoch  5 Batch  95 / 228  Training Loss  0.0011651685927063227\n","Epoch  5 Batch  96 / 228  Training Loss  0.0013215461513027549\n","Epoch  5 Batch  97 / 228  Training Loss  0.0011198617285117507\n","Epoch  5 Batch  98 / 228  Training Loss  0.0019801417365670204\n","Epoch  5 Batch  99 / 228  Training Loss  0.001399729517288506\n","Epoch  5 Batch  100 / 228  Training Loss  0.002898047212511301\n","Epoch  5 Batch  101 / 228  Training Loss  0.0035051654558628798\n","Epoch  5 Batch  102 / 228  Training Loss  0.003475605044513941\n","Epoch  5 Batch  103 / 228  Training Loss  0.002051211427897215\n","Epoch  5 Batch  104 / 228  Training Loss  0.002268739277496934\n","Epoch  5 Batch  105 / 228  Training Loss  0.007165456656366587\n","Epoch  5 Batch  106 / 228  Training Loss  0.003501946572214365\n","Epoch  5 Batch  107 / 228  Training Loss  0.0015649159904569387\n","Epoch  5 Batch  108 / 228  Training Loss  0.002328770002350211\n","Epoch  5 Batch  109 / 228  Training Loss  0.001903388649225235\n","Epoch  5 Batch  110 / 228  Training Loss  0.003215423319488764\n","Epoch  5 Batch  111 / 228  Training Loss  0.0036918423138558865\n","Epoch  5 Batch  112 / 228  Training Loss  0.009308820590376854\n","Epoch  5 Batch  113 / 228  Training Loss  0.013743375428020954\n","Epoch  5 Batch  114 / 228  Training Loss  0.003518297104164958\n","Epoch  5 Batch  115 / 228  Training Loss  0.002925227163359523\n","Epoch  5 Batch  116 / 228  Training Loss  0.005355305504053831\n","Epoch  5 Batch  117 / 228  Training Loss  0.008226725272834301\n","Epoch  5 Batch  118 / 228  Training Loss  0.004862311761826277\n","Epoch  5 Batch  119 / 228  Training Loss  0.004808464087545872\n","Epoch  5 Batch  120 / 228  Training Loss  0.0017701985780149698\n","Epoch  5 Batch  121 / 228  Training Loss  0.003216186538338661\n","Epoch  5 Batch  122 / 228  Training Loss  0.00714930659160018\n","Epoch  5 Batch  123 / 228  Training Loss  0.01023111306130886\n","Epoch  5 Batch  124 / 228  Training Loss  0.0021995280403643847\n","Epoch  5 Batch  125 / 228  Training Loss  0.0017856223275884986\n","Epoch  5 Batch  126 / 228  Training Loss  0.002107903826981783\n","Epoch  5 Batch  127 / 228  Training Loss  0.0051232329569756985\n","Epoch  5 Batch  128 / 228  Training Loss  0.0009841620922088623\n","Epoch  5 Batch  129 / 228  Training Loss  0.0011613505193963647\n","Epoch  5 Batch  130 / 228  Training Loss  0.001385843614116311\n","Epoch  5 Batch  131 / 228  Training Loss  0.0007638346287421882\n","Epoch  5 Batch  132 / 228  Training Loss  0.0011133552761748433\n","Epoch  5 Batch  133 / 228  Training Loss  0.0018217175966128707\n","Epoch  5 Batch  134 / 228  Training Loss  0.002281314693391323\n","Epoch  5 Batch  135 / 228  Training Loss  0.002479812130331993\n","Epoch  5 Batch  136 / 228  Training Loss  0.005659868009388447\n","Epoch  5 Batch  137 / 228  Training Loss  0.0014566493919119239\n","Epoch  5 Batch  138 / 228  Training Loss  0.009001878090202808\n","Epoch  5 Batch  139 / 228  Training Loss  0.020902596414089203\n","Epoch  5 Batch  140 / 228  Training Loss  0.02290927991271019\n","Epoch  5 Batch  141 / 228  Training Loss  0.00278010917827487\n","Epoch  5 Batch  142 / 228  Training Loss  0.0052363150753080845\n","Epoch  5 Batch  143 / 228  Training Loss  0.010188063606619835\n","Epoch  5 Batch  144 / 228  Training Loss  0.01038764975965023\n","Epoch  5 Batch  145 / 228  Training Loss  0.008684828877449036\n","Epoch  5 Batch  146 / 228  Training Loss  0.0060954163782298565\n","Epoch  5 Batch  147 / 228  Training Loss  0.02455204725265503\n","Epoch  5 Batch  148 / 228  Training Loss  0.013854694552719593\n","Epoch  5 Batch  149 / 228  Training Loss  0.013746688142418861\n","Epoch  5 Batch  150 / 228  Training Loss  0.00421102112159133\n","Epoch  5 Batch  151 / 228  Training Loss  0.00461540836840868\n","Epoch  5 Batch  152 / 228  Training Loss  0.0015280095394700766\n","Epoch  5 Batch  153 / 228  Training Loss  0.0021692237351089716\n","Epoch  5 Batch  154 / 228  Training Loss  0.002531451638787985\n","Epoch  5 Batch  155 / 228  Training Loss  0.008985189720988274\n","Epoch  5 Batch  156 / 228  Training Loss  0.004356003366410732\n","Epoch  5 Batch  157 / 228  Training Loss  0.007784997578710318\n","Epoch  5 Batch  158 / 228  Training Loss  0.0022521077189594507\n","Epoch  5 Batch  159 / 228  Training Loss  0.009389644488692284\n","Epoch  5 Batch  160 / 228  Training Loss  0.007065309677273035\n","Epoch  5 Batch  161 / 228  Training Loss  0.004792456515133381\n","Epoch  5 Batch  162 / 228  Training Loss  0.008426015265285969\n","Epoch  5 Batch  163 / 228  Training Loss  0.0016051942948251963\n","Epoch  5 Batch  164 / 228  Training Loss  0.0013976668706163764\n","Epoch  5 Batch  165 / 228  Training Loss  0.0034827659837901592\n","Epoch  5 Batch  166 / 228  Training Loss  0.0020400916691869497\n","Epoch  5 Batch  167 / 228  Training Loss  0.0017708875238895416\n","Epoch  5 Batch  168 / 228  Training Loss  0.0014462799299508333\n","Epoch  5 Batch  169 / 228  Training Loss  0.004876813385635614\n","Epoch  5 Batch  170 / 228  Training Loss  0.0024643440265208483\n","Epoch  5 Batch  171 / 228  Training Loss  0.01707600988447666\n","Epoch  5 Batch  172 / 228  Training Loss  0.004204390104860067\n","Epoch  5 Batch  173 / 228  Training Loss  0.0011034271446987987\n","Epoch  5 Batch  174 / 228  Training Loss  0.0030560896266251802\n","Epoch  5 Batch  175 / 228  Training Loss  0.0014630736550316215\n","Epoch  5 Batch  176 / 228  Training Loss  0.0013612039620056748\n","Epoch  5 Batch  177 / 228  Training Loss  0.0014362474903464317\n","Epoch  5 Batch  178 / 228  Training Loss  0.0013715771492570639\n","Epoch  5 Batch  179 / 228  Training Loss  0.0031886748038232327\n","Epoch  5 Batch  180 / 228  Training Loss  0.002753365086391568\n","Epoch  5 Batch  181 / 228  Training Loss  0.00266772648319602\n","Epoch  5 Batch  182 / 228  Training Loss  0.003564128652215004\n","Epoch  5 Batch  183 / 228  Training Loss  0.0016836386639624834\n","Epoch  5 Batch  184 / 228  Training Loss  0.0016835222486406565\n","Epoch  5 Batch  185 / 228  Training Loss  0.0012770610628649592\n","Epoch  5 Batch  186 / 228  Training Loss  0.0009547827066853642\n","Epoch  5 Batch  187 / 228  Training Loss  0.0011475158389657736\n","Epoch  5 Batch  188 / 228  Training Loss  0.0015793921193107963\n","Epoch  5 Batch  189 / 228  Training Loss  0.0013659846736118197\n","Epoch  5 Batch  190 / 228  Training Loss  0.0011991532519459724\n","Epoch  5 Batch  191 / 228  Training Loss  0.002033841097727418\n","Epoch  5 Batch  192 / 228  Training Loss  0.003965606912970543\n","Epoch  5 Batch  193 / 228  Training Loss  0.0010141844395548105\n","Epoch  5 Batch  194 / 228  Training Loss  0.001532033202238381\n","Epoch  5 Batch  195 / 228  Training Loss  0.002484096447005868\n","Epoch  5 Batch  196 / 228  Training Loss  0.0014346230309456587\n","Epoch  5 Batch  197 / 228  Training Loss  0.0022424531634896994\n","Epoch  5 Batch  198 / 228  Training Loss  0.003579426556825638\n","Epoch  5 Batch  199 / 228  Training Loss  0.0034839059226214886\n","Epoch  5 Batch  200 / 228  Training Loss  0.0011754375882446766\n","Epoch  5 Batch  201 / 228  Training Loss  0.0014901608228683472\n","Epoch  5 Batch  202 / 228  Training Loss  0.0008471040055155754\n","Epoch  5 Batch  203 / 228  Training Loss  0.0016737093683332205\n","Epoch  5 Batch  204 / 228  Training Loss  0.001457033446058631\n","Epoch  5 Batch  205 / 228  Training Loss  0.0070076314732432365\n","Epoch  5 Batch  206 / 228  Training Loss  0.0009774611098691821\n","Epoch  5 Batch  207 / 228  Training Loss  0.00467025488615036\n","Epoch  5 Batch  208 / 228  Training Loss  0.005914195906370878\n","Epoch  5 Batch  209 / 228  Training Loss  0.006434791721403599\n","Epoch  5 Batch  210 / 228  Training Loss  0.003267767373472452\n","Epoch  5 Batch  211 / 228  Training Loss  0.007739501539617777\n","Epoch  5 Batch  212 / 228  Training Loss  0.008775336667895317\n","Epoch  5 Batch  213 / 228  Training Loss  0.006498272530734539\n","Epoch  5 Batch  214 / 228  Training Loss  0.004273639060556889\n","Epoch  5 Batch  215 / 228  Training Loss  0.0022958863992244005\n","Epoch  5 Batch  216 / 228  Training Loss  0.001000734861008823\n","Epoch  5 Batch  217 / 228  Training Loss  0.0011965145822614431\n","Epoch  5 Batch  218 / 228  Training Loss  0.006500283721834421\n","Epoch  5 Batch  219 / 228  Training Loss  0.009007817134261131\n","Epoch  5 Batch  220 / 228  Training Loss  0.00055959407472983\n","Epoch  5 Batch  221 / 228  Training Loss  0.00936036929488182\n","Epoch  5 Batch  222 / 228  Training Loss  0.055048465728759766\n","Epoch  5 Batch  223 / 228  Training Loss  0.6650453805923462\n","Epoch  5 Batch  224 / 228  Training Loss  0.08678559958934784\n","Epoch  5 Batch  225 / 228  Training Loss  0.033031828701496124\n","Epoch  5 Batch  226 / 228  Training Loss  0.03635076433420181\n","Epoch  5 Batch  227 / 228  Training Loss  0.0157329011708498\n","   6    |    -    |   0.008396   | 88.414634\n","----------------------------------------------------------------------\n","Running epoch: 6\n","Epoch  6 Batch  0 / 228  Training Loss  0.01515183039009571\n","Epoch  6 Batch  1 / 228  Training Loss  0.008021676912903786\n","Epoch  6 Batch  2 / 228  Training Loss  0.015574166551232338\n","Epoch  6 Batch  3 / 228  Training Loss  0.015994977205991745\n","Epoch  6 Batch  4 / 228  Training Loss  0.007799380924552679\n","Epoch  6 Batch  5 / 228  Training Loss  0.00380168785341084\n","Epoch  6 Batch  6 / 228  Training Loss  0.02390826679766178\n","Epoch  6 Batch  7 / 228  Training Loss  0.004184235353022814\n","Epoch  6 Batch  8 / 228  Training Loss  0.007485959678888321\n","Epoch  6 Batch  9 / 228  Training Loss  0.003094363259151578\n","Epoch  6 Batch  10 / 228  Training Loss  0.0048438007943332195\n","Epoch  6 Batch  11 / 228  Training Loss  0.0038618252146989107\n","Epoch  6 Batch  12 / 228  Training Loss  0.005191499833017588\n","Epoch  6 Batch  13 / 228  Training Loss  0.016407880932092667\n","Epoch  6 Batch  14 / 228  Training Loss  0.02093815803527832\n","Epoch  6 Batch  15 / 228  Training Loss  0.019770074635744095\n","Epoch  6 Batch  16 / 228  Training Loss  0.008531013503670692\n","Epoch  6 Batch  17 / 228  Training Loss  0.006633218377828598\n","Epoch  6 Batch  18 / 228  Training Loss  0.003410765202715993\n","Epoch  6 Batch  19 / 228  Training Loss  0.004250884987413883\n","Epoch  6 Batch  20 / 228  Training Loss  0.006445274688303471\n","Epoch  6 Batch  21 / 228  Training Loss  0.005882626865059137\n","Epoch  6 Batch  22 / 228  Training Loss  0.0039488887414336205\n","Epoch  6 Batch  23 / 228  Training Loss  0.005206964910030365\n","Epoch  6 Batch  24 / 228  Training Loss  0.004410305060446262\n","Epoch  6 Batch  25 / 228  Training Loss  0.004848698619753122\n","Epoch  6 Batch  26 / 228  Training Loss  0.04172252118587494\n","Epoch  6 Batch  27 / 228  Training Loss  0.34817585349082947\n","Epoch  6 Batch  28 / 228  Training Loss  0.31173163652420044\n","Epoch  6 Batch  29 / 228  Training Loss  0.049301691353321075\n","Epoch  6 Batch  30 / 228  Training Loss  0.012518529780209064\n","Epoch  6 Batch  31 / 228  Training Loss  0.009549432434141636\n","Epoch  6 Batch  32 / 228  Training Loss  0.01762700267136097\n","Epoch  6 Batch  33 / 228  Training Loss  0.009732838720083237\n","Epoch  6 Batch  34 / 228  Training Loss  0.021737897768616676\n","Epoch  6 Batch  35 / 228  Training Loss  0.020229032263159752\n","Epoch  6 Batch  36 / 228  Training Loss  0.009869092144072056\n","Epoch  6 Batch  37 / 228  Training Loss  0.0072751762345433235\n","Epoch  6 Batch  38 / 228  Training Loss  0.008840254507958889\n","Epoch  6 Batch  39 / 228  Training Loss  0.014690758660435677\n","Epoch  6 Batch  40 / 228  Training Loss  0.017506355419754982\n","Epoch  6 Batch  41 / 228  Training Loss  0.008103892207145691\n","Epoch  6 Batch  42 / 228  Training Loss  0.003825361607596278\n","Epoch  6 Batch  43 / 228  Training Loss  0.009955709800124168\n","Epoch  6 Batch  44 / 228  Training Loss  0.0043189432471990585\n","Epoch  6 Batch  45 / 228  Training Loss  0.002218436449766159\n","Epoch  6 Batch  46 / 228  Training Loss  0.002129812492057681\n","Epoch  6 Batch  47 / 228  Training Loss  0.004692489746958017\n","Epoch  6 Batch  48 / 228  Training Loss  0.004502973519265652\n","Epoch  6 Batch  49 / 228  Training Loss  0.007358177099376917\n","Epoch  6 Batch  50 / 228  Training Loss  0.005112635903060436\n","Epoch  6 Batch  51 / 228  Training Loss  0.0031481455080211163\n","Epoch  6 Batch  52 / 228  Training Loss  0.0031743429135531187\n","Epoch  6 Batch  53 / 228  Training Loss  0.0024309076834470034\n","Epoch  6 Batch  54 / 228  Training Loss  0.0044088782742619514\n","Epoch  6 Batch  55 / 228  Training Loss  0.003274260088801384\n","Epoch  6 Batch  56 / 228  Training Loss  0.011454147286713123\n","Epoch  6 Batch  57 / 228  Training Loss  0.0028784163296222687\n","Epoch  6 Batch  58 / 228  Training Loss  0.004609859082847834\n","Epoch  6 Batch  59 / 228  Training Loss  0.0026148695033043623\n","Epoch  6 Batch  60 / 228  Training Loss  0.002924122381955385\n","Epoch  6 Batch  61 / 228  Training Loss  0.0033482902217656374\n","Epoch  6 Batch  62 / 228  Training Loss  0.002141425386071205\n","Epoch  6 Batch  63 / 228  Training Loss  0.00199753581546247\n","Epoch  6 Batch  64 / 228  Training Loss  0.002594324294477701\n","Epoch  6 Batch  65 / 228  Training Loss  0.0042345779947936535\n","Epoch  6 Batch  66 / 228  Training Loss  0.0043235523626208305\n","Epoch  6 Batch  67 / 228  Training Loss  0.002191264880821109\n","Epoch  6 Batch  68 / 228  Training Loss  0.0033620218746364117\n","Epoch  6 Batch  69 / 228  Training Loss  0.0014523016288876534\n","Epoch  6 Batch  70 / 228  Training Loss  0.009730057790875435\n","Epoch  6 Batch  71 / 228  Training Loss  0.009760879911482334\n","Epoch  6 Batch  72 / 228  Training Loss  0.0038837268948554993\n","Epoch  6 Batch  73 / 228  Training Loss  0.003508245572447777\n","Epoch  6 Batch  74 / 228  Training Loss  0.0024924064055085182\n","Epoch  6 Batch  75 / 228  Training Loss  0.0015373843489214778\n","Epoch  6 Batch  76 / 228  Training Loss  0.001423341571353376\n","Epoch  6 Batch  77 / 228  Training Loss  0.003002303186804056\n","Epoch  6 Batch  78 / 228  Training Loss  0.005707537289708853\n","Epoch  6 Batch  79 / 228  Training Loss  0.005503953900188208\n","Epoch  6 Batch  80 / 228  Training Loss  0.00211933976970613\n","Epoch  6 Batch  81 / 228  Training Loss  0.0035628373734652996\n","Epoch  6 Batch  82 / 228  Training Loss  0.001500277896411717\n","Epoch  6 Batch  83 / 228  Training Loss  0.005124312359839678\n","Epoch  6 Batch  84 / 228  Training Loss  0.0018444356974214315\n","Epoch  6 Batch  85 / 228  Training Loss  0.004138939082622528\n","Epoch  6 Batch  86 / 228  Training Loss  0.00544761074706912\n","Epoch  6 Batch  87 / 228  Training Loss  0.0026046321727335453\n","Epoch  6 Batch  88 / 228  Training Loss  0.0013758967397734523\n","Epoch  6 Batch  89 / 228  Training Loss  0.001328450394794345\n","Epoch  6 Batch  90 / 228  Training Loss  0.0014437090139836073\n","Epoch  6 Batch  91 / 228  Training Loss  0.0020713871344923973\n","Epoch  6 Batch  92 / 228  Training Loss  0.0039002958219498396\n","Epoch  6 Batch  93 / 228  Training Loss  0.00307639641687274\n","Epoch  6 Batch  94 / 228  Training Loss  0.0029175803065299988\n","Epoch  6 Batch  95 / 228  Training Loss  0.00526203541085124\n","Epoch  6 Batch  96 / 228  Training Loss  0.0020758898463100195\n","Epoch  6 Batch  97 / 228  Training Loss  0.0025695678777992725\n","Epoch  6 Batch  98 / 228  Training Loss  0.0029807179234921932\n","Epoch  6 Batch  99 / 228  Training Loss  0.004317736253142357\n","Epoch  6 Batch  100 / 228  Training Loss  0.005510394461452961\n","Epoch  6 Batch  101 / 228  Training Loss  0.0010551465675234795\n","Epoch  6 Batch  102 / 228  Training Loss  0.005645907483994961\n","Epoch  6 Batch  103 / 228  Training Loss  0.0030942708253860474\n","Epoch  6 Batch  104 / 228  Training Loss  0.0008994952659122646\n","Epoch  6 Batch  105 / 228  Training Loss  0.0012352990452200174\n","Epoch  6 Batch  106 / 228  Training Loss  0.0009609796106815338\n","Epoch  6 Batch  107 / 228  Training Loss  0.0014819151256233454\n","Epoch  6 Batch  108 / 228  Training Loss  0.0007439220789819956\n","Epoch  6 Batch  109 / 228  Training Loss  0.0014522401615977287\n","Epoch  6 Batch  110 / 228  Training Loss  0.0025892772246152163\n","Epoch  6 Batch  111 / 228  Training Loss  0.005920033901929855\n","Epoch  6 Batch  112 / 228  Training Loss  0.0008267667144536972\n","Epoch  6 Batch  113 / 228  Training Loss  0.0024036341346800327\n","Epoch  6 Batch  114 / 228  Training Loss  0.002800008747726679\n","Epoch  6 Batch  115 / 228  Training Loss  0.0016845179488882422\n","Epoch  6 Batch  116 / 228  Training Loss  0.0018825370352715254\n","Epoch  6 Batch  117 / 228  Training Loss  0.0007345796329900622\n","Epoch  6 Batch  118 / 228  Training Loss  0.002283276990056038\n","Epoch  6 Batch  119 / 228  Training Loss  0.002445653546601534\n","Epoch  6 Batch  120 / 228  Training Loss  0.0018888901686295867\n","Epoch  6 Batch  121 / 228  Training Loss  0.0023968406021595\n","Epoch  6 Batch  122 / 228  Training Loss  0.0037739016115665436\n","Epoch  6 Batch  123 / 228  Training Loss  0.004676127806305885\n","Epoch  6 Batch  124 / 228  Training Loss  0.00211526220664382\n","Epoch  6 Batch  125 / 228  Training Loss  0.0026969793252646923\n","Epoch  6 Batch  126 / 228  Training Loss  0.018632782623171806\n","Epoch  6 Batch  127 / 228  Training Loss  0.0026432150043547153\n","Epoch  6 Batch  128 / 228  Training Loss  0.005541470367461443\n","Epoch  6 Batch  129 / 228  Training Loss  0.0019848968368023634\n","Epoch  6 Batch  130 / 228  Training Loss  0.00396096333861351\n","Epoch  6 Batch  131 / 228  Training Loss  0.0009369057370349765\n","Epoch  6 Batch  132 / 228  Training Loss  0.0032801765482872725\n","Epoch  6 Batch  133 / 228  Training Loss  0.0011121019488200545\n","Epoch  6 Batch  134 / 228  Training Loss  0.0035058199428021908\n","Epoch  6 Batch  135 / 228  Training Loss  0.0051428708247840405\n","Epoch  6 Batch  136 / 228  Training Loss  0.0021700167562812567\n","Epoch  6 Batch  137 / 228  Training Loss  0.0014059956884011626\n","Epoch  6 Batch  138 / 228  Training Loss  0.0014875142369419336\n","Epoch  6 Batch  139 / 228  Training Loss  0.005248816683888435\n","Epoch  6 Batch  140 / 228  Training Loss  0.000970902678091079\n","Epoch  6 Batch  141 / 228  Training Loss  0.0021932092495262623\n","Epoch  6 Batch  142 / 228  Training Loss  0.002514815656468272\n","Epoch  6 Batch  143 / 228  Training Loss  0.014177754521369934\n","Epoch  6 Batch  144 / 228  Training Loss  0.0033406305592507124\n","Epoch  6 Batch  145 / 228  Training Loss  0.001744783134199679\n","Epoch  6 Batch  146 / 228  Training Loss  0.004069936461746693\n","Epoch  6 Batch  147 / 228  Training Loss  0.0013604284031316638\n","Epoch  6 Batch  148 / 228  Training Loss  0.0009116609580814838\n","Epoch  6 Batch  149 / 228  Training Loss  0.004157244227826595\n","Epoch  6 Batch  150 / 228  Training Loss  0.0009541846811771393\n","Epoch  6 Batch  151 / 228  Training Loss  0.00348467449657619\n","Epoch  6 Batch  152 / 228  Training Loss  0.0017896706704050303\n","Epoch  6 Batch  153 / 228  Training Loss  0.0011105227749794722\n","Epoch  6 Batch  154 / 228  Training Loss  0.002863069297745824\n","Epoch  6 Batch  155 / 228  Training Loss  0.004995537921786308\n","Epoch  6 Batch  156 / 228  Training Loss  0.005531178321689367\n","Epoch  6 Batch  157 / 228  Training Loss  0.0017440819647163153\n","Epoch  6 Batch  158 / 228  Training Loss  0.005689946934580803\n","Epoch  6 Batch  159 / 228  Training Loss  0.0006758301169611514\n","Epoch  6 Batch  160 / 228  Training Loss  0.004301354289054871\n","Epoch  6 Batch  161 / 228  Training Loss  0.0009058859432116151\n","Epoch  6 Batch  162 / 228  Training Loss  0.0015449101338163018\n","Epoch  6 Batch  163 / 228  Training Loss  0.0007272041984833777\n","Epoch  6 Batch  164 / 228  Training Loss  0.002705763094127178\n","Epoch  6 Batch  165 / 228  Training Loss  0.0028746435418725014\n","Epoch  6 Batch  166 / 228  Training Loss  0.0016300093848258257\n","Epoch  6 Batch  167 / 228  Training Loss  0.0007891532150097191\n","Epoch  6 Batch  168 / 228  Training Loss  0.002154116053134203\n","Epoch  6 Batch  169 / 228  Training Loss  0.0023877762723714113\n","Epoch  6 Batch  170 / 228  Training Loss  0.0006944359629414976\n","Epoch  6 Batch  171 / 228  Training Loss  0.0012987246736884117\n","Epoch  6 Batch  172 / 228  Training Loss  0.0032996051013469696\n","Epoch  6 Batch  173 / 228  Training Loss  0.006551495287567377\n","Epoch  6 Batch  174 / 228  Training Loss  0.0030458816327154636\n","Epoch  6 Batch  175 / 228  Training Loss  0.0013996174093335867\n","Epoch  6 Batch  176 / 228  Training Loss  0.0006405685562640429\n","Epoch  6 Batch  177 / 228  Training Loss  0.0011218946892768145\n","Epoch  6 Batch  178 / 228  Training Loss  0.002669271780177951\n","Epoch  6 Batch  179 / 228  Training Loss  0.000932958151679486\n","Epoch  6 Batch  180 / 228  Training Loss  0.0038597919046878815\n","Epoch  6 Batch  181 / 228  Training Loss  0.0073564983904361725\n","Epoch  6 Batch  182 / 228  Training Loss  0.007908823899924755\n","Epoch  6 Batch  183 / 228  Training Loss  0.001912853098474443\n","Epoch  6 Batch  184 / 228  Training Loss  0.0013535970356315374\n","Epoch  6 Batch  185 / 228  Training Loss  0.002991603221744299\n","Epoch  6 Batch  186 / 228  Training Loss  0.0009780358523130417\n","Epoch  6 Batch  187 / 228  Training Loss  0.0020204689353704453\n","Epoch  6 Batch  188 / 228  Training Loss  0.003210697788745165\n","Epoch  6 Batch  189 / 228  Training Loss  0.0010923525551334023\n","Epoch  6 Batch  190 / 228  Training Loss  0.0010054400190711021\n","Epoch  6 Batch  191 / 228  Training Loss  0.002388096647337079\n","Epoch  6 Batch  192 / 228  Training Loss  0.0014565272722393274\n","Epoch  6 Batch  193 / 228  Training Loss  0.0020498032681643963\n","Epoch  6 Batch  194 / 228  Training Loss  0.0009735474595800042\n","Epoch  6 Batch  195 / 228  Training Loss  0.001023793825879693\n","Epoch  6 Batch  196 / 228  Training Loss  0.0015457989647984505\n","Epoch  6 Batch  197 / 228  Training Loss  0.0006491667591035366\n","Epoch  6 Batch  198 / 228  Training Loss  0.0008668603259138763\n","Epoch  6 Batch  199 / 228  Training Loss  0.0020836740732192993\n","Epoch  6 Batch  200 / 228  Training Loss  0.0021684111561626196\n","Epoch  6 Batch  201 / 228  Training Loss  0.00844921637326479\n","Epoch  6 Batch  202 / 228  Training Loss  0.0057981680147349834\n","Epoch  6 Batch  203 / 228  Training Loss  0.004039464518427849\n","Epoch  6 Batch  204 / 228  Training Loss  0.004449192434549332\n","Epoch  6 Batch  205 / 228  Training Loss  0.002692698035389185\n","Epoch  6 Batch  206 / 228  Training Loss  0.007788052316755056\n","Epoch  6 Batch  207 / 228  Training Loss  0.0013946013059467077\n","Epoch  6 Batch  208 / 228  Training Loss  0.001472179195843637\n","Epoch  6 Batch  209 / 228  Training Loss  0.0007523857639171183\n","Epoch  6 Batch  210 / 228  Training Loss  0.002554195700213313\n","Epoch  6 Batch  211 / 228  Training Loss  0.0015792511403560638\n","Epoch  6 Batch  212 / 228  Training Loss  0.003137692343443632\n","Epoch  6 Batch  213 / 228  Training Loss  0.0013580693630501628\n","Epoch  6 Batch  214 / 228  Training Loss  0.0021286404225975275\n","Epoch  6 Batch  215 / 228  Training Loss  0.0017349583795294166\n","Epoch  6 Batch  216 / 228  Training Loss  0.001564186648465693\n","Epoch  6 Batch  217 / 228  Training Loss  0.0008852080209180713\n","Epoch  6 Batch  218 / 228  Training Loss  0.0013974372996017337\n","Epoch  6 Batch  219 / 228  Training Loss  0.0010665897279977798\n","Epoch  6 Batch  220 / 228  Training Loss  0.0025509034749120474\n","Epoch  6 Batch  221 / 228  Training Loss  0.0011219459120184183\n","Epoch  6 Batch  222 / 228  Training Loss  0.0007837609155103564\n","Epoch  6 Batch  223 / 228  Training Loss  0.003775236662477255\n","Epoch  6 Batch  224 / 228  Training Loss  0.001705653965473175\n","Epoch  6 Batch  225 / 228  Training Loss  0.005817519500851631\n","Epoch  6 Batch  226 / 228  Training Loss  0.0007016632589511573\n","Epoch  6 Batch  227 / 228  Training Loss  0.0007050262065604329\n","   7    |    -    |   0.007463   | 97.827744\n","----------------------------------------------------------------------\n","Running epoch: 7\n","Epoch  7 Batch  0 / 228  Training Loss  0.0013296396937221289\n","Epoch  7 Batch  1 / 228  Training Loss  0.0006000538705848157\n","Epoch  7 Batch  2 / 228  Training Loss  0.0010308206547051668\n","Epoch  7 Batch  3 / 228  Training Loss  0.0006986799999140203\n","Epoch  7 Batch  4 / 228  Training Loss  0.0012411202769726515\n","Epoch  7 Batch  5 / 228  Training Loss  0.0012311139144003391\n","Epoch  7 Batch  6 / 228  Training Loss  0.0005608712672255933\n","Epoch  7 Batch  7 / 228  Training Loss  0.0005427533760666847\n","Epoch  7 Batch  8 / 228  Training Loss  0.0008055107900872827\n","Epoch  7 Batch  9 / 228  Training Loss  0.0004341588937677443\n","Epoch  7 Batch  10 / 228  Training Loss  0.0006701482343487442\n","Epoch  7 Batch  11 / 228  Training Loss  0.002853458747267723\n","Epoch  7 Batch  12 / 228  Training Loss  0.0012815369991585612\n","Epoch  7 Batch  13 / 228  Training Loss  0.0015813319478183985\n","Epoch  7 Batch  14 / 228  Training Loss  0.0015805658185854554\n","Epoch  7 Batch  15 / 228  Training Loss  0.003202452091500163\n","Epoch  7 Batch  16 / 228  Training Loss  0.00591578334569931\n","Epoch  7 Batch  17 / 228  Training Loss  0.0009145862422883511\n","Epoch  7 Batch  18 / 228  Training Loss  0.0008032220648601651\n","Epoch  7 Batch  19 / 228  Training Loss  0.0016075363382697105\n","Epoch  7 Batch  20 / 228  Training Loss  0.0012525944039225578\n","Epoch  7 Batch  21 / 228  Training Loss  0.002410012995824218\n","Epoch  7 Batch  22 / 228  Training Loss  0.0008717424934729934\n","Epoch  7 Batch  23 / 228  Training Loss  0.0018194944132119417\n","Epoch  7 Batch  24 / 228  Training Loss  0.0011024253908544779\n","Epoch  7 Batch  25 / 228  Training Loss  0.0010472915600985289\n","Epoch  7 Batch  26 / 228  Training Loss  0.009184660390019417\n","Epoch  7 Batch  27 / 228  Training Loss  0.0010467988904565573\n","Epoch  7 Batch  28 / 228  Training Loss  0.0008363627712242305\n","Epoch  7 Batch  29 / 228  Training Loss  0.0007657795795239508\n","Epoch  7 Batch  30 / 228  Training Loss  0.0005046986625529826\n","Epoch  7 Batch  31 / 228  Training Loss  0.001549809705466032\n","Epoch  7 Batch  32 / 228  Training Loss  0.0027321563102304935\n","Epoch  7 Batch  33 / 228  Training Loss  0.0022142352536320686\n","Epoch  7 Batch  34 / 228  Training Loss  0.0025641578249633312\n","Epoch  7 Batch  35 / 228  Training Loss  0.0009930335218086839\n","Epoch  7 Batch  36 / 228  Training Loss  0.0011912269983440638\n","Epoch  7 Batch  37 / 228  Training Loss  0.0010486175306141376\n","Epoch  7 Batch  38 / 228  Training Loss  0.0010230033658444881\n","Epoch  7 Batch  39 / 228  Training Loss  0.0011267493246123195\n","Epoch  7 Batch  40 / 228  Training Loss  0.00031339359702542424\n","Epoch  7 Batch  41 / 228  Training Loss  0.0007487075054086745\n","Epoch  7 Batch  42 / 228  Training Loss  0.0009561866754665971\n","Epoch  7 Batch  43 / 228  Training Loss  0.0003492042305879295\n","Epoch  7 Batch  44 / 228  Training Loss  0.0011647260980680585\n","Epoch  7 Batch  45 / 228  Training Loss  0.0010012165876105428\n","Epoch  7 Batch  46 / 228  Training Loss  0.0016413085395470262\n","Epoch  7 Batch  47 / 228  Training Loss  0.001597546273842454\n","Epoch  7 Batch  48 / 228  Training Loss  0.0008197211427614093\n","Epoch  7 Batch  49 / 228  Training Loss  0.0010854082647711039\n","Epoch  7 Batch  50 / 228  Training Loss  0.0004507018602453172\n","Epoch  7 Batch  51 / 228  Training Loss  0.000544440932571888\n","Epoch  7 Batch  52 / 228  Training Loss  0.000848798721563071\n","Epoch  7 Batch  53 / 228  Training Loss  0.00055570068070665\n","Epoch  7 Batch  54 / 228  Training Loss  0.0004800194001290947\n","Epoch  7 Batch  55 / 228  Training Loss  0.0005053585628047585\n","Epoch  7 Batch  56 / 228  Training Loss  0.0006813026266172528\n","Epoch  7 Batch  57 / 228  Training Loss  0.0006821967544965446\n","Epoch  7 Batch  58 / 228  Training Loss  0.00045085279271006584\n","Epoch  7 Batch  59 / 228  Training Loss  0.0006332154734991491\n","Epoch  7 Batch  60 / 228  Training Loss  0.00075284403283149\n","Epoch  7 Batch  61 / 228  Training Loss  0.000835935294162482\n","Epoch  7 Batch  62 / 228  Training Loss  0.00033009995240718126\n","Epoch  7 Batch  63 / 228  Training Loss  0.00039448635652661324\n","Epoch  7 Batch  64 / 228  Training Loss  0.0008585730683989823\n","Epoch  7 Batch  65 / 228  Training Loss  0.0009132223203778267\n","Epoch  7 Batch  66 / 228  Training Loss  0.0006580854533240199\n","Epoch  7 Batch  67 / 228  Training Loss  0.0045488374307751656\n","Epoch  7 Batch  68 / 228  Training Loss  0.0009860941208899021\n","Epoch  7 Batch  69 / 228  Training Loss  0.0005224045016802847\n","Epoch  7 Batch  70 / 228  Training Loss  0.0013685593148693442\n","Epoch  7 Batch  71 / 228  Training Loss  0.0006456064293161035\n","Epoch  7 Batch  72 / 228  Training Loss  0.0007002140628173947\n","Epoch  7 Batch  73 / 228  Training Loss  0.00047601357800886035\n","Epoch  7 Batch  74 / 228  Training Loss  0.0004961785161867738\n","Epoch  7 Batch  75 / 228  Training Loss  0.0004349027294665575\n","Epoch  7 Batch  76 / 228  Training Loss  0.0003549758403096348\n","Epoch  7 Batch  77 / 228  Training Loss  0.0002914060023613274\n","Epoch  7 Batch  78 / 228  Training Loss  0.000494783278554678\n","Epoch  7 Batch  79 / 228  Training Loss  0.00041904114186763763\n","Epoch  7 Batch  80 / 228  Training Loss  0.000493385479785502\n","Epoch  7 Batch  81 / 228  Training Loss  0.0005547156906686723\n","Epoch  7 Batch  82 / 228  Training Loss  0.0005397131317295134\n","Epoch  7 Batch  83 / 228  Training Loss  0.0003004726895596832\n","Epoch  7 Batch  84 / 228  Training Loss  0.0006652326555922627\n","Epoch  7 Batch  85 / 228  Training Loss  0.0005928083555772901\n","Epoch  7 Batch  86 / 228  Training Loss  0.0014541835989803076\n","Epoch  7 Batch  87 / 228  Training Loss  0.01471190620213747\n","Epoch  7 Batch  88 / 228  Training Loss  0.0008812819723971188\n","Epoch  7 Batch  89 / 228  Training Loss  0.00046740853576920927\n","Epoch  7 Batch  90 / 228  Training Loss  0.00045456859515979886\n","Epoch  7 Batch  91 / 228  Training Loss  0.0007435765583068132\n","Epoch  7 Batch  92 / 228  Training Loss  0.0003622808144427836\n","Epoch  7 Batch  93 / 228  Training Loss  0.0002855420170817524\n","Epoch  7 Batch  94 / 228  Training Loss  0.0006784262368455529\n","Epoch  7 Batch  95 / 228  Training Loss  0.0004080943763256073\n","Epoch  7 Batch  96 / 228  Training Loss  0.00027433768264018\n","Epoch  7 Batch  97 / 228  Training Loss  0.0005853333277627826\n","Epoch  7 Batch  98 / 228  Training Loss  0.0004617221711669117\n","Epoch  7 Batch  99 / 228  Training Loss  0.0007664580480195582\n","Epoch  7 Batch  100 / 228  Training Loss  0.0004013872821815312\n","Epoch  7 Batch  101 / 228  Training Loss  0.0007044899393804371\n","Epoch  7 Batch  102 / 228  Training Loss  0.00044363801134750247\n","Epoch  7 Batch  103 / 228  Training Loss  0.0005582290468737483\n","Epoch  7 Batch  104 / 228  Training Loss  0.0005410355515778065\n","Epoch  7 Batch  105 / 228  Training Loss  0.00046161646605469286\n","Epoch  7 Batch  106 / 228  Training Loss  0.0006991829141043127\n","Epoch  7 Batch  107 / 228  Training Loss  0.001779688522219658\n","Epoch  7 Batch  108 / 228  Training Loss  0.0006330854957923293\n","Epoch  7 Batch  109 / 228  Training Loss  0.00039750587893649936\n","Epoch  7 Batch  110 / 228  Training Loss  0.00048316107131540775\n","Epoch  7 Batch  111 / 228  Training Loss  0.0007201036205515265\n","Epoch  7 Batch  112 / 228  Training Loss  0.0006893255631439388\n","Epoch  7 Batch  113 / 228  Training Loss  0.0028285507578402758\n","Epoch  7 Batch  114 / 228  Training Loss  0.0003506339271552861\n","Epoch  7 Batch  115 / 228  Training Loss  0.003031180938705802\n","Epoch  7 Batch  116 / 228  Training Loss  0.001323722186498344\n","Epoch  7 Batch  117 / 228  Training Loss  0.0007910305866971612\n","Epoch  7 Batch  118 / 228  Training Loss  0.0032775807194411755\n","Epoch  7 Batch  119 / 228  Training Loss  0.0004274997045286\n","Epoch  7 Batch  120 / 228  Training Loss  0.0005858557997271419\n","Epoch  7 Batch  121 / 228  Training Loss  0.0008370168507099152\n","Epoch  7 Batch  122 / 228  Training Loss  0.0004669325426220894\n","Epoch  7 Batch  123 / 228  Training Loss  0.0011398146161809564\n","Epoch  7 Batch  124 / 228  Training Loss  0.0005653110565617681\n","Epoch  7 Batch  125 / 228  Training Loss  0.0008883355185389519\n","Epoch  7 Batch  126 / 228  Training Loss  0.0006450803484767675\n","Epoch  7 Batch  127 / 228  Training Loss  0.004601942375302315\n","Epoch  7 Batch  128 / 228  Training Loss  0.0034794011153280735\n","Epoch  7 Batch  129 / 228  Training Loss  0.0012097593862563372\n","Epoch  7 Batch  130 / 228  Training Loss  0.0034151137806475163\n","Epoch  7 Batch  131 / 228  Training Loss  0.0007154206978157163\n","Epoch  7 Batch  132 / 228  Training Loss  0.0029417334590107203\n","Epoch  7 Batch  133 / 228  Training Loss  0.0026406575925648212\n","Epoch  7 Batch  134 / 228  Training Loss  0.006525165401399136\n","Epoch  7 Batch  135 / 228  Training Loss  0.0007141259266063571\n","Epoch  7 Batch  136 / 228  Training Loss  0.00038436593604274094\n","Epoch  7 Batch  137 / 228  Training Loss  0.0016962792724370956\n","Epoch  7 Batch  138 / 228  Training Loss  0.001633649691939354\n","Epoch  7 Batch  139 / 228  Training Loss  0.0016886144876480103\n","Epoch  7 Batch  140 / 228  Training Loss  0.011736463755369186\n","Epoch  7 Batch  141 / 228  Training Loss  0.006756046321243048\n","Epoch  7 Batch  142 / 228  Training Loss  0.0005241837934590876\n","Epoch  7 Batch  143 / 228  Training Loss  0.0009210485150106251\n","Epoch  7 Batch  144 / 228  Training Loss  0.001902625197544694\n","Epoch  7 Batch  145 / 228  Training Loss  0.01368801761418581\n","Epoch  7 Batch  146 / 228  Training Loss  0.0003761338593903929\n","Epoch  7 Batch  147 / 228  Training Loss  0.001390490448102355\n","Epoch  7 Batch  148 / 228  Training Loss  0.0005905091529712081\n","Epoch  7 Batch  149 / 228  Training Loss  0.0007851559785194695\n","Epoch  7 Batch  150 / 228  Training Loss  0.000863495864905417\n","Epoch  7 Batch  151 / 228  Training Loss  0.010989632457494736\n","Epoch  7 Batch  152 / 228  Training Loss  0.0006547681405209005\n","Epoch  7 Batch  153 / 228  Training Loss  0.009284207597374916\n","Epoch  7 Batch  154 / 228  Training Loss  0.0035425678361207247\n","Epoch  7 Batch  155 / 228  Training Loss  0.0005837758653797209\n","Epoch  7 Batch  156 / 228  Training Loss  0.0008008869481272995\n","Epoch  7 Batch  157 / 228  Training Loss  0.0005111340433359146\n","Epoch  7 Batch  158 / 228  Training Loss  0.0007770718657411635\n","Epoch  7 Batch  159 / 228  Training Loss  0.002551967976614833\n","Epoch  7 Batch  160 / 228  Training Loss  0.0006397715187631547\n","Epoch  7 Batch  161 / 228  Training Loss  0.0005182013264857233\n","Epoch  7 Batch  162 / 228  Training Loss  0.0004792422987520695\n","Epoch  7 Batch  163 / 228  Training Loss  0.005770143587142229\n","Epoch  7 Batch  164 / 228  Training Loss  0.001802629791200161\n","Epoch  7 Batch  165 / 228  Training Loss  0.0008388595888391137\n","Epoch  7 Batch  166 / 228  Training Loss  0.0014752043643966317\n","Epoch  7 Batch  167 / 228  Training Loss  0.0007777686696499586\n","Epoch  7 Batch  168 / 228  Training Loss  0.0007835794240236282\n","Epoch  7 Batch  169 / 228  Training Loss  0.0009761097026057541\n","Epoch  7 Batch  170 / 228  Training Loss  0.0005707803065888584\n","Epoch  7 Batch  171 / 228  Training Loss  0.0009410581551492214\n","Epoch  7 Batch  172 / 228  Training Loss  0.0005353466258384287\n","Epoch  7 Batch  173 / 228  Training Loss  0.0003307340375613421\n","Epoch  7 Batch  174 / 228  Training Loss  0.00038070196751505136\n","Epoch  7 Batch  175 / 228  Training Loss  0.0004388207453303039\n","Epoch  7 Batch  176 / 228  Training Loss  0.001078248955309391\n","Epoch  7 Batch  177 / 228  Training Loss  0.0008925332804210484\n","Epoch  7 Batch  178 / 228  Training Loss  0.0010112107265740633\n","Epoch  7 Batch  179 / 228  Training Loss  0.0008429797599092126\n","Epoch  7 Batch  180 / 228  Training Loss  0.0006029431824572384\n","Epoch  7 Batch  181 / 228  Training Loss  0.0010999208316206932\n","Epoch  7 Batch  182 / 228  Training Loss  0.004902707412838936\n","Epoch  7 Batch  183 / 228  Training Loss  0.003273542271926999\n","Epoch  7 Batch  184 / 228  Training Loss  0.0005875529604963958\n","Epoch  7 Batch  185 / 228  Training Loss  0.00041683996096253395\n","Epoch  7 Batch  186 / 228  Training Loss  0.0012868039775639772\n","Epoch  7 Batch  187 / 228  Training Loss  0.0003848894848488271\n","Epoch  7 Batch  188 / 228  Training Loss  0.0007228789618238807\n","Epoch  7 Batch  189 / 228  Training Loss  0.0005765889654867351\n","Epoch  7 Batch  190 / 228  Training Loss  0.0008069351315498352\n","Epoch  7 Batch  191 / 228  Training Loss  0.00039731519063934684\n","Epoch  7 Batch  192 / 228  Training Loss  0.000495567568577826\n","Epoch  7 Batch  193 / 228  Training Loss  0.0004927160916849971\n","Epoch  7 Batch  194 / 228  Training Loss  0.004576302599161863\n","Epoch  7 Batch  195 / 228  Training Loss  0.004725855775177479\n","Epoch  7 Batch  196 / 228  Training Loss  0.001432952587492764\n","Epoch  7 Batch  197 / 228  Training Loss  0.0005158237181603909\n","Epoch  7 Batch  198 / 228  Training Loss  0.0017127401661127806\n","Epoch  7 Batch  199 / 228  Training Loss  0.0007120398222468793\n","Epoch  7 Batch  200 / 228  Training Loss  0.0008819185313768685\n","Epoch  7 Batch  201 / 228  Training Loss  0.0006678501958958805\n","Epoch  7 Batch  202 / 228  Training Loss  0.0006026054616086185\n","Epoch  7 Batch  203 / 228  Training Loss  0.0011471414472907782\n","Epoch  7 Batch  204 / 228  Training Loss  0.0006742826662957668\n","Epoch  7 Batch  205 / 228  Training Loss  0.0005337743205018342\n","Epoch  7 Batch  206 / 228  Training Loss  0.0003700220549944788\n","Epoch  7 Batch  207 / 228  Training Loss  0.0005308840190991759\n","Epoch  7 Batch  208 / 228  Training Loss  0.0007275192765519023\n","Epoch  7 Batch  209 / 228  Training Loss  0.0002884980058297515\n","Epoch  7 Batch  210 / 228  Training Loss  0.0015143121127039194\n","Epoch  7 Batch  211 / 228  Training Loss  0.0008239127928391099\n","Epoch  7 Batch  212 / 228  Training Loss  0.0040180557407438755\n","Epoch  7 Batch  213 / 228  Training Loss  0.00789328757673502\n","Epoch  7 Batch  214 / 228  Training Loss  0.03211890906095505\n","Epoch  7 Batch  215 / 228  Training Loss  0.02750294841825962\n","Epoch  7 Batch  216 / 228  Training Loss  0.02461150474846363\n","Epoch  7 Batch  217 / 228  Training Loss  0.02609211578965187\n","Epoch  7 Batch  218 / 228  Training Loss  0.019436296075582504\n","Epoch  7 Batch  219 / 228  Training Loss  0.010281512513756752\n","Epoch  7 Batch  220 / 228  Training Loss  0.004087886773049831\n","Epoch  7 Batch  221 / 228  Training Loss  0.007418626453727484\n","Epoch  7 Batch  222 / 228  Training Loss  0.0024871109053492546\n","Epoch  7 Batch  223 / 228  Training Loss  0.008401243947446346\n","Epoch  7 Batch  224 / 228  Training Loss  0.005359966307878494\n","Epoch  7 Batch  225 / 228  Training Loss  0.0025290469639003277\n","Epoch  7 Batch  226 / 228  Training Loss  0.0020954932551831007\n","Epoch  7 Batch  227 / 228  Training Loss  0.020144619047641754\n","   8    |    -    |   0.002248   | 94.397866\n","----------------------------------------------------------------------\n","Running epoch: 8\n","Epoch  8 Batch  0 / 228  Training Loss  0.024454262107610703\n","Epoch  8 Batch  1 / 228  Training Loss  0.011059373617172241\n","Epoch  8 Batch  2 / 228  Training Loss  0.0028293656650930643\n","Epoch  8 Batch  3 / 228  Training Loss  0.0017583221197128296\n","Epoch  8 Batch  4 / 228  Training Loss  0.0015068785287439823\n","Epoch  8 Batch  5 / 228  Training Loss  0.0018436923855915666\n","Epoch  8 Batch  6 / 228  Training Loss  0.0005229795933701098\n","Epoch  8 Batch  7 / 228  Training Loss  0.0007874681614339352\n","Epoch  8 Batch  8 / 228  Training Loss  0.0014586582547053695\n","Epoch  8 Batch  9 / 228  Training Loss  0.0005016353679820895\n","Epoch  8 Batch  10 / 228  Training Loss  0.0005881133256480098\n","Epoch  8 Batch  11 / 228  Training Loss  0.0012422576546669006\n","Epoch  8 Batch  12 / 228  Training Loss  0.0055123805068433285\n","Epoch  8 Batch  13 / 228  Training Loss  0.001145779388025403\n","Epoch  8 Batch  14 / 228  Training Loss  0.0031316247768700123\n","Epoch  8 Batch  15 / 228  Training Loss  0.009672528132796288\n","Epoch  8 Batch  16 / 228  Training Loss  0.0005349235725589097\n","Epoch  8 Batch  17 / 228  Training Loss  0.00068084237864241\n","Epoch  8 Batch  18 / 228  Training Loss  0.0006974433781579137\n","Epoch  8 Batch  19 / 228  Training Loss  0.0008126517059281468\n","Epoch  8 Batch  20 / 228  Training Loss  0.0007705738535150886\n","Epoch  8 Batch  21 / 228  Training Loss  0.0020571514032781124\n","Epoch  8 Batch  22 / 228  Training Loss  0.0015464512398466468\n","Epoch  8 Batch  23 / 228  Training Loss  0.0011897662188857794\n","Epoch  8 Batch  24 / 228  Training Loss  0.0019407626241445541\n","Epoch  8 Batch  25 / 228  Training Loss  0.0009987447410821915\n","Epoch  8 Batch  26 / 228  Training Loss  0.0009430452482774854\n","Epoch  8 Batch  27 / 228  Training Loss  0.0030170995742082596\n","Epoch  8 Batch  28 / 228  Training Loss  0.003485207911580801\n","Epoch  8 Batch  29 / 228  Training Loss  0.002062791958451271\n","Epoch  8 Batch  30 / 228  Training Loss  0.0005344684468582273\n","Epoch  8 Batch  31 / 228  Training Loss  0.0011314561124891043\n","Epoch  8 Batch  32 / 228  Training Loss  0.0018909508362412453\n","Epoch  8 Batch  33 / 228  Training Loss  0.0006776362424716353\n","Epoch  8 Batch  34 / 228  Training Loss  0.001319277798756957\n","Epoch  8 Batch  35 / 228  Training Loss  0.000939084799028933\n","Epoch  8 Batch  36 / 228  Training Loss  0.0006820557173341513\n","Epoch  8 Batch  37 / 228  Training Loss  0.0003779555845540017\n","Epoch  8 Batch  38 / 228  Training Loss  0.0002888763847295195\n","Epoch  8 Batch  39 / 228  Training Loss  0.0003490050439722836\n","Epoch  8 Batch  40 / 228  Training Loss  0.006365916226059198\n","Epoch  8 Batch  41 / 228  Training Loss  0.0014391331933438778\n","Epoch  8 Batch  42 / 228  Training Loss  0.0008217423455789685\n","Epoch  8 Batch  43 / 228  Training Loss  0.003722392488270998\n","Epoch  8 Batch  44 / 228  Training Loss  0.006063042674213648\n","Epoch  8 Batch  45 / 228  Training Loss  0.0016035984735935926\n","Epoch  8 Batch  46 / 228  Training Loss  0.00041781930485740304\n","Epoch  8 Batch  47 / 228  Training Loss  0.00681629404425621\n","Epoch  8 Batch  48 / 228  Training Loss  0.0008884003618732095\n","Epoch  8 Batch  49 / 228  Training Loss  0.0005672789993695915\n","Epoch  8 Batch  50 / 228  Training Loss  0.0009250291623175144\n","Epoch  8 Batch  51 / 228  Training Loss  0.0006339960382319987\n","Epoch  8 Batch  52 / 228  Training Loss  0.000514955201651901\n","Epoch  8 Batch  53 / 228  Training Loss  0.005555733107030392\n","Epoch  8 Batch  54 / 228  Training Loss  0.0010950688738375902\n","Epoch  8 Batch  55 / 228  Training Loss  0.004590590018779039\n","Epoch  8 Batch  56 / 228  Training Loss  0.006512806750833988\n","Epoch  8 Batch  57 / 228  Training Loss  0.0008646203204989433\n","Epoch  8 Batch  58 / 228  Training Loss  0.0013276664540171623\n","Epoch  8 Batch  59 / 228  Training Loss  0.001039483118802309\n","Epoch  8 Batch  60 / 228  Training Loss  0.000551139353774488\n","Epoch  8 Batch  61 / 228  Training Loss  0.004447862971574068\n","Epoch  8 Batch  62 / 228  Training Loss  0.002188796875998378\n","Epoch  8 Batch  63 / 228  Training Loss  0.0035067652352154255\n","Epoch  8 Batch  64 / 228  Training Loss  0.0024237935431301594\n","Epoch  8 Batch  65 / 228  Training Loss  0.003190299030393362\n","Epoch  8 Batch  66 / 228  Training Loss  0.003612109227105975\n","Epoch  8 Batch  67 / 228  Training Loss  0.0007532435702160001\n","Epoch  8 Batch  68 / 228  Training Loss  0.0005816181655973196\n","Epoch  8 Batch  69 / 228  Training Loss  0.0006435493705794215\n","Epoch  8 Batch  70 / 228  Training Loss  0.0011451022000983357\n","Epoch  8 Batch  71 / 228  Training Loss  0.002849417272955179\n","Epoch  8 Batch  72 / 228  Training Loss  0.0007090583676472306\n","Epoch  8 Batch  73 / 228  Training Loss  0.00345209869556129\n","Epoch  8 Batch  74 / 228  Training Loss  0.001163964276202023\n","Epoch  8 Batch  75 / 228  Training Loss  0.00030208431417122483\n","Epoch  8 Batch  76 / 228  Training Loss  0.00038790033431723714\n","Epoch  8 Batch  77 / 228  Training Loss  0.0014007764402776957\n","Epoch  8 Batch  78 / 228  Training Loss  0.0008169083739630878\n","Epoch  8 Batch  79 / 228  Training Loss  0.0006717054056935012\n","Epoch  8 Batch  80 / 228  Training Loss  0.0003712557372637093\n","Epoch  8 Batch  81 / 228  Training Loss  0.00031171724549494684\n","Epoch  8 Batch  82 / 228  Training Loss  0.002009127987548709\n","Epoch  8 Batch  83 / 228  Training Loss  0.00041856669122353196\n","Epoch  8 Batch  84 / 228  Training Loss  0.00031946011586114764\n","Epoch  8 Batch  85 / 228  Training Loss  0.0015511906240135431\n","Epoch  8 Batch  86 / 228  Training Loss  0.0006578422035090625\n","Epoch  8 Batch  87 / 228  Training Loss  0.00042572966776788235\n","Epoch  8 Batch  88 / 228  Training Loss  0.0004895446472801268\n","Epoch  8 Batch  89 / 228  Training Loss  0.0004047011607326567\n","Epoch  8 Batch  90 / 228  Training Loss  0.0002868222654797137\n","Epoch  8 Batch  91 / 228  Training Loss  0.0004839980392716825\n","Epoch  8 Batch  92 / 228  Training Loss  0.00026138804969377816\n","Epoch  8 Batch  93 / 228  Training Loss  0.000907036941498518\n","Epoch  8 Batch  94 / 228  Training Loss  0.0013416879810392857\n","Epoch  8 Batch  95 / 228  Training Loss  0.00035319168819114566\n","Epoch  8 Batch  96 / 228  Training Loss  0.0002774419845081866\n","Epoch  8 Batch  97 / 228  Training Loss  0.0007933505694381893\n","Epoch  8 Batch  98 / 228  Training Loss  0.0004192836640868336\n","Epoch  8 Batch  99 / 228  Training Loss  0.0021684293169528246\n","Epoch  8 Batch  100 / 228  Training Loss  0.0008338264306075871\n","Epoch  8 Batch  101 / 228  Training Loss  0.001230842201039195\n","Epoch  8 Batch  102 / 228  Training Loss  0.0004325437475927174\n","Epoch  8 Batch  103 / 228  Training Loss  0.0007101925439201295\n","Epoch  8 Batch  104 / 228  Training Loss  0.0011848497670143843\n","Epoch  8 Batch  105 / 228  Training Loss  0.001745087094604969\n","Epoch  8 Batch  106 / 228  Training Loss  0.0002713334688451141\n","Epoch  8 Batch  107 / 228  Training Loss  0.0003195752506144345\n","Epoch  8 Batch  108 / 228  Training Loss  0.0006598748732358217\n","Epoch  8 Batch  109 / 228  Training Loss  0.0003954918938688934\n","Epoch  8 Batch  110 / 228  Training Loss  0.001648784033022821\n","Epoch  8 Batch  111 / 228  Training Loss  0.001002439996227622\n","Epoch  8 Batch  112 / 228  Training Loss  0.0005071485065855086\n","Epoch  8 Batch  113 / 228  Training Loss  0.0003845749597530812\n","Epoch  8 Batch  114 / 228  Training Loss  0.000384868384571746\n","Epoch  8 Batch  115 / 228  Training Loss  0.00036680660559795797\n","Epoch  8 Batch  116 / 228  Training Loss  0.0004321348387748003\n","Epoch  8 Batch  117 / 228  Training Loss  0.00029008457204326987\n","Epoch  8 Batch  118 / 228  Training Loss  0.00025995951727963984\n","Epoch  8 Batch  119 / 228  Training Loss  0.0003768957394640893\n","Epoch  8 Batch  120 / 228  Training Loss  0.0007576617645099759\n","Epoch  8 Batch  121 / 228  Training Loss  0.00047396417357958853\n","Epoch  8 Batch  122 / 228  Training Loss  0.0005885021528229117\n","Epoch  8 Batch  123 / 228  Training Loss  0.0007220368715934455\n","Epoch  8 Batch  124 / 228  Training Loss  0.00041621370473876595\n","Epoch  8 Batch  125 / 228  Training Loss  0.001100311172194779\n","Epoch  8 Batch  126 / 228  Training Loss  0.0004328167997300625\n","Epoch  8 Batch  127 / 228  Training Loss  0.0010682956781238317\n","Epoch  8 Batch  128 / 228  Training Loss  0.0005034689093008637\n","Epoch  8 Batch  129 / 228  Training Loss  0.00035831640707328916\n","Epoch  8 Batch  130 / 228  Training Loss  0.0008400111692026258\n","Epoch  8 Batch  131 / 228  Training Loss  0.008087022230029106\n","Epoch  8 Batch  132 / 228  Training Loss  0.0008875540224835277\n","Epoch  8 Batch  133 / 228  Training Loss  0.0010209623724222183\n","Epoch  8 Batch  134 / 228  Training Loss  0.0004726468469016254\n","Epoch  8 Batch  135 / 228  Training Loss  0.0003257236094214022\n","Epoch  8 Batch  136 / 228  Training Loss  0.0005029112799093127\n","Epoch  8 Batch  137 / 228  Training Loss  0.00046047434443607926\n","Epoch  8 Batch  138 / 228  Training Loss  0.0036740682553499937\n","Epoch  8 Batch  139 / 228  Training Loss  0.00037840422010049224\n","Epoch  8 Batch  140 / 228  Training Loss  0.0010555821936577559\n","Epoch  8 Batch  141 / 228  Training Loss  0.0003696572093758732\n","Epoch  8 Batch  142 / 228  Training Loss  0.00035613946965895593\n","Epoch  8 Batch  143 / 228  Training Loss  0.0008947281166911125\n","Epoch  8 Batch  144 / 228  Training Loss  0.00029008620185777545\n","Epoch  8 Batch  145 / 228  Training Loss  0.0002684412174858153\n","Epoch  8 Batch  146 / 228  Training Loss  0.000365114159649238\n","Epoch  8 Batch  147 / 228  Training Loss  0.00037197681376710534\n","Epoch  8 Batch  148 / 228  Training Loss  0.00026452410384081304\n","Epoch  8 Batch  149 / 228  Training Loss  0.0003478401340544224\n","Epoch  8 Batch  150 / 228  Training Loss  0.00022218543745111674\n","Epoch  8 Batch  151 / 228  Training Loss  0.0004046890535391867\n","Epoch  8 Batch  152 / 228  Training Loss  0.0002606903435662389\n","Epoch  8 Batch  153 / 228  Training Loss  0.0009093974949792027\n","Epoch  8 Batch  154 / 228  Training Loss  0.0001987241266760975\n","Epoch  8 Batch  155 / 228  Training Loss  0.0004938726196996868\n","Epoch  8 Batch  156 / 228  Training Loss  0.0018472097581252456\n","Epoch  8 Batch  157 / 228  Training Loss  0.0005735188024118543\n","Epoch  8 Batch  158 / 228  Training Loss  0.0004119824734516442\n","Epoch  8 Batch  159 / 228  Training Loss  0.00028820967418141663\n","Epoch  8 Batch  160 / 228  Training Loss  0.00023789782426320016\n","Epoch  8 Batch  161 / 228  Training Loss  0.00030490889912471175\n","Epoch  8 Batch  162 / 228  Training Loss  0.00043480092426761985\n","Epoch  8 Batch  163 / 228  Training Loss  0.0024328497238457203\n","Epoch  8 Batch  164 / 228  Training Loss  0.0007257939432747662\n","Epoch  8 Batch  165 / 228  Training Loss  0.0008590799989178777\n","Epoch  8 Batch  166 / 228  Training Loss  0.00045407580910250545\n","Epoch  8 Batch  167 / 228  Training Loss  0.00016984206740744412\n","Epoch  8 Batch  168 / 228  Training Loss  0.000291370612103492\n","Epoch  8 Batch  169 / 228  Training Loss  0.0005473672645166516\n","Epoch  8 Batch  170 / 228  Training Loss  0.0015501771122217178\n","Epoch  8 Batch  171 / 228  Training Loss  0.0005284407525323331\n","Epoch  8 Batch  172 / 228  Training Loss  0.0004989441367797554\n","Epoch  8 Batch  173 / 228  Training Loss  0.0021085937041789293\n","Epoch  8 Batch  174 / 228  Training Loss  0.00024998580920509994\n","Epoch  8 Batch  175 / 228  Training Loss  0.0011435092892497778\n","Epoch  8 Batch  176 / 228  Training Loss  0.002127581275999546\n","Epoch  8 Batch  177 / 228  Training Loss  0.001901403651572764\n","Epoch  8 Batch  178 / 228  Training Loss  0.0004645279550459236\n","Epoch  8 Batch  179 / 228  Training Loss  0.0007004699436947703\n","Epoch  8 Batch  180 / 228  Training Loss  0.00792239885777235\n","Epoch  8 Batch  181 / 228  Training Loss  0.0065267132595181465\n","Epoch  8 Batch  182 / 228  Training Loss  0.0033473861403763294\n","Epoch  8 Batch  183 / 228  Training Loss  0.00047163706040009856\n","Epoch  8 Batch  184 / 228  Training Loss  0.005459095351397991\n","Epoch  8 Batch  185 / 228  Training Loss  0.018210146576166153\n","Epoch  8 Batch  186 / 228  Training Loss  0.0005600774893537164\n","Epoch  8 Batch  187 / 228  Training Loss  0.00046741432743147016\n","Epoch  8 Batch  188 / 228  Training Loss  0.0005369008285924792\n","Epoch  8 Batch  189 / 228  Training Loss  0.0003397891123313457\n","Epoch  8 Batch  190 / 228  Training Loss  0.010911377146840096\n","Epoch  8 Batch  191 / 228  Training Loss  0.0027569078374654055\n","Epoch  8 Batch  192 / 228  Training Loss  0.0005978636327199638\n","Epoch  8 Batch  193 / 228  Training Loss  0.0034767836332321167\n","Epoch  8 Batch  194 / 228  Training Loss  0.0013101566582918167\n","Epoch  8 Batch  195 / 228  Training Loss  0.0018146991496905684\n","Epoch  8 Batch  196 / 228  Training Loss  0.0060945963487029076\n","Epoch  8 Batch  197 / 228  Training Loss  0.002225654199719429\n","Epoch  8 Batch  198 / 228  Training Loss  0.00045083757140673697\n","Epoch  8 Batch  199 / 228  Training Loss  0.005414637271314859\n","Epoch  8 Batch  200 / 228  Training Loss  0.0019067877437919378\n","Epoch  8 Batch  201 / 228  Training Loss  0.003092528786510229\n","Epoch  8 Batch  202 / 228  Training Loss  0.0032844548113644123\n","Epoch  8 Batch  203 / 228  Training Loss  0.0020285043865442276\n","Epoch  8 Batch  204 / 228  Training Loss  0.0017016965430229902\n","Epoch  8 Batch  205 / 228  Training Loss  0.0006734560010954738\n","Epoch  8 Batch  206 / 228  Training Loss  0.0026378901675343513\n","Epoch  8 Batch  207 / 228  Training Loss  0.0047433567233383656\n","Epoch  8 Batch  208 / 228  Training Loss  0.00044688163325190544\n","Epoch  8 Batch  209 / 228  Training Loss  0.000468942045699805\n","Epoch  8 Batch  210 / 228  Training Loss  0.00048070872435346246\n","Epoch  8 Batch  211 / 228  Training Loss  0.0008752791327424347\n","Epoch  8 Batch  212 / 228  Training Loss  0.0017942206468433142\n","Epoch  8 Batch  213 / 228  Training Loss  0.003516237484291196\n","Epoch  8 Batch  214 / 228  Training Loss  0.00046390070929192007\n","Epoch  8 Batch  215 / 228  Training Loss  0.009993664920330048\n","Epoch  8 Batch  216 / 228  Training Loss  0.01728571020066738\n","Epoch  8 Batch  217 / 228  Training Loss  0.03537257760763168\n","Epoch  8 Batch  218 / 228  Training Loss  0.00946732610464096\n","Epoch  8 Batch  219 / 228  Training Loss  0.004184551537036896\n","Epoch  8 Batch  220 / 228  Training Loss  0.007076481822878122\n","Epoch  8 Batch  221 / 228  Training Loss  0.0012046747142449021\n","Epoch  8 Batch  222 / 228  Training Loss  0.0042177774012088776\n","Epoch  8 Batch  223 / 228  Training Loss  0.0008200666634365916\n","Epoch  8 Batch  224 / 228  Training Loss  0.0039941999129951\n","Epoch  8 Batch  225 / 228  Training Loss  0.017116885632276535\n","Epoch  8 Batch  226 / 228  Training Loss  0.005309759639203548\n","Epoch  8 Batch  227 / 228  Training Loss  0.0006572689162567258\n","   9    |    -    |   0.002131   | 95.655488\n","----------------------------------------------------------------------\n","Running epoch: 9\n","Epoch  9 Batch  0 / 228  Training Loss  0.0020051095634698868\n","Epoch  9 Batch  1 / 228  Training Loss  0.002721034921705723\n","Epoch  9 Batch  2 / 228  Training Loss  0.0015284379478543997\n","Epoch  9 Batch  3 / 228  Training Loss  0.001325579360127449\n","Epoch  9 Batch  4 / 228  Training Loss  0.001036207308061421\n","Epoch  9 Batch  5 / 228  Training Loss  0.005020040087401867\n","Epoch  9 Batch  6 / 228  Training Loss  0.0015288344584405422\n","Epoch  9 Batch  7 / 228  Training Loss  0.0009539785096421838\n","Epoch  9 Batch  8 / 228  Training Loss  0.0020859669893980026\n","Epoch  9 Batch  9 / 228  Training Loss  0.00339834438636899\n","Epoch  9 Batch  10 / 228  Training Loss  0.0005403159884735942\n","Epoch  9 Batch  11 / 228  Training Loss  0.010221446864306927\n","Epoch  9 Batch  12 / 228  Training Loss  0.0004485601675696671\n","Epoch  9 Batch  13 / 228  Training Loss  0.0010270786006003618\n","Epoch  9 Batch  14 / 228  Training Loss  0.0010788594372570515\n","Epoch  9 Batch  15 / 228  Training Loss  0.0011675686109811068\n","Epoch  9 Batch  16 / 228  Training Loss  0.0005743855144828558\n","Epoch  9 Batch  17 / 228  Training Loss  0.0005108253099024296\n","Epoch  9 Batch  18 / 228  Training Loss  0.000316642748657614\n","Epoch  9 Batch  19 / 228  Training Loss  0.0008411750895902514\n","Epoch  9 Batch  20 / 228  Training Loss  0.0004002764471806586\n","Epoch  9 Batch  21 / 228  Training Loss  0.0004297643026802689\n","Epoch  9 Batch  22 / 228  Training Loss  0.0016808032523840666\n","Epoch  9 Batch  23 / 228  Training Loss  0.0026618200354278088\n","Epoch  9 Batch  24 / 228  Training Loss  0.0006519909366033971\n","Epoch  9 Batch  25 / 228  Training Loss  0.0016130490694195032\n","Epoch  9 Batch  26 / 228  Training Loss  0.0022088922560214996\n","Epoch  9 Batch  27 / 228  Training Loss  0.0008268111268989742\n","Epoch  9 Batch  28 / 228  Training Loss  0.001434785546734929\n","Epoch  9 Batch  29 / 228  Training Loss  0.0014763879589736462\n","Epoch  9 Batch  30 / 228  Training Loss  0.0012099200394004583\n","Epoch  9 Batch  31 / 228  Training Loss  0.0004336903803050518\n","Epoch  9 Batch  32 / 228  Training Loss  0.0005084380391053855\n","Epoch  9 Batch  33 / 228  Training Loss  0.0004483303055167198\n","Epoch  9 Batch  34 / 228  Training Loss  0.0006429407512769103\n","Epoch  9 Batch  35 / 228  Training Loss  0.0006010715733282268\n","Epoch  9 Batch  36 / 228  Training Loss  0.00028216137434355915\n","Epoch  9 Batch  37 / 228  Training Loss  0.0032557875383645296\n","Epoch  9 Batch  38 / 228  Training Loss  0.0012412525247782469\n","Epoch  9 Batch  39 / 228  Training Loss  0.00041738449363037944\n","Epoch  9 Batch  40 / 228  Training Loss  0.0004396548611111939\n","Epoch  9 Batch  41 / 228  Training Loss  0.004080860875546932\n","Epoch  9 Batch  42 / 228  Training Loss  0.0010434146970510483\n","Epoch  9 Batch  43 / 228  Training Loss  0.0002516197564546019\n","Epoch  9 Batch  44 / 228  Training Loss  0.00046379221021197736\n","Epoch  9 Batch  45 / 228  Training Loss  0.0003857390838675201\n","Epoch  9 Batch  46 / 228  Training Loss  0.0032326877117156982\n","Epoch  9 Batch  47 / 228  Training Loss  0.0005528373294509947\n","Epoch  9 Batch  48 / 228  Training Loss  0.0005245751817710698\n","Epoch  9 Batch  49 / 228  Training Loss  0.0005096618551760912\n","Epoch  9 Batch  50 / 228  Training Loss  0.0005724175716750324\n","Epoch  9 Batch  51 / 228  Training Loss  0.0005402171518653631\n","Epoch  9 Batch  52 / 228  Training Loss  0.00031981844222173095\n","Epoch  9 Batch  53 / 228  Training Loss  0.0018415838712826371\n","Epoch  9 Batch  54 / 228  Training Loss  0.0007931847358122468\n","Epoch  9 Batch  55 / 228  Training Loss  0.0007418479071930051\n","Epoch  9 Batch  56 / 228  Training Loss  0.00031116107129491866\n","Epoch  9 Batch  57 / 228  Training Loss  0.00039681856287643313\n","Epoch  9 Batch  58 / 228  Training Loss  0.0006105924258008599\n","Epoch  9 Batch  59 / 228  Training Loss  0.0005410866579040885\n","Epoch  9 Batch  60 / 228  Training Loss  0.00035172380739822984\n","Epoch  9 Batch  61 / 228  Training Loss  0.0014129061019048095\n","Epoch  9 Batch  62 / 228  Training Loss  0.0004361573955975473\n","Epoch  9 Batch  63 / 228  Training Loss  0.001792690483853221\n","Epoch  9 Batch  64 / 228  Training Loss  0.001277103554457426\n","Epoch  9 Batch  65 / 228  Training Loss  0.0004429440596140921\n","Epoch  9 Batch  66 / 228  Training Loss  0.001068426645360887\n","Epoch  9 Batch  67 / 228  Training Loss  0.0007347830687649548\n","Epoch  9 Batch  68 / 228  Training Loss  0.0016865745419636369\n","Epoch  9 Batch  69 / 228  Training Loss  0.00035724116605706513\n","Epoch  9 Batch  70 / 228  Training Loss  0.00027899633278138936\n","Epoch  9 Batch  71 / 228  Training Loss  0.0003706521529238671\n","Epoch  9 Batch  72 / 228  Training Loss  0.00030406902078539133\n","Epoch  9 Batch  73 / 228  Training Loss  0.0004159475793130696\n","Epoch  9 Batch  74 / 228  Training Loss  0.005474288947880268\n","Epoch  9 Batch  75 / 228  Training Loss  0.0007780586020089686\n","Epoch  9 Batch  76 / 228  Training Loss  0.0006114361458458006\n","Epoch  9 Batch  77 / 228  Training Loss  0.00047589922905899584\n","Epoch  9 Batch  78 / 228  Training Loss  0.0032591361086815596\n","Epoch  9 Batch  79 / 228  Training Loss  0.0011581709841266274\n","Epoch  9 Batch  80 / 228  Training Loss  0.000549279386177659\n","Epoch  9 Batch  81 / 228  Training Loss  0.00040437871939502656\n","Epoch  9 Batch  82 / 228  Training Loss  0.00043815613025799394\n","Epoch  9 Batch  83 / 228  Training Loss  0.0001991346653085202\n","Epoch  9 Batch  84 / 228  Training Loss  0.00034303084248676896\n","Epoch  9 Batch  85 / 228  Training Loss  0.0012704895343631506\n","Epoch  9 Batch  86 / 228  Training Loss  0.0003850966750178486\n","Epoch  9 Batch  87 / 228  Training Loss  0.0012312893522903323\n","Epoch  9 Batch  88 / 228  Training Loss  0.0002976912655867636\n","Epoch  9 Batch  89 / 228  Training Loss  0.0010624564019963145\n","Epoch  9 Batch  90 / 228  Training Loss  0.0003206465335097164\n","Epoch  9 Batch  91 / 228  Training Loss  0.0003110947727691382\n","Epoch  9 Batch  92 / 228  Training Loss  0.0011059653479605913\n","Epoch  9 Batch  93 / 228  Training Loss  0.0013721100986003876\n","Epoch  9 Batch  94 / 228  Training Loss  0.0005623020697385073\n","Epoch  9 Batch  95 / 228  Training Loss  0.004057057201862335\n","Epoch  9 Batch  96 / 228  Training Loss  0.0117383673787117\n","Epoch  9 Batch  97 / 228  Training Loss  0.021798886358737946\n","Epoch  9 Batch  98 / 228  Training Loss  0.018445977941155434\n","Epoch  9 Batch  99 / 228  Training Loss  0.0029151816852390766\n","Epoch  9 Batch  100 / 228  Training Loss  0.006180457305163145\n","Epoch  9 Batch  101 / 228  Training Loss  0.010711709037423134\n","Epoch  9 Batch  102 / 228  Training Loss  0.0023718189913779497\n","Epoch  9 Batch  103 / 228  Training Loss  0.022023774683475494\n","Epoch  9 Batch  104 / 228  Training Loss  0.004322770517319441\n","Epoch  9 Batch  105 / 228  Training Loss  0.0017426644917577505\n","Epoch  9 Batch  106 / 228  Training Loss  0.000609893468208611\n","Epoch  9 Batch  107 / 228  Training Loss  0.0031101014465093613\n","Epoch  9 Batch  108 / 228  Training Loss  0.003574558300897479\n","Epoch  9 Batch  109 / 228  Training Loss  0.004431998357176781\n","Epoch  9 Batch  110 / 228  Training Loss  0.0036239218898117542\n","Epoch  9 Batch  111 / 228  Training Loss  0.0015907015185803175\n","Epoch  9 Batch  112 / 228  Training Loss  0.0005528717301785946\n","Epoch  9 Batch  113 / 228  Training Loss  0.0005690291873179376\n","Epoch  9 Batch  114 / 228  Training Loss  0.0013837594306096435\n","Epoch  9 Batch  115 / 228  Training Loss  0.0054313261061906815\n","Epoch  9 Batch  116 / 228  Training Loss  0.0016655467916280031\n","Epoch  9 Batch  117 / 228  Training Loss  0.0021833600476384163\n","Epoch  9 Batch  118 / 228  Training Loss  0.0023854325991123915\n","Epoch  9 Batch  119 / 228  Training Loss  0.005965105723589659\n","Epoch  9 Batch  120 / 228  Training Loss  0.012870823964476585\n","Epoch  9 Batch  121 / 228  Training Loss  0.00070670509012416\n","Epoch  9 Batch  122 / 228  Training Loss  0.006113694980740547\n","Epoch  9 Batch  123 / 228  Training Loss  0.0039059934206306934\n","Epoch  9 Batch  124 / 228  Training Loss  0.0009978709276765585\n","Epoch  9 Batch  125 / 228  Training Loss  0.009584128856658936\n","Epoch  9 Batch  126 / 228  Training Loss  0.008718831464648247\n","Epoch  9 Batch  127 / 228  Training Loss  0.0006165652303025126\n","Epoch  9 Batch  128 / 228  Training Loss  0.01445484347641468\n","Epoch  9 Batch  129 / 228  Training Loss  0.007173496298491955\n","Epoch  9 Batch  130 / 228  Training Loss  0.005261438898742199\n","Epoch  9 Batch  131 / 228  Training Loss  0.01627698540687561\n","Epoch  9 Batch  132 / 228  Training Loss  0.0025465681683272123\n","Epoch  9 Batch  133 / 228  Training Loss  0.0011303069768473506\n","Epoch  9 Batch  134 / 228  Training Loss  0.01650378666818142\n","Epoch  9 Batch  135 / 228  Training Loss  0.002375879092141986\n","Epoch  9 Batch  136 / 228  Training Loss  0.00118441937956959\n","Epoch  9 Batch  137 / 228  Training Loss  0.004135804250836372\n","Epoch  9 Batch  138 / 228  Training Loss  0.0007563097169622779\n","Epoch  9 Batch  139 / 228  Training Loss  0.0005578100099228323\n","Epoch  9 Batch  140 / 228  Training Loss  0.004530874080955982\n","Epoch  9 Batch  141 / 228  Training Loss  0.002028854563832283\n","Epoch  9 Batch  142 / 228  Training Loss  0.008967582136392593\n","Epoch  9 Batch  143 / 228  Training Loss  0.002380420919507742\n","Epoch  9 Batch  144 / 228  Training Loss  0.003046580357477069\n","Epoch  9 Batch  145 / 228  Training Loss  0.0008182468009181321\n","Epoch  9 Batch  146 / 228  Training Loss  0.0016716124955564737\n","Epoch  9 Batch  147 / 228  Training Loss  0.0037295904476195574\n","Epoch  9 Batch  148 / 228  Training Loss  0.0016766563057899475\n","Epoch  9 Batch  149 / 228  Training Loss  0.0009147191303782165\n","Epoch  9 Batch  150 / 228  Training Loss  0.000821340479888022\n","Epoch  9 Batch  151 / 228  Training Loss  0.0005973486113362014\n","Epoch  9 Batch  152 / 228  Training Loss  0.0013685693265870214\n","Epoch  9 Batch  153 / 228  Training Loss  0.0020736083388328552\n","Epoch  9 Batch  154 / 228  Training Loss  0.0014899849193170667\n","Epoch  9 Batch  155 / 228  Training Loss  0.0003416345571167767\n","Epoch  9 Batch  156 / 228  Training Loss  0.0007289063651114702\n","Epoch  9 Batch  157 / 228  Training Loss  0.00035509897861629725\n","Epoch  9 Batch  158 / 228  Training Loss  0.0005510100163519382\n","Epoch  9 Batch  159 / 228  Training Loss  0.0004650314222089946\n","Epoch  9 Batch  160 / 228  Training Loss  0.0003145909868180752\n","Epoch  9 Batch  161 / 228  Training Loss  0.001321838004514575\n","Epoch  9 Batch  162 / 228  Training Loss  0.0012113142292946577\n","Epoch  9 Batch  163 / 228  Training Loss  0.0018581995973363519\n","Epoch  9 Batch  164 / 228  Training Loss  0.0010305637260898948\n","Epoch  9 Batch  165 / 228  Training Loss  0.0006188105326145887\n","Epoch  9 Batch  166 / 228  Training Loss  0.0005941406125202775\n","Epoch  9 Batch  167 / 228  Training Loss  0.00038555575883947313\n","Epoch  9 Batch  168 / 228  Training Loss  0.0005082281422801316\n","Epoch  9 Batch  169 / 228  Training Loss  0.000728696002624929\n","Epoch  9 Batch  170 / 228  Training Loss  0.00025703615392558277\n","Epoch  9 Batch  171 / 228  Training Loss  0.0007412839913740754\n","Epoch  9 Batch  172 / 228  Training Loss  0.00030799483647570014\n","Epoch  9 Batch  173 / 228  Training Loss  0.000494890206027776\n","Epoch  9 Batch  174 / 228  Training Loss  0.0005283037899062037\n","Epoch  9 Batch  175 / 228  Training Loss  0.00041177798993885517\n","Epoch  9 Batch  176 / 228  Training Loss  0.002012066775932908\n","Epoch  9 Batch  177 / 228  Training Loss  0.00046890322118997574\n","Epoch  9 Batch  178 / 228  Training Loss  0.0006450851215049624\n","Epoch  9 Batch  179 / 228  Training Loss  0.000680978293530643\n","Epoch  9 Batch  180 / 228  Training Loss  0.0003976672305725515\n","Epoch  9 Batch  181 / 228  Training Loss  0.00043270393507555127\n","Epoch  9 Batch  182 / 228  Training Loss  0.0003203954838681966\n","Epoch  9 Batch  183 / 228  Training Loss  0.0007203866844065487\n","Epoch  9 Batch  184 / 228  Training Loss  0.0004339014703873545\n","Epoch  9 Batch  185 / 228  Training Loss  0.0007236872334033251\n","Epoch  9 Batch  186 / 228  Training Loss  0.001674577360972762\n","Epoch  9 Batch  187 / 228  Training Loss  0.00046225948608480394\n","Epoch  9 Batch  188 / 228  Training Loss  0.00041845295345410705\n","Epoch  9 Batch  189 / 228  Training Loss  0.009365695528686047\n","Epoch  9 Batch  190 / 228  Training Loss  0.0007078436901792884\n","Epoch  9 Batch  191 / 228  Training Loss  0.0003156063030473888\n","Epoch  9 Batch  192 / 228  Training Loss  0.00042112902156077325\n","Epoch  9 Batch  193 / 228  Training Loss  0.0012034309329465032\n","Epoch  9 Batch  194 / 228  Training Loss  0.0002443554694764316\n","Epoch  9 Batch  195 / 228  Training Loss  0.0005068947793915868\n","Epoch  9 Batch  196 / 228  Training Loss  0.0003737457445822656\n","Epoch  9 Batch  197 / 228  Training Loss  0.00076061743311584\n","Epoch  9 Batch  198 / 228  Training Loss  0.0005620309384539723\n","Epoch  9 Batch  199 / 228  Training Loss  0.0003261374367866665\n","Epoch  9 Batch  200 / 228  Training Loss  0.0038071442395448685\n","Epoch  9 Batch  201 / 228  Training Loss  0.005872632376849651\n","Epoch  9 Batch  202 / 228  Training Loss  0.0005906677688471973\n","Epoch  9 Batch  203 / 228  Training Loss  0.00028444992494769394\n","Epoch  9 Batch  204 / 228  Training Loss  0.00028320340788923204\n","Epoch  9 Batch  205 / 228  Training Loss  0.00042928181937895715\n","Epoch  9 Batch  206 / 228  Training Loss  0.004155139904469252\n","Epoch  9 Batch  207 / 228  Training Loss  0.002731539774686098\n","Epoch  9 Batch  208 / 228  Training Loss  0.0005254216957837343\n","Epoch  9 Batch  209 / 228  Training Loss  0.00028813915560021996\n","Epoch  9 Batch  210 / 228  Training Loss  0.0013687346363440156\n","Epoch  9 Batch  211 / 228  Training Loss  0.0002594618417788297\n","Epoch  9 Batch  212 / 228  Training Loss  0.0005595920374616981\n","Epoch  9 Batch  213 / 228  Training Loss  0.0007292198715731502\n","Epoch  9 Batch  214 / 228  Training Loss  0.00024110820959322155\n","Epoch  9 Batch  215 / 228  Training Loss  0.0005964933661743999\n","Epoch  9 Batch  216 / 228  Training Loss  0.0005084187141619623\n","Epoch  9 Batch  217 / 228  Training Loss  0.0007189948810264468\n","Epoch  9 Batch  218 / 228  Training Loss  0.0002689937246032059\n","Epoch  9 Batch  219 / 228  Training Loss  0.0008038508822210133\n","Epoch  9 Batch  220 / 228  Training Loss  0.001956016756594181\n","Epoch  9 Batch  221 / 228  Training Loss  0.000708188337739557\n","Epoch  9 Batch  222 / 228  Training Loss  0.0003955847641918808\n","Epoch  9 Batch  223 / 228  Training Loss  0.00038907339330762625\n","Epoch  9 Batch  224 / 228  Training Loss  0.0003407642070669681\n","Epoch  9 Batch  225 / 228  Training Loss  0.0009616432944312692\n","Epoch  9 Batch  226 / 228  Training Loss  0.00038382079219445586\n","Epoch  9 Batch  227 / 228  Training Loss  0.0012577945599332452\n","  10    |    -    |   0.002088   | 98.894817\n","----------------------------------------------------------------------\n","Running epoch: 10\n","Epoch  10 Batch  0 / 228  Training Loss  0.0003458281862549484\n","Epoch  10 Batch  1 / 228  Training Loss  0.0002493790234439075\n","Epoch  10 Batch  2 / 228  Training Loss  0.00023198375129140913\n","Epoch  10 Batch  3 / 228  Training Loss  0.0003533892158884555\n","Epoch  10 Batch  4 / 228  Training Loss  0.00039969399222172797\n","Epoch  10 Batch  5 / 228  Training Loss  0.000292701181024313\n","Epoch  10 Batch  6 / 228  Training Loss  0.00026378771872259676\n","Epoch  10 Batch  7 / 228  Training Loss  0.0008824460092000663\n","Epoch  10 Batch  8 / 228  Training Loss  0.0002488179015927017\n","Epoch  10 Batch  9 / 228  Training Loss  0.0029057790525257587\n","Epoch  10 Batch  10 / 228  Training Loss  0.0011785943061113358\n","Epoch  10 Batch  11 / 228  Training Loss  0.000576081860344857\n","Epoch  10 Batch  12 / 228  Training Loss  0.00036907504545524716\n","Epoch  10 Batch  13 / 228  Training Loss  0.0002411200839560479\n","Epoch  10 Batch  14 / 228  Training Loss  0.0003625014505814761\n","Epoch  10 Batch  15 / 228  Training Loss  0.0004128114669583738\n","Epoch  10 Batch  16 / 228  Training Loss  0.000275277066975832\n","Epoch  10 Batch  17 / 228  Training Loss  0.0012643623631447554\n","Epoch  10 Batch  18 / 228  Training Loss  0.0002804162504617125\n","Epoch  10 Batch  19 / 228  Training Loss  0.00029644000460393727\n","Epoch  10 Batch  20 / 228  Training Loss  0.0004106930864509195\n","Epoch  10 Batch  21 / 228  Training Loss  0.00042542751180008054\n","Epoch  10 Batch  22 / 228  Training Loss  0.0004447929677553475\n","Epoch  10 Batch  23 / 228  Training Loss  0.00022237600933294743\n","Epoch  10 Batch  24 / 228  Training Loss  0.00025799794821068645\n","Epoch  10 Batch  25 / 228  Training Loss  0.0002184928162023425\n","Epoch  10 Batch  26 / 228  Training Loss  0.0003832699148915708\n","Epoch  10 Batch  27 / 228  Training Loss  0.0005190322990529239\n","Epoch  10 Batch  28 / 228  Training Loss  0.00039922818541526794\n","Epoch  10 Batch  29 / 228  Training Loss  0.00019652904302347451\n","Epoch  10 Batch  30 / 228  Training Loss  0.00018774298951029778\n","Epoch  10 Batch  31 / 228  Training Loss  0.0004906157846562564\n","Epoch  10 Batch  32 / 228  Training Loss  0.00021602798369713128\n","Epoch  10 Batch  33 / 228  Training Loss  0.00021765851124655455\n","Epoch  10 Batch  34 / 228  Training Loss  0.0001764248445397243\n","Epoch  10 Batch  35 / 228  Training Loss  0.0001978275104193017\n","Epoch  10 Batch  36 / 228  Training Loss  0.0003046496713068336\n","Epoch  10 Batch  37 / 228  Training Loss  0.00039339004433713853\n","Epoch  10 Batch  38 / 228  Training Loss  0.0002989014028571546\n","Epoch  10 Batch  39 / 228  Training Loss  0.0031431540846824646\n","Epoch  10 Batch  40 / 228  Training Loss  0.0005656207795254886\n","Epoch  10 Batch  41 / 228  Training Loss  0.008266143500804901\n","Epoch  10 Batch  42 / 228  Training Loss  0.0006209835992194712\n","Epoch  10 Batch  43 / 228  Training Loss  0.0005598898278549314\n","Epoch  10 Batch  44 / 228  Training Loss  0.000293426972348243\n","Epoch  10 Batch  45 / 228  Training Loss  0.003056006273254752\n","Epoch  10 Batch  46 / 228  Training Loss  0.0009301620302721858\n","Epoch  10 Batch  47 / 228  Training Loss  0.028790947049856186\n","Epoch  10 Batch  48 / 228  Training Loss  0.0009463898604735732\n","Epoch  10 Batch  49 / 228  Training Loss  0.0029457167256623507\n","Epoch  10 Batch  50 / 228  Training Loss  0.0037467621732503176\n","Epoch  10 Batch  51 / 228  Training Loss  0.004526949487626553\n","Epoch  10 Batch  52 / 228  Training Loss  0.013143716380000114\n","Epoch  10 Batch  53 / 228  Training Loss  0.002518858527764678\n","Epoch  10 Batch  54 / 228  Training Loss  0.0011109558399766684\n","Epoch  10 Batch  55 / 228  Training Loss  0.0005314553272910416\n","Epoch  10 Batch  56 / 228  Training Loss  0.003851513843983412\n","Epoch  10 Batch  57 / 228  Training Loss  0.0018581568729132414\n","Epoch  10 Batch  58 / 228  Training Loss  0.0016912739956751466\n","Epoch  10 Batch  59 / 228  Training Loss  0.0025510883424431086\n","Epoch  10 Batch  60 / 228  Training Loss  0.001005445490591228\n","Epoch  10 Batch  61 / 228  Training Loss  0.0008107319590635598\n","Epoch  10 Batch  62 / 228  Training Loss  0.0007577681681141257\n","Epoch  10 Batch  63 / 228  Training Loss  0.015573011711239815\n","Epoch  10 Batch  64 / 228  Training Loss  0.0006874902173876762\n","Epoch  10 Batch  65 / 228  Training Loss  0.00030689541017636657\n","Epoch  10 Batch  66 / 228  Training Loss  0.0009564279462210834\n","Epoch  10 Batch  67 / 228  Training Loss  0.0009396038949489594\n","Epoch  10 Batch  68 / 228  Training Loss  0.0004041604115627706\n","Epoch  10 Batch  69 / 228  Training Loss  0.0006990108522586524\n","Epoch  10 Batch  70 / 228  Training Loss  0.00031332331127487123\n","Epoch  10 Batch  71 / 228  Training Loss  0.00034540396882221103\n","Epoch  10 Batch  72 / 228  Training Loss  0.004067528527230024\n","Epoch  10 Batch  73 / 228  Training Loss  0.0008275600848719478\n","Epoch  10 Batch  74 / 228  Training Loss  0.0007807567017152905\n","Epoch  10 Batch  75 / 228  Training Loss  0.0010467206593602896\n","Epoch  10 Batch  76 / 228  Training Loss  0.0003770240000449121\n","Epoch  10 Batch  77 / 228  Training Loss  0.0023868868593126535\n","Epoch  10 Batch  78 / 228  Training Loss  0.00161642802413553\n","Epoch  10 Batch  79 / 228  Training Loss  0.00031224923441186547\n","Epoch  10 Batch  80 / 228  Training Loss  0.0004500166396610439\n","Epoch  10 Batch  81 / 228  Training Loss  0.0005003695841878653\n","Epoch  10 Batch  82 / 228  Training Loss  0.0007171641918830574\n","Epoch  10 Batch  83 / 228  Training Loss  0.0009988010860979557\n","Epoch  10 Batch  84 / 228  Training Loss  0.0006653713644482195\n","Epoch  10 Batch  85 / 228  Training Loss  0.00012353734928183258\n","Epoch  10 Batch  86 / 228  Training Loss  0.0002507115132175386\n","Epoch  10 Batch  87 / 228  Training Loss  0.00029782982892356813\n","Epoch  10 Batch  88 / 228  Training Loss  0.00015757366782054305\n","Epoch  10 Batch  89 / 228  Training Loss  0.0009689071448519826\n","Epoch  10 Batch  90 / 228  Training Loss  0.00041958800284191966\n","Epoch  10 Batch  91 / 228  Training Loss  0.0003655020263977349\n","Epoch  10 Batch  92 / 228  Training Loss  0.0009967831429094076\n","Epoch  10 Batch  93 / 228  Training Loss  0.00043527461821213365\n","Epoch  10 Batch  94 / 228  Training Loss  0.0004343235050328076\n","Epoch  10 Batch  95 / 228  Training Loss  0.00038381823105737567\n","Epoch  10 Batch  96 / 228  Training Loss  0.00033172525581903756\n","Epoch  10 Batch  97 / 228  Training Loss  0.00024132081307470798\n","Epoch  10 Batch  98 / 228  Training Loss  0.0068015046417713165\n","Epoch  10 Batch  99 / 228  Training Loss  0.00020376825705170631\n","Epoch  10 Batch  100 / 228  Training Loss  0.0001772158866515383\n","Epoch  10 Batch  101 / 228  Training Loss  0.0003857622214127332\n","Epoch  10 Batch  102 / 228  Training Loss  0.00027420721016824245\n","Epoch  10 Batch  103 / 228  Training Loss  0.000512737431563437\n","Epoch  10 Batch  104 / 228  Training Loss  0.00023216936097014695\n","Epoch  10 Batch  105 / 228  Training Loss  0.0003238516510464251\n","Epoch  10 Batch  106 / 228  Training Loss  0.00031678442610427737\n","Epoch  10 Batch  107 / 228  Training Loss  0.00031181678059510887\n","Epoch  10 Batch  108 / 228  Training Loss  0.00037796571268700063\n","Epoch  10 Batch  109 / 228  Training Loss  0.00028280518017709255\n","Epoch  10 Batch  110 / 228  Training Loss  0.0009053767425939441\n","Epoch  10 Batch  111 / 228  Training Loss  0.0003232489980291575\n","Epoch  10 Batch  112 / 228  Training Loss  0.0002083161089103669\n","Epoch  10 Batch  113 / 228  Training Loss  0.00023883290123194456\n","Epoch  10 Batch  114 / 228  Training Loss  0.0005195761332288384\n","Epoch  10 Batch  115 / 228  Training Loss  0.0003582271747291088\n","Epoch  10 Batch  116 / 228  Training Loss  0.0003066646750085056\n","Epoch  10 Batch  117 / 228  Training Loss  0.0009895821567624807\n","Epoch  10 Batch  118 / 228  Training Loss  0.0005664221826009452\n","Epoch  10 Batch  119 / 228  Training Loss  0.000270363234449178\n","Epoch  10 Batch  120 / 228  Training Loss  0.0002924109867308289\n","Epoch  10 Batch  121 / 228  Training Loss  0.0001353010447928682\n","Epoch  10 Batch  122 / 228  Training Loss  0.00017196648695971817\n","Epoch  10 Batch  123 / 228  Training Loss  0.00025301461573690176\n","Epoch  10 Batch  124 / 228  Training Loss  0.00018231544527225196\n","Epoch  10 Batch  125 / 228  Training Loss  0.0002667772932909429\n","Epoch  10 Batch  126 / 228  Training Loss  0.00033129137591458857\n","Epoch  10 Batch  127 / 228  Training Loss  0.00024159818713087589\n","Epoch  10 Batch  128 / 228  Training Loss  0.0011982055148109794\n","Epoch  10 Batch  129 / 228  Training Loss  0.0014233618276193738\n","Epoch  10 Batch  130 / 228  Training Loss  0.000212980477954261\n","Epoch  10 Batch  131 / 228  Training Loss  0.0006889271317049861\n","Epoch  10 Batch  132 / 228  Training Loss  0.0005985647439956665\n","Epoch  10 Batch  133 / 228  Training Loss  0.00017403019592165947\n","Epoch  10 Batch  134 / 228  Training Loss  0.00028952781576663256\n","Epoch  10 Batch  135 / 228  Training Loss  0.0004053808224853128\n","Epoch  10 Batch  136 / 228  Training Loss  0.0004273674567230046\n","Epoch  10 Batch  137 / 228  Training Loss  0.0003794141230173409\n","Epoch  10 Batch  138 / 228  Training Loss  0.00035615096567198634\n","Epoch  10 Batch  139 / 228  Training Loss  0.0001764305488904938\n","Epoch  10 Batch  140 / 228  Training Loss  0.00025595369515940547\n","Epoch  10 Batch  141 / 228  Training Loss  0.0003683333343360573\n","Epoch  10 Batch  142 / 228  Training Loss  0.0005898511735722423\n","Epoch  10 Batch  143 / 228  Training Loss  0.0007477217586711049\n","Epoch  10 Batch  144 / 228  Training Loss  0.00029782779165543616\n","Epoch  10 Batch  145 / 228  Training Loss  0.0002624154440127313\n","Epoch  10 Batch  146 / 228  Training Loss  0.00017042570107150823\n","Epoch  10 Batch  147 / 228  Training Loss  0.00023410974245052785\n","Epoch  10 Batch  148 / 228  Training Loss  0.0001284568279515952\n","Epoch  10 Batch  149 / 228  Training Loss  0.0001510088623035699\n","Epoch  10 Batch  150 / 228  Training Loss  0.0031673796474933624\n","Epoch  10 Batch  151 / 228  Training Loss  0.00027466510073281825\n","Epoch  10 Batch  152 / 228  Training Loss  0.00025376497069373727\n","Epoch  10 Batch  153 / 228  Training Loss  0.0001561159733682871\n","Epoch  10 Batch  154 / 228  Training Loss  0.0002541440480854362\n","Epoch  10 Batch  155 / 228  Training Loss  0.00018556359282229096\n","Epoch  10 Batch  156 / 228  Training Loss  0.00018053382518701255\n","Epoch  10 Batch  157 / 228  Training Loss  0.00018318570801056921\n","Epoch  10 Batch  158 / 228  Training Loss  0.0001177098965854384\n","Epoch  10 Batch  159 / 228  Training Loss  0.0002160721196560189\n","Epoch  10 Batch  160 / 228  Training Loss  0.00014385610120370984\n","Epoch  10 Batch  161 / 228  Training Loss  0.0001709792559267953\n","Epoch  10 Batch  162 / 228  Training Loss  0.0003508289810270071\n","Epoch  10 Batch  163 / 228  Training Loss  0.0002201648021582514\n","Epoch  10 Batch  164 / 228  Training Loss  0.00013629579916596413\n","Epoch  10 Batch  165 / 228  Training Loss  0.0001241538702743128\n","Epoch  10 Batch  166 / 228  Training Loss  0.00027388695161789656\n","Epoch  10 Batch  167 / 228  Training Loss  0.00037667356082238257\n","Epoch  10 Batch  168 / 228  Training Loss  0.00017797836335375905\n","Epoch  10 Batch  169 / 228  Training Loss  0.00022813228133600205\n","Epoch  10 Batch  170 / 228  Training Loss  0.0003933478146791458\n","Epoch  10 Batch  171 / 228  Training Loss  0.00018669370911084116\n","Epoch  10 Batch  172 / 228  Training Loss  0.00013671282795257866\n","Epoch  10 Batch  173 / 228  Training Loss  0.0001552717003505677\n","Epoch  10 Batch  174 / 228  Training Loss  0.00022336917754728347\n","Epoch  10 Batch  175 / 228  Training Loss  0.00033339631045237184\n","Epoch  10 Batch  176 / 228  Training Loss  0.00016143047832883894\n","Epoch  10 Batch  177 / 228  Training Loss  0.0001923762320075184\n","Epoch  10 Batch  178 / 228  Training Loss  0.00018650927813723683\n","Epoch  10 Batch  179 / 228  Training Loss  0.0002190412487834692\n","Epoch  10 Batch  180 / 228  Training Loss  0.00017146088066510856\n","Epoch  10 Batch  181 / 228  Training Loss  0.00011429488949943334\n","Epoch  10 Batch  182 / 228  Training Loss  0.00016419528401456773\n","Epoch  10 Batch  183 / 228  Training Loss  6.990146357566118e-05\n","Epoch  10 Batch  184 / 228  Training Loss  0.0002077793178614229\n","Epoch  10 Batch  185 / 228  Training Loss  0.00018455555255059153\n","Epoch  10 Batch  186 / 228  Training Loss  0.0001724450703477487\n","Epoch  10 Batch  187 / 228  Training Loss  0.0002683078055270016\n","Epoch  10 Batch  188 / 228  Training Loss  0.0004272167570888996\n","Epoch  10 Batch  189 / 228  Training Loss  0.0006821072893217206\n","Epoch  10 Batch  190 / 228  Training Loss  0.000195795510080643\n","Epoch  10 Batch  191 / 228  Training Loss  0.0001232574722962454\n","Epoch  10 Batch  192 / 228  Training Loss  0.00020311361004132777\n","Epoch  10 Batch  193 / 228  Training Loss  0.0001063451636582613\n","Epoch  10 Batch  194 / 228  Training Loss  0.00492189871147275\n","Epoch  10 Batch  195 / 228  Training Loss  0.00024155287246685475\n","Epoch  10 Batch  196 / 228  Training Loss  0.00021995751012582332\n","Epoch  10 Batch  197 / 228  Training Loss  0.0006406287429854274\n","Epoch  10 Batch  198 / 228  Training Loss  0.00023918649822007865\n","Epoch  10 Batch  199 / 228  Training Loss  0.0023203655146062374\n","Epoch  10 Batch  200 / 228  Training Loss  0.0002694172435440123\n","Epoch  10 Batch  201 / 228  Training Loss  0.0001583499542903155\n","Epoch  10 Batch  202 / 228  Training Loss  0.000142309203511104\n","Epoch  10 Batch  203 / 228  Training Loss  0.00018189879483543336\n","Epoch  10 Batch  204 / 228  Training Loss  0.0001239808480022475\n","Epoch  10 Batch  205 / 228  Training Loss  0.000158739770995453\n","Epoch  10 Batch  206 / 228  Training Loss  0.00018461488070897758\n","Epoch  10 Batch  207 / 228  Training Loss  0.0002542966976761818\n","Epoch  10 Batch  208 / 228  Training Loss  0.00035418602055869997\n","Epoch  10 Batch  209 / 228  Training Loss  0.00022137272753752768\n","Epoch  10 Batch  210 / 228  Training Loss  0.0007859580218791962\n","Epoch  10 Batch  211 / 228  Training Loss  0.00014866111450828612\n","Epoch  10 Batch  212 / 228  Training Loss  0.0002736059541348368\n","Epoch  10 Batch  213 / 228  Training Loss  0.00023843471717555076\n","Epoch  10 Batch  214 / 228  Training Loss  0.0001492257579229772\n","Epoch  10 Batch  215 / 228  Training Loss  0.00019322641310282052\n","Epoch  10 Batch  216 / 228  Training Loss  0.0001891607535071671\n","Epoch  10 Batch  217 / 228  Training Loss  0.00023661207524128258\n","Epoch  10 Batch  218 / 228  Training Loss  0.0006242277449928224\n","Epoch  10 Batch  219 / 228  Training Loss  0.0002402830868959427\n","Epoch  10 Batch  220 / 228  Training Loss  0.0013571014860644937\n","Epoch  10 Batch  221 / 228  Training Loss  0.000209265504963696\n","Epoch  10 Batch  222 / 228  Training Loss  0.00023883662652224302\n","Epoch  10 Batch  223 / 228  Training Loss  0.00015103843179531395\n","Epoch  10 Batch  224 / 228  Training Loss  0.00016532264999113977\n","Epoch  10 Batch  225 / 228  Training Loss  0.00013658155512530357\n","Epoch  10 Batch  226 / 228  Training Loss  0.00014788153930567205\n","Epoch  10 Batch  227 / 228  Training Loss  0.00013809368829242885\n","  11    |    -    |   0.000882   | 98.932927\n","----------------------------------------------------------------------\n","Running epoch: 11\n","Epoch  11 Batch  0 / 228  Training Loss  0.0001611192274140194\n","Epoch  11 Batch  1 / 228  Training Loss  0.0002722536737564951\n","Epoch  11 Batch  2 / 228  Training Loss  0.0001567595318192616\n","Epoch  11 Batch  3 / 228  Training Loss  0.00013093675079289824\n","Epoch  11 Batch  4 / 228  Training Loss  0.00012648766278289258\n","Epoch  11 Batch  5 / 228  Training Loss  0.0001576562790432945\n","Epoch  11 Batch  6 / 228  Training Loss  0.0002598745340947062\n","Epoch  11 Batch  7 / 228  Training Loss  0.00028343676240183413\n","Epoch  11 Batch  8 / 228  Training Loss  0.00012761908874381334\n","Epoch  11 Batch  9 / 228  Training Loss  0.00027354643680155277\n","Epoch  11 Batch  10 / 228  Training Loss  0.00017494252824690193\n","Epoch  11 Batch  11 / 228  Training Loss  0.00016149735893122852\n","Epoch  11 Batch  12 / 228  Training Loss  0.00017735945584718138\n","Epoch  11 Batch  13 / 228  Training Loss  0.00015709581202827394\n","Epoch  11 Batch  14 / 228  Training Loss  0.00012155422882642597\n","Epoch  11 Batch  15 / 228  Training Loss  0.00012079416774213314\n","Epoch  11 Batch  16 / 228  Training Loss  0.0001606293662916869\n","Epoch  11 Batch  17 / 228  Training Loss  0.000192618666915223\n","Epoch  11 Batch  18 / 228  Training Loss  0.00013881063205190003\n","Epoch  11 Batch  19 / 228  Training Loss  0.00015564200293738395\n","Epoch  11 Batch  20 / 228  Training Loss  0.00024001703422982246\n","Epoch  11 Batch  21 / 228  Training Loss  0.00023253235849551857\n","Epoch  11 Batch  22 / 228  Training Loss  0.00015232873556669801\n","Epoch  11 Batch  23 / 228  Training Loss  0.00014869411825202405\n","Epoch  11 Batch  24 / 228  Training Loss  0.00034615525510162115\n","Epoch  11 Batch  25 / 228  Training Loss  0.00010555299377301708\n","Epoch  11 Batch  26 / 228  Training Loss  0.0003120123874396086\n","Epoch  11 Batch  27 / 228  Training Loss  0.0002178158174501732\n","Epoch  11 Batch  28 / 228  Training Loss  0.0002582845918368548\n","Epoch  11 Batch  29 / 228  Training Loss  0.00017865373229142278\n","Epoch  11 Batch  30 / 228  Training Loss  0.00021585442300420254\n","Epoch  11 Batch  31 / 228  Training Loss  0.00019721027638297528\n","Epoch  11 Batch  32 / 228  Training Loss  0.00018638264737091959\n","Epoch  11 Batch  33 / 228  Training Loss  0.00011970349441980943\n","Epoch  11 Batch  34 / 228  Training Loss  0.00021782722615171224\n","Epoch  11 Batch  35 / 228  Training Loss  0.00013393642439041287\n","Epoch  11 Batch  36 / 228  Training Loss  9.997094457503408e-05\n","Epoch  11 Batch  37 / 228  Training Loss  0.00010931602446362376\n","Epoch  11 Batch  38 / 228  Training Loss  0.0003293063200544566\n","Epoch  11 Batch  39 / 228  Training Loss  0.00021351236500777304\n","Epoch  11 Batch  40 / 228  Training Loss  0.00037114002043381333\n","Epoch  11 Batch  41 / 228  Training Loss  0.00019989485736005008\n","Epoch  11 Batch  42 / 228  Training Loss  0.0003181064093951136\n","Epoch  11 Batch  43 / 228  Training Loss  0.00043379998533055186\n","Epoch  11 Batch  44 / 228  Training Loss  0.00017164924065582454\n","Epoch  11 Batch  45 / 228  Training Loss  0.00014885664859320968\n","Epoch  11 Batch  46 / 228  Training Loss  0.00018332437321078032\n","Epoch  11 Batch  47 / 228  Training Loss  0.00020525764557532966\n","Epoch  11 Batch  48 / 228  Training Loss  0.0001286563347093761\n","Epoch  11 Batch  49 / 228  Training Loss  0.0006381728453561664\n","Epoch  11 Batch  50 / 228  Training Loss  0.00014281108451541513\n","Epoch  11 Batch  51 / 228  Training Loss  9.49192326515913e-05\n","Epoch  11 Batch  52 / 228  Training Loss  0.0002035344223259017\n","Epoch  11 Batch  53 / 228  Training Loss  0.0005057292291894555\n","Epoch  11 Batch  54 / 228  Training Loss  0.00011841157538583502\n","Epoch  11 Batch  55 / 228  Training Loss  0.0003178871120326221\n","Epoch  11 Batch  56 / 228  Training Loss  9.873538510873914e-05\n","Epoch  11 Batch  57 / 228  Training Loss  0.000162900731083937\n","Epoch  11 Batch  58 / 228  Training Loss  0.0001507564156781882\n","Epoch  11 Batch  59 / 228  Training Loss  9.819892147788778e-05\n","Epoch  11 Batch  60 / 228  Training Loss  8.305485243909061e-05\n","Epoch  11 Batch  61 / 228  Training Loss  8.79365106811747e-05\n","Epoch  11 Batch  62 / 228  Training Loss  0.00011155461834277958\n","Epoch  11 Batch  63 / 228  Training Loss  0.00480129336938262\n","Epoch  11 Batch  64 / 228  Training Loss  0.0002468838938511908\n","Epoch  11 Batch  65 / 228  Training Loss  0.00011621784506132826\n","Epoch  11 Batch  66 / 228  Training Loss  0.0002018291997956112\n","Epoch  11 Batch  67 / 228  Training Loss  0.00028628192376345396\n","Epoch  11 Batch  68 / 228  Training Loss  0.00015824676665943116\n","Epoch  11 Batch  69 / 228  Training Loss  0.00019980307843070477\n","Epoch  11 Batch  70 / 228  Training Loss  9.948932711267844e-05\n","Epoch  11 Batch  71 / 228  Training Loss  9.319801756646484e-05\n","Epoch  11 Batch  72 / 228  Training Loss  0.00012658219202421606\n","Epoch  11 Batch  73 / 228  Training Loss  0.00019073628936894238\n","Epoch  11 Batch  74 / 228  Training Loss  0.00010028380347648636\n","Epoch  11 Batch  75 / 228  Training Loss  0.00010605288844089955\n","Epoch  11 Batch  76 / 228  Training Loss  0.0002103967999573797\n","Epoch  11 Batch  77 / 228  Training Loss  0.0001461256033508107\n","Epoch  11 Batch  78 / 228  Training Loss  7.28619415895082e-05\n","Epoch  11 Batch  79 / 228  Training Loss  0.0001467022520955652\n","Epoch  11 Batch  80 / 228  Training Loss  0.00028337619733065367\n","Epoch  11 Batch  81 / 228  Training Loss  0.00015668029664084315\n","Epoch  11 Batch  82 / 228  Training Loss  0.00012531381798908114\n","Epoch  11 Batch  83 / 228  Training Loss  0.00016499034245498478\n","Epoch  11 Batch  84 / 228  Training Loss  0.00039820087840780616\n","Epoch  11 Batch  85 / 228  Training Loss  0.00010577763896435499\n","Epoch  11 Batch  86 / 228  Training Loss  0.0001606425503268838\n","Epoch  11 Batch  87 / 228  Training Loss  0.00011950690532103181\n","Epoch  11 Batch  88 / 228  Training Loss  0.00012537313159555197\n","Epoch  11 Batch  89 / 228  Training Loss  9.765736467670649e-05\n","Epoch  11 Batch  90 / 228  Training Loss  0.00011001657549059018\n","Epoch  11 Batch  91 / 228  Training Loss  7.118582288967445e-05\n","Epoch  11 Batch  92 / 228  Training Loss  8.95917764864862e-05\n","Epoch  11 Batch  93 / 228  Training Loss  8.335581514984369e-05\n","Epoch  11 Batch  94 / 228  Training Loss  0.00014733572606928647\n","Epoch  11 Batch  95 / 228  Training Loss  0.00012203423830214888\n","Epoch  11 Batch  96 / 228  Training Loss  0.00023478959337808192\n","Epoch  11 Batch  97 / 228  Training Loss  0.00015616387827321887\n","Epoch  11 Batch  98 / 228  Training Loss  0.00017196043336298317\n","Epoch  11 Batch  99 / 228  Training Loss  0.00010435799777042121\n","Epoch  11 Batch  100 / 228  Training Loss  0.00011989547783741727\n","Epoch  11 Batch  101 / 228  Training Loss  0.00010336013656342402\n","Epoch  11 Batch  102 / 228  Training Loss  0.00011338751210132614\n","Epoch  11 Batch  103 / 228  Training Loss  0.00011125105811515823\n","Epoch  11 Batch  104 / 228  Training Loss  0.00017061157268472016\n","Epoch  11 Batch  105 / 228  Training Loss  0.00012301839888095856\n","Epoch  11 Batch  106 / 228  Training Loss  0.00014360863133333623\n","Epoch  11 Batch  107 / 228  Training Loss  9.102995682042092e-05\n","Epoch  11 Batch  108 / 228  Training Loss  0.00013660329568665475\n","Epoch  11 Batch  109 / 228  Training Loss  0.0001382569462293759\n","Epoch  11 Batch  110 / 228  Training Loss  9.334391506854445e-05\n","Epoch  11 Batch  111 / 228  Training Loss  0.0001495402684668079\n","Epoch  11 Batch  112 / 228  Training Loss  0.0001401137706125155\n","Epoch  11 Batch  113 / 228  Training Loss  0.0001486651017330587\n","Epoch  11 Batch  114 / 228  Training Loss  0.00012366854934953153\n","Epoch  11 Batch  115 / 228  Training Loss  9.449104982195422e-05\n","Epoch  11 Batch  116 / 228  Training Loss  0.00011655837442958727\n","Epoch  11 Batch  117 / 228  Training Loss  9.233208402292803e-05\n","Epoch  11 Batch  118 / 228  Training Loss  0.00010508359264349565\n","Epoch  11 Batch  119 / 228  Training Loss  0.000138265109853819\n","Epoch  11 Batch  120 / 228  Training Loss  0.00012838191469199955\n","Epoch  11 Batch  121 / 228  Training Loss  0.0002876052458304912\n","Epoch  11 Batch  122 / 228  Training Loss  8.749652624828741e-05\n","Epoch  11 Batch  123 / 228  Training Loss  0.00020242090977262706\n","Epoch  11 Batch  124 / 228  Training Loss  0.00011607736814767122\n","Epoch  11 Batch  125 / 228  Training Loss  0.00014086147712077945\n","Epoch  11 Batch  126 / 228  Training Loss  0.0001415092556271702\n","Epoch  11 Batch  127 / 228  Training Loss  0.00010626355651766062\n","Epoch  11 Batch  128 / 228  Training Loss  0.00011228070070501417\n","Epoch  11 Batch  129 / 228  Training Loss  0.0001677588588790968\n","Epoch  11 Batch  130 / 228  Training Loss  0.00014707681839354336\n","Epoch  11 Batch  131 / 228  Training Loss  0.00011068462481489405\n","Epoch  11 Batch  132 / 228  Training Loss  0.00013600137026514858\n","Epoch  11 Batch  133 / 228  Training Loss  0.00015121258911676705\n","Epoch  11 Batch  134 / 228  Training Loss  0.00012120179599151015\n","Epoch  11 Batch  135 / 228  Training Loss  5.472793782246299e-05\n","Epoch  11 Batch  136 / 228  Training Loss  0.00013747328193858266\n","Epoch  11 Batch  137 / 228  Training Loss  0.00014455788186751306\n","Epoch  11 Batch  138 / 228  Training Loss  9.462509478908032e-05\n","Epoch  11 Batch  139 / 228  Training Loss  0.00010046999523183331\n","Epoch  11 Batch  140 / 228  Training Loss  8.61899388837628e-05\n","Epoch  11 Batch  141 / 228  Training Loss  0.00020801890059374273\n","Epoch  11 Batch  142 / 228  Training Loss  9.276153286918998e-05\n","Epoch  11 Batch  143 / 228  Training Loss  9.704979311209172e-05\n","Epoch  11 Batch  144 / 228  Training Loss  0.00014636717969551682\n","Epoch  11 Batch  145 / 228  Training Loss  9.845781460171565e-05\n","Epoch  11 Batch  146 / 228  Training Loss  9.300882084062323e-05\n","Epoch  11 Batch  147 / 228  Training Loss  0.00015545742644462734\n","Epoch  11 Batch  148 / 228  Training Loss  7.78596440795809e-05\n","Epoch  11 Batch  149 / 228  Training Loss  9.576376032782719e-05\n","Epoch  11 Batch  150 / 228  Training Loss  0.00010581033711787313\n","Epoch  11 Batch  151 / 228  Training Loss  0.00014664666377939284\n","Epoch  11 Batch  152 / 228  Training Loss  0.00013762080925516784\n","Epoch  11 Batch  153 / 228  Training Loss  0.00010337886487832293\n","Epoch  11 Batch  154 / 228  Training Loss  9.75774964899756e-05\n","Epoch  11 Batch  155 / 228  Training Loss  8.275604341179132e-05\n","Epoch  11 Batch  156 / 228  Training Loss  0.00033207700471393764\n","Epoch  11 Batch  157 / 228  Training Loss  0.00016014768334571272\n","Epoch  11 Batch  158 / 228  Training Loss  0.0001384149945806712\n","Epoch  11 Batch  159 / 228  Training Loss  6.900310108903795e-05\n","Epoch  11 Batch  160 / 228  Training Loss  8.819381037028506e-05\n","Epoch  11 Batch  161 / 228  Training Loss  0.00013782620953861624\n","Epoch  11 Batch  162 / 228  Training Loss  0.00010322728485334665\n","Epoch  11 Batch  163 / 228  Training Loss  6.0431826568674296e-05\n","Epoch  11 Batch  164 / 228  Training Loss  9.028030035551637e-05\n","Epoch  11 Batch  165 / 228  Training Loss  0.00011918342352146283\n","Epoch  11 Batch  166 / 228  Training Loss  5.236053402768448e-05\n","Epoch  11 Batch  167 / 228  Training Loss  6.658541678916663e-05\n","Epoch  11 Batch  168 / 228  Training Loss  5.254807911114767e-05\n","Epoch  11 Batch  169 / 228  Training Loss  5.683100243913941e-05\n","Epoch  11 Batch  170 / 228  Training Loss  8.017082291189581e-05\n","Epoch  11 Batch  171 / 228  Training Loss  0.00010554595064604655\n","Epoch  11 Batch  172 / 228  Training Loss  9.030767250806093e-05\n","Epoch  11 Batch  173 / 228  Training Loss  7.291639485629275e-05\n","Epoch  11 Batch  174 / 228  Training Loss  0.00011342069774400443\n","Epoch  11 Batch  175 / 228  Training Loss  9.383674478158355e-05\n","Epoch  11 Batch  176 / 228  Training Loss  0.0001178736420115456\n","Epoch  11 Batch  177 / 228  Training Loss  0.0001133667683461681\n","Epoch  11 Batch  178 / 228  Training Loss  0.00012840966519434005\n","Epoch  11 Batch  179 / 228  Training Loss  9.060774755198509e-05\n","Epoch  11 Batch  180 / 228  Training Loss  0.0010906498646363616\n","Epoch  11 Batch  181 / 228  Training Loss  0.0003695082268677652\n","Epoch  11 Batch  182 / 228  Training Loss  0.00016096202307380736\n","Epoch  11 Batch  183 / 228  Training Loss  0.00014831763110123575\n","Epoch  11 Batch  184 / 228  Training Loss  6.696216587442905e-05\n","Epoch  11 Batch  185 / 228  Training Loss  0.00026954468921758235\n","Epoch  11 Batch  186 / 228  Training Loss  0.00011776500468840823\n","Epoch  11 Batch  187 / 228  Training Loss  0.00019338444690220058\n","Epoch  11 Batch  188 / 228  Training Loss  0.00019774065003730357\n","Epoch  11 Batch  189 / 228  Training Loss  0.00011796185572165996\n","Epoch  11 Batch  190 / 228  Training Loss  0.00011943998106289655\n","Epoch  11 Batch  191 / 228  Training Loss  0.00010515475878491998\n","Epoch  11 Batch  192 / 228  Training Loss  0.00013163371477276087\n","Epoch  11 Batch  193 / 228  Training Loss  0.00013300537830218673\n","Epoch  11 Batch  194 / 228  Training Loss  8.400093793170527e-05\n","Epoch  11 Batch  195 / 228  Training Loss  7.054451998556033e-05\n","Epoch  11 Batch  196 / 228  Training Loss  0.00010328735515940934\n","Epoch  11 Batch  197 / 228  Training Loss  8.136719407048076e-05\n","Epoch  11 Batch  198 / 228  Training Loss  0.00011681064643198624\n","Epoch  11 Batch  199 / 228  Training Loss  0.00013490031415130943\n","Epoch  11 Batch  200 / 228  Training Loss  7.813726551830769e-05\n","Epoch  11 Batch  201 / 228  Training Loss  0.00016739388229325414\n","Epoch  11 Batch  202 / 228  Training Loss  9.027832857100293e-05\n","Epoch  11 Batch  203 / 228  Training Loss  0.000634636206086725\n","Epoch  11 Batch  204 / 228  Training Loss  0.00015905503823887557\n","Epoch  11 Batch  205 / 228  Training Loss  0.00017148061306215823\n","Epoch  11 Batch  206 / 228  Training Loss  8.344178058905527e-05\n","Epoch  11 Batch  207 / 228  Training Loss  7.152715988922864e-05\n","Epoch  11 Batch  208 / 228  Training Loss  0.00012371783668641\n","Epoch  11 Batch  209 / 228  Training Loss  0.0001068524070433341\n","Epoch  11 Batch  210 / 228  Training Loss  0.00010980634397128597\n","Epoch  11 Batch  211 / 228  Training Loss  0.00013164334814064205\n","Epoch  11 Batch  212 / 228  Training Loss  8.64260655362159e-05\n","Epoch  11 Batch  213 / 228  Training Loss  0.00015476955741178244\n","Epoch  11 Batch  214 / 228  Training Loss  8.913811325328425e-05\n","Epoch  11 Batch  215 / 228  Training Loss  0.00011323259968776256\n","Epoch  11 Batch  216 / 228  Training Loss  0.0010678888065740466\n","Epoch  11 Batch  217 / 228  Training Loss  0.00012291086022742093\n","Epoch  11 Batch  218 / 228  Training Loss  6.775485962862149e-05\n","Epoch  11 Batch  219 / 228  Training Loss  0.00012741165119223297\n","Epoch  11 Batch  220 / 228  Training Loss  0.00025883756461553276\n","Epoch  11 Batch  221 / 228  Training Loss  0.00013837458391208202\n","Epoch  11 Batch  222 / 228  Training Loss  0.00015810594777576625\n","Epoch  11 Batch  223 / 228  Training Loss  0.0001707115734461695\n","Epoch  11 Batch  224 / 228  Training Loss  0.002453776076436043\n","Epoch  11 Batch  225 / 228  Training Loss  0.004344946704804897\n","Epoch  11 Batch  226 / 228  Training Loss  0.000553946360014379\n","Epoch  11 Batch  227 / 228  Training Loss  0.0002641562314238399\n","  12    |    -    |   0.000212   | 98.666159\n","----------------------------------------------------------------------\n","Running epoch: 12\n","Epoch  12 Batch  0 / 228  Training Loss  9.040749864652753e-05\n","Epoch  12 Batch  1 / 228  Training Loss  0.0034455328714102507\n","Epoch  12 Batch  2 / 228  Training Loss  0.0005303345969878137\n","Epoch  12 Batch  3 / 228  Training Loss  0.006572254933416843\n","Epoch  12 Batch  4 / 228  Training Loss  0.0016015113797038794\n","Epoch  12 Batch  5 / 228  Training Loss  0.000236638356000185\n","Epoch  12 Batch  6 / 228  Training Loss  0.0028519011102616787\n","Epoch  12 Batch  7 / 228  Training Loss  0.00019317360420245677\n","Epoch  12 Batch  8 / 228  Training Loss  0.0007597276708111167\n","Epoch  12 Batch  9 / 228  Training Loss  0.00018995485152117908\n","Epoch  12 Batch  10 / 228  Training Loss  0.0008998747798614204\n","Epoch  12 Batch  11 / 228  Training Loss  9.806349407881498e-05\n","Epoch  12 Batch  12 / 228  Training Loss  0.0001431470300303772\n","Epoch  12 Batch  13 / 228  Training Loss  7.620367250638083e-05\n","Epoch  12 Batch  14 / 228  Training Loss  0.00014771513815503567\n","Epoch  12 Batch  15 / 228  Training Loss  0.0003673166793305427\n","Epoch  12 Batch  16 / 228  Training Loss  0.00017750747792888433\n","Epoch  12 Batch  17 / 228  Training Loss  0.00014543006545864046\n","Epoch  12 Batch  18 / 228  Training Loss  0.00010175417264690623\n","Epoch  12 Batch  19 / 228  Training Loss  0.00013823277549818158\n","Epoch  12 Batch  20 / 228  Training Loss  0.00016292966029141098\n","Epoch  12 Batch  21 / 228  Training Loss  0.00017892855976242572\n","Epoch  12 Batch  22 / 228  Training Loss  0.00014052903861738741\n","Epoch  12 Batch  23 / 228  Training Loss  0.00010172192560276017\n","Epoch  12 Batch  24 / 228  Training Loss  0.00011438552610343322\n","Epoch  12 Batch  25 / 228  Training Loss  0.0002524666488170624\n","Epoch  12 Batch  26 / 228  Training Loss  0.00014887432917021215\n","Epoch  12 Batch  27 / 228  Training Loss  0.00017692902474664152\n","Epoch  12 Batch  28 / 228  Training Loss  0.00022029927640687674\n","Epoch  12 Batch  29 / 228  Training Loss  0.00014368399570230395\n","Epoch  12 Batch  30 / 228  Training Loss  0.00014717776502948254\n","Epoch  12 Batch  31 / 228  Training Loss  8.593896200181916e-05\n","Epoch  12 Batch  32 / 228  Training Loss  0.00027749326545745134\n","Epoch  12 Batch  33 / 228  Training Loss  0.0001569386658957228\n","Epoch  12 Batch  34 / 228  Training Loss  0.00016112826415337622\n","Epoch  12 Batch  35 / 228  Training Loss  0.00014700362225994468\n","Epoch  12 Batch  36 / 228  Training Loss  7.536980410804972e-05\n","Epoch  12 Batch  37 / 228  Training Loss  0.0005966061144135892\n","Epoch  12 Batch  38 / 228  Training Loss  0.0001452843425795436\n","Epoch  12 Batch  39 / 228  Training Loss  0.0005236432189121842\n","Epoch  12 Batch  40 / 228  Training Loss  0.00012989234528504312\n","Epoch  12 Batch  41 / 228  Training Loss  0.00013293238589540124\n","Epoch  12 Batch  42 / 228  Training Loss  0.00015523002366535366\n","Epoch  12 Batch  43 / 228  Training Loss  0.00012019678251817822\n","Epoch  12 Batch  44 / 228  Training Loss  0.00019109284039586782\n","Epoch  12 Batch  45 / 228  Training Loss  0.00018279277719557285\n","Epoch  12 Batch  46 / 228  Training Loss  0.0001329756632912904\n","Epoch  12 Batch  47 / 228  Training Loss  8.432897448074073e-05\n","Epoch  12 Batch  48 / 228  Training Loss  8.053516648942605e-05\n","Epoch  12 Batch  49 / 228  Training Loss  8.880634413799271e-05\n","Epoch  12 Batch  50 / 228  Training Loss  0.00013613796909339726\n","Epoch  12 Batch  51 / 228  Training Loss  0.00010426064545754343\n","Epoch  12 Batch  52 / 228  Training Loss  0.0005759806372225285\n","Epoch  12 Batch  53 / 228  Training Loss  0.0014889742014929652\n","Epoch  12 Batch  54 / 228  Training Loss  0.00010086923430208117\n","Epoch  12 Batch  55 / 228  Training Loss  0.0001848911924753338\n","Epoch  12 Batch  56 / 228  Training Loss  0.00012693430471699685\n","Epoch  12 Batch  57 / 228  Training Loss  0.00011538746912265196\n","Epoch  12 Batch  58 / 228  Training Loss  0.00022342491138260812\n","Epoch  12 Batch  59 / 228  Training Loss  0.00013956551265437156\n","Epoch  12 Batch  60 / 228  Training Loss  0.0005007236613892019\n","Epoch  12 Batch  61 / 228  Training Loss  0.0001617894449736923\n","Epoch  12 Batch  62 / 228  Training Loss  0.00042344070971012115\n","Epoch  12 Batch  63 / 228  Training Loss  0.00013492637663148344\n","Epoch  12 Batch  64 / 228  Training Loss  0.0001518693898105994\n","Epoch  12 Batch  65 / 228  Training Loss  8.548131154384464e-05\n","Epoch  12 Batch  66 / 228  Training Loss  0.00024803931592032313\n","Epoch  12 Batch  67 / 228  Training Loss  0.00010560484224697575\n","Epoch  12 Batch  68 / 228  Training Loss  9.260141814593226e-05\n","Epoch  12 Batch  69 / 228  Training Loss  0.00013826890790369362\n","Epoch  12 Batch  70 / 228  Training Loss  0.00011407483543734998\n","Epoch  12 Batch  71 / 228  Training Loss  0.00011346677638357505\n","Epoch  12 Batch  72 / 228  Training Loss  0.00011988500773441046\n","Epoch  12 Batch  73 / 228  Training Loss  0.0001859678013715893\n","Epoch  12 Batch  74 / 228  Training Loss  0.00011419340444263071\n","Epoch  12 Batch  75 / 228  Training Loss  0.00010076281614601612\n","Epoch  12 Batch  76 / 228  Training Loss  0.00014785779058001935\n","Epoch  12 Batch  77 / 228  Training Loss  0.00013078554184176028\n","Epoch  12 Batch  78 / 228  Training Loss  0.00010367758659413084\n","Epoch  12 Batch  79 / 228  Training Loss  8.181968587450683e-05\n","Epoch  12 Batch  80 / 228  Training Loss  0.00010914402082562447\n","Epoch  12 Batch  81 / 228  Training Loss  0.00020581833086907864\n","Epoch  12 Batch  82 / 228  Training Loss  0.0001402305206283927\n","Epoch  12 Batch  83 / 228  Training Loss  0.00019703374709933996\n","Epoch  12 Batch  84 / 228  Training Loss  7.830336835468188e-05\n","Epoch  12 Batch  85 / 228  Training Loss  0.000144905861816369\n","Epoch  12 Batch  86 / 228  Training Loss  0.00011291216651443392\n","Epoch  12 Batch  87 / 228  Training Loss  8.888052252586931e-05\n","Epoch  12 Batch  88 / 228  Training Loss  6.087377551011741e-05\n","Epoch  12 Batch  89 / 228  Training Loss  7.883227954152972e-05\n","Epoch  12 Batch  90 / 228  Training Loss  0.000158224836923182\n","Epoch  12 Batch  91 / 228  Training Loss  0.0002422352263238281\n","Epoch  12 Batch  92 / 228  Training Loss  6.158505857456475e-05\n","Epoch  12 Batch  93 / 228  Training Loss  0.00011933690257137641\n","Epoch  12 Batch  94 / 228  Training Loss  7.759875006740913e-05\n","Epoch  12 Batch  95 / 228  Training Loss  9.581506310496479e-05\n","Epoch  12 Batch  96 / 228  Training Loss  0.00011049016757169738\n","Epoch  12 Batch  97 / 228  Training Loss  0.0001123386318795383\n","Epoch  12 Batch  98 / 228  Training Loss  7.896576425991952e-05\n","Epoch  12 Batch  99 / 228  Training Loss  7.588987500639632e-05\n","Epoch  12 Batch  100 / 228  Training Loss  8.969246118795127e-05\n","Epoch  12 Batch  101 / 228  Training Loss  6.22775696683675e-05\n","Epoch  12 Batch  102 / 228  Training Loss  0.00011204868496861309\n","Epoch  12 Batch  103 / 228  Training Loss  9.485369082540274e-05\n","Epoch  12 Batch  104 / 228  Training Loss  6.160452903714031e-05\n","Epoch  12 Batch  105 / 228  Training Loss  7.359895971603692e-05\n","Epoch  12 Batch  106 / 228  Training Loss  8.006782445590943e-05\n","Epoch  12 Batch  107 / 228  Training Loss  9.634943126002327e-05\n","Epoch  12 Batch  108 / 228  Training Loss  0.00013799776206724346\n","Epoch  12 Batch  109 / 228  Training Loss  7.751649536658078e-05\n","Epoch  12 Batch  110 / 228  Training Loss  5.98626138526015e-05\n","Epoch  12 Batch  111 / 228  Training Loss  6.840551213826984e-05\n","Epoch  12 Batch  112 / 228  Training Loss  0.00014658809232059866\n","Epoch  12 Batch  113 / 228  Training Loss  0.0002675516589079052\n","Epoch  12 Batch  114 / 228  Training Loss  0.00013791199307888746\n","Epoch  12 Batch  115 / 228  Training Loss  9.753413905855268e-05\n","Epoch  12 Batch  116 / 228  Training Loss  9.829195914790034e-05\n","Epoch  12 Batch  117 / 228  Training Loss  9.452691301703453e-05\n","Epoch  12 Batch  118 / 228  Training Loss  0.00010417051089461893\n","Epoch  12 Batch  119 / 228  Training Loss  9.990675607696176e-05\n","Epoch  12 Batch  120 / 228  Training Loss  6.89707521814853e-05\n","Epoch  12 Batch  121 / 228  Training Loss  8.65017282194458e-05\n","Epoch  12 Batch  122 / 228  Training Loss  0.00019594142213463783\n","Epoch  12 Batch  123 / 228  Training Loss  5.051015250501223e-05\n","Epoch  12 Batch  124 / 228  Training Loss  4.0804618038237095e-05\n","Epoch  12 Batch  125 / 228  Training Loss  9.194752055918798e-05\n","Epoch  12 Batch  126 / 228  Training Loss  9.841836435953155e-05\n","Epoch  12 Batch  127 / 228  Training Loss  8.827474812278524e-05\n","Epoch  12 Batch  128 / 228  Training Loss  6.207615660969168e-05\n","Epoch  12 Batch  129 / 228  Training Loss  0.00011058595555368811\n","Epoch  12 Batch  130 / 228  Training Loss  8.701493788976222e-05\n","Epoch  12 Batch  131 / 228  Training Loss  7.174827624112368e-05\n","Epoch  12 Batch  132 / 228  Training Loss  9.143666829913855e-05\n","Epoch  12 Batch  133 / 228  Training Loss  8.51328150019981e-05\n","Epoch  12 Batch  134 / 228  Training Loss  8.412415627390146e-05\n","Epoch  12 Batch  135 / 228  Training Loss  0.0028889740351587534\n","Epoch  12 Batch  136 / 228  Training Loss  0.0024258217308670282\n","Epoch  12 Batch  137 / 228  Training Loss  0.0001119738444685936\n","Epoch  12 Batch  138 / 228  Training Loss  0.00014815825852565467\n","Epoch  12 Batch  139 / 228  Training Loss  0.00040073925629258156\n","Epoch  12 Batch  140 / 228  Training Loss  8.774026355240494e-05\n","Epoch  12 Batch  141 / 228  Training Loss  0.0001492516021244228\n","Epoch  12 Batch  142 / 228  Training Loss  0.0013054850278422236\n","Epoch  12 Batch  143 / 228  Training Loss  0.00021294524776749313\n","Epoch  12 Batch  144 / 228  Training Loss  9.755077917361632e-05\n","Epoch  12 Batch  145 / 228  Training Loss  0.00012793864880222827\n","Epoch  12 Batch  146 / 228  Training Loss  0.00017311450210399926\n","Epoch  12 Batch  147 / 228  Training Loss  8.99199367268011e-05\n","Epoch  12 Batch  148 / 228  Training Loss  0.00015846421592868865\n","Epoch  12 Batch  149 / 228  Training Loss  0.0001311361265834421\n","Epoch  12 Batch  150 / 228  Training Loss  7.717722473898903e-05\n","Epoch  12 Batch  151 / 228  Training Loss  0.00011637403804343194\n","Epoch  12 Batch  152 / 228  Training Loss  0.00011283658386673778\n","Epoch  12 Batch  153 / 228  Training Loss  9.918769501382485e-05\n","Epoch  12 Batch  154 / 228  Training Loss  9.760561806615442e-05\n","Epoch  12 Batch  155 / 228  Training Loss  0.00010975983605021611\n","Epoch  12 Batch  156 / 228  Training Loss  9.319699165644124e-05\n","Epoch  12 Batch  157 / 228  Training Loss  0.00032630731584504247\n","Epoch  12 Batch  158 / 228  Training Loss  9.242087253369391e-05\n","Epoch  12 Batch  159 / 228  Training Loss  0.0003182188665959984\n","Epoch  12 Batch  160 / 228  Training Loss  0.0002105265884893015\n","Epoch  12 Batch  161 / 228  Training Loss  6.907108763698488e-05\n","Epoch  12 Batch  162 / 228  Training Loss  7.961911614984274e-05\n","Epoch  12 Batch  163 / 228  Training Loss  0.00010762724559754133\n","Epoch  12 Batch  164 / 228  Training Loss  8.096023520920426e-05\n","Epoch  12 Batch  165 / 228  Training Loss  0.00012515332491602749\n","Epoch  12 Batch  166 / 228  Training Loss  0.00011012757749995217\n","Epoch  12 Batch  167 / 228  Training Loss  8.542504656361416e-05\n","Epoch  12 Batch  168 / 228  Training Loss  8.642386819701642e-05\n","Epoch  12 Batch  169 / 228  Training Loss  0.00011055967479478568\n","Epoch  12 Batch  170 / 228  Training Loss  0.00011781535431509838\n","Epoch  12 Batch  171 / 228  Training Loss  8.927294402383268e-05\n","Epoch  12 Batch  172 / 228  Training Loss  6.048024079063907e-05\n","Epoch  12 Batch  173 / 228  Training Loss  0.00024162631598301232\n","Epoch  12 Batch  174 / 228  Training Loss  8.962360880104825e-05\n","Epoch  12 Batch  175 / 228  Training Loss  8.107251778710634e-05\n","Epoch  12 Batch  176 / 228  Training Loss  6.672944437013939e-05\n","Epoch  12 Batch  177 / 228  Training Loss  7.551533781224862e-05\n","Epoch  12 Batch  178 / 228  Training Loss  6.892868259456009e-05\n","Epoch  12 Batch  179 / 228  Training Loss  7.710889622103423e-05\n","Epoch  12 Batch  180 / 228  Training Loss  0.00012572857667692006\n","Epoch  12 Batch  181 / 228  Training Loss  0.00014117923274170607\n","Epoch  12 Batch  182 / 228  Training Loss  9.293181938119233e-05\n","Epoch  12 Batch  183 / 228  Training Loss  6.146066880319268e-05\n","Epoch  12 Batch  184 / 228  Training Loss  5.361238800105639e-05\n","Epoch  12 Batch  185 / 228  Training Loss  0.0001326255442108959\n","Epoch  12 Batch  186 / 228  Training Loss  0.00012827017053496093\n","Epoch  12 Batch  187 / 228  Training Loss  0.00015180083573795855\n","Epoch  12 Batch  188 / 228  Training Loss  8.851380698615685e-05\n","Epoch  12 Batch  189 / 228  Training Loss  0.0001468085974920541\n","Epoch  12 Batch  190 / 228  Training Loss  6.456732808146626e-05\n","Epoch  12 Batch  191 / 228  Training Loss  0.00012246225378476083\n","Epoch  12 Batch  192 / 228  Training Loss  6.609882257180288e-05\n","Epoch  12 Batch  193 / 228  Training Loss  0.00011797876504715532\n","Epoch  12 Batch  194 / 228  Training Loss  6.810152990510687e-05\n","Epoch  12 Batch  195 / 228  Training Loss  0.00023838566266931593\n","Epoch  12 Batch  196 / 228  Training Loss  6.135697185527533e-05\n","Epoch  12 Batch  197 / 228  Training Loss  0.00010443772043799981\n","Epoch  12 Batch  198 / 228  Training Loss  0.00012660340871661901\n","Epoch  12 Batch  199 / 228  Training Loss  7.118604116840288e-05\n","Epoch  12 Batch  200 / 228  Training Loss  7.242010906338692e-05\n","Epoch  12 Batch  201 / 228  Training Loss  4.836487278225832e-05\n","Epoch  12 Batch  202 / 228  Training Loss  5.969565245322883e-05\n","Epoch  12 Batch  203 / 228  Training Loss  7.050381100270897e-05\n","Epoch  12 Batch  204 / 228  Training Loss  6.139867764431983e-05\n","Epoch  12 Batch  205 / 228  Training Loss  5.563734521274455e-05\n","Epoch  12 Batch  206 / 228  Training Loss  7.471444405382499e-05\n","Epoch  12 Batch  207 / 228  Training Loss  0.00015807095041964203\n","Epoch  12 Batch  208 / 228  Training Loss  7.999622175702825e-05\n","Epoch  12 Batch  209 / 228  Training Loss  7.931060099508613e-05\n","Epoch  12 Batch  210 / 228  Training Loss  4.586802970152348e-05\n","Epoch  12 Batch  211 / 228  Training Loss  8.659476588945836e-05\n","Epoch  12 Batch  212 / 228  Training Loss  0.00020528375171124935\n","Epoch  12 Batch  213 / 228  Training Loss  5.6952208979055285e-05\n","Epoch  12 Batch  214 / 228  Training Loss  7.198983075795695e-05\n","Epoch  12 Batch  215 / 228  Training Loss  0.00012267357669770718\n","Epoch  12 Batch  216 / 228  Training Loss  0.00016175037308130413\n","Epoch  12 Batch  217 / 228  Training Loss  0.000116397874080576\n","Epoch  12 Batch  218 / 228  Training Loss  7.661648123757914e-05\n","Epoch  12 Batch  219 / 228  Training Loss  9.665654215496033e-05\n","Epoch  12 Batch  220 / 228  Training Loss  7.467945397365838e-05\n","Epoch  12 Batch  221 / 228  Training Loss  6.383465370163321e-05\n","Epoch  12 Batch  222 / 228  Training Loss  4.024297959404066e-05\n","Epoch  12 Batch  223 / 228  Training Loss  6.79460063111037e-05\n","Epoch  12 Batch  224 / 228  Training Loss  4.8057980166049674e-05\n","Epoch  12 Batch  225 / 228  Training Loss  4.952572635374963e-05\n","Epoch  12 Batch  226 / 228  Training Loss  4.937285484629683e-05\n","Epoch  12 Batch  227 / 228  Training Loss  8.242634066846222e-05\n","  13    |    -    |   0.000231   | 99.123476\n","----------------------------------------------------------------------\n","Running epoch: 13\n","Epoch  13 Batch  0 / 228  Training Loss  7.474759331671521e-05\n","Epoch  13 Batch  1 / 228  Training Loss  8.822692325338721e-05\n","Epoch  13 Batch  2 / 228  Training Loss  5.160821456229314e-05\n","Epoch  13 Batch  3 / 228  Training Loss  7.395724242087454e-05\n","Epoch  13 Batch  4 / 228  Training Loss  3.742923217942007e-05\n","Epoch  13 Batch  5 / 228  Training Loss  8.423522376688197e-05\n","Epoch  13 Batch  6 / 228  Training Loss  0.00013341449084691703\n","Epoch  13 Batch  7 / 228  Training Loss  7.140198431443423e-05\n","Epoch  13 Batch  8 / 228  Training Loss  8.278066525235772e-05\n","Epoch  13 Batch  9 / 228  Training Loss  9.660639625508338e-05\n","Epoch  13 Batch  10 / 228  Training Loss  5.6072451116051525e-05\n","Epoch  13 Batch  11 / 228  Training Loss  7.231372001115233e-05\n","Epoch  13 Batch  12 / 228  Training Loss  6.536364526255056e-05\n","Epoch  13 Batch  13 / 228  Training Loss  5.84902118134778e-05\n","Epoch  13 Batch  14 / 228  Training Loss  0.004237879998981953\n","Epoch  13 Batch  15 / 228  Training Loss  0.0009035737020894885\n","Epoch  13 Batch  16 / 228  Training Loss  0.00015291110321413726\n","Epoch  13 Batch  17 / 228  Training Loss  0.00012007636541966349\n","Epoch  13 Batch  18 / 228  Training Loss  0.0022521698847413063\n","Epoch  13 Batch  19 / 228  Training Loss  0.0003020755830220878\n","Epoch  13 Batch  20 / 228  Training Loss  9.388805483467877e-05\n","Epoch  13 Batch  21 / 228  Training Loss  9.483941539656371e-05\n","Epoch  13 Batch  22 / 228  Training Loss  0.0001336266432190314\n","Epoch  13 Batch  23 / 228  Training Loss  0.0001163540146080777\n","Epoch  13 Batch  24 / 228  Training Loss  6.836747343186289e-05\n","Epoch  13 Batch  25 / 228  Training Loss  0.0001226270105689764\n","Epoch  13 Batch  26 / 228  Training Loss  0.00010392279364168644\n","Epoch  13 Batch  27 / 228  Training Loss  9.108282392844558e-05\n","Epoch  13 Batch  28 / 228  Training Loss  4.811781764146872e-05\n","Epoch  13 Batch  29 / 228  Training Loss  0.00011756506137317047\n","Epoch  13 Batch  30 / 228  Training Loss  0.00024412665516138077\n","Epoch  13 Batch  31 / 228  Training Loss  9.88734100246802e-05\n","Epoch  13 Batch  32 / 228  Training Loss  0.00011758222535718232\n","Epoch  13 Batch  33 / 228  Training Loss  9.189695992972702e-05\n","Epoch  13 Batch  34 / 228  Training Loss  8.703276398591697e-05\n","Epoch  13 Batch  35 / 228  Training Loss  0.00016013815184123814\n","Epoch  13 Batch  36 / 228  Training Loss  9.872089140117168e-05\n","Epoch  13 Batch  37 / 228  Training Loss  0.00010597925574984401\n","Epoch  13 Batch  38 / 228  Training Loss  8.664785127621144e-05\n","Epoch  13 Batch  39 / 228  Training Loss  8.391501614823937e-05\n","Epoch  13 Batch  40 / 228  Training Loss  7.51943516661413e-05\n","Epoch  13 Batch  41 / 228  Training Loss  6.767736340407282e-05\n","Epoch  13 Batch  42 / 228  Training Loss  8.636158599983901e-05\n","Epoch  13 Batch  43 / 228  Training Loss  7.799260492902249e-05\n","Epoch  13 Batch  44 / 228  Training Loss  7.801788888173178e-05\n","Epoch  13 Batch  45 / 228  Training Loss  7.702266884734854e-05\n","Epoch  13 Batch  46 / 228  Training Loss  7.371239189524204e-05\n","Epoch  13 Batch  47 / 228  Training Loss  0.00010651080083334818\n","Epoch  13 Batch  48 / 228  Training Loss  7.12679247953929e-05\n","Epoch  13 Batch  49 / 228  Training Loss  0.00012064656766597182\n","Epoch  13 Batch  50 / 228  Training Loss  5.68471041333396e-05\n","Epoch  13 Batch  51 / 228  Training Loss  0.00020269448577892035\n","Epoch  13 Batch  52 / 228  Training Loss  7.009691762505099e-05\n","Epoch  13 Batch  53 / 228  Training Loss  9.853520896285772e-05\n","Epoch  13 Batch  54 / 228  Training Loss  8.51956574479118e-05\n","Epoch  13 Batch  55 / 228  Training Loss  8.982069266494364e-05\n","Epoch  13 Batch  56 / 228  Training Loss  8.113592775771394e-05\n","Epoch  13 Batch  57 / 228  Training Loss  6.397839024430141e-05\n","Epoch  13 Batch  58 / 228  Training Loss  7.675661618122831e-05\n","Epoch  13 Batch  59 / 228  Training Loss  6.596794992219657e-05\n","Epoch  13 Batch  60 / 228  Training Loss  8.41072978801094e-05\n","Epoch  13 Batch  61 / 228  Training Loss  8.154170063789934e-05\n","Epoch  13 Batch  62 / 228  Training Loss  0.00014830849249847233\n","Epoch  13 Batch  63 / 228  Training Loss  5.508006506715901e-05\n","Epoch  13 Batch  64 / 228  Training Loss  8.919399988371879e-05\n","Epoch  13 Batch  65 / 228  Training Loss  6.889505311846733e-05\n","Epoch  13 Batch  66 / 228  Training Loss  0.00019959555356763303\n","Epoch  13 Batch  67 / 228  Training Loss  9.232565207639709e-05\n","Epoch  13 Batch  68 / 228  Training Loss  7.649496546946466e-05\n","Epoch  13 Batch  69 / 228  Training Loss  6.841916183475405e-05\n","Epoch  13 Batch  70 / 228  Training Loss  7.17743459972553e-05\n","Epoch  13 Batch  71 / 228  Training Loss  8.394456381211057e-05\n","Epoch  13 Batch  72 / 228  Training Loss  9.589157707523555e-05\n","Epoch  13 Batch  73 / 228  Training Loss  9.30919632082805e-05\n","Epoch  13 Batch  74 / 228  Training Loss  4.520715810940601e-05\n","Epoch  13 Batch  75 / 228  Training Loss  7.237178215291351e-05\n","Epoch  13 Batch  76 / 228  Training Loss  0.00010874379950109869\n","Epoch  13 Batch  77 / 228  Training Loss  0.00010733322415035218\n","Epoch  13 Batch  78 / 228  Training Loss  5.257220982457511e-05\n","Epoch  13 Batch  79 / 228  Training Loss  5.766631875303574e-05\n","Epoch  13 Batch  80 / 228  Training Loss  9.766294533619657e-05\n","Epoch  13 Batch  81 / 228  Training Loss  7.33252745703794e-05\n","Epoch  13 Batch  82 / 228  Training Loss  8.362691733054817e-05\n","Epoch  13 Batch  83 / 228  Training Loss  6.201623182278126e-05\n","Epoch  13 Batch  84 / 228  Training Loss  7.797378930263221e-05\n","Epoch  13 Batch  85 / 228  Training Loss  5.520003105630167e-05\n","Epoch  13 Batch  86 / 228  Training Loss  6.742354162270203e-05\n","Epoch  13 Batch  87 / 228  Training Loss  0.00011603155871853232\n","Epoch  13 Batch  88 / 228  Training Loss  8.650963718537241e-05\n","Epoch  13 Batch  89 / 228  Training Loss  7.175109931267798e-05\n","Epoch  13 Batch  90 / 228  Training Loss  4.8532732762396336e-05\n","Epoch  13 Batch  91 / 228  Training Loss  4.623006680049002e-05\n","Epoch  13 Batch  92 / 228  Training Loss  9.386213787365705e-05\n","Epoch  13 Batch  93 / 228  Training Loss  8.056947990553454e-05\n","Epoch  13 Batch  94 / 228  Training Loss  0.00010469871631357819\n","Epoch  13 Batch  95 / 228  Training Loss  7.462505163857713e-05\n","Epoch  13 Batch  96 / 228  Training Loss  4.975586125510745e-05\n","Epoch  13 Batch  97 / 228  Training Loss  7.134904444683343e-05\n","Epoch  13 Batch  98 / 228  Training Loss  4.940888538840227e-05\n","Epoch  13 Batch  99 / 228  Training Loss  6.256617052713409e-05\n","Epoch  13 Batch  100 / 228  Training Loss  7.471551361959428e-05\n","Epoch  13 Batch  101 / 228  Training Loss  5.521826824406162e-05\n","Epoch  13 Batch  102 / 228  Training Loss  5.670816244673915e-05\n","Epoch  13 Batch  103 / 228  Training Loss  4.1184466681443155e-05\n","Epoch  13 Batch  104 / 228  Training Loss  8.421437087235972e-05\n","Epoch  13 Batch  105 / 228  Training Loss  5.558874545386061e-05\n","Epoch  13 Batch  106 / 228  Training Loss  9.46686850511469e-05\n","Epoch  13 Batch  107 / 228  Training Loss  6.856558320578188e-05\n","Epoch  13 Batch  108 / 228  Training Loss  5.869184315088205e-05\n","Epoch  13 Batch  109 / 228  Training Loss  8.217163849622011e-05\n","Epoch  13 Batch  110 / 228  Training Loss  0.00012853088264819235\n","Epoch  13 Batch  111 / 228  Training Loss  7.020669727353379e-05\n","Epoch  13 Batch  112 / 228  Training Loss  6.960501195862889e-05\n","Epoch  13 Batch  113 / 228  Training Loss  7.00300806784071e-05\n","Epoch  13 Batch  114 / 228  Training Loss  9.195691382046789e-05\n","Epoch  13 Batch  115 / 228  Training Loss  6.414618110284209e-05\n","Epoch  13 Batch  116 / 228  Training Loss  8.622190216556191e-05\n","Epoch  13 Batch  117 / 228  Training Loss  7.688235928071663e-05\n","Epoch  13 Batch  118 / 228  Training Loss  6.489734369097278e-05\n","Epoch  13 Batch  119 / 228  Training Loss  8.436375355813652e-05\n","Epoch  13 Batch  120 / 228  Training Loss  6.286876305239275e-05\n","Epoch  13 Batch  121 / 228  Training Loss  5.46825940546114e-05\n","Epoch  13 Batch  122 / 228  Training Loss  6.257920176722109e-05\n","Epoch  13 Batch  123 / 228  Training Loss  8.882380643626675e-05\n","Epoch  13 Batch  124 / 228  Training Loss  4.8070814955281094e-05\n","Epoch  13 Batch  125 / 228  Training Loss  6.9373709266074e-05\n","Epoch  13 Batch  126 / 228  Training Loss  5.7144083257298917e-05\n","Epoch  13 Batch  127 / 228  Training Loss  4.519606591202319e-05\n","Epoch  13 Batch  128 / 228  Training Loss  7.746709889033809e-05\n","Epoch  13 Batch  129 / 228  Training Loss  7.441075285896659e-05\n","Epoch  13 Batch  130 / 228  Training Loss  6.237291381694376e-05\n","Epoch  13 Batch  131 / 228  Training Loss  0.00013643284910358489\n","Epoch  13 Batch  132 / 228  Training Loss  4.7835907025728375e-05\n","Epoch  13 Batch  133 / 228  Training Loss  9.049314394360408e-05\n","Epoch  13 Batch  134 / 228  Training Loss  4.917347541777417e-05\n","Epoch  13 Batch  135 / 228  Training Loss  3.8329220842570066e-05\n","Epoch  13 Batch  136 / 228  Training Loss  7.4345400207676e-05\n","Epoch  13 Batch  137 / 228  Training Loss  6.949807720957324e-05\n","Epoch  13 Batch  138 / 228  Training Loss  5.596902337856591e-05\n","Epoch  13 Batch  139 / 228  Training Loss  8.31815050332807e-05\n","Epoch  13 Batch  140 / 228  Training Loss  9.068764484254643e-05\n","Epoch  13 Batch  141 / 228  Training Loss  7.543915853602812e-05\n","Epoch  13 Batch  142 / 228  Training Loss  6.713227776344866e-05\n","Epoch  13 Batch  143 / 228  Training Loss  7.43836208130233e-05\n","Epoch  13 Batch  144 / 228  Training Loss  5.545157546293922e-05\n","Epoch  13 Batch  145 / 228  Training Loss  6.135299918241799e-05\n","Epoch  13 Batch  146 / 228  Training Loss  6.48449786240235e-05\n","Epoch  13 Batch  147 / 228  Training Loss  8.363744564121589e-05\n","Epoch  13 Batch  148 / 228  Training Loss  6.0714355640811846e-05\n","Epoch  13 Batch  149 / 228  Training Loss  7.633758650626987e-05\n","Epoch  13 Batch  150 / 228  Training Loss  4.987337160855532e-05\n","Epoch  13 Batch  151 / 228  Training Loss  7.009769615251571e-05\n","Epoch  13 Batch  152 / 228  Training Loss  7.984910189406946e-05\n","Epoch  13 Batch  153 / 228  Training Loss  5.7368721172679216e-05\n","Epoch  13 Batch  154 / 228  Training Loss  7.81441485742107e-05\n","Epoch  13 Batch  155 / 228  Training Loss  5.399391375249252e-05\n","Epoch  13 Batch  156 / 228  Training Loss  3.113905404461548e-05\n","Epoch  13 Batch  157 / 228  Training Loss  4.854004509979859e-05\n","Epoch  13 Batch  158 / 228  Training Loss  4.72581377835013e-05\n","Epoch  13 Batch  159 / 228  Training Loss  4.733466630568728e-05\n","Epoch  13 Batch  160 / 228  Training Loss  6.440564175136387e-05\n","Epoch  13 Batch  161 / 228  Training Loss  4.874852311331779e-05\n","Epoch  13 Batch  162 / 228  Training Loss  6.525438220705837e-05\n","Epoch  13 Batch  163 / 228  Training Loss  7.267417095135897e-05\n","Epoch  13 Batch  164 / 228  Training Loss  0.00013327828492037952\n","Epoch  13 Batch  165 / 228  Training Loss  6.845332245575264e-05\n","Epoch  13 Batch  166 / 228  Training Loss  5.6844768550945446e-05\n","Epoch  13 Batch  167 / 228  Training Loss  5.357159534469247e-05\n","Epoch  13 Batch  168 / 228  Training Loss  4.53971151728183e-05\n","Epoch  13 Batch  169 / 228  Training Loss  5.972450162516907e-05\n","Epoch  13 Batch  170 / 228  Training Loss  0.0001094940016628243\n","Epoch  13 Batch  171 / 228  Training Loss  6.491453677881509e-05\n","Epoch  13 Batch  172 / 228  Training Loss  8.527571480954066e-05\n","Epoch  13 Batch  173 / 228  Training Loss  5.6616041547385976e-05\n","Epoch  13 Batch  174 / 228  Training Loss  4.1597937524784356e-05\n","Epoch  13 Batch  175 / 228  Training Loss  4.730709406430833e-05\n","Epoch  13 Batch  176 / 228  Training Loss  6.632234726566821e-05\n","Epoch  13 Batch  177 / 228  Training Loss  7.241209095809609e-05\n","Epoch  13 Batch  178 / 228  Training Loss  4.741484735859558e-05\n","Epoch  13 Batch  179 / 228  Training Loss  9.771282930159941e-05\n","Epoch  13 Batch  180 / 228  Training Loss  8.297826570924371e-05\n","Epoch  13 Batch  181 / 228  Training Loss  4.277507832739502e-05\n","Epoch  13 Batch  182 / 228  Training Loss  6.0409161960706115e-05\n","Epoch  13 Batch  183 / 228  Training Loss  7.96198146417737e-05\n","Epoch  13 Batch  184 / 228  Training Loss  5.6199067330453545e-05\n","Epoch  13 Batch  185 / 228  Training Loss  8.63745590322651e-05\n","Epoch  13 Batch  186 / 228  Training Loss  6.783922435715795e-05\n","Epoch  13 Batch  187 / 228  Training Loss  3.9494159864261746e-05\n","Epoch  13 Batch  188 / 228  Training Loss  4.8341094952775165e-05\n","Epoch  13 Batch  189 / 228  Training Loss  7.480934436898679e-05\n","Epoch  13 Batch  190 / 228  Training Loss  4.398734017740935e-05\n","Epoch  13 Batch  191 / 228  Training Loss  5.092606443213299e-05\n","Epoch  13 Batch  192 / 228  Training Loss  5.0039107009069994e-05\n","Epoch  13 Batch  193 / 228  Training Loss  6.688063149340451e-05\n","Epoch  13 Batch  194 / 228  Training Loss  5.3569034207612276e-05\n","Epoch  13 Batch  195 / 228  Training Loss  4.377034929348156e-05\n","Epoch  13 Batch  196 / 228  Training Loss  0.00012031893857056275\n","Epoch  13 Batch  197 / 228  Training Loss  7.655190711375326e-05\n","Epoch  13 Batch  198 / 228  Training Loss  4.721782170236111e-05\n","Epoch  13 Batch  199 / 228  Training Loss  4.7079549403861165e-05\n","Epoch  13 Batch  200 / 228  Training Loss  5.026154758525081e-05\n","Epoch  13 Batch  201 / 228  Training Loss  0.0001367974909953773\n","Epoch  13 Batch  202 / 228  Training Loss  4.1895204049069434e-05\n","Epoch  13 Batch  203 / 228  Training Loss  3.8887737900950015e-05\n","Epoch  13 Batch  204 / 228  Training Loss  5.2292754844529554e-05\n","Epoch  13 Batch  205 / 228  Training Loss  5.796700861537829e-05\n","Epoch  13 Batch  206 / 228  Training Loss  4.6539262257283553e-05\n","Epoch  13 Batch  207 / 228  Training Loss  0.00010595618368824944\n","Epoch  13 Batch  208 / 228  Training Loss  5.50707227375824e-05\n","Epoch  13 Batch  209 / 228  Training Loss  5.388522913563065e-05\n","Epoch  13 Batch  210 / 228  Training Loss  5.2038707508472726e-05\n","Epoch  13 Batch  211 / 228  Training Loss  5.8116762374993414e-05\n","Epoch  13 Batch  212 / 228  Training Loss  4.160136450082064e-05\n","Epoch  13 Batch  213 / 228  Training Loss  5.2496714488370344e-05\n","Epoch  13 Batch  214 / 228  Training Loss  6.402659346349537e-05\n","Epoch  13 Batch  215 / 228  Training Loss  4.0940554754342884e-05\n","Epoch  13 Batch  216 / 228  Training Loss  6.62125603412278e-05\n","Epoch  13 Batch  217 / 228  Training Loss  4.685424573835917e-05\n","Epoch  13 Batch  218 / 228  Training Loss  0.00022510772396344692\n","Epoch  13 Batch  219 / 228  Training Loss  5.518325633602217e-05\n","Epoch  13 Batch  220 / 228  Training Loss  4.124292900087312e-05\n","Epoch  13 Batch  221 / 228  Training Loss  6.646181282121688e-05\n","Epoch  13 Batch  222 / 228  Training Loss  5.08771809109021e-05\n","Epoch  13 Batch  223 / 228  Training Loss  6.604331429116428e-05\n","Epoch  13 Batch  224 / 228  Training Loss  6.463158933911473e-05\n","Epoch  13 Batch  225 / 228  Training Loss  3.879594078171067e-05\n","Epoch  13 Batch  226 / 228  Training Loss  6.998305616434664e-05\n","Epoch  13 Batch  227 / 228  Training Loss  3.580205884645693e-05\n","  14    |    -    |   0.000108   | 99.161585\n","----------------------------------------------------------------------\n","Running epoch: 14\n","Epoch  14 Batch  0 / 228  Training Loss  7.724262832198292e-05\n","Epoch  14 Batch  1 / 228  Training Loss  5.2452680392889306e-05\n","Epoch  14 Batch  2 / 228  Training Loss  4.813650230062194e-05\n","Epoch  14 Batch  3 / 228  Training Loss  7.452993304468691e-05\n","Epoch  14 Batch  4 / 228  Training Loss  5.247590161161497e-05\n","Epoch  14 Batch  5 / 228  Training Loss  6.494922854471952e-05\n","Epoch  14 Batch  6 / 228  Training Loss  7.369588274741545e-05\n","Epoch  14 Batch  7 / 228  Training Loss  5.052236883784644e-05\n","Epoch  14 Batch  8 / 228  Training Loss  3.580570773920044e-05\n","Epoch  14 Batch  9 / 228  Training Loss  7.827868103049695e-05\n","Epoch  14 Batch  10 / 228  Training Loss  4.0292648918693885e-05\n","Epoch  14 Batch  11 / 228  Training Loss  8.023842383408919e-05\n","Epoch  14 Batch  12 / 228  Training Loss  7.439309410983697e-05\n","Epoch  14 Batch  13 / 228  Training Loss  7.433655264321715e-05\n","Epoch  14 Batch  14 / 228  Training Loss  6.140668119769543e-05\n","Epoch  14 Batch  15 / 228  Training Loss  6.043766188668087e-05\n","Epoch  14 Batch  16 / 228  Training Loss  3.69203116861172e-05\n","Epoch  14 Batch  17 / 228  Training Loss  5.3351202950580046e-05\n","Epoch  14 Batch  18 / 228  Training Loss  7.142058166209608e-05\n","Epoch  14 Batch  19 / 228  Training Loss  5.27563170180656e-05\n","Epoch  14 Batch  20 / 228  Training Loss  5.3366791689768434e-05\n","Epoch  14 Batch  21 / 228  Training Loss  4.668124165618792e-05\n","Epoch  14 Batch  22 / 228  Training Loss  6.031552402419038e-05\n","Epoch  14 Batch  23 / 228  Training Loss  5.750164928031154e-05\n","Epoch  14 Batch  24 / 228  Training Loss  3.817726974375546e-05\n","Epoch  14 Batch  25 / 228  Training Loss  4.371126851765439e-05\n","Epoch  14 Batch  26 / 228  Training Loss  4.4200300180818886e-05\n","Epoch  14 Batch  27 / 228  Training Loss  5.7136854593409225e-05\n","Epoch  14 Batch  28 / 228  Training Loss  5.038386007072404e-05\n","Epoch  14 Batch  29 / 228  Training Loss  3.8209698686841875e-05\n","Epoch  14 Batch  30 / 228  Training Loss  8.861910464474931e-05\n","Epoch  14 Batch  31 / 228  Training Loss  5.7771936553763226e-05\n","Epoch  14 Batch  32 / 228  Training Loss  4.7112785978242755e-05\n","Epoch  14 Batch  33 / 228  Training Loss  5.323383447830565e-05\n","Epoch  14 Batch  34 / 228  Training Loss  3.446009213803336e-05\n","Epoch  14 Batch  35 / 228  Training Loss  5.604272155323997e-05\n","Epoch  14 Batch  36 / 228  Training Loss  5.211405732552521e-05\n","Epoch  14 Batch  37 / 228  Training Loss  5.19656459800899e-05\n","Epoch  14 Batch  38 / 228  Training Loss  4.33186323789414e-05\n","Epoch  14 Batch  39 / 228  Training Loss  4.788354999618605e-05\n","Epoch  14 Batch  40 / 228  Training Loss  4.7774323320481926e-05\n","Epoch  14 Batch  41 / 228  Training Loss  5.6316290283575654e-05\n","Epoch  14 Batch  42 / 228  Training Loss  6.51630834909156e-05\n","Epoch  14 Batch  43 / 228  Training Loss  5.499923645402305e-05\n","Epoch  14 Batch  44 / 228  Training Loss  3.088777884840965e-05\n","Epoch  14 Batch  45 / 228  Training Loss  5.7629524235380813e-05\n","Epoch  14 Batch  46 / 228  Training Loss  5.2180646889610216e-05\n","Epoch  14 Batch  47 / 228  Training Loss  5.2382256399141625e-05\n","Epoch  14 Batch  48 / 228  Training Loss  4.518016430665739e-05\n","Epoch  14 Batch  49 / 228  Training Loss  3.896910129697062e-05\n","Epoch  14 Batch  50 / 228  Training Loss  6.303907139226794e-05\n","Epoch  14 Batch  51 / 228  Training Loss  6.299082451732829e-05\n","Epoch  14 Batch  52 / 228  Training Loss  5.239359597908333e-05\n","Epoch  14 Batch  53 / 228  Training Loss  4.491658910410479e-05\n","Epoch  14 Batch  54 / 228  Training Loss  4.36282207374461e-05\n","Epoch  14 Batch  55 / 228  Training Loss  2.4106031560222618e-05\n","Epoch  14 Batch  56 / 228  Training Loss  7.309720967896283e-05\n","Epoch  14 Batch  57 / 228  Training Loss  5.36848274350632e-05\n","Epoch  14 Batch  58 / 228  Training Loss  4.592830373439938e-05\n","Epoch  14 Batch  59 / 228  Training Loss  4.100141450180672e-05\n","Epoch  14 Batch  60 / 228  Training Loss  4.465792153496295e-05\n","Epoch  14 Batch  61 / 228  Training Loss  4.4595686631510034e-05\n","Epoch  14 Batch  62 / 228  Training Loss  7.354297849815339e-05\n","Epoch  14 Batch  63 / 228  Training Loss  3.4613585739862174e-05\n","Epoch  14 Batch  64 / 228  Training Loss  5.142243389855139e-05\n","Epoch  14 Batch  65 / 228  Training Loss  6.889995711389929e-05\n","Epoch  14 Batch  66 / 228  Training Loss  3.818123150267638e-05\n","Epoch  14 Batch  67 / 228  Training Loss  4.943805470247753e-05\n","Epoch  14 Batch  68 / 228  Training Loss  3.936903158319183e-05\n","Epoch  14 Batch  69 / 228  Training Loss  5.1083170546917245e-05\n","Epoch  14 Batch  70 / 228  Training Loss  5.5425560276489705e-05\n","Epoch  14 Batch  71 / 228  Training Loss  3.8430596760008484e-05\n","Epoch  14 Batch  72 / 228  Training Loss  4.764193363371305e-05\n","Epoch  14 Batch  73 / 228  Training Loss  5.08331686432939e-05\n","Epoch  14 Batch  74 / 228  Training Loss  8.159891876857728e-05\n","Epoch  14 Batch  75 / 228  Training Loss  4.9465343181509525e-05\n","Epoch  14 Batch  76 / 228  Training Loss  4.884774534730241e-05\n","Epoch  14 Batch  77 / 228  Training Loss  5.482095366460271e-05\n","Epoch  14 Batch  78 / 228  Training Loss  4.541110683931038e-05\n","Epoch  14 Batch  79 / 228  Training Loss  6.155889423098415e-05\n","Epoch  14 Batch  80 / 228  Training Loss  3.521766484482214e-05\n","Epoch  14 Batch  81 / 228  Training Loss  4.4282871385803446e-05\n","Epoch  14 Batch  82 / 228  Training Loss  5.301782948663458e-05\n","Epoch  14 Batch  83 / 228  Training Loss  5.833658360643312e-05\n","Epoch  14 Batch  84 / 228  Training Loss  7.146981806727126e-05\n","Epoch  14 Batch  85 / 228  Training Loss  7.713783998042345e-05\n","Epoch  14 Batch  86 / 228  Training Loss  6.1254613683559e-05\n","Epoch  14 Batch  87 / 228  Training Loss  4.402915510581806e-05\n","Epoch  14 Batch  88 / 228  Training Loss  5.5174954468384385e-05\n","Epoch  14 Batch  89 / 228  Training Loss  6.95406342856586e-05\n","Epoch  14 Batch  90 / 228  Training Loss  5.3418265451909974e-05\n","Epoch  14 Batch  91 / 228  Training Loss  3.6854296922683716e-05\n","Epoch  14 Batch  92 / 228  Training Loss  4.8433375923195854e-05\n","Epoch  14 Batch  93 / 228  Training Loss  5.9862031775992364e-05\n","Epoch  14 Batch  94 / 228  Training Loss  7.39322422305122e-05\n","Epoch  14 Batch  95 / 228  Training Loss  3.889440995408222e-05\n","Epoch  14 Batch  96 / 228  Training Loss  4.630345938494429e-05\n","Epoch  14 Batch  97 / 228  Training Loss  6.215703615453094e-05\n","Epoch  14 Batch  98 / 228  Training Loss  5.706943920813501e-05\n","Epoch  14 Batch  99 / 228  Training Loss  4.735268157674e-05\n","Epoch  14 Batch  100 / 228  Training Loss  4.5010798203293234e-05\n","Epoch  14 Batch  101 / 228  Training Loss  4.314525722293183e-05\n","Epoch  14 Batch  102 / 228  Training Loss  4.709698987426236e-05\n","Epoch  14 Batch  103 / 228  Training Loss  2.828383912856225e-05\n","Epoch  14 Batch  104 / 228  Training Loss  4.9589107220526785e-05\n","Epoch  14 Batch  105 / 228  Training Loss  4.535325933829881e-05\n","Epoch  14 Batch  106 / 228  Training Loss  4.417369927978143e-05\n","Epoch  14 Batch  107 / 228  Training Loss  6.087829387979582e-05\n","Epoch  14 Batch  108 / 228  Training Loss  3.889323488692753e-05\n","Epoch  14 Batch  109 / 228  Training Loss  6.275079067563638e-05\n","Epoch  14 Batch  110 / 228  Training Loss  4.899054692941718e-05\n","Epoch  14 Batch  111 / 228  Training Loss  4.628655005944893e-05\n","Epoch  14 Batch  112 / 228  Training Loss  6.456619303207844e-05\n","Epoch  14 Batch  113 / 228  Training Loss  4.795967834070325e-05\n","Epoch  14 Batch  114 / 228  Training Loss  5.2423361921682954e-05\n","Epoch  14 Batch  115 / 228  Training Loss  4.498141788644716e-05\n","Epoch  14 Batch  116 / 228  Training Loss  3.960947651648894e-05\n","Epoch  14 Batch  117 / 228  Training Loss  4.008723044535145e-05\n","Epoch  14 Batch  118 / 228  Training Loss  6.403630686691031e-05\n","Epoch  14 Batch  119 / 228  Training Loss  5.912911728955805e-05\n","Epoch  14 Batch  120 / 228  Training Loss  4.4618638639803976e-05\n","Epoch  14 Batch  121 / 228  Training Loss  3.727735747816041e-05\n","Epoch  14 Batch  122 / 228  Training Loss  5.967960532871075e-05\n","Epoch  14 Batch  123 / 228  Training Loss  8.897943916963413e-05\n","Epoch  14 Batch  124 / 228  Training Loss  4.0247505239676684e-05\n","Epoch  14 Batch  125 / 228  Training Loss  7.261418068083003e-05\n","Epoch  14 Batch  126 / 228  Training Loss  6.54388204566203e-05\n","Epoch  14 Batch  127 / 228  Training Loss  3.487364301690832e-05\n","Epoch  14 Batch  128 / 228  Training Loss  6.122987542767078e-05\n","Epoch  14 Batch  129 / 228  Training Loss  2.5098202968365513e-05\n","Epoch  14 Batch  130 / 228  Training Loss  4.3168634874746203e-05\n","Epoch  14 Batch  131 / 228  Training Loss  4.371420800453052e-05\n","Epoch  14 Batch  132 / 228  Training Loss  4.309897485654801e-05\n","Epoch  14 Batch  133 / 228  Training Loss  6.865397153887898e-05\n","Epoch  14 Batch  134 / 228  Training Loss  3.5407130781095475e-05\n","Epoch  14 Batch  135 / 228  Training Loss  6.207211845321581e-05\n","Epoch  14 Batch  136 / 228  Training Loss  3.724415000760928e-05\n","Epoch  14 Batch  137 / 228  Training Loss  4.4743381295120344e-05\n","Epoch  14 Batch  138 / 228  Training Loss  3.164940790156834e-05\n","Epoch  14 Batch  139 / 228  Training Loss  4.620000981958583e-05\n","Epoch  14 Batch  140 / 228  Training Loss  3.516072320053354e-05\n","Epoch  14 Batch  141 / 228  Training Loss  4.2883701098617166e-05\n","Epoch  14 Batch  142 / 228  Training Loss  3.0790732125751674e-05\n","Epoch  14 Batch  143 / 228  Training Loss  4.831477417610586e-05\n","Epoch  14 Batch  144 / 228  Training Loss  4.515819455264136e-05\n","Epoch  14 Batch  145 / 228  Training Loss  7.262143481057137e-05\n","Epoch  14 Batch  146 / 228  Training Loss  5.655729182763025e-05\n","Epoch  14 Batch  147 / 228  Training Loss  4.5087665057508275e-05\n","Epoch  14 Batch  148 / 228  Training Loss  4.282249210518785e-05\n","Epoch  14 Batch  149 / 228  Training Loss  4.151787288719788e-05\n","Epoch  14 Batch  150 / 228  Training Loss  6.77744101267308e-05\n","Epoch  14 Batch  151 / 228  Training Loss  5.1623308536363766e-05\n","Epoch  14 Batch  152 / 228  Training Loss  8.321800851263106e-05\n","Epoch  14 Batch  153 / 228  Training Loss  3.53632822225336e-05\n","Epoch  14 Batch  154 / 228  Training Loss  3.9104830648284405e-05\n","Epoch  14 Batch  155 / 228  Training Loss  2.7495818358147517e-05\n","Epoch  14 Batch  156 / 228  Training Loss  4.9622416554484516e-05\n","Epoch  14 Batch  157 / 228  Training Loss  4.306512346374802e-05\n","Epoch  14 Batch  158 / 228  Training Loss  3.989003744209185e-05\n","Epoch  14 Batch  159 / 228  Training Loss  5.006230276194401e-05\n","Epoch  14 Batch  160 / 228  Training Loss  5.8091980463359505e-05\n","Epoch  14 Batch  161 / 228  Training Loss  4.361928949947469e-05\n","Epoch  14 Batch  162 / 228  Training Loss  3.636302426457405e-05\n","Epoch  14 Batch  163 / 228  Training Loss  4.7312092647189274e-05\n","Epoch  14 Batch  164 / 228  Training Loss  5.542588769458234e-05\n","Epoch  14 Batch  165 / 228  Training Loss  3.895586996804923e-05\n","Epoch  14 Batch  166 / 228  Training Loss  3.006171937158797e-05\n","Epoch  14 Batch  167 / 228  Training Loss  4.635916411643848e-05\n","Epoch  14 Batch  168 / 228  Training Loss  4.503640957409516e-05\n","Epoch  14 Batch  169 / 228  Training Loss  3.536522854119539e-05\n","Epoch  14 Batch  170 / 228  Training Loss  4.5655473513761535e-05\n","Epoch  14 Batch  171 / 228  Training Loss  4.701861325884238e-05\n","Epoch  14 Batch  172 / 228  Training Loss  4.30015024903696e-05\n","Epoch  14 Batch  173 / 228  Training Loss  4.546096897684038e-05\n","Epoch  14 Batch  174 / 228  Training Loss  7.35717621864751e-05\n","Epoch  14 Batch  175 / 228  Training Loss  3.9078538975445554e-05\n","Epoch  14 Batch  176 / 228  Training Loss  6.604144437005743e-05\n","Epoch  14 Batch  177 / 228  Training Loss  6.461773591581732e-05\n","Epoch  14 Batch  178 / 228  Training Loss  5.79912266402971e-05\n","Epoch  14 Batch  179 / 228  Training Loss  3.0999417504062876e-05\n","Epoch  14 Batch  180 / 228  Training Loss  6.581321213161573e-05\n","Epoch  14 Batch  181 / 228  Training Loss  3.979765824624337e-05\n","Epoch  14 Batch  182 / 228  Training Loss  4.068003181600943e-05\n","Epoch  14 Batch  183 / 228  Training Loss  4.750984226120636e-05\n","Epoch  14 Batch  184 / 228  Training Loss  4.4728993088938296e-05\n","Epoch  14 Batch  185 / 228  Training Loss  4.998044460080564e-05\n","Epoch  14 Batch  186 / 228  Training Loss  7.277297117980197e-05\n","Epoch  14 Batch  187 / 228  Training Loss  5.827837230754085e-05\n","Epoch  14 Batch  188 / 228  Training Loss  4.754455221700482e-05\n","Epoch  14 Batch  189 / 228  Training Loss  3.828548506135121e-05\n","Epoch  14 Batch  190 / 228  Training Loss  3.0521488952217624e-05\n","Epoch  14 Batch  191 / 228  Training Loss  6.672785093542188e-05\n","Epoch  14 Batch  192 / 228  Training Loss  4.541849557426758e-05\n","Epoch  14 Batch  193 / 228  Training Loss  3.32427371176891e-05\n","Epoch  14 Batch  194 / 228  Training Loss  4.4273841922404245e-05\n","Epoch  14 Batch  195 / 228  Training Loss  4.29854953836184e-05\n","Epoch  14 Batch  196 / 228  Training Loss  4.13744492107071e-05\n","Epoch  14 Batch  197 / 228  Training Loss  5.340284405974671e-05\n","Epoch  14 Batch  198 / 228  Training Loss  3.942741022910923e-05\n","Epoch  14 Batch  199 / 228  Training Loss  3.9637678128201514e-05\n","Epoch  14 Batch  200 / 228  Training Loss  3.756198566406965e-05\n","Epoch  14 Batch  201 / 228  Training Loss  4.906644971924834e-05\n","Epoch  14 Batch  202 / 228  Training Loss  3.164956433465704e-05\n","Epoch  14 Batch  203 / 228  Training Loss  4.468591941986233e-05\n","Epoch  14 Batch  204 / 228  Training Loss  8.110242197290063e-05\n","Epoch  14 Batch  205 / 228  Training Loss  5.077747118775733e-05\n","Epoch  14 Batch  206 / 228  Training Loss  5.945470911683515e-05\n","Epoch  14 Batch  207 / 228  Training Loss  4.3131294660270214e-05\n","Epoch  14 Batch  208 / 228  Training Loss  2.4415034204139374e-05\n","Epoch  14 Batch  209 / 228  Training Loss  3.162774373777211e-05\n","Epoch  14 Batch  210 / 228  Training Loss  5.4619005823042244e-05\n","Epoch  14 Batch  211 / 228  Training Loss  4.075811375514604e-05\n","Epoch  14 Batch  212 / 228  Training Loss  4.969337533111684e-05\n","Epoch  14 Batch  213 / 228  Training Loss  4.458697003428824e-05\n","Epoch  14 Batch  214 / 228  Training Loss  3.8918333302717656e-05\n","Epoch  14 Batch  215 / 228  Training Loss  2.5670557079138234e-05\n","Epoch  14 Batch  216 / 228  Training Loss  6.521891191368923e-05\n","Epoch  14 Batch  217 / 228  Training Loss  5.063931894255802e-05\n","Epoch  14 Batch  218 / 228  Training Loss  3.081683462369256e-05\n","Epoch  14 Batch  219 / 228  Training Loss  4.766164784086868e-05\n","Epoch  14 Batch  220 / 228  Training Loss  3.555540388333611e-05\n","Epoch  14 Batch  221 / 228  Training Loss  4.7171452024485916e-05\n","Epoch  14 Batch  222 / 228  Training Loss  3.4444623452145606e-05\n","Epoch  14 Batch  223 / 228  Training Loss  3.99404889321886e-05\n","Epoch  14 Batch  224 / 228  Training Loss  4.16599796153605e-05\n","Epoch  14 Batch  225 / 228  Training Loss  6.424696766771376e-05\n","Epoch  14 Batch  226 / 228  Training Loss  5.445235365186818e-05\n","Epoch  14 Batch  227 / 228  Training Loss  3.484069020487368e-05\n","  15    |    -    |   0.000050   | 99.161585\n","----------------------------------------------------------------------\n","Running epoch: 15\n","Epoch  15 Batch  0 / 228  Training Loss  3.916769492207095e-05\n","Epoch  15 Batch  1 / 228  Training Loss  3.834707604255527e-05\n","Epoch  15 Batch  2 / 228  Training Loss  3.2120624382514507e-05\n","Epoch  15 Batch  3 / 228  Training Loss  3.217361881979741e-05\n","Epoch  15 Batch  4 / 228  Training Loss  4.301706576370634e-05\n","Epoch  15 Batch  5 / 228  Training Loss  3.728442607098259e-05\n","Epoch  15 Batch  6 / 228  Training Loss  3.4379852877464145e-05\n","Epoch  15 Batch  7 / 228  Training Loss  4.152524707023986e-05\n","Epoch  15 Batch  8 / 228  Training Loss  5.661647446686402e-05\n","Epoch  15 Batch  9 / 228  Training Loss  3.7208879803074524e-05\n","Epoch  15 Batch  10 / 228  Training Loss  4.2354808101663366e-05\n","Epoch  15 Batch  11 / 228  Training Loss  5.5605276429560035e-05\n","Epoch  15 Batch  12 / 228  Training Loss  5.0992042815778404e-05\n","Epoch  15 Batch  13 / 228  Training Loss  4.352007090346888e-05\n","Epoch  15 Batch  14 / 228  Training Loss  4.710386929218657e-05\n","Epoch  15 Batch  15 / 228  Training Loss  3.9145925256889313e-05\n","Epoch  15 Batch  16 / 228  Training Loss  4.432363493833691e-05\n","Epoch  15 Batch  17 / 228  Training Loss  4.416846059029922e-05\n","Epoch  15 Batch  18 / 228  Training Loss  3.3784926927182823e-05\n","Epoch  15 Batch  19 / 228  Training Loss  4.028010516776703e-05\n","Epoch  15 Batch  20 / 228  Training Loss  3.490457675070502e-05\n","Epoch  15 Batch  21 / 228  Training Loss  5.5718082876410335e-05\n","Epoch  15 Batch  22 / 228  Training Loss  5.8286444982513785e-05\n","Epoch  15 Batch  23 / 228  Training Loss  4.178286326350644e-05\n","Epoch  15 Batch  24 / 228  Training Loss  2.6887970307143405e-05\n","Epoch  15 Batch  25 / 228  Training Loss  3.7581019569188356e-05\n","Epoch  15 Batch  26 / 228  Training Loss  3.2837655453477055e-05\n","Epoch  15 Batch  27 / 228  Training Loss  2.8979800845263526e-05\n","Epoch  15 Batch  28 / 228  Training Loss  3.968879173044115e-05\n","Epoch  15 Batch  29 / 228  Training Loss  5.292352943797596e-05\n","Epoch  15 Batch  30 / 228  Training Loss  5.79174084123224e-05\n","Epoch  15 Batch  31 / 228  Training Loss  5.858287477167323e-05\n","Epoch  15 Batch  32 / 228  Training Loss  3.721750545082614e-05\n","Epoch  15 Batch  33 / 228  Training Loss  4.781696043210104e-05\n","Epoch  15 Batch  34 / 228  Training Loss  2.984502680192236e-05\n","Epoch  15 Batch  35 / 228  Training Loss  6.523203774122521e-05\n","Epoch  15 Batch  36 / 228  Training Loss  4.4602977141039446e-05\n","Epoch  15 Batch  37 / 228  Training Loss  5.213770782575011e-05\n","Epoch  15 Batch  38 / 228  Training Loss  3.799228579737246e-05\n","Epoch  15 Batch  39 / 228  Training Loss  5.420721208793111e-05\n","Epoch  15 Batch  40 / 228  Training Loss  4.0576029277872294e-05\n","Epoch  15 Batch  41 / 228  Training Loss  3.371162529219873e-05\n","Epoch  15 Batch  42 / 228  Training Loss  4.65620614704676e-05\n","Epoch  15 Batch  43 / 228  Training Loss  4.378314770292491e-05\n","Epoch  15 Batch  44 / 228  Training Loss  3.6091347283218056e-05\n","Epoch  15 Batch  45 / 228  Training Loss  4.345151683082804e-05\n","Epoch  15 Batch  46 / 228  Training Loss  3.8487498386530206e-05\n","Epoch  15 Batch  47 / 228  Training Loss  3.533807102940045e-05\n","Epoch  15 Batch  48 / 228  Training Loss  5.3737068810733035e-05\n","Epoch  15 Batch  49 / 228  Training Loss  4.1262836020905524e-05\n","Epoch  15 Batch  50 / 228  Training Loss  4.5865974243497476e-05\n","Epoch  15 Batch  51 / 228  Training Loss  6.021005174261518e-05\n","Epoch  15 Batch  52 / 228  Training Loss  6.576998566742986e-05\n","Epoch  15 Batch  53 / 228  Training Loss  4.475066452869214e-05\n","Epoch  15 Batch  54 / 228  Training Loss  4.143013575230725e-05\n","Epoch  15 Batch  55 / 228  Training Loss  3.231472146580927e-05\n","Epoch  15 Batch  56 / 228  Training Loss  4.930986324325204e-05\n","Epoch  15 Batch  57 / 228  Training Loss  4.019981497549452e-05\n","Epoch  15 Batch  58 / 228  Training Loss  2.5525911041768268e-05\n","Epoch  15 Batch  59 / 228  Training Loss  3.8679361750837415e-05\n","Epoch  15 Batch  60 / 228  Training Loss  6.830511119915172e-05\n","Epoch  15 Batch  61 / 228  Training Loss  3.623932207119651e-05\n","Epoch  15 Batch  62 / 228  Training Loss  4.171427644905634e-05\n","Epoch  15 Batch  63 / 228  Training Loss  4.185302896075882e-05\n","Epoch  15 Batch  64 / 228  Training Loss  2.7287067496217787e-05\n","Epoch  15 Batch  65 / 228  Training Loss  4.296306360629387e-05\n","Epoch  15 Batch  66 / 228  Training Loss  3.7662604881916195e-05\n","Epoch  15 Batch  67 / 228  Training Loss  4.5791523007210344e-05\n","Epoch  15 Batch  68 / 228  Training Loss  4.682359576690942e-05\n","Epoch  15 Batch  69 / 228  Training Loss  6.073251643101685e-05\n","Epoch  15 Batch  70 / 228  Training Loss  5.8731751778395846e-05\n","Epoch  15 Batch  71 / 228  Training Loss  4.415443981997669e-05\n","Epoch  15 Batch  72 / 228  Training Loss  3.2853589800652117e-05\n","Epoch  15 Batch  73 / 228  Training Loss  3.46701817761641e-05\n","Epoch  15 Batch  74 / 228  Training Loss  4.099208672414534e-05\n","Epoch  15 Batch  75 / 228  Training Loss  2.9040302251814865e-05\n","Epoch  15 Batch  76 / 228  Training Loss  4.770502710016444e-05\n","Epoch  15 Batch  77 / 228  Training Loss  5.2526273066177964e-05\n","Epoch  15 Batch  78 / 228  Training Loss  2.046127337962389e-05\n","Epoch  15 Batch  79 / 228  Training Loss  3.839063720079139e-05\n","Epoch  15 Batch  80 / 228  Training Loss  5.1968021580250934e-05\n","Epoch  15 Batch  81 / 228  Training Loss  4.027057366329245e-05\n","Epoch  15 Batch  82 / 228  Training Loss  5.5906355555634946e-05\n","Epoch  15 Batch  83 / 228  Training Loss  3.5396260500419885e-05\n","Epoch  15 Batch  84 / 228  Training Loss  3.620879942900501e-05\n","Epoch  15 Batch  85 / 228  Training Loss  4.408757740748115e-05\n","Epoch  15 Batch  86 / 228  Training Loss  3.908431972377002e-05\n","Epoch  15 Batch  87 / 228  Training Loss  2.6053141482407227e-05\n","Epoch  15 Batch  88 / 228  Training Loss  3.6579775041900575e-05\n","Epoch  15 Batch  89 / 228  Training Loss  4.4639666157308966e-05\n","Epoch  15 Batch  90 / 228  Training Loss  3.3205018553417176e-05\n","Epoch  15 Batch  91 / 228  Training Loss  5.873544068890624e-05\n","Epoch  15 Batch  92 / 228  Training Loss  3.499020021990873e-05\n","Epoch  15 Batch  93 / 228  Training Loss  2.5578998247510754e-05\n","Epoch  15 Batch  94 / 228  Training Loss  3.8548969314433634e-05\n","Epoch  15 Batch  95 / 228  Training Loss  3.276058123447001e-05\n","Epoch  15 Batch  96 / 228  Training Loss  3.200771607225761e-05\n","Epoch  15 Batch  97 / 228  Training Loss  2.5966190150938928e-05\n","Epoch  15 Batch  98 / 228  Training Loss  6.119996396591887e-05\n","Epoch  15 Batch  99 / 228  Training Loss  3.98595911974553e-05\n","Epoch  15 Batch  100 / 228  Training Loss  4.364346750662662e-05\n","Epoch  15 Batch  101 / 228  Training Loss  4.359658487373963e-05\n","Epoch  15 Batch  102 / 228  Training Loss  5.6740893342066556e-05\n","Epoch  15 Batch  103 / 228  Training Loss  3.3290594728896394e-05\n","Epoch  15 Batch  104 / 228  Training Loss  4.610493124346249e-05\n","Epoch  15 Batch  105 / 228  Training Loss  4.637616075342521e-05\n","Epoch  15 Batch  106 / 228  Training Loss  3.983836359111592e-05\n","Epoch  15 Batch  107 / 228  Training Loss  2.384669460298028e-05\n","Epoch  15 Batch  108 / 228  Training Loss  3.106489748461172e-05\n","Epoch  15 Batch  109 / 228  Training Loss  3.0264045562944375e-05\n","Epoch  15 Batch  110 / 228  Training Loss  3.383395232958719e-05\n","Epoch  15 Batch  111 / 228  Training Loss  3.3718588383635506e-05\n","Epoch  15 Batch  112 / 228  Training Loss  2.8162336093373597e-05\n","Epoch  15 Batch  113 / 228  Training Loss  2.7481690267450176e-05\n","Epoch  15 Batch  114 / 228  Training Loss  4.5366705307969823e-05\n","Epoch  15 Batch  115 / 228  Training Loss  4.526583506958559e-05\n","Epoch  15 Batch  116 / 228  Training Loss  3.8116082578198984e-05\n","Epoch  15 Batch  117 / 228  Training Loss  2.807268174365163e-05\n","Epoch  15 Batch  118 / 228  Training Loss  4.665155938710086e-05\n","Epoch  15 Batch  119 / 228  Training Loss  3.671984813991003e-05\n","Epoch  15 Batch  120 / 228  Training Loss  4.542519309325144e-05\n","Epoch  15 Batch  121 / 228  Training Loss  3.5907702113036066e-05\n","Epoch  15 Batch  122 / 228  Training Loss  3.3201955375261605e-05\n","Epoch  15 Batch  123 / 228  Training Loss  3.318618837511167e-05\n","Epoch  15 Batch  124 / 228  Training Loss  3.910013037966564e-05\n","Epoch  15 Batch  125 / 228  Training Loss  3.868319618050009e-05\n","Epoch  15 Batch  126 / 228  Training Loss  3.190509960404597e-05\n","Epoch  15 Batch  127 / 228  Training Loss  5.008922380511649e-05\n","Epoch  15 Batch  128 / 228  Training Loss  4.4770909880753607e-05\n","Epoch  15 Batch  129 / 228  Training Loss  4.537515269475989e-05\n","Epoch  15 Batch  130 / 228  Training Loss  3.658077184809372e-05\n","Epoch  15 Batch  131 / 228  Training Loss  4.867908137384802e-05\n","Epoch  15 Batch  132 / 228  Training Loss  3.2085499697132036e-05\n","Epoch  15 Batch  133 / 228  Training Loss  3.586416278267279e-05\n","Epoch  15 Batch  134 / 228  Training Loss  4.449847256182693e-05\n","Epoch  15 Batch  135 / 228  Training Loss  3.220381404389627e-05\n","Epoch  15 Batch  136 / 228  Training Loss  4.550653829937801e-05\n","Epoch  15 Batch  137 / 228  Training Loss  6.920007581356913e-05\n","Epoch  15 Batch  138 / 228  Training Loss  6.825036689406261e-05\n","Epoch  15 Batch  139 / 228  Training Loss  3.67882166756317e-05\n","Epoch  15 Batch  140 / 228  Training Loss  3.933432890335098e-05\n","Epoch  15 Batch  141 / 228  Training Loss  4.581250686896965e-05\n","Epoch  15 Batch  142 / 228  Training Loss  3.201478102710098e-05\n","Epoch  15 Batch  143 / 228  Training Loss  4.038874612888321e-05\n","Epoch  15 Batch  144 / 228  Training Loss  2.6945655918098055e-05\n","Epoch  15 Batch  145 / 228  Training Loss  3.4181361115770414e-05\n","Epoch  15 Batch  146 / 228  Training Loss  4.818334127776325e-05\n","Epoch  15 Batch  147 / 228  Training Loss  2.8094378649257123e-05\n","Epoch  15 Batch  148 / 228  Training Loss  3.4530123230069876e-05\n","Epoch  15 Batch  149 / 228  Training Loss  3.7650857848348096e-05\n","Epoch  15 Batch  150 / 228  Training Loss  2.3534845240646973e-05\n","Epoch  15 Batch  151 / 228  Training Loss  3.698932778206654e-05\n","Epoch  15 Batch  152 / 228  Training Loss  5.2521492762025446e-05\n","Epoch  15 Batch  153 / 228  Training Loss  3.392084181541577e-05\n","Epoch  15 Batch  154 / 228  Training Loss  2.8456934160203673e-05\n","Epoch  15 Batch  155 / 228  Training Loss  4.772143074660562e-05\n","Epoch  15 Batch  156 / 228  Training Loss  2.8153610401204787e-05\n","Epoch  15 Batch  157 / 228  Training Loss  4.616687147063203e-05\n","Epoch  15 Batch  158 / 228  Training Loss  4.45935656898655e-05\n","Epoch  15 Batch  159 / 228  Training Loss  3.8296613638522103e-05\n","Epoch  15 Batch  160 / 228  Training Loss  5.7005283451871946e-05\n","Epoch  15 Batch  161 / 228  Training Loss  3.2810523407533765e-05\n","Epoch  15 Batch  162 / 228  Training Loss  4.400038960739039e-05\n","Epoch  15 Batch  163 / 228  Training Loss  2.4612632842035964e-05\n","Epoch  15 Batch  164 / 228  Training Loss  4.219593029120006e-05\n","Epoch  15 Batch  165 / 228  Training Loss  3.9130012737587094e-05\n","Epoch  15 Batch  166 / 228  Training Loss  2.89950748992851e-05\n","Epoch  15 Batch  167 / 228  Training Loss  3.6553879908751696e-05\n","Epoch  15 Batch  168 / 228  Training Loss  4.261578942532651e-05\n","Epoch  15 Batch  169 / 228  Training Loss  4.324865585658699e-05\n","Epoch  15 Batch  170 / 228  Training Loss  3.202384687028825e-05\n","Epoch  15 Batch  171 / 228  Training Loss  3.8019301427993923e-05\n","Epoch  15 Batch  172 / 228  Training Loss  4.550434459815733e-05\n","Epoch  15 Batch  173 / 228  Training Loss  4.392101618577726e-05\n","Epoch  15 Batch  174 / 228  Training Loss  2.5726698368089274e-05\n","Epoch  15 Batch  175 / 228  Training Loss  3.568001920939423e-05\n","Epoch  15 Batch  176 / 228  Training Loss  3.200560240657069e-05\n","Epoch  15 Batch  177 / 228  Training Loss  3.836923497146927e-05\n","Epoch  15 Batch  178 / 228  Training Loss  3.0217974199331366e-05\n","Epoch  15 Batch  179 / 228  Training Loss  4.426211671670899e-05\n","Epoch  15 Batch  180 / 228  Training Loss  2.9251623345771804e-05\n","Epoch  15 Batch  181 / 228  Training Loss  2.6024787075584754e-05\n","Epoch  15 Batch  182 / 228  Training Loss  5.5797427194193006e-05\n","Epoch  15 Batch  183 / 228  Training Loss  2.650064925546758e-05\n","Epoch  15 Batch  184 / 228  Training Loss  5.865326238563284e-05\n","Epoch  15 Batch  185 / 228  Training Loss  3.1042149203130975e-05\n","Epoch  15 Batch  186 / 228  Training Loss  4.327169881435111e-05\n","Epoch  15 Batch  187 / 228  Training Loss  5.056760346633382e-05\n","Epoch  15 Batch  188 / 228  Training Loss  2.691780900931917e-05\n","Epoch  15 Batch  189 / 228  Training Loss  2.7690213755704463e-05\n","Epoch  15 Batch  190 / 228  Training Loss  3.818347249762155e-05\n","Epoch  15 Batch  191 / 228  Training Loss  3.3900782000273466e-05\n","Epoch  15 Batch  192 / 228  Training Loss  3.310324245830998e-05\n","Epoch  15 Batch  193 / 228  Training Loss  3.578376708901487e-05\n","Epoch  15 Batch  194 / 228  Training Loss  3.2647942134644836e-05\n","Epoch  15 Batch  195 / 228  Training Loss  4.764930417877622e-05\n","Epoch  15 Batch  196 / 228  Training Loss  3.507542714942247e-05\n","Epoch  15 Batch  197 / 228  Training Loss  5.255622090771794e-05\n","Epoch  15 Batch  198 / 228  Training Loss  4.1911291191354394e-05\n","Epoch  15 Batch  199 / 228  Training Loss  2.8420061426004395e-05\n","Epoch  15 Batch  200 / 228  Training Loss  3.708561052917503e-05\n","Epoch  15 Batch  201 / 228  Training Loss  3.6990735679864883e-05\n","Epoch  15 Batch  202 / 228  Training Loss  3.691169695230201e-05\n","Epoch  15 Batch  203 / 228  Training Loss  5.0143193220719695e-05\n","Epoch  15 Batch  204 / 228  Training Loss  3.125462171738036e-05\n","Epoch  15 Batch  205 / 228  Training Loss  4.121088932151906e-05\n","Epoch  15 Batch  206 / 228  Training Loss  5.054210851085372e-05\n","Epoch  15 Batch  207 / 228  Training Loss  3.856499824905768e-05\n","Epoch  15 Batch  208 / 228  Training Loss  2.9666154659935273e-05\n","Epoch  15 Batch  209 / 228  Training Loss  3.36425073328428e-05\n","Epoch  15 Batch  210 / 228  Training Loss  4.626029476639815e-05\n","Epoch  15 Batch  211 / 228  Training Loss  3.468808063189499e-05\n","Epoch  15 Batch  212 / 228  Training Loss  3.1325766030931845e-05\n","Epoch  15 Batch  213 / 228  Training Loss  2.907193629653193e-05\n","Epoch  15 Batch  214 / 228  Training Loss  3.8169520848896354e-05\n","Epoch  15 Batch  215 / 228  Training Loss  3.4549964766483754e-05\n","Epoch  15 Batch  216 / 228  Training Loss  2.6744814022094943e-05\n","Epoch  15 Batch  217 / 228  Training Loss  2.389417204540223e-05\n","Epoch  15 Batch  218 / 228  Training Loss  3.800519334618002e-05\n","Epoch  15 Batch  219 / 228  Training Loss  3.6630237445933744e-05\n","Epoch  15 Batch  220 / 228  Training Loss  2.6794612494995818e-05\n","Epoch  15 Batch  221 / 228  Training Loss  4.881744825979695e-05\n","Epoch  15 Batch  222 / 228  Training Loss  3.053571708733216e-05\n","Epoch  15 Batch  223 / 228  Training Loss  3.335470682941377e-05\n","Epoch  15 Batch  224 / 228  Training Loss  3.5327662772033364e-05\n","Epoch  15 Batch  225 / 228  Training Loss  5.397275526775047e-05\n","Epoch  15 Batch  226 / 228  Training Loss  2.9310805985005572e-05\n","Epoch  15 Batch  227 / 228  Training Loss  8.874369086697698e-05\n","  16    |    -    |   0.000040   | 99.085366\n","----------------------------------------------------------------------\n","Running epoch: 16\n","Epoch  16 Batch  0 / 228  Training Loss  4.0141254430636764e-05\n","Epoch  16 Batch  1 / 228  Training Loss  3.8170121115399525e-05\n","Epoch  16 Batch  2 / 228  Training Loss  4.186831938568503e-05\n","Epoch  16 Batch  3 / 228  Training Loss  4.683551378548145e-05\n","Epoch  16 Batch  4 / 228  Training Loss  3.4140692150685936e-05\n","Epoch  16 Batch  5 / 228  Training Loss  2.2920426999917254e-05\n","Epoch  16 Batch  6 / 228  Training Loss  3.83672522730194e-05\n","Epoch  16 Batch  7 / 228  Training Loss  2.4378590751439333e-05\n","Epoch  16 Batch  8 / 228  Training Loss  3.152718272758648e-05\n","Epoch  16 Batch  9 / 228  Training Loss  4.100190199096687e-05\n","Epoch  16 Batch  10 / 228  Training Loss  3.356577144586481e-05\n","Epoch  16 Batch  11 / 228  Training Loss  3.0847684683976695e-05\n","Epoch  16 Batch  12 / 228  Training Loss  2.5525263481540605e-05\n","Epoch  16 Batch  13 / 228  Training Loss  3.555049261194654e-05\n","Epoch  16 Batch  14 / 228  Training Loss  4.267177791916765e-05\n","Epoch  16 Batch  15 / 228  Training Loss  4.051315409014933e-05\n","Epoch  16 Batch  16 / 228  Training Loss  2.6176649043918587e-05\n","Epoch  16 Batch  17 / 228  Training Loss  3.025259320565965e-05\n","Epoch  16 Batch  18 / 228  Training Loss  4.0744926081970334e-05\n","Epoch  16 Batch  19 / 228  Training Loss  3.139229374937713e-05\n","Epoch  16 Batch  20 / 228  Training Loss  2.899951687140856e-05\n","Epoch  16 Batch  21 / 228  Training Loss  3.2127143640536815e-05\n","Epoch  16 Batch  22 / 228  Training Loss  4.2180647142231464e-05\n","Epoch  16 Batch  23 / 228  Training Loss  3.3177399018313736e-05\n","Epoch  16 Batch  24 / 228  Training Loss  3.695369014167227e-05\n","Epoch  16 Batch  25 / 228  Training Loss  3.3391079341527075e-05\n","Epoch  16 Batch  26 / 228  Training Loss  3.159645348205231e-05\n","Epoch  16 Batch  27 / 228  Training Loss  3.041304080397822e-05\n","Epoch  16 Batch  28 / 228  Training Loss  3.811154965660535e-05\n","Epoch  16 Batch  29 / 228  Training Loss  2.4333387045771815e-05\n","Epoch  16 Batch  30 / 228  Training Loss  2.442305594740901e-05\n","Epoch  16 Batch  31 / 228  Training Loss  5.2998213504906744e-05\n","Epoch  16 Batch  32 / 228  Training Loss  2.5730560082593e-05\n","Epoch  16 Batch  33 / 228  Training Loss  3.144525544485077e-05\n","Epoch  16 Batch  34 / 228  Training Loss  2.004571615543682e-05\n","Epoch  16 Batch  35 / 228  Training Loss  3.8572856283281e-05\n","Epoch  16 Batch  36 / 228  Training Loss  4.179794996161945e-05\n","Epoch  16 Batch  37 / 228  Training Loss  2.6734394850791432e-05\n","Epoch  16 Batch  38 / 228  Training Loss  3.031283267773688e-05\n","Epoch  16 Batch  39 / 228  Training Loss  2.7935147954849526e-05\n","Epoch  16 Batch  40 / 228  Training Loss  3.103167182416655e-05\n","Epoch  16 Batch  41 / 228  Training Loss  2.4061615476966836e-05\n","Epoch  16 Batch  42 / 228  Training Loss  3.674668550956994e-05\n","Epoch  16 Batch  43 / 228  Training Loss  3.627034675446339e-05\n","Epoch  16 Batch  44 / 228  Training Loss  3.597813702072017e-05\n","Epoch  16 Batch  45 / 228  Training Loss  3.848953201668337e-05\n","Epoch  16 Batch  46 / 228  Training Loss  3.6156056012259796e-05\n","Epoch  16 Batch  47 / 228  Training Loss  4.757981878356077e-05\n","Epoch  16 Batch  48 / 228  Training Loss  2.797651723085437e-05\n","Epoch  16 Batch  49 / 228  Training Loss  3.1820010917726904e-05\n","Epoch  16 Batch  50 / 228  Training Loss  3.700116212712601e-05\n","Epoch  16 Batch  51 / 228  Training Loss  3.109806129941717e-05\n","Epoch  16 Batch  52 / 228  Training Loss  3.849409768008627e-05\n","Epoch  16 Batch  53 / 228  Training Loss  5.100893395137973e-05\n","Epoch  16 Batch  54 / 228  Training Loss  3.876917253364809e-05\n","Epoch  16 Batch  55 / 228  Training Loss  4.539057772490196e-05\n","Epoch  16 Batch  56 / 228  Training Loss  4.004297443316318e-05\n","Epoch  16 Batch  57 / 228  Training Loss  4.845969669986516e-05\n","Epoch  16 Batch  58 / 228  Training Loss  2.755231071205344e-05\n","Epoch  16 Batch  59 / 228  Training Loss  3.301926335552707e-05\n","Epoch  16 Batch  60 / 228  Training Loss  4.053224256495014e-05\n","Epoch  16 Batch  61 / 228  Training Loss  3.762919004657306e-05\n","Epoch  16 Batch  62 / 228  Training Loss  2.600792868179269e-05\n","Epoch  16 Batch  63 / 228  Training Loss  4.539692235994153e-05\n","Epoch  16 Batch  64 / 228  Training Loss  3.525354986777529e-05\n","Epoch  16 Batch  65 / 228  Training Loss  3.396005558897741e-05\n","Epoch  16 Batch  66 / 228  Training Loss  3.156504317303188e-05\n","Epoch  16 Batch  67 / 228  Training Loss  3.6026263842359185e-05\n","Epoch  16 Batch  68 / 228  Training Loss  3.7674799386877567e-05\n","Epoch  16 Batch  69 / 228  Training Loss  4.100628575542942e-05\n","Epoch  16 Batch  70 / 228  Training Loss  3.624858436523937e-05\n","Epoch  16 Batch  71 / 228  Training Loss  4.491682193474844e-05\n","Epoch  16 Batch  72 / 228  Training Loss  3.52492461388465e-05\n","Epoch  16 Batch  73 / 228  Training Loss  2.9393937438726425e-05\n","Epoch  16 Batch  74 / 228  Training Loss  3.467592250672169e-05\n","Epoch  16 Batch  75 / 228  Training Loss  3.300488242530264e-05\n","Epoch  16 Batch  76 / 228  Training Loss  3.761906555155292e-05\n","Epoch  16 Batch  77 / 228  Training Loss  4.2735959141282365e-05\n","Epoch  16 Batch  78 / 228  Training Loss  2.5392841052962467e-05\n","Epoch  16 Batch  79 / 228  Training Loss  2.7223179131397046e-05\n","Epoch  16 Batch  80 / 228  Training Loss  4.29678475484252e-05\n","Epoch  16 Batch  81 / 228  Training Loss  1.8548109437688254e-05\n","Epoch  16 Batch  82 / 228  Training Loss  3.958376692025922e-05\n","Epoch  16 Batch  83 / 228  Training Loss  4.843753777095117e-05\n","Epoch  16 Batch  84 / 228  Training Loss  2.039846731349826e-05\n","Epoch  16 Batch  85 / 228  Training Loss  2.7714864700101316e-05\n","Epoch  16 Batch  86 / 228  Training Loss  2.5765102691366337e-05\n","Epoch  16 Batch  87 / 228  Training Loss  3.973274215240963e-05\n","Epoch  16 Batch  88 / 228  Training Loss  5.132095975568518e-05\n","Epoch  16 Batch  89 / 228  Training Loss  3.153055877191946e-05\n","Epoch  16 Batch  90 / 228  Training Loss  2.6729747332865372e-05\n","Epoch  16 Batch  91 / 228  Training Loss  4.284679744159803e-05\n","Epoch  16 Batch  92 / 228  Training Loss  2.700895674934145e-05\n","Epoch  16 Batch  93 / 228  Training Loss  3.2266943890135735e-05\n","Epoch  16 Batch  94 / 228  Training Loss  4.080669168615714e-05\n","Epoch  16 Batch  95 / 228  Training Loss  2.57931442320114e-05\n","Epoch  16 Batch  96 / 228  Training Loss  3.593323708628304e-05\n","Epoch  16 Batch  97 / 228  Training Loss  3.0440103728324175e-05\n","Epoch  16 Batch  98 / 228  Training Loss  2.578830208221916e-05\n","Epoch  16 Batch  99 / 228  Training Loss  2.1456025933730416e-05\n","Epoch  16 Batch  100 / 228  Training Loss  3.381392525625415e-05\n","Epoch  16 Batch  101 / 228  Training Loss  3.317140362923965e-05\n","Epoch  16 Batch  102 / 228  Training Loss  2.4663799194968306e-05\n","Epoch  16 Batch  103 / 228  Training Loss  3.487234062049538e-05\n","Epoch  16 Batch  104 / 228  Training Loss  3.495136115816422e-05\n","Epoch  16 Batch  105 / 228  Training Loss  3.310035390313715e-05\n","Epoch  16 Batch  106 / 228  Training Loss  4.427485691849142e-05\n","Epoch  16 Batch  107 / 228  Training Loss  3.529547757352702e-05\n","Epoch  16 Batch  108 / 228  Training Loss  3.971275873482227e-05\n","Epoch  16 Batch  109 / 228  Training Loss  3.1846604542806745e-05\n","Epoch  16 Batch  110 / 228  Training Loss  3.269693479523994e-05\n","Epoch  16 Batch  111 / 228  Training Loss  2.883524211938493e-05\n","Epoch  16 Batch  112 / 228  Training Loss  5.1076542149530724e-05\n","Epoch  16 Batch  113 / 228  Training Loss  4.100558362551965e-05\n","Epoch  16 Batch  114 / 228  Training Loss  2.5273990104324184e-05\n","Epoch  16 Batch  115 / 228  Training Loss  2.0678276996477507e-05\n","Epoch  16 Batch  116 / 228  Training Loss  3.967918019043282e-05\n","Epoch  16 Batch  117 / 228  Training Loss  3.230552829336375e-05\n","Epoch  16 Batch  118 / 228  Training Loss  2.9387165341177024e-05\n","Epoch  16 Batch  119 / 228  Training Loss  2.660673635546118e-05\n","Epoch  16 Batch  120 / 228  Training Loss  4.109832661924884e-05\n","Epoch  16 Batch  121 / 228  Training Loss  4.2452717025298625e-05\n","Epoch  16 Batch  122 / 228  Training Loss  3.084693889832124e-05\n","Epoch  16 Batch  123 / 228  Training Loss  4.675706077250652e-05\n","Epoch  16 Batch  124 / 228  Training Loss  3.220783401047811e-05\n","Epoch  16 Batch  125 / 228  Training Loss  3.6747009289683774e-05\n","Epoch  16 Batch  126 / 228  Training Loss  3.237490454921499e-05\n","Epoch  16 Batch  127 / 228  Training Loss  3.762314372579567e-05\n","Epoch  16 Batch  128 / 228  Training Loss  3.882595046889037e-05\n","Epoch  16 Batch  129 / 228  Training Loss  2.5602372261346318e-05\n","Epoch  16 Batch  130 / 228  Training Loss  3.7858597352169454e-05\n","Epoch  16 Batch  131 / 228  Training Loss  3.615159948822111e-05\n","Epoch  16 Batch  132 / 228  Training Loss  2.7484911697683856e-05\n","Epoch  16 Batch  133 / 228  Training Loss  4.1707891796249896e-05\n","Epoch  16 Batch  134 / 228  Training Loss  3.7484765925910324e-05\n","Epoch  16 Batch  135 / 228  Training Loss  2.8787751944037154e-05\n","Epoch  16 Batch  136 / 228  Training Loss  3.3188418456120417e-05\n","Epoch  16 Batch  137 / 228  Training Loss  3.5683791793417186e-05\n","Epoch  16 Batch  138 / 228  Training Loss  4.268343764124438e-05\n","Epoch  16 Batch  139 / 228  Training Loss  3.0779596272623166e-05\n","Epoch  16 Batch  140 / 228  Training Loss  2.8399294023984112e-05\n","Epoch  16 Batch  141 / 228  Training Loss  3.702992762555368e-05\n","Epoch  16 Batch  142 / 228  Training Loss  4.4181135308463126e-05\n","Epoch  16 Batch  143 / 228  Training Loss  3.428835771046579e-05\n","Epoch  16 Batch  144 / 228  Training Loss  3.154134901706129e-05\n","Epoch  16 Batch  145 / 228  Training Loss  3.419262066017836e-05\n","Epoch  16 Batch  146 / 228  Training Loss  3.701647074194625e-05\n","Epoch  16 Batch  147 / 228  Training Loss  3.206858673365787e-05\n","Epoch  16 Batch  148 / 228  Training Loss  2.229170422651805e-05\n","Epoch  16 Batch  149 / 228  Training Loss  3.0606410291511565e-05\n","Epoch  16 Batch  150 / 228  Training Loss  3.6380795791046694e-05\n","Epoch  16 Batch  151 / 228  Training Loss  3.892339373123832e-05\n","Epoch  16 Batch  152 / 228  Training Loss  3.335002838866785e-05\n","Epoch  16 Batch  153 / 228  Training Loss  2.939229307230562e-05\n","Epoch  16 Batch  154 / 228  Training Loss  1.9143224562867545e-05\n","Epoch  16 Batch  155 / 228  Training Loss  3.6411202017916366e-05\n","Epoch  16 Batch  156 / 228  Training Loss  3.4874548873631284e-05\n","Epoch  16 Batch  157 / 228  Training Loss  2.8821081286878325e-05\n","Epoch  16 Batch  158 / 228  Training Loss  2.728107756411191e-05\n","Epoch  16 Batch  159 / 228  Training Loss  2.9136674129404128e-05\n","Epoch  16 Batch  160 / 228  Training Loss  3.4963311918545514e-05\n","Epoch  16 Batch  161 / 228  Training Loss  3.1668612791690975e-05\n","Epoch  16 Batch  162 / 228  Training Loss  3.3235617593163624e-05\n","Epoch  16 Batch  163 / 228  Training Loss  3.8954862247919664e-05\n","Epoch  16 Batch  164 / 228  Training Loss  2.9650031137862243e-05\n","Epoch  16 Batch  165 / 228  Training Loss  4.1601128032198176e-05\n","Epoch  16 Batch  166 / 228  Training Loss  3.4729255276033655e-05\n","Epoch  16 Batch  167 / 228  Training Loss  4.259493653080426e-05\n","Epoch  16 Batch  168 / 228  Training Loss  3.30580587615259e-05\n","Epoch  16 Batch  169 / 228  Training Loss  2.553212470957078e-05\n","Epoch  16 Batch  170 / 228  Training Loss  4.1831659473245963e-05\n","Epoch  16 Batch  171 / 228  Training Loss  2.7503303499543108e-05\n","Epoch  16 Batch  172 / 228  Training Loss  4.545607225736603e-05\n","Epoch  16 Batch  173 / 228  Training Loss  4.133993206778541e-05\n","Epoch  16 Batch  174 / 228  Training Loss  3.0814710044069216e-05\n","Epoch  16 Batch  175 / 228  Training Loss  3.285197453806177e-05\n","Epoch  16 Batch  176 / 228  Training Loss  3.3985917980317026e-05\n","Epoch  16 Batch  177 / 228  Training Loss  2.1878529878449626e-05\n","Epoch  16 Batch  178 / 228  Training Loss  4.170439569861628e-05\n","Epoch  16 Batch  179 / 228  Training Loss  2.6274650736013427e-05\n","Epoch  16 Batch  180 / 228  Training Loss  3.509976522764191e-05\n","Epoch  16 Batch  181 / 228  Training Loss  3.491229290375486e-05\n","Epoch  16 Batch  182 / 228  Training Loss  3.6060231650481e-05\n","Epoch  16 Batch  183 / 228  Training Loss  4.5291562855709344e-05\n","Epoch  16 Batch  184 / 228  Training Loss  3.46234992321115e-05\n","Epoch  16 Batch  185 / 228  Training Loss  2.8721337002934888e-05\n","Epoch  16 Batch  186 / 228  Training Loss  3.2560630643274635e-05\n","Epoch  16 Batch  187 / 228  Training Loss  2.7764323021983728e-05\n","Epoch  16 Batch  188 / 228  Training Loss  3.7796777178300545e-05\n","Epoch  16 Batch  189 / 228  Training Loss  2.3920274543343112e-05\n","Epoch  16 Batch  190 / 228  Training Loss  2.204165684815962e-05\n","Epoch  16 Batch  191 / 228  Training Loss  3.136224768240936e-05\n","Epoch  16 Batch  192 / 228  Training Loss  3.186845424352214e-05\n","Epoch  16 Batch  193 / 228  Training Loss  3.0151819373713806e-05\n","Epoch  16 Batch  194 / 228  Training Loss  4.53861357527785e-05\n","Epoch  16 Batch  195 / 228  Training Loss  3.8206122553674504e-05\n","Epoch  16 Batch  196 / 228  Training Loss  3.750955511350185e-05\n","Epoch  16 Batch  197 / 228  Training Loss  2.3633736418560147e-05\n","Epoch  16 Batch  198 / 228  Training Loss  3.81775971618481e-05\n","Epoch  16 Batch  199 / 228  Training Loss  3.893667599186301e-05\n","Epoch  16 Batch  200 / 228  Training Loss  2.97837450489169e-05\n","Epoch  16 Batch  201 / 228  Training Loss  2.4391365514020436e-05\n","Epoch  16 Batch  202 / 228  Training Loss  3.553684291546233e-05\n","Epoch  16 Batch  203 / 228  Training Loss  3.7449462979566306e-05\n","Epoch  16 Batch  204 / 228  Training Loss  3.371983621036634e-05\n","Epoch  16 Batch  205 / 228  Training Loss  3.5995377402286977e-05\n","Epoch  16 Batch  206 / 228  Training Loss  4.330164665589109e-05\n","Epoch  16 Batch  207 / 228  Training Loss  2.9986224035383202e-05\n","Epoch  16 Batch  208 / 228  Training Loss  2.2125062969280407e-05\n","Epoch  16 Batch  209 / 228  Training Loss  2.5673352865851484e-05\n","Epoch  16 Batch  210 / 228  Training Loss  2.868498813768383e-05\n","Epoch  16 Batch  211 / 228  Training Loss  3.6324177926871926e-05\n","Epoch  16 Batch  212 / 228  Training Loss  3.90750246879179e-05\n","Epoch  16 Batch  213 / 228  Training Loss  1.71656247403007e-05\n","Epoch  16 Batch  214 / 228  Training Loss  2.08044693863485e-05\n","Epoch  16 Batch  215 / 228  Training Loss  2.5870433091768064e-05\n","Epoch  16 Batch  216 / 228  Training Loss  5.4752086725784466e-05\n","Epoch  16 Batch  217 / 228  Training Loss  4.631521733244881e-05\n","Epoch  16 Batch  218 / 228  Training Loss  2.9300019377842546e-05\n","Epoch  16 Batch  219 / 228  Training Loss  2.474616121617146e-05\n","Epoch  16 Batch  220 / 228  Training Loss  2.902141204685904e-05\n","Epoch  16 Batch  221 / 228  Training Loss  3.879232463077642e-05\n","Epoch  16 Batch  222 / 228  Training Loss  2.5939405531971715e-05\n","Epoch  16 Batch  223 / 228  Training Loss  3.6152381653664634e-05\n","Epoch  16 Batch  224 / 228  Training Loss  2.4280830984935164e-05\n","Epoch  16 Batch  225 / 228  Training Loss  3.822079816018231e-05\n","Epoch  16 Batch  226 / 228  Training Loss  2.491948544047773e-05\n","Epoch  16 Batch  227 / 228  Training Loss  3.41685808962211e-05\n","  17    |    -    |   0.000034   | 99.161585\n","----------------------------------------------------------------------\n","Running epoch: 17\n","Epoch  17 Batch  0 / 228  Training Loss  3.873287278111093e-05\n","Epoch  17 Batch  1 / 228  Training Loss  3.0385152058443055e-05\n","Epoch  17 Batch  2 / 228  Training Loss  2.7110238079330884e-05\n","Epoch  17 Batch  3 / 228  Training Loss  2.800182846840471e-05\n","Epoch  17 Batch  4 / 228  Training Loss  3.7297315429896116e-05\n","Epoch  17 Batch  5 / 228  Training Loss  3.011456283275038e-05\n","Epoch  17 Batch  6 / 228  Training Loss  3.0317556593217887e-05\n","Epoch  17 Batch  7 / 228  Training Loss  3.3078478736570105e-05\n","Epoch  17 Batch  8 / 228  Training Loss  3.1014918931759894e-05\n","Epoch  17 Batch  9 / 228  Training Loss  2.853999649232719e-05\n","Epoch  17 Batch  10 / 228  Training Loss  2.7494315872900188e-05\n","Epoch  17 Batch  11 / 228  Training Loss  2.8585040126927197e-05\n","Epoch  17 Batch  12 / 228  Training Loss  3.9073569496395066e-05\n","Epoch  17 Batch  13 / 228  Training Loss  4.341701423982158e-05\n","Epoch  17 Batch  14 / 228  Training Loss  2.6497898943489417e-05\n","Epoch  17 Batch  15 / 228  Training Loss  2.6492320102988742e-05\n","Epoch  17 Batch  16 / 228  Training Loss  3.874134927173145e-05\n","Epoch  17 Batch  17 / 228  Training Loss  3.2837240723893046e-05\n","Epoch  17 Batch  18 / 228  Training Loss  2.2641253963229246e-05\n","Epoch  17 Batch  19 / 228  Training Loss  2.1536950953304768e-05\n","Epoch  17 Batch  20 / 228  Training Loss  3.1411138479597867e-05\n","Epoch  17 Batch  21 / 228  Training Loss  3.5854016459779814e-05\n","Epoch  17 Batch  22 / 228  Training Loss  2.771481558738742e-05\n","Epoch  17 Batch  23 / 228  Training Loss  1.660618909227196e-05\n","Epoch  17 Batch  24 / 228  Training Loss  2.574477366579231e-05\n","Epoch  17 Batch  25 / 228  Training Loss  3.1712970667285845e-05\n","Epoch  17 Batch  26 / 228  Training Loss  2.9802817152813077e-05\n","Epoch  17 Batch  27 / 228  Training Loss  3.273965558037162e-05\n","Epoch  17 Batch  28 / 228  Training Loss  4.327091664890759e-05\n","Epoch  17 Batch  29 / 228  Training Loss  1.845966835389845e-05\n","Epoch  17 Batch  30 / 228  Training Loss  2.5645966161391698e-05\n","Epoch  17 Batch  31 / 228  Training Loss  2.4046650651143864e-05\n","Epoch  17 Batch  32 / 228  Training Loss  2.8708289391943254e-05\n","Epoch  17 Batch  33 / 228  Training Loss  3.55305164703168e-05\n","Epoch  17 Batch  34 / 228  Training Loss  2.211874380009249e-05\n","Epoch  17 Batch  35 / 228  Training Loss  5.8370154874864966e-05\n","Epoch  17 Batch  36 / 228  Training Loss  2.227864388260059e-05\n","Epoch  17 Batch  37 / 228  Training Loss  3.11219300783705e-05\n","Epoch  17 Batch  38 / 228  Training Loss  2.1435009330161847e-05\n","Epoch  17 Batch  39 / 228  Training Loss  2.9053388061583973e-05\n","Epoch  17 Batch  40 / 228  Training Loss  2.36822925216984e-05\n","Epoch  17 Batch  41 / 228  Training Loss  1.921244984259829e-05\n","Epoch  17 Batch  42 / 228  Training Loss  2.9429484129650518e-05\n","Epoch  17 Batch  43 / 228  Training Loss  1.698188680165913e-05\n","Epoch  17 Batch  44 / 228  Training Loss  3.620855204644613e-05\n","Epoch  17 Batch  45 / 228  Training Loss  2.616504207253456e-05\n","Epoch  17 Batch  46 / 228  Training Loss  2.692956149985548e-05\n","Epoch  17 Batch  47 / 228  Training Loss  3.441085937083699e-05\n","Epoch  17 Batch  48 / 228  Training Loss  2.0003530153189786e-05\n","Epoch  17 Batch  49 / 228  Training Loss  3.173918230459094e-05\n","Epoch  17 Batch  50 / 228  Training Loss  2.647869405336678e-05\n","Epoch  17 Batch  51 / 228  Training Loss  2.3864002287155017e-05\n","Epoch  17 Batch  52 / 228  Training Loss  3.761562766158022e-05\n","Epoch  17 Batch  53 / 228  Training Loss  2.9135062504792586e-05\n","Epoch  17 Batch  54 / 228  Training Loss  3.1211373425321653e-05\n","Epoch  17 Batch  55 / 228  Training Loss  2.4018194380914792e-05\n","Epoch  17 Batch  56 / 228  Training Loss  3.7257301300996915e-05\n","Epoch  17 Batch  57 / 228  Training Loss  3.234890755265951e-05\n","Epoch  17 Batch  58 / 228  Training Loss  2.1264024326228537e-05\n","Epoch  17 Batch  59 / 228  Training Loss  2.8950153136975132e-05\n","Epoch  17 Batch  60 / 228  Training Loss  2.6212475859210826e-05\n","Epoch  17 Batch  61 / 228  Training Loss  2.917896199505776e-05\n","Epoch  17 Batch  62 / 228  Training Loss  2.966171632579062e-05\n","Epoch  17 Batch  63 / 228  Training Loss  2.546183532103896e-05\n","Epoch  17 Batch  64 / 228  Training Loss  3.034026303794235e-05\n","Epoch  17 Batch  65 / 228  Training Loss  4.4324264308670536e-05\n","Epoch  17 Batch  66 / 228  Training Loss  3.904115146724507e-05\n","Epoch  17 Batch  67 / 228  Training Loss  2.753324770310428e-05\n","Epoch  17 Batch  68 / 228  Training Loss  2.4809425667626783e-05\n","Epoch  17 Batch  69 / 228  Training Loss  3.856837429339066e-05\n","Epoch  17 Batch  70 / 228  Training Loss  3.258901779190637e-05\n","Epoch  17 Batch  71 / 228  Training Loss  2.0530564142973162e-05\n","Epoch  17 Batch  72 / 228  Training Loss  3.254552575526759e-05\n","Epoch  17 Batch  73 / 228  Training Loss  2.5154931790893897e-05\n","Epoch  17 Batch  74 / 228  Training Loss  2.43502563535003e-05\n","Epoch  17 Batch  75 / 228  Training Loss  2.7620981200016104e-05\n","Epoch  17 Batch  76 / 228  Training Loss  2.286411836394109e-05\n","Epoch  17 Batch  77 / 228  Training Loss  3.289090818725526e-05\n","Epoch  17 Batch  78 / 228  Training Loss  1.940551919688005e-05\n","Epoch  17 Batch  79 / 228  Training Loss  3.059801019844599e-05\n","Epoch  17 Batch  80 / 228  Training Loss  3.155668673571199e-05\n","Epoch  17 Batch  81 / 228  Training Loss  3.0203820642782375e-05\n","Epoch  17 Batch  82 / 228  Training Loss  3.1725656299386173e-05\n","Epoch  17 Batch  83 / 228  Training Loss  4.875601007370278e-05\n","Epoch  17 Batch  84 / 228  Training Loss  2.493408646841999e-05\n","Epoch  17 Batch  85 / 228  Training Loss  4.849133620155044e-05\n","Epoch  17 Batch  86 / 228  Training Loss  2.949581539724022e-05\n","Epoch  17 Batch  87 / 228  Training Loss  4.181628173682839e-05\n","Epoch  17 Batch  88 / 228  Training Loss  2.03383660846157e-05\n","Epoch  17 Batch  89 / 228  Training Loss  3.6887107853544876e-05\n","Epoch  17 Batch  90 / 228  Training Loss  1.9944274754379876e-05\n","Epoch  17 Batch  91 / 228  Training Loss  3.5363475035410374e-05\n","Epoch  17 Batch  92 / 228  Training Loss  2.2178777726367116e-05\n","Epoch  17 Batch  93 / 228  Training Loss  3.8969395973253995e-05\n","Epoch  17 Batch  94 / 228  Training Loss  3.827325417660177e-05\n","Epoch  17 Batch  95 / 228  Training Loss  3.429002390475944e-05\n","Epoch  17 Batch  96 / 228  Training Loss  3.0960054573370144e-05\n","Epoch  17 Batch  97 / 228  Training Loss  3.472756361588836e-05\n","Epoch  17 Batch  98 / 228  Training Loss  2.4469345589750446e-05\n","Epoch  17 Batch  99 / 228  Training Loss  3.085992648266256e-05\n","Epoch  17 Batch  100 / 228  Training Loss  1.5871133655309677e-05\n","Epoch  17 Batch  101 / 228  Training Loss  3.329648097860627e-05\n","Epoch  17 Batch  102 / 228  Training Loss  2.98791619570693e-05\n","Epoch  17 Batch  103 / 228  Training Loss  4.361374885775149e-05\n","Epoch  17 Batch  104 / 228  Training Loss  3.793536961893551e-05\n","Epoch  17 Batch  105 / 228  Training Loss  3.853319867630489e-05\n","Epoch  17 Batch  106 / 228  Training Loss  2.6929588784696534e-05\n","Epoch  17 Batch  107 / 228  Training Loss  2.6248142603435554e-05\n","Epoch  17 Batch  108 / 228  Training Loss  5.1016133511438966e-05\n","Epoch  17 Batch  109 / 228  Training Loss  2.7804358978755772e-05\n","Epoch  17 Batch  110 / 228  Training Loss  3.1949435651768e-05\n","Epoch  17 Batch  111 / 228  Training Loss  2.2159740183269605e-05\n","Epoch  17 Batch  112 / 228  Training Loss  2.9782264391542412e-05\n","Epoch  17 Batch  113 / 228  Training Loss  3.5714689147425815e-05\n","Epoch  17 Batch  114 / 228  Training Loss  2.4766453861957416e-05\n","Epoch  17 Batch  115 / 228  Training Loss  2.017575388890691e-05\n","Epoch  17 Batch  116 / 228  Training Loss  3.07941299979575e-05\n","Epoch  17 Batch  117 / 228  Training Loss  3.9126833144109696e-05\n","Epoch  17 Batch  118 / 228  Training Loss  2.4342001779587008e-05\n","Epoch  17 Batch  119 / 228  Training Loss  2.1064075554022565e-05\n","Epoch  17 Batch  120 / 228  Training Loss  2.8602280508494005e-05\n","Epoch  17 Batch  121 / 228  Training Loss  3.056088826269843e-05\n","Epoch  17 Batch  122 / 228  Training Loss  4.308828283683397e-05\n","Epoch  17 Batch  123 / 228  Training Loss  2.1989648303133436e-05\n","Epoch  17 Batch  124 / 228  Training Loss  3.4416178095852956e-05\n","Epoch  17 Batch  125 / 228  Training Loss  3.155521335429512e-05\n","Epoch  17 Batch  126 / 228  Training Loss  1.882782089523971e-05\n","Epoch  17 Batch  127 / 228  Training Loss  3.952319821109995e-05\n","Epoch  17 Batch  128 / 228  Training Loss  3.5357796150492504e-05\n","Epoch  17 Batch  129 / 228  Training Loss  2.0440165826585144e-05\n","Epoch  17 Batch  130 / 228  Training Loss  3.088794619543478e-05\n","Epoch  17 Batch  131 / 228  Training Loss  2.138116724381689e-05\n","Epoch  17 Batch  132 / 228  Training Loss  2.6047593564726412e-05\n","Epoch  17 Batch  133 / 228  Training Loss  4.9049995141103864e-05\n","Epoch  17 Batch  134 / 228  Training Loss  2.877060069295112e-05\n","Epoch  17 Batch  135 / 228  Training Loss  2.0362002032925375e-05\n","Epoch  17 Batch  136 / 228  Training Loss  4.297676059650257e-05\n","Epoch  17 Batch  137 / 228  Training Loss  3.270984598202631e-05\n","Epoch  17 Batch  138 / 228  Training Loss  3.326249498059042e-05\n","Epoch  17 Batch  139 / 228  Training Loss  3.1392115488415584e-05\n","Epoch  17 Batch  140 / 228  Training Loss  3.2203737646341324e-05\n","Epoch  17 Batch  141 / 228  Training Loss  1.8002105207415298e-05\n","Epoch  17 Batch  142 / 228  Training Loss  3.3238808100577444e-05\n","Epoch  17 Batch  143 / 228  Training Loss  2.554579077695962e-05\n","Epoch  17 Batch  144 / 228  Training Loss  2.6087776859640144e-05\n","Epoch  17 Batch  145 / 228  Training Loss  3.734394704224542e-05\n","Epoch  17 Batch  146 / 228  Training Loss  2.435438182146754e-05\n","Epoch  17 Batch  147 / 228  Training Loss  2.368323112023063e-05\n","Epoch  17 Batch  148 / 228  Training Loss  3.504442429402843e-05\n","Epoch  17 Batch  149 / 228  Training Loss  2.7839265385409817e-05\n","Epoch  17 Batch  150 / 228  Training Loss  2.0895928173558787e-05\n","Epoch  17 Batch  151 / 228  Training Loss  2.6139347028220072e-05\n","Epoch  17 Batch  152 / 228  Training Loss  2.6421999791637063e-05\n","Epoch  17 Batch  153 / 228  Training Loss  2.4830322217894718e-05\n","Epoch  17 Batch  154 / 228  Training Loss  2.703168684092816e-05\n","Epoch  17 Batch  155 / 228  Training Loss  2.4166101866285317e-05\n","Epoch  17 Batch  156 / 228  Training Loss  2.6914067348116077e-05\n","Epoch  17 Batch  157 / 228  Training Loss  3.770421608351171e-05\n","Epoch  17 Batch  158 / 228  Training Loss  2.6040914235636592e-05\n","Epoch  17 Batch  159 / 228  Training Loss  2.8994199965381995e-05\n","Epoch  17 Batch  160 / 228  Training Loss  2.8899812605232e-05\n","Epoch  17 Batch  161 / 228  Training Loss  2.218764348072e-05\n","Epoch  17 Batch  162 / 228  Training Loss  2.726548700593412e-05\n","Epoch  17 Batch  163 / 228  Training Loss  3.164710506098345e-05\n","Epoch  17 Batch  164 / 228  Training Loss  3.294132329756394e-05\n","Epoch  17 Batch  165 / 228  Training Loss  2.42906990024494e-05\n","Epoch  17 Batch  166 / 228  Training Loss  1.895416971819941e-05\n","Epoch  17 Batch  167 / 228  Training Loss  2.6828924092114903e-05\n","Epoch  17 Batch  168 / 228  Training Loss  2.7910185963264666e-05\n","Epoch  17 Batch  169 / 228  Training Loss  2.9793172870995477e-05\n","Epoch  17 Batch  170 / 228  Training Loss  2.865646092686802e-05\n","Epoch  17 Batch  171 / 228  Training Loss  2.1442016077344306e-05\n","Epoch  17 Batch  172 / 228  Training Loss  2.5233695851056837e-05\n","Epoch  17 Batch  173 / 228  Training Loss  3.077245128224604e-05\n","Epoch  17 Batch  174 / 228  Training Loss  4.708996129920706e-05\n","Epoch  17 Batch  175 / 228  Training Loss  2.2959979105507955e-05\n","Epoch  17 Batch  176 / 228  Training Loss  3.6922621802659705e-05\n","Epoch  17 Batch  177 / 228  Training Loss  2.2975626052357256e-05\n","Epoch  17 Batch  178 / 228  Training Loss  2.8063994250260293e-05\n","Epoch  17 Batch  179 / 228  Training Loss  3.0106946724117734e-05\n","Epoch  17 Batch  180 / 228  Training Loss  2.1485962861333974e-05\n","Epoch  17 Batch  181 / 228  Training Loss  3.5098408261546865e-05\n","Epoch  17 Batch  182 / 228  Training Loss  2.8138008929090574e-05\n","Epoch  17 Batch  183 / 228  Training Loss  3.249249130021781e-05\n","Epoch  17 Batch  184 / 228  Training Loss  3.754652425413951e-05\n","Epoch  17 Batch  185 / 228  Training Loss  2.920482620538678e-05\n","Epoch  17 Batch  186 / 228  Training Loss  3.718026709975675e-05\n","Epoch  17 Batch  187 / 228  Training Loss  3.5408233088674024e-05\n","Epoch  17 Batch  188 / 228  Training Loss  2.9503158657462336e-05\n","Epoch  17 Batch  189 / 228  Training Loss  2.0572313587763347e-05\n","Epoch  17 Batch  190 / 228  Training Loss  2.7978854632237926e-05\n","Epoch  17 Batch  191 / 228  Training Loss  2.9750901376246475e-05\n","Epoch  17 Batch  192 / 228  Training Loss  2.015310019487515e-05\n","Epoch  17 Batch  193 / 228  Training Loss  2.6495215934119187e-05\n","Epoch  17 Batch  194 / 228  Training Loss  3.360672417329624e-05\n","Epoch  17 Batch  195 / 228  Training Loss  2.9757738957414404e-05\n","Epoch  17 Batch  196 / 228  Training Loss  4.734718459076248e-05\n","Epoch  17 Batch  197 / 228  Training Loss  2.6860288926400244e-05\n","Epoch  17 Batch  198 / 228  Training Loss  2.5252546038245782e-05\n","Epoch  17 Batch  199 / 228  Training Loss  2.6401723516755737e-05\n","Epoch  17 Batch  200 / 228  Training Loss  2.6643683668226004e-05\n","Epoch  17 Batch  201 / 228  Training Loss  2.506094824639149e-05\n","Epoch  17 Batch  202 / 228  Training Loss  2.807865894283168e-05\n","Epoch  17 Batch  203 / 228  Training Loss  3.302294862805866e-05\n","Epoch  17 Batch  204 / 228  Training Loss  2.9839404305676e-05\n","Epoch  17 Batch  205 / 228  Training Loss  3.050359919143375e-05\n","Epoch  17 Batch  206 / 228  Training Loss  3.889608706231229e-05\n","Epoch  17 Batch  207 / 228  Training Loss  2.5876672225422226e-05\n","Epoch  17 Batch  208 / 228  Training Loss  3.64653387805447e-05\n","Epoch  17 Batch  209 / 228  Training Loss  3.2069503504317254e-05\n","Epoch  17 Batch  210 / 228  Training Loss  2.0936768123647198e-05\n","Epoch  17 Batch  211 / 228  Training Loss  3.1722440326120704e-05\n","Epoch  17 Batch  212 / 228  Training Loss  1.9018676539417356e-05\n","Epoch  17 Batch  213 / 228  Training Loss  4.2359850340289995e-05\n","Epoch  17 Batch  214 / 228  Training Loss  2.4534121621400118e-05\n","Epoch  17 Batch  215 / 228  Training Loss  2.1750773157691583e-05\n","Epoch  17 Batch  216 / 228  Training Loss  2.4951779778348282e-05\n","Epoch  17 Batch  217 / 228  Training Loss  2.9058652216917835e-05\n","Epoch  17 Batch  218 / 228  Training Loss  3.243895844207145e-05\n","Epoch  17 Batch  219 / 228  Training Loss  2.1017182007199153e-05\n","Epoch  17 Batch  220 / 228  Training Loss  3.101694892393425e-05\n","Epoch  17 Batch  221 / 228  Training Loss  2.715166556299664e-05\n","Epoch  17 Batch  222 / 228  Training Loss  2.3855198378441855e-05\n","Epoch  17 Batch  223 / 228  Training Loss  2.3993607101147063e-05\n","Epoch  17 Batch  224 / 228  Training Loss  3.5375684092286974e-05\n","Epoch  17 Batch  225 / 228  Training Loss  2.283375033584889e-05\n","Epoch  17 Batch  226 / 228  Training Loss  3.055761771975085e-05\n","Epoch  17 Batch  227 / 228  Training Loss  1.6383282854803838e-05\n","  18    |    -    |   0.000030   | 99.199695\n","----------------------------------------------------------------------\n","Running epoch: 18\n","Epoch  18 Batch  0 / 228  Training Loss  4.63706164737232e-05\n","Epoch  18 Batch  1 / 228  Training Loss  3.078236841247417e-05\n","Epoch  18 Batch  2 / 228  Training Loss  2.5582770831533708e-05\n","Epoch  18 Batch  3 / 228  Training Loss  2.4276338081108406e-05\n","Epoch  18 Batch  4 / 228  Training Loss  3.177075268467888e-05\n","Epoch  18 Batch  5 / 228  Training Loss  3.5485525586409494e-05\n","Epoch  18 Batch  6 / 228  Training Loss  1.7738580936565995e-05\n","Epoch  18 Batch  7 / 228  Training Loss  3.497583747957833e-05\n","Epoch  18 Batch  8 / 228  Training Loss  2.860389940906316e-05\n","Epoch  18 Batch  9 / 228  Training Loss  3.2070729503175244e-05\n","Epoch  18 Batch  10 / 228  Training Loss  2.527915421524085e-05\n","Epoch  18 Batch  11 / 228  Training Loss  2.6008685381384566e-05\n","Epoch  18 Batch  12 / 228  Training Loss  2.4742670575506054e-05\n","Epoch  18 Batch  13 / 228  Training Loss  2.552903606556356e-05\n","Epoch  18 Batch  14 / 228  Training Loss  2.078110264847055e-05\n","Epoch  18 Batch  15 / 228  Training Loss  2.7491245418787003e-05\n","Epoch  18 Batch  16 / 228  Training Loss  2.8252030460862443e-05\n","Epoch  18 Batch  17 / 228  Training Loss  2.1388155801105313e-05\n","Epoch  18 Batch  18 / 228  Training Loss  2.8441092581488192e-05\n","Epoch  18 Batch  19 / 228  Training Loss  3.189760536770336e-05\n","Epoch  18 Batch  20 / 228  Training Loss  1.9581266315071844e-05\n","Epoch  18 Batch  21 / 228  Training Loss  2.3582468202221207e-05\n","Epoch  18 Batch  22 / 228  Training Loss  2.9967404771014117e-05\n","Epoch  18 Batch  23 / 228  Training Loss  2.9279724913067184e-05\n","Epoch  18 Batch  24 / 228  Training Loss  2.2127802367322147e-05\n","Epoch  18 Batch  25 / 228  Training Loss  2.360632788622752e-05\n","Epoch  18 Batch  26 / 228  Training Loss  3.184286106261425e-05\n","Epoch  18 Batch  27 / 228  Training Loss  2.247519660159014e-05\n","Epoch  18 Batch  28 / 228  Training Loss  2.7075939215137623e-05\n","Epoch  18 Batch  29 / 228  Training Loss  3.2609004847472534e-05\n","Epoch  18 Batch  30 / 228  Training Loss  2.50236589636188e-05\n","Epoch  18 Batch  31 / 228  Training Loss  2.2661568436888047e-05\n","Epoch  18 Batch  32 / 228  Training Loss  2.6477122446522117e-05\n","Epoch  18 Batch  33 / 228  Training Loss  2.65134331129957e-05\n","Epoch  18 Batch  34 / 228  Training Loss  2.6946980142383836e-05\n","Epoch  18 Batch  35 / 228  Training Loss  1.6897343812161125e-05\n","Epoch  18 Batch  36 / 228  Training Loss  3.369440310052596e-05\n","Epoch  18 Batch  37 / 228  Training Loss  3.981285044574179e-05\n","Epoch  18 Batch  38 / 228  Training Loss  2.568662239355035e-05\n","Epoch  18 Batch  39 / 228  Training Loss  2.958617187687196e-05\n","Epoch  18 Batch  40 / 228  Training Loss  2.5438213924644515e-05\n","Epoch  18 Batch  41 / 228  Training Loss  2.081234924844466e-05\n","Epoch  18 Batch  42 / 228  Training Loss  2.1051902876934037e-05\n","Epoch  18 Batch  43 / 228  Training Loss  2.674819916137494e-05\n","Epoch  18 Batch  44 / 228  Training Loss  2.8270262191654183e-05\n","Epoch  18 Batch  45 / 228  Training Loss  2.7742633392335847e-05\n","Epoch  18 Batch  46 / 228  Training Loss  2.5452905902056955e-05\n","Epoch  18 Batch  47 / 228  Training Loss  3.7556579627562314e-05\n","Epoch  18 Batch  48 / 228  Training Loss  2.860338099708315e-05\n","Epoch  18 Batch  49 / 228  Training Loss  2.8674863642663695e-05\n","Epoch  18 Batch  50 / 228  Training Loss  1.9468887330731377e-05\n","Epoch  18 Batch  51 / 228  Training Loss  1.933642488438636e-05\n","Epoch  18 Batch  52 / 228  Training Loss  3.1473333365283906e-05\n","Epoch  18 Batch  53 / 228  Training Loss  2.5907836970873177e-05\n","Epoch  18 Batch  54 / 228  Training Loss  2.2819654986960813e-05\n","Epoch  18 Batch  55 / 228  Training Loss  2.3744756617816165e-05\n","Epoch  18 Batch  56 / 228  Training Loss  2.7674908778863028e-05\n","Epoch  18 Batch  57 / 228  Training Loss  2.8545342502184212e-05\n","Epoch  18 Batch  58 / 228  Training Loss  2.0542374841170385e-05\n","Epoch  18 Batch  59 / 228  Training Loss  2.8377960916259326e-05\n","Epoch  18 Batch  60 / 228  Training Loss  3.262602331233211e-05\n","Epoch  18 Batch  61 / 228  Training Loss  2.544898961787112e-05\n","Epoch  18 Batch  62 / 228  Training Loss  2.945919732155744e-05\n","Epoch  18 Batch  63 / 228  Training Loss  2.464955650793854e-05\n","Epoch  18 Batch  64 / 228  Training Loss  3.016935079358518e-05\n","Epoch  18 Batch  65 / 228  Training Loss  2.0698371372418478e-05\n","Epoch  18 Batch  66 / 228  Training Loss  3.3597541914787143e-05\n","Epoch  18 Batch  67 / 228  Training Loss  3.4086879168171436e-05\n","Epoch  18 Batch  68 / 228  Training Loss  2.6384417651570402e-05\n","Epoch  18 Batch  69 / 228  Training Loss  2.4078010028460994e-05\n","Epoch  18 Batch  70 / 228  Training Loss  2.4480035790475085e-05\n","Epoch  18 Batch  71 / 228  Training Loss  3.3413096389267594e-05\n","Epoch  18 Batch  72 / 228  Training Loss  2.831822530424688e-05\n","Epoch  18 Batch  73 / 228  Training Loss  2.1649017071467824e-05\n","Epoch  18 Batch  74 / 228  Training Loss  2.057838355540298e-05\n","Epoch  18 Batch  75 / 228  Training Loss  2.3738099116599187e-05\n","Epoch  18 Batch  76 / 228  Training Loss  1.877707472885959e-05\n","Epoch  18 Batch  77 / 228  Training Loss  2.6737678126664832e-05\n","Epoch  18 Batch  78 / 228  Training Loss  3.57451535819564e-05\n","Epoch  18 Batch  79 / 228  Training Loss  2.5445484425290488e-05\n","Epoch  18 Batch  80 / 228  Training Loss  2.8229009330971166e-05\n","Epoch  18 Batch  81 / 228  Training Loss  3.4151253203162923e-05\n","Epoch  18 Batch  82 / 228  Training Loss  2.2719688786310144e-05\n","Epoch  18 Batch  83 / 228  Training Loss  1.9812297978205606e-05\n","Epoch  18 Batch  84 / 228  Training Loss  2.889044480980374e-05\n","Epoch  18 Batch  85 / 228  Training Loss  1.5741299648652785e-05\n","Epoch  18 Batch  86 / 228  Training Loss  1.7196780390804633e-05\n","Epoch  18 Batch  87 / 228  Training Loss  2.2801290469942614e-05\n","Epoch  18 Batch  88 / 228  Training Loss  3.0102393793640658e-05\n","Epoch  18 Batch  89 / 228  Training Loss  2.6779522158904e-05\n","Epoch  18 Batch  90 / 228  Training Loss  2.652263174240943e-05\n","Epoch  18 Batch  91 / 228  Training Loss  1.9148212231812067e-05\n","Epoch  18 Batch  92 / 228  Training Loss  4.1015344322659075e-05\n","Epoch  18 Batch  93 / 228  Training Loss  2.5876153813442215e-05\n","Epoch  18 Batch  94 / 228  Training Loss  4.6143824874889106e-05\n","Epoch  18 Batch  95 / 228  Training Loss  3.306708094896749e-05\n","Epoch  18 Batch  96 / 228  Training Loss  2.4951241357484832e-05\n","Epoch  18 Batch  97 / 228  Training Loss  2.256611332995817e-05\n","Epoch  18 Batch  98 / 228  Training Loss  3.0497680199914612e-05\n","Epoch  18 Batch  99 / 228  Training Loss  1.5892796000116505e-05\n","Epoch  18 Batch  100 / 228  Training Loss  3.900331284967251e-05\n","Epoch  18 Batch  101 / 228  Training Loss  2.971268622786738e-05\n","Epoch  18 Batch  102 / 228  Training Loss  1.9827210053335875e-05\n","Epoch  18 Batch  103 / 228  Training Loss  2.9244934921734966e-05\n","Epoch  18 Batch  104 / 228  Training Loss  2.6128656827495433e-05\n","Epoch  18 Batch  105 / 228  Training Loss  1.4296437257144134e-05\n","Epoch  18 Batch  106 / 228  Training Loss  2.612220669107046e-05\n","Epoch  18 Batch  107 / 228  Training Loss  3.675655898405239e-05\n","Epoch  18 Batch  108 / 228  Training Loss  2.3058011720422655e-05\n","Epoch  18 Batch  109 / 228  Training Loss  3.3807467843871564e-05\n","Epoch  18 Batch  110 / 228  Training Loss  2.692824273253791e-05\n","Epoch  18 Batch  111 / 228  Training Loss  2.9314320272533223e-05\n","Epoch  18 Batch  112 / 228  Training Loss  2.8215217753313482e-05\n","Epoch  18 Batch  113 / 228  Training Loss  2.0814351955777965e-05\n","Epoch  18 Batch  114 / 228  Training Loss  3.0022134524188004e-05\n","Epoch  18 Batch  115 / 228  Training Loss  2.333424890821334e-05\n","Epoch  18 Batch  116 / 228  Training Loss  2.0843262973357923e-05\n","Epoch  18 Batch  117 / 228  Training Loss  2.766581928881351e-05\n","Epoch  18 Batch  118 / 228  Training Loss  2.0144663722021505e-05\n","Epoch  18 Batch  119 / 228  Training Loss  1.9738636183319613e-05\n","Epoch  18 Batch  120 / 228  Training Loss  3.406468385946937e-05\n","Epoch  18 Batch  121 / 228  Training Loss  2.9255450499476865e-05\n","Epoch  18 Batch  122 / 228  Training Loss  2.8582167942658998e-05\n","Epoch  18 Batch  123 / 228  Training Loss  2.508042052795645e-05\n","Epoch  18 Batch  124 / 228  Training Loss  1.544045881018974e-05\n","Epoch  18 Batch  125 / 228  Training Loss  2.485111508576665e-05\n","Epoch  18 Batch  126 / 228  Training Loss  2.1546973584918305e-05\n","Epoch  18 Batch  127 / 228  Training Loss  2.6486577553441748e-05\n","Epoch  18 Batch  128 / 228  Training Loss  2.9923603506176732e-05\n","Epoch  18 Batch  129 / 228  Training Loss  4.56005145679228e-05\n","Epoch  18 Batch  130 / 228  Training Loss  2.3026130293146707e-05\n","Epoch  18 Batch  131 / 228  Training Loss  2.3290631361305714e-05\n","Epoch  18 Batch  132 / 228  Training Loss  1.729374889691826e-05\n","Epoch  18 Batch  133 / 228  Training Loss  2.8753667720593512e-05\n","Epoch  18 Batch  134 / 228  Training Loss  2.0654919353546575e-05\n","Epoch  18 Batch  135 / 228  Training Loss  2.0579853298841044e-05\n","Epoch  18 Batch  136 / 228  Training Loss  2.8950724299647845e-05\n","Epoch  18 Batch  137 / 228  Training Loss  2.0528330423985608e-05\n","Epoch  18 Batch  138 / 228  Training Loss  3.391222708160058e-05\n","Epoch  18 Batch  139 / 228  Training Loss  2.122965997841675e-05\n","Epoch  18 Batch  140 / 228  Training Loss  3.0410674298764206e-05\n","Epoch  18 Batch  141 / 228  Training Loss  3.0200357286958024e-05\n","Epoch  18 Batch  142 / 228  Training Loss  2.777838017209433e-05\n","Epoch  18 Batch  143 / 228  Training Loss  3.657817069324665e-05\n","Epoch  18 Batch  144 / 228  Training Loss  1.6213434719247743e-05\n","Epoch  18 Batch  145 / 228  Training Loss  2.9078375519020483e-05\n","Epoch  18 Batch  146 / 228  Training Loss  2.968545231851749e-05\n","Epoch  18 Batch  147 / 228  Training Loss  2.440188472974114e-05\n","Epoch  18 Batch  148 / 228  Training Loss  2.183846299885772e-05\n","Epoch  18 Batch  149 / 228  Training Loss  2.6829569833353162e-05\n","Epoch  18 Batch  150 / 228  Training Loss  2.7473210138850845e-05\n","Epoch  18 Batch  151 / 228  Training Loss  2.364739339100197e-05\n","Epoch  18 Batch  152 / 228  Training Loss  2.774122913251631e-05\n","Epoch  18 Batch  153 / 228  Training Loss  3.4129123378079385e-05\n","Epoch  18 Batch  154 / 228  Training Loss  1.7036794815794565e-05\n","Epoch  18 Batch  155 / 228  Training Loss  2.3254888219526038e-05\n","Epoch  18 Batch  156 / 228  Training Loss  2.9880340662202798e-05\n","Epoch  18 Batch  157 / 228  Training Loss  2.229975325462874e-05\n","Epoch  18 Batch  158 / 228  Training Loss  2.6668003556551412e-05\n","Epoch  18 Batch  159 / 228  Training Loss  2.455733556416817e-05\n","Epoch  18 Batch  160 / 228  Training Loss  2.7803904231404886e-05\n","Epoch  18 Batch  161 / 228  Training Loss  3.167943941662088e-05\n","Epoch  18 Batch  162 / 228  Training Loss  1.6137990314746276e-05\n","Epoch  18 Batch  163 / 228  Training Loss  2.7793732442660257e-05\n","Epoch  18 Batch  164 / 228  Training Loss  2.199659138568677e-05\n","Epoch  18 Batch  165 / 228  Training Loss  1.7557649698574096e-05\n","Epoch  18 Batch  166 / 228  Training Loss  2.754793422354851e-05\n","Epoch  18 Batch  167 / 228  Training Loss  1.700337088550441e-05\n","Epoch  18 Batch  168 / 228  Training Loss  3.250420559197664e-05\n","Epoch  18 Batch  169 / 228  Training Loss  2.8809037758037448e-05\n","Epoch  18 Batch  170 / 228  Training Loss  2.8320981073193252e-05\n","Epoch  18 Batch  171 / 228  Training Loss  2.3974447685759515e-05\n","Epoch  18 Batch  172 / 228  Training Loss  3.0812072509434074e-05\n","Epoch  18 Batch  173 / 228  Training Loss  2.4357494112337008e-05\n","Epoch  18 Batch  174 / 228  Training Loss  1.8446753529133275e-05\n","Epoch  18 Batch  175 / 228  Training Loss  2.6670222723623738e-05\n","Epoch  18 Batch  176 / 228  Training Loss  3.72691938537173e-05\n","Epoch  18 Batch  177 / 228  Training Loss  2.8525664674816653e-05\n","Epoch  18 Batch  178 / 228  Training Loss  2.2658165107714012e-05\n","Epoch  18 Batch  179 / 228  Training Loss  2.0171914002276026e-05\n","Epoch  18 Batch  180 / 228  Training Loss  2.627173307701014e-05\n","Epoch  18 Batch  181 / 228  Training Loss  2.805474287015386e-05\n","Epoch  18 Batch  182 / 228  Training Loss  2.689107168407645e-05\n","Epoch  18 Batch  183 / 228  Training Loss  3.3658059692243114e-05\n","Epoch  18 Batch  184 / 228  Training Loss  2.3347236492554657e-05\n","Epoch  18 Batch  185 / 228  Training Loss  1.901970426843036e-05\n","Epoch  18 Batch  186 / 228  Training Loss  2.8001979444525205e-05\n","Epoch  18 Batch  187 / 228  Training Loss  2.9759516110061668e-05\n","Epoch  18 Batch  188 / 228  Training Loss  2.30492623813916e-05\n","Epoch  18 Batch  189 / 228  Training Loss  2.0757030142704025e-05\n","Epoch  18 Batch  190 / 228  Training Loss  2.4517823476344347e-05\n","Epoch  18 Batch  191 / 228  Training Loss  2.7929627322009765e-05\n","Epoch  18 Batch  192 / 228  Training Loss  1.5813322534086183e-05\n","Epoch  18 Batch  193 / 228  Training Loss  2.7879219487658702e-05\n","Epoch  18 Batch  194 / 228  Training Loss  3.4304310247534886e-05\n","Epoch  18 Batch  195 / 228  Training Loss  2.897850026783999e-05\n","Epoch  18 Batch  196 / 228  Training Loss  2.4546761778765358e-05\n","Epoch  18 Batch  197 / 228  Training Loss  3.2561420084675774e-05\n","Epoch  18 Batch  198 / 228  Training Loss  1.854730726336129e-05\n","Epoch  18 Batch  199 / 228  Training Loss  3.018030292878393e-05\n","Epoch  18 Batch  200 / 228  Training Loss  2.6793899451149628e-05\n","Epoch  18 Batch  201 / 228  Training Loss  2.7064894311479293e-05\n","Epoch  18 Batch  202 / 228  Training Loss  1.740986954246182e-05\n","Epoch  18 Batch  203 / 228  Training Loss  3.199901766492985e-05\n","Epoch  18 Batch  204 / 228  Training Loss  2.7769241569330916e-05\n","Epoch  18 Batch  205 / 228  Training Loss  1.6638728993711993e-05\n","Epoch  18 Batch  206 / 228  Training Loss  1.8794775314745493e-05\n","Epoch  18 Batch  207 / 228  Training Loss  2.3542947019450366e-05\n","Epoch  18 Batch  208 / 228  Training Loss  2.253766797366552e-05\n","Epoch  18 Batch  209 / 228  Training Loss  2.1906847905484028e-05\n","Epoch  18 Batch  210 / 228  Training Loss  3.2461557566421106e-05\n","Epoch  18 Batch  211 / 228  Training Loss  2.644909000082407e-05\n","Epoch  18 Batch  212 / 228  Training Loss  2.0748939277837053e-05\n","Epoch  18 Batch  213 / 228  Training Loss  2.3770560801494867e-05\n","Epoch  18 Batch  214 / 228  Training Loss  3.62153587047942e-05\n","Epoch  18 Batch  215 / 228  Training Loss  2.3678698198636994e-05\n","Epoch  18 Batch  216 / 228  Training Loss  2.161690645152703e-05\n","Epoch  18 Batch  217 / 228  Training Loss  1.7079957615351304e-05\n","Epoch  18 Batch  218 / 228  Training Loss  2.4098653739201836e-05\n","Epoch  18 Batch  219 / 228  Training Loss  1.6194691852433607e-05\n","Epoch  18 Batch  220 / 228  Training Loss  2.673417839105241e-05\n","Epoch  18 Batch  221 / 228  Training Loss  3.244220351916738e-05\n","Epoch  18 Batch  222 / 228  Training Loss  2.601631058496423e-05\n","Epoch  18 Batch  223 / 228  Training Loss  2.8661839678534307e-05\n","Epoch  18 Batch  224 / 228  Training Loss  2.6298210286768153e-05\n","Epoch  18 Batch  225 / 228  Training Loss  3.5688884963747114e-05\n","Epoch  18 Batch  226 / 228  Training Loss  1.7251933968509547e-05\n","Epoch  18 Batch  227 / 228  Training Loss  3.0451657949015498e-05\n","  19    |    -    |   0.000026   | 99.199695\n","----------------------------------------------------------------------\n","Running epoch: 19\n","Epoch  19 Batch  0 / 228  Training Loss  3.713087789947167e-05\n","Epoch  19 Batch  1 / 228  Training Loss  2.8157999622635543e-05\n","Epoch  19 Batch  2 / 228  Training Loss  2.5256984372390434e-05\n","Epoch  19 Batch  3 / 228  Training Loss  2.5549892598064616e-05\n","Epoch  19 Batch  4 / 228  Training Loss  2.2805666958447546e-05\n","Epoch  19 Batch  5 / 228  Training Loss  2.7134641641168855e-05\n","Epoch  19 Batch  6 / 228  Training Loss  1.5245490430970676e-05\n","Epoch  19 Batch  7 / 228  Training Loss  2.2743337467545643e-05\n","Epoch  19 Batch  8 / 228  Training Loss  1.7136950191343203e-05\n","Epoch  19 Batch  9 / 228  Training Loss  2.0719176973216236e-05\n","Epoch  19 Batch  10 / 228  Training Loss  2.0209719878039323e-05\n","Epoch  19 Batch  11 / 228  Training Loss  1.9847970179398544e-05\n","Epoch  19 Batch  12 / 228  Training Loss  2.5418008590349928e-05\n","Epoch  19 Batch  13 / 228  Training Loss  1.943915594893042e-05\n","Epoch  19 Batch  14 / 228  Training Loss  2.482321360730566e-05\n","Epoch  19 Batch  15 / 228  Training Loss  1.5540957974735647e-05\n","Epoch  19 Batch  16 / 228  Training Loss  1.8911934603238478e-05\n","Epoch  19 Batch  17 / 228  Training Loss  3.029033905477263e-05\n","Epoch  19 Batch  18 / 228  Training Loss  2.3577496904181316e-05\n","Epoch  19 Batch  19 / 228  Training Loss  3.73803086404223e-05\n","Epoch  19 Batch  20 / 228  Training Loss  1.6732165022403933e-05\n","Epoch  19 Batch  21 / 228  Training Loss  3.069603189942427e-05\n","Epoch  19 Batch  22 / 228  Training Loss  1.8974964405060746e-05\n","Epoch  19 Batch  23 / 228  Training Loss  1.8062917661154643e-05\n","Epoch  19 Batch  24 / 228  Training Loss  2.112449328706134e-05\n","Epoch  19 Batch  25 / 228  Training Loss  2.226265860372223e-05\n","Epoch  19 Batch  26 / 228  Training Loss  2.323684202565346e-05\n","Epoch  19 Batch  27 / 228  Training Loss  3.507794099277817e-05\n","Epoch  19 Batch  28 / 228  Training Loss  2.4076251065707766e-05\n","Epoch  19 Batch  29 / 228  Training Loss  3.388724508113228e-05\n","Epoch  19 Batch  30 / 228  Training Loss  2.7420808692113496e-05\n","Epoch  19 Batch  31 / 228  Training Loss  2.0328012396930717e-05\n","Epoch  19 Batch  32 / 228  Training Loss  2.6494706617086194e-05\n","Epoch  19 Batch  33 / 228  Training Loss  2.718663563427981e-05\n","Epoch  19 Batch  34 / 228  Training Loss  2.5310368073405698e-05\n","Epoch  19 Batch  35 / 228  Training Loss  1.50354335346492e-05\n","Epoch  19 Batch  36 / 228  Training Loss  2.288081122969743e-05\n","Epoch  19 Batch  37 / 228  Training Loss  2.2389558580471203e-05\n","Epoch  19 Batch  38 / 228  Training Loss  2.6196730686933734e-05\n","Epoch  19 Batch  39 / 228  Training Loss  2.658890480233822e-05\n","Epoch  19 Batch  40 / 228  Training Loss  1.6107809642562643e-05\n","Epoch  19 Batch  41 / 228  Training Loss  1.8596147128846496e-05\n","Epoch  19 Batch  42 / 228  Training Loss  2.8537808248074725e-05\n","Epoch  19 Batch  43 / 228  Training Loss  2.170093466702383e-05\n","Epoch  19 Batch  44 / 228  Training Loss  1.9867091396008618e-05\n","Epoch  19 Batch  45 / 228  Training Loss  2.2239306417759508e-05\n","Epoch  19 Batch  46 / 228  Training Loss  1.54270492203068e-05\n","Epoch  19 Batch  47 / 228  Training Loss  2.2002222976880148e-05\n","Epoch  19 Batch  48 / 228  Training Loss  2.069244692393113e-05\n","Epoch  19 Batch  49 / 228  Training Loss  3.4080087061738595e-05\n","Epoch  19 Batch  50 / 228  Training Loss  2.5485012884018943e-05\n","Epoch  19 Batch  51 / 228  Training Loss  1.9677328964462504e-05\n","Epoch  19 Batch  52 / 228  Training Loss  1.9867280570906587e-05\n","Epoch  19 Batch  53 / 228  Training Loss  3.2224226742982864e-05\n","Epoch  19 Batch  54 / 228  Training Loss  2.788321035041008e-05\n","Epoch  19 Batch  55 / 228  Training Loss  2.0023802790092304e-05\n","Epoch  19 Batch  56 / 228  Training Loss  2.2110370991867967e-05\n","Epoch  19 Batch  57 / 228  Training Loss  1.8353312043473125e-05\n","Epoch  19 Batch  58 / 228  Training Loss  2.0187651898595504e-05\n","Epoch  19 Batch  59 / 228  Training Loss  1.655706910241861e-05\n","Epoch  19 Batch  60 / 228  Training Loss  2.0743254935950972e-05\n","Epoch  19 Batch  61 / 228  Training Loss  2.471480365784373e-05\n","Epoch  19 Batch  62 / 228  Training Loss  2.3292355763260275e-05\n","Epoch  19 Batch  63 / 228  Training Loss  2.2305721358861774e-05\n","Epoch  19 Batch  64 / 228  Training Loss  3.623569864430465e-05\n","Epoch  19 Batch  65 / 228  Training Loss  2.241896618215833e-05\n","Epoch  19 Batch  66 / 228  Training Loss  2.587459130154457e-05\n","Epoch  19 Batch  67 / 228  Training Loss  3.525313877617009e-05\n","Epoch  19 Batch  68 / 228  Training Loss  1.9614853954408318e-05\n","Epoch  19 Batch  69 / 228  Training Loss  1.793707997421734e-05\n","Epoch  19 Batch  70 / 228  Training Loss  2.6169736884185113e-05\n","Epoch  19 Batch  71 / 228  Training Loss  1.729695577523671e-05\n","Epoch  19 Batch  72 / 228  Training Loss  1.763974614732433e-05\n","Epoch  19 Batch  73 / 228  Training Loss  3.213626041542739e-05\n","Epoch  19 Batch  74 / 228  Training Loss  2.0799254343728535e-05\n","Epoch  19 Batch  75 / 228  Training Loss  1.539715231047012e-05\n","Epoch  19 Batch  76 / 228  Training Loss  2.5839650334091857e-05\n","Epoch  19 Batch  77 / 228  Training Loss  2.313623917871155e-05\n","Epoch  19 Batch  78 / 228  Training Loss  2.146598490071483e-05\n","Epoch  19 Batch  79 / 228  Training Loss  2.6284771593054757e-05\n","Epoch  19 Batch  80 / 228  Training Loss  2.8522594220703468e-05\n","Epoch  19 Batch  81 / 228  Training Loss  2.3448479623766616e-05\n","Epoch  19 Batch  82 / 228  Training Loss  1.812916889321059e-05\n","Epoch  19 Batch  83 / 228  Training Loss  2.1460817151819356e-05\n","Epoch  19 Batch  84 / 228  Training Loss  2.8561631552292965e-05\n","Epoch  19 Batch  85 / 228  Training Loss  2.443543417030014e-05\n","Epoch  19 Batch  86 / 228  Training Loss  2.104917984979693e-05\n","Epoch  19 Batch  87 / 228  Training Loss  1.6061614587670192e-05\n","Epoch  19 Batch  88 / 228  Training Loss  1.9294684534543194e-05\n","Epoch  19 Batch  89 / 228  Training Loss  2.9434380849124864e-05\n","Epoch  19 Batch  90 / 228  Training Loss  2.4767126888036728e-05\n","Epoch  19 Batch  91 / 228  Training Loss  2.9456748961820267e-05\n","Epoch  19 Batch  92 / 228  Training Loss  2.8811919037252665e-05\n","Epoch  19 Batch  93 / 228  Training Loss  3.31038681906648e-05\n","Epoch  19 Batch  94 / 228  Training Loss  2.1343383195926435e-05\n","Epoch  19 Batch  95 / 228  Training Loss  2.533889710321091e-05\n","Epoch  19 Batch  96 / 228  Training Loss  1.6588111975579523e-05\n","Epoch  19 Batch  97 / 228  Training Loss  3.8812682760180905e-05\n","Epoch  19 Batch  98 / 228  Training Loss  1.7932363334693946e-05\n","Epoch  19 Batch  99 / 228  Training Loss  2.0603405573638156e-05\n","Epoch  19 Batch  100 / 228  Training Loss  1.4887807083141524e-05\n","Epoch  19 Batch  101 / 228  Training Loss  1.9990982764284126e-05\n","Epoch  19 Batch  102 / 228  Training Loss  2.2275657101999968e-05\n","Epoch  19 Batch  103 / 228  Training Loss  2.7562240575207397e-05\n","Epoch  19 Batch  104 / 228  Training Loss  2.8001508326269686e-05\n","Epoch  19 Batch  105 / 228  Training Loss  2.9530070605687797e-05\n","Epoch  19 Batch  106 / 228  Training Loss  2.9689841539948247e-05\n","Epoch  19 Batch  107 / 228  Training Loss  3.244063555030152e-05\n","Epoch  19 Batch  108 / 228  Training Loss  1.9380984667805023e-05\n","Epoch  19 Batch  109 / 228  Training Loss  1.5245224858517759e-05\n","Epoch  19 Batch  110 / 228  Training Loss  2.115318420692347e-05\n","Epoch  19 Batch  111 / 228  Training Loss  4.041863576276228e-05\n","Epoch  19 Batch  112 / 228  Training Loss  2.3688282453804277e-05\n","Epoch  19 Batch  113 / 228  Training Loss  1.7698836018098518e-05\n","Epoch  19 Batch  114 / 228  Training Loss  1.9969740606029518e-05\n","Epoch  19 Batch  115 / 228  Training Loss  2.3427401174558327e-05\n","Epoch  19 Batch  116 / 228  Training Loss  2.310460331500508e-05\n","Epoch  19 Batch  117 / 228  Training Loss  1.6090654753497802e-05\n","Epoch  19 Batch  118 / 228  Training Loss  2.25788498937618e-05\n","Epoch  19 Batch  119 / 228  Training Loss  2.653961200849153e-05\n","Epoch  19 Batch  120 / 228  Training Loss  2.0687708456534892e-05\n","Epoch  19 Batch  121 / 228  Training Loss  2.9239905416034162e-05\n","Epoch  19 Batch  122 / 228  Training Loss  1.989578413486015e-05\n","Epoch  19 Batch  123 / 228  Training Loss  1.7197644410771318e-05\n","Epoch  19 Batch  124 / 228  Training Loss  2.2063502910896204e-05\n","Epoch  19 Batch  125 / 228  Training Loss  2.8045429644407704e-05\n","Epoch  19 Batch  126 / 228  Training Loss  1.6913789295358583e-05\n","Epoch  19 Batch  127 / 228  Training Loss  2.2711825295118615e-05\n","Epoch  19 Batch  128 / 228  Training Loss  2.9892024031141773e-05\n","Epoch  19 Batch  129 / 228  Training Loss  2.4147731892298907e-05\n","Epoch  19 Batch  130 / 228  Training Loss  2.096516800520476e-05\n","Epoch  19 Batch  131 / 228  Training Loss  2.5563151211827062e-05\n","Epoch  19 Batch  132 / 228  Training Loss  2.021727959800046e-05\n","Epoch  19 Batch  133 / 228  Training Loss  2.6786006856127642e-05\n","Epoch  19 Batch  134 / 228  Training Loss  1.8822658603312448e-05\n","Epoch  19 Batch  135 / 228  Training Loss  2.8716214728774503e-05\n","Epoch  19 Batch  136 / 228  Training Loss  1.501073256804375e-05\n","Epoch  19 Batch  137 / 228  Training Loss  2.347238114452921e-05\n","Epoch  19 Batch  138 / 228  Training Loss  2.4397653760388494e-05\n","Epoch  19 Batch  139 / 228  Training Loss  2.789208883768879e-05\n","Epoch  19 Batch  140 / 228  Training Loss  2.547494659665972e-05\n","Epoch  19 Batch  141 / 228  Training Loss  2.5541161448927596e-05\n","Epoch  19 Batch  142 / 228  Training Loss  2.9911539968452416e-05\n","Epoch  19 Batch  143 / 228  Training Loss  2.4492906959494576e-05\n","Epoch  19 Batch  144 / 228  Training Loss  1.4405627553060185e-05\n","Epoch  19 Batch  145 / 228  Training Loss  2.8853488402091898e-05\n","Epoch  19 Batch  146 / 228  Training Loss  2.1751502572442405e-05\n","Epoch  19 Batch  147 / 228  Training Loss  2.8270695111132227e-05\n","Epoch  19 Batch  148 / 228  Training Loss  2.808390672726091e-05\n","Epoch  19 Batch  149 / 228  Training Loss  2.7381029212847352e-05\n","Epoch  19 Batch  150 / 228  Training Loss  2.0709168893517926e-05\n","Epoch  19 Batch  151 / 228  Training Loss  2.2017966330167837e-05\n","Epoch  19 Batch  152 / 228  Training Loss  2.613003016449511e-05\n","Epoch  19 Batch  153 / 228  Training Loss  2.5935998564818874e-05\n","Epoch  19 Batch  154 / 228  Training Loss  2.6619993150234222e-05\n","Epoch  19 Batch  155 / 228  Training Loss  1.8796734366333112e-05\n","Epoch  19 Batch  156 / 228  Training Loss  2.0421703084139153e-05\n","Epoch  19 Batch  157 / 228  Training Loss  1.78554946614895e-05\n","Epoch  19 Batch  158 / 228  Training Loss  1.857117604231462e-05\n","Epoch  19 Batch  159 / 228  Training Loss  2.4474607926094905e-05\n","Epoch  19 Batch  160 / 228  Training Loss  2.255291292385664e-05\n","Epoch  19 Batch  161 / 228  Training Loss  3.383341390872374e-05\n","Epoch  19 Batch  162 / 228  Training Loss  1.8940070731332526e-05\n","Epoch  19 Batch  163 / 228  Training Loss  2.015812788158655e-05\n","Epoch  19 Batch  164 / 228  Training Loss  2.0419745851540938e-05\n","Epoch  19 Batch  165 / 228  Training Loss  2.4043172743404284e-05\n","Epoch  19 Batch  166 / 228  Training Loss  2.265099283249583e-05\n","Epoch  19 Batch  167 / 228  Training Loss  2.670774483704008e-05\n","Epoch  19 Batch  168 / 228  Training Loss  2.4491053409292363e-05\n","Epoch  19 Batch  169 / 228  Training Loss  1.8906845070887357e-05\n","Epoch  19 Batch  170 / 228  Training Loss  1.9024682842427865e-05\n","Epoch  19 Batch  171 / 228  Training Loss  2.4150105673470534e-05\n","Epoch  19 Batch  172 / 228  Training Loss  2.7027257601730525e-05\n","Epoch  19 Batch  173 / 228  Training Loss  3.097274384344928e-05\n","Epoch  19 Batch  174 / 228  Training Loss  2.9082148103043437e-05\n","Epoch  19 Batch  175 / 228  Training Loss  3.519479287206195e-05\n","Epoch  19 Batch  176 / 228  Training Loss  2.307490103703458e-05\n","Epoch  19 Batch  177 / 228  Training Loss  2.359420614084229e-05\n","Epoch  19 Batch  178 / 228  Training Loss  2.553165177232586e-05\n","Epoch  19 Batch  179 / 228  Training Loss  1.8477086996426806e-05\n","Epoch  19 Batch  180 / 228  Training Loss  2.353149830014445e-05\n","Epoch  19 Batch  181 / 228  Training Loss  2.6799822080647573e-05\n","Epoch  19 Batch  182 / 228  Training Loss  1.562780926178675e-05\n","Epoch  19 Batch  183 / 228  Training Loss  1.7972237401409075e-05\n","Epoch  19 Batch  184 / 228  Training Loss  2.0404679162311368e-05\n","Epoch  19 Batch  185 / 228  Training Loss  1.394477476424072e-05\n","Epoch  19 Batch  186 / 228  Training Loss  2.6306404834031127e-05\n","Epoch  19 Batch  187 / 228  Training Loss  2.0745041183545254e-05\n","Epoch  19 Batch  188 / 228  Training Loss  1.8746473870123737e-05\n","Epoch  19 Batch  189 / 228  Training Loss  2.468699312885292e-05\n","Epoch  19 Batch  190 / 228  Training Loss  1.744358814903535e-05\n","Epoch  19 Batch  191 / 228  Training Loss  2.9301660106284544e-05\n","Epoch  19 Batch  192 / 228  Training Loss  2.8876904252683744e-05\n","Epoch  19 Batch  193 / 228  Training Loss  2.4989287339849398e-05\n","Epoch  19 Batch  194 / 228  Training Loss  2.4529621441615745e-05\n","Epoch  19 Batch  195 / 228  Training Loss  1.9798344510491006e-05\n","Epoch  19 Batch  196 / 228  Training Loss  2.2611280655837618e-05\n","Epoch  19 Batch  197 / 228  Training Loss  2.2207083020475693e-05\n","Epoch  19 Batch  198 / 228  Training Loss  2.8747643227688968e-05\n","Epoch  19 Batch  199 / 228  Training Loss  1.8928465578937903e-05\n","Epoch  19 Batch  200 / 228  Training Loss  2.255828985653352e-05\n","Epoch  19 Batch  201 / 228  Training Loss  2.1110174202476628e-05\n","Epoch  19 Batch  202 / 228  Training Loss  2.3635997422388755e-05\n","Epoch  19 Batch  203 / 228  Training Loss  2.362963823543396e-05\n","Epoch  19 Batch  204 / 228  Training Loss  1.6741907529649325e-05\n","Epoch  19 Batch  205 / 228  Training Loss  2.361926635785494e-05\n","Epoch  19 Batch  206 / 228  Training Loss  2.9210897992015816e-05\n","Epoch  19 Batch  207 / 228  Training Loss  3.824973464361392e-05\n","Epoch  19 Batch  208 / 228  Training Loss  2.6650490326574072e-05\n","Epoch  19 Batch  209 / 228  Training Loss  2.73819259746233e-05\n","Epoch  19 Batch  210 / 228  Training Loss  2.3212114683701657e-05\n","Epoch  19 Batch  211 / 228  Training Loss  2.5946452296921052e-05\n","Epoch  19 Batch  212 / 228  Training Loss  1.8472916053724475e-05\n","Epoch  19 Batch  213 / 228  Training Loss  1.7692997062113136e-05\n","Epoch  19 Batch  214 / 228  Training Loss  2.302907159901224e-05\n","Epoch  19 Batch  215 / 228  Training Loss  1.796427932276856e-05\n","Epoch  19 Batch  216 / 228  Training Loss  2.177943679271266e-05\n","Epoch  19 Batch  217 / 228  Training Loss  1.9202569092158228e-05\n","Epoch  19 Batch  218 / 228  Training Loss  1.8065289623336866e-05\n","Epoch  19 Batch  219 / 228  Training Loss  3.7429148505907506e-05\n","Epoch  19 Batch  220 / 228  Training Loss  3.7006808270234615e-05\n","Epoch  19 Batch  221 / 228  Training Loss  2.3782706193742342e-05\n","Epoch  19 Batch  222 / 228  Training Loss  2.5323966838186607e-05\n","Epoch  19 Batch  223 / 228  Training Loss  3.19094215228688e-05\n","Epoch  19 Batch  224 / 228  Training Loss  4.11164146498777e-05\n","Epoch  19 Batch  225 / 228  Training Loss  2.0853614842053503e-05\n","Epoch  19 Batch  226 / 228  Training Loss  1.7537138774059713e-05\n","Epoch  19 Batch  227 / 228  Training Loss  1.2944707123097032e-05\n","  20    |    -    |   0.000024   | 99.199695\n","----------------------------------------------------------------------\n","Running epoch: 20\n","Epoch  20 Batch  0 / 228  Training Loss  1.775610689946916e-05\n","Epoch  20 Batch  1 / 228  Training Loss  2.0630799554055557e-05\n","Epoch  20 Batch  2 / 228  Training Loss  1.7001922969939187e-05\n","Epoch  20 Batch  3 / 228  Training Loss  1.768971560522914e-05\n","Epoch  20 Batch  4 / 228  Training Loss  2.3184562451206148e-05\n","Epoch  20 Batch  5 / 228  Training Loss  2.692482667043805e-05\n","Epoch  20 Batch  6 / 228  Training Loss  2.7980053346254863e-05\n","Epoch  20 Batch  7 / 228  Training Loss  2.6538542442722246e-05\n","Epoch  20 Batch  8 / 228  Training Loss  2.5885494324029423e-05\n","Epoch  20 Batch  9 / 228  Training Loss  2.0488823793129995e-05\n","Epoch  20 Batch  10 / 228  Training Loss  2.715178743528668e-05\n","Epoch  20 Batch  11 / 228  Training Loss  1.9605788111221045e-05\n","Epoch  20 Batch  12 / 228  Training Loss  2.4527591449441388e-05\n","Epoch  20 Batch  13 / 228  Training Loss  2.4134540581144392e-05\n","Epoch  20 Batch  14 / 228  Training Loss  3.2888317946344614e-05\n","Epoch  20 Batch  15 / 228  Training Loss  2.705950100789778e-05\n","Epoch  20 Batch  16 / 228  Training Loss  2.7904281523660757e-05\n","Epoch  20 Batch  17 / 228  Training Loss  1.1275822544121183e-05\n","Epoch  20 Batch  18 / 228  Training Loss  2.663643317646347e-05\n","Epoch  20 Batch  19 / 228  Training Loss  1.4811436813033652e-05\n","Epoch  20 Batch  20 / 228  Training Loss  1.5538123989244923e-05\n","Epoch  20 Batch  21 / 228  Training Loss  1.617481575522106e-05\n","Epoch  20 Batch  22 / 228  Training Loss  2.120513454428874e-05\n","Epoch  20 Batch  23 / 228  Training Loss  2.0677533029811457e-05\n","Epoch  20 Batch  24 / 228  Training Loss  2.4845285224728286e-05\n","Epoch  20 Batch  25 / 228  Training Loss  2.1108780856593512e-05\n","Epoch  20 Batch  26 / 228  Training Loss  1.826364859880414e-05\n","Epoch  20 Batch  27 / 228  Training Loss  2.3595994207425974e-05\n","Epoch  20 Batch  28 / 228  Training Loss  1.435148897144245e-05\n","Epoch  20 Batch  29 / 228  Training Loss  2.054852848232258e-05\n","Epoch  20 Batch  30 / 228  Training Loss  2.464038880134467e-05\n","Epoch  20 Batch  31 / 228  Training Loss  2.5442432161071338e-05\n","Epoch  20 Batch  32 / 228  Training Loss  1.4733724128745962e-05\n","Epoch  20 Batch  33 / 228  Training Loss  1.6224406863329932e-05\n","Epoch  20 Batch  34 / 228  Training Loss  2.1861353161511943e-05\n","Epoch  20 Batch  35 / 228  Training Loss  1.1320859812258277e-05\n","Epoch  20 Batch  36 / 228  Training Loss  2.5197881768690422e-05\n","Epoch  20 Batch  37 / 228  Training Loss  2.8449216188164428e-05\n","Epoch  20 Batch  38 / 228  Training Loss  2.1349933376768604e-05\n","Epoch  20 Batch  39 / 228  Training Loss  1.6073672668426298e-05\n","Epoch  20 Batch  40 / 228  Training Loss  2.2643971533398144e-05\n","Epoch  20 Batch  41 / 228  Training Loss  2.4758064682828262e-05\n","Epoch  20 Batch  42 / 228  Training Loss  2.3683925974182785e-05\n","Epoch  20 Batch  43 / 228  Training Loss  3.68046821677126e-05\n","Epoch  20 Batch  44 / 228  Training Loss  1.458356564398855e-05\n","Epoch  20 Batch  45 / 228  Training Loss  1.7333948562736623e-05\n","Epoch  20 Batch  46 / 228  Training Loss  3.0747451091883704e-05\n","Epoch  20 Batch  47 / 228  Training Loss  1.8260028809891082e-05\n","Epoch  20 Batch  48 / 228  Training Loss  2.2184194676810876e-05\n","Epoch  20 Batch  49 / 228  Training Loss  3.5586046578828245e-05\n","Epoch  20 Batch  50 / 228  Training Loss  2.9158161851228215e-05\n","Epoch  20 Batch  51 / 228  Training Loss  1.9186500139767304e-05\n","Epoch  20 Batch  52 / 228  Training Loss  2.1576126528088935e-05\n","Epoch  20 Batch  53 / 228  Training Loss  3.1139083148445934e-05\n","Epoch  20 Batch  54 / 228  Training Loss  1.8299975636182353e-05\n","Epoch  20 Batch  55 / 228  Training Loss  2.00758186110761e-05\n","Epoch  20 Batch  56 / 228  Training Loss  2.244320603494998e-05\n","Epoch  20 Batch  57 / 228  Training Loss  1.6782509192125872e-05\n","Epoch  20 Batch  58 / 228  Training Loss  1.4734729120391421e-05\n","Epoch  20 Batch  59 / 228  Training Loss  2.2485295630758628e-05\n","Epoch  20 Batch  60 / 228  Training Loss  2.739399133133702e-05\n","Epoch  20 Batch  61 / 228  Training Loss  1.3271409443404991e-05\n","Epoch  20 Batch  62 / 228  Training Loss  2.832138488884084e-05\n","Epoch  20 Batch  63 / 228  Training Loss  1.463383341615554e-05\n","Epoch  20 Batch  64 / 228  Training Loss  2.5750408894964494e-05\n","Epoch  20 Batch  65 / 228  Training Loss  1.808294291549828e-05\n","Epoch  20 Batch  66 / 228  Training Loss  1.7018737707985565e-05\n","Epoch  20 Batch  67 / 228  Training Loss  1.4433529941015877e-05\n","Epoch  20 Batch  68 / 228  Training Loss  2.142683297279291e-05\n","Epoch  20 Batch  69 / 228  Training Loss  1.916035034810193e-05\n","Epoch  20 Batch  70 / 228  Training Loss  2.2586513296118937e-05\n","Epoch  20 Batch  71 / 228  Training Loss  1.3255268640932627e-05\n","Epoch  20 Batch  72 / 228  Training Loss  2.2910269763087854e-05\n","Epoch  20 Batch  73 / 228  Training Loss  2.5689321773825213e-05\n","Epoch  20 Batch  74 / 228  Training Loss  2.222679540864192e-05\n","Epoch  20 Batch  75 / 228  Training Loss  1.8155267753172666e-05\n","Epoch  20 Batch  76 / 228  Training Loss  2.6092680855072103e-05\n","Epoch  20 Batch  77 / 228  Training Loss  2.1766205463791266e-05\n","Epoch  20 Batch  78 / 228  Training Loss  2.2400152374757454e-05\n","Epoch  20 Batch  79 / 228  Training Loss  2.1422660211101174e-05\n","Epoch  20 Batch  80 / 228  Training Loss  1.3184720955905505e-05\n","Epoch  20 Batch  81 / 228  Training Loss  1.6281146599794738e-05\n","Epoch  20 Batch  82 / 228  Training Loss  1.806406544346828e-05\n","Epoch  20 Batch  83 / 228  Training Loss  1.615538349142298e-05\n","Epoch  20 Batch  84 / 228  Training Loss  2.6261022867402062e-05\n","Epoch  20 Batch  85 / 228  Training Loss  3.101295078522526e-05\n","Epoch  20 Batch  86 / 228  Training Loss  2.5675655706436373e-05\n","Epoch  20 Batch  87 / 228  Training Loss  2.005057285714429e-05\n","Epoch  20 Batch  88 / 228  Training Loss  2.5257060769945383e-05\n","Epoch  20 Batch  89 / 228  Training Loss  2.6461808374733664e-05\n","Epoch  20 Batch  90 / 228  Training Loss  1.7635478798183613e-05\n","Epoch  20 Batch  91 / 228  Training Loss  2.7683458029059693e-05\n","Epoch  20 Batch  92 / 228  Training Loss  2.345971552131232e-05\n","Epoch  20 Batch  93 / 228  Training Loss  3.735701466212049e-05\n","Epoch  20 Batch  94 / 228  Training Loss  3.4931552363559604e-05\n","Epoch  20 Batch  95 / 228  Training Loss  2.6044461264973506e-05\n","Epoch  20 Batch  96 / 228  Training Loss  2.6622306904755533e-05\n","Epoch  20 Batch  97 / 228  Training Loss  2.0526287698885426e-05\n","Epoch  20 Batch  98 / 228  Training Loss  1.9968549167970195e-05\n","Epoch  20 Batch  99 / 228  Training Loss  2.50565408350667e-05\n","Epoch  20 Batch  100 / 228  Training Loss  2.979183591378387e-05\n","Epoch  20 Batch  101 / 228  Training Loss  1.5691195585532114e-05\n","Epoch  20 Batch  102 / 228  Training Loss  2.959115408884827e-05\n","Epoch  20 Batch  103 / 228  Training Loss  2.4691649741725996e-05\n","Epoch  20 Batch  104 / 228  Training Loss  2.827076787070837e-05\n","Epoch  20 Batch  105 / 228  Training Loss  2.0007850253023207e-05\n","Epoch  20 Batch  106 / 228  Training Loss  2.3883314497652464e-05\n","Epoch  20 Batch  107 / 228  Training Loss  2.7276377295493148e-05\n","Epoch  20 Batch  108 / 228  Training Loss  2.1365411157603376e-05\n","Epoch  20 Batch  109 / 228  Training Loss  2.0017565475427546e-05\n","Epoch  20 Batch  110 / 228  Training Loss  1.7968668544199318e-05\n","Epoch  20 Batch  111 / 228  Training Loss  2.2084079319029115e-05\n","Epoch  20 Batch  112 / 228  Training Loss  1.8219903722638264e-05\n","Epoch  20 Batch  113 / 228  Training Loss  2.147702980437316e-05\n","Epoch  20 Batch  114 / 228  Training Loss  1.620318835193757e-05\n","Epoch  20 Batch  115 / 228  Training Loss  3.340532202855684e-05\n","Epoch  20 Batch  116 / 228  Training Loss  2.3230782971950248e-05\n","Epoch  20 Batch  117 / 228  Training Loss  1.71272531588329e-05\n","Epoch  20 Batch  118 / 228  Training Loss  2.030374525929801e-05\n","Epoch  20 Batch  119 / 228  Training Loss  2.1590638425550424e-05\n","Epoch  20 Batch  120 / 228  Training Loss  1.9246621377533302e-05\n","Epoch  20 Batch  121 / 228  Training Loss  1.5165656805038452e-05\n","Epoch  20 Batch  122 / 228  Training Loss  3.6469107726588845e-05\n","Epoch  20 Batch  123 / 228  Training Loss  2.540257446526084e-05\n","Epoch  20 Batch  124 / 228  Training Loss  2.1329795345081948e-05\n","Epoch  20 Batch  125 / 228  Training Loss  1.573466943227686e-05\n","Epoch  20 Batch  126 / 228  Training Loss  2.2552352675120346e-05\n","Epoch  20 Batch  127 / 228  Training Loss  2.9032849852228537e-05\n","Epoch  20 Batch  128 / 228  Training Loss  2.8081954951630905e-05\n","Epoch  20 Batch  129 / 228  Training Loss  1.6042507922975346e-05\n","Epoch  20 Batch  130 / 228  Training Loss  1.9220158719690517e-05\n","Epoch  20 Batch  131 / 228  Training Loss  1.9315973986522295e-05\n","Epoch  20 Batch  132 / 228  Training Loss  2.3216587578644976e-05\n","Epoch  20 Batch  133 / 228  Training Loss  2.3213307940750383e-05\n","Epoch  20 Batch  134 / 228  Training Loss  1.463287935621338e-05\n","Epoch  20 Batch  135 / 228  Training Loss  2.1579562599072233e-05\n","Epoch  20 Batch  136 / 228  Training Loss  2.218853114754893e-05\n","Epoch  20 Batch  137 / 228  Training Loss  2.1219148038653657e-05\n","Epoch  20 Batch  138 / 228  Training Loss  1.9444556528469548e-05\n","Epoch  20 Batch  139 / 228  Training Loss  1.9185539713362232e-05\n","Epoch  20 Batch  140 / 228  Training Loss  1.5625981177436188e-05\n","Epoch  20 Batch  141 / 228  Training Loss  1.686760333541315e-05\n","Epoch  20 Batch  142 / 228  Training Loss  1.805938154575415e-05\n","Epoch  20 Batch  143 / 228  Training Loss  2.437646617181599e-05\n","Epoch  20 Batch  144 / 228  Training Loss  2.103190854541026e-05\n","Epoch  20 Batch  145 / 228  Training Loss  1.9845694623654708e-05\n","Epoch  20 Batch  146 / 228  Training Loss  1.9745628378586844e-05\n","Epoch  20 Batch  147 / 228  Training Loss  2.1271704099490307e-05\n","Epoch  20 Batch  148 / 228  Training Loss  2.1022446162533015e-05\n","Epoch  20 Batch  149 / 228  Training Loss  2.3618013074155897e-05\n","Epoch  20 Batch  150 / 228  Training Loss  1.9683855498442426e-05\n","Epoch  20 Batch  151 / 228  Training Loss  1.8853088477044366e-05\n","Epoch  20 Batch  152 / 228  Training Loss  2.8076616217731498e-05\n","Epoch  20 Batch  153 / 228  Training Loss  1.8292363165528513e-05\n","Epoch  20 Batch  154 / 228  Training Loss  1.9386501662665978e-05\n","Epoch  20 Batch  155 / 228  Training Loss  1.8248250853503123e-05\n","Epoch  20 Batch  156 / 228  Training Loss  1.6945472452789545e-05\n","Epoch  20 Batch  157 / 228  Training Loss  1.9193339539924636e-05\n","Epoch  20 Batch  158 / 228  Training Loss  1.4978752005845308e-05\n","Epoch  20 Batch  159 / 228  Training Loss  1.9894705474143848e-05\n","Epoch  20 Batch  160 / 228  Training Loss  1.9645376596599817e-05\n","Epoch  20 Batch  161 / 228  Training Loss  1.8592534615891054e-05\n","Epoch  20 Batch  162 / 228  Training Loss  2.1158102754270658e-05\n","Epoch  20 Batch  163 / 228  Training Loss  1.6940188288572244e-05\n","Epoch  20 Batch  164 / 228  Training Loss  2.129085987689905e-05\n","Epoch  20 Batch  165 / 228  Training Loss  2.1915300749242306e-05\n","Epoch  20 Batch  166 / 228  Training Loss  2.6746560251922347e-05\n","Epoch  20 Batch  167 / 228  Training Loss  2.4686703909537755e-05\n","Epoch  20 Batch  168 / 228  Training Loss  1.6040574337239377e-05\n","Epoch  20 Batch  169 / 228  Training Loss  1.8028384147328325e-05\n","Epoch  20 Batch  170 / 228  Training Loss  2.168673017877154e-05\n","Epoch  20 Batch  171 / 228  Training Loss  1.7514685168862343e-05\n","Epoch  20 Batch  172 / 228  Training Loss  2.973786831717007e-05\n","Epoch  20 Batch  173 / 228  Training Loss  2.4420072804787196e-05\n","Epoch  20 Batch  174 / 228  Training Loss  2.4825611035339534e-05\n","Epoch  20 Batch  175 / 228  Training Loss  2.0472127289394848e-05\n","Epoch  20 Batch  176 / 228  Training Loss  1.9864253772539087e-05\n","Epoch  20 Batch  177 / 228  Training Loss  1.794880154193379e-05\n","Epoch  20 Batch  178 / 228  Training Loss  1.576430440763943e-05\n","Epoch  20 Batch  179 / 228  Training Loss  1.8434071535011753e-05\n","Epoch  20 Batch  180 / 228  Training Loss  2.2110827558208257e-05\n","Epoch  20 Batch  181 / 228  Training Loss  2.8249103706912138e-05\n","Epoch  20 Batch  182 / 228  Training Loss  2.8151034712209366e-05\n","Epoch  20 Batch  183 / 228  Training Loss  2.0054741980857216e-05\n","Epoch  20 Batch  184 / 228  Training Loss  1.1661073585855775e-05\n","Epoch  20 Batch  185 / 228  Training Loss  2.1346995708881877e-05\n","Epoch  20 Batch  186 / 228  Training Loss  2.197071080445312e-05\n","Epoch  20 Batch  187 / 228  Training Loss  1.7518714230391197e-05\n","Epoch  20 Batch  188 / 228  Training Loss  1.5463096133316867e-05\n","Epoch  20 Batch  189 / 228  Training Loss  1.188411442853976e-05\n","Epoch  20 Batch  190 / 228  Training Loss  1.9738781702471897e-05\n","Epoch  20 Batch  191 / 228  Training Loss  1.6543939636903815e-05\n","Epoch  20 Batch  192 / 228  Training Loss  2.304011104570236e-05\n","Epoch  20 Batch  193 / 228  Training Loss  1.0918301086348947e-05\n","Epoch  20 Batch  194 / 228  Training Loss  1.9277485989732668e-05\n","Epoch  20 Batch  195 / 228  Training Loss  2.702899837458972e-05\n","Epoch  20 Batch  196 / 228  Training Loss  1.4171858310874086e-05\n","Epoch  20 Batch  197 / 228  Training Loss  1.8938122593681328e-05\n","Epoch  20 Batch  198 / 228  Training Loss  2.8487105737440288e-05\n","Epoch  20 Batch  199 / 228  Training Loss  2.552050500526093e-05\n","Epoch  20 Batch  200 / 228  Training Loss  1.7640326404944062e-05\n","Epoch  20 Batch  201 / 228  Training Loss  2.349258284084499e-05\n","Epoch  20 Batch  202 / 228  Training Loss  4.295334656490013e-05\n","Epoch  20 Batch  203 / 228  Training Loss  2.7878355467692018e-05\n","Epoch  20 Batch  204 / 228  Training Loss  2.4183736968552694e-05\n","Epoch  20 Batch  205 / 228  Training Loss  2.5877456209855154e-05\n","Epoch  20 Batch  206 / 228  Training Loss  2.325621244381182e-05\n","Epoch  20 Batch  207 / 228  Training Loss  2.0164910893072374e-05\n","Epoch  20 Batch  208 / 228  Training Loss  2.931457493104972e-05\n","Epoch  20 Batch  209 / 228  Training Loss  1.8594044377095997e-05\n","Epoch  20 Batch  210 / 228  Training Loss  1.72012260009069e-05\n","Epoch  20 Batch  211 / 228  Training Loss  2.144800964742899e-05\n","Epoch  20 Batch  212 / 228  Training Loss  2.152367596863769e-05\n","Epoch  20 Batch  213 / 228  Training Loss  1.6702975699445233e-05\n","Epoch  20 Batch  214 / 228  Training Loss  1.3100390788167715e-05\n","Epoch  20 Batch  215 / 228  Training Loss  2.121032048307825e-05\n","Epoch  20 Batch  216 / 228  Training Loss  1.88628528121626e-05\n","Epoch  20 Batch  217 / 228  Training Loss  2.0732160919578746e-05\n","Epoch  20 Batch  218 / 228  Training Loss  2.193090767832473e-05\n","Epoch  20 Batch  219 / 228  Training Loss  2.048861097136978e-05\n","Epoch  20 Batch  220 / 228  Training Loss  2.8159955036244355e-05\n","Epoch  20 Batch  221 / 228  Training Loss  1.889616578409914e-05\n","Epoch  20 Batch  222 / 228  Training Loss  1.8475828255759552e-05\n","Epoch  20 Batch  223 / 228  Training Loss  1.866959428298287e-05\n","Epoch  20 Batch  224 / 228  Training Loss  1.9608500224421732e-05\n","Epoch  20 Batch  225 / 228  Training Loss  1.6831594621180557e-05\n","Epoch  20 Batch  226 / 228  Training Loss  1.5440298739122227e-05\n","Epoch  20 Batch  227 / 228  Training Loss  1.9858138330164365e-05\n","  21    |    -    |   0.000022   | 99.085366\n","----------------------------------------------------------------------\n","Running epoch: 21\n","Epoch  21 Batch  0 / 228  Training Loss  2.808060889947228e-05\n","Epoch  21 Batch  1 / 228  Training Loss  2.429356572974939e-05\n","Epoch  21 Batch  2 / 228  Training Loss  2.3242686438607052e-05\n","Epoch  21 Batch  3 / 228  Training Loss  1.5345118299592286e-05\n","Epoch  21 Batch  4 / 228  Training Loss  1.3703506738238502e-05\n","Epoch  21 Batch  5 / 228  Training Loss  2.3145452360040508e-05\n","Epoch  21 Batch  6 / 228  Training Loss  1.5408593753818423e-05\n","Epoch  21 Batch  7 / 228  Training Loss  1.900194547488354e-05\n","Epoch  21 Batch  8 / 228  Training Loss  2.540601053624414e-05\n","Epoch  21 Batch  9 / 228  Training Loss  1.9733113731490448e-05\n","Epoch  21 Batch  10 / 228  Training Loss  2.8933427529409528e-05\n","Epoch  21 Batch  11 / 228  Training Loss  1.778975456545595e-05\n","Epoch  21 Batch  12 / 228  Training Loss  2.0065976059413515e-05\n","Epoch  21 Batch  13 / 228  Training Loss  1.5393912690342404e-05\n","Epoch  21 Batch  14 / 228  Training Loss  1.7569058400113136e-05\n","Epoch  21 Batch  15 / 228  Training Loss  2.4137896616593935e-05\n","Epoch  21 Batch  16 / 228  Training Loss  2.0017881979583763e-05\n","Epoch  21 Batch  17 / 228  Training Loss  1.994229751289822e-05\n","Epoch  21 Batch  18 / 228  Training Loss  2.8926238883286715e-05\n","Epoch  21 Batch  19 / 228  Training Loss  2.448294253554195e-05\n","Epoch  21 Batch  20 / 228  Training Loss  2.3253462131833658e-05\n","Epoch  21 Batch  21 / 228  Training Loss  2.115567258442752e-05\n","Epoch  21 Batch  22 / 228  Training Loss  1.9974746464868076e-05\n","Epoch  21 Batch  23 / 228  Training Loss  2.0478457372519188e-05\n","Epoch  21 Batch  24 / 228  Training Loss  2.7056692488258705e-05\n","Epoch  21 Batch  25 / 228  Training Loss  2.4356975700356998e-05\n","Epoch  21 Batch  26 / 228  Training Loss  1.2014565982099157e-05\n","Epoch  21 Batch  27 / 228  Training Loss  2.8107246180297807e-05\n","Epoch  21 Batch  28 / 228  Training Loss  2.3079239326762035e-05\n","Epoch  21 Batch  29 / 228  Training Loss  1.9803355826297775e-05\n","Epoch  21 Batch  30 / 228  Training Loss  1.85114549822174e-05\n","Epoch  21 Batch  31 / 228  Training Loss  1.573001645738259e-05\n","Epoch  21 Batch  32 / 228  Training Loss  1.798555240384303e-05\n","Epoch  21 Batch  33 / 228  Training Loss  1.7621132428757846e-05\n","Epoch  21 Batch  34 / 228  Training Loss  2.4743954782024957e-05\n","Epoch  21 Batch  35 / 228  Training Loss  1.636722299735993e-05\n","Epoch  21 Batch  36 / 228  Training Loss  1.6987525668810122e-05\n","Epoch  21 Batch  37 / 228  Training Loss  1.8204893422080204e-05\n","Epoch  21 Batch  38 / 228  Training Loss  1.8062104572891258e-05\n","Epoch  21 Batch  39 / 228  Training Loss  1.5578116290271282e-05\n","Epoch  21 Batch  40 / 228  Training Loss  2.249979297630489e-05\n","Epoch  21 Batch  41 / 228  Training Loss  2.060687620542012e-05\n","Epoch  21 Batch  42 / 228  Training Loss  2.6706798962550238e-05\n","Epoch  21 Batch  43 / 228  Training Loss  2.256570769532118e-05\n","Epoch  21 Batch  44 / 228  Training Loss  1.3342253623704892e-05\n","Epoch  21 Batch  45 / 228  Training Loss  2.814290019159671e-05\n","Epoch  21 Batch  46 / 228  Training Loss  1.7217946151504293e-05\n","Epoch  21 Batch  47 / 228  Training Loss  2.4820506951073185e-05\n","Epoch  21 Batch  48 / 228  Training Loss  2.077710087178275e-05\n","Epoch  21 Batch  49 / 228  Training Loss  2.3445831175195053e-05\n","Epoch  21 Batch  50 / 228  Training Loss  1.809823515941389e-05\n","Epoch  21 Batch  51 / 228  Training Loss  1.3656464943778701e-05\n","Epoch  21 Batch  52 / 228  Training Loss  1.894216620712541e-05\n","Epoch  21 Batch  53 / 228  Training Loss  1.9521488866303116e-05\n","Epoch  21 Batch  54 / 228  Training Loss  1.565288403071463e-05\n","Epoch  21 Batch  55 / 228  Training Loss  2.08045712497551e-05\n","Epoch  21 Batch  56 / 228  Training Loss  2.0108225726289675e-05\n","Epoch  21 Batch  57 / 228  Training Loss  1.958958091563545e-05\n","Epoch  21 Batch  58 / 228  Training Loss  1.5925232219160534e-05\n","Epoch  21 Batch  59 / 228  Training Loss  1.7663049220573157e-05\n","Epoch  21 Batch  60 / 228  Training Loss  1.5456713299499825e-05\n","Epoch  21 Batch  61 / 228  Training Loss  1.5718558643129654e-05\n","Epoch  21 Batch  62 / 228  Training Loss  1.753884498612024e-05\n","Epoch  21 Batch  63 / 228  Training Loss  1.724773392197676e-05\n","Epoch  21 Batch  64 / 228  Training Loss  1.915745815495029e-05\n","Epoch  21 Batch  65 / 228  Training Loss  1.920937211252749e-05\n","Epoch  21 Batch  66 / 228  Training Loss  2.259455868625082e-05\n","Epoch  21 Batch  67 / 228  Training Loss  1.6028625395847484e-05\n","Epoch  21 Batch  68 / 228  Training Loss  2.2826934582553804e-05\n","Epoch  21 Batch  69 / 228  Training Loss  2.1102467144373804e-05\n","Epoch  21 Batch  70 / 228  Training Loss  2.325960304006003e-05\n","Epoch  21 Batch  71 / 228  Training Loss  1.8041644580080174e-05\n","Epoch  21 Batch  72 / 228  Training Loss  2.3288788725039922e-05\n","Epoch  21 Batch  73 / 228  Training Loss  2.3150118067860603e-05\n","Epoch  21 Batch  74 / 228  Training Loss  2.2538804842042737e-05\n","Epoch  21 Batch  75 / 228  Training Loss  1.9358600184204988e-05\n","Epoch  21 Batch  76 / 228  Training Loss  2.2228730813367292e-05\n","Epoch  21 Batch  77 / 228  Training Loss  1.2492227142502088e-05\n","Epoch  21 Batch  78 / 228  Training Loss  2.9865424949093722e-05\n","Epoch  21 Batch  79 / 228  Training Loss  1.5109831110748928e-05\n","Epoch  21 Batch  80 / 228  Training Loss  1.7378548363922164e-05\n","Epoch  21 Batch  81 / 228  Training Loss  2.6855948817683384e-05\n","Epoch  21 Batch  82 / 228  Training Loss  1.679321940173395e-05\n","Epoch  21 Batch  83 / 228  Training Loss  1.6986312402877957e-05\n","Epoch  21 Batch  84 / 228  Training Loss  1.77443798747845e-05\n","Epoch  21 Batch  85 / 228  Training Loss  1.6894593500182964e-05\n","Epoch  21 Batch  86 / 228  Training Loss  2.225440039183013e-05\n","Epoch  21 Batch  87 / 228  Training Loss  1.848563078965526e-05\n","Epoch  21 Batch  88 / 228  Training Loss  2.9902552341809496e-05\n","Epoch  21 Batch  89 / 228  Training Loss  1.9835822968161665e-05\n","Epoch  21 Batch  90 / 228  Training Loss  1.704780152067542e-05\n","Epoch  21 Batch  91 / 228  Training Loss  1.7306621884927154e-05\n","Epoch  21 Batch  92 / 228  Training Loss  1.6595366105320863e-05\n","Epoch  21 Batch  93 / 228  Training Loss  2.1923251551925205e-05\n","Epoch  21 Batch  94 / 228  Training Loss  2.2024536519893445e-05\n","Epoch  21 Batch  95 / 228  Training Loss  1.7975315131479874e-05\n","Epoch  21 Batch  96 / 228  Training Loss  1.7742269847076386e-05\n","Epoch  21 Batch  97 / 228  Training Loss  2.1712412490160204e-05\n","Epoch  21 Batch  98 / 228  Training Loss  2.1305400878190994e-05\n","Epoch  21 Batch  99 / 228  Training Loss  2.1060704966657795e-05\n","Epoch  21 Batch  100 / 228  Training Loss  2.2971547878114507e-05\n","Epoch  21 Batch  101 / 228  Training Loss  2.3024866095511243e-05\n","Epoch  21 Batch  102 / 228  Training Loss  3.390522761037573e-05\n","Epoch  21 Batch  103 / 228  Training Loss  1.8709291907725856e-05\n","Epoch  21 Batch  104 / 228  Training Loss  1.8461241779732518e-05\n","Epoch  21 Batch  105 / 228  Training Loss  1.5807332601980306e-05\n","Epoch  21 Batch  106 / 228  Training Loss  1.8303715478396043e-05\n","Epoch  21 Batch  107 / 228  Training Loss  2.5547933546476997e-05\n","Epoch  21 Batch  108 / 228  Training Loss  2.056989978882484e-05\n","Epoch  21 Batch  109 / 228  Training Loss  1.9382830942049623e-05\n","Epoch  21 Batch  110 / 228  Training Loss  1.1732927305274643e-05\n","Epoch  21 Batch  111 / 228  Training Loss  1.869385778263677e-05\n","Epoch  21 Batch  112 / 228  Training Loss  1.9397861251491122e-05\n","Epoch  21 Batch  113 / 228  Training Loss  1.801903999876231e-05\n","Epoch  21 Batch  114 / 228  Training Loss  1.3688275430467911e-05\n","Epoch  21 Batch  115 / 228  Training Loss  2.103535553032998e-05\n","Epoch  21 Batch  116 / 228  Training Loss  2.3576012608828023e-05\n","Epoch  21 Batch  117 / 228  Training Loss  2.3171109205577523e-05\n","Epoch  21 Batch  118 / 228  Training Loss  1.28061319628614e-05\n","Epoch  21 Batch  119 / 228  Training Loss  1.76617377292132e-05\n","Epoch  21 Batch  120 / 228  Training Loss  1.884167068055831e-05\n","Epoch  21 Batch  121 / 228  Training Loss  2.31570411415305e-05\n","Epoch  21 Batch  122 / 228  Training Loss  1.8837939933291636e-05\n","Epoch  21 Batch  123 / 228  Training Loss  1.4660127817478497e-05\n","Epoch  21 Batch  124 / 228  Training Loss  1.5697345588705502e-05\n","Epoch  21 Batch  125 / 228  Training Loss  2.7811507607111707e-05\n","Epoch  21 Batch  126 / 228  Training Loss  1.765944398357533e-05\n","Epoch  21 Batch  127 / 228  Training Loss  1.4055425708647817e-05\n","Epoch  21 Batch  128 / 228  Training Loss  1.9025534129468724e-05\n","Epoch  21 Batch  129 / 228  Training Loss  2.4335618945769966e-05\n","Epoch  21 Batch  130 / 228  Training Loss  1.1045118299080059e-05\n","Epoch  21 Batch  131 / 228  Training Loss  2.287253300892189e-05\n","Epoch  21 Batch  132 / 228  Training Loss  2.628042784635909e-05\n","Epoch  21 Batch  133 / 228  Training Loss  1.5759871530462988e-05\n","Epoch  21 Batch  134 / 228  Training Loss  2.7427342502051033e-05\n","Epoch  21 Batch  135 / 228  Training Loss  1.986505048989784e-05\n","Epoch  21 Batch  136 / 228  Training Loss  1.965231422218494e-05\n","Epoch  21 Batch  137 / 228  Training Loss  1.9778988644247875e-05\n","Epoch  21 Batch  138 / 228  Training Loss  1.9605455236160196e-05\n","Epoch  21 Batch  139 / 228  Training Loss  1.9085053281742148e-05\n","Epoch  21 Batch  140 / 228  Training Loss  1.3822903383697849e-05\n","Epoch  21 Batch  141 / 228  Training Loss  1.3078104529995471e-05\n","Epoch  21 Batch  142 / 228  Training Loss  2.0795267118955962e-05\n","Epoch  21 Batch  143 / 228  Training Loss  2.076245982607361e-05\n","Epoch  21 Batch  144 / 228  Training Loss  1.9779086869675666e-05\n","Epoch  21 Batch  145 / 228  Training Loss  2.6843466912396252e-05\n","Epoch  21 Batch  146 / 228  Training Loss  2.3671846065553837e-05\n","Epoch  21 Batch  147 / 228  Training Loss  1.829898974392563e-05\n","Epoch  21 Batch  148 / 228  Training Loss  1.564601916470565e-05\n","Epoch  21 Batch  149 / 228  Training Loss  2.0919726011925377e-05\n","Epoch  21 Batch  150 / 228  Training Loss  1.8813232600223273e-05\n","Epoch  21 Batch  151 / 228  Training Loss  1.6094112652353942e-05\n","Epoch  21 Batch  152 / 228  Training Loss  1.4488068700302392e-05\n","Epoch  21 Batch  153 / 228  Training Loss  1.6562149539822713e-05\n","Epoch  21 Batch  154 / 228  Training Loss  1.761113162501715e-05\n","Epoch  21 Batch  155 / 228  Training Loss  2.226159631391056e-05\n","Epoch  21 Batch  156 / 228  Training Loss  1.752665775711648e-05\n","Epoch  21 Batch  157 / 228  Training Loss  1.5817193343536928e-05\n","Epoch  21 Batch  158 / 228  Training Loss  1.3093905181449372e-05\n","Epoch  21 Batch  159 / 228  Training Loss  1.730975418468006e-05\n","Epoch  21 Batch  160 / 228  Training Loss  1.630193401069846e-05\n","Epoch  21 Batch  161 / 228  Training Loss  2.076392775052227e-05\n","Epoch  21 Batch  162 / 228  Training Loss  2.326202047697734e-05\n","Epoch  21 Batch  163 / 228  Training Loss  1.6137630154844373e-05\n","Epoch  21 Batch  164 / 228  Training Loss  1.5999092283891514e-05\n","Epoch  21 Batch  165 / 228  Training Loss  1.443432665837463e-05\n","Epoch  21 Batch  166 / 228  Training Loss  2.9051783712930046e-05\n","Epoch  21 Batch  167 / 228  Training Loss  1.3055316230747849e-05\n","Epoch  21 Batch  168 / 228  Training Loss  1.7781097994884476e-05\n","Epoch  21 Batch  169 / 228  Training Loss  1.837381387304049e-05\n","Epoch  21 Batch  170 / 228  Training Loss  2.0281497199903242e-05\n","Epoch  21 Batch  171 / 228  Training Loss  1.6419062376371585e-05\n","Epoch  21 Batch  172 / 228  Training Loss  2.8565200409502722e-05\n","Epoch  21 Batch  173 / 228  Training Loss  2.1318170183803886e-05\n","Epoch  21 Batch  174 / 228  Training Loss  1.4541705240844749e-05\n","Epoch  21 Batch  175 / 228  Training Loss  1.7743206626619212e-05\n","Epoch  21 Batch  176 / 228  Training Loss  2.124895945598837e-05\n","Epoch  21 Batch  177 / 228  Training Loss  1.683085247350391e-05\n","Epoch  21 Batch  178 / 228  Training Loss  3.0335208066389896e-05\n","Epoch  21 Batch  179 / 228  Training Loss  1.825149229262024e-05\n","Epoch  21 Batch  180 / 228  Training Loss  1.5970910681062378e-05\n","Epoch  21 Batch  181 / 228  Training Loss  1.4376882973010652e-05\n","Epoch  21 Batch  182 / 228  Training Loss  2.8467704396462068e-05\n","Epoch  21 Batch  183 / 228  Training Loss  2.9467326385201886e-05\n","Epoch  21 Batch  184 / 228  Training Loss  2.5694171199575067e-05\n","Epoch  21 Batch  185 / 228  Training Loss  1.9449635146884248e-05\n","Epoch  21 Batch  186 / 228  Training Loss  9.697369023342617e-06\n","Epoch  21 Batch  187 / 228  Training Loss  1.543620601296425e-05\n","Epoch  21 Batch  188 / 228  Training Loss  1.54601384565467e-05\n","Epoch  21 Batch  189 / 228  Training Loss  2.559559652581811e-05\n","Epoch  21 Batch  190 / 228  Training Loss  2.518688233976718e-05\n","Epoch  21 Batch  191 / 228  Training Loss  1.7162539734272286e-05\n","Epoch  21 Batch  192 / 228  Training Loss  2.0469726223382168e-05\n","Epoch  21 Batch  193 / 228  Training Loss  1.7421085431124084e-05\n","Epoch  21 Batch  194 / 228  Training Loss  2.3246719138114713e-05\n","Epoch  21 Batch  195 / 228  Training Loss  2.9038492357358336e-05\n","Epoch  21 Batch  196 / 228  Training Loss  1.4713386008224916e-05\n","Epoch  21 Batch  197 / 228  Training Loss  1.5361078112618998e-05\n","Epoch  21 Batch  198 / 228  Training Loss  1.947041710081976e-05\n","Epoch  21 Batch  199 / 228  Training Loss  1.5306952263927087e-05\n","Epoch  21 Batch  200 / 228  Training Loss  2.668169872777071e-05\n","Epoch  21 Batch  201 / 228  Training Loss  1.6555497495573945e-05\n","Epoch  21 Batch  202 / 228  Training Loss  2.334981400053948e-05\n","Epoch  21 Batch  203 / 228  Training Loss  3.20560866384767e-05\n","Epoch  21 Batch  204 / 228  Training Loss  2.062586645479314e-05\n","Epoch  21 Batch  205 / 228  Training Loss  1.964004877663683e-05\n","Epoch  21 Batch  206 / 228  Training Loss  1.789783163985703e-05\n","Epoch  21 Batch  207 / 228  Training Loss  1.6152800526469946e-05\n","Epoch  21 Batch  208 / 228  Training Loss  2.0160616259090602e-05\n","Epoch  21 Batch  209 / 228  Training Loss  2.017593214986846e-05\n","Epoch  21 Batch  210 / 228  Training Loss  2.5593977625248954e-05\n","Epoch  21 Batch  211 / 228  Training Loss  1.9039302060264163e-05\n","Epoch  21 Batch  212 / 228  Training Loss  1.683749542280566e-05\n","Epoch  21 Batch  213 / 228  Training Loss  1.729954601614736e-05\n","Epoch  21 Batch  214 / 228  Training Loss  1.8832470232155174e-05\n","Epoch  21 Batch  215 / 228  Training Loss  2.0985064111300744e-05\n","Epoch  21 Batch  216 / 228  Training Loss  2.052632044069469e-05\n","Epoch  21 Batch  217 / 228  Training Loss  2.3929012968437746e-05\n","Epoch  21 Batch  218 / 228  Training Loss  2.2956932298257016e-05\n","Epoch  21 Batch  219 / 228  Training Loss  1.2848904589191079e-05\n","Epoch  21 Batch  220 / 228  Training Loss  1.5195872038020752e-05\n","Epoch  21 Batch  221 / 228  Training Loss  1.7523298083688132e-05\n","Epoch  21 Batch  222 / 228  Training Loss  2.0008032151963562e-05\n","Epoch  21 Batch  223 / 228  Training Loss  1.91049839486368e-05\n","Epoch  21 Batch  224 / 228  Training Loss  1.1686390280374326e-05\n","Epoch  21 Batch  225 / 228  Training Loss  1.8021326468442567e-05\n","Epoch  21 Batch  226 / 228  Training Loss  1.895020977826789e-05\n","Epoch  21 Batch  227 / 228  Training Loss  1.9503724615788087e-05\n","  22    |    -    |   0.000020   | 99.199695\n","----------------------------------------------------------------------\n","Running epoch: 22\n","Epoch  22 Batch  0 / 228  Training Loss  1.792069997463841e-05\n","Epoch  22 Batch  1 / 228  Training Loss  2.7075977413915098e-05\n","Epoch  22 Batch  2 / 228  Training Loss  1.3609234883915633e-05\n","Epoch  22 Batch  3 / 228  Training Loss  1.8079801520798355e-05\n","Epoch  22 Batch  4 / 228  Training Loss  3.159346670145169e-05\n","Epoch  22 Batch  5 / 228  Training Loss  2.328714072064031e-05\n","Epoch  22 Batch  6 / 228  Training Loss  1.4853869288344868e-05\n","Epoch  22 Batch  7 / 228  Training Loss  1.3526024304155726e-05\n","Epoch  22 Batch  8 / 228  Training Loss  2.4309967557201162e-05\n","Epoch  22 Batch  9 / 228  Training Loss  1.6042260540416464e-05\n","Epoch  22 Batch  10 / 228  Training Loss  1.2306605640333146e-05\n","Epoch  22 Batch  11 / 228  Training Loss  1.7564419977134094e-05\n","Epoch  22 Batch  12 / 228  Training Loss  2.2405349227483384e-05\n","Epoch  22 Batch  13 / 228  Training Loss  2.4449316697428003e-05\n","Epoch  22 Batch  14 / 228  Training Loss  1.7147196558653377e-05\n","Epoch  22 Batch  15 / 228  Training Loss  1.6873893400770612e-05\n","Epoch  22 Batch  16 / 228  Training Loss  1.9900111510651186e-05\n","Epoch  22 Batch  17 / 228  Training Loss  1.9719636838999577e-05\n","Epoch  22 Batch  18 / 228  Training Loss  1.4439321603276767e-05\n","Epoch  22 Batch  19 / 228  Training Loss  2.9063336114631966e-05\n","Epoch  22 Batch  20 / 228  Training Loss  1.463578337279614e-05\n","Epoch  22 Batch  21 / 228  Training Loss  1.325359789916547e-05\n","Epoch  22 Batch  22 / 228  Training Loss  1.9970831999671645e-05\n","Epoch  22 Batch  23 / 228  Training Loss  1.9490038539515808e-05\n","Epoch  22 Batch  24 / 228  Training Loss  1.5357702068286017e-05\n","Epoch  22 Batch  25 / 228  Training Loss  1.475691533414647e-05\n","Epoch  22 Batch  26 / 228  Training Loss  1.7223088434548117e-05\n","Epoch  22 Batch  27 / 228  Training Loss  1.8121754692401737e-05\n","Epoch  22 Batch  28 / 228  Training Loss  1.9969309505540878e-05\n","Epoch  22 Batch  29 / 228  Training Loss  1.8478411220712587e-05\n","Epoch  22 Batch  30 / 228  Training Loss  1.6421567124780267e-05\n","Epoch  22 Batch  31 / 228  Training Loss  1.4747897694178391e-05\n","Epoch  22 Batch  32 / 228  Training Loss  2.247670454380568e-05\n","Epoch  22 Batch  33 / 228  Training Loss  1.7503693015896715e-05\n","Epoch  22 Batch  34 / 228  Training Loss  2.154060530301649e-05\n","Epoch  22 Batch  35 / 228  Training Loss  1.6738424164941534e-05\n","Epoch  22 Batch  36 / 228  Training Loss  1.9051163690164685e-05\n","Epoch  22 Batch  37 / 228  Training Loss  2.3131395209929906e-05\n","Epoch  22 Batch  38 / 228  Training Loss  1.8739805454970337e-05\n","Epoch  22 Batch  39 / 228  Training Loss  1.8060773072647862e-05\n","Epoch  22 Batch  40 / 228  Training Loss  1.9145070837112144e-05\n","Epoch  22 Batch  41 / 228  Training Loss  1.1704809367074631e-05\n","Epoch  22 Batch  42 / 228  Training Loss  1.2437756595318206e-05\n","Epoch  22 Batch  43 / 228  Training Loss  2.1452151486300863e-05\n","Epoch  22 Batch  44 / 228  Training Loss  1.5146986697800457e-05\n","Epoch  22 Batch  45 / 228  Training Loss  1.7176225810544565e-05\n","Epoch  22 Batch  46 / 228  Training Loss  2.0481371393543668e-05\n","Epoch  22 Batch  47 / 228  Training Loss  1.3355485862120986e-05\n","Epoch  22 Batch  48 / 228  Training Loss  1.7638223653193563e-05\n","Epoch  22 Batch  49 / 228  Training Loss  1.2262689779163338e-05\n","Epoch  22 Batch  50 / 228  Training Loss  1.7364011000609025e-05\n","Epoch  22 Batch  51 / 228  Training Loss  2.096039315802045e-05\n","Epoch  22 Batch  52 / 228  Training Loss  1.3225483598944265e-05\n","Epoch  22 Batch  53 / 228  Training Loss  1.8108219592249952e-05\n","Epoch  22 Batch  54 / 228  Training Loss  1.9907589376089163e-05\n","Epoch  22 Batch  55 / 228  Training Loss  2.4478620616719127e-05\n","Epoch  22 Batch  56 / 228  Training Loss  2.4794138880679384e-05\n","Epoch  22 Batch  57 / 228  Training Loss  1.2244292520335875e-05\n","Epoch  22 Batch  58 / 228  Training Loss  1.4111607924860436e-05\n","Epoch  22 Batch  59 / 228  Training Loss  2.631956886034459e-05\n","Epoch  22 Batch  60 / 228  Training Loss  1.8629752958077006e-05\n","Epoch  22 Batch  61 / 228  Training Loss  1.987920404644683e-05\n","Epoch  22 Batch  62 / 228  Training Loss  2.1947731511318125e-05\n","Epoch  22 Batch  63 / 228  Training Loss  1.5391769920825027e-05\n","Epoch  22 Batch  64 / 228  Training Loss  1.6330874132108875e-05\n","Epoch  22 Batch  65 / 228  Training Loss  1.670728306635283e-05\n","Epoch  22 Batch  66 / 228  Training Loss  3.0620296456618235e-05\n","Epoch  22 Batch  67 / 228  Training Loss  2.0760431652888656e-05\n","Epoch  22 Batch  68 / 228  Training Loss  2.4796125217108056e-05\n","Epoch  22 Batch  69 / 228  Training Loss  1.585935751791112e-05\n","Epoch  22 Batch  70 / 228  Training Loss  2.514151310606394e-05\n","Epoch  22 Batch  71 / 228  Training Loss  1.5758905647089705e-05\n","Epoch  22 Batch  72 / 228  Training Loss  1.9514096493367106e-05\n","Epoch  22 Batch  73 / 228  Training Loss  1.4614148312830366e-05\n","Epoch  22 Batch  74 / 228  Training Loss  1.5722313037258573e-05\n","Epoch  22 Batch  75 / 228  Training Loss  1.8534823539084755e-05\n","Epoch  22 Batch  76 / 228  Training Loss  1.85440294444561e-05\n","Epoch  22 Batch  77 / 228  Training Loss  2.0558512915158644e-05\n","Epoch  22 Batch  78 / 228  Training Loss  1.5595949662383646e-05\n","Epoch  22 Batch  79 / 228  Training Loss  1.765524757502135e-05\n","Epoch  22 Batch  80 / 228  Training Loss  1.529034852865152e-05\n","Epoch  22 Batch  81 / 228  Training Loss  2.0299632524256594e-05\n","Epoch  22 Batch  82 / 228  Training Loss  1.8869668565457687e-05\n","Epoch  22 Batch  83 / 228  Training Loss  3.65034538845066e-05\n","Epoch  22 Batch  84 / 228  Training Loss  1.921092189149931e-05\n","Epoch  22 Batch  85 / 228  Training Loss  1.898363552754745e-05\n","Epoch  22 Batch  86 / 228  Training Loss  1.988019357668236e-05\n","Epoch  22 Batch  87 / 228  Training Loss  1.685779898252804e-05\n","Epoch  22 Batch  88 / 228  Training Loss  1.6208181477850303e-05\n","Epoch  22 Batch  89 / 228  Training Loss  2.6442890884936787e-05\n","Epoch  22 Batch  90 / 228  Training Loss  1.506930584582733e-05\n","Epoch  22 Batch  91 / 228  Training Loss  1.5048346540424973e-05\n","Epoch  22 Batch  92 / 228  Training Loss  1.4869297046971042e-05\n","Epoch  22 Batch  93 / 228  Training Loss  1.3530421711038798e-05\n","Epoch  22 Batch  94 / 228  Training Loss  2.9093318516970612e-05\n","Epoch  22 Batch  95 / 228  Training Loss  1.879460978670977e-05\n","Epoch  22 Batch  96 / 228  Training Loss  1.6166415662155487e-05\n","Epoch  22 Batch  97 / 228  Training Loss  1.6917401808314025e-05\n","Epoch  22 Batch  98 / 228  Training Loss  1.5301626262953505e-05\n","Epoch  22 Batch  99 / 228  Training Loss  2.2943066142033786e-05\n","Epoch  22 Batch  100 / 228  Training Loss  1.931334008986596e-05\n","Epoch  22 Batch  101 / 228  Training Loss  2.3856331608840264e-05\n","Epoch  22 Batch  102 / 228  Training Loss  1.4312538951344322e-05\n","Epoch  22 Batch  103 / 228  Training Loss  1.3399601812125184e-05\n","Epoch  22 Batch  104 / 228  Training Loss  1.7808215488912538e-05\n","Epoch  22 Batch  105 / 228  Training Loss  2.45446808548877e-05\n","Epoch  22 Batch  106 / 228  Training Loss  1.8495811673346907e-05\n","Epoch  22 Batch  107 / 228  Training Loss  1.8090793673763983e-05\n","Epoch  22 Batch  108 / 228  Training Loss  1.4330270460050087e-05\n","Epoch  22 Batch  109 / 228  Training Loss  1.3842834960087202e-05\n","Epoch  22 Batch  110 / 228  Training Loss  2.286575545440428e-05\n","Epoch  22 Batch  111 / 228  Training Loss  1.5663121303077787e-05\n","Epoch  22 Batch  112 / 228  Training Loss  1.9051665731240064e-05\n","Epoch  22 Batch  113 / 228  Training Loss  1.7796213796827942e-05\n","Epoch  22 Batch  114 / 228  Training Loss  1.647748467803467e-05\n","Epoch  22 Batch  115 / 228  Training Loss  1.8114893464371562e-05\n","Epoch  22 Batch  116 / 228  Training Loss  1.6311827494064346e-05\n","Epoch  22 Batch  117 / 228  Training Loss  2.0083385606994852e-05\n","Epoch  22 Batch  118 / 228  Training Loss  1.5562203770969063e-05\n","Epoch  22 Batch  119 / 228  Training Loss  2.59084135905141e-05\n","Epoch  22 Batch  120 / 228  Training Loss  1.8745904526440427e-05\n","Epoch  22 Batch  121 / 228  Training Loss  1.8621076378622092e-05\n","Epoch  22 Batch  122 / 228  Training Loss  1.2551499821711332e-05\n","Epoch  22 Batch  123 / 228  Training Loss  1.3873966054234188e-05\n","Epoch  22 Batch  124 / 228  Training Loss  1.9212453480577096e-05\n","Epoch  22 Batch  125 / 228  Training Loss  1.9141079974360764e-05\n","Epoch  22 Batch  126 / 228  Training Loss  1.581281321705319e-05\n","Epoch  22 Batch  127 / 228  Training Loss  1.4655946870334446e-05\n","Epoch  22 Batch  128 / 228  Training Loss  1.762062674970366e-05\n","Epoch  22 Batch  129 / 228  Training Loss  1.8182323401561007e-05\n","Epoch  22 Batch  130 / 228  Training Loss  1.9938906916650012e-05\n","Epoch  22 Batch  131 / 228  Training Loss  1.6710087948013097e-05\n","Epoch  22 Batch  132 / 228  Training Loss  1.7242329704458825e-05\n","Epoch  22 Batch  133 / 228  Training Loss  1.653012805036269e-05\n","Epoch  22 Batch  134 / 228  Training Loss  2.6099596652784385e-05\n","Epoch  22 Batch  135 / 228  Training Loss  1.3631160072691273e-05\n","Epoch  22 Batch  136 / 228  Training Loss  1.8196958990301937e-05\n","Epoch  22 Batch  137 / 228  Training Loss  1.2709909242403228e-05\n","Epoch  22 Batch  138 / 228  Training Loss  1.8848228137358092e-05\n","Epoch  22 Batch  139 / 228  Training Loss  2.263788519485388e-05\n","Epoch  22 Batch  140 / 228  Training Loss  2.1835161533090286e-05\n","Epoch  22 Batch  141 / 228  Training Loss  1.70845723914681e-05\n","Epoch  22 Batch  142 / 228  Training Loss  1.4389760508493055e-05\n","Epoch  22 Batch  143 / 228  Training Loss  1.4220463526726235e-05\n","Epoch  22 Batch  144 / 228  Training Loss  2.01297843887005e-05\n","Epoch  22 Batch  145 / 228  Training Loss  1.3138625945430249e-05\n","Epoch  22 Batch  146 / 228  Training Loss  1.8222637663711794e-05\n","Epoch  22 Batch  147 / 228  Training Loss  2.2374742911779322e-05\n","Epoch  22 Batch  148 / 228  Training Loss  1.6582669559284113e-05\n","Epoch  22 Batch  149 / 228  Training Loss  1.6618503650533967e-05\n","Epoch  22 Batch  150 / 228  Training Loss  1.4391352124221157e-05\n","Epoch  22 Batch  151 / 228  Training Loss  1.4236303286452312e-05\n","Epoch  22 Batch  152 / 228  Training Loss  2.427310755592771e-05\n","Epoch  22 Batch  153 / 228  Training Loss  1.6228246749960817e-05\n","Epoch  22 Batch  154 / 228  Training Loss  1.898006121336948e-05\n","Epoch  22 Batch  155 / 228  Training Loss  1.236705884366529e-05\n","Epoch  22 Batch  156 / 228  Training Loss  2.500623850210104e-05\n","Epoch  22 Batch  157 / 228  Training Loss  1.7535277947899885e-05\n","Epoch  22 Batch  158 / 228  Training Loss  1.603318378329277e-05\n","Epoch  22 Batch  159 / 228  Training Loss  1.4284536518971436e-05\n","Epoch  22 Batch  160 / 228  Training Loss  1.3256374586489983e-05\n","Epoch  22 Batch  161 / 228  Training Loss  1.7896303688758053e-05\n","Epoch  22 Batch  162 / 228  Training Loss  1.4095729056862183e-05\n","Epoch  22 Batch  163 / 228  Training Loss  3.411624493310228e-05\n","Epoch  22 Batch  164 / 228  Training Loss  2.582244087534491e-05\n","Epoch  22 Batch  165 / 228  Training Loss  1.5898660421953537e-05\n","Epoch  22 Batch  166 / 228  Training Loss  2.0706796931335703e-05\n","Epoch  22 Batch  167 / 228  Training Loss  1.720392538118176e-05\n","Epoch  22 Batch  168 / 228  Training Loss  1.6763757230364718e-05\n","Epoch  22 Batch  169 / 228  Training Loss  2.595487967482768e-05\n","Epoch  22 Batch  170 / 228  Training Loss  1.986665665754117e-05\n","Epoch  22 Batch  171 / 228  Training Loss  2.0599840354407206e-05\n","Epoch  22 Batch  172 / 228  Training Loss  1.6733500160626136e-05\n","Epoch  22 Batch  173 / 228  Training Loss  1.5504369002883323e-05\n","Epoch  22 Batch  174 / 228  Training Loss  2.099074117722921e-05\n","Epoch  22 Batch  175 / 228  Training Loss  2.0866795239271596e-05\n","Epoch  22 Batch  176 / 228  Training Loss  1.7476961147622205e-05\n","Epoch  22 Batch  177 / 228  Training Loss  1.7702508557704277e-05\n","Epoch  22 Batch  178 / 228  Training Loss  1.712444645818323e-05\n","Epoch  22 Batch  179 / 228  Training Loss  2.0953910279786214e-05\n","Epoch  22 Batch  180 / 228  Training Loss  1.4194230971042998e-05\n","Epoch  22 Batch  181 / 228  Training Loss  1.3670649423147552e-05\n","Epoch  22 Batch  182 / 228  Training Loss  1.7835413018474355e-05\n","Epoch  22 Batch  183 / 228  Training Loss  1.860162228695117e-05\n","Epoch  22 Batch  184 / 228  Training Loss  1.8893191736424342e-05\n","Epoch  22 Batch  185 / 228  Training Loss  2.7584519557422027e-05\n","Epoch  22 Batch  186 / 228  Training Loss  1.741574305924587e-05\n","Epoch  22 Batch  187 / 228  Training Loss  1.6008913007681258e-05\n","Epoch  22 Batch  188 / 228  Training Loss  1.4772342183277942e-05\n","Epoch  22 Batch  189 / 228  Training Loss  1.751803938532248e-05\n","Epoch  22 Batch  190 / 228  Training Loss  1.3016750926908571e-05\n","Epoch  22 Batch  191 / 228  Training Loss  2.2235923097468913e-05\n","Epoch  22 Batch  192 / 228  Training Loss  1.8813632777892053e-05\n","Epoch  22 Batch  193 / 228  Training Loss  1.8689926946535707e-05\n","Epoch  22 Batch  194 / 228  Training Loss  1.3513958947442006e-05\n","Epoch  22 Batch  195 / 228  Training Loss  1.6531115761608817e-05\n","Epoch  22 Batch  196 / 228  Training Loss  1.708331001282204e-05\n","Epoch  22 Batch  197 / 228  Training Loss  1.8633429135661572e-05\n","Epoch  22 Batch  198 / 228  Training Loss  1.3730907994613517e-05\n","Epoch  22 Batch  199 / 228  Training Loss  1.760244595061522e-05\n","Epoch  22 Batch  200 / 228  Training Loss  2.0999275875510648e-05\n","Epoch  22 Batch  201 / 228  Training Loss  2.05775741051184e-05\n","Epoch  22 Batch  202 / 228  Training Loss  1.673711449257098e-05\n","Epoch  22 Batch  203 / 228  Training Loss  1.2405966117512435e-05\n","Epoch  22 Batch  204 / 228  Training Loss  3.126823139609769e-05\n","Epoch  22 Batch  205 / 228  Training Loss  1.8269089196110144e-05\n","Epoch  22 Batch  206 / 228  Training Loss  1.6479267287650146e-05\n","Epoch  22 Batch  207 / 228  Training Loss  2.492616476956755e-05\n","Epoch  22 Batch  208 / 228  Training Loss  1.3051858331891708e-05\n","Epoch  22 Batch  209 / 228  Training Loss  1.3211599252827e-05\n","Epoch  22 Batch  210 / 228  Training Loss  1.6706415408407338e-05\n","Epoch  22 Batch  211 / 228  Training Loss  1.6837673683767207e-05\n","Epoch  22 Batch  212 / 228  Training Loss  1.4376300896401517e-05\n","Epoch  22 Batch  213 / 228  Training Loss  1.756749043124728e-05\n","Epoch  22 Batch  214 / 228  Training Loss  1.459461600461509e-05\n","Epoch  22 Batch  215 / 228  Training Loss  1.739236176945269e-05\n","Epoch  22 Batch  216 / 228  Training Loss  1.4193198694556486e-05\n","Epoch  22 Batch  217 / 228  Training Loss  1.4736059711140115e-05\n","Epoch  22 Batch  218 / 228  Training Loss  1.5185161828412674e-05\n","Epoch  22 Batch  219 / 228  Training Loss  1.3321192454895936e-05\n","Epoch  22 Batch  220 / 228  Training Loss  2.7177593437954783e-05\n","Epoch  22 Batch  221 / 228  Training Loss  2.6280686142854393e-05\n","Epoch  22 Batch  222 / 228  Training Loss  1.7156728063127957e-05\n","Epoch  22 Batch  223 / 228  Training Loss  1.302259352087276e-05\n","Epoch  22 Batch  224 / 228  Training Loss  1.801367398002185e-05\n","Epoch  22 Batch  225 / 228  Training Loss  1.988779149542097e-05\n","Epoch  22 Batch  226 / 228  Training Loss  1.3536495316657238e-05\n","Epoch  22 Batch  227 / 228  Training Loss  1.743096436257474e-05\n","  23    |    -    |   0.000018   | 99.237805\n","----------------------------------------------------------------------\n","Running epoch: 23\n","Epoch  23 Batch  0 / 228  Training Loss  1.6171052266145125e-05\n","Epoch  23 Batch  1 / 228  Training Loss  2.1716756236855872e-05\n","Epoch  23 Batch  2 / 228  Training Loss  1.8505592379369773e-05\n","Epoch  23 Batch  3 / 228  Training Loss  2.1108648070367053e-05\n","Epoch  23 Batch  4 / 228  Training Loss  2.945352389360778e-05\n","Epoch  23 Batch  5 / 228  Training Loss  1.5700430594733916e-05\n","Epoch  23 Batch  6 / 228  Training Loss  1.9268383766757324e-05\n","Epoch  23 Batch  7 / 228  Training Loss  1.4353699043567758e-05\n","Epoch  23 Batch  8 / 228  Training Loss  1.2722417523036711e-05\n","Epoch  23 Batch  9 / 228  Training Loss  1.798281300580129e-05\n","Epoch  23 Batch  10 / 228  Training Loss  2.5264371288358234e-05\n","Epoch  23 Batch  11 / 228  Training Loss  1.661733040236868e-05\n","Epoch  23 Batch  12 / 228  Training Loss  1.6305968529195525e-05\n","Epoch  23 Batch  13 / 228  Training Loss  1.930893267854117e-05\n","Epoch  23 Batch  14 / 228  Training Loss  2.1163508790777996e-05\n","Epoch  23 Batch  15 / 228  Training Loss  1.4231251952878665e-05\n","Epoch  23 Batch  16 / 228  Training Loss  1.651232742005959e-05\n","Epoch  23 Batch  17 / 228  Training Loss  1.7702055629342794e-05\n","Epoch  23 Batch  18 / 228  Training Loss  1.6762438463047147e-05\n","Epoch  23 Batch  19 / 228  Training Loss  1.3503980881068856e-05\n","Epoch  23 Batch  20 / 228  Training Loss  1.9285896996734664e-05\n","Epoch  23 Batch  21 / 228  Training Loss  1.6104069800348952e-05\n","Epoch  23 Batch  22 / 228  Training Loss  1.303828594245715e-05\n","Epoch  23 Batch  23 / 228  Training Loss  1.0714791642385535e-05\n","Epoch  23 Batch  24 / 228  Training Loss  1.2838380825996865e-05\n","Epoch  23 Batch  25 / 228  Training Loss  1.6108440831885673e-05\n","Epoch  23 Batch  26 / 228  Training Loss  1.4793309674132615e-05\n","Epoch  23 Batch  27 / 228  Training Loss  1.134962985815946e-05\n","Epoch  23 Batch  28 / 228  Training Loss  2.2504624212160707e-05\n","Epoch  23 Batch  29 / 228  Training Loss  2.4442295398330316e-05\n","Epoch  23 Batch  30 / 228  Training Loss  1.6238494936260395e-05\n","Epoch  23 Batch  31 / 228  Training Loss  1.4460686543316115e-05\n","Epoch  23 Batch  32 / 228  Training Loss  1.3332935850485228e-05\n","Epoch  23 Batch  33 / 228  Training Loss  1.4505125363939442e-05\n","Epoch  23 Batch  34 / 228  Training Loss  1.3051188034296501e-05\n","Epoch  23 Batch  35 / 228  Training Loss  1.975669692910742e-05\n","Epoch  23 Batch  36 / 228  Training Loss  1.5950414308463223e-05\n","Epoch  23 Batch  37 / 228  Training Loss  1.8499940779292956e-05\n","Epoch  23 Batch  38 / 228  Training Loss  3.0339366276166402e-05\n","Epoch  23 Batch  39 / 228  Training Loss  1.9660346879391e-05\n","Epoch  23 Batch  40 / 228  Training Loss  1.9268039977760054e-05\n","Epoch  23 Batch  41 / 228  Training Loss  1.7712614862830378e-05\n","Epoch  23 Batch  42 / 228  Training Loss  2.974000744870864e-05\n","Epoch  23 Batch  43 / 228  Training Loss  1.5810332115506753e-05\n","Epoch  23 Batch  44 / 228  Training Loss  2.2333135348162614e-05\n","Epoch  23 Batch  45 / 228  Training Loss  1.3424034477793612e-05\n","Epoch  23 Batch  46 / 228  Training Loss  1.2026384865748696e-05\n","Epoch  23 Batch  47 / 228  Training Loss  2.028893504757434e-05\n","Epoch  23 Batch  48 / 228  Training Loss  1.586942744324915e-05\n","Epoch  23 Batch  49 / 228  Training Loss  1.576932663738262e-05\n","Epoch  23 Batch  50 / 228  Training Loss  1.2005910321022384e-05\n","Epoch  23 Batch  51 / 228  Training Loss  1.7752263374859467e-05\n","Epoch  23 Batch  52 / 228  Training Loss  1.2387972674332559e-05\n","Epoch  23 Batch  53 / 228  Training Loss  1.1960022675339133e-05\n","Epoch  23 Batch  54 / 228  Training Loss  1.683570553723257e-05\n","Epoch  23 Batch  55 / 228  Training Loss  2.2192161850398406e-05\n","Epoch  23 Batch  56 / 228  Training Loss  1.361495833407389e-05\n","Epoch  23 Batch  57 / 228  Training Loss  2.061034865619149e-05\n","Epoch  23 Batch  58 / 228  Training Loss  1.7422260498278774e-05\n","Epoch  23 Batch  59 / 228  Training Loss  2.182309799536597e-05\n","Epoch  23 Batch  60 / 228  Training Loss  1.4921030015102588e-05\n","Epoch  23 Batch  61 / 228  Training Loss  1.529314977233298e-05\n","Epoch  23 Batch  62 / 228  Training Loss  1.9656044969451614e-05\n","Epoch  23 Batch  63 / 228  Training Loss  1.6186886568902992e-05\n","Epoch  23 Batch  64 / 228  Training Loss  1.7644473700784147e-05\n","Epoch  23 Batch  65 / 228  Training Loss  2.7843823772855103e-05\n","Epoch  23 Batch  66 / 228  Training Loss  1.7764290532795712e-05\n","Epoch  23 Batch  67 / 228  Training Loss  1.743820757837966e-05\n","Epoch  23 Batch  68 / 228  Training Loss  1.4001528143126052e-05\n","Epoch  23 Batch  69 / 228  Training Loss  1.931532096932642e-05\n","Epoch  23 Batch  70 / 228  Training Loss  2.5430248570046388e-05\n","Epoch  23 Batch  71 / 228  Training Loss  1.5828070900170133e-05\n","Epoch  23 Batch  72 / 228  Training Loss  1.9858749510603957e-05\n","Epoch  23 Batch  73 / 228  Training Loss  1.5353811249951832e-05\n","Epoch  23 Batch  74 / 228  Training Loss  1.698995583865326e-05\n","Epoch  23 Batch  75 / 228  Training Loss  1.4171945622365456e-05\n","Epoch  23 Batch  76 / 228  Training Loss  1.922392584674526e-05\n","Epoch  23 Batch  77 / 228  Training Loss  1.726433401927352e-05\n","Epoch  23 Batch  78 / 228  Training Loss  1.7475456843385473e-05\n","Epoch  23 Batch  79 / 228  Training Loss  1.5566502042929642e-05\n","Epoch  23 Batch  80 / 228  Training Loss  1.723695640976075e-05\n","Epoch  23 Batch  81 / 228  Training Loss  1.3242299246485345e-05\n","Epoch  23 Batch  82 / 228  Training Loss  1.2573650565173011e-05\n","Epoch  23 Batch  83 / 228  Training Loss  1.2013802006549668e-05\n","Epoch  23 Batch  84 / 228  Training Loss  2.366023909416981e-05\n","Epoch  23 Batch  85 / 228  Training Loss  2.147897976101376e-05\n","Epoch  23 Batch  86 / 228  Training Loss  1.6870884792297147e-05\n","Epoch  23 Batch  87 / 228  Training Loss  1.4199913493939675e-05\n","Epoch  23 Batch  88 / 228  Training Loss  1.5975023416103795e-05\n","Epoch  23 Batch  89 / 228  Training Loss  1.5741079550934955e-05\n","Epoch  23 Batch  90 / 228  Training Loss  1.5188366887741722e-05\n","Epoch  23 Batch  91 / 228  Training Loss  1.7275366190006025e-05\n","Epoch  23 Batch  92 / 228  Training Loss  1.4826064216322266e-05\n","Epoch  23 Batch  93 / 228  Training Loss  1.286936276301276e-05\n","Epoch  23 Batch  94 / 228  Training Loss  2.1312156604835764e-05\n","Epoch  23 Batch  95 / 228  Training Loss  1.982214962481521e-05\n","Epoch  23 Batch  96 / 228  Training Loss  1.3046019375906326e-05\n","Epoch  23 Batch  97 / 228  Training Loss  1.663096554693766e-05\n","Epoch  23 Batch  98 / 228  Training Loss  1.818692726374138e-05\n","Epoch  23 Batch  99 / 228  Training Loss  2.115230199706275e-05\n","Epoch  23 Batch  100 / 228  Training Loss  1.793432420527097e-05\n","Epoch  23 Batch  101 / 228  Training Loss  1.7650294466875494e-05\n","Epoch  23 Batch  102 / 228  Training Loss  1.8020002244156785e-05\n","Epoch  23 Batch  103 / 228  Training Loss  1.349067588307662e-05\n","Epoch  23 Batch  104 / 228  Training Loss  1.903909469547216e-05\n","Epoch  23 Batch  105 / 228  Training Loss  1.721889930195175e-05\n","Epoch  23 Batch  106 / 228  Training Loss  1.536746640340425e-05\n","Epoch  23 Batch  107 / 228  Training Loss  1.4048232515051495e-05\n","Epoch  23 Batch  108 / 228  Training Loss  1.781352148100268e-05\n","Epoch  23 Batch  109 / 228  Training Loss  9.52495429373812e-06\n","Epoch  23 Batch  110 / 228  Training Loss  1.2238089766469784e-05\n","Epoch  23 Batch  111 / 228  Training Loss  1.8750231902231462e-05\n","Epoch  23 Batch  112 / 228  Training Loss  2.064066575258039e-05\n","Epoch  23 Batch  113 / 228  Training Loss  2.029427378147375e-05\n","Epoch  23 Batch  114 / 228  Training Loss  1.851355227699969e-05\n","Epoch  23 Batch  115 / 228  Training Loss  2.0007239072583616e-05\n","Epoch  23 Batch  116 / 228  Training Loss  1.7082293197745457e-05\n","Epoch  23 Batch  117 / 228  Training Loss  1.755290213623084e-05\n","Epoch  23 Batch  118 / 228  Training Loss  1.7981004930334166e-05\n","Epoch  23 Batch  119 / 228  Training Loss  1.5933472241158597e-05\n","Epoch  23 Batch  120 / 228  Training Loss  1.5030438589747064e-05\n","Epoch  23 Batch  121 / 228  Training Loss  1.7007148926495574e-05\n","Epoch  23 Batch  122 / 228  Training Loss  1.8725007976172492e-05\n","Epoch  23 Batch  123 / 228  Training Loss  1.312426775257336e-05\n","Epoch  23 Batch  124 / 228  Training Loss  2.176167254219763e-05\n","Epoch  23 Batch  125 / 228  Training Loss  2.422563557047397e-05\n","Epoch  23 Batch  126 / 228  Training Loss  1.0825579010997899e-05\n","Epoch  23 Batch  127 / 228  Training Loss  1.3725203643843997e-05\n","Epoch  23 Batch  128 / 228  Training Loss  9.909042091749143e-06\n","Epoch  23 Batch  129 / 228  Training Loss  1.4799935343035031e-05\n","Epoch  23 Batch  130 / 228  Training Loss  1.2936450730194338e-05\n","Epoch  23 Batch  131 / 228  Training Loss  1.7578056940692477e-05\n","Epoch  23 Batch  132 / 228  Training Loss  1.1929502761631738e-05\n","Epoch  23 Batch  133 / 228  Training Loss  1.2169441106379963e-05\n","Epoch  23 Batch  134 / 228  Training Loss  1.5620909834979102e-05\n","Epoch  23 Batch  135 / 228  Training Loss  1.3792091522191186e-05\n","Epoch  23 Batch  136 / 228  Training Loss  2.3433038222719915e-05\n","Epoch  23 Batch  137 / 228  Training Loss  1.2815665286325384e-05\n","Epoch  23 Batch  138 / 228  Training Loss  1.4698382074129768e-05\n","Epoch  23 Batch  139 / 228  Training Loss  1.4665984963357914e-05\n","Epoch  23 Batch  140 / 228  Training Loss  1.6669382603140548e-05\n","Epoch  23 Batch  141 / 228  Training Loss  1.9160090232617222e-05\n","Epoch  23 Batch  142 / 228  Training Loss  2.6967600206262432e-05\n","Epoch  23 Batch  143 / 228  Training Loss  1.8363047274760902e-05\n","Epoch  23 Batch  144 / 228  Training Loss  1.5809850083314814e-05\n","Epoch  23 Batch  145 / 228  Training Loss  1.3644059436046518e-05\n","Epoch  23 Batch  146 / 228  Training Loss  1.8951801393995993e-05\n","Epoch  23 Batch  147 / 228  Training Loss  1.2478851203923114e-05\n","Epoch  23 Batch  148 / 228  Training Loss  1.5712797903688625e-05\n","Epoch  23 Batch  149 / 228  Training Loss  1.5549745512544177e-05\n","Epoch  23 Batch  150 / 228  Training Loss  1.3136302186467219e-05\n","Epoch  23 Batch  151 / 228  Training Loss  1.4921895854058675e-05\n","Epoch  23 Batch  152 / 228  Training Loss  1.963311297004111e-05\n","Epoch  23 Batch  153 / 228  Training Loss  1.234937099070521e-05\n","Epoch  23 Batch  154 / 228  Training Loss  2.217429573647678e-05\n","Epoch  23 Batch  155 / 228  Training Loss  1.8244169041281566e-05\n","Epoch  23 Batch  156 / 228  Training Loss  1.5646985048078932e-05\n","Epoch  23 Batch  157 / 228  Training Loss  1.4529767213389277e-05\n","Epoch  23 Batch  158 / 228  Training Loss  1.5132645785342902e-05\n","Epoch  23 Batch  159 / 228  Training Loss  1.9816236090264283e-05\n","Epoch  23 Batch  160 / 228  Training Loss  2.138497802661732e-05\n","Epoch  23 Batch  161 / 228  Training Loss  2.19472058233805e-05\n","Epoch  23 Batch  162 / 228  Training Loss  1.780515958671458e-05\n","Epoch  23 Batch  163 / 228  Training Loss  1.702012559690047e-05\n","Epoch  23 Batch  164 / 228  Training Loss  1.537732532597147e-05\n","Epoch  23 Batch  165 / 228  Training Loss  2.1046607798780315e-05\n","Epoch  23 Batch  166 / 228  Training Loss  1.2200616765767336e-05\n","Epoch  23 Batch  167 / 228  Training Loss  1.3124197721481323e-05\n","Epoch  23 Batch  168 / 228  Training Loss  1.905135650304146e-05\n","Epoch  23 Batch  169 / 228  Training Loss  1.3630417925014626e-05\n","Epoch  23 Batch  170 / 228  Training Loss  2.026810216193553e-05\n","Epoch  23 Batch  171 / 228  Training Loss  2.1660409402102232e-05\n","Epoch  23 Batch  172 / 228  Training Loss  1.5146081750572193e-05\n","Epoch  23 Batch  173 / 228  Training Loss  1.571603752381634e-05\n","Epoch  23 Batch  174 / 228  Training Loss  1.6143119864864275e-05\n","Epoch  23 Batch  175 / 228  Training Loss  1.4804565580561757e-05\n","Epoch  23 Batch  176 / 228  Training Loss  1.3322593076736666e-05\n","Epoch  23 Batch  177 / 228  Training Loss  1.2366821465548128e-05\n","Epoch  23 Batch  178 / 228  Training Loss  1.844872531364672e-05\n","Epoch  23 Batch  179 / 228  Training Loss  2.327222318854183e-05\n","Epoch  23 Batch  180 / 228  Training Loss  1.5890371287241578e-05\n","Epoch  23 Batch  181 / 228  Training Loss  2.0367317119962536e-05\n","Epoch  23 Batch  182 / 228  Training Loss  2.0248025975888595e-05\n","Epoch  23 Batch  183 / 228  Training Loss  1.484861240896862e-05\n","Epoch  23 Batch  184 / 228  Training Loss  1.2878957022621762e-05\n","Epoch  23 Batch  185 / 228  Training Loss  2.4415217922069132e-05\n","Epoch  23 Batch  186 / 228  Training Loss  1.5778390661580488e-05\n","Epoch  23 Batch  187 / 228  Training Loss  1.4881569768476766e-05\n","Epoch  23 Batch  188 / 228  Training Loss  2.3114849682315253e-05\n","Epoch  23 Batch  189 / 228  Training Loss  2.0496871002251282e-05\n","Epoch  23 Batch  190 / 228  Training Loss  1.4722357263963204e-05\n","Epoch  23 Batch  191 / 228  Training Loss  1.5296922356355935e-05\n","Epoch  23 Batch  192 / 228  Training Loss  1.6375690393033437e-05\n","Epoch  23 Batch  193 / 228  Training Loss  1.2384432011458557e-05\n","Epoch  23 Batch  194 / 228  Training Loss  1.1090448424511123e-05\n","Epoch  23 Batch  195 / 228  Training Loss  1.157312453869963e-05\n","Epoch  23 Batch  196 / 228  Training Loss  1.6225736544583924e-05\n","Epoch  23 Batch  197 / 228  Training Loss  1.5040386642795056e-05\n","Epoch  23 Batch  198 / 228  Training Loss  1.6377320207539015e-05\n","Epoch  23 Batch  199 / 228  Training Loss  1.8166785594075918e-05\n","Epoch  23 Batch  200 / 228  Training Loss  2.5375271434313618e-05\n","Epoch  23 Batch  201 / 228  Training Loss  1.967570642591454e-05\n","Epoch  23 Batch  202 / 228  Training Loss  1.5356299627455883e-05\n","Epoch  23 Batch  203 / 228  Training Loss  2.12516933970619e-05\n","Epoch  23 Batch  204 / 228  Training Loss  2.2041476768208668e-05\n","Epoch  23 Batch  205 / 228  Training Loss  1.2576007975440007e-05\n","Epoch  23 Batch  206 / 228  Training Loss  1.9106944819213822e-05\n","Epoch  23 Batch  207 / 228  Training Loss  1.5296887795557268e-05\n","Epoch  23 Batch  208 / 228  Training Loss  1.7643713363213465e-05\n","Epoch  23 Batch  209 / 228  Training Loss  1.7403566744178534e-05\n","Epoch  23 Batch  210 / 228  Training Loss  2.2454725694842637e-05\n","Epoch  23 Batch  211 / 228  Training Loss  1.8421744243823923e-05\n","Epoch  23 Batch  212 / 228  Training Loss  1.1712619198078755e-05\n","Epoch  23 Batch  213 / 228  Training Loss  1.2122963198635262e-05\n","Epoch  23 Batch  214 / 228  Training Loss  1.650004560360685e-05\n","Epoch  23 Batch  215 / 228  Training Loss  2.3998292817850597e-05\n","Epoch  23 Batch  216 / 228  Training Loss  1.756869096425362e-05\n","Epoch  23 Batch  217 / 228  Training Loss  1.2261438314453699e-05\n","Epoch  23 Batch  218 / 228  Training Loss  1.3243205103208311e-05\n","Epoch  23 Batch  219 / 228  Training Loss  1.4233842193789314e-05\n","Epoch  23 Batch  220 / 228  Training Loss  1.53669789142441e-05\n","Epoch  23 Batch  221 / 228  Training Loss  2.325726927665528e-05\n","Epoch  23 Batch  222 / 228  Training Loss  1.4962408386054449e-05\n","Epoch  23 Batch  223 / 228  Training Loss  1.617429734324105e-05\n","Epoch  23 Batch  224 / 228  Training Loss  1.561316821607761e-05\n","Epoch  23 Batch  225 / 228  Training Loss  1.264508864551317e-05\n","Epoch  23 Batch  226 / 228  Training Loss  1.8173986973124556e-05\n","Epoch  23 Batch  227 / 228  Training Loss  1.2001582035736647e-05\n","  24    |    -    |   0.000017   | 99.237805\n","----------------------------------------------------------------------\n","Running epoch: 24\n","Epoch  24 Batch  0 / 228  Training Loss  1.393465390719939e-05\n","Epoch  24 Batch  1 / 228  Training Loss  1.0763668797153514e-05\n","Epoch  24 Batch  2 / 228  Training Loss  1.6390416931244545e-05\n","Epoch  24 Batch  3 / 228  Training Loss  1.5673209418309852e-05\n","Epoch  24 Batch  4 / 228  Training Loss  1.5712488675490022e-05\n","Epoch  24 Batch  5 / 228  Training Loss  1.4059590284887236e-05\n","Epoch  24 Batch  6 / 228  Training Loss  2.0790801499970257e-05\n","Epoch  24 Batch  7 / 228  Training Loss  1.5342398910433985e-05\n","Epoch  24 Batch  8 / 228  Training Loss  1.2648670235648751e-05\n","Epoch  24 Batch  9 / 228  Training Loss  1.62047945195809e-05\n","Epoch  24 Batch  10 / 228  Training Loss  1.82981530088e-05\n","Epoch  24 Batch  11 / 228  Training Loss  1.0009604920924176e-05\n","Epoch  24 Batch  12 / 228  Training Loss  1.5149375940382015e-05\n","Epoch  24 Batch  13 / 228  Training Loss  2.3732933186693117e-05\n","Epoch  24 Batch  14 / 228  Training Loss  1.8409362382953987e-05\n","Epoch  24 Batch  15 / 228  Training Loss  2.1345675122574903e-05\n","Epoch  24 Batch  16 / 228  Training Loss  1.1896666364918929e-05\n","Epoch  24 Batch  17 / 228  Training Loss  1.968787910300307e-05\n","Epoch  24 Batch  18 / 228  Training Loss  1.719260217214469e-05\n","Epoch  24 Batch  19 / 228  Training Loss  1.9137829440296628e-05\n","Epoch  24 Batch  20 / 228  Training Loss  1.1692908628901932e-05\n","Epoch  24 Batch  21 / 228  Training Loss  1.2760026947944425e-05\n","Epoch  24 Batch  22 / 228  Training Loss  1.9878199964296073e-05\n","Epoch  24 Batch  23 / 228  Training Loss  1.1981508578173816e-05\n","Epoch  24 Batch  24 / 228  Training Loss  1.4098130122874863e-05\n","Epoch  24 Batch  25 / 228  Training Loss  1.5817033272469416e-05\n","Epoch  24 Batch  26 / 228  Training Loss  1.6750058421166614e-05\n","Epoch  24 Batch  27 / 228  Training Loss  1.978196087293327e-05\n","Epoch  24 Batch  28 / 228  Training Loss  1.4713376003783196e-05\n","Epoch  24 Batch  29 / 228  Training Loss  1.2918857464683242e-05\n","Epoch  24 Batch  30 / 228  Training Loss  1.6919950212468393e-05\n","Epoch  24 Batch  31 / 228  Training Loss  1.5543577319476753e-05\n","Epoch  24 Batch  32 / 228  Training Loss  2.0049739759997465e-05\n","Epoch  24 Batch  33 / 228  Training Loss  1.5631581845809706e-05\n","Epoch  24 Batch  34 / 228  Training Loss  2.0687191863544285e-05\n","Epoch  24 Batch  35 / 228  Training Loss  1.779154990799725e-05\n","Epoch  24 Batch  36 / 228  Training Loss  1.1778000953199808e-05\n","Epoch  24 Batch  37 / 228  Training Loss  1.716647602734156e-05\n","Epoch  24 Batch  38 / 228  Training Loss  1.7295809811912477e-05\n","Epoch  24 Batch  39 / 228  Training Loss  1.4425741028389893e-05\n","Epoch  24 Batch  40 / 228  Training Loss  1.8872411601478234e-05\n","Epoch  24 Batch  41 / 228  Training Loss  1.6825020793476142e-05\n","Epoch  24 Batch  42 / 228  Training Loss  1.5802159396116622e-05\n","Epoch  24 Batch  43 / 228  Training Loss  2.3537013476016e-05\n","Epoch  24 Batch  44 / 228  Training Loss  1.941213122336194e-05\n","Epoch  24 Batch  45 / 228  Training Loss  1.5677022020099685e-05\n","Epoch  24 Batch  46 / 228  Training Loss  1.915839129651431e-05\n","Epoch  24 Batch  47 / 228  Training Loss  1.4252696018957067e-05\n","Epoch  24 Batch  48 / 228  Training Loss  1.8975948478328064e-05\n","Epoch  24 Batch  49 / 228  Training Loss  1.194913602375891e-05\n","Epoch  24 Batch  50 / 228  Training Loss  1.5685154721722938e-05\n","Epoch  24 Batch  51 / 228  Training Loss  2.628391121106688e-05\n","Epoch  24 Batch  52 / 228  Training Loss  1.650151716603432e-05\n","Epoch  24 Batch  53 / 228  Training Loss  1.8777363948174752e-05\n","Epoch  24 Batch  54 / 228  Training Loss  9.355420843348838e-06\n","Epoch  24 Batch  55 / 228  Training Loss  1.9641367543954402e-05\n","Epoch  24 Batch  56 / 228  Training Loss  1.680470086284913e-05\n","Epoch  24 Batch  57 / 228  Training Loss  1.5010575225460343e-05\n","Epoch  24 Batch  58 / 228  Training Loss  1.770146263879724e-05\n","Epoch  24 Batch  59 / 228  Training Loss  1.1911399269592948e-05\n","Epoch  24 Batch  60 / 228  Training Loss  1.8075039406539872e-05\n","Epoch  24 Batch  61 / 228  Training Loss  1.3262234460853506e-05\n","Epoch  24 Batch  62 / 228  Training Loss  1.2147804227424785e-05\n","Epoch  24 Batch  63 / 228  Training Loss  1.9320505089126527e-05\n","Epoch  24 Batch  64 / 228  Training Loss  2.306739224877674e-05\n","Epoch  24 Batch  65 / 228  Training Loss  1.7612652300158516e-05\n","Epoch  24 Batch  66 / 228  Training Loss  1.476202487538103e-05\n","Epoch  24 Batch  67 / 228  Training Loss  1.3663340723724104e-05\n","Epoch  24 Batch  68 / 228  Training Loss  1.1237117178097833e-05\n","Epoch  24 Batch  69 / 228  Training Loss  1.3699715964321513e-05\n","Epoch  24 Batch  70 / 228  Training Loss  1.6374766346416436e-05\n","Epoch  24 Batch  71 / 228  Training Loss  1.7731654224917293e-05\n","Epoch  24 Batch  72 / 228  Training Loss  1.7228221622644924e-05\n","Epoch  24 Batch  73 / 228  Training Loss  1.3352039786695968e-05\n","Epoch  24 Batch  74 / 228  Training Loss  1.7495553038315848e-05\n","Epoch  24 Batch  75 / 228  Training Loss  1.494751177233411e-05\n","Epoch  24 Batch  76 / 228  Training Loss  1.2921647794428281e-05\n","Epoch  24 Batch  77 / 228  Training Loss  1.699347012618091e-05\n","Epoch  24 Batch  78 / 228  Training Loss  1.1108784747193567e-05\n","Epoch  24 Batch  79 / 228  Training Loss  2.4858411052264273e-05\n","Epoch  24 Batch  80 / 228  Training Loss  1.5065357729326934e-05\n","Epoch  24 Batch  81 / 228  Training Loss  1.9283050278318115e-05\n","Epoch  24 Batch  82 / 228  Training Loss  1.575629175931681e-05\n","Epoch  24 Batch  83 / 228  Training Loss  1.4385764188773464e-05\n","Epoch  24 Batch  84 / 228  Training Loss  2.224344279966317e-05\n","Epoch  24 Batch  85 / 228  Training Loss  1.702690678939689e-05\n","Epoch  24 Batch  86 / 228  Training Loss  1.8684535461943597e-05\n","Epoch  24 Batch  87 / 228  Training Loss  1.5399655239889398e-05\n","Epoch  24 Batch  88 / 228  Training Loss  1.220899594045477e-05\n","Epoch  24 Batch  89 / 228  Training Loss  1.6077825421234593e-05\n","Epoch  24 Batch  90 / 228  Training Loss  1.4297531379270367e-05\n","Epoch  24 Batch  91 / 228  Training Loss  1.245038856723113e-05\n","Epoch  24 Batch  92 / 228  Training Loss  2.3044931367621757e-05\n","Epoch  24 Batch  93 / 228  Training Loss  1.6171448805835098e-05\n","Epoch  24 Batch  94 / 228  Training Loss  1.134566991822794e-05\n","Epoch  24 Batch  95 / 228  Training Loss  1.4217844181985129e-05\n","Epoch  24 Batch  96 / 228  Training Loss  1.839776996348519e-05\n","Epoch  24 Batch  97 / 228  Training Loss  1.029907707561506e-05\n","Epoch  24 Batch  98 / 228  Training Loss  1.801532562240027e-05\n","Epoch  24 Batch  99 / 228  Training Loss  1.311880441789981e-05\n","Epoch  24 Batch  100 / 228  Training Loss  1.394679111399455e-05\n","Epoch  24 Batch  101 / 228  Training Loss  1.3611303074867465e-05\n","Epoch  24 Batch  102 / 228  Training Loss  1.511546997789992e-05\n","Epoch  24 Batch  103 / 228  Training Loss  1.532909482193645e-05\n","Epoch  24 Batch  104 / 228  Training Loss  1.5371460904134437e-05\n","Epoch  24 Batch  105 / 228  Training Loss  1.9959603378083557e-05\n","Epoch  24 Batch  106 / 228  Training Loss  1.1283725143584888e-05\n","Epoch  24 Batch  107 / 228  Training Loss  1.5565181456622668e-05\n","Epoch  24 Batch  108 / 228  Training Loss  1.6658737877150998e-05\n","Epoch  24 Batch  109 / 228  Training Loss  1.3799161024508066e-05\n","Epoch  24 Batch  110 / 228  Training Loss  1.8636830645846203e-05\n","Epoch  24 Batch  111 / 228  Training Loss  1.71190458786441e-05\n","Epoch  24 Batch  112 / 228  Training Loss  1.6478647012263536e-05\n","Epoch  24 Batch  113 / 228  Training Loss  9.31911745283287e-06\n","Epoch  24 Batch  114 / 228  Training Loss  1.825158324209042e-05\n","Epoch  24 Batch  115 / 228  Training Loss  8.362096195924096e-06\n","Epoch  24 Batch  116 / 228  Training Loss  1.5297840946004726e-05\n","Epoch  24 Batch  117 / 228  Training Loss  1.5324965715990402e-05\n","Epoch  24 Batch  118 / 228  Training Loss  1.2718768630293198e-05\n","Epoch  24 Batch  119 / 228  Training Loss  2.1238598492345773e-05\n","Epoch  24 Batch  120 / 228  Training Loss  1.7555468730279244e-05\n","Epoch  24 Batch  121 / 228  Training Loss  1.6766745829954743e-05\n","Epoch  24 Batch  122 / 228  Training Loss  1.4361266039486509e-05\n","Epoch  24 Batch  123 / 228  Training Loss  9.96448943624273e-06\n","Epoch  24 Batch  124 / 228  Training Loss  2.1628020476782694e-05\n","Epoch  24 Batch  125 / 228  Training Loss  1.6062986105680466e-05\n","Epoch  24 Batch  126 / 228  Training Loss  1.323880860581994e-05\n","Epoch  24 Batch  127 / 228  Training Loss  1.9876784790540114e-05\n","Epoch  24 Batch  128 / 228  Training Loss  1.7917051081894897e-05\n","Epoch  24 Batch  129 / 228  Training Loss  1.3441477676678915e-05\n","Epoch  24 Batch  130 / 228  Training Loss  2.2133843231131323e-05\n","Epoch  24 Batch  131 / 228  Training Loss  1.815208452171646e-05\n","Epoch  24 Batch  132 / 228  Training Loss  1.5275190889951773e-05\n","Epoch  24 Batch  133 / 228  Training Loss  1.4058426131668966e-05\n","Epoch  24 Batch  134 / 228  Training Loss  1.076123498933157e-05\n","Epoch  24 Batch  135 / 228  Training Loss  1.2524958947324194e-05\n","Epoch  24 Batch  136 / 228  Training Loss  2.0073644918738864e-05\n","Epoch  24 Batch  137 / 228  Training Loss  1.3787308489554562e-05\n","Epoch  24 Batch  138 / 228  Training Loss  1.256936684512766e-05\n","Epoch  24 Batch  139 / 228  Training Loss  1.2898771274194587e-05\n","Epoch  24 Batch  140 / 228  Training Loss  1.5359872122644447e-05\n","Epoch  24 Batch  141 / 228  Training Loss  1.748533577483613e-05\n","Epoch  24 Batch  142 / 228  Training Loss  1.4927836673450656e-05\n","Epoch  24 Batch  143 / 228  Training Loss  1.0299294444848783e-05\n","Epoch  24 Batch  144 / 228  Training Loss  1.6379079170292243e-05\n","Epoch  24 Batch  145 / 228  Training Loss  1.7031718016369268e-05\n","Epoch  24 Batch  146 / 228  Training Loss  1.3313287126948126e-05\n","Epoch  24 Batch  147 / 228  Training Loss  1.560273813083768e-05\n","Epoch  24 Batch  148 / 228  Training Loss  1.4726184417668264e-05\n","Epoch  24 Batch  149 / 228  Training Loss  1.3055980161880143e-05\n","Epoch  24 Batch  150 / 228  Training Loss  1.6448486348963343e-05\n","Epoch  24 Batch  151 / 228  Training Loss  2.3714152121101506e-05\n","Epoch  24 Batch  152 / 228  Training Loss  8.152560440066736e-06\n","Epoch  24 Batch  153 / 228  Training Loss  1.0874116924242117e-05\n","Epoch  24 Batch  154 / 228  Training Loss  1.0028311407950241e-05\n","Epoch  24 Batch  155 / 228  Training Loss  1.866503407654818e-05\n","Epoch  24 Batch  156 / 228  Training Loss  1.1427888239268214e-05\n","Epoch  24 Batch  157 / 228  Training Loss  1.4555938832927495e-05\n","Epoch  24 Batch  158 / 228  Training Loss  1.9338931451784447e-05\n","Epoch  24 Batch  159 / 228  Training Loss  1.7813041267800145e-05\n","Epoch  24 Batch  160 / 228  Training Loss  2.184181721531786e-05\n","Epoch  24 Batch  161 / 228  Training Loss  1.675644307397306e-05\n","Epoch  24 Batch  162 / 228  Training Loss  1.720677755656652e-05\n","Epoch  24 Batch  163 / 228  Training Loss  1.1000242011505179e-05\n","Epoch  24 Batch  164 / 228  Training Loss  1.6813968613860197e-05\n","Epoch  24 Batch  165 / 228  Training Loss  1.7752447092789225e-05\n","Epoch  24 Batch  166 / 228  Training Loss  1.9545343093341216e-05\n","Epoch  24 Batch  167 / 228  Training Loss  1.5363435522885993e-05\n","Epoch  24 Batch  168 / 228  Training Loss  1.0723747436713893e-05\n","Epoch  24 Batch  169 / 228  Training Loss  1.3375664821069222e-05\n","Epoch  24 Batch  170 / 228  Training Loss  1.3839086932421196e-05\n","Epoch  24 Batch  171 / 228  Training Loss  1.2292250175960362e-05\n","Epoch  24 Batch  172 / 228  Training Loss  2.0038127331645228e-05\n","Epoch  24 Batch  173 / 228  Training Loss  1.840827098931186e-05\n","Epoch  24 Batch  174 / 228  Training Loss  1.4172083865560126e-05\n","Epoch  24 Batch  175 / 228  Training Loss  1.4311035556602292e-05\n","Epoch  24 Batch  176 / 228  Training Loss  1.2446091204765253e-05\n","Epoch  24 Batch  177 / 228  Training Loss  2.0830122593906708e-05\n","Epoch  24 Batch  178 / 228  Training Loss  2.164618854294531e-05\n","Epoch  24 Batch  179 / 228  Training Loss  1.333259297098266e-05\n","Epoch  24 Batch  180 / 228  Training Loss  1.6553542081965134e-05\n","Epoch  24 Batch  181 / 228  Training Loss  2.546833457017783e-05\n","Epoch  24 Batch  182 / 228  Training Loss  1.7276188373216428e-05\n","Epoch  24 Batch  183 / 228  Training Loss  1.175172656076029e-05\n","Epoch  24 Batch  184 / 228  Training Loss  1.4949939213693142e-05\n","Epoch  24 Batch  185 / 228  Training Loss  1.1043761332985014e-05\n","Epoch  24 Batch  186 / 228  Training Loss  1.1574486961762886e-05\n","Epoch  24 Batch  187 / 228  Training Loss  1.7100786863011308e-05\n","Epoch  24 Batch  188 / 228  Training Loss  1.2191074347356334e-05\n","Epoch  24 Batch  189 / 228  Training Loss  1.239895664184587e-05\n","Epoch  24 Batch  190 / 228  Training Loss  1.1352867659297772e-05\n","Epoch  24 Batch  191 / 228  Training Loss  1.2799240721506067e-05\n","Epoch  24 Batch  192 / 228  Training Loss  1.8326569261262193e-05\n","Epoch  24 Batch  193 / 228  Training Loss  1.2330907338764518e-05\n","Epoch  24 Batch  194 / 228  Training Loss  1.8679338609217666e-05\n","Epoch  24 Batch  195 / 228  Training Loss  1.7147893231594935e-05\n","Epoch  24 Batch  196 / 228  Training Loss  1.1759673725464381e-05\n","Epoch  24 Batch  197 / 228  Training Loss  2.1349647795432247e-05\n","Epoch  24 Batch  198 / 228  Training Loss  1.3007838788325898e-05\n","Epoch  24 Batch  199 / 228  Training Loss  1.9361450540600345e-05\n","Epoch  24 Batch  200 / 228  Training Loss  1.563136356708128e-05\n","Epoch  24 Batch  201 / 228  Training Loss  2.3488324586651288e-05\n","Epoch  24 Batch  202 / 228  Training Loss  1.5915235053398646e-05\n","Epoch  24 Batch  203 / 228  Training Loss  1.25449687402579e-05\n","Epoch  24 Batch  204 / 228  Training Loss  1.4081859262660146e-05\n","Epoch  24 Batch  205 / 228  Training Loss  1.8597818780108355e-05\n","Epoch  24 Batch  206 / 228  Training Loss  1.668166441959329e-05\n","Epoch  24 Batch  207 / 228  Training Loss  7.941789590404369e-06\n","Epoch  24 Batch  208 / 228  Training Loss  1.5226074538077228e-05\n","Epoch  24 Batch  209 / 228  Training Loss  1.7870272131403908e-05\n","Epoch  24 Batch  210 / 228  Training Loss  2.5861056201392785e-05\n","Epoch  24 Batch  211 / 228  Training Loss  1.483768573962152e-05\n","Epoch  24 Batch  212 / 228  Training Loss  1.2574208085425198e-05\n","Epoch  24 Batch  213 / 228  Training Loss  1.7608059351914562e-05\n","Epoch  24 Batch  214 / 228  Training Loss  1.9692652131197974e-05\n","Epoch  24 Batch  215 / 228  Training Loss  1.8823107893695123e-05\n","Epoch  24 Batch  216 / 228  Training Loss  1.3382002180151176e-05\n","Epoch  24 Batch  217 / 228  Training Loss  1.653520121180918e-05\n","Epoch  24 Batch  218 / 228  Training Loss  2.024760760832578e-05\n","Epoch  24 Batch  219 / 228  Training Loss  1.2732272807625122e-05\n","Epoch  24 Batch  220 / 228  Training Loss  1.8694005120778456e-05\n","Epoch  24 Batch  221 / 228  Training Loss  1.8370918041910045e-05\n","Epoch  24 Batch  222 / 228  Training Loss  1.8408034520689398e-05\n","Epoch  24 Batch  223 / 228  Training Loss  1.4097631719778292e-05\n","Epoch  24 Batch  224 / 228  Training Loss  1.0753414244391024e-05\n","Epoch  24 Batch  225 / 228  Training Loss  1.6861167750903405e-05\n","Epoch  24 Batch  226 / 228  Training Loss  1.6360520021407865e-05\n","Epoch  24 Batch  227 / 228  Training Loss  1.3751237929682247e-05\n","  25    |    -    |   0.000016   | 99.199695\n","----------------------------------------------------------------------\n","Running epoch: 25\n","Epoch  25 Batch  0 / 228  Training Loss  1.6437919839518145e-05\n","Epoch  25 Batch  1 / 228  Training Loss  2.1952169845462777e-05\n","Epoch  25 Batch  2 / 228  Training Loss  2.2225638531381264e-05\n","Epoch  25 Batch  3 / 228  Training Loss  1.0929774362011813e-05\n","Epoch  25 Batch  4 / 228  Training Loss  1.3153157851775177e-05\n","Epoch  25 Batch  5 / 228  Training Loss  1.9471153791528195e-05\n","Epoch  25 Batch  6 / 228  Training Loss  1.7059783203876577e-05\n","Epoch  25 Batch  7 / 228  Training Loss  1.765916749718599e-05\n","Epoch  25 Batch  8 / 228  Training Loss  1.446192072762642e-05\n","Epoch  25 Batch  9 / 228  Training Loss  1.6424257410108112e-05\n","Epoch  25 Batch  10 / 228  Training Loss  1.9708715626620688e-05\n","Epoch  25 Batch  11 / 228  Training Loss  1.0308622222510166e-05\n","Epoch  25 Batch  12 / 228  Training Loss  1.4399562132894062e-05\n","Epoch  25 Batch  13 / 228  Training Loss  1.4171955626807176e-05\n","Epoch  25 Batch  14 / 228  Training Loss  1.6307512851199135e-05\n","Epoch  25 Batch  15 / 228  Training Loss  1.9249002434662543e-05\n","Epoch  25 Batch  16 / 228  Training Loss  1.7310192561126314e-05\n","Epoch  25 Batch  17 / 228  Training Loss  1.2876956134277862e-05\n","Epoch  25 Batch  18 / 228  Training Loss  1.7618147467146628e-05\n","Epoch  25 Batch  19 / 228  Training Loss  1.125826111092465e-05\n","Epoch  25 Batch  20 / 228  Training Loss  1.149919171439251e-05\n","Epoch  25 Batch  21 / 228  Training Loss  1.389270164509071e-05\n","Epoch  25 Batch  22 / 228  Training Loss  1.3918082913733087e-05\n","Epoch  25 Batch  23 / 228  Training Loss  1.6191772374440916e-05\n","Epoch  25 Batch  24 / 228  Training Loss  1.661395072005689e-05\n","Epoch  25 Batch  25 / 228  Training Loss  1.421464367012959e-05\n","Epoch  25 Batch  26 / 228  Training Loss  1.4735427612322383e-05\n","Epoch  25 Batch  27 / 228  Training Loss  1.2771261026500724e-05\n","Epoch  25 Batch  28 / 228  Training Loss  1.4425191693590023e-05\n","Epoch  25 Batch  29 / 228  Training Loss  1.262533351109596e-05\n","Epoch  25 Batch  30 / 228  Training Loss  1.6083689843071625e-05\n","Epoch  25 Batch  31 / 228  Training Loss  1.3821612810716033e-05\n","Epoch  25 Batch  32 / 228  Training Loss  9.084402336156927e-06\n","Epoch  25 Batch  33 / 228  Training Loss  1.3243047760624904e-05\n","Epoch  25 Batch  34 / 228  Training Loss  1.712619632598944e-05\n","Epoch  25 Batch  35 / 228  Training Loss  1.5027861991256941e-05\n","Epoch  25 Batch  36 / 228  Training Loss  1.2251844964339398e-05\n","Epoch  25 Batch  37 / 228  Training Loss  1.4939226275600959e-05\n","Epoch  25 Batch  38 / 228  Training Loss  1.30011703731725e-05\n","Epoch  25 Batch  39 / 228  Training Loss  1.498106121289311e-05\n","Epoch  25 Batch  40 / 228  Training Loss  1.4643543181591667e-05\n","Epoch  25 Batch  41 / 228  Training Loss  1.632094972592313e-05\n","Epoch  25 Batch  42 / 228  Training Loss  8.962362699094228e-06\n","Epoch  25 Batch  43 / 228  Training Loss  1.3788072465104051e-05\n","Epoch  25 Batch  44 / 228  Training Loss  2.7335036065778695e-05\n","Epoch  25 Batch  45 / 228  Training Loss  1.228701694344636e-05\n","Epoch  25 Batch  46 / 228  Training Loss  1.3270340787130408e-05\n","Epoch  25 Batch  47 / 228  Training Loss  1.7729053070070222e-05\n","Epoch  25 Batch  48 / 228  Training Loss  1.7581938664079644e-05\n","Epoch  25 Batch  49 / 228  Training Loss  1.398908170813229e-05\n","Epoch  25 Batch  50 / 228  Training Loss  1.346635508525651e-05\n","Epoch  25 Batch  51 / 228  Training Loss  1.542340214655269e-05\n","Epoch  25 Batch  52 / 228  Training Loss  1.3570030205301009e-05\n","Epoch  25 Batch  53 / 228  Training Loss  8.831688319332898e-06\n","Epoch  25 Batch  54 / 228  Training Loss  8.828835234453436e-06\n","Epoch  25 Batch  55 / 228  Training Loss  1.3637656593346037e-05\n","Epoch  25 Batch  56 / 228  Training Loss  8.954983968578745e-06\n","Epoch  25 Batch  57 / 228  Training Loss  1.6260190022876486e-05\n","Epoch  25 Batch  58 / 228  Training Loss  1.4836963600828312e-05\n","Epoch  25 Batch  59 / 228  Training Loss  1.93991836567875e-05\n","Epoch  25 Batch  60 / 228  Training Loss  1.613311906112358e-05\n","Epoch  25 Batch  61 / 228  Training Loss  1.92443112609908e-05\n","Epoch  25 Batch  62 / 228  Training Loss  1.0789256521093193e-05\n","Epoch  25 Batch  63 / 228  Training Loss  1.1667731087072752e-05\n","Epoch  25 Batch  64 / 228  Training Loss  1.42290637086262e-05\n","Epoch  25 Batch  65 / 228  Training Loss  1.3037375538260676e-05\n","Epoch  25 Batch  66 / 228  Training Loss  1.8099690350936726e-05\n","Epoch  25 Batch  67 / 228  Training Loss  1.3107101040077396e-05\n","Epoch  25 Batch  68 / 228  Training Loss  1.3545522051572334e-05\n","Epoch  25 Batch  69 / 228  Training Loss  1.7781389033189043e-05\n","Epoch  25 Batch  70 / 228  Training Loss  1.054300901159877e-05\n","Epoch  25 Batch  71 / 228  Training Loss  1.6905540178413503e-05\n","Epoch  25 Batch  72 / 228  Training Loss  1.241058089362923e-05\n","Epoch  25 Batch  73 / 228  Training Loss  1.2147963388997596e-05\n","Epoch  25 Batch  74 / 228  Training Loss  1.5159950635279529e-05\n","Epoch  25 Batch  75 / 228  Training Loss  1.5731897292425856e-05\n","Epoch  25 Batch  76 / 228  Training Loss  1.8800268662744202e-05\n","Epoch  25 Batch  77 / 228  Training Loss  1.4288474631030113e-05\n","Epoch  25 Batch  78 / 228  Training Loss  1.7282405678997748e-05\n","Epoch  25 Batch  79 / 228  Training Loss  1.766629793564789e-05\n","Epoch  25 Batch  80 / 228  Training Loss  1.3209617463871837e-05\n","Epoch  25 Batch  81 / 228  Training Loss  1.4813890629739035e-05\n","Epoch  25 Batch  82 / 228  Training Loss  1.3848596609022934e-05\n","Epoch  25 Batch  83 / 228  Training Loss  1.3763436982117128e-05\n","Epoch  25 Batch  84 / 228  Training Loss  1.437407081539277e-05\n","Epoch  25 Batch  85 / 228  Training Loss  2.0971583580831066e-05\n","Epoch  25 Batch  86 / 228  Training Loss  1.4263896446209401e-05\n","Epoch  25 Batch  87 / 228  Training Loss  1.2411277566570789e-05\n","Epoch  25 Batch  88 / 228  Training Loss  1.6949950804701075e-05\n","Epoch  25 Batch  89 / 228  Training Loss  1.2633473488676827e-05\n","Epoch  25 Batch  90 / 228  Training Loss  1.4108493815001566e-05\n","Epoch  25 Batch  91 / 228  Training Loss  1.7983755242312327e-05\n","Epoch  25 Batch  92 / 228  Training Loss  1.3752401173405815e-05\n","Epoch  25 Batch  93 / 228  Training Loss  1.0703119187382981e-05\n","Epoch  25 Batch  94 / 228  Training Loss  2.310808122274466e-05\n","Epoch  25 Batch  95 / 228  Training Loss  1.2154771866335068e-05\n","Epoch  25 Batch  96 / 228  Training Loss  1.627821256988682e-05\n","Epoch  25 Batch  97 / 228  Training Loss  1.1941939192183781e-05\n","Epoch  25 Batch  98 / 228  Training Loss  1.574983798491303e-05\n","Epoch  25 Batch  99 / 228  Training Loss  1.2815145964850672e-05\n","Epoch  25 Batch  100 / 228  Training Loss  1.987345785892103e-05\n","Epoch  25 Batch  101 / 228  Training Loss  1.3415378816716839e-05\n","Epoch  25 Batch  102 / 228  Training Loss  1.0249597835354507e-05\n","Epoch  25 Batch  103 / 228  Training Loss  1.197765777760651e-05\n","Epoch  25 Batch  104 / 228  Training Loss  8.58713156048907e-06\n","Epoch  25 Batch  105 / 228  Training Loss  1.1960383744735736e-05\n","Epoch  25 Batch  106 / 228  Training Loss  1.5287771020666696e-05\n","Epoch  25 Batch  107 / 228  Training Loss  1.6286936443066224e-05\n","Epoch  25 Batch  108 / 228  Training Loss  1.5110407730389852e-05\n","Epoch  25 Batch  109 / 228  Training Loss  1.3652810594066978e-05\n","Epoch  25 Batch  110 / 228  Training Loss  1.7266136637772433e-05\n","Epoch  25 Batch  111 / 228  Training Loss  2.058077370747924e-05\n","Epoch  25 Batch  112 / 228  Training Loss  1.5123402590688784e-05\n","Epoch  25 Batch  113 / 228  Training Loss  1.0546432349656243e-05\n","Epoch  25 Batch  114 / 228  Training Loss  1.3026445230934769e-05\n","Epoch  25 Batch  115 / 228  Training Loss  1.222852552018594e-05\n","Epoch  25 Batch  116 / 228  Training Loss  1.746087582432665e-05\n","Epoch  25 Batch  117 / 228  Training Loss  1.079424600902712e-05\n","Epoch  25 Batch  118 / 228  Training Loss  1.3575018783740234e-05\n","Epoch  25 Batch  119 / 228  Training Loss  1.0052270226879045e-05\n","Epoch  25 Batch  120 / 228  Training Loss  2.0743409550050274e-05\n","Epoch  25 Batch  121 / 228  Training Loss  1.6020198017940857e-05\n","Epoch  25 Batch  122 / 228  Training Loss  9.682324161985889e-06\n","Epoch  25 Batch  123 / 228  Training Loss  1.2028622222715057e-05\n","Epoch  25 Batch  124 / 228  Training Loss  1.5266974514815956e-05\n","Epoch  25 Batch  125 / 228  Training Loss  1.4192111848387867e-05\n","Epoch  25 Batch  126 / 228  Training Loss  1.1933339919778518e-05\n","Epoch  25 Batch  127 / 228  Training Loss  1.526874621049501e-05\n","Epoch  25 Batch  128 / 228  Training Loss  2.133627094735857e-05\n","Epoch  25 Batch  129 / 228  Training Loss  1.2708173017017543e-05\n","Epoch  25 Batch  130 / 228  Training Loss  1.4464058040175587e-05\n","Epoch  25 Batch  131 / 228  Training Loss  1.2512644389062189e-05\n","Epoch  25 Batch  132 / 228  Training Loss  1.5373938367702067e-05\n","Epoch  25 Batch  133 / 228  Training Loss  1.3349717846722342e-05\n","Epoch  25 Batch  134 / 228  Training Loss  1.3528840099752415e-05\n","Epoch  25 Batch  135 / 228  Training Loss  1.7638323697610758e-05\n","Epoch  25 Batch  136 / 228  Training Loss  1.2669207535509486e-05\n","Epoch  25 Batch  137 / 228  Training Loss  1.9758186681428924e-05\n","Epoch  25 Batch  138 / 228  Training Loss  1.3661056982527953e-05\n","Epoch  25 Batch  139 / 228  Training Loss  1.808640081435442e-05\n","Epoch  25 Batch  140 / 228  Training Loss  1.1817026461358182e-05\n","Epoch  25 Batch  141 / 228  Training Loss  1.4569527593266685e-05\n","Epoch  25 Batch  142 / 228  Training Loss  2.0597022739821114e-05\n","Epoch  25 Batch  143 / 228  Training Loss  1.4855674635327887e-05\n","Epoch  25 Batch  144 / 228  Training Loss  1.777477518771775e-05\n","Epoch  25 Batch  145 / 228  Training Loss  1.5712710592197254e-05\n","Epoch  25 Batch  146 / 228  Training Loss  1.7464546544943005e-05\n","Epoch  25 Batch  147 / 228  Training Loss  1.4817333067185245e-05\n","Epoch  25 Batch  148 / 228  Training Loss  1.5400162737932988e-05\n","Epoch  25 Batch  149 / 228  Training Loss  1.2469690773286857e-05\n","Epoch  25 Batch  150 / 228  Training Loss  1.3773415048490278e-05\n","Epoch  25 Batch  151 / 228  Training Loss  1.6689202311681584e-05\n","Epoch  25 Batch  152 / 228  Training Loss  1.962502210517414e-05\n","Epoch  25 Batch  153 / 228  Training Loss  1.3557296369981486e-05\n","Epoch  25 Batch  154 / 228  Training Loss  1.8029331840807572e-05\n","Epoch  25 Batch  155 / 228  Training Loss  1.1565903150767554e-05\n","Epoch  25 Batch  156 / 228  Training Loss  1.132665784098208e-05\n","Epoch  25 Batch  157 / 228  Training Loss  2.0176801626803353e-05\n","Epoch  25 Batch  158 / 228  Training Loss  1.352968411083566e-05\n","Epoch  25 Batch  159 / 228  Training Loss  1.4733431271451991e-05\n","Epoch  25 Batch  160 / 228  Training Loss  1.7504318748251535e-05\n","Epoch  25 Batch  161 / 228  Training Loss  1.6921263522817753e-05\n","Epoch  25 Batch  162 / 228  Training Loss  1.7958705939236097e-05\n","Epoch  25 Batch  163 / 228  Training Loss  1.8211769202025607e-05\n","Epoch  25 Batch  164 / 228  Training Loss  1.582987533765845e-05\n","Epoch  25 Batch  165 / 228  Training Loss  1.5025315406091977e-05\n","Epoch  25 Batch  166 / 228  Training Loss  1.679297383816447e-05\n","Epoch  25 Batch  167 / 228  Training Loss  1.8601418560137972e-05\n","Epoch  25 Batch  168 / 228  Training Loss  1.1502440429467242e-05\n","Epoch  25 Batch  169 / 228  Training Loss  1.7227914213435724e-05\n","Epoch  25 Batch  170 / 228  Training Loss  1.3895745723857544e-05\n","Epoch  25 Batch  171 / 228  Training Loss  1.3429968021227978e-05\n","Epoch  25 Batch  172 / 228  Training Loss  2.07582488656044e-05\n","Epoch  25 Batch  173 / 228  Training Loss  1.6740703358664177e-05\n","Epoch  25 Batch  174 / 228  Training Loss  1.4445382475969382e-05\n","Epoch  25 Batch  175 / 228  Training Loss  1.5840803825994954e-05\n","Epoch  25 Batch  176 / 228  Training Loss  1.9723518562386744e-05\n","Epoch  25 Batch  177 / 228  Training Loss  1.204472118843114e-05\n","Epoch  25 Batch  178 / 228  Training Loss  1.4019821719557513e-05\n","Epoch  25 Batch  179 / 228  Training Loss  1.2934647202200722e-05\n","Epoch  25 Batch  180 / 228  Training Loss  1.7601289073354565e-05\n","Epoch  25 Batch  181 / 228  Training Loss  1.5273046301444992e-05\n","Epoch  25 Batch  182 / 228  Training Loss  1.4186455700837541e-05\n","Epoch  25 Batch  183 / 228  Training Loss  1.4098613974056207e-05\n","Epoch  25 Batch  184 / 228  Training Loss  1.9719342162716202e-05\n","Epoch  25 Batch  185 / 228  Training Loss  1.970008815987967e-05\n","Epoch  25 Batch  186 / 228  Training Loss  1.406942647008691e-05\n","Epoch  25 Batch  187 / 228  Training Loss  1.7402873709215783e-05\n","Epoch  25 Batch  188 / 228  Training Loss  1.1507645467645489e-05\n","Epoch  25 Batch  189 / 228  Training Loss  6.700297035422409e-06\n","Epoch  25 Batch  190 / 228  Training Loss  1.2560534742078744e-05\n","Epoch  25 Batch  191 / 228  Training Loss  1.4138143342279363e-05\n","Epoch  25 Batch  192 / 228  Training Loss  1.658305336604826e-05\n","Epoch  25 Batch  193 / 228  Training Loss  1.3066227438685019e-05\n","Epoch  25 Batch  194 / 228  Training Loss  1.6328067431459203e-05\n","Epoch  25 Batch  195 / 228  Training Loss  2.0535178919089958e-05\n","Epoch  25 Batch  196 / 228  Training Loss  1.5593101124977693e-05\n","Epoch  25 Batch  197 / 228  Training Loss  1.0526452570047695e-05\n","Epoch  25 Batch  198 / 228  Training Loss  8.041523869906086e-06\n","Epoch  25 Batch  199 / 228  Training Loss  1.2487333151511848e-05\n","Epoch  25 Batch  200 / 228  Training Loss  1.2481586963986047e-05\n","Epoch  25 Batch  201 / 228  Training Loss  1.3534673598769587e-05\n","Epoch  25 Batch  202 / 228  Training Loss  1.7175701941596344e-05\n","Epoch  25 Batch  203 / 228  Training Loss  1.4794340131629724e-05\n","Epoch  25 Batch  204 / 228  Training Loss  1.5823501598788425e-05\n","Epoch  25 Batch  205 / 228  Training Loss  1.3475955711328425e-05\n","Epoch  25 Batch  206 / 228  Training Loss  1.503514067735523e-05\n","Epoch  25 Batch  207 / 228  Training Loss  1.8498509234632365e-05\n","Epoch  25 Batch  208 / 228  Training Loss  1.3542225133278407e-05\n","Epoch  25 Batch  209 / 228  Training Loss  1.9112543668597937e-05\n","Epoch  25 Batch  210 / 228  Training Loss  1.4469301277131308e-05\n","Epoch  25 Batch  211 / 228  Training Loss  1.589082421560306e-05\n","Epoch  25 Batch  212 / 228  Training Loss  1.4393500350706745e-05\n","Epoch  25 Batch  213 / 228  Training Loss  2.002676592383068e-05\n","Epoch  25 Batch  214 / 228  Training Loss  1.281633558392059e-05\n","Epoch  25 Batch  215 / 228  Training Loss  1.2585034710355103e-05\n","Epoch  25 Batch  216 / 228  Training Loss  1.363714545732364e-05\n","Epoch  25 Batch  217 / 228  Training Loss  1.3139166185283102e-05\n","Epoch  25 Batch  218 / 228  Training Loss  2.414859591226559e-05\n","Epoch  25 Batch  219 / 228  Training Loss  1.1775860912166536e-05\n","Epoch  25 Batch  220 / 228  Training Loss  1.4856219422654249e-05\n","Epoch  25 Batch  221 / 228  Training Loss  1.3986755220685154e-05\n","Epoch  25 Batch  222 / 228  Training Loss  1.1874424671987072e-05\n","Epoch  25 Batch  223 / 228  Training Loss  1.6922542272368446e-05\n","Epoch  25 Batch  224 / 228  Training Loss  1.6907299141166732e-05\n","Epoch  25 Batch  225 / 228  Training Loss  1.3485388990375213e-05\n","Epoch  25 Batch  226 / 228  Training Loss  1.2790122127626091e-05\n","Epoch  25 Batch  227 / 228  Training Loss  1.0361896784161218e-05\n","  26    |    -    |   0.000015   | 99.275915\n","----------------------------------------------------------------------\n"]}],"source":["import gc\n","\n","val_acc = 0\n","train_accuracy = 0\n","\n","# Sets the module in training mode\n","model.train()\n","\n","for epoch in range(1,num_of_epochs+1):\n","  print('Running epoch: {}'.format(epoch))\n","  running_loss=0\n","  # out = display(progress(1, num_of_batches+1), display_id=True)\n","  i =0 \n","  for batch in train_dataloader:\n","    \n","    input_ids, attn_mask, labels = tuple(t.to(device) for t in batch)\n","\n","    # clear out the gradients of all Variables \n","    optimizer.zero_grad()\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    # Forward propogation\n","    # print(model(input_ids=input_ids, attention_mask=attn_mask, labels=labels))\n","    outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n","    \n","    loss = outputs.loss\n","    loss_num=loss.item()\n","    logits = outputs.logits\n","    running_loss+=loss_num\n","    # out.update(progress(loss_num,i, num_of_batches+1))\n","\n","    # calculating the gradients\n","    loss.backward()\n","\n","    # updating the params\n","    optimizer.step()\n","\n","    print(\"Epoch \", epoch, \"Batch \", i, \"/\", len(train_dataloader), \" Training Loss \", loss_num)\n","    i += 1\n","\n","  running_loss = running_loss/len(train_dataloader)\n","  # v_input_ids, v_attn_mask, v_labels = tuple(t.to(device) for t in data_valid)\n","  \n","  curr_accuracy = evaluate(model, val_dataloader)\n","\n","  # print('Epoch: {} , Running loss: {}'.format(epoch,running_loss))\n","  print(f\"{epoch + 1:^7} | {'-':^7} | {running_loss:^12.6f} | {curr_accuracy:^9.6f}\")\n","  print(\"-\"*70)\n","\n","  if curr_accuracy > val_acc:\n","    val_acc = curr_accuracy\n","    # Saving the best model\n","    torch.save(model.state_dict(),'Fine_tune_10based_model_v2_with_10per_ood.bin')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iTiNbM081LlD"},"outputs":[],"source":["# Changing the directory to store the model there.\n","# print(os.getcwd())\n","# os.chdir('/content/drive/My Drive/Colab Notebooks/new/')\n","# print(os.getcwd())"]},{"cell_type":"markdown","metadata":{"id":"c3bq66KD1eg7"},"source":["#### Saving the Model (creating checkpoint)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-fJcvYcG1nd3"},"outputs":[],"source":["# PATH = \"fine_tune_10e_25eph.pt\"\n","# torch.save({\n","#             'epoch': num_of_epochs,\n","#             'model_state_dict': model.state_dict(),\n","#             'optimizer_state_dict': optimizer.state_dict(),\n","#             'loss': running_loss,\n","#             }, PATH)"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":22790,"status":"ok","timestamp":1638664525050,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"AuyVxTpoGouA"},"outputs":[],"source":["model_load = T5ForConditionalGeneration.from_pretrained('Fine_tune_10based_model_v2_with_10per_ood.bin', return_dict=True, config='t5-base-config.json')"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":102668,"status":"ok","timestamp":1638664678909,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"ot6G4FJhBCG5","outputId":"f72f7365-e765-4482-99f7-4378e52c1119"},"outputs":[{"data":{"text/plain":["99.26658163265306"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["model_load.to(device)\n","evaluate(model_load, test_dataloader)"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":611,"status":"ok","timestamp":1638664758518,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"elq45fvZvxKR","outputId":"bd3fba0d-8809-4b21-dd59-db0014489d95"},"outputs":[{"data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["model_load.to('cpu')\n","# evaluate(model_load, test_dataloader)"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":801,"status":"ok","timestamp":1638664762743,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"vM-_2K5uyV6V","outputId":"fd5ae80d-cd60-4767-bd85-c3594450604d"},"outputs":[{"data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["model_load.to('cpu')"]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1638664764151,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"lWLx5DywGtHI"},"outputs":[],"source":["# Function to generate sentences from symptoms on the test dataset\n","def generateText(text):\n","  model_load.eval()\n","  input_ids = tokenizer.encode(text, return_tensors=\"pt\")  # Batch size 1\n","  # s = time.time()\n","  outputs = model_load.generate(input_ids)\n","  prediction=tokenizer.decode(outputs[0]).replace('<pad>','').replace('</s>','')\n","  return prediction"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1638664766019,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"H8ZwnGO_xplA","outputId":"920b07b3-0f80-419c-88fb-687df40f31b8"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["model_load.device"]},{"cell_type":"code","execution_count":62,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":580},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1638664767240,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"KllLHDqnJEUV","outputId":"f90d977f-35f7-4b34-bdad-520735da2ac7"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>inputs</th>\n","      <th>target</th>\n","      <th>maxlen</th>\n","      <th>minlen</th>\n","      <th>sumlen</th>\n","      <th>target_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The sum of 1 1000 2 100 7 10 9 and 1 1000 8 10...</td>\n","      <td>3 1000 1 100 6 10 9</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>3 1000 1 100 6 10 9</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The sum of 4 100 8 10 0 and 1 1000 8 100 1 10 ...</td>\n","      <td>2 1000 2 100 9 10 0</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>2 1000 2 100 9 10 0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The sum of 3 100 2 10 7 and 1 1000 2 100 1 10 ...</td>\n","      <td>1 1000 5 100 4 10 1</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1 1000 5 100 4 10 1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The sum of 1 1000 4 100 6 10 0 and 4 100 0 10 ...</td>\n","      <td>1 1000 8 100 6 10 8</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1 1000 8 100 6 10 8</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The sum of 1 1000 8 100 1 10 8 and 1 1000 1 10...</td>\n","      <td>2 1000 9 100 3 10 1</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2 1000 9 100 3 10 1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2595</th>\n","      <td>The sum of 2 10000 5 1000 4 100 3 10 6 and 1 1...</td>\n","      <td>4 10000 3 1000 4 100 0 10 8</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>4 10000 3 1000 4 100 0 10 8</td>\n","    </tr>\n","    <tr>\n","      <th>2596</th>\n","      <td>The sum of 2 100 6 10 3 and 5 100 6 10 6 is</td>\n","      <td>8 100 2 10 9</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>8 100 2 10 9</td>\n","    </tr>\n","    <tr>\n","      <th>2597</th>\n","      <td>The sum of 1 1000 9 100 8 10 7 and 1 100 9 10 ...</td>\n","      <td>2 1000 1 100 8 10 1</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>2 1000 1 100 8 10 1</td>\n","    </tr>\n","    <tr>\n","      <th>2598</th>\n","      <td>The sum of 1 1000 5 100 7 10 6 and 5 100 8 10 ...</td>\n","      <td>2 1000 1 100 5 10 6</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>2 1000 1 100 5 10 6</td>\n","    </tr>\n","    <tr>\n","      <th>2599</th>\n","      <td>The sum of 3 100 5 10 2 and 1 1000 4 100 3 10 ...</td>\n","      <td>1 1000 7 100 8 10 8</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1 1000 7 100 8 10 8</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2600 rows × 6 columns</p>\n","</div>"],"text/plain":["                                                 inputs  ...                   target_str\n","0     The sum of 1 1000 2 100 7 10 9 and 1 1000 8 10...  ...          3 1000 1 100 6 10 9\n","1     The sum of 4 100 8 10 0 and 1 1000 8 100 1 10 ...  ...          2 1000 2 100 9 10 0\n","2     The sum of 3 100 2 10 7 and 1 1000 2 100 1 10 ...  ...          1 1000 5 100 4 10 1\n","3     The sum of 1 1000 4 100 6 10 0 and 4 100 0 10 ...  ...          1 1000 8 100 6 10 8\n","4     The sum of 1 1000 8 100 1 10 8 and 1 1000 1 10...  ...          2 1000 9 100 3 10 1\n","...                                                 ...  ...                          ...\n","2595  The sum of 2 10000 5 1000 4 100 3 10 6 and 1 1...  ...  4 10000 3 1000 4 100 0 10 8\n","2596       The sum of 2 100 6 10 3 and 5 100 6 10 6 is   ...                 8 100 2 10 9\n","2597  The sum of 1 1000 9 100 8 10 7 and 1 100 9 10 ...  ...          2 1000 1 100 8 10 1\n","2598  The sum of 1 1000 5 100 7 10 6 and 5 100 8 10 ...  ...          2 1000 1 100 5 10 6\n","2599  The sum of 3 100 5 10 2 and 1 1000 4 100 3 10 ...  ...          1 1000 7 100 8 10 8\n","\n","[2600 rows x 6 columns]"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["data_valid\n","# testing on this for now"]},{"cell_type":"code","execution_count":63,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":1176,"status":"ok","timestamp":1638664770768,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"mMFAn0nSGy2y","outputId":"d3c7f2a3-0d93-46ef-fa5d-7dbb79e45e60"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' 3 1000 1 100 6 10 9'"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["generateText(data_valid.inputs[0])"]},{"cell_type":"code","execution_count":106,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":411,"status":"ok","timestamp":1638665001263,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"f5HnhuzcrudM","outputId":"1962aa4e-6e3c-459a-a2c9-9882a764adc7"},"outputs":[{"name":"stdout","output_type":"stream","text":["                                              inputs  ... sumlen\n","0  The sum of 1 10000 8 1000 9 100 7 10 6 and 1 1...  ...      5\n","1  The sum of 2 10000 8 1000 6 100 2 10 6 and 1 1...  ...      5\n","2  The sum of 2 10000 6 1000 2 100 7 10 8 and 2 1...  ...      5\n","3  The sum of 1 10000 8 1000 7 100 2 10 3 and 1 1...  ...      5\n","4  The sum of 2 10000 9 1000 4 100 3 10 6 and 3 1...  ...      5\n","\n","[5 rows x 5 columns] (15000, 5)\n"]}],"source":["# data_ood = pd.read_csv('data_10_ood_full.csv')\n","print(data_ood.head(5), data_ood.shape)"]},{"cell_type":"code","execution_count":107,"metadata":{"executionInfo":{"elapsed":240,"status":"ok","timestamp":1638665092449,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"5Pq1QnIxC7ax"},"outputs":[],"source":["data_ood = data_ood.sample(n = 6000, random_state = 42).reset_index(drop=True)"]},{"cell_type":"code","execution_count":108,"metadata":{"executionInfo":{"elapsed":803,"status":"ok","timestamp":1638665099974,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"blh9aBAb3Zq0"},"outputs":[],"source":["data_ood['type'] = \"OOD\""]},{"cell_type":"code","execution_count":109,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1638665105058,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"g6POXTzo3mEl"},"outputs":[],"source":["data_test['type'] = 'iid'\n","data_ood['target_str'] = data_ood['target'].astype(str)"]},{"cell_type":"code","execution_count":74,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1638664889984,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"x53-zStY3rVU"},"outputs":[],"source":["data_full = data_ood + data_test\n"]},{"cell_type":"code","execution_count":110,"metadata":{"executionInfo":{"elapsed":300,"status":"ok","timestamp":1638665111499,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"_jzqgm7k3ysN"},"outputs":[],"source":["bigdata = data_test.append(data_ood, ignore_index=True)"]},{"cell_type":"code","execution_count":111,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":597},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1638665113030,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"SACmT5aL4CuX","outputId":"30e35651-9a74-47ec-b4a1-1ae2d2abb5fa"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>inputs</th>\n","      <th>target</th>\n","      <th>maxlen</th>\n","      <th>minlen</th>\n","      <th>sumlen</th>\n","      <th>target_str</th>\n","      <th>type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The sum of 1 1000 3 100 4 10 5 and 1 1000 2 10...</td>\n","      <td>2 1000 6 100 0 10 6</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2 1000 6 100 0 10 6</td>\n","      <td>iid</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The sum of 1 1000 5 100 2 10 0 and 1 1000 5 10...</td>\n","      <td>3 1000 1 100 0 10 7</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>3 1000 1 100 0 10 7</td>\n","      <td>iid</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The sum of 1 10 2 and 1 1000 5 100 2 10 3 is</td>\n","      <td>1 1000 5 100 3 10 5</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1 1000 5 100 3 10 5</td>\n","      <td>iid</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The sum of 3 100 3 10 7 and 4 100 1 10 3 is</td>\n","      <td>7 100 5 10 0</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>7 100 5 10 0</td>\n","      <td>iid</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The sum of 2 100 5 10 2 and 1 1000 1 100 2 10 ...</td>\n","      <td>1 1000 3 100 8 10 0</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1 1000 3 100 8 10 0</td>\n","      <td>iid</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9115</th>\n","      <td>The sum of 2 10000 9 1000 5 100 6 10 5 and 2 1...</td>\n","      <td>5 10000 0 1000 9 100 8 10 7</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5 10000 0 1000 9 100 8 10 7</td>\n","      <td>OOD</td>\n","    </tr>\n","    <tr>\n","      <th>9116</th>\n","      <td>The sum of 3 10000 4 1000 8 100 0 10 0 and 3 1...</td>\n","      <td>6 10000 5 1000 1 100 2 10 3</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>6 10000 5 1000 1 100 2 10 3</td>\n","      <td>OOD</td>\n","    </tr>\n","    <tr>\n","      <th>9117</th>\n","      <td>The sum of 3 10000 4 1000 3 100 4 10 1 and 1 1...</td>\n","      <td>5 10000 0 1000 4 100 0 10 4</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5 10000 0 1000 4 100 0 10 4</td>\n","      <td>OOD</td>\n","    </tr>\n","    <tr>\n","      <th>9118</th>\n","      <td>The sum of 2 10000 8 1000 0 100 7 10 6 and 3 1...</td>\n","      <td>6 10000 4 1000 6 100 0 10 2</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>6 10000 4 1000 6 100 0 10 2</td>\n","      <td>OOD</td>\n","    </tr>\n","    <tr>\n","      <th>9119</th>\n","      <td>The sum of 1 10000 6 1000 7 100 1 10 5 and 2 1...</td>\n","      <td>4 10000 6 1000 1 100 8 10 2</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>5</td>\n","      <td>4 10000 6 1000 1 100 8 10 2</td>\n","      <td>OOD</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9120 rows × 7 columns</p>\n","</div>"],"text/plain":["                                                 inputs  ... type\n","0     The sum of 1 1000 3 100 4 10 5 and 1 1000 2 10...  ...  iid\n","1     The sum of 1 1000 5 100 2 10 0 and 1 1000 5 10...  ...  iid\n","2         The sum of 1 10 2 and 1 1000 5 100 2 10 3 is   ...  iid\n","3          The sum of 3 100 3 10 7 and 4 100 1 10 3 is   ...  iid\n","4     The sum of 2 100 5 10 2 and 1 1000 1 100 2 10 ...  ...  iid\n","...                                                 ...  ...  ...\n","9115  The sum of 2 10000 9 1000 5 100 6 10 5 and 2 1...  ...  OOD\n","9116  The sum of 3 10000 4 1000 8 100 0 10 0 and 3 1...  ...  OOD\n","9117  The sum of 3 10000 4 1000 3 100 4 10 1 and 1 1...  ...  OOD\n","9118  The sum of 2 10000 8 1000 0 100 7 10 6 and 3 1...  ...  OOD\n","9119  The sum of 1 10000 6 1000 7 100 1 10 5 and 2 1...  ...  OOD\n","\n","[9120 rows x 7 columns]"]},"execution_count":111,"metadata":{},"output_type":"execute_result"}],"source":["bigdata"]},{"cell_type":"code","execution_count":112,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"elapsed":374,"status":"ok","timestamp":1638665120761,"user":{"displayName":"Saicharan Papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghem6OtR1-7aztJ-B-dfSHPBAkn2487OmV7lBDV=s64","userId":"05517061699814178607"},"user_tz":420},"id":"qj7E2DPhfwpf","outputId":"fe754cc4-fbc8-4000-f821-92ab190cd150"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>maxlen</th>\n","      <th>minlen</th>\n","      <th>sumlen</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>9120.000000</td>\n","      <td>9120.000000</td>\n","      <td>9120.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>4.612061</td>\n","      <td>4.423794</td>\n","      <td>4.653070</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.628793</td>\n","      <td>0.921234</td>\n","      <td>0.549583</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>2.000000</td>\n","      <td>1.000000</td>\n","      <td>2.000000</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","      <td>4.000000</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>5.000000</td>\n","      <td>5.000000</td>\n","      <td>5.000000</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>5.000000</td>\n","      <td>5.000000</td>\n","      <td>5.000000</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>5.000000</td>\n","      <td>5.000000</td>\n","      <td>5.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["            maxlen       minlen       sumlen\n","count  9120.000000  9120.000000  9120.000000\n","mean      4.612061     4.423794     4.653070\n","std       0.628793     0.921234     0.549583\n","min       2.000000     1.000000     2.000000\n","25%       4.000000     4.000000     4.000000\n","50%       5.000000     5.000000     5.000000\n","75%       5.000000     5.000000     5.000000\n","max       5.000000     5.000000     5.000000"]},"execution_count":112,"metadata":{},"output_type":"execute_result"}],"source":["bigdata.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbXnMdzJ5PjT"},"outputs":[],"source":["bigdata['predictions'] = bigdata.apply(lambda x: generateText(x['inputs']), axis=1)\n","bigdata.to_csv('results_prediction.csv', index = False, header=None)"]},{"cell_type":"markdown","metadata":{"id":"kp5fCMtn6ave"},"source":["Don't execute from below."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":528},"executionInfo":{"elapsed":420,"status":"ok","timestamp":1638624484149,"user":{"displayName":"Saicharan papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYb-KV99mWggCvT5i1QCvUAzk9FDecIkkSg4fpqQ=s64","userId":"10631477352058960402"},"user_tz":420},"id":"fxX7Javx6Zoj","outputId":"98485e02-f225-4988-b436-5cddb3abf6fc"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>inputs</th>\n","      <th>target</th>\n","      <th>maxlen</th>\n","      <th>minlen</th>\n","      <th>sumlen</th>\n","      <th>target_str</th>\n","      <th>type</th>\n","      <th>predictions</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The sum of 1 1000 3 100 4 10 5 and 1 1000 2 10...</td>\n","      <td>2 1000 6 100 0 10 6</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>2 1000 6 100 0 10 6</td>\n","      <td>iid</td>\n","      <td>2 1000 6 100 0 10 6</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The sum of 1 1000 5 100 2 10 0 and 1 1000 5 10...</td>\n","      <td>3 1000 1 100 0 10 7</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>4</td>\n","      <td>3 1000 1 100 0 10 7</td>\n","      <td>iid</td>\n","      <td>3 1000 1 100 0 10 7</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The sum of 1 10 2 and 1 1000 5 100 2 10 3 is</td>\n","      <td>1 1000 5 100 3 10 5</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>4</td>\n","      <td>1 1000 5 100 3 10 5</td>\n","      <td>iid</td>\n","      <td>1 1000 5 100 3 10 5</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The sum of 3 100 3 10 7 and 4 100 1 10 3 is</td>\n","      <td>7 100 5 10 0</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>7 100 5 10 0</td>\n","      <td>iid</td>\n","      <td>7 100 5 10 0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The sum of 2 100 5 10 2 and 1 1000 1 100 2 10 ...</td>\n","      <td>1 1000 3 100 8 10 0</td>\n","      <td>4</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1 1000 3 100 8 10 0</td>\n","      <td>iid</td>\n","      <td>1 1000 3 100 8 10 0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>11067</th>\n","      <td>The sum of 5 100 8 10 1 and 5 100 1 10 1 is</td>\n","      <td>1 1000 0 100 9 10 2</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1 1000 0 100 9 10 2</td>\n","      <td>OOD</td>\n","      <td>1 1000 0 100 9 10 2</td>\n","    </tr>\n","    <tr>\n","      <th>11068</th>\n","      <td>The sum of 1 100 9 10 5 and 6 100 7 10 3 is</td>\n","      <td>8 100 6 10 8</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>8 100 6 10 8</td>\n","      <td>OOD</td>\n","      <td>8 100 6 10 8</td>\n","    </tr>\n","    <tr>\n","      <th>11069</th>\n","      <td>The sum of 1 100 8 10 4 and 2 100 3 10 6 is</td>\n","      <td>4 100 2 10 0</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>4 100 2 10 0</td>\n","      <td>OOD</td>\n","      <td>4 100 2 10 0</td>\n","    </tr>\n","    <tr>\n","      <th>11070</th>\n","      <td>The sum of 3 100 4 10 2 and 9 100 0 10 7 is</td>\n","      <td>1 1000 2 100 4 10 9</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>1 1000 2 100 4 10 9</td>\n","      <td>OOD</td>\n","      <td>1 1000 2 100 4 10 9</td>\n","    </tr>\n","    <tr>\n","      <th>11071</th>\n","      <td>The sum of 3 100 4 10 0 and 3 100 2 10 7 is</td>\n","      <td>6 100 6 10 7</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>3</td>\n","      <td>6 100 6 10 7</td>\n","      <td>OOD</td>\n","      <td>6 100 6 10 7</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>11072 rows × 8 columns</p>\n","</div>"],"text/plain":["                                                  inputs  ...           predictions\n","0      The sum of 1 1000 3 100 4 10 5 and 1 1000 2 10...  ...   2 1000 6 100 0 10 6\n","1      The sum of 1 1000 5 100 2 10 0 and 1 1000 5 10...  ...   3 1000 1 100 0 10 7\n","2          The sum of 1 10 2 and 1 1000 5 100 2 10 3 is   ...   1 1000 5 100 3 10 5\n","3           The sum of 3 100 3 10 7 and 4 100 1 10 3 is   ...          7 100 5 10 0\n","4      The sum of 2 100 5 10 2 and 1 1000 1 100 2 10 ...  ...   1 1000 3 100 8 10 0\n","...                                                  ...  ...                   ...\n","11067       The sum of 5 100 8 10 1 and 5 100 1 10 1 is   ...   1 1000 0 100 9 10 2\n","11068       The sum of 1 100 9 10 5 and 6 100 7 10 3 is   ...          8 100 6 10 8\n","11069       The sum of 1 100 8 10 4 and 2 100 3 10 6 is   ...          4 100 2 10 0\n","11070       The sum of 3 100 4 10 2 and 9 100 0 10 7 is   ...   1 1000 2 100 4 10 9\n","11071       The sum of 3 100 4 10 0 and 3 100 2 10 7 is   ...          6 100 6 10 7\n","\n","[11072 rows x 8 columns]"]},"execution_count":114,"metadata":{},"output_type":"execute_result"}],"source":["bigdata"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":395,"status":"ok","timestamp":1638624831280,"user":{"displayName":"Saicharan papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYb-KV99mWggCvT5i1QCvUAzk9FDecIkkSg4fpqQ=s64","userId":"10631477352058960402"},"user_tz":420},"id":"pT50mdfFoepk","outputId":"9f474419-98c9-4d3c-8f67-00b4244855b4"},"outputs":[{"name":"stdout","output_type":"stream","text":["57 11015\n"]}],"source":["lest = []\n","same_ = []\n","for i, j, isood in zip(bigdata['target_str'], bigdata['predictions'], bigdata['type']):\n","  if i.strip() != j.strip():\n","    lest.append((i, j, isood))\n","  else:\n","    same_.append((i, j, isood))\n","print(len(lest), len(same_))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306,"status":"ok","timestamp":1638624839014,"user":{"displayName":"Saicharan papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYb-KV99mWggCvT5i1QCvUAzk9FDecIkkSg4fpqQ=s64","userId":"10631477352058960402"},"user_tz":420},"id":"yGlB3l6rprOV","outputId":"73450709-b3b6-479f-8a82-6d98aa1cb8e7"},"outputs":[{"data":{"text/plain":["[('1 1000 1 100 9 10 9', ' 1 1000 2 100 9 10 9', 'iid'),\n"," ('5 100 0 10 0', ' 4 100 0 10 0', 'iid'),\n"," ('1 1000 9 100 2 10 9', ' 1 1000 9 100 3 10 9', 'iid'),\n"," ('8 100 9 10 8', ' 9 100 9 10 8', 'iid'),\n"," ('2 1000 3 100 9 10 9', ' 2 1000 4 100 9 10 9', 'iid'),\n"," ('1 1000 8 100 5 10 7', ' 1 1000 8 100 5 10 8', 'iid'),\n"," ('8 100 9 10 1', ' 9 100 9 10 1', 'OOD'),\n"," ('3 100 0 10 1', ' 2 100 0 10 1', 'OOD'),\n"," ('5 10 6', ' 6 10 6', 'OOD'),\n"," ('8 100 9 10 9', ' 9 100 9 10 9', 'OOD'),\n"," ('9 100 9 10 5', ' 1 1000 0 100 9 10 5', 'OOD'),\n"," ('1 100 4 10 5', ' 1 1000 4 10 5', 'OOD'),\n"," ('6 100 0 10 7', ' 5 100 0 10 7', 'OOD'),\n"," ('8 10 7', ' 9 10 7', 'OOD'),\n"," ('3 100 0 10 1', ' 2 100 0 10 1', 'OOD'),\n"," ('4 100 0 10 0', ' 3 100 0 10 0', 'OOD'),\n"," ('8 10 9', ' 9 10 9', 'OOD'),\n"," ('2 100 1 10 3', ' 2 100 0 10 3', 'OOD'),\n"," ('8 100 9 10 7', ' 9 100 9 10 7', 'OOD'),\n"," ('8 100 9 10 8', ' 9 100 9 10 8', 'OOD'),\n"," ('5 10 7', ' 6 10 7', 'OOD'),\n"," ('1 100 2 10 2', ' 1 1000 2 10 2', 'OOD'),\n"," ('8 100 9 10 9', ' 9 100 9 10 9', 'OOD'),\n"," ('7 100 9 10 7', ' 8 100 9 10 7', 'OOD'),\n"," ('1 10 2', ' 1 and 2', 'OOD'),\n"," ('3 100 0 10 0', ' 2 100 0 10 0', 'OOD'),\n"," ('3 100 2 10 0', ' 3 100 1 10 0', 'OOD'),\n"," ('3 100 0 10 5', ' 2 100 0 10 5', 'OOD'),\n"," ('3 100 0 10 9', ' 2 100 0 10 9', 'OOD'),\n"," ('9 100 0 10 1', ' 8 100 0 10 1', 'OOD'),\n"," ('8 100 4 10 9', ' 8 100 5 10 9', 'OOD'),\n"," ('6 100 9 10 8', ' 7 100 9 10 8', 'OOD'),\n"," ('8 100 7 10 5', ' 8 100 7 10 7', 'OOD'),\n"," ('1 1000 0 100 0 10 9', ' 1 1000 0 100 1 10 9', 'OOD'),\n"," ('1 100 2 10 2', ' 1 1000 2 10 2', 'OOD'),\n"," ('4 100 1 10 1', ' 4 100 0 10 1', 'OOD'),\n"," ('8 100 1 10 1', ' 8 100 0 10 1', 'OOD'),\n"," ('8 100 0 10 1', ' 7 100 0 10 1', 'OOD'),\n"," ('5 100 0 10 5', ' 4 100 0 10 5', 'OOD'),\n"," ('2 100 1 10 2', ' 2 100 0 10 2', 'OOD'),\n"," ('6 100 6 10 2', ' 6 100 6 10 4', 'OOD'),\n"," ('7 100 1 10 3', ' 7 100 0 10 3', 'OOD'),\n"," ('7 100 9 10 7', ' 8 100 9 10 7', 'OOD'),\n"," ('1 100 1 10 4', ' 1 1000 1 10 4', 'OOD'),\n"," ('3 100 8 10 5', ' 3 100 8 10 8', 'OOD'),\n"," ('2 10 4', ' 1 10 4', 'OOD'),\n"," ('2 10 0', ' 1 10 0', 'OOD'),\n"," ('8 100 9 10 5', ' 9 100 9 10 5', 'OOD'),\n"," ('1 1000 0 100 1 10 0', ' 1 1000 1 100 1 10 0', 'OOD'),\n"," ('8 100 9 10 1', ' 9 100 9 10 1', 'OOD'),\n"," ('5 100 7 10 1', ' 6 100 7 10 1', 'OOD'),\n"," ('7 100 6 10 0', ' 7 100 5 10 0', 'OOD'),\n"," ('5 100 9 10 4', ' 6 100 9 10 4', 'OOD'),\n"," ('3 100 0 10 4', ' 2 100 0 10 4', 'OOD'),\n"," ('6 100 0 10 0', ' 5 100 0 10 0', 'OOD'),\n"," ('5 100 9 10 9', ' 6 100 9 10 9', 'OOD'),\n"," ('7 100 1 10 0', ' 7 100 0 10 1', 'OOD')]"]},"execution_count":121,"metadata":{},"output_type":"execute_result"}],"source":["lest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-qnPQqYmxBz"},"outputs":[],"source":["def convert_to_10ebased(number: str, split_type: str, invert_number: bool) -> str:\n","    signal = None\n","    if number[0] == '-':\n","        signal = '-'\n","        number = number[1:]\n","\n","    output = []\n","    for i, digit in enumerate(number[::-1]):\n","        if split_type is None:\n","            output.append('10e' + str(i))\n","        elif split_type == 'underscore':\n","            output.append('10e' + '_'.join(str(i)))\n","        elif split_type == 'character':\n","            output.append(' '.join('D' + str(i) + 'E'))\n","        else:\n","            raise Exception(f'Wrong split_type: {split_type}')\n","        output.append(digit)\n","\n","    if signal:\n","        output.append(signal)\n","\n","    # The output is already inverted. If we want it to _not_ be inverted, then we invert it.\n","    if not invert_number:\n","        output = output[::-1]\n","\n","    return ' '.join(output)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DkPBZikmluAT"},"outputs":[],"source":["def convert_to_10based(number: str, invert_number: bool) -> str:\n","    signal = None\n","    if number[0] == '-':\n","        signal = '-'\n","        number = number[1:]\n","\n","    output = []\n","    for i, digit in enumerate(number[::-1]):\n","        if i > 0:\n","            output.append('1' + i * '0')\n","        output.append(digit)\n","\n","    if signal:\n","        output.append(signal)\n","\n","    # The output is already inverted. If we want it to not be inverted, then we invert it.\n","    if not invert_number:\n","        output = output[::-1]\n","\n","    return ' '.join(output)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":319,"status":"ok","timestamp":1638606922241,"user":{"displayName":"Saicharan papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYb-KV99mWggCvT5i1QCvUAzk9FDecIkkSg4fpqQ=s64","userId":"10631477352058960402"},"user_tz":420},"id":"LSKVJpXRrqj3","outputId":"e918f7d4-f24c-474a-9525-196742aff430"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'9 10e2 3 10e1 2 10e0'"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["convert_to_10ebased(\n","                \"932\", split_type=None, invert_number=False)"]},{"cell_type":"markdown","metadata":{"id":"QdFMBWGPYcLx"},"source":["Testing with OOD"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"elapsed":504,"status":"ok","timestamp":1637798362351,"user":{"displayName":"Saicharan papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYb-KV99mWggCvT5i1QCvUAzk9FDecIkkSg4fpqQ=s64","userId":"10631477352058960402"},"user_tz":420},"id":"n1aW3PfjYmf9","outputId":"4322b8e8-a108-49a5-e1ae-186ee0c362e9"},"outputs":[{"data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7fa662e2aa90>"]},"execution_count":40,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["seq_len = [len(i.split()) for i in data_ood['inputs']]\n","\n","pd.Series(seq_len).hist(bins = 30)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2959,"status":"ok","timestamp":1637798422994,"user":{"displayName":"Saicharan papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYb-KV99mWggCvT5i1QCvUAzk9FDecIkkSg4fpqQ=s64","userId":"10631477352058960402"},"user_tz":420},"id":"NnqaqPnAY7pC","outputId":"0be258c4-c95b-44bd-b5bf-101ad329ac35"},"outputs":[{"data":{"text/plain":["46"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["token_lens_ood = []\n","\n","for txt in data_ood.inputs:\n","  # doubt\n","  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n","  token_lens_ood.append(len(tokens))\n","\n","max(token_lens_ood)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2122,"status":"ok","timestamp":1637798475822,"user":{"displayName":"Saicharan papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYb-KV99mWggCvT5i1QCvUAzk9FDecIkkSg4fpqQ=s64","userId":"10631477352058960402"},"user_tz":420},"id":"ALXua3mtZJ2B","outputId":"211b44f7-b5ab-4267-de5f-1328b457877e"},"outputs":[{"data":{"text/plain":["24"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["token_lens_target_ood = []\n","\n","for txt in data_ood.target:\n","  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n","  token_lens_target_ood.append(len(tokens))\n","\n","max(token_lens_target_ood)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6034,"status":"ok","timestamp":1637798832446,"user":{"displayName":"Saicharan papani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYb-KV99mWggCvT5i1QCvUAzk9FDecIkkSg4fpqQ=s64","userId":"10631477352058960402"},"user_tz":420},"id":"0ZyxTIlsZW2v","outputId":"59a5297f-74a3-4223-f041-7e0433416747"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}],"source":["ood_inputs, ood_masks = get_word_embeddings(data_ood['inputs'], 55)\n","data_ood['target_str'] = data_ood['target'].astype(str)\n","ood_labels = get_word_embeddings(data_ood['target_str'], 30)[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fd0mq9gLaBPF"},"outputs":[],"source":["ood_data = TensorDataset(ood_inputs, ood_masks, ood_labels)\n","ood_dataloader = DataLoader(ood_data, shuffle = True, batch_size = batch_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MlpL5uElayw6"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"NLP_Project_model_finetune_10_with10_OOD.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
