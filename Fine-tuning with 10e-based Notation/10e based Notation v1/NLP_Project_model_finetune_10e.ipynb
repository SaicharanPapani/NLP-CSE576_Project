{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Project_model_finetune_10e.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d96da2b43a94dc69bb6d0f6179e38c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fe7879d8700b48baac01f083bc6c1c85",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c32835fa8c3c462bb44be6eccf37e519",
              "IPY_MODEL_7e8917d0a4314788b7ec0453ee162885",
              "IPY_MODEL_168a07d77b6848cfad3d3acb76a1605f"
            ]
          }
        },
        "fe7879d8700b48baac01f083bc6c1c85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c32835fa8c3c462bb44be6eccf37e519": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_68fd34cd5ae54b8fa9c12e979f9c0eee",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_52a39ecb0cbd4d46941cf0787ba3e9ba"
          }
        },
        "7e8917d0a4314788b7ec0453ee162885": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e6a0d0c08fc5436f8f60f19ae3c27088",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 791656,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 791656,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3db033e672c64ece9df7e7c598bd118f"
          }
        },
        "168a07d77b6848cfad3d3acb76a1605f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_32e21acc68e3457289963e14e73d1db5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 773k/773k [00:00&lt;00:00, 910kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_38dc896845d941fd81c06a2d3dd20c36"
          }
        },
        "68fd34cd5ae54b8fa9c12e979f9c0eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "52a39ecb0cbd4d46941cf0787ba3e9ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e6a0d0c08fc5436f8f60f19ae3c27088": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3db033e672c64ece9df7e7c598bd118f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "32e21acc68e3457289963e14e73d1db5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "38dc896845d941fd81c06a2d3dd20c36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c91dca00a59f45f3a25cdedabe0394b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d89f004bbfd949f4b1f44883f1c7a9bd",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_32fea6661541400aa9dc868ef3331455",
              "IPY_MODEL_bc7c1fe8ba6b4707be1468909f97593d",
              "IPY_MODEL_954b43b377854e2a9c423f88aecf89ba"
            ]
          }
        },
        "d89f004bbfd949f4b1f44883f1c7a9bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "32fea6661541400aa9dc868ef3331455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a64d262edc82482b928fdf989e8c090d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_897e881eaf2f472fb74aa45ec2628a9e"
          }
        },
        "bc7c1fe8ba6b4707be1468909f97593d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_fc22a2a3df1041799aa0ffc149527693",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1389353,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1389353,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a8f6df85d333461f908c39541827bde9"
          }
        },
        "954b43b377854e2a9c423f88aecf89ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b423a5ac962d452a82bd5331130252e5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.32M/1.32M [00:00&lt;00:00, 1.42MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7945e75577924d55a0bc652099872197"
          }
        },
        "a64d262edc82482b928fdf989e8c090d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "897e881eaf2f472fb74aa45ec2628a9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fc22a2a3df1041799aa0ffc149527693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a8f6df85d333461f908c39541827bde9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b423a5ac962d452a82bd5331130252e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7945e75577924d55a0bc652099872197": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "34e4b0c2261f4fa3809c21608bb4e3b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a66fb10210b348cc99bd48f1cb37b1e0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f2762e77c49741198334e23ab2918182",
              "IPY_MODEL_9144735e605041b5ad4265c4a997a6ef",
              "IPY_MODEL_876986d0ea7a49c19e3e540ad84f3214"
            ]
          }
        },
        "a66fb10210b348cc99bd48f1cb37b1e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f2762e77c49741198334e23ab2918182": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_181b6b4e93c24c14be7d672764f53ff2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_fdde1cea1e334520831da9a02aa90b8f"
          }
        },
        "9144735e605041b5ad4265c4a997a6ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4e13938665054040a2bfc669821342fd",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1199,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1199,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c0ec7862956f4ec2a3764593166aefe2"
          }
        },
        "876986d0ea7a49c19e3e540ad84f3214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d0cf38c8f2204476be485bc85501da61",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.17k/1.17k [00:00&lt;00:00, 28.6kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_948a037040ff4f6bb136a5d922602ace"
          }
        },
        "181b6b4e93c24c14be7d672764f53ff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "fdde1cea1e334520831da9a02aa90b8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4e13938665054040a2bfc669821342fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c0ec7862956f4ec2a3764593166aefe2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d0cf38c8f2204476be485bc85501da61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "948a037040ff4f6bb136a5d922602ace": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb15342f599a49c59a09f35b97bc8031": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_3549d39fb2bc46a3be9ae30d0032973a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_40e2c93774f24c01b2a2d4f1a7c7f95f",
              "IPY_MODEL_2cf3a7a1d04940a1a7cee968d1083edc",
              "IPY_MODEL_2d0b42573e3d4d0c9905421474dc1c08"
            ]
          }
        },
        "3549d39fb2bc46a3be9ae30d0032973a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "40e2c93774f24c01b2a2d4f1a7c7f95f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_db479e9e213942d99aa92962c676aa09",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b6d51b5bb0af4d55b89bf257452499aa"
          }
        },
        "2cf3a7a1d04940a1a7cee968d1083edc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4d1b83e98f3a48b388b6525ed94d0597",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 891691430,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 891691430,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_532a66e3bff442b5a71f87b0f0335f01"
          }
        },
        "2d0b42573e3d4d0c9905421474dc1c08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_8b17fe6e186848e48b87605106bd17a4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 850M/850M [00:28&lt;00:00, 27.4MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7530a347d0f2498f9bab5f62e85db2ad"
          }
        },
        "db479e9e213942d99aa92962c676aa09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b6d51b5bb0af4d55b89bf257452499aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4d1b83e98f3a48b388b6525ed94d0597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "532a66e3bff442b5a71f87b0f0335f01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8b17fe6e186848e48b87605106bd17a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7530a347d0f2498f9bab5f62e85db2ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFErrTqq_ybT"
      },
      "source": [
        "### Installing neccessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3OMB-x8_0vK",
        "outputId": "98f99f52-d79d-4125-a621-7924d3d3bc55"
      },
      "source": [
        "!pip install transformers\n",
        "# https://huggingface.co/transformers/installation.html\n",
        "!pip install sentencepiece\n",
        "# https://pypi.org/project/sentencepiece/\n",
        "# Python wrapper for SentencePiece. This API will offer the encoding, decoding and training of Sentencepiece.\n",
        "!pip install Cython\n",
        "# https://pypi.org/project/Cython/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 35.6 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 44.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 6.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 55.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (0.29.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-AVcK4gBhW7"
      },
      "source": [
        "## Checking the GPU availabilty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5BIx7Mj1x9M",
        "outputId": "3b945733-9732-4f61-c8d0-6384c0c94450"
      },
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\") \n",
        "    print(\"GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CPU\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1wUPLeYJ6GO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "203f5581-768f-4935-d739-d16d94c65841"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU-UZe2cBPpq"
      },
      "source": [
        "## Importing the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrGEtltY6SIa"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlokiVxO7jy0"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "from transformers.optimization import Adafactor \n",
        "import time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "import torch\n",
        "import random\n",
        "import re\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/new_run')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdoJ8keH8pLP",
        "outputId": "feb9f38f-4eae-4f32-8b01-8c2c79ffa4dd"
      },
      "source": [
        "import pandas as pd\n",
        "# Reading csv\n",
        "data = pd.read_csv('data_10e.csv')\n",
        "print(data.head(5))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              inputs                       target\n",
            "0  The sum of 4 10e2 0 10e1 2 10e0 and 8 10e2 9 1...  1 10e3 2 10e2 9 10e1 8 10e0\n",
            "1  The sum of 7 10e2 9 10e1 4 10e0 and 5 10e2 1 1...  1 10e3 3 10e2 0 10e1 7 10e0\n",
            "2  The sum of 7 10e2 6 10e1 0 10e0 and 3 10e2 1 1...  1 10e3 0 10e2 7 10e1 6 10e0\n",
            "3  The sum of 4 10e2 2 10e1 8 10e0 and 4 10e2 5 1...         8 10e2 8 10e1 3 10e0\n",
            "4  The sum of 6 10e2 5 10e1 4 10e0 and 2 10e2 9 1...         9 10e2 4 10e1 4 10e0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh3S1gzkGt7E",
        "outputId": "ac475a33-eb9f-4ab2-e2a1-92ddc550f488"
      },
      "source": [
        "data.info"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.info of                                                   inputs                       target\n",
              "0      The sum of 4 10e2 0 10e1 2 10e0 and 8 10e2 9 1...  1 10e3 2 10e2 9 10e1 8 10e0\n",
              "1      The sum of 7 10e2 9 10e1 4 10e0 and 5 10e2 1 1...  1 10e3 3 10e2 0 10e1 7 10e0\n",
              "2      The sum of 7 10e2 6 10e1 0 10e0 and 3 10e2 1 1...  1 10e3 0 10e2 7 10e1 6 10e0\n",
              "3      The sum of 4 10e2 2 10e1 8 10e0 and 4 10e2 5 1...         8 10e2 8 10e1 3 10e0\n",
              "4      The sum of 6 10e2 5 10e1 4 10e0 and 2 10e2 9 1...         9 10e2 4 10e1 4 10e0\n",
              "...                                                  ...                          ...\n",
              "34412  The sum of 2 10e2 0 10e1 9 10e0 and 8 10e2 8 1...  1 10e3 0 10e2 9 10e1 6 10e0\n",
              "34413  The sum of 3 10e2 4 10e1 8 10e0 and 5 10e2 0 1...         8 10e2 4 10e1 9 10e0\n",
              "34414  The sum of 9 10e2 4 10e1 5 10e0 and 5 10e2 4 1...  1 10e3 4 10e2 9 10e1 3 10e0\n",
              "34415  The sum of 9 10e2 2 10e1 5 10e0 and 5 10e2 7 1...  1 10e3 5 10e2 0 10e1 1 10e0\n",
              "34416  The sum of 7 10e2 8 10e1 5 10e0 and 5 10e2 3 1...  1 10e3 3 10e2 2 10e1 2 10e0\n",
              "\n",
              "[34417 rows x 2 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA_luy6aGn45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b030eb1d-b629-4b2c-e043-4226239eaa97"
      },
      "source": [
        "data = data.sample(n = 34000, random_state = 42).reset_index(drop=True)\n",
        "len(data)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "34000"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcgHXMgv8606"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Test and validation split\n",
        "train, validation = train_test_split(data, test_size=0.3, random_state=42)\n",
        "train, test = train_test_split(train, test_size=0.4, random_state=42)\n",
        "\n",
        "data_train = train.reset_index(drop=True)\n",
        "data_valid = validation.reset_index(drop=True)\n",
        "data_test = test.reset_index(drop=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7YLTIG87LGc",
        "outputId": "070463b7-5161-4ec3-d6c5-b08b13626228"
      },
      "source": [
        "data_train.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(14280, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vohFU6vyHAiq",
        "outputId": "8694487e-b781-43d5-f2d8-f8d18c5509ff"
      },
      "source": [
        "data_valid.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10200, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ls6YKC19R07"
      },
      "source": [
        "# Initializing Parameters \n",
        "batch_size, num_of_epochs = 32, 25\n",
        "num_of_batches = int(len(data_train)/batch_size)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye-gC3y2YI5g"
      },
      "source": [
        "# Reference\n",
        "# https://huggingface.co/transformers/model_doc/t5.html\n",
        "# https://medium.com/analytics-vidhya/t5-a-detailed-explanation-a0ac9bc53e51\n",
        "# https://towardsdatascience.com/data-to-text-generation-with-t5-building-a-simple-yet-advanced-nlg-model-b5cce5a6df45"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "HngN0xRX1Rk5",
        "outputId": "cd4b1485-5d50-4dc5-8464-4f66f61e642a"
      },
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in data_train['inputs']]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa664bdcc50>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ad1Lt8c9iDX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2d96da2b43a94dc69bb6d0f6179e38c2",
            "fe7879d8700b48baac01f083bc6c1c85",
            "c32835fa8c3c462bb44be6eccf37e519",
            "7e8917d0a4314788b7ec0453ee162885",
            "168a07d77b6848cfad3d3acb76a1605f",
            "68fd34cd5ae54b8fa9c12e979f9c0eee",
            "52a39ecb0cbd4d46941cf0787ba3e9ba",
            "e6a0d0c08fc5436f8f60f19ae3c27088",
            "3db033e672c64ece9df7e7c598bd118f",
            "32e21acc68e3457289963e14e73d1db5",
            "38dc896845d941fd81c06a2d3dd20c36",
            "c91dca00a59f45f3a25cdedabe0394b3",
            "d89f004bbfd949f4b1f44883f1c7a9bd",
            "32fea6661541400aa9dc868ef3331455",
            "bc7c1fe8ba6b4707be1468909f97593d",
            "954b43b377854e2a9c423f88aecf89ba",
            "a64d262edc82482b928fdf989e8c090d",
            "897e881eaf2f472fb74aa45ec2628a9e",
            "fc22a2a3df1041799aa0ffc149527693",
            "a8f6df85d333461f908c39541827bde9",
            "b423a5ac962d452a82bd5331130252e5",
            "7945e75577924d55a0bc652099872197",
            "34e4b0c2261f4fa3809c21608bb4e3b3",
            "a66fb10210b348cc99bd48f1cb37b1e0",
            "f2762e77c49741198334e23ab2918182",
            "9144735e605041b5ad4265c4a997a6ef",
            "876986d0ea7a49c19e3e540ad84f3214",
            "181b6b4e93c24c14be7d672764f53ff2",
            "fdde1cea1e334520831da9a02aa90b8f",
            "4e13938665054040a2bfc669821342fd",
            "c0ec7862956f4ec2a3764593166aefe2",
            "d0cf38c8f2204476be485bc85501da61",
            "948a037040ff4f6bb136a5d922602ace",
            "eb15342f599a49c59a09f35b97bc8031",
            "3549d39fb2bc46a3be9ae30d0032973a",
            "40e2c93774f24c01b2a2d4f1a7c7f95f",
            "2cf3a7a1d04940a1a7cee968d1083edc",
            "2d0b42573e3d4d0c9905421474dc1c08",
            "db479e9e213942d99aa92962c676aa09",
            "b6d51b5bb0af4d55b89bf257452499aa",
            "4d1b83e98f3a48b388b6525ed94d0597",
            "532a66e3bff442b5a71f87b0f0335f01",
            "8b17fe6e186848e48b87605106bd17a4",
            "7530a347d0f2498f9bab5f62e85db2ad"
          ]
        },
        "outputId": "9a39d3c2-f2d6-4893-a486-1fa2e8270c55"
      },
      "source": [
        "# T5-base\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base', return_dict=True)\n",
        "# moving the model to device(GPU/CPU)\n",
        "model.to(device)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d96da2b43a94dc69bb6d0f6179e38c2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c91dca00a59f45f3a25cdedabe0394b3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34e4b0c2261f4fa3809c21608bb4e3b3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eb15342f599a49c59a09f35b97bc8031",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/850M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNRQcyTh3GQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "349ccac1-ff0c-49ed-aaef-55490c21850d"
      },
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in data_train.inputs:\n",
        "  # doubt\n",
        "  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
        "  token_lens.append(len(tokens))\n",
        "\n",
        "max(token_lens)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Efo0J6kA-yt",
        "outputId": "2a973c9d-b922-4dee-d13d-ed917501d52a"
      },
      "source": [
        "token_lens_target = []\n",
        "\n",
        "for txt in data_train.target:\n",
        "  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
        "  token_lens_target.append(len(tokens))\n",
        "\n",
        "max(token_lens_target)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "OT1Us38M2ULV",
        "outputId": "bda5330d-5da8-4801-b59d-75d927113cd3"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(1,len(token_lens)+1), token_lens)\n",
        "plt.ylabel('length of tokens')\n",
        "plt.show()\n",
        "\n",
        "MAX_LEN = max(token_lens)\n",
        "print(\"Maximum length is: \", MAX_LEN)\n",
        "# when sample with first 40k and last 40k we got the maximum length is 14"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length is:  38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6rUVKln1Ff5"
      },
      "source": [
        "MAX_LEN = 45"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FqnJxC4u8XO"
      },
      "source": [
        "def get_word_embeddings(data, MAX_LEN=45):\n",
        "  input_ids=[]\n",
        "  attention_masks = []\n",
        "  for sent in data:\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=sent,  \n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "  # Convert lists to tensors\n",
        "  input_ids = torch.tensor(input_ids)\n",
        "  attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "  return input_ids, attention_masks"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAxmAoDrvrWO",
        "outputId": "2023fbbf-e3d0-4535-a729-05b02fee8fba"
      },
      "source": [
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "train_inputs, train_masks = get_word_embeddings(data_train['inputs'])\n",
        "val_inputs, val_masks = get_word_embeddings(data_valid['inputs'])\n",
        "test_inputs, test_masks = get_word_embeddings(data_test['inputs'])"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbPJ2deo71p5"
      },
      "source": [
        "data_train['target_str'] = data_train['target'].astype(str)\n",
        "data_valid['target_str'] = data_valid['target'].astype(str)\n",
        "data_test['target_str'] = data_test['target'].astype(str)\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3dHakeNv66S",
        "outputId": "a6f551d3-9f6b-455d-bfb4-e45889698407"
      },
      "source": [
        "#convert lists to tensors\n",
        "train_labels = get_word_embeddings(data_train['target_str'], 25)[0]\n",
        "val_labels = get_word_embeddings(data_valid['target_str'], 25)[0]\n",
        "test_labels = get_word_embeddings(data_test['target_str'], 25)[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHUnt0aF8GQk",
        "outputId": "cbbb8d98-361f-444f-a692-70fb4a7eebae"
      },
      "source": [
        "train_labels.shape\n",
        "# doubt"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([14280, 25])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgegmxBtoqZV",
        "outputId": "13b99cda-1014-4a61-a125-9938f28251a1"
      },
      "source": [
        "train_labels"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[209, 335,  15,  ...,   0,   0,   0],\n",
              "        [505, 335,  15,  ...,   0,   0,   0],\n",
              "        [209, 335,  15,  ...,   0,   0,   0],\n",
              "        ...,\n",
              "        [220, 335,  15,  ...,   0,   0,   0],\n",
              "        [209, 335,  15,  ...,   0,   0,   0],\n",
              "        [209, 335,  15,  ...,   0,   0,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ezRHf2e7rPL",
        "outputId": "e35b7224-dc0b-46a3-ee0f-0ed6244b15bb"
      },
      "source": [
        "data_train['target']"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        1 10e3 2 10e2 8 10e1 2 10e0\n",
              "1               8 10e2 9 10e1 3 10e0\n",
              "2        1 10e3 3 10e2 5 10e1 0 10e0\n",
              "3        1 10e3 5 10e2 0 10e1 8 10e0\n",
              "4               1 10e2 2 10e1 2 10e0\n",
              "                    ...             \n",
              "14275           8 10e2 0 10e1 2 10e0\n",
              "14276           8 10e2 8 10e1 2 10e0\n",
              "14277           3 10e2 6 10e1 5 10e0\n",
              "14278    1 10e3 0 10e2 4 10e1 3 10e0\n",
              "14279    1 10e3 2 10e2 6 10e1 6 10e0\n",
              "Name: target, Length: 14280, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osOohece1-tJ",
        "outputId": "d14b7079-44b5-492f-a8ff-aec24889ef31"
      },
      "source": [
        "train_inputs.shape"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([14280, 45])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjDbWMIlv8Iq"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(train_data, shuffle = True, batch_size = batch_size)\n",
        "\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_dataloader = DataLoader(val_data, shuffle = True, batch_size = batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_dataloader = DataLoader(test_data, shuffle = True, batch_size = batch_size)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q88he4D9Lw0L"
      },
      "source": [
        "#  Optimizer\n",
        "# https://huggingface.co/transformers/model_doc/t5.html#overview\n",
        "optimizer = Adafactor(\n",
        "    model.parameters(),\n",
        "    lr=5e-4, # Initializing the learning Rate as suggested in the T5 official documentation\n",
        "    eps=(1e-8, 1e-3),\n",
        "    clip_threshold=1.0,\n",
        "    decay_rate=-0.3,\n",
        "    beta1=None,\n",
        "    weight_decay=0.0,\n",
        "    relative_step=False,\n",
        "    scale_parameter=False,\n",
        "    warmup_init=False\n",
        ")"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQ9Bj35-56kQ",
        "outputId": "8664cc1e-dbc3-4b91-92b3-24d96512f9db"
      },
      "source": [
        "# Changing the directory to store the model there.\n",
        "print(os.getcwd())\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/new_run')\n",
        "print(os.getcwd())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/new_run\n",
            "/content/drive/MyDrive/Colab Notebooks/new_run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COfO6s0z3kit",
        "outputId": "a0ba1767-26b0-4b74-e206-525ee929c179"
      },
      "source": [
        "# Loading the configuration file for 't5-base' model\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-24 23:45:21--  https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.79.54\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.79.54|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1199 (1.2K) [application/json]\n",
            "Saving to: ‘t5-base-config.json.4’\n",
            "\n",
            "\rt5-base-config.json   0%[                    ]       0  --.-KB/s               \rt5-base-config.json 100%[===================>]   1.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-11-24 23:45:21 (34.7 MB/s) - ‘t5-base-config.json.4’ saved [1199/1199]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYMdr0Xu92Yv"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "# Setting the progress, with html as UI.\n",
        "def progress(loss, value, max=100):\n",
        "    return HTML(\"\"\" Batch loss :{loss}\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(loss=loss,value=value, max=max))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C2w5YpsN7C1"
      },
      "source": [
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    v_accuracy = []\n",
        "    v_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        \n",
        "        v_input_ids, v_attn_mask, v_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # print(v_input_ids.shape, v_labels.shape)\n",
        "\n",
        "        val_outputs = model.generate(input_ids=v_input_ids, attention_mask=v_attn_mask)\n",
        "\n",
        "        val_preds = [ tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            for output in val_outputs]\n",
        "\n",
        "        val_labels = [ tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            for output in v_labels]\n",
        "        \n",
        "        # v_loss.append(val_outputs.loss)\n",
        "\n",
        "        # Get the predictions\n",
        "        # print(val_outputs.logits.shape)\n",
        "        # val_preds = torch.argmax(val_outputs.logits, dim=1).flatten()\n",
        "        # print(val_preds, val_labels)\n",
        "        # Calculate the accuracy rate\n",
        "\n",
        "        val_preds = np.array(val_preds)\n",
        "        val_labels = np.array(val_labels)\n",
        "        accuracy = ((val_preds == val_labels).sum() / len(val_labels)) * 100\n",
        "        v_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    # v_loss = np.mean(v_loss)\n",
        "    v_accuracy = np.mean(v_accuracy)\n",
        "\n",
        "    return v_accuracy"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlCbmfAQ-CWJ",
        "outputId": "12468d81-16d5-46ad-bc84-9c1acc0dfdb1"
      },
      "source": [
        "import gc\n",
        "\n",
        "val_acc = 0\n",
        "train_accuracy = 0\n",
        "\n",
        "# Sets the module in training mode\n",
        "model.train()\n",
        "\n",
        "for epoch in range(1,num_of_epochs+1):\n",
        "  print('Running epoch: {}'.format(epoch))\n",
        "  running_loss=0\n",
        "  # out = display(progress(1, num_of_batches+1), display_id=True)\n",
        "  i =0 \n",
        "  for batch in train_dataloader:\n",
        "    \n",
        "    input_ids, attn_mask, labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # clear out the gradients of all Variables \n",
        "    optimizer.zero_grad()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Forward propogation\n",
        "    # print(model(input_ids=input_ids, attention_mask=attn_mask, labels=labels))\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n",
        "    \n",
        "    loss = outputs.loss\n",
        "    loss_num=loss.item()\n",
        "    logits = outputs.logits\n",
        "    running_loss+=loss_num\n",
        "    # out.update(progress(loss_num,i, num_of_batches+1))\n",
        "\n",
        "    # calculating the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # updating the params\n",
        "    optimizer.step()\n",
        "\n",
        "    print(\"Epoch \", epoch, \"Batch \", i, \"/\", len(train_dataloader), \" Training Loss \", loss_num)\n",
        "    i += 1\n",
        "\n",
        "  running_loss = running_loss/len(train_dataloader)\n",
        "  # v_input_ids, v_attn_mask, v_labels = tuple(t.to(device) for t in data_valid)\n",
        "  \n",
        "  curr_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "  # print('Epoch: {} , Running loss: {}'.format(epoch,running_loss))\n",
        "  print(f\"{epoch + 1:^7} | {'-':^7} | {running_loss:^12.6f} | {curr_accuracy:^9.6f}\")\n",
        "  print(\"-\"*70)\n",
        "\n",
        "  if curr_accuracy > val_acc:\n",
        "    val_acc = curr_accuracy\n",
        "    # Saving the best model\n",
        "    torch.save(model.state_dict(),'Best_Masked_Number_Prediction_Model_5e_4.bin')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch  1 Batch  397 / 447  Training Loss  0.09981050342321396\n",
            "Epoch  1 Batch  398 / 447  Training Loss  0.11064958572387695\n",
            "Epoch  1 Batch  399 / 447  Training Loss  0.11968269944190979\n",
            "Epoch  1 Batch  400 / 447  Training Loss  0.10754252225160599\n",
            "Epoch  1 Batch  401 / 447  Training Loss  0.08934428542852402\n",
            "Epoch  1 Batch  402 / 447  Training Loss  0.09466633945703506\n",
            "Epoch  1 Batch  403 / 447  Training Loss  0.09032633155584335\n",
            "Epoch  1 Batch  404 / 447  Training Loss  0.10032534599304199\n",
            "Epoch  1 Batch  405 / 447  Training Loss  0.09716043621301651\n",
            "Epoch  1 Batch  406 / 447  Training Loss  0.10798841714859009\n",
            "Epoch  1 Batch  407 / 447  Training Loss  0.09147920459508896\n",
            "Epoch  1 Batch  408 / 447  Training Loss  0.10511592030525208\n",
            "Epoch  1 Batch  409 / 447  Training Loss  0.09988780319690704\n",
            "Epoch  1 Batch  410 / 447  Training Loss  0.07968875765800476\n",
            "Epoch  1 Batch  411 / 447  Training Loss  0.09433939307928085\n",
            "Epoch  1 Batch  412 / 447  Training Loss  0.08596763014793396\n",
            "Epoch  1 Batch  413 / 447  Training Loss  0.08574352413415909\n",
            "Epoch  1 Batch  414 / 447  Training Loss  0.10009297728538513\n",
            "Epoch  1 Batch  415 / 447  Training Loss  0.09708823263645172\n",
            "Epoch  1 Batch  416 / 447  Training Loss  0.09032311290502548\n",
            "Epoch  1 Batch  417 / 447  Training Loss  0.09917661547660828\n",
            "Epoch  1 Batch  418 / 447  Training Loss  0.07320764660835266\n",
            "Epoch  1 Batch  419 / 447  Training Loss  0.09400071948766708\n",
            "Epoch  1 Batch  420 / 447  Training Loss  0.09566909074783325\n",
            "Epoch  1 Batch  421 / 447  Training Loss  0.08206022530794144\n",
            "Epoch  1 Batch  422 / 447  Training Loss  0.10378646850585938\n",
            "Epoch  1 Batch  423 / 447  Training Loss  0.09122481942176819\n",
            "Epoch  1 Batch  424 / 447  Training Loss  0.08202274143695831\n",
            "Epoch  1 Batch  425 / 447  Training Loss  0.08658470213413239\n",
            "Epoch  1 Batch  426 / 447  Training Loss  0.09270825982093811\n",
            "Epoch  1 Batch  427 / 447  Training Loss  0.08491665869951248\n",
            "Epoch  1 Batch  428 / 447  Training Loss  0.10359513014554977\n",
            "Epoch  1 Batch  429 / 447  Training Loss  0.0891324058175087\n",
            "Epoch  1 Batch  430 / 447  Training Loss  0.09353765845298767\n",
            "Epoch  1 Batch  431 / 447  Training Loss  0.09011344611644745\n",
            "Epoch  1 Batch  432 / 447  Training Loss  0.10860779136419296\n",
            "Epoch  1 Batch  433 / 447  Training Loss  0.0868702381849289\n",
            "Epoch  1 Batch  434 / 447  Training Loss  0.08027713745832443\n",
            "Epoch  1 Batch  435 / 447  Training Loss  0.09031805396080017\n",
            "Epoch  1 Batch  436 / 447  Training Loss  0.09662468731403351\n",
            "Epoch  1 Batch  437 / 447  Training Loss  0.07330108433961868\n",
            "Epoch  1 Batch  438 / 447  Training Loss  0.07501375675201416\n",
            "Epoch  1 Batch  439 / 447  Training Loss  0.07737516611814499\n",
            "Epoch  1 Batch  440 / 447  Training Loss  0.08750443160533905\n",
            "Epoch  1 Batch  441 / 447  Training Loss  0.0839136391878128\n",
            "Epoch  1 Batch  442 / 447  Training Loss  0.10120171308517456\n",
            "Epoch  1 Batch  443 / 447  Training Loss  0.10890373587608337\n",
            "Epoch  1 Batch  444 / 447  Training Loss  0.10066507011651993\n",
            "Epoch  1 Batch  445 / 447  Training Loss  0.10213061422109604\n",
            "Epoch  1 Batch  446 / 447  Training Loss  0.09653778374195099\n",
            "   2    |    -    |   0.221278   | 57.383098\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 2\n",
            "Epoch  2 Batch  0 / 447  Training Loss  0.053016796708106995\n",
            "Epoch  2 Batch  1 / 447  Training Loss  0.04811619594693184\n",
            "Epoch  2 Batch  2 / 447  Training Loss  0.056132569909095764\n",
            "Epoch  2 Batch  3 / 447  Training Loss  0.055263031274080276\n",
            "Epoch  2 Batch  4 / 447  Training Loss  0.04447885602712631\n",
            "Epoch  2 Batch  5 / 447  Training Loss  0.0450826920568943\n",
            "Epoch  2 Batch  6 / 447  Training Loss  0.055852919816970825\n",
            "Epoch  2 Batch  7 / 447  Training Loss  0.05143244192004204\n",
            "Epoch  2 Batch  8 / 447  Training Loss  0.043152593076229095\n",
            "Epoch  2 Batch  9 / 447  Training Loss  0.04793567955493927\n",
            "Epoch  2 Batch  10 / 447  Training Loss  0.03965076804161072\n",
            "Epoch  2 Batch  11 / 447  Training Loss  0.056730080395936966\n",
            "Epoch  2 Batch  12 / 447  Training Loss  0.04217036813497543\n",
            "Epoch  2 Batch  13 / 447  Training Loss  0.0456363782286644\n",
            "Epoch  2 Batch  14 / 447  Training Loss  0.045061249285936356\n",
            "Epoch  2 Batch  15 / 447  Training Loss  0.044763680547475815\n",
            "Epoch  2 Batch  16 / 447  Training Loss  0.036187756806612015\n",
            "Epoch  2 Batch  17 / 447  Training Loss  0.039245713502168655\n",
            "Epoch  2 Batch  18 / 447  Training Loss  0.04152742400765419\n",
            "Epoch  2 Batch  19 / 447  Training Loss  0.04844256490468979\n",
            "Epoch  2 Batch  20 / 447  Training Loss  0.03766879066824913\n",
            "Epoch  2 Batch  21 / 447  Training Loss  0.04718659073114395\n",
            "Epoch  2 Batch  22 / 447  Training Loss  0.052699390798807144\n",
            "Epoch  2 Batch  23 / 447  Training Loss  0.07117900997400284\n",
            "Epoch  2 Batch  24 / 447  Training Loss  0.05118962377309799\n",
            "Epoch  2 Batch  25 / 447  Training Loss  0.03627323731780052\n",
            "Epoch  2 Batch  26 / 447  Training Loss  0.03587067872285843\n",
            "Epoch  2 Batch  27 / 447  Training Loss  0.033670999109745026\n",
            "Epoch  2 Batch  28 / 447  Training Loss  0.04334552213549614\n",
            "Epoch  2 Batch  29 / 447  Training Loss  0.04338939115405083\n",
            "Epoch  2 Batch  30 / 447  Training Loss  0.055511053651571274\n",
            "Epoch  2 Batch  31 / 447  Training Loss  0.0398891344666481\n",
            "Epoch  2 Batch  32 / 447  Training Loss  0.03632676973938942\n",
            "Epoch  2 Batch  33 / 447  Training Loss  0.049362506717443466\n",
            "Epoch  2 Batch  34 / 447  Training Loss  0.04317368566989899\n",
            "Epoch  2 Batch  35 / 447  Training Loss  0.03861851990222931\n",
            "Epoch  2 Batch  36 / 447  Training Loss  0.02985156513750553\n",
            "Epoch  2 Batch  37 / 447  Training Loss  0.035394299775362015\n",
            "Epoch  2 Batch  38 / 447  Training Loss  0.03919067233800888\n",
            "Epoch  2 Batch  39 / 447  Training Loss  0.03979688882827759\n",
            "Epoch  2 Batch  40 / 447  Training Loss  0.03630364313721657\n",
            "Epoch  2 Batch  41 / 447  Training Loss  0.03421693295240402\n",
            "Epoch  2 Batch  42 / 447  Training Loss  0.05058756843209267\n",
            "Epoch  2 Batch  43 / 447  Training Loss  0.033823445439338684\n",
            "Epoch  2 Batch  44 / 447  Training Loss  0.036914926022291183\n",
            "Epoch  2 Batch  45 / 447  Training Loss  0.02897682599723339\n",
            "Epoch  2 Batch  46 / 447  Training Loss  0.03314254432916641\n",
            "Epoch  2 Batch  47 / 447  Training Loss  0.03728543967008591\n",
            "Epoch  2 Batch  48 / 447  Training Loss  0.04802856966853142\n",
            "Epoch  2 Batch  49 / 447  Training Loss  0.07233721017837524\n",
            "Epoch  2 Batch  50 / 447  Training Loss  0.10401616245508194\n",
            "Epoch  2 Batch  51 / 447  Training Loss  0.09751300513744354\n",
            "Epoch  2 Batch  52 / 447  Training Loss  0.04402533918619156\n",
            "Epoch  2 Batch  53 / 447  Training Loss  0.031410738825798035\n",
            "Epoch  2 Batch  54 / 447  Training Loss  0.04328455030918121\n",
            "Epoch  2 Batch  55 / 447  Training Loss  0.044840842485427856\n",
            "Epoch  2 Batch  56 / 447  Training Loss  0.033136285841464996\n",
            "Epoch  2 Batch  57 / 447  Training Loss  0.03822088614106178\n",
            "Epoch  2 Batch  58 / 447  Training Loss  0.02829071693122387\n",
            "Epoch  2 Batch  59 / 447  Training Loss  0.026161685585975647\n",
            "Epoch  2 Batch  60 / 447  Training Loss  0.035048238933086395\n",
            "Epoch  2 Batch  61 / 447  Training Loss  0.029065176844596863\n",
            "Epoch  2 Batch  62 / 447  Training Loss  0.024152278900146484\n",
            "Epoch  2 Batch  63 / 447  Training Loss  0.03393471613526344\n",
            "Epoch  2 Batch  64 / 447  Training Loss  0.03435845300555229\n",
            "Epoch  2 Batch  65 / 447  Training Loss  0.022425755858421326\n",
            "Epoch  2 Batch  66 / 447  Training Loss  0.03338649868965149\n",
            "Epoch  2 Batch  67 / 447  Training Loss  0.016823384910821915\n",
            "Epoch  2 Batch  68 / 447  Training Loss  0.022598760202527046\n",
            "Epoch  2 Batch  69 / 447  Training Loss  0.027153220027685165\n",
            "Epoch  2 Batch  70 / 447  Training Loss  0.02824113517999649\n",
            "Epoch  2 Batch  71 / 447  Training Loss  0.08161763846874237\n",
            "Epoch  2 Batch  72 / 447  Training Loss  0.06605023145675659\n",
            "Epoch  2 Batch  73 / 447  Training Loss  0.043192051351070404\n",
            "Epoch  2 Batch  74 / 447  Training Loss  0.028751077130436897\n",
            "Epoch  2 Batch  75 / 447  Training Loss  0.029990579932928085\n",
            "Epoch  2 Batch  76 / 447  Training Loss  0.02776445634663105\n",
            "Epoch  2 Batch  77 / 447  Training Loss  0.022845743224024773\n",
            "Epoch  2 Batch  78 / 447  Training Loss  0.031973715871572495\n",
            "Epoch  2 Batch  79 / 447  Training Loss  0.027698982506990433\n",
            "Epoch  2 Batch  80 / 447  Training Loss  0.0280070211738348\n",
            "Epoch  2 Batch  81 / 447  Training Loss  0.03822906315326691\n",
            "Epoch  2 Batch  82 / 447  Training Loss  0.030792297795414925\n",
            "Epoch  2 Batch  83 / 447  Training Loss  0.020832771435379982\n",
            "Epoch  2 Batch  84 / 447  Training Loss  0.028033573180437088\n",
            "Epoch  2 Batch  85 / 447  Training Loss  0.019020812585949898\n",
            "Epoch  2 Batch  86 / 447  Training Loss  0.02477797493338585\n",
            "Epoch  2 Batch  87 / 447  Training Loss  0.03040795773267746\n",
            "Epoch  2 Batch  88 / 447  Training Loss  0.031853675842285156\n",
            "Epoch  2 Batch  89 / 447  Training Loss  0.03160270303487778\n",
            "Epoch  2 Batch  90 / 447  Training Loss  0.02698846347630024\n",
            "Epoch  2 Batch  91 / 447  Training Loss  0.027474377304315567\n",
            "Epoch  2 Batch  92 / 447  Training Loss  0.027357375249266624\n",
            "Epoch  2 Batch  93 / 447  Training Loss  0.025129614397883415\n",
            "Epoch  2 Batch  94 / 447  Training Loss  0.02546863630414009\n",
            "Epoch  2 Batch  95 / 447  Training Loss  0.023048674687743187\n",
            "Epoch  2 Batch  96 / 447  Training Loss  0.03138958662748337\n",
            "Epoch  2 Batch  97 / 447  Training Loss  0.021581847220659256\n",
            "Epoch  2 Batch  98 / 447  Training Loss  0.02060500904917717\n",
            "Epoch  2 Batch  99 / 447  Training Loss  0.03703860193490982\n",
            "Epoch  2 Batch  100 / 447  Training Loss  0.019626572728157043\n",
            "Epoch  2 Batch  101 / 447  Training Loss  0.033274341374635696\n",
            "Epoch  2 Batch  102 / 447  Training Loss  0.027581837028265\n",
            "Epoch  2 Batch  103 / 447  Training Loss  0.020367778837680817\n",
            "Epoch  2 Batch  104 / 447  Training Loss  0.02684655226767063\n",
            "Epoch  2 Batch  105 / 447  Training Loss  0.02201136201620102\n",
            "Epoch  2 Batch  106 / 447  Training Loss  0.02040400356054306\n",
            "Epoch  2 Batch  107 / 447  Training Loss  0.024930033832788467\n",
            "Epoch  2 Batch  108 / 447  Training Loss  0.026280269026756287\n",
            "Epoch  2 Batch  109 / 447  Training Loss  0.02637278102338314\n",
            "Epoch  2 Batch  110 / 447  Training Loss  0.021421903744339943\n",
            "Epoch  2 Batch  111 / 447  Training Loss  0.019546300172805786\n",
            "Epoch  2 Batch  112 / 447  Training Loss  0.017373548820614815\n",
            "Epoch  2 Batch  113 / 447  Training Loss  0.015375078655779362\n",
            "Epoch  2 Batch  114 / 447  Training Loss  0.01836097054183483\n",
            "Epoch  2 Batch  115 / 447  Training Loss  0.015460197813808918\n",
            "Epoch  2 Batch  116 / 447  Training Loss  0.023427795618772507\n",
            "Epoch  2 Batch  117 / 447  Training Loss  0.02211149036884308\n",
            "Epoch  2 Batch  118 / 447  Training Loss  0.020931916311383247\n",
            "Epoch  2 Batch  119 / 447  Training Loss  0.018703429028391838\n",
            "Epoch  2 Batch  120 / 447  Training Loss  0.02592594176530838\n",
            "Epoch  2 Batch  121 / 447  Training Loss  0.025002045556902885\n",
            "Epoch  2 Batch  122 / 447  Training Loss  0.04466778412461281\n",
            "Epoch  2 Batch  123 / 447  Training Loss  0.05888381972908974\n",
            "Epoch  2 Batch  124 / 447  Training Loss  0.022630246356129646\n",
            "Epoch  2 Batch  125 / 447  Training Loss  0.013647998683154583\n",
            "Epoch  2 Batch  126 / 447  Training Loss  0.024453260004520416\n",
            "Epoch  2 Batch  127 / 447  Training Loss  0.026442909613251686\n",
            "Epoch  2 Batch  128 / 447  Training Loss  0.02578447386622429\n",
            "Epoch  2 Batch  129 / 447  Training Loss  0.01852083019912243\n",
            "Epoch  2 Batch  130 / 447  Training Loss  0.02540656551718712\n",
            "Epoch  2 Batch  131 / 447  Training Loss  0.019445890560746193\n",
            "Epoch  2 Batch  132 / 447  Training Loss  0.01435292698442936\n",
            "Epoch  2 Batch  133 / 447  Training Loss  0.013626142404973507\n",
            "Epoch  2 Batch  134 / 447  Training Loss  0.018323319032788277\n",
            "Epoch  2 Batch  135 / 447  Training Loss  0.022156408056616783\n",
            "Epoch  2 Batch  136 / 447  Training Loss  0.013632834888994694\n",
            "Epoch  2 Batch  137 / 447  Training Loss  0.016533369198441505\n",
            "Epoch  2 Batch  138 / 447  Training Loss  0.012122771702706814\n",
            "Epoch  2 Batch  139 / 447  Training Loss  0.01257608737796545\n",
            "Epoch  2 Batch  140 / 447  Training Loss  0.03048538975417614\n",
            "Epoch  2 Batch  141 / 447  Training Loss  0.012285454198718071\n",
            "Epoch  2 Batch  142 / 447  Training Loss  0.013551443815231323\n",
            "Epoch  2 Batch  143 / 447  Training Loss  0.024690477177500725\n",
            "Epoch  2 Batch  144 / 447  Training Loss  0.02438385784626007\n",
            "Epoch  2 Batch  145 / 447  Training Loss  0.023756681010127068\n",
            "Epoch  2 Batch  146 / 447  Training Loss  0.02145698480308056\n",
            "Epoch  2 Batch  147 / 447  Training Loss  0.02575523778796196\n",
            "Epoch  2 Batch  148 / 447  Training Loss  0.02534801885485649\n",
            "Epoch  2 Batch  149 / 447  Training Loss  0.041553396731615067\n",
            "Epoch  2 Batch  150 / 447  Training Loss  0.058871008455753326\n",
            "Epoch  2 Batch  151 / 447  Training Loss  0.025605574250221252\n",
            "Epoch  2 Batch  152 / 447  Training Loss  0.013405000790953636\n",
            "Epoch  2 Batch  153 / 447  Training Loss  0.018409864977002144\n",
            "Epoch  2 Batch  154 / 447  Training Loss  0.026955850422382355\n",
            "Epoch  2 Batch  155 / 447  Training Loss  0.02878759056329727\n",
            "Epoch  2 Batch  156 / 447  Training Loss  0.014625543728470802\n",
            "Epoch  2 Batch  157 / 447  Training Loss  0.016234802082180977\n",
            "Epoch  2 Batch  158 / 447  Training Loss  0.01273080799728632\n",
            "Epoch  2 Batch  159 / 447  Training Loss  0.009332177229225636\n",
            "Epoch  2 Batch  160 / 447  Training Loss  0.013000678271055222\n",
            "Epoch  2 Batch  161 / 447  Training Loss  0.019739124923944473\n",
            "Epoch  2 Batch  162 / 447  Training Loss  0.017320143058896065\n",
            "Epoch  2 Batch  163 / 447  Training Loss  0.011363629251718521\n",
            "Epoch  2 Batch  164 / 447  Training Loss  0.008105435408651829\n",
            "Epoch  2 Batch  165 / 447  Training Loss  0.0127120241522789\n",
            "Epoch  2 Batch  166 / 447  Training Loss  0.010095447301864624\n",
            "Epoch  2 Batch  167 / 447  Training Loss  0.013098415918648243\n",
            "Epoch  2 Batch  168 / 447  Training Loss  0.013595464639365673\n",
            "Epoch  2 Batch  169 / 447  Training Loss  0.009855429641902447\n",
            "Epoch  2 Batch  170 / 447  Training Loss  0.014255085028707981\n",
            "Epoch  2 Batch  171 / 447  Training Loss  0.012055902741849422\n",
            "Epoch  2 Batch  172 / 447  Training Loss  0.012589449062943459\n",
            "Epoch  2 Batch  173 / 447  Training Loss  0.010938343591988087\n",
            "Epoch  2 Batch  174 / 447  Training Loss  0.02658808045089245\n",
            "Epoch  2 Batch  175 / 447  Training Loss  0.023120298981666565\n",
            "Epoch  2 Batch  176 / 447  Training Loss  0.016505010426044464\n",
            "Epoch  2 Batch  177 / 447  Training Loss  0.01723451353609562\n",
            "Epoch  2 Batch  178 / 447  Training Loss  0.012504074722528458\n",
            "Epoch  2 Batch  179 / 447  Training Loss  0.011477094143629074\n",
            "Epoch  2 Batch  180 / 447  Training Loss  0.013803106732666492\n",
            "Epoch  2 Batch  181 / 447  Training Loss  0.009968816302716732\n",
            "Epoch  2 Batch  182 / 447  Training Loss  0.017335880547761917\n",
            "Epoch  2 Batch  183 / 447  Training Loss  0.012284405529499054\n",
            "Epoch  2 Batch  184 / 447  Training Loss  0.027886725962162018\n",
            "Epoch  2 Batch  185 / 447  Training Loss  0.028428591787815094\n",
            "Epoch  2 Batch  186 / 447  Training Loss  0.02283964306116104\n",
            "Epoch  2 Batch  187 / 447  Training Loss  0.010800380259752274\n",
            "Epoch  2 Batch  188 / 447  Training Loss  0.018207155168056488\n",
            "Epoch  2 Batch  189 / 447  Training Loss  0.008009320124983788\n",
            "Epoch  2 Batch  190 / 447  Training Loss  0.012356311082839966\n",
            "Epoch  2 Batch  191 / 447  Training Loss  0.0074464138597249985\n",
            "Epoch  2 Batch  192 / 447  Training Loss  0.010615844279527664\n",
            "Epoch  2 Batch  193 / 447  Training Loss  0.0076303379610180855\n",
            "Epoch  2 Batch  194 / 447  Training Loss  0.014625051990151405\n",
            "Epoch  2 Batch  195 / 447  Training Loss  0.015158243477344513\n",
            "Epoch  2 Batch  196 / 447  Training Loss  0.021256742998957634\n",
            "Epoch  2 Batch  197 / 447  Training Loss  0.012779486365616322\n",
            "Epoch  2 Batch  198 / 447  Training Loss  0.012897176668047905\n",
            "Epoch  2 Batch  199 / 447  Training Loss  0.00883443746715784\n",
            "Epoch  2 Batch  200 / 447  Training Loss  0.015320924110710621\n",
            "Epoch  2 Batch  201 / 447  Training Loss  0.014465916901826859\n",
            "Epoch  2 Batch  202 / 447  Training Loss  0.010866178199648857\n",
            "Epoch  2 Batch  203 / 447  Training Loss  0.010077758692204952\n",
            "Epoch  2 Batch  204 / 447  Training Loss  0.007830793038010597\n",
            "Epoch  2 Batch  205 / 447  Training Loss  0.017351867631077766\n",
            "Epoch  2 Batch  206 / 447  Training Loss  0.009673107415437698\n",
            "Epoch  2 Batch  207 / 447  Training Loss  0.008934526704251766\n",
            "Epoch  2 Batch  208 / 447  Training Loss  0.012975134886801243\n",
            "Epoch  2 Batch  209 / 447  Training Loss  0.006002458278089762\n",
            "Epoch  2 Batch  210 / 447  Training Loss  0.01576344668865204\n",
            "Epoch  2 Batch  211 / 447  Training Loss  0.019490068778395653\n",
            "Epoch  2 Batch  212 / 447  Training Loss  0.013534185476601124\n",
            "Epoch  2 Batch  213 / 447  Training Loss  0.012615582905709743\n",
            "Epoch  2 Batch  214 / 447  Training Loss  0.014980248175561428\n",
            "Epoch  2 Batch  215 / 447  Training Loss  0.031244944781064987\n",
            "Epoch  2 Batch  216 / 447  Training Loss  0.03674067184329033\n",
            "Epoch  2 Batch  217 / 447  Training Loss  0.0165725015103817\n",
            "Epoch  2 Batch  218 / 447  Training Loss  0.013948439620435238\n",
            "Epoch  2 Batch  219 / 447  Training Loss  0.024542666971683502\n",
            "Epoch  2 Batch  220 / 447  Training Loss  0.012149041518568993\n",
            "Epoch  2 Batch  221 / 447  Training Loss  0.008171189576387405\n",
            "Epoch  2 Batch  222 / 447  Training Loss  0.016771327704191208\n",
            "Epoch  2 Batch  223 / 447  Training Loss  0.013487482443451881\n",
            "Epoch  2 Batch  224 / 447  Training Loss  0.016022812575101852\n",
            "Epoch  2 Batch  225 / 447  Training Loss  0.011490092612802982\n",
            "Epoch  2 Batch  226 / 447  Training Loss  0.012947414070367813\n",
            "Epoch  2 Batch  227 / 447  Training Loss  0.009438601322472095\n",
            "Epoch  2 Batch  228 / 447  Training Loss  0.010062037967145443\n",
            "Epoch  2 Batch  229 / 447  Training Loss  0.016315175220370293\n",
            "Epoch  2 Batch  230 / 447  Training Loss  0.013789641670882702\n",
            "Epoch  2 Batch  231 / 447  Training Loss  0.011787108145654202\n",
            "Epoch  2 Batch  232 / 447  Training Loss  0.010588611476123333\n",
            "Epoch  2 Batch  233 / 447  Training Loss  0.006526065990328789\n",
            "Epoch  2 Batch  234 / 447  Training Loss  0.019733969122171402\n",
            "Epoch  2 Batch  235 / 447  Training Loss  0.009781572967767715\n",
            "Epoch  2 Batch  236 / 447  Training Loss  0.007103435229510069\n",
            "Epoch  2 Batch  237 / 447  Training Loss  0.006086365785449743\n",
            "Epoch  2 Batch  238 / 447  Training Loss  0.011208530515432358\n",
            "Epoch  2 Batch  239 / 447  Training Loss  0.0102070989087224\n",
            "Epoch  2 Batch  240 / 447  Training Loss  0.015834394842386246\n",
            "Epoch  2 Batch  241 / 447  Training Loss  0.013012256473302841\n",
            "Epoch  2 Batch  242 / 447  Training Loss  0.014233929105103016\n",
            "Epoch  2 Batch  243 / 447  Training Loss  0.01192403957247734\n",
            "Epoch  2 Batch  244 / 447  Training Loss  0.00945860706269741\n",
            "Epoch  2 Batch  245 / 447  Training Loss  0.006521411705762148\n",
            "Epoch  2 Batch  246 / 447  Training Loss  0.005449512042105198\n",
            "Epoch  2 Batch  247 / 447  Training Loss  0.005106640048325062\n",
            "Epoch  2 Batch  248 / 447  Training Loss  0.020236093550920486\n",
            "Epoch  2 Batch  249 / 447  Training Loss  0.01485177967697382\n",
            "Epoch  2 Batch  250 / 447  Training Loss  0.013961228542029858\n",
            "Epoch  2 Batch  251 / 447  Training Loss  0.007883423939347267\n",
            "Epoch  2 Batch  252 / 447  Training Loss  0.008030903525650501\n",
            "Epoch  2 Batch  253 / 447  Training Loss  0.011701093055307865\n",
            "Epoch  2 Batch  254 / 447  Training Loss  0.018566736951470375\n",
            "Epoch  2 Batch  255 / 447  Training Loss  0.01254276279360056\n",
            "Epoch  2 Batch  256 / 447  Training Loss  0.013896014541387558\n",
            "Epoch  2 Batch  257 / 447  Training Loss  0.0134340301156044\n",
            "Epoch  2 Batch  258 / 447  Training Loss  0.0138046033680439\n",
            "Epoch  2 Batch  259 / 447  Training Loss  0.018144909292459488\n",
            "Epoch  2 Batch  260 / 447  Training Loss  0.022431781515479088\n",
            "Epoch  2 Batch  261 / 447  Training Loss  0.006392637267708778\n",
            "Epoch  2 Batch  262 / 447  Training Loss  0.0070581031031906605\n",
            "Epoch  2 Batch  263 / 447  Training Loss  0.009446022100746632\n",
            "Epoch  2 Batch  264 / 447  Training Loss  0.014643392525613308\n",
            "Epoch  2 Batch  265 / 447  Training Loss  0.005555957555770874\n",
            "Epoch  2 Batch  266 / 447  Training Loss  0.019589027389883995\n",
            "Epoch  2 Batch  267 / 447  Training Loss  0.013515856117010117\n",
            "Epoch  2 Batch  268 / 447  Training Loss  0.014170675538480282\n",
            "Epoch  2 Batch  269 / 447  Training Loss  0.016679931432008743\n",
            "Epoch  2 Batch  270 / 447  Training Loss  0.02032242715358734\n",
            "Epoch  2 Batch  271 / 447  Training Loss  0.010512925684452057\n",
            "Epoch  2 Batch  272 / 447  Training Loss  0.006971412338316441\n",
            "Epoch  2 Batch  273 / 447  Training Loss  0.00837556179612875\n",
            "Epoch  2 Batch  274 / 447  Training Loss  0.015755370259284973\n",
            "Epoch  2 Batch  275 / 447  Training Loss  0.01519123837351799\n",
            "Epoch  2 Batch  276 / 447  Training Loss  0.00974399596452713\n",
            "Epoch  2 Batch  277 / 447  Training Loss  0.013771713711321354\n",
            "Epoch  2 Batch  278 / 447  Training Loss  0.03350386768579483\n",
            "Epoch  2 Batch  279 / 447  Training Loss  0.0162192452698946\n",
            "Epoch  2 Batch  280 / 447  Training Loss  0.014579503796994686\n",
            "Epoch  2 Batch  281 / 447  Training Loss  0.02717590518295765\n",
            "Epoch  2 Batch  282 / 447  Training Loss  0.012891501188278198\n",
            "Epoch  2 Batch  283 / 447  Training Loss  0.03755367919802666\n",
            "Epoch  2 Batch  284 / 447  Training Loss  0.006668733432888985\n",
            "Epoch  2 Batch  285 / 447  Training Loss  0.008445261977612972\n",
            "Epoch  2 Batch  286 / 447  Training Loss  0.007140063215047121\n",
            "Epoch  2 Batch  287 / 447  Training Loss  0.016054868698120117\n",
            "Epoch  2 Batch  288 / 447  Training Loss  0.008708960376679897\n",
            "Epoch  2 Batch  289 / 447  Training Loss  0.01629490964114666\n",
            "Epoch  2 Batch  290 / 447  Training Loss  0.012965155765414238\n",
            "Epoch  2 Batch  291 / 447  Training Loss  0.00916372798383236\n",
            "Epoch  2 Batch  292 / 447  Training Loss  0.011179988272488117\n",
            "Epoch  2 Batch  293 / 447  Training Loss  0.015826355665922165\n",
            "Epoch  2 Batch  294 / 447  Training Loss  0.015569058246910572\n",
            "Epoch  2 Batch  295 / 447  Training Loss  0.009194377809762955\n",
            "Epoch  2 Batch  296 / 447  Training Loss  0.021755967289209366\n",
            "Epoch  2 Batch  297 / 447  Training Loss  0.00999198667705059\n",
            "Epoch  2 Batch  298 / 447  Training Loss  0.006442518439143896\n",
            "Epoch  2 Batch  299 / 447  Training Loss  0.00354761746712029\n",
            "Epoch  2 Batch  300 / 447  Training Loss  0.005084338132292032\n",
            "Epoch  2 Batch  301 / 447  Training Loss  0.010184258222579956\n",
            "Epoch  2 Batch  302 / 447  Training Loss  0.005668628960847855\n",
            "Epoch  2 Batch  303 / 447  Training Loss  0.005591496359556913\n",
            "Epoch  2 Batch  304 / 447  Training Loss  0.014901570975780487\n",
            "Epoch  2 Batch  305 / 447  Training Loss  0.017156746238470078\n",
            "Epoch  2 Batch  306 / 447  Training Loss  0.013956189155578613\n",
            "Epoch  2 Batch  307 / 447  Training Loss  0.0062092868611216545\n",
            "Epoch  2 Batch  308 / 447  Training Loss  0.013870214112102985\n",
            "Epoch  2 Batch  309 / 447  Training Loss  0.012294303625822067\n",
            "Epoch  2 Batch  310 / 447  Training Loss  0.044483914971351624\n",
            "Epoch  2 Batch  311 / 447  Training Loss  0.021025044843554497\n",
            "Epoch  2 Batch  312 / 447  Training Loss  0.017413515597581863\n",
            "Epoch  2 Batch  313 / 447  Training Loss  0.013609269633889198\n",
            "Epoch  2 Batch  314 / 447  Training Loss  0.024442747235298157\n",
            "Epoch  2 Batch  315 / 447  Training Loss  0.023311255499720573\n",
            "Epoch  2 Batch  316 / 447  Training Loss  0.015031609684228897\n",
            "Epoch  2 Batch  317 / 447  Training Loss  0.01321490854024887\n",
            "Epoch  2 Batch  318 / 447  Training Loss  0.015375634655356407\n",
            "Epoch  2 Batch  319 / 447  Training Loss  0.020265281200408936\n",
            "Epoch  2 Batch  320 / 447  Training Loss  0.013564646244049072\n",
            "Epoch  2 Batch  321 / 447  Training Loss  0.014309325255453587\n",
            "Epoch  2 Batch  322 / 447  Training Loss  0.010297605767846107\n",
            "Epoch  2 Batch  323 / 447  Training Loss  0.009533395990729332\n",
            "Epoch  2 Batch  324 / 447  Training Loss  0.011028898879885674\n",
            "Epoch  2 Batch  325 / 447  Training Loss  0.008473634719848633\n",
            "Epoch  2 Batch  326 / 447  Training Loss  0.013565381988883018\n",
            "Epoch  2 Batch  327 / 447  Training Loss  0.006927053444087505\n",
            "Epoch  2 Batch  328 / 447  Training Loss  0.007442925125360489\n",
            "Epoch  2 Batch  329 / 447  Training Loss  0.005415579304099083\n",
            "Epoch  2 Batch  330 / 447  Training Loss  0.018674319609999657\n",
            "Epoch  2 Batch  331 / 447  Training Loss  0.01758035644888878\n",
            "Epoch  2 Batch  332 / 447  Training Loss  0.01168904174119234\n",
            "Epoch  2 Batch  333 / 447  Training Loss  0.005999601446092129\n",
            "Epoch  2 Batch  334 / 447  Training Loss  0.004457218572497368\n",
            "Epoch  2 Batch  335 / 447  Training Loss  0.005247299559414387\n",
            "Epoch  2 Batch  336 / 447  Training Loss  0.005721617955714464\n",
            "Epoch  2 Batch  337 / 447  Training Loss  0.010164584964513779\n",
            "Epoch  2 Batch  338 / 447  Training Loss  0.004423799924552441\n",
            "Epoch  2 Batch  339 / 447  Training Loss  0.011341087520122528\n",
            "Epoch  2 Batch  340 / 447  Training Loss  0.006474171299487352\n",
            "Epoch  2 Batch  341 / 447  Training Loss  0.006735499016940594\n",
            "Epoch  2 Batch  342 / 447  Training Loss  0.010296721011400223\n",
            "Epoch  2 Batch  343 / 447  Training Loss  0.010130605660378933\n",
            "Epoch  2 Batch  344 / 447  Training Loss  0.013651961460709572\n",
            "Epoch  2 Batch  345 / 447  Training Loss  0.010084342211484909\n",
            "Epoch  2 Batch  346 / 447  Training Loss  0.00858529657125473\n",
            "Epoch  2 Batch  347 / 447  Training Loss  0.0035985768772661686\n",
            "Epoch  2 Batch  348 / 447  Training Loss  0.010699017904698849\n",
            "Epoch  2 Batch  349 / 447  Training Loss  0.0102384677156806\n",
            "Epoch  2 Batch  350 / 447  Training Loss  0.011013085953891277\n",
            "Epoch  2 Batch  351 / 447  Training Loss  0.009698877111077309\n",
            "Epoch  2 Batch  352 / 447  Training Loss  0.014022555202245712\n",
            "Epoch  2 Batch  353 / 447  Training Loss  0.00494805071502924\n",
            "Epoch  2 Batch  354 / 447  Training Loss  0.012997609563171864\n",
            "Epoch  2 Batch  355 / 447  Training Loss  0.0067452858202159405\n",
            "Epoch  2 Batch  356 / 447  Training Loss  0.0066440291702747345\n",
            "Epoch  2 Batch  357 / 447  Training Loss  0.011704642325639725\n",
            "Epoch  2 Batch  358 / 447  Training Loss  0.0164682324975729\n",
            "Epoch  2 Batch  359 / 447  Training Loss  0.02115217037498951\n",
            "Epoch  2 Batch  360 / 447  Training Loss  0.012036383152008057\n",
            "Epoch  2 Batch  361 / 447  Training Loss  0.003096691332757473\n",
            "Epoch  2 Batch  362 / 447  Training Loss  0.004221642855554819\n",
            "Epoch  2 Batch  363 / 447  Training Loss  0.002719963202252984\n",
            "Epoch  2 Batch  364 / 447  Training Loss  0.0031740956474095583\n",
            "Epoch  2 Batch  365 / 447  Training Loss  0.007915940135717392\n",
            "Epoch  2 Batch  366 / 447  Training Loss  0.008027820847928524\n",
            "Epoch  2 Batch  367 / 447  Training Loss  0.005870198830962181\n",
            "Epoch  2 Batch  368 / 447  Training Loss  0.0038947928696870804\n",
            "Epoch  2 Batch  369 / 447  Training Loss  0.0031897390726953745\n",
            "Epoch  2 Batch  370 / 447  Training Loss  0.005955057684332132\n",
            "Epoch  2 Batch  371 / 447  Training Loss  0.008063712157309055\n",
            "Epoch  2 Batch  372 / 447  Training Loss  0.01057989802211523\n",
            "Epoch  2 Batch  373 / 447  Training Loss  0.005144769325852394\n",
            "Epoch  2 Batch  374 / 447  Training Loss  0.01058157067745924\n",
            "Epoch  2 Batch  375 / 447  Training Loss  0.005232160910964012\n",
            "Epoch  2 Batch  376 / 447  Training Loss  0.005229592323303223\n",
            "Epoch  2 Batch  377 / 447  Training Loss  0.0038086993154138327\n",
            "Epoch  2 Batch  378 / 447  Training Loss  0.004698661155998707\n",
            "Epoch  2 Batch  379 / 447  Training Loss  0.008964848704636097\n",
            "Epoch  2 Batch  380 / 447  Training Loss  0.0059981513768434525\n",
            "Epoch  2 Batch  381 / 447  Training Loss  0.0037691316101700068\n",
            "Epoch  2 Batch  382 / 447  Training Loss  0.007162098772823811\n",
            "Epoch  2 Batch  383 / 447  Training Loss  0.008149891160428524\n",
            "Epoch  2 Batch  384 / 447  Training Loss  0.004777595866471529\n",
            "Epoch  2 Batch  385 / 447  Training Loss  0.009742064401507378\n",
            "Epoch  2 Batch  386 / 447  Training Loss  0.014583060517907143\n",
            "Epoch  2 Batch  387 / 447  Training Loss  0.009326265193521976\n",
            "Epoch  2 Batch  388 / 447  Training Loss  0.008551999926567078\n",
            "Epoch  2 Batch  389 / 447  Training Loss  0.006297900807112455\n",
            "Epoch  2 Batch  390 / 447  Training Loss  0.012247399426996708\n",
            "Epoch  2 Batch  391 / 447  Training Loss  0.0045392923057079315\n",
            "Epoch  2 Batch  392 / 447  Training Loss  0.004972470458596945\n",
            "Epoch  2 Batch  393 / 447  Training Loss  0.005296970717608929\n",
            "Epoch  2 Batch  394 / 447  Training Loss  0.008361132815480232\n",
            "Epoch  2 Batch  395 / 447  Training Loss  0.01432263944298029\n",
            "Epoch  2 Batch  396 / 447  Training Loss  0.008248575031757355\n",
            "Epoch  2 Batch  397 / 447  Training Loss  0.008803576231002808\n",
            "Epoch  2 Batch  398 / 447  Training Loss  0.00738699734210968\n",
            "Epoch  2 Batch  399 / 447  Training Loss  0.01700262352824211\n",
            "Epoch  2 Batch  400 / 447  Training Loss  0.02163473144173622\n",
            "Epoch  2 Batch  401 / 447  Training Loss  0.012208725325763226\n",
            "Epoch  2 Batch  402 / 447  Training Loss  0.011912520974874496\n",
            "Epoch  2 Batch  403 / 447  Training Loss  0.004641822539269924\n",
            "Epoch  2 Batch  404 / 447  Training Loss  0.0070980871096253395\n",
            "Epoch  2 Batch  405 / 447  Training Loss  0.005155757535248995\n",
            "Epoch  2 Batch  406 / 447  Training Loss  0.0053698038682341576\n",
            "Epoch  2 Batch  407 / 447  Training Loss  0.005807317327708006\n",
            "Epoch  2 Batch  408 / 447  Training Loss  0.013355902396142483\n",
            "Epoch  2 Batch  409 / 447  Training Loss  0.011010167188942432\n",
            "Epoch  2 Batch  410 / 447  Training Loss  0.004249161574989557\n",
            "Epoch  2 Batch  411 / 447  Training Loss  0.00880136713385582\n",
            "Epoch  2 Batch  412 / 447  Training Loss  0.01815926283597946\n",
            "Epoch  2 Batch  413 / 447  Training Loss  0.008778556250035763\n",
            "Epoch  2 Batch  414 / 447  Training Loss  0.007149301934987307\n",
            "Epoch  2 Batch  415 / 447  Training Loss  0.007640562020242214\n",
            "Epoch  2 Batch  416 / 447  Training Loss  0.009579435922205448\n",
            "Epoch  2 Batch  417 / 447  Training Loss  0.010005180723965168\n",
            "Epoch  2 Batch  418 / 447  Training Loss  0.007907898165285587\n",
            "Epoch  2 Batch  419 / 447  Training Loss  0.011778438463807106\n",
            "Epoch  2 Batch  420 / 447  Training Loss  0.003628490027040243\n",
            "Epoch  2 Batch  421 / 447  Training Loss  0.003336426569148898\n",
            "Epoch  2 Batch  422 / 447  Training Loss  0.016239115968346596\n",
            "Epoch  2 Batch  423 / 447  Training Loss  0.0048356144689023495\n",
            "Epoch  2 Batch  424 / 447  Training Loss  0.006909100338816643\n",
            "Epoch  2 Batch  425 / 447  Training Loss  0.0067128767259418964\n",
            "Epoch  2 Batch  426 / 447  Training Loss  0.002116234740242362\n",
            "Epoch  2 Batch  427 / 447  Training Loss  0.005898898467421532\n",
            "Epoch  2 Batch  428 / 447  Training Loss  0.008831235580146313\n",
            "Epoch  2 Batch  429 / 447  Training Loss  0.012921354733407497\n",
            "Epoch  2 Batch  430 / 447  Training Loss  0.003782901680096984\n",
            "Epoch  2 Batch  431 / 447  Training Loss  0.005701943766325712\n",
            "Epoch  2 Batch  432 / 447  Training Loss  0.0026498311199247837\n",
            "Epoch  2 Batch  433 / 447  Training Loss  0.003823904786258936\n",
            "Epoch  2 Batch  434 / 447  Training Loss  0.003854900598526001\n",
            "Epoch  2 Batch  435 / 447  Training Loss  0.006514762528240681\n",
            "Epoch  2 Batch  436 / 447  Training Loss  0.002113419584929943\n",
            "Epoch  2 Batch  437 / 447  Training Loss  0.0037698617670685053\n",
            "Epoch  2 Batch  438 / 447  Training Loss  0.004584196489304304\n",
            "Epoch  2 Batch  439 / 447  Training Loss  0.004755497444421053\n",
            "Epoch  2 Batch  440 / 447  Training Loss  0.004481566604226828\n",
            "Epoch  2 Batch  441 / 447  Training Loss  0.007058222312480211\n",
            "Epoch  2 Batch  442 / 447  Training Loss  0.002867473056539893\n",
            "Epoch  2 Batch  443 / 447  Training Loss  0.005246710032224655\n",
            "Epoch  2 Batch  444 / 447  Training Loss  0.0060002002865076065\n",
            "Epoch  2 Batch  445 / 447  Training Loss  0.017804445698857307\n",
            "Epoch  2 Batch  446 / 447  Training Loss  0.0023094567004591227\n",
            "   3    |    -    |   0.019161   | 95.768025\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 3\n",
            "Epoch  3 Batch  0 / 447  Training Loss  0.005288754124194384\n",
            "Epoch  3 Batch  1 / 447  Training Loss  0.0037097102031111717\n",
            "Epoch  3 Batch  2 / 447  Training Loss  0.001639812020584941\n",
            "Epoch  3 Batch  3 / 447  Training Loss  0.0027467841282486916\n",
            "Epoch  3 Batch  4 / 447  Training Loss  0.003690225537866354\n",
            "Epoch  3 Batch  5 / 447  Training Loss  0.00218345713801682\n",
            "Epoch  3 Batch  6 / 447  Training Loss  0.0018881562864407897\n",
            "Epoch  3 Batch  7 / 447  Training Loss  0.0015933045651763678\n",
            "Epoch  3 Batch  8 / 447  Training Loss  0.003317272989079356\n",
            "Epoch  3 Batch  9 / 447  Training Loss  0.007634554523974657\n",
            "Epoch  3 Batch  10 / 447  Training Loss  0.006760419812053442\n",
            "Epoch  3 Batch  11 / 447  Training Loss  0.0017377185868099332\n",
            "Epoch  3 Batch  12 / 447  Training Loss  0.005967415403574705\n",
            "Epoch  3 Batch  13 / 447  Training Loss  0.0019936528988182545\n",
            "Epoch  3 Batch  14 / 447  Training Loss  0.002788434736430645\n",
            "Epoch  3 Batch  15 / 447  Training Loss  0.0015634276205673814\n",
            "Epoch  3 Batch  16 / 447  Training Loss  0.004145137034356594\n",
            "Epoch  3 Batch  17 / 447  Training Loss  0.002305507892742753\n",
            "Epoch  3 Batch  18 / 447  Training Loss  0.0025532839354127645\n",
            "Epoch  3 Batch  19 / 447  Training Loss  0.002598317340016365\n",
            "Epoch  3 Batch  20 / 447  Training Loss  0.002633369294926524\n",
            "Epoch  3 Batch  21 / 447  Training Loss  0.0019013877026736736\n",
            "Epoch  3 Batch  22 / 447  Training Loss  0.002420190954580903\n",
            "Epoch  3 Batch  23 / 447  Training Loss  0.005136815831065178\n",
            "Epoch  3 Batch  24 / 447  Training Loss  0.006797478999942541\n",
            "Epoch  3 Batch  25 / 447  Training Loss  0.0050613367930054665\n",
            "Epoch  3 Batch  26 / 447  Training Loss  0.013591050170361996\n",
            "Epoch  3 Batch  27 / 447  Training Loss  0.004126126412302256\n",
            "Epoch  3 Batch  28 / 447  Training Loss  0.0038718553259968758\n",
            "Epoch  3 Batch  29 / 447  Training Loss  0.001617288333363831\n",
            "Epoch  3 Batch  30 / 447  Training Loss  0.005944380071014166\n",
            "Epoch  3 Batch  31 / 447  Training Loss  0.004200372379273176\n",
            "Epoch  3 Batch  32 / 447  Training Loss  0.0041068061254918575\n",
            "Epoch  3 Batch  33 / 447  Training Loss  0.005332833155989647\n",
            "Epoch  3 Batch  34 / 447  Training Loss  0.001818833057768643\n",
            "Epoch  3 Batch  35 / 447  Training Loss  0.003076681401580572\n",
            "Epoch  3 Batch  36 / 447  Training Loss  0.002090552356094122\n",
            "Epoch  3 Batch  37 / 447  Training Loss  0.004314072895795107\n",
            "Epoch  3 Batch  38 / 447  Training Loss  0.002788713900372386\n",
            "Epoch  3 Batch  39 / 447  Training Loss  0.003552832640707493\n",
            "Epoch  3 Batch  40 / 447  Training Loss  0.0036203903146088123\n",
            "Epoch  3 Batch  41 / 447  Training Loss  0.004873411264270544\n",
            "Epoch  3 Batch  42 / 447  Training Loss  0.0021049969363957644\n",
            "Epoch  3 Batch  43 / 447  Training Loss  0.003173272591084242\n",
            "Epoch  3 Batch  44 / 447  Training Loss  0.0027009740006178617\n",
            "Epoch  3 Batch  45 / 447  Training Loss  0.003459370695054531\n",
            "Epoch  3 Batch  46 / 447  Training Loss  0.00240379199385643\n",
            "Epoch  3 Batch  47 / 447  Training Loss  0.0032352081034332514\n",
            "Epoch  3 Batch  48 / 447  Training Loss  0.002291144570335746\n",
            "Epoch  3 Batch  49 / 447  Training Loss  0.001259910175576806\n",
            "Epoch  3 Batch  50 / 447  Training Loss  0.0025605943519622087\n",
            "Epoch  3 Batch  51 / 447  Training Loss  0.005404350813478231\n",
            "Epoch  3 Batch  52 / 447  Training Loss  0.009412997402250767\n",
            "Epoch  3 Batch  53 / 447  Training Loss  0.010840142145752907\n",
            "Epoch  3 Batch  54 / 447  Training Loss  0.004124498926103115\n",
            "Epoch  3 Batch  55 / 447  Training Loss  0.005321143660694361\n",
            "Epoch  3 Batch  56 / 447  Training Loss  0.0019215333741158247\n",
            "Epoch  3 Batch  57 / 447  Training Loss  0.003318732138723135\n",
            "Epoch  3 Batch  58 / 447  Training Loss  0.0011987356701865792\n",
            "Epoch  3 Batch  59 / 447  Training Loss  0.0013805544003844261\n",
            "Epoch  3 Batch  60 / 447  Training Loss  0.00100516970269382\n",
            "Epoch  3 Batch  61 / 447  Training Loss  0.0012790820328518748\n",
            "Epoch  3 Batch  62 / 447  Training Loss  0.004423346370458603\n",
            "Epoch  3 Batch  63 / 447  Training Loss  0.003255717223510146\n",
            "Epoch  3 Batch  64 / 447  Training Loss  0.006868553813546896\n",
            "Epoch  3 Batch  65 / 447  Training Loss  0.007629873231053352\n",
            "Epoch  3 Batch  66 / 447  Training Loss  0.0023614028468728065\n",
            "Epoch  3 Batch  67 / 447  Training Loss  0.004936316516250372\n",
            "Epoch  3 Batch  68 / 447  Training Loss  0.012867625802755356\n",
            "Epoch  3 Batch  69 / 447  Training Loss  0.009589293040335178\n",
            "Epoch  3 Batch  70 / 447  Training Loss  0.007795778568834066\n",
            "Epoch  3 Batch  71 / 447  Training Loss  0.01636062189936638\n",
            "Epoch  3 Batch  72 / 447  Training Loss  0.004006415139883757\n",
            "Epoch  3 Batch  73 / 447  Training Loss  0.0023729505483061075\n",
            "Epoch  3 Batch  74 / 447  Training Loss  0.0015931582311168313\n",
            "Epoch  3 Batch  75 / 447  Training Loss  0.006743505131453276\n",
            "Epoch  3 Batch  76 / 447  Training Loss  0.0018411308992654085\n",
            "Epoch  3 Batch  77 / 447  Training Loss  0.0022703981958329678\n",
            "Epoch  3 Batch  78 / 447  Training Loss  0.010379046201705933\n",
            "Epoch  3 Batch  79 / 447  Training Loss  0.033053625375032425\n",
            "Epoch  3 Batch  80 / 447  Training Loss  0.040298569947481155\n",
            "Epoch  3 Batch  81 / 447  Training Loss  0.015186762437224388\n",
            "Epoch  3 Batch  82 / 447  Training Loss  0.003803444793447852\n",
            "Epoch  3 Batch  83 / 447  Training Loss  0.0066085136495530605\n",
            "Epoch  3 Batch  84 / 447  Training Loss  0.010415870696306229\n",
            "Epoch  3 Batch  85 / 447  Training Loss  0.004688300658017397\n",
            "Epoch  3 Batch  86 / 447  Training Loss  0.005654989276081324\n",
            "Epoch  3 Batch  87 / 447  Training Loss  0.011635216884315014\n",
            "Epoch  3 Batch  88 / 447  Training Loss  0.018609708175063133\n",
            "Epoch  3 Batch  89 / 447  Training Loss  0.021820561960339546\n",
            "Epoch  3 Batch  90 / 447  Training Loss  0.0023061903193593025\n",
            "Epoch  3 Batch  91 / 447  Training Loss  0.018193328753113747\n",
            "Epoch  3 Batch  92 / 447  Training Loss  0.009668385609984398\n",
            "Epoch  3 Batch  93 / 447  Training Loss  0.0031179857905954123\n",
            "Epoch  3 Batch  94 / 447  Training Loss  0.005891740322113037\n",
            "Epoch  3 Batch  95 / 447  Training Loss  0.003009892301633954\n",
            "Epoch  3 Batch  96 / 447  Training Loss  0.00418639974668622\n",
            "Epoch  3 Batch  97 / 447  Training Loss  0.00848645530641079\n",
            "Epoch  3 Batch  98 / 447  Training Loss  0.0023864824324846268\n",
            "Epoch  3 Batch  99 / 447  Training Loss  0.004860800690948963\n",
            "Epoch  3 Batch  100 / 447  Training Loss  0.005468473304063082\n",
            "Epoch  3 Batch  101 / 447  Training Loss  0.00486100185662508\n",
            "Epoch  3 Batch  102 / 447  Training Loss  0.004039507359266281\n",
            "Epoch  3 Batch  103 / 447  Training Loss  0.004706738982349634\n",
            "Epoch  3 Batch  104 / 447  Training Loss  0.002529754303395748\n",
            "Epoch  3 Batch  105 / 447  Training Loss  0.001906416960991919\n",
            "Epoch  3 Batch  106 / 447  Training Loss  0.0013528730487450957\n",
            "Epoch  3 Batch  107 / 447  Training Loss  0.0014987858012318611\n",
            "Epoch  3 Batch  108 / 447  Training Loss  0.0025424701161682606\n",
            "Epoch  3 Batch  109 / 447  Training Loss  0.0029948712326586246\n",
            "Epoch  3 Batch  110 / 447  Training Loss  0.0019251784542575479\n",
            "Epoch  3 Batch  111 / 447  Training Loss  0.002555171260610223\n",
            "Epoch  3 Batch  112 / 447  Training Loss  0.004975166637450457\n",
            "Epoch  3 Batch  113 / 447  Training Loss  0.004426097963005304\n",
            "Epoch  3 Batch  114 / 447  Training Loss  0.003893605200573802\n",
            "Epoch  3 Batch  115 / 447  Training Loss  0.006072903983294964\n",
            "Epoch  3 Batch  116 / 447  Training Loss  0.0017645938787609339\n",
            "Epoch  3 Batch  117 / 447  Training Loss  0.0016785041661933064\n",
            "Epoch  3 Batch  118 / 447  Training Loss  0.0050749885849654675\n",
            "Epoch  3 Batch  119 / 447  Training Loss  0.003422744804993272\n",
            "Epoch  3 Batch  120 / 447  Training Loss  0.007171490229666233\n",
            "Epoch  3 Batch  121 / 447  Training Loss  0.003466457361355424\n",
            "Epoch  3 Batch  122 / 447  Training Loss  0.001733482233248651\n",
            "Epoch  3 Batch  123 / 447  Training Loss  0.001084627932868898\n",
            "Epoch  3 Batch  124 / 447  Training Loss  0.001618886017240584\n",
            "Epoch  3 Batch  125 / 447  Training Loss  0.003148414893075824\n",
            "Epoch  3 Batch  126 / 447  Training Loss  0.005730031058192253\n",
            "Epoch  3 Batch  127 / 447  Training Loss  0.0025991906877607107\n",
            "Epoch  3 Batch  128 / 447  Training Loss  0.0008555416716262698\n",
            "Epoch  3 Batch  129 / 447  Training Loss  0.0009052911191247404\n",
            "Epoch  3 Batch  130 / 447  Training Loss  0.0012547691585496068\n",
            "Epoch  3 Batch  131 / 447  Training Loss  0.0021816790103912354\n",
            "Epoch  3 Batch  132 / 447  Training Loss  0.0015293755568563938\n",
            "Epoch  3 Batch  133 / 447  Training Loss  0.00354274595156312\n",
            "Epoch  3 Batch  134 / 447  Training Loss  0.0016916844760999084\n",
            "Epoch  3 Batch  135 / 447  Training Loss  0.0011938627576455474\n",
            "Epoch  3 Batch  136 / 447  Training Loss  0.0023755328729748726\n",
            "Epoch  3 Batch  137 / 447  Training Loss  0.0007361980387941003\n",
            "Epoch  3 Batch  138 / 447  Training Loss  0.0013818496372550726\n",
            "Epoch  3 Batch  139 / 447  Training Loss  0.002076808363199234\n",
            "Epoch  3 Batch  140 / 447  Training Loss  0.0012712304014712572\n",
            "Epoch  3 Batch  141 / 447  Training Loss  0.004597931168973446\n",
            "Epoch  3 Batch  142 / 447  Training Loss  0.0035063668619841337\n",
            "Epoch  3 Batch  143 / 447  Training Loss  0.0008237480651587248\n",
            "Epoch  3 Batch  144 / 447  Training Loss  0.0007196002406999469\n",
            "Epoch  3 Batch  145 / 447  Training Loss  0.0011217374121770263\n",
            "Epoch  3 Batch  146 / 447  Training Loss  0.0008374031167477369\n",
            "Epoch  3 Batch  147 / 447  Training Loss  0.0018761649262160063\n",
            "Epoch  3 Batch  148 / 447  Training Loss  0.0016561199445277452\n",
            "Epoch  3 Batch  149 / 447  Training Loss  0.005405754316598177\n",
            "Epoch  3 Batch  150 / 447  Training Loss  0.0008376524783670902\n",
            "Epoch  3 Batch  151 / 447  Training Loss  0.0011140843853354454\n",
            "Epoch  3 Batch  152 / 447  Training Loss  0.0013811184326186776\n",
            "Epoch  3 Batch  153 / 447  Training Loss  0.0008453587652184069\n",
            "Epoch  3 Batch  154 / 447  Training Loss  0.0006345734000205994\n",
            "Epoch  3 Batch  155 / 447  Training Loss  0.0082719000056386\n",
            "Epoch  3 Batch  156 / 447  Training Loss  0.0008191982633434236\n",
            "Epoch  3 Batch  157 / 447  Training Loss  0.0015518541913479567\n",
            "Epoch  3 Batch  158 / 447  Training Loss  0.0010896835010498762\n",
            "Epoch  3 Batch  159 / 447  Training Loss  0.0014967068564146757\n",
            "Epoch  3 Batch  160 / 447  Training Loss  0.0023023122921586037\n",
            "Epoch  3 Batch  161 / 447  Training Loss  0.005732945632189512\n",
            "Epoch  3 Batch  162 / 447  Training Loss  0.0027437868993729353\n",
            "Epoch  3 Batch  163 / 447  Training Loss  0.0017861180240288377\n",
            "Epoch  3 Batch  164 / 447  Training Loss  0.002221264410763979\n",
            "Epoch  3 Batch  165 / 447  Training Loss  0.0018823284190148115\n",
            "Epoch  3 Batch  166 / 447  Training Loss  0.001550530199892819\n",
            "Epoch  3 Batch  167 / 447  Training Loss  0.0011125357123091817\n",
            "Epoch  3 Batch  168 / 447  Training Loss  0.0009156971354968846\n",
            "Epoch  3 Batch  169 / 447  Training Loss  0.0014009644510224462\n",
            "Epoch  3 Batch  170 / 447  Training Loss  0.0021934264805167913\n",
            "Epoch  3 Batch  171 / 447  Training Loss  0.0025881582405418158\n",
            "Epoch  3 Batch  172 / 447  Training Loss  0.0035914143081754446\n",
            "Epoch  3 Batch  173 / 447  Training Loss  0.0013300402788445354\n",
            "Epoch  3 Batch  174 / 447  Training Loss  0.001030234037898481\n",
            "Epoch  3 Batch  175 / 447  Training Loss  0.0069554829970002174\n",
            "Epoch  3 Batch  176 / 447  Training Loss  0.007758256047964096\n",
            "Epoch  3 Batch  177 / 447  Training Loss  0.002405149396508932\n",
            "Epoch  3 Batch  178 / 447  Training Loss  0.0013281875289976597\n",
            "Epoch  3 Batch  179 / 447  Training Loss  0.0020436164923012257\n",
            "Epoch  3 Batch  180 / 447  Training Loss  0.004050223156809807\n",
            "Epoch  3 Batch  181 / 447  Training Loss  0.0010915483580902219\n",
            "Epoch  3 Batch  182 / 447  Training Loss  0.0009385767043568194\n",
            "Epoch  3 Batch  183 / 447  Training Loss  0.007562443148344755\n",
            "Epoch  3 Batch  184 / 447  Training Loss  0.0013079155469313264\n",
            "Epoch  3 Batch  185 / 447  Training Loss  0.0009756042272783816\n",
            "Epoch  3 Batch  186 / 447  Training Loss  0.0008913978817872703\n",
            "Epoch  3 Batch  187 / 447  Training Loss  0.0007885829545557499\n",
            "Epoch  3 Batch  188 / 447  Training Loss  0.0007893487927503884\n",
            "Epoch  3 Batch  189 / 447  Training Loss  0.0007409666432067752\n",
            "Epoch  3 Batch  190 / 447  Training Loss  0.001322522759437561\n",
            "Epoch  3 Batch  191 / 447  Training Loss  0.0009799624094739556\n",
            "Epoch  3 Batch  192 / 447  Training Loss  0.0006849281489849091\n",
            "Epoch  3 Batch  193 / 447  Training Loss  0.0012143971398472786\n",
            "Epoch  3 Batch  194 / 447  Training Loss  0.0010673408396542072\n",
            "Epoch  3 Batch  195 / 447  Training Loss  0.002324142726138234\n",
            "Epoch  3 Batch  196 / 447  Training Loss  0.005078318063169718\n",
            "Epoch  3 Batch  197 / 447  Training Loss  0.0025103187654167414\n",
            "Epoch  3 Batch  198 / 447  Training Loss  0.0007397488225251436\n",
            "Epoch  3 Batch  199 / 447  Training Loss  0.00129127933178097\n",
            "Epoch  3 Batch  200 / 447  Training Loss  0.0014453036710619926\n",
            "Epoch  3 Batch  201 / 447  Training Loss  0.0027666466776281595\n",
            "Epoch  3 Batch  202 / 447  Training Loss  0.0011634628754109144\n",
            "Epoch  3 Batch  203 / 447  Training Loss  0.0008005262352526188\n",
            "Epoch  3 Batch  204 / 447  Training Loss  0.0009336102521046996\n",
            "Epoch  3 Batch  205 / 447  Training Loss  0.004771577659994364\n",
            "Epoch  3 Batch  206 / 447  Training Loss  0.0016845694044604897\n",
            "Epoch  3 Batch  207 / 447  Training Loss  0.0018172014970332384\n",
            "Epoch  3 Batch  208 / 447  Training Loss  0.001060249051079154\n",
            "Epoch  3 Batch  209 / 447  Training Loss  0.0030132851097732782\n",
            "Epoch  3 Batch  210 / 447  Training Loss  0.0025968337431550026\n",
            "Epoch  3 Batch  211 / 447  Training Loss  0.0038171643391251564\n",
            "Epoch  3 Batch  212 / 447  Training Loss  0.0037477698642760515\n",
            "Epoch  3 Batch  213 / 447  Training Loss  0.0007481980137526989\n",
            "Epoch  3 Batch  214 / 447  Training Loss  0.0038048436399549246\n",
            "Epoch  3 Batch  215 / 447  Training Loss  0.0036050137132406235\n",
            "Epoch  3 Batch  216 / 447  Training Loss  0.005113525316119194\n",
            "Epoch  3 Batch  217 / 447  Training Loss  0.0011881684185937047\n",
            "Epoch  3 Batch  218 / 447  Training Loss  0.002298091072589159\n",
            "Epoch  3 Batch  219 / 447  Training Loss  0.002444458194077015\n",
            "Epoch  3 Batch  220 / 447  Training Loss  0.0018786605214700103\n",
            "Epoch  3 Batch  221 / 447  Training Loss  0.001294581452384591\n",
            "Epoch  3 Batch  222 / 447  Training Loss  0.0036691036075353622\n",
            "Epoch  3 Batch  223 / 447  Training Loss  0.0031312913633883\n",
            "Epoch  3 Batch  224 / 447  Training Loss  0.0011887385044246912\n",
            "Epoch  3 Batch  225 / 447  Training Loss  0.0008336877799592912\n",
            "Epoch  3 Batch  226 / 447  Training Loss  0.0008725050138309598\n",
            "Epoch  3 Batch  227 / 447  Training Loss  0.0013077238108962774\n",
            "Epoch  3 Batch  228 / 447  Training Loss  0.0011391486041247845\n",
            "Epoch  3 Batch  229 / 447  Training Loss  0.0007539716898463666\n",
            "Epoch  3 Batch  230 / 447  Training Loss  0.0014539844123646617\n",
            "Epoch  3 Batch  231 / 447  Training Loss  0.0009090235689654946\n",
            "Epoch  3 Batch  232 / 447  Training Loss  0.0005673307459801435\n",
            "Epoch  3 Batch  233 / 447  Training Loss  0.0008833286119624972\n",
            "Epoch  3 Batch  234 / 447  Training Loss  0.0011044868733733892\n",
            "Epoch  3 Batch  235 / 447  Training Loss  0.001282643643207848\n",
            "Epoch  3 Batch  236 / 447  Training Loss  0.0006397220422513783\n",
            "Epoch  3 Batch  237 / 447  Training Loss  0.0008302054484374821\n",
            "Epoch  3 Batch  238 / 447  Training Loss  0.0013319168938323855\n",
            "Epoch  3 Batch  239 / 447  Training Loss  0.0006924490444362164\n",
            "Epoch  3 Batch  240 / 447  Training Loss  0.0009112664847634733\n",
            "Epoch  3 Batch  241 / 447  Training Loss  0.0008680770406499505\n",
            "Epoch  3 Batch  242 / 447  Training Loss  0.0008647691574878991\n",
            "Epoch  3 Batch  243 / 447  Training Loss  0.000605350942350924\n",
            "Epoch  3 Batch  244 / 447  Training Loss  0.004422160796821117\n",
            "Epoch  3 Batch  245 / 447  Training Loss  0.006160834804177284\n",
            "Epoch  3 Batch  246 / 447  Training Loss  0.0012239650823175907\n",
            "Epoch  3 Batch  247 / 447  Training Loss  0.0026122871786355972\n",
            "Epoch  3 Batch  248 / 447  Training Loss  0.0011270138202235103\n",
            "Epoch  3 Batch  249 / 447  Training Loss  0.0025699224788695574\n",
            "Epoch  3 Batch  250 / 447  Training Loss  0.007672457490116358\n",
            "Epoch  3 Batch  251 / 447  Training Loss  0.001074233208782971\n",
            "Epoch  3 Batch  252 / 447  Training Loss  0.0021024022717028856\n",
            "Epoch  3 Batch  253 / 447  Training Loss  0.0006087017245590687\n",
            "Epoch  3 Batch  254 / 447  Training Loss  0.0010751873487606645\n",
            "Epoch  3 Batch  255 / 447  Training Loss  0.0005785975372418761\n",
            "Epoch  3 Batch  256 / 447  Training Loss  0.0012604414951056242\n",
            "Epoch  3 Batch  257 / 447  Training Loss  0.0006007409538142383\n",
            "Epoch  3 Batch  258 / 447  Training Loss  0.00044767686631530523\n",
            "Epoch  3 Batch  259 / 447  Training Loss  0.0034958156757056713\n",
            "Epoch  3 Batch  260 / 447  Training Loss  0.0006233501480892301\n",
            "Epoch  3 Batch  261 / 447  Training Loss  0.00120555202011019\n",
            "Epoch  3 Batch  262 / 447  Training Loss  0.0036927610635757446\n",
            "Epoch  3 Batch  263 / 447  Training Loss  0.000678776006679982\n",
            "Epoch  3 Batch  264 / 447  Training Loss  0.0038298286963254213\n",
            "Epoch  3 Batch  265 / 447  Training Loss  0.002762947930023074\n",
            "Epoch  3 Batch  266 / 447  Training Loss  0.003713139332830906\n",
            "Epoch  3 Batch  267 / 447  Training Loss  0.0006410761852748692\n",
            "Epoch  3 Batch  268 / 447  Training Loss  0.003848468419164419\n",
            "Epoch  3 Batch  269 / 447  Training Loss  0.0073678032495081425\n",
            "Epoch  3 Batch  270 / 447  Training Loss  0.0011455857893452048\n",
            "Epoch  3 Batch  271 / 447  Training Loss  0.00112436106428504\n",
            "Epoch  3 Batch  272 / 447  Training Loss  0.0005006325663998723\n",
            "Epoch  3 Batch  273 / 447  Training Loss  0.008472912944853306\n",
            "Epoch  3 Batch  274 / 447  Training Loss  0.006129266694188118\n",
            "Epoch  3 Batch  275 / 447  Training Loss  0.001133125158958137\n",
            "Epoch  3 Batch  276 / 447  Training Loss  0.004598697647452354\n",
            "Epoch  3 Batch  277 / 447  Training Loss  0.0012150718830525875\n",
            "Epoch  3 Batch  278 / 447  Training Loss  0.000775160442572087\n",
            "Epoch  3 Batch  279 / 447  Training Loss  0.001783738611266017\n",
            "Epoch  3 Batch  280 / 447  Training Loss  0.0007387330988422036\n",
            "Epoch  3 Batch  281 / 447  Training Loss  0.0030644305516034365\n",
            "Epoch  3 Batch  282 / 447  Training Loss  0.0008538336842320859\n",
            "Epoch  3 Batch  283 / 447  Training Loss  0.0021815882064402103\n",
            "Epoch  3 Batch  284 / 447  Training Loss  0.0006749831372871995\n",
            "Epoch  3 Batch  285 / 447  Training Loss  0.0006502052419818938\n",
            "Epoch  3 Batch  286 / 447  Training Loss  0.0007725711911916733\n",
            "Epoch  3 Batch  287 / 447  Training Loss  0.00370282051153481\n",
            "Epoch  3 Batch  288 / 447  Training Loss  0.002850943012163043\n",
            "Epoch  3 Batch  289 / 447  Training Loss  0.0009626444661989808\n",
            "Epoch  3 Batch  290 / 447  Training Loss  0.0009000778081826866\n",
            "Epoch  3 Batch  291 / 447  Training Loss  0.0005403787363320589\n",
            "Epoch  3 Batch  292 / 447  Training Loss  0.0014321848284453154\n",
            "Epoch  3 Batch  293 / 447  Training Loss  0.0017868010327219963\n",
            "Epoch  3 Batch  294 / 447  Training Loss  0.0005039399839006364\n",
            "Epoch  3 Batch  295 / 447  Training Loss  0.001551358262076974\n",
            "Epoch  3 Batch  296 / 447  Training Loss  0.0012235654285177588\n",
            "Epoch  3 Batch  297 / 447  Training Loss  0.0006450887885876\n",
            "Epoch  3 Batch  298 / 447  Training Loss  0.000549742195289582\n",
            "Epoch  3 Batch  299 / 447  Training Loss  0.0012290815357118845\n",
            "Epoch  3 Batch  300 / 447  Training Loss  0.0024347100406885147\n",
            "Epoch  3 Batch  301 / 447  Training Loss  0.003704686416313052\n",
            "Epoch  3 Batch  302 / 447  Training Loss  0.004614289849996567\n",
            "Epoch  3 Batch  303 / 447  Training Loss  0.0018447847105562687\n",
            "Epoch  3 Batch  304 / 447  Training Loss  0.002456321381032467\n",
            "Epoch  3 Batch  305 / 447  Training Loss  0.0006538993329741061\n",
            "Epoch  3 Batch  306 / 447  Training Loss  0.000518816988915205\n",
            "Epoch  3 Batch  307 / 447  Training Loss  0.000632070004940033\n",
            "Epoch  3 Batch  308 / 447  Training Loss  0.0006097633158788085\n",
            "Epoch  3 Batch  309 / 447  Training Loss  0.0015062476741150022\n",
            "Epoch  3 Batch  310 / 447  Training Loss  0.0009944759076461196\n",
            "Epoch  3 Batch  311 / 447  Training Loss  0.0009826610330492258\n",
            "Epoch  3 Batch  312 / 447  Training Loss  0.0009945629863068461\n",
            "Epoch  3 Batch  313 / 447  Training Loss  0.000718509778380394\n",
            "Epoch  3 Batch  314 / 447  Training Loss  0.0005089223268441856\n",
            "Epoch  3 Batch  315 / 447  Training Loss  0.0014452188042923808\n",
            "Epoch  3 Batch  316 / 447  Training Loss  0.0009078406728804111\n",
            "Epoch  3 Batch  317 / 447  Training Loss  0.0033618479501456022\n",
            "Epoch  3 Batch  318 / 447  Training Loss  0.010490803979337215\n",
            "Epoch  3 Batch  319 / 447  Training Loss  0.006156422663480043\n",
            "Epoch  3 Batch  320 / 447  Training Loss  0.001384343602694571\n",
            "Epoch  3 Batch  321 / 447  Training Loss  0.0018467300105839968\n",
            "Epoch  3 Batch  322 / 447  Training Loss  0.0007871518027968705\n",
            "Epoch  3 Batch  323 / 447  Training Loss  0.007201169151812792\n",
            "Epoch  3 Batch  324 / 447  Training Loss  0.007750412914901972\n",
            "Epoch  3 Batch  325 / 447  Training Loss  0.0007721271249465644\n",
            "Epoch  3 Batch  326 / 447  Training Loss  0.00370758306235075\n",
            "Epoch  3 Batch  327 / 447  Training Loss  0.0006898615974932909\n",
            "Epoch  3 Batch  328 / 447  Training Loss  0.0010435045696794987\n",
            "Epoch  3 Batch  329 / 447  Training Loss  0.0008529015467502177\n",
            "Epoch  3 Batch  330 / 447  Training Loss  0.002172391628846526\n",
            "Epoch  3 Batch  331 / 447  Training Loss  0.00129511090926826\n",
            "Epoch  3 Batch  332 / 447  Training Loss  0.0024342006072402\n",
            "Epoch  3 Batch  333 / 447  Training Loss  0.0012795844813808799\n",
            "Epoch  3 Batch  334 / 447  Training Loss  0.0009223114466294646\n",
            "Epoch  3 Batch  335 / 447  Training Loss  0.0003886161430273205\n",
            "Epoch  3 Batch  336 / 447  Training Loss  0.000833831902127713\n",
            "Epoch  3 Batch  337 / 447  Training Loss  0.0007215501391328871\n",
            "Epoch  3 Batch  338 / 447  Training Loss  0.0008217477006837726\n",
            "Epoch  3 Batch  339 / 447  Training Loss  0.0011117515387013555\n",
            "Epoch  3 Batch  340 / 447  Training Loss  0.004205473233014345\n",
            "Epoch  3 Batch  341 / 447  Training Loss  0.00043331243796274066\n",
            "Epoch  3 Batch  342 / 447  Training Loss  0.0010295838583260775\n",
            "Epoch  3 Batch  343 / 447  Training Loss  0.0007259467383846641\n",
            "Epoch  3 Batch  344 / 447  Training Loss  0.0006272587925195694\n",
            "Epoch  3 Batch  345 / 447  Training Loss  0.0011013559997081757\n",
            "Epoch  3 Batch  346 / 447  Training Loss  0.001230360008776188\n",
            "Epoch  3 Batch  347 / 447  Training Loss  0.00043290184112265706\n",
            "Epoch  3 Batch  348 / 447  Training Loss  0.0007891457644291222\n",
            "Epoch  3 Batch  349 / 447  Training Loss  0.0009339210228063166\n",
            "Epoch  3 Batch  350 / 447  Training Loss  0.0017632507951930165\n",
            "Epoch  3 Batch  351 / 447  Training Loss  0.0005785293760709465\n",
            "Epoch  3 Batch  352 / 447  Training Loss  0.0007345688063651323\n",
            "Epoch  3 Batch  353 / 447  Training Loss  0.0007175622158683836\n",
            "Epoch  3 Batch  354 / 447  Training Loss  0.0005793247255496681\n",
            "Epoch  3 Batch  355 / 447  Training Loss  0.0003950086538679898\n",
            "Epoch  3 Batch  356 / 447  Training Loss  0.00047227469622157514\n",
            "Epoch  3 Batch  357 / 447  Training Loss  0.0004575502243824303\n",
            "Epoch  3 Batch  358 / 447  Training Loss  0.00318880844861269\n",
            "Epoch  3 Batch  359 / 447  Training Loss  0.0005737997707910836\n",
            "Epoch  3 Batch  360 / 447  Training Loss  0.0013460925547406077\n",
            "Epoch  3 Batch  361 / 447  Training Loss  0.0006539223832078278\n",
            "Epoch  3 Batch  362 / 447  Training Loss  0.0007925824029371142\n",
            "Epoch  3 Batch  363 / 447  Training Loss  0.00048296910244971514\n",
            "Epoch  3 Batch  364 / 447  Training Loss  0.0026140741538256407\n",
            "Epoch  3 Batch  365 / 447  Training Loss  0.0006729660672135651\n",
            "Epoch  3 Batch  366 / 447  Training Loss  0.0004233267973177135\n",
            "Epoch  3 Batch  367 / 447  Training Loss  0.0003531187539920211\n",
            "Epoch  3 Batch  368 / 447  Training Loss  0.00035885386751033366\n",
            "Epoch  3 Batch  369 / 447  Training Loss  0.0004148232692386955\n",
            "Epoch  3 Batch  370 / 447  Training Loss  0.000462475378299132\n",
            "Epoch  3 Batch  371 / 447  Training Loss  0.0003759760002139956\n",
            "Epoch  3 Batch  372 / 447  Training Loss  0.0008465389255434275\n",
            "Epoch  3 Batch  373 / 447  Training Loss  0.00045514001976698637\n",
            "Epoch  3 Batch  374 / 447  Training Loss  0.0004872992285527289\n",
            "Epoch  3 Batch  375 / 447  Training Loss  0.0015638157492503524\n",
            "Epoch  3 Batch  376 / 447  Training Loss  0.001764872227795422\n",
            "Epoch  3 Batch  377 / 447  Training Loss  0.0012276339111849666\n",
            "Epoch  3 Batch  378 / 447  Training Loss  0.003062430303543806\n",
            "Epoch  3 Batch  379 / 447  Training Loss  0.002156629227101803\n",
            "Epoch  3 Batch  380 / 447  Training Loss  0.0008832555031403899\n",
            "Epoch  3 Batch  381 / 447  Training Loss  0.0005141229485161602\n",
            "Epoch  3 Batch  382 / 447  Training Loss  0.0005055617075413465\n",
            "Epoch  3 Batch  383 / 447  Training Loss  0.0003424830501899123\n",
            "Epoch  3 Batch  384 / 447  Training Loss  0.000615945493336767\n",
            "Epoch  3 Batch  385 / 447  Training Loss  0.0006547286175191402\n",
            "Epoch  3 Batch  386 / 447  Training Loss  0.0007170860189944506\n",
            "Epoch  3 Batch  387 / 447  Training Loss  0.0013996013440191746\n",
            "Epoch  3 Batch  388 / 447  Training Loss  0.0005070737097412348\n",
            "Epoch  3 Batch  389 / 447  Training Loss  0.0007037661271169782\n",
            "Epoch  3 Batch  390 / 447  Training Loss  0.00029047360294498503\n",
            "Epoch  3 Batch  391 / 447  Training Loss  0.0005149448988959193\n",
            "Epoch  3 Batch  392 / 447  Training Loss  0.0005997737753205001\n",
            "Epoch  3 Batch  393 / 447  Training Loss  0.0003483403124846518\n",
            "Epoch  3 Batch  394 / 447  Training Loss  0.0005539474659599364\n",
            "Epoch  3 Batch  395 / 447  Training Loss  0.0005520702688954771\n",
            "Epoch  3 Batch  396 / 447  Training Loss  0.0016483886865898967\n",
            "Epoch  3 Batch  397 / 447  Training Loss  0.0009700353257358074\n",
            "Epoch  3 Batch  398 / 447  Training Loss  0.00031846933416090906\n",
            "Epoch  3 Batch  399 / 447  Training Loss  0.00046638809726573527\n",
            "Epoch  3 Batch  400 / 447  Training Loss  0.0006659236387349665\n",
            "Epoch  3 Batch  401 / 447  Training Loss  0.00037678491207771003\n",
            "Epoch  3 Batch  402 / 447  Training Loss  0.0003866358019877225\n",
            "Epoch  3 Batch  403 / 447  Training Loss  0.00036927603650838137\n",
            "Epoch  3 Batch  404 / 447  Training Loss  0.0017650005174800754\n",
            "Epoch  3 Batch  405 / 447  Training Loss  0.0005186751950532198\n",
            "Epoch  3 Batch  406 / 447  Training Loss  0.0006461438606493175\n",
            "Epoch  3 Batch  407 / 447  Training Loss  0.0005766592803411186\n",
            "Epoch  3 Batch  408 / 447  Training Loss  0.0005848253495059907\n",
            "Epoch  3 Batch  409 / 447  Training Loss  0.0005997030530124903\n",
            "Epoch  3 Batch  410 / 447  Training Loss  0.00045825954293832183\n",
            "Epoch  3 Batch  411 / 447  Training Loss  0.00034537899773567915\n",
            "Epoch  3 Batch  412 / 447  Training Loss  0.0014724077191203833\n",
            "Epoch  3 Batch  413 / 447  Training Loss  0.00038027233676984906\n",
            "Epoch  3 Batch  414 / 447  Training Loss  0.0009295960771851242\n",
            "Epoch  3 Batch  415 / 447  Training Loss  0.0003633032029028982\n",
            "Epoch  3 Batch  416 / 447  Training Loss  0.0004359491867944598\n",
            "Epoch  3 Batch  417 / 447  Training Loss  0.0005372815066948533\n",
            "Epoch  3 Batch  418 / 447  Training Loss  0.0005053632194176316\n",
            "Epoch  3 Batch  419 / 447  Training Loss  0.0006176225724630058\n",
            "Epoch  3 Batch  420 / 447  Training Loss  0.0005516167148016393\n",
            "Epoch  3 Batch  421 / 447  Training Loss  0.0012714971089735627\n",
            "Epoch  3 Batch  422 / 447  Training Loss  0.0005797527846880257\n",
            "Epoch  3 Batch  423 / 447  Training Loss  0.0021383685525506735\n",
            "Epoch  3 Batch  424 / 447  Training Loss  0.00027433072682470083\n",
            "Epoch  3 Batch  425 / 447  Training Loss  0.00039018341340124607\n",
            "Epoch  3 Batch  426 / 447  Training Loss  0.00041443906957283616\n",
            "Epoch  3 Batch  427 / 447  Training Loss  0.0008753251167945564\n",
            "Epoch  3 Batch  428 / 447  Training Loss  0.000732993648853153\n",
            "Epoch  3 Batch  429 / 447  Training Loss  0.0007407998200505972\n",
            "Epoch  3 Batch  430 / 447  Training Loss  0.001969710225239396\n",
            "Epoch  3 Batch  431 / 447  Training Loss  0.0006357113015837967\n",
            "Epoch  3 Batch  432 / 447  Training Loss  0.00021898133854847401\n",
            "Epoch  3 Batch  433 / 447  Training Loss  0.0003631644358392805\n",
            "Epoch  3 Batch  434 / 447  Training Loss  0.0004608484741766006\n",
            "Epoch  3 Batch  435 / 447  Training Loss  0.0003989092365372926\n",
            "Epoch  3 Batch  436 / 447  Training Loss  0.00040631782030686736\n",
            "Epoch  3 Batch  437 / 447  Training Loss  0.0003728506271727383\n",
            "Epoch  3 Batch  438 / 447  Training Loss  0.00048086215974763036\n",
            "Epoch  3 Batch  439 / 447  Training Loss  0.00045951190986670554\n",
            "Epoch  3 Batch  440 / 447  Training Loss  0.00046655177720822394\n",
            "Epoch  3 Batch  441 / 447  Training Loss  0.001253402093425393\n",
            "Epoch  3 Batch  442 / 447  Training Loss  0.0037378177512437105\n",
            "Epoch  3 Batch  443 / 447  Training Loss  0.0004549257573671639\n",
            "Epoch  3 Batch  444 / 447  Training Loss  0.00041949504520744085\n",
            "Epoch  3 Batch  445 / 447  Training Loss  0.00045681773917749524\n",
            "Epoch  3 Batch  446 / 447  Training Loss  0.0003798376419581473\n",
            "   4    |    -    |   0.002636   | 98.648119\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 4\n",
            "Epoch  4 Batch  0 / 447  Training Loss  0.0017529960023239255\n",
            "Epoch  4 Batch  1 / 447  Training Loss  0.012847509235143661\n",
            "Epoch  4 Batch  2 / 447  Training Loss  0.09686139971017838\n",
            "Epoch  4 Batch  3 / 447  Training Loss  0.051356539130210876\n",
            "Epoch  4 Batch  4 / 447  Training Loss  0.044417932629585266\n",
            "Epoch  4 Batch  5 / 447  Training Loss  0.029502158984541893\n",
            "Epoch  4 Batch  6 / 447  Training Loss  0.027825966477394104\n",
            "Epoch  4 Batch  7 / 447  Training Loss  0.012185783125460148\n",
            "Epoch  4 Batch  8 / 447  Training Loss  0.0076737613417208195\n",
            "Epoch  4 Batch  9 / 447  Training Loss  0.012796349823474884\n",
            "Epoch  4 Batch  10 / 447  Training Loss  0.0054038590751588345\n",
            "Epoch  4 Batch  11 / 447  Training Loss  0.010866924189031124\n",
            "Epoch  4 Batch  12 / 447  Training Loss  0.00444989325478673\n",
            "Epoch  4 Batch  13 / 447  Training Loss  0.002934906864538789\n",
            "Epoch  4 Batch  14 / 447  Training Loss  0.0008521191193722188\n",
            "Epoch  4 Batch  15 / 447  Training Loss  0.00915644783526659\n",
            "Epoch  4 Batch  16 / 447  Training Loss  0.0022242784034460783\n",
            "Epoch  4 Batch  17 / 447  Training Loss  0.0021884331945329905\n",
            "Epoch  4 Batch  18 / 447  Training Loss  0.002305502537637949\n",
            "Epoch  4 Batch  19 / 447  Training Loss  0.005328507628291845\n",
            "Epoch  4 Batch  20 / 447  Training Loss  0.013137806206941605\n",
            "Epoch  4 Batch  21 / 447  Training Loss  0.013370407745242119\n",
            "Epoch  4 Batch  22 / 447  Training Loss  0.00905612763017416\n",
            "Epoch  4 Batch  23 / 447  Training Loss  0.020496558398008347\n",
            "Epoch  4 Batch  24 / 447  Training Loss  0.015468593686819077\n",
            "Epoch  4 Batch  25 / 447  Training Loss  0.0019075858872383833\n",
            "Epoch  4 Batch  26 / 447  Training Loss  0.0071627311408519745\n",
            "Epoch  4 Batch  27 / 447  Training Loss  0.0036069939378648996\n",
            "Epoch  4 Batch  28 / 447  Training Loss  0.0015884526073932648\n",
            "Epoch  4 Batch  29 / 447  Training Loss  0.002307618735358119\n",
            "Epoch  4 Batch  30 / 447  Training Loss  0.002414281014353037\n",
            "Epoch  4 Batch  31 / 447  Training Loss  0.0016231570625677705\n",
            "Epoch  4 Batch  32 / 447  Training Loss  0.001209115143865347\n",
            "Epoch  4 Batch  33 / 447  Training Loss  0.0024177441373467445\n",
            "Epoch  4 Batch  34 / 447  Training Loss  0.0012109850067645311\n",
            "Epoch  4 Batch  35 / 447  Training Loss  0.001573497662320733\n",
            "Epoch  4 Batch  36 / 447  Training Loss  0.0025117972400039434\n",
            "Epoch  4 Batch  37 / 447  Training Loss  0.0016079982742667198\n",
            "Epoch  4 Batch  38 / 447  Training Loss  0.0026454050093889236\n",
            "Epoch  4 Batch  39 / 447  Training Loss  0.001331811654381454\n",
            "Epoch  4 Batch  40 / 447  Training Loss  0.004087321925908327\n",
            "Epoch  4 Batch  41 / 447  Training Loss  0.004335426259785891\n",
            "Epoch  4 Batch  42 / 447  Training Loss  0.005446120630949736\n",
            "Epoch  4 Batch  43 / 447  Training Loss  0.0013962459051981568\n",
            "Epoch  4 Batch  44 / 447  Training Loss  0.005099869333207607\n",
            "Epoch  4 Batch  45 / 447  Training Loss  0.0034237029030919075\n",
            "Epoch  4 Batch  46 / 447  Training Loss  0.0037549170665442944\n",
            "Epoch  4 Batch  47 / 447  Training Loss  0.006272447295486927\n",
            "Epoch  4 Batch  48 / 447  Training Loss  0.029308054596185684\n",
            "Epoch  4 Batch  49 / 447  Training Loss  0.003125307848677039\n",
            "Epoch  4 Batch  50 / 447  Training Loss  0.003697390202432871\n",
            "Epoch  4 Batch  51 / 447  Training Loss  0.008458570577204227\n",
            "Epoch  4 Batch  52 / 447  Training Loss  0.0008222824544645846\n",
            "Epoch  4 Batch  53 / 447  Training Loss  0.0017135686939582229\n",
            "Epoch  4 Batch  54 / 447  Training Loss  0.0007546291453763843\n",
            "Epoch  4 Batch  55 / 447  Training Loss  0.002637381199747324\n",
            "Epoch  4 Batch  56 / 447  Training Loss  0.005259500350803137\n",
            "Epoch  4 Batch  57 / 447  Training Loss  0.003386896336451173\n",
            "Epoch  4 Batch  58 / 447  Training Loss  0.008038575761020184\n",
            "Epoch  4 Batch  59 / 447  Training Loss  0.0017037598881870508\n",
            "Epoch  4 Batch  60 / 447  Training Loss  0.0010012908605858684\n",
            "Epoch  4 Batch  61 / 447  Training Loss  0.0013562622480094433\n",
            "Epoch  4 Batch  62 / 447  Training Loss  0.0017366234678775072\n",
            "Epoch  4 Batch  63 / 447  Training Loss  0.002234248211607337\n",
            "Epoch  4 Batch  64 / 447  Training Loss  0.0010731217917054892\n",
            "Epoch  4 Batch  65 / 447  Training Loss  0.005224746651947498\n",
            "Epoch  4 Batch  66 / 447  Training Loss  0.0008760530035942793\n",
            "Epoch  4 Batch  67 / 447  Training Loss  0.0014763479121029377\n",
            "Epoch  4 Batch  68 / 447  Training Loss  0.006178140174597502\n",
            "Epoch  4 Batch  69 / 447  Training Loss  0.000817598425783217\n",
            "Epoch  4 Batch  70 / 447  Training Loss  0.004581895656883717\n",
            "Epoch  4 Batch  71 / 447  Training Loss  0.0022465824149549007\n",
            "Epoch  4 Batch  72 / 447  Training Loss  0.0026307108346372843\n",
            "Epoch  4 Batch  73 / 447  Training Loss  0.0009294085903093219\n",
            "Epoch  4 Batch  74 / 447  Training Loss  0.0004313396057114005\n",
            "Epoch  4 Batch  75 / 447  Training Loss  0.00702829472720623\n",
            "Epoch  4 Batch  76 / 447  Training Loss  0.0015661438228562474\n",
            "Epoch  4 Batch  77 / 447  Training Loss  0.001611721352674067\n",
            "Epoch  4 Batch  78 / 447  Training Loss  0.0029640791472047567\n",
            "Epoch  4 Batch  79 / 447  Training Loss  0.004893707111477852\n",
            "Epoch  4 Batch  80 / 447  Training Loss  0.0007826280780136585\n",
            "Epoch  4 Batch  81 / 447  Training Loss  0.00115560507401824\n",
            "Epoch  4 Batch  82 / 447  Training Loss  0.0026047141291201115\n",
            "Epoch  4 Batch  83 / 447  Training Loss  0.002667027059942484\n",
            "Epoch  4 Batch  84 / 447  Training Loss  0.0076592606492340565\n",
            "Epoch  4 Batch  85 / 447  Training Loss  0.0021257088519632816\n",
            "Epoch  4 Batch  86 / 447  Training Loss  0.001377489767037332\n",
            "Epoch  4 Batch  87 / 447  Training Loss  0.0026804537046700716\n",
            "Epoch  4 Batch  88 / 447  Training Loss  0.0032563158310949802\n",
            "Epoch  4 Batch  89 / 447  Training Loss  0.0017294292338192463\n",
            "Epoch  4 Batch  90 / 447  Training Loss  0.004804822616279125\n",
            "Epoch  4 Batch  91 / 447  Training Loss  0.001040291623212397\n",
            "Epoch  4 Batch  92 / 447  Training Loss  0.0010037575848400593\n",
            "Epoch  4 Batch  93 / 447  Training Loss  0.0019622978288680315\n",
            "Epoch  4 Batch  94 / 447  Training Loss  0.004116919357329607\n",
            "Epoch  4 Batch  95 / 447  Training Loss  0.004471639171242714\n",
            "Epoch  4 Batch  96 / 447  Training Loss  0.0011102019343525171\n",
            "Epoch  4 Batch  97 / 447  Training Loss  0.0011844929540529847\n",
            "Epoch  4 Batch  98 / 447  Training Loss  0.0005907473969273269\n",
            "Epoch  4 Batch  99 / 447  Training Loss  0.0013975712936371565\n",
            "Epoch  4 Batch  100 / 447  Training Loss  0.0013991347514092922\n",
            "Epoch  4 Batch  101 / 447  Training Loss  0.0007050110143609345\n",
            "Epoch  4 Batch  102 / 447  Training Loss  0.0008757156319916248\n",
            "Epoch  4 Batch  103 / 447  Training Loss  0.001132685923948884\n",
            "Epoch  4 Batch  104 / 447  Training Loss  0.0006625581299886107\n",
            "Epoch  4 Batch  105 / 447  Training Loss  0.000430901040090248\n",
            "Epoch  4 Batch  106 / 447  Training Loss  0.0006730870809406042\n",
            "Epoch  4 Batch  107 / 447  Training Loss  0.0007545138942077756\n",
            "Epoch  4 Batch  108 / 447  Training Loss  0.0038197385147213936\n",
            "Epoch  4 Batch  109 / 447  Training Loss  0.007881313562393188\n",
            "Epoch  4 Batch  110 / 447  Training Loss  0.00041219621198251843\n",
            "Epoch  4 Batch  111 / 447  Training Loss  0.001280264463275671\n",
            "Epoch  4 Batch  112 / 447  Training Loss  0.0008420244557783008\n",
            "Epoch  4 Batch  113 / 447  Training Loss  0.0017627213383093476\n",
            "Epoch  4 Batch  114 / 447  Training Loss  0.0012557213194668293\n",
            "Epoch  4 Batch  115 / 447  Training Loss  0.0009479141444899142\n",
            "Epoch  4 Batch  116 / 447  Training Loss  0.0004523602547124028\n",
            "Epoch  4 Batch  117 / 447  Training Loss  0.0007393124978989363\n",
            "Epoch  4 Batch  118 / 447  Training Loss  0.0023356336168944836\n",
            "Epoch  4 Batch  119 / 447  Training Loss  0.0020833455491811037\n",
            "Epoch  4 Batch  120 / 447  Training Loss  0.0006536958389915526\n",
            "Epoch  4 Batch  121 / 447  Training Loss  0.0005683987983502448\n",
            "Epoch  4 Batch  122 / 447  Training Loss  0.0005750650307163596\n",
            "Epoch  4 Batch  123 / 447  Training Loss  0.002232032362371683\n",
            "Epoch  4 Batch  124 / 447  Training Loss  0.002537364140152931\n",
            "Epoch  4 Batch  125 / 447  Training Loss  0.0010410924442112446\n",
            "Epoch  4 Batch  126 / 447  Training Loss  0.001551370369270444\n",
            "Epoch  4 Batch  127 / 447  Training Loss  0.0003933799162041396\n",
            "Epoch  4 Batch  128 / 447  Training Loss  0.0007820159080438316\n",
            "Epoch  4 Batch  129 / 447  Training Loss  0.00043290943722240627\n",
            "Epoch  4 Batch  130 / 447  Training Loss  0.0023240430746227503\n",
            "Epoch  4 Batch  131 / 447  Training Loss  0.0006009048083797097\n",
            "Epoch  4 Batch  132 / 447  Training Loss  0.0007576130446977913\n",
            "Epoch  4 Batch  133 / 447  Training Loss  0.0057500493712723255\n",
            "Epoch  4 Batch  134 / 447  Training Loss  0.00902463123202324\n",
            "Epoch  4 Batch  135 / 447  Training Loss  0.0007313272217288613\n",
            "Epoch  4 Batch  136 / 447  Training Loss  0.0019065563101321459\n",
            "Epoch  4 Batch  137 / 447  Training Loss  0.0035814971197396517\n",
            "Epoch  4 Batch  138 / 447  Training Loss  0.0010281995637342334\n",
            "Epoch  4 Batch  139 / 447  Training Loss  0.0011302210623398423\n",
            "Epoch  4 Batch  140 / 447  Training Loss  0.0005230107344686985\n",
            "Epoch  4 Batch  141 / 447  Training Loss  0.0005231978138908744\n",
            "Epoch  4 Batch  142 / 447  Training Loss  0.0006911924574524164\n",
            "Epoch  4 Batch  143 / 447  Training Loss  0.0006203704397194088\n",
            "Epoch  4 Batch  144 / 447  Training Loss  0.0010283306473866105\n",
            "Epoch  4 Batch  145 / 447  Training Loss  0.0004934921162202954\n",
            "Epoch  4 Batch  146 / 447  Training Loss  0.0005862248945049942\n",
            "Epoch  4 Batch  147 / 447  Training Loss  0.002155124908313155\n",
            "Epoch  4 Batch  148 / 447  Training Loss  0.00039555097464472055\n",
            "Epoch  4 Batch  149 / 447  Training Loss  0.0003110301913693547\n",
            "Epoch  4 Batch  150 / 447  Training Loss  0.0027012748178094625\n",
            "Epoch  4 Batch  151 / 447  Training Loss  0.0035304436460137367\n",
            "Epoch  4 Batch  152 / 447  Training Loss  0.011041274294257164\n",
            "Epoch  4 Batch  153 / 447  Training Loss  0.00773890595883131\n",
            "Epoch  4 Batch  154 / 447  Training Loss  0.023799024522304535\n",
            "Epoch  4 Batch  155 / 447  Training Loss  0.00410205265507102\n",
            "Epoch  4 Batch  156 / 447  Training Loss  0.0016089683631435037\n",
            "Epoch  4 Batch  157 / 447  Training Loss  0.014571858569979668\n",
            "Epoch  4 Batch  158 / 447  Training Loss  0.016159068793058395\n",
            "Epoch  4 Batch  159 / 447  Training Loss  0.004248269367963076\n",
            "Epoch  4 Batch  160 / 447  Training Loss  0.002075865399092436\n",
            "Epoch  4 Batch  161 / 447  Training Loss  0.0057775117456912994\n",
            "Epoch  4 Batch  162 / 447  Training Loss  0.00343768741004169\n",
            "Epoch  4 Batch  163 / 447  Training Loss  0.002477778121829033\n",
            "Epoch  4 Batch  164 / 447  Training Loss  0.0018520717276260257\n",
            "Epoch  4 Batch  165 / 447  Training Loss  0.0005607649800367653\n",
            "Epoch  4 Batch  166 / 447  Training Loss  0.0009466912015341222\n",
            "Epoch  4 Batch  167 / 447  Training Loss  0.0017868302529677749\n",
            "Epoch  4 Batch  168 / 447  Training Loss  0.0006520050228573382\n",
            "Epoch  4 Batch  169 / 447  Training Loss  0.0012213392183184624\n",
            "Epoch  4 Batch  170 / 447  Training Loss  0.001292389235459268\n",
            "Epoch  4 Batch  171 / 447  Training Loss  0.002656815107911825\n",
            "Epoch  4 Batch  172 / 447  Training Loss  0.006337578874081373\n",
            "Epoch  4 Batch  173 / 447  Training Loss  0.0008969425689429045\n",
            "Epoch  4 Batch  174 / 447  Training Loss  0.0019295391393825412\n",
            "Epoch  4 Batch  175 / 447  Training Loss  0.003385802498087287\n",
            "Epoch  4 Batch  176 / 447  Training Loss  0.0029075059574097395\n",
            "Epoch  4 Batch  177 / 447  Training Loss  0.001808100612834096\n",
            "Epoch  4 Batch  178 / 447  Training Loss  0.0015368713065981865\n",
            "Epoch  4 Batch  179 / 447  Training Loss  0.0007238976540975273\n",
            "Epoch  4 Batch  180 / 447  Training Loss  0.0009301662212237716\n",
            "Epoch  4 Batch  181 / 447  Training Loss  0.0005248709348961711\n",
            "Epoch  4 Batch  182 / 447  Training Loss  0.0005497715901583433\n",
            "Epoch  4 Batch  183 / 447  Training Loss  0.0006631987635046244\n",
            "Epoch  4 Batch  184 / 447  Training Loss  0.0006091121467761695\n",
            "Epoch  4 Batch  185 / 447  Training Loss  0.0003811934730038047\n",
            "Epoch  4 Batch  186 / 447  Training Loss  0.0007945990655571222\n",
            "Epoch  4 Batch  187 / 447  Training Loss  0.002376845106482506\n",
            "Epoch  4 Batch  188 / 447  Training Loss  0.0006482305470854044\n",
            "Epoch  4 Batch  189 / 447  Training Loss  0.0007040139753371477\n",
            "Epoch  4 Batch  190 / 447  Training Loss  0.0012087171198800206\n",
            "Epoch  4 Batch  191 / 447  Training Loss  0.0010966494446620345\n",
            "Epoch  4 Batch  192 / 447  Training Loss  0.0004910568241029978\n",
            "Epoch  4 Batch  193 / 447  Training Loss  0.00039125737384893\n",
            "Epoch  4 Batch  194 / 447  Training Loss  0.0007141613750718534\n",
            "Epoch  4 Batch  195 / 447  Training Loss  0.0003672670864034444\n",
            "Epoch  4 Batch  196 / 447  Training Loss  0.0004819182795472443\n",
            "Epoch  4 Batch  197 / 447  Training Loss  0.0005293661961331964\n",
            "Epoch  4 Batch  198 / 447  Training Loss  0.00032414638553746045\n",
            "Epoch  4 Batch  199 / 447  Training Loss  0.00046489204396493733\n",
            "Epoch  4 Batch  200 / 447  Training Loss  0.00048663318739272654\n",
            "Epoch  4 Batch  201 / 447  Training Loss  0.0003850815410260111\n",
            "Epoch  4 Batch  202 / 447  Training Loss  0.0005331144784577191\n",
            "Epoch  4 Batch  203 / 447  Training Loss  0.0007463447982445359\n",
            "Epoch  4 Batch  204 / 447  Training Loss  0.0008031132747419178\n",
            "Epoch  4 Batch  205 / 447  Training Loss  0.0002992393565364182\n",
            "Epoch  4 Batch  206 / 447  Training Loss  0.0005070557235740125\n",
            "Epoch  4 Batch  207 / 447  Training Loss  0.0004674249794334173\n",
            "Epoch  4 Batch  208 / 447  Training Loss  0.00028837521676905453\n",
            "Epoch  4 Batch  209 / 447  Training Loss  0.0003870422369800508\n",
            "Epoch  4 Batch  210 / 447  Training Loss  0.00039646471850574017\n",
            "Epoch  4 Batch  211 / 447  Training Loss  0.004447647370398045\n",
            "Epoch  4 Batch  212 / 447  Training Loss  0.0014973554061725736\n",
            "Epoch  4 Batch  213 / 447  Training Loss  0.0005516074015758932\n",
            "Epoch  4 Batch  214 / 447  Training Loss  0.0006722657126374543\n",
            "Epoch  4 Batch  215 / 447  Training Loss  0.0002765331882983446\n",
            "Epoch  4 Batch  216 / 447  Training Loss  0.0004288047493901104\n",
            "Epoch  4 Batch  217 / 447  Training Loss  0.00034495198633521795\n",
            "Epoch  4 Batch  218 / 447  Training Loss  0.00031148697598837316\n",
            "Epoch  4 Batch  219 / 447  Training Loss  0.0002828918513841927\n",
            "Epoch  4 Batch  220 / 447  Training Loss  0.0012078771833330393\n",
            "Epoch  4 Batch  221 / 447  Training Loss  0.00047186206211335957\n",
            "Epoch  4 Batch  222 / 447  Training Loss  0.0002674063725862652\n",
            "Epoch  4 Batch  223 / 447  Training Loss  0.0005029637832194567\n",
            "Epoch  4 Batch  224 / 447  Training Loss  0.0005016099894419312\n",
            "Epoch  4 Batch  225 / 447  Training Loss  0.000602205574978143\n",
            "Epoch  4 Batch  226 / 447  Training Loss  0.0004883035435341299\n",
            "Epoch  4 Batch  227 / 447  Training Loss  0.0038110199384391308\n",
            "Epoch  4 Batch  228 / 447  Training Loss  0.0005467618466354907\n",
            "Epoch  4 Batch  229 / 447  Training Loss  0.0003560730838216841\n",
            "Epoch  4 Batch  230 / 447  Training Loss  0.0004383921332191676\n",
            "Epoch  4 Batch  231 / 447  Training Loss  0.0003363408031873405\n",
            "Epoch  4 Batch  232 / 447  Training Loss  0.00029495672788470984\n",
            "Epoch  4 Batch  233 / 447  Training Loss  0.0003499562735669315\n",
            "Epoch  4 Batch  234 / 447  Training Loss  0.0003384793526493013\n",
            "Epoch  4 Batch  235 / 447  Training Loss  0.00031170231522992253\n",
            "Epoch  4 Batch  236 / 447  Training Loss  0.0011121126590296626\n",
            "Epoch  4 Batch  237 / 447  Training Loss  0.0007848180830478668\n",
            "Epoch  4 Batch  238 / 447  Training Loss  0.0002598616119939834\n",
            "Epoch  4 Batch  239 / 447  Training Loss  0.0031610948499292135\n",
            "Epoch  4 Batch  240 / 447  Training Loss  0.0008545725722797215\n",
            "Epoch  4 Batch  241 / 447  Training Loss  0.0005750852869823575\n",
            "Epoch  4 Batch  242 / 447  Training Loss  0.0003551876579876989\n",
            "Epoch  4 Batch  243 / 447  Training Loss  0.0002470816543791443\n",
            "Epoch  4 Batch  244 / 447  Training Loss  0.0003384285664651543\n",
            "Epoch  4 Batch  245 / 447  Training Loss  0.0004136689240112901\n",
            "Epoch  4 Batch  246 / 447  Training Loss  0.00031810151995159686\n",
            "Epoch  4 Batch  247 / 447  Training Loss  0.0003294729394838214\n",
            "Epoch  4 Batch  248 / 447  Training Loss  0.0003509457455947995\n",
            "Epoch  4 Batch  249 / 447  Training Loss  0.0002956122625619173\n",
            "Epoch  4 Batch  250 / 447  Training Loss  0.00028799832216463983\n",
            "Epoch  4 Batch  251 / 447  Training Loss  0.0003953607811126858\n",
            "Epoch  4 Batch  252 / 447  Training Loss  0.0005886952858418226\n",
            "Epoch  4 Batch  253 / 447  Training Loss  0.0002706933009903878\n",
            "Epoch  4 Batch  254 / 447  Training Loss  0.000619484344497323\n",
            "Epoch  4 Batch  255 / 447  Training Loss  0.00034821926965378225\n",
            "Epoch  4 Batch  256 / 447  Training Loss  0.0005636880523525178\n",
            "Epoch  4 Batch  257 / 447  Training Loss  0.0011911425972357392\n",
            "Epoch  4 Batch  258 / 447  Training Loss  0.0007370598614215851\n",
            "Epoch  4 Batch  259 / 447  Training Loss  0.0005675583961419761\n",
            "Epoch  4 Batch  260 / 447  Training Loss  0.00032981549156829715\n",
            "Epoch  4 Batch  261 / 447  Training Loss  0.00031935106380842626\n",
            "Epoch  4 Batch  262 / 447  Training Loss  0.00032942875986918807\n",
            "Epoch  4 Batch  263 / 447  Training Loss  0.0007908327388577163\n",
            "Epoch  4 Batch  264 / 447  Training Loss  0.0003370169142726809\n",
            "Epoch  4 Batch  265 / 447  Training Loss  0.00020800369384232908\n",
            "Epoch  4 Batch  266 / 447  Training Loss  0.000325287546729669\n",
            "Epoch  4 Batch  267 / 447  Training Loss  0.00020422425586730242\n",
            "Epoch  4 Batch  268 / 447  Training Loss  0.0003154606674797833\n",
            "Epoch  4 Batch  269 / 447  Training Loss  0.0002995815593749285\n",
            "Epoch  4 Batch  270 / 447  Training Loss  0.0004242172581143677\n",
            "Epoch  4 Batch  271 / 447  Training Loss  0.0003764308930840343\n",
            "Epoch  4 Batch  272 / 447  Training Loss  0.00036019322578795254\n",
            "Epoch  4 Batch  273 / 447  Training Loss  0.00034671984030865133\n",
            "Epoch  4 Batch  274 / 447  Training Loss  0.00029270711820572615\n",
            "Epoch  4 Batch  275 / 447  Training Loss  0.0003029705258086324\n",
            "Epoch  4 Batch  276 / 447  Training Loss  0.0003468084614723921\n",
            "Epoch  4 Batch  277 / 447  Training Loss  0.0003202140796929598\n",
            "Epoch  4 Batch  278 / 447  Training Loss  0.00037407263880595565\n",
            "Epoch  4 Batch  279 / 447  Training Loss  0.0003079310990869999\n",
            "Epoch  4 Batch  280 / 447  Training Loss  0.00032289500813931227\n",
            "Epoch  4 Batch  281 / 447  Training Loss  0.00024112797109410167\n",
            "Epoch  4 Batch  282 / 447  Training Loss  0.00021161902986932546\n",
            "Epoch  4 Batch  283 / 447  Training Loss  0.0004668516048695892\n",
            "Epoch  4 Batch  284 / 447  Training Loss  0.0002263674687128514\n",
            "Epoch  4 Batch  285 / 447  Training Loss  0.00025650562020018697\n",
            "Epoch  4 Batch  286 / 447  Training Loss  0.00046662858221679926\n",
            "Epoch  4 Batch  287 / 447  Training Loss  0.000382190482923761\n",
            "Epoch  4 Batch  288 / 447  Training Loss  0.00039082049624994397\n",
            "Epoch  4 Batch  289 / 447  Training Loss  0.0003421460569370538\n",
            "Epoch  4 Batch  290 / 447  Training Loss  0.0005126944743096828\n",
            "Epoch  4 Batch  291 / 447  Training Loss  0.00024267159460578114\n",
            "Epoch  4 Batch  292 / 447  Training Loss  0.0005149611970409751\n",
            "Epoch  4 Batch  293 / 447  Training Loss  0.0004745956393890083\n",
            "Epoch  4 Batch  294 / 447  Training Loss  0.0009432948427274823\n",
            "Epoch  4 Batch  295 / 447  Training Loss  0.00025825557531788945\n",
            "Epoch  4 Batch  296 / 447  Training Loss  0.00023164422600530088\n",
            "Epoch  4 Batch  297 / 447  Training Loss  0.0007553827599622309\n",
            "Epoch  4 Batch  298 / 447  Training Loss  0.0002817971399053931\n",
            "Epoch  4 Batch  299 / 447  Training Loss  0.000388959189876914\n",
            "Epoch  4 Batch  300 / 447  Training Loss  0.0002502499846741557\n",
            "Epoch  4 Batch  301 / 447  Training Loss  0.00017030253366101533\n",
            "Epoch  4 Batch  302 / 447  Training Loss  0.0002919793187174946\n",
            "Epoch  4 Batch  303 / 447  Training Loss  0.0004371458780951798\n",
            "Epoch  4 Batch  304 / 447  Training Loss  0.0002638267178554088\n",
            "Epoch  4 Batch  305 / 447  Training Loss  0.00024842197308316827\n",
            "Epoch  4 Batch  306 / 447  Training Loss  0.0003266276034992188\n",
            "Epoch  4 Batch  307 / 447  Training Loss  0.00032006294350139797\n",
            "Epoch  4 Batch  308 / 447  Training Loss  0.00022497506870422512\n",
            "Epoch  4 Batch  309 / 447  Training Loss  0.00028691330226138234\n",
            "Epoch  4 Batch  310 / 447  Training Loss  0.000273807265330106\n",
            "Epoch  4 Batch  311 / 447  Training Loss  0.00026092224288731813\n",
            "Epoch  4 Batch  312 / 447  Training Loss  0.0002587822964414954\n",
            "Epoch  4 Batch  313 / 447  Training Loss  0.0005614272668026388\n",
            "Epoch  4 Batch  314 / 447  Training Loss  0.00028246056172065437\n",
            "Epoch  4 Batch  315 / 447  Training Loss  0.00015705630357842892\n",
            "Epoch  4 Batch  316 / 447  Training Loss  0.00023635569959878922\n",
            "Epoch  4 Batch  317 / 447  Training Loss  0.0005515641532838345\n",
            "Epoch  4 Batch  318 / 447  Training Loss  0.00024079231661744416\n",
            "Epoch  4 Batch  319 / 447  Training Loss  0.0002529644116293639\n",
            "Epoch  4 Batch  320 / 447  Training Loss  0.00021726817067246884\n",
            "Epoch  4 Batch  321 / 447  Training Loss  0.0002932258357759565\n",
            "Epoch  4 Batch  322 / 447  Training Loss  0.0003341734118293971\n",
            "Epoch  4 Batch  323 / 447  Training Loss  0.0002810739679262042\n",
            "Epoch  4 Batch  324 / 447  Training Loss  0.00014355276653077453\n",
            "Epoch  4 Batch  325 / 447  Training Loss  0.0001139693267759867\n",
            "Epoch  4 Batch  326 / 447  Training Loss  0.0009924701880663633\n",
            "Epoch  4 Batch  327 / 447  Training Loss  0.00028382340678945184\n",
            "Epoch  4 Batch  328 / 447  Training Loss  0.00035214517265558243\n",
            "Epoch  4 Batch  329 / 447  Training Loss  0.00019674414943438023\n",
            "Epoch  4 Batch  330 / 447  Training Loss  0.00033859486575238407\n",
            "Epoch  4 Batch  331 / 447  Training Loss  0.00023716865689493716\n",
            "Epoch  4 Batch  332 / 447  Training Loss  0.00124928739387542\n",
            "Epoch  4 Batch  333 / 447  Training Loss  0.0002480690600350499\n",
            "Epoch  4 Batch  334 / 447  Training Loss  0.004122630693018436\n",
            "Epoch  4 Batch  335 / 447  Training Loss  0.0001525904517620802\n",
            "Epoch  4 Batch  336 / 447  Training Loss  0.00031746196327731013\n",
            "Epoch  4 Batch  337 / 447  Training Loss  0.005917316302657127\n",
            "Epoch  4 Batch  338 / 447  Training Loss  0.0003039370058104396\n",
            "Epoch  4 Batch  339 / 447  Training Loss  0.000387022242648527\n",
            "Epoch  4 Batch  340 / 447  Training Loss  0.005351549945771694\n",
            "Epoch  4 Batch  341 / 447  Training Loss  0.0006081603933125734\n",
            "Epoch  4 Batch  342 / 447  Training Loss  0.0007462716312147677\n",
            "Epoch  4 Batch  343 / 447  Training Loss  0.0015602101339027286\n",
            "Epoch  4 Batch  344 / 447  Training Loss  0.00022803367755841464\n",
            "Epoch  4 Batch  345 / 447  Training Loss  0.0003101326583418995\n",
            "Epoch  4 Batch  346 / 447  Training Loss  0.0006224007811397314\n",
            "Epoch  4 Batch  347 / 447  Training Loss  0.0002929546753875911\n",
            "Epoch  4 Batch  348 / 447  Training Loss  0.004023376852273941\n",
            "Epoch  4 Batch  349 / 447  Training Loss  0.00027878672699443996\n",
            "Epoch  4 Batch  350 / 447  Training Loss  0.0005714947474189103\n",
            "Epoch  4 Batch  351 / 447  Training Loss  0.0001828081876737997\n",
            "Epoch  4 Batch  352 / 447  Training Loss  0.00044278224231675267\n",
            "Epoch  4 Batch  353 / 447  Training Loss  0.00019125865946989506\n",
            "Epoch  4 Batch  354 / 447  Training Loss  0.0001890128041850403\n",
            "Epoch  4 Batch  355 / 447  Training Loss  0.00012549461098387837\n",
            "Epoch  4 Batch  356 / 447  Training Loss  0.00017157651018351316\n",
            "Epoch  4 Batch  357 / 447  Training Loss  0.00018745720444712788\n",
            "Epoch  4 Batch  358 / 447  Training Loss  0.006392425391823053\n",
            "Epoch  4 Batch  359 / 447  Training Loss  0.0017186242621392012\n",
            "Epoch  4 Batch  360 / 447  Training Loss  0.001048864098265767\n",
            "Epoch  4 Batch  361 / 447  Training Loss  0.0002362831000937149\n",
            "Epoch  4 Batch  362 / 447  Training Loss  0.00016089979908429086\n",
            "Epoch  4 Batch  363 / 447  Training Loss  0.00015740010712761432\n",
            "Epoch  4 Batch  364 / 447  Training Loss  0.007197607308626175\n",
            "Epoch  4 Batch  365 / 447  Training Loss  0.00045829248847439885\n",
            "Epoch  4 Batch  366 / 447  Training Loss  0.0010232890490442514\n",
            "Epoch  4 Batch  367 / 447  Training Loss  0.002909300848841667\n",
            "Epoch  4 Batch  368 / 447  Training Loss  0.00014918812667019665\n",
            "Epoch  4 Batch  369 / 447  Training Loss  0.00018512319365981966\n",
            "Epoch  4 Batch  370 / 447  Training Loss  0.0006534227868542075\n",
            "Epoch  4 Batch  371 / 447  Training Loss  0.0003592169377952814\n",
            "Epoch  4 Batch  372 / 447  Training Loss  0.0012431835057213902\n",
            "Epoch  4 Batch  373 / 447  Training Loss  0.0011267811059951782\n",
            "Epoch  4 Batch  374 / 447  Training Loss  0.0012436930555850267\n",
            "Epoch  4 Batch  375 / 447  Training Loss  0.00035033829044550657\n",
            "Epoch  4 Batch  376 / 447  Training Loss  0.0002618514990899712\n",
            "Epoch  4 Batch  377 / 447  Training Loss  0.000519587891176343\n",
            "Epoch  4 Batch  378 / 447  Training Loss  0.00020766012312378734\n",
            "Epoch  4 Batch  379 / 447  Training Loss  0.00021207233658060431\n",
            "Epoch  4 Batch  380 / 447  Training Loss  0.00022255016665440053\n",
            "Epoch  4 Batch  381 / 447  Training Loss  0.0003409675555303693\n",
            "Epoch  4 Batch  382 / 447  Training Loss  0.004391564521938562\n",
            "Epoch  4 Batch  383 / 447  Training Loss  0.0007874314323998988\n",
            "Epoch  4 Batch  384 / 447  Training Loss  0.0003300620592199266\n",
            "Epoch  4 Batch  385 / 447  Training Loss  0.000256392260780558\n",
            "Epoch  4 Batch  386 / 447  Training Loss  0.00019774437532760203\n",
            "Epoch  4 Batch  387 / 447  Training Loss  0.0003125436487607658\n",
            "Epoch  4 Batch  388 / 447  Training Loss  0.00037083137431181967\n",
            "Epoch  4 Batch  389 / 447  Training Loss  0.00021319271763786674\n",
            "Epoch  4 Batch  390 / 447  Training Loss  0.0003510743845254183\n",
            "Epoch  4 Batch  391 / 447  Training Loss  0.00032623662264086306\n",
            "Epoch  4 Batch  392 / 447  Training Loss  0.00012899878493044525\n",
            "Epoch  4 Batch  393 / 447  Training Loss  0.0035506722051650286\n",
            "Epoch  4 Batch  394 / 447  Training Loss  0.0002312741125933826\n",
            "Epoch  4 Batch  395 / 447  Training Loss  0.0009991922415792942\n",
            "Epoch  4 Batch  396 / 447  Training Loss  0.015265700407326221\n",
            "Epoch  4 Batch  397 / 447  Training Loss  0.0029724163468927145\n",
            "Epoch  4 Batch  398 / 447  Training Loss  0.0021386677399277687\n",
            "Epoch  4 Batch  399 / 447  Training Loss  0.00036177769652567804\n",
            "Epoch  4 Batch  400 / 447  Training Loss  0.006868652533739805\n",
            "Epoch  4 Batch  401 / 447  Training Loss  0.00711484719067812\n",
            "Epoch  4 Batch  402 / 447  Training Loss  0.0007521543884649873\n",
            "Epoch  4 Batch  403 / 447  Training Loss  0.0003874812973663211\n",
            "Epoch  4 Batch  404 / 447  Training Loss  0.00034897783189080656\n",
            "Epoch  4 Batch  405 / 447  Training Loss  0.0004873588914051652\n",
            "Epoch  4 Batch  406 / 447  Training Loss  0.00039595967973582447\n",
            "Epoch  4 Batch  407 / 447  Training Loss  0.0003281857934780419\n",
            "Epoch  4 Batch  408 / 447  Training Loss  0.0005310879787430167\n",
            "Epoch  4 Batch  409 / 447  Training Loss  0.0011883650440722704\n",
            "Epoch  4 Batch  410 / 447  Training Loss  0.002171499654650688\n",
            "Epoch  4 Batch  411 / 447  Training Loss  0.0008575852261856198\n",
            "Epoch  4 Batch  412 / 447  Training Loss  0.005343281198292971\n",
            "Epoch  4 Batch  413 / 447  Training Loss  0.0010552508756518364\n",
            "Epoch  4 Batch  414 / 447  Training Loss  0.0005068863392807543\n",
            "Epoch  4 Batch  415 / 447  Training Loss  0.00035386194940656424\n",
            "Epoch  4 Batch  416 / 447  Training Loss  0.0017490987665951252\n",
            "Epoch  4 Batch  417 / 447  Training Loss  0.0018900478025898337\n",
            "Epoch  4 Batch  418 / 447  Training Loss  0.00039197440491989255\n",
            "Epoch  4 Batch  419 / 447  Training Loss  0.0007187168812379241\n",
            "Epoch  4 Batch  420 / 447  Training Loss  0.0005020413082093\n",
            "Epoch  4 Batch  421 / 447  Training Loss  0.000296585145406425\n",
            "Epoch  4 Batch  422 / 447  Training Loss  0.0002978503762278706\n",
            "Epoch  4 Batch  423 / 447  Training Loss  0.00018702704983297735\n",
            "Epoch  4 Batch  424 / 447  Training Loss  0.0008942715357989073\n",
            "Epoch  4 Batch  425 / 447  Training Loss  0.0003967017401009798\n",
            "Epoch  4 Batch  426 / 447  Training Loss  0.0013252341886982322\n",
            "Epoch  4 Batch  427 / 447  Training Loss  0.0003044253389816731\n",
            "Epoch  4 Batch  428 / 447  Training Loss  0.00021030895004514605\n",
            "Epoch  4 Batch  429 / 447  Training Loss  0.00033420883119106293\n",
            "Epoch  4 Batch  430 / 447  Training Loss  0.00023093972413334996\n",
            "Epoch  4 Batch  431 / 447  Training Loss  0.0007670074119232595\n",
            "Epoch  4 Batch  432 / 447  Training Loss  0.000337469537043944\n",
            "Epoch  4 Batch  433 / 447  Training Loss  0.00037273962516337633\n",
            "Epoch  4 Batch  434 / 447  Training Loss  0.00391025235876441\n",
            "Epoch  4 Batch  435 / 447  Training Loss  0.00022780205472372472\n",
            "Epoch  4 Batch  436 / 447  Training Loss  0.00043019000440835953\n",
            "Epoch  4 Batch  437 / 447  Training Loss  0.0010145967826247215\n",
            "Epoch  4 Batch  438 / 447  Training Loss  0.000189261554623954\n",
            "Epoch  4 Batch  439 / 447  Training Loss  0.00022197587531991303\n",
            "Epoch  4 Batch  440 / 447  Training Loss  0.0002891767071560025\n",
            "Epoch  4 Batch  441 / 447  Training Loss  0.0005084493895992637\n",
            "Epoch  4 Batch  442 / 447  Training Loss  0.0005420995294116437\n",
            "Epoch  4 Batch  443 / 447  Training Loss  0.0008488339371979237\n",
            "Epoch  4 Batch  444 / 447  Training Loss  0.0006802132702432573\n",
            "Epoch  4 Batch  445 / 447  Training Loss  0.000284961803117767\n",
            "Epoch  4 Batch  446 / 447  Training Loss  0.0002617430000100285\n",
            "   5    |    -    |   0.002412   | 99.471003\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 5\n",
            "Epoch  5 Batch  0 / 447  Training Loss  0.00017457667854614556\n",
            "Epoch  5 Batch  1 / 447  Training Loss  0.00033099521533586085\n",
            "Epoch  5 Batch  2 / 447  Training Loss  0.0006669295835308731\n",
            "Epoch  5 Batch  3 / 447  Training Loss  0.00019954702293034643\n",
            "Epoch  5 Batch  4 / 447  Training Loss  0.00022158944921102375\n",
            "Epoch  5 Batch  5 / 447  Training Loss  0.00024603577912785113\n",
            "Epoch  5 Batch  6 / 447  Training Loss  0.0002870387688744813\n",
            "Epoch  5 Batch  7 / 447  Training Loss  0.00022334064124152064\n",
            "Epoch  5 Batch  8 / 447  Training Loss  0.0001638743415242061\n",
            "Epoch  5 Batch  9 / 447  Training Loss  0.0001758252619765699\n",
            "Epoch  5 Batch  10 / 447  Training Loss  0.0002394581533735618\n",
            "Epoch  5 Batch  11 / 447  Training Loss  0.00015469250502064824\n",
            "Epoch  5 Batch  12 / 447  Training Loss  0.00020424510876182467\n",
            "Epoch  5 Batch  13 / 447  Training Loss  0.00018706450646277517\n",
            "Epoch  5 Batch  14 / 447  Training Loss  0.00029660097789019346\n",
            "Epoch  5 Batch  15 / 447  Training Loss  0.00018706859555095434\n",
            "Epoch  5 Batch  16 / 447  Training Loss  0.0010645795846357942\n",
            "Epoch  5 Batch  17 / 447  Training Loss  0.00028606149135157466\n",
            "Epoch  5 Batch  18 / 447  Training Loss  0.00016976990445982665\n",
            "Epoch  5 Batch  19 / 447  Training Loss  0.0002418845397187397\n",
            "Epoch  5 Batch  20 / 447  Training Loss  0.00012832040374632925\n",
            "Epoch  5 Batch  21 / 447  Training Loss  0.00020840996876358986\n",
            "Epoch  5 Batch  22 / 447  Training Loss  0.0001844033831730485\n",
            "Epoch  5 Batch  23 / 447  Training Loss  0.00020784040680155158\n",
            "Epoch  5 Batch  24 / 447  Training Loss  0.0003102438640780747\n",
            "Epoch  5 Batch  25 / 447  Training Loss  0.0006924138288013637\n",
            "Epoch  5 Batch  26 / 447  Training Loss  0.0011028565932065248\n",
            "Epoch  5 Batch  27 / 447  Training Loss  0.0002780822687782347\n",
            "Epoch  5 Batch  28 / 447  Training Loss  0.00017701834440231323\n",
            "Epoch  5 Batch  29 / 447  Training Loss  0.00015775694919284433\n",
            "Epoch  5 Batch  30 / 447  Training Loss  0.00025399631704203784\n",
            "Epoch  5 Batch  31 / 447  Training Loss  0.00016536537441425025\n",
            "Epoch  5 Batch  32 / 447  Training Loss  0.00015188788529485464\n",
            "Epoch  5 Batch  33 / 447  Training Loss  0.0002784959797281772\n",
            "Epoch  5 Batch  34 / 447  Training Loss  0.00016817697905935347\n",
            "Epoch  5 Batch  35 / 447  Training Loss  0.00014929863391444087\n",
            "Epoch  5 Batch  36 / 447  Training Loss  0.00020250726083759218\n",
            "Epoch  5 Batch  37 / 447  Training Loss  0.000272279983619228\n",
            "Epoch  5 Batch  38 / 447  Training Loss  0.00020279508316889405\n",
            "Epoch  5 Batch  39 / 447  Training Loss  0.0008168966160155833\n",
            "Epoch  5 Batch  40 / 447  Training Loss  0.00013671343913301826\n",
            "Epoch  5 Batch  41 / 447  Training Loss  0.0001701258879620582\n",
            "Epoch  5 Batch  42 / 447  Training Loss  0.00010997983918059617\n",
            "Epoch  5 Batch  43 / 447  Training Loss  0.0001748305803630501\n",
            "Epoch  5 Batch  44 / 447  Training Loss  0.00012725642591249198\n",
            "Epoch  5 Batch  45 / 447  Training Loss  0.00028047841624356806\n",
            "Epoch  5 Batch  46 / 447  Training Loss  0.00028095601010136306\n",
            "Epoch  5 Batch  47 / 447  Training Loss  0.003066367469727993\n",
            "Epoch  5 Batch  48 / 447  Training Loss  0.0002932395145762712\n",
            "Epoch  5 Batch  49 / 447  Training Loss  0.0003496930003166199\n",
            "Epoch  5 Batch  50 / 447  Training Loss  0.00014655044651590288\n",
            "Epoch  5 Batch  51 / 447  Training Loss  0.00014528969768434763\n",
            "Epoch  5 Batch  52 / 447  Training Loss  0.00016666522424202412\n",
            "Epoch  5 Batch  53 / 447  Training Loss  0.0002475063083693385\n",
            "Epoch  5 Batch  54 / 447  Training Loss  0.0001708858326310292\n",
            "Epoch  5 Batch  55 / 447  Training Loss  0.00016333696839865297\n",
            "Epoch  5 Batch  56 / 447  Training Loss  0.00014046125579625368\n",
            "Epoch  5 Batch  57 / 447  Training Loss  0.0001440387568436563\n",
            "Epoch  5 Batch  58 / 447  Training Loss  0.00020225340267643332\n",
            "Epoch  5 Batch  59 / 447  Training Loss  0.0009894364047795534\n",
            "Epoch  5 Batch  60 / 447  Training Loss  0.0006771340267732739\n",
            "Epoch  5 Batch  61 / 447  Training Loss  0.00013785225746687502\n",
            "Epoch  5 Batch  62 / 447  Training Loss  0.0001581076649017632\n",
            "Epoch  5 Batch  63 / 447  Training Loss  0.0002932945208158344\n",
            "Epoch  5 Batch  64 / 447  Training Loss  0.0026733947452157736\n",
            "Epoch  5 Batch  65 / 447  Training Loss  0.00022351993538904935\n",
            "Epoch  5 Batch  66 / 447  Training Loss  0.0001677381369518116\n",
            "Epoch  5 Batch  67 / 447  Training Loss  0.0005416162894107401\n",
            "Epoch  5 Batch  68 / 447  Training Loss  0.00021154878777451813\n",
            "Epoch  5 Batch  69 / 447  Training Loss  0.00021008170733693987\n",
            "Epoch  5 Batch  70 / 447  Training Loss  0.000175072142155841\n",
            "Epoch  5 Batch  71 / 447  Training Loss  0.00036119032301940024\n",
            "Epoch  5 Batch  72 / 447  Training Loss  0.0005345505778677762\n",
            "Epoch  5 Batch  73 / 447  Training Loss  0.00015696295304223895\n",
            "Epoch  5 Batch  74 / 447  Training Loss  0.00015770841855555773\n",
            "Epoch  5 Batch  75 / 447  Training Loss  0.000210269630770199\n",
            "Epoch  5 Batch  76 / 447  Training Loss  0.00013957936607766896\n",
            "Epoch  5 Batch  77 / 447  Training Loss  0.00013126048725098372\n",
            "Epoch  5 Batch  78 / 447  Training Loss  0.00036990229273214936\n",
            "Epoch  5 Batch  79 / 447  Training Loss  0.00012490974040701985\n",
            "Epoch  5 Batch  80 / 447  Training Loss  0.00020110052719246596\n",
            "Epoch  5 Batch  81 / 447  Training Loss  0.00014049159653950483\n",
            "Epoch  5 Batch  82 / 447  Training Loss  0.00015220501518342644\n",
            "Epoch  5 Batch  83 / 447  Training Loss  0.00019093358423560858\n",
            "Epoch  5 Batch  84 / 447  Training Loss  0.00020528888853732497\n",
            "Epoch  5 Batch  85 / 447  Training Loss  0.00018198246834799647\n",
            "Epoch  5 Batch  86 / 447  Training Loss  0.00014585405006073415\n",
            "Epoch  5 Batch  87 / 447  Training Loss  0.00017859858053270727\n",
            "Epoch  5 Batch  88 / 447  Training Loss  0.00011194816033821553\n",
            "Epoch  5 Batch  89 / 447  Training Loss  0.00013876160664949566\n",
            "Epoch  5 Batch  90 / 447  Training Loss  0.00016027249512262642\n",
            "Epoch  5 Batch  91 / 447  Training Loss  0.00021224739612080157\n",
            "Epoch  5 Batch  92 / 447  Training Loss  0.00012489216169342399\n",
            "Epoch  5 Batch  93 / 447  Training Loss  0.00010338404536014423\n",
            "Epoch  5 Batch  94 / 447  Training Loss  0.000186586010386236\n",
            "Epoch  5 Batch  95 / 447  Training Loss  0.00010562643728917465\n",
            "Epoch  5 Batch  96 / 447  Training Loss  0.0001207935347338207\n",
            "Epoch  5 Batch  97 / 447  Training Loss  0.00013003911590203643\n",
            "Epoch  5 Batch  98 / 447  Training Loss  0.0004034928570035845\n",
            "Epoch  5 Batch  99 / 447  Training Loss  0.00012519532174337655\n",
            "Epoch  5 Batch  100 / 447  Training Loss  0.00014936196384951472\n",
            "Epoch  5 Batch  101 / 447  Training Loss  0.00014441885286942124\n",
            "Epoch  5 Batch  102 / 447  Training Loss  0.0003472637035883963\n",
            "Epoch  5 Batch  103 / 447  Training Loss  0.0001257915428141132\n",
            "Epoch  5 Batch  104 / 447  Training Loss  0.00016883586067706347\n",
            "Epoch  5 Batch  105 / 447  Training Loss  0.00017727291560731828\n",
            "Epoch  5 Batch  106 / 447  Training Loss  0.00021412753267213702\n",
            "Epoch  5 Batch  107 / 447  Training Loss  0.00017935156938619912\n",
            "Epoch  5 Batch  108 / 447  Training Loss  0.0002983130398206413\n",
            "Epoch  5 Batch  109 / 447  Training Loss  0.0002478871028870344\n",
            "Epoch  5 Batch  110 / 447  Training Loss  0.00023366589448414743\n",
            "Epoch  5 Batch  111 / 447  Training Loss  0.00012807140592485666\n",
            "Epoch  5 Batch  112 / 447  Training Loss  0.00014464939886238426\n",
            "Epoch  5 Batch  113 / 447  Training Loss  0.00012969256204087287\n",
            "Epoch  5 Batch  114 / 447  Training Loss  0.0003823027655016631\n",
            "Epoch  5 Batch  115 / 447  Training Loss  8.714212162885815e-05\n",
            "Epoch  5 Batch  116 / 447  Training Loss  0.00012439890997484326\n",
            "Epoch  5 Batch  117 / 447  Training Loss  0.00022573661408387125\n",
            "Epoch  5 Batch  118 / 447  Training Loss  0.0004423854988999665\n",
            "Epoch  5 Batch  119 / 447  Training Loss  0.00026704955962486565\n",
            "Epoch  5 Batch  120 / 447  Training Loss  9.543833584757522e-05\n",
            "Epoch  5 Batch  121 / 447  Training Loss  0.00012009974307147786\n",
            "Epoch  5 Batch  122 / 447  Training Loss  0.00023757453891448677\n",
            "Epoch  5 Batch  123 / 447  Training Loss  0.00013300369028002024\n",
            "Epoch  5 Batch  124 / 447  Training Loss  0.00016981110093183815\n",
            "Epoch  5 Batch  125 / 447  Training Loss  9.253432654077187e-05\n",
            "Epoch  5 Batch  126 / 447  Training Loss  0.0003393540100660175\n",
            "Epoch  5 Batch  127 / 447  Training Loss  0.0002044783759629354\n",
            "Epoch  5 Batch  128 / 447  Training Loss  0.0001222676655743271\n",
            "Epoch  5 Batch  129 / 447  Training Loss  0.0001421986671630293\n",
            "Epoch  5 Batch  130 / 447  Training Loss  0.00018260003707837313\n",
            "Epoch  5 Batch  131 / 447  Training Loss  0.00011327959509799257\n",
            "Epoch  5 Batch  132 / 447  Training Loss  0.00012520259770099074\n",
            "Epoch  5 Batch  133 / 447  Training Loss  0.0001188598689623177\n",
            "Epoch  5 Batch  134 / 447  Training Loss  9.399835835210979e-05\n",
            "Epoch  5 Batch  135 / 447  Training Loss  0.00018186355009675026\n",
            "Epoch  5 Batch  136 / 447  Training Loss  8.497497765347362e-05\n",
            "Epoch  5 Batch  137 / 447  Training Loss  8.228179649449885e-05\n",
            "Epoch  5 Batch  138 / 447  Training Loss  9.605711966287345e-05\n",
            "Epoch  5 Batch  139 / 447  Training Loss  9.101410250877962e-05\n",
            "Epoch  5 Batch  140 / 447  Training Loss  0.000194541658856906\n",
            "Epoch  5 Batch  141 / 447  Training Loss  0.0009653188171796501\n",
            "Epoch  5 Batch  142 / 447  Training Loss  0.00018880590505432338\n",
            "Epoch  5 Batch  143 / 447  Training Loss  0.00013576824858319014\n",
            "Epoch  5 Batch  144 / 447  Training Loss  0.000474750209832564\n",
            "Epoch  5 Batch  145 / 447  Training Loss  0.00010755355469882488\n",
            "Epoch  5 Batch  146 / 447  Training Loss  0.00010792652756208554\n",
            "Epoch  5 Batch  147 / 447  Training Loss  0.00011065919534303248\n",
            "Epoch  5 Batch  148 / 447  Training Loss  0.0001856334274634719\n",
            "Epoch  5 Batch  149 / 447  Training Loss  0.00019894406432285905\n",
            "Epoch  5 Batch  150 / 447  Training Loss  0.00012097135186195374\n",
            "Epoch  5 Batch  151 / 447  Training Loss  0.00014993571676313877\n",
            "Epoch  5 Batch  152 / 447  Training Loss  9.680909715825692e-05\n",
            "Epoch  5 Batch  153 / 447  Training Loss  0.0038354930002242327\n",
            "Epoch  5 Batch  154 / 447  Training Loss  0.00035523195401765406\n",
            "Epoch  5 Batch  155 / 447  Training Loss  0.0001343818730674684\n",
            "Epoch  5 Batch  156 / 447  Training Loss  0.0001242308208020404\n",
            "Epoch  5 Batch  157 / 447  Training Loss  0.00012446061009541154\n",
            "Epoch  5 Batch  158 / 447  Training Loss  0.0002636927820276469\n",
            "Epoch  5 Batch  159 / 447  Training Loss  0.0019702494610100985\n",
            "Epoch  5 Batch  160 / 447  Training Loss  0.00014114072837401181\n",
            "Epoch  5 Batch  161 / 447  Training Loss  8.452960173599422e-05\n",
            "Epoch  5 Batch  162 / 447  Training Loss  0.00020976524683646858\n",
            "Epoch  5 Batch  163 / 447  Training Loss  0.00011406173871364444\n",
            "Epoch  5 Batch  164 / 447  Training Loss  0.00021506966731976718\n",
            "Epoch  5 Batch  165 / 447  Training Loss  0.0002411908353678882\n",
            "Epoch  5 Batch  166 / 447  Training Loss  0.00011186284245923162\n",
            "Epoch  5 Batch  167 / 447  Training Loss  0.00012808074825443327\n",
            "Epoch  5 Batch  168 / 447  Training Loss  0.0002331719151698053\n",
            "Epoch  5 Batch  169 / 447  Training Loss  0.0006762603879906237\n",
            "Epoch  5 Batch  170 / 447  Training Loss  0.0001382201153319329\n",
            "Epoch  5 Batch  171 / 447  Training Loss  9.746165596880019e-05\n",
            "Epoch  5 Batch  172 / 447  Training Loss  0.00015176557644736022\n",
            "Epoch  5 Batch  173 / 447  Training Loss  0.00011536416423041373\n",
            "Epoch  5 Batch  174 / 447  Training Loss  0.0001124771370086819\n",
            "Epoch  5 Batch  175 / 447  Training Loss  9.926858183462173e-05\n",
            "Epoch  5 Batch  176 / 447  Training Loss  0.00012927108036819845\n",
            "Epoch  5 Batch  177 / 447  Training Loss  0.0003085177449975163\n",
            "Epoch  5 Batch  178 / 447  Training Loss  0.00013814138947054744\n",
            "Epoch  5 Batch  179 / 447  Training Loss  0.00010268171899951994\n",
            "Epoch  5 Batch  180 / 447  Training Loss  0.00013628334272652864\n",
            "Epoch  5 Batch  181 / 447  Training Loss  0.0001290914515266195\n",
            "Epoch  5 Batch  182 / 447  Training Loss  9.643442899687216e-05\n",
            "Epoch  5 Batch  183 / 447  Training Loss  0.00014706538058817387\n",
            "Epoch  5 Batch  184 / 447  Training Loss  0.00015144476492423564\n",
            "Epoch  5 Batch  185 / 447  Training Loss  0.00010276101238559932\n",
            "Epoch  5 Batch  186 / 447  Training Loss  0.00018182297935709357\n",
            "Epoch  5 Batch  187 / 447  Training Loss  0.0005713683203794062\n",
            "Epoch  5 Batch  188 / 447  Training Loss  0.00016432482516393065\n",
            "Epoch  5 Batch  189 / 447  Training Loss  0.00011143789015477523\n",
            "Epoch  5 Batch  190 / 447  Training Loss  0.00015641729987692088\n",
            "Epoch  5 Batch  191 / 447  Training Loss  0.00013515610771719366\n",
            "Epoch  5 Batch  192 / 447  Training Loss  0.00010887467942666262\n",
            "Epoch  5 Batch  193 / 447  Training Loss  0.001963034039363265\n",
            "Epoch  5 Batch  194 / 447  Training Loss  0.00020113955542910844\n",
            "Epoch  5 Batch  195 / 447  Training Loss  0.00010206548176938668\n",
            "Epoch  5 Batch  196 / 447  Training Loss  0.0011295954464003444\n",
            "Epoch  5 Batch  197 / 447  Training Loss  0.0022869177628308535\n",
            "Epoch  5 Batch  198 / 447  Training Loss  0.00013429427053779364\n",
            "Epoch  5 Batch  199 / 447  Training Loss  0.00014439213555306196\n",
            "Epoch  5 Batch  200 / 447  Training Loss  0.00755978561937809\n",
            "Epoch  5 Batch  201 / 447  Training Loss  0.00016794074326753616\n",
            "Epoch  5 Batch  202 / 447  Training Loss  0.00014736851153429598\n",
            "Epoch  5 Batch  203 / 447  Training Loss  0.00033404704299755394\n",
            "Epoch  5 Batch  204 / 447  Training Loss  0.00021343521075323224\n",
            "Epoch  5 Batch  205 / 447  Training Loss  0.00025932115386240184\n",
            "Epoch  5 Batch  206 / 447  Training Loss  0.00015093445836100727\n",
            "Epoch  5 Batch  207 / 447  Training Loss  0.0002771542640402913\n",
            "Epoch  5 Batch  208 / 447  Training Loss  0.00022474917932413518\n",
            "Epoch  5 Batch  209 / 447  Training Loss  0.00010931748693110421\n",
            "Epoch  5 Batch  210 / 447  Training Loss  0.00014736561570316553\n",
            "Epoch  5 Batch  211 / 447  Training Loss  8.771847933530807e-05\n",
            "Epoch  5 Batch  212 / 447  Training Loss  0.00011156739492435008\n",
            "Epoch  5 Batch  213 / 447  Training Loss  0.00013607775326818228\n",
            "Epoch  5 Batch  214 / 447  Training Loss  9.936287824530154e-05\n",
            "Epoch  5 Batch  215 / 447  Training Loss  0.00030391651671379805\n",
            "Epoch  5 Batch  216 / 447  Training Loss  0.00012913621321786195\n",
            "Epoch  5 Batch  217 / 447  Training Loss  0.00019827191135846078\n",
            "Epoch  5 Batch  218 / 447  Training Loss  0.00019705384329427034\n",
            "Epoch  5 Batch  219 / 447  Training Loss  0.0002351350267417729\n",
            "Epoch  5 Batch  220 / 447  Training Loss  0.00015632547729182988\n",
            "Epoch  5 Batch  221 / 447  Training Loss  0.00012114608398405835\n",
            "Epoch  5 Batch  222 / 447  Training Loss  0.000104858772829175\n",
            "Epoch  5 Batch  223 / 447  Training Loss  0.00019787975179497153\n",
            "Epoch  5 Batch  224 / 447  Training Loss  0.000150233056047\n",
            "Epoch  5 Batch  225 / 447  Training Loss  0.00016908168618101627\n",
            "Epoch  5 Batch  226 / 447  Training Loss  0.0001165038047474809\n",
            "Epoch  5 Batch  227 / 447  Training Loss  0.0006008775671944022\n",
            "Epoch  5 Batch  228 / 447  Training Loss  7.929357525426894e-05\n",
            "Epoch  5 Batch  229 / 447  Training Loss  0.00012895897089038044\n",
            "Epoch  5 Batch  230 / 447  Training Loss  0.00014960135740693659\n",
            "Epoch  5 Batch  231 / 447  Training Loss  0.00011063795682275668\n",
            "Epoch  5 Batch  232 / 447  Training Loss  0.000125116654089652\n",
            "Epoch  5 Batch  233 / 447  Training Loss  0.0001306509948335588\n",
            "Epoch  5 Batch  234 / 447  Training Loss  0.00013677925744559616\n",
            "Epoch  5 Batch  235 / 447  Training Loss  0.0005123198498040438\n",
            "Epoch  5 Batch  236 / 447  Training Loss  0.0001276635011890903\n",
            "Epoch  5 Batch  237 / 447  Training Loss  0.00013250579650048167\n",
            "Epoch  5 Batch  238 / 447  Training Loss  0.00014433586329687387\n",
            "Epoch  5 Batch  239 / 447  Training Loss  0.00010427090455777943\n",
            "Epoch  5 Batch  240 / 447  Training Loss  0.0001234922674484551\n",
            "Epoch  5 Batch  241 / 447  Training Loss  0.00011387340782675892\n",
            "Epoch  5 Batch  242 / 447  Training Loss  0.00011180478031747043\n",
            "Epoch  5 Batch  243 / 447  Training Loss  9.925920312525705e-05\n",
            "Epoch  5 Batch  244 / 447  Training Loss  0.0002224146155640483\n",
            "Epoch  5 Batch  245 / 447  Training Loss  0.00010264056618325412\n",
            "Epoch  5 Batch  246 / 447  Training Loss  0.00018483759777154773\n",
            "Epoch  5 Batch  247 / 447  Training Loss  0.00016859674360603094\n",
            "Epoch  5 Batch  248 / 447  Training Loss  9.659467468736693e-05\n",
            "Epoch  5 Batch  249 / 447  Training Loss  0.0002090170164592564\n",
            "Epoch  5 Batch  250 / 447  Training Loss  0.00014426070265471935\n",
            "Epoch  5 Batch  251 / 447  Training Loss  0.00010123149695573375\n",
            "Epoch  5 Batch  252 / 447  Training Loss  0.00015033019008114934\n",
            "Epoch  5 Batch  253 / 447  Training Loss  0.00018370947509538382\n",
            "Epoch  5 Batch  254 / 447  Training Loss  8.530446939403191e-05\n",
            "Epoch  5 Batch  255 / 447  Training Loss  7.826339424354956e-05\n",
            "Epoch  5 Batch  256 / 447  Training Loss  0.00012867472833022475\n",
            "Epoch  5 Batch  257 / 447  Training Loss  0.00021614371507894248\n",
            "Epoch  5 Batch  258 / 447  Training Loss  0.0001407298113917932\n",
            "Epoch  5 Batch  259 / 447  Training Loss  8.892203914001584e-05\n",
            "Epoch  5 Batch  260 / 447  Training Loss  0.00019619014346972108\n",
            "Epoch  5 Batch  261 / 447  Training Loss  7.935995381558314e-05\n",
            "Epoch  5 Batch  262 / 447  Training Loss  0.00012156285083619878\n",
            "Epoch  5 Batch  263 / 447  Training Loss  0.0001008312901831232\n",
            "Epoch  5 Batch  264 / 447  Training Loss  0.0001271443034056574\n",
            "Epoch  5 Batch  265 / 447  Training Loss  0.0002198220754507929\n",
            "Epoch  5 Batch  266 / 447  Training Loss  0.00010432289127493277\n",
            "Epoch  5 Batch  267 / 447  Training Loss  9.704155672807246e-05\n",
            "Epoch  5 Batch  268 / 447  Training Loss  0.0001376272994093597\n",
            "Epoch  5 Batch  269 / 447  Training Loss  0.00013099968782626092\n",
            "Epoch  5 Batch  270 / 447  Training Loss  8.71452284627594e-05\n",
            "Epoch  5 Batch  271 / 447  Training Loss  0.00010217157250735909\n",
            "Epoch  5 Batch  272 / 447  Training Loss  0.00012424397573340684\n",
            "Epoch  5 Batch  273 / 447  Training Loss  0.00013095732720103115\n",
            "Epoch  5 Batch  274 / 447  Training Loss  9.984261123463511e-05\n",
            "Epoch  5 Batch  275 / 447  Training Loss  0.00011118929978692904\n",
            "Epoch  5 Batch  276 / 447  Training Loss  9.931779641192406e-05\n",
            "Epoch  5 Batch  277 / 447  Training Loss  0.00011141023423988372\n",
            "Epoch  5 Batch  278 / 447  Training Loss  7.971234299475327e-05\n",
            "Epoch  5 Batch  279 / 447  Training Loss  0.00013509744894690812\n",
            "Epoch  5 Batch  280 / 447  Training Loss  7.269412162713706e-05\n",
            "Epoch  5 Batch  281 / 447  Training Loss  0.00034020293969661\n",
            "Epoch  5 Batch  282 / 447  Training Loss  8.425789565080777e-05\n",
            "Epoch  5 Batch  283 / 447  Training Loss  0.0010131378658115864\n",
            "Epoch  5 Batch  284 / 447  Training Loss  0.0001378308079438284\n",
            "Epoch  5 Batch  285 / 447  Training Loss  0.005061016883701086\n",
            "Epoch  5 Batch  286 / 447  Training Loss  8.468882879242301e-05\n",
            "Epoch  5 Batch  287 / 447  Training Loss  0.00019249993783887476\n",
            "Epoch  5 Batch  288 / 447  Training Loss  0.0001917862828122452\n",
            "Epoch  5 Batch  289 / 447  Training Loss  0.0004549688892439008\n",
            "Epoch  5 Batch  290 / 447  Training Loss  0.00016291117935907096\n",
            "Epoch  5 Batch  291 / 447  Training Loss  0.00010627511073835194\n",
            "Epoch  5 Batch  292 / 447  Training Loss  8.523815631633624e-05\n",
            "Epoch  5 Batch  293 / 447  Training Loss  0.0001376124273519963\n",
            "Epoch  5 Batch  294 / 447  Training Loss  8.60548680066131e-05\n",
            "Epoch  5 Batch  295 / 447  Training Loss  0.00010202535486314446\n",
            "Epoch  5 Batch  296 / 447  Training Loss  0.00011967827595071867\n",
            "Epoch  5 Batch  297 / 447  Training Loss  0.000309173105051741\n",
            "Epoch  5 Batch  298 / 447  Training Loss  0.0001545634731883183\n",
            "Epoch  5 Batch  299 / 447  Training Loss  0.00018114848353434354\n",
            "Epoch  5 Batch  300 / 447  Training Loss  0.0001769395312294364\n",
            "Epoch  5 Batch  301 / 447  Training Loss  0.00014124838344287127\n",
            "Epoch  5 Batch  302 / 447  Training Loss  0.0002221594622824341\n",
            "Epoch  5 Batch  303 / 447  Training Loss  0.00013082010264042765\n",
            "Epoch  5 Batch  304 / 447  Training Loss  0.0002903480199165642\n",
            "Epoch  5 Batch  305 / 447  Training Loss  0.00021099350124131888\n",
            "Epoch  5 Batch  306 / 447  Training Loss  0.00021564967755693942\n",
            "Epoch  5 Batch  307 / 447  Training Loss  0.00010233016655547544\n",
            "Epoch  5 Batch  308 / 447  Training Loss  9.116217552218586e-05\n",
            "Epoch  5 Batch  309 / 447  Training Loss  7.209891191450879e-05\n",
            "Epoch  5 Batch  310 / 447  Training Loss  0.00013449428661260754\n",
            "Epoch  5 Batch  311 / 447  Training Loss  8.689004607731476e-05\n",
            "Epoch  5 Batch  312 / 447  Training Loss  0.00011011945753125474\n",
            "Epoch  5 Batch  313 / 447  Training Loss  0.00010491399734746665\n",
            "Epoch  5 Batch  314 / 447  Training Loss  8.74721517902799e-05\n",
            "Epoch  5 Batch  315 / 447  Training Loss  6.276152998907492e-05\n",
            "Epoch  5 Batch  316 / 447  Training Loss  7.79059118940495e-05\n",
            "Epoch  5 Batch  317 / 447  Training Loss  8.520450501237065e-05\n",
            "Epoch  5 Batch  318 / 447  Training Loss  8.869366138242185e-05\n",
            "Epoch  5 Batch  319 / 447  Training Loss  0.00023891348973847926\n",
            "Epoch  5 Batch  320 / 447  Training Loss  0.0001504073297837749\n",
            "Epoch  5 Batch  321 / 447  Training Loss  0.00010564476542640477\n",
            "Epoch  5 Batch  322 / 447  Training Loss  0.00029372519929893315\n",
            "Epoch  5 Batch  323 / 447  Training Loss  0.00020754497381858528\n",
            "Epoch  5 Batch  324 / 447  Training Loss  0.00011384563549654558\n",
            "Epoch  5 Batch  325 / 447  Training Loss  0.00010748650674941018\n",
            "Epoch  5 Batch  326 / 447  Training Loss  8.392001473112032e-05\n",
            "Epoch  5 Batch  327 / 447  Training Loss  0.0001195875956909731\n",
            "Epoch  5 Batch  328 / 447  Training Loss  9.44632847676985e-05\n",
            "Epoch  5 Batch  329 / 447  Training Loss  0.00011901566904271021\n",
            "Epoch  5 Batch  330 / 447  Training Loss  9.334743663202971e-05\n",
            "Epoch  5 Batch  331 / 447  Training Loss  8.404174877796322e-05\n",
            "Epoch  5 Batch  332 / 447  Training Loss  9.412811778020114e-05\n",
            "Epoch  5 Batch  333 / 447  Training Loss  0.00011318841279717162\n",
            "Epoch  5 Batch  334 / 447  Training Loss  0.00015163831994868815\n",
            "Epoch  5 Batch  335 / 447  Training Loss  0.00012139635509811342\n",
            "Epoch  5 Batch  336 / 447  Training Loss  0.000311049836454913\n",
            "Epoch  5 Batch  337 / 447  Training Loss  0.00015201613132376224\n",
            "Epoch  5 Batch  338 / 447  Training Loss  6.93473921273835e-05\n",
            "Epoch  5 Batch  339 / 447  Training Loss  7.367983198491856e-05\n",
            "Epoch  5 Batch  340 / 447  Training Loss  0.0001787150977179408\n",
            "Epoch  5 Batch  341 / 447  Training Loss  7.689863559789956e-05\n",
            "Epoch  5 Batch  342 / 447  Training Loss  0.00015596904268022627\n",
            "Epoch  5 Batch  343 / 447  Training Loss  8.193963003577664e-05\n",
            "Epoch  5 Batch  344 / 447  Training Loss  0.00011388507846277207\n",
            "Epoch  5 Batch  345 / 447  Training Loss  8.690240792930126e-05\n",
            "Epoch  5 Batch  346 / 447  Training Loss  7.110914884833619e-05\n",
            "Epoch  5 Batch  347 / 447  Training Loss  0.00010065806418424472\n",
            "Epoch  5 Batch  348 / 447  Training Loss  0.0006450976943597198\n",
            "Epoch  5 Batch  349 / 447  Training Loss  9.808818140299991e-05\n",
            "Epoch  5 Batch  350 / 447  Training Loss  0.0001635989610804245\n",
            "Epoch  5 Batch  351 / 447  Training Loss  0.0003658631758298725\n",
            "Epoch  5 Batch  352 / 447  Training Loss  0.0001334567932644859\n",
            "Epoch  5 Batch  353 / 447  Training Loss  0.00010921021021204069\n",
            "Epoch  5 Batch  354 / 447  Training Loss  0.00013909401604905725\n",
            "Epoch  5 Batch  355 / 447  Training Loss  0.00010385169298388064\n",
            "Epoch  5 Batch  356 / 447  Training Loss  7.456523599103093e-05\n",
            "Epoch  5 Batch  357 / 447  Training Loss  0.0013706196332350373\n",
            "Epoch  5 Batch  358 / 447  Training Loss  0.00025577907217666507\n",
            "Epoch  5 Batch  359 / 447  Training Loss  9.360769035993144e-05\n",
            "Epoch  5 Batch  360 / 447  Training Loss  6.421014404622838e-05\n",
            "Epoch  5 Batch  361 / 447  Training Loss  0.00011903380072908476\n",
            "Epoch  5 Batch  362 / 447  Training Loss  0.00012955692363902926\n",
            "Epoch  5 Batch  363 / 447  Training Loss  0.00011972983338637277\n",
            "Epoch  5 Batch  364 / 447  Training Loss  0.00015956995775923133\n",
            "Epoch  5 Batch  365 / 447  Training Loss  0.00013236032100394368\n",
            "Epoch  5 Batch  366 / 447  Training Loss  9.093544213101268e-05\n",
            "Epoch  5 Batch  367 / 447  Training Loss  0.0001001537311822176\n",
            "Epoch  5 Batch  368 / 447  Training Loss  0.0006204821402207017\n",
            "Epoch  5 Batch  369 / 447  Training Loss  9.889600914902985e-05\n",
            "Epoch  5 Batch  370 / 447  Training Loss  7.44517456041649e-05\n",
            "Epoch  5 Batch  371 / 447  Training Loss  6.953398406039923e-05\n",
            "Epoch  5 Batch  372 / 447  Training Loss  9.194233280140907e-05\n",
            "Epoch  5 Batch  373 / 447  Training Loss  8.19566412246786e-05\n",
            "Epoch  5 Batch  374 / 447  Training Loss  0.00011608476052060723\n",
            "Epoch  5 Batch  375 / 447  Training Loss  9.309064625995234e-05\n",
            "Epoch  5 Batch  376 / 447  Training Loss  6.280976231209934e-05\n",
            "Epoch  5 Batch  377 / 447  Training Loss  7.936518522910774e-05\n",
            "Epoch  5 Batch  378 / 447  Training Loss  6.461238081101328e-05\n",
            "Epoch  5 Batch  379 / 447  Training Loss  0.00011668301885947585\n",
            "Epoch  5 Batch  380 / 447  Training Loss  9.72509296843782e-05\n",
            "Epoch  5 Batch  381 / 447  Training Loss  0.00010632970224833116\n",
            "Epoch  5 Batch  382 / 447  Training Loss  6.828700134065002e-05\n",
            "Epoch  5 Batch  383 / 447  Training Loss  6.864110036985949e-05\n",
            "Epoch  5 Batch  384 / 447  Training Loss  8.688550587976351e-05\n",
            "Epoch  5 Batch  385 / 447  Training Loss  0.00015137350419536233\n",
            "Epoch  5 Batch  386 / 447  Training Loss  0.00065375812118873\n",
            "Epoch  5 Batch  387 / 447  Training Loss  7.430963160004467e-05\n",
            "Epoch  5 Batch  388 / 447  Training Loss  0.00011468010779935867\n",
            "Epoch  5 Batch  389 / 447  Training Loss  9.156620217254385e-05\n",
            "Epoch  5 Batch  390 / 447  Training Loss  8.05649469839409e-05\n",
            "Epoch  5 Batch  391 / 447  Training Loss  8.858466753736138e-05\n",
            "Epoch  5 Batch  392 / 447  Training Loss  0.00011016558710252866\n",
            "Epoch  5 Batch  393 / 447  Training Loss  9.153821156360209e-05\n",
            "Epoch  5 Batch  394 / 447  Training Loss  0.00015323780826292932\n",
            "Epoch  5 Batch  395 / 447  Training Loss  8.527978206984699e-05\n",
            "Epoch  5 Batch  396 / 447  Training Loss  0.00010410936374682933\n",
            "Epoch  5 Batch  397 / 447  Training Loss  8.951357449404895e-05\n",
            "Epoch  5 Batch  398 / 447  Training Loss  5.89448754908517e-05\n",
            "Epoch  5 Batch  399 / 447  Training Loss  5.4737938626203686e-05\n",
            "Epoch  5 Batch  400 / 447  Training Loss  9.800433326745406e-05\n",
            "Epoch  5 Batch  401 / 447  Training Loss  0.00012747915752697736\n",
            "Epoch  5 Batch  402 / 447  Training Loss  7.069938874337822e-05\n",
            "Epoch  5 Batch  403 / 447  Training Loss  0.00010974030010402203\n",
            "Epoch  5 Batch  404 / 447  Training Loss  0.00010790782835101709\n",
            "Epoch  5 Batch  405 / 447  Training Loss  6.467129423981532e-05\n",
            "Epoch  5 Batch  406 / 447  Training Loss  5.285088991513476e-05\n",
            "Epoch  5 Batch  407 / 447  Training Loss  7.444236689480022e-05\n",
            "Epoch  5 Batch  408 / 447  Training Loss  6.238476635189727e-05\n",
            "Epoch  5 Batch  409 / 447  Training Loss  0.00020713050616905093\n",
            "Epoch  5 Batch  410 / 447  Training Loss  8.300488843815401e-05\n",
            "Epoch  5 Batch  411 / 447  Training Loss  7.887036190368235e-05\n",
            "Epoch  5 Batch  412 / 447  Training Loss  9.229273564415053e-05\n",
            "Epoch  5 Batch  413 / 447  Training Loss  7.330683001782745e-05\n",
            "Epoch  5 Batch  414 / 447  Training Loss  7.024931255728006e-05\n",
            "Epoch  5 Batch  415 / 447  Training Loss  8.161972073139623e-05\n",
            "Epoch  5 Batch  416 / 447  Training Loss  9.979910828405991e-05\n",
            "Epoch  5 Batch  417 / 447  Training Loss  6.157721509225667e-05\n",
            "Epoch  5 Batch  418 / 447  Training Loss  3.9493137592216954e-05\n",
            "Epoch  5 Batch  419 / 447  Training Loss  5.327248072717339e-05\n",
            "Epoch  5 Batch  420 / 447  Training Loss  8.32364967209287e-05\n",
            "Epoch  5 Batch  421 / 447  Training Loss  7.888559048296884e-05\n",
            "Epoch  5 Batch  422 / 447  Training Loss  0.00010920918430201709\n",
            "Epoch  5 Batch  423 / 447  Training Loss  0.00011662111501209438\n",
            "Epoch  5 Batch  424 / 447  Training Loss  9.241166844731197e-05\n",
            "Epoch  5 Batch  425 / 447  Training Loss  7.810755778336897e-05\n",
            "Epoch  5 Batch  426 / 447  Training Loss  7.475611346308142e-05\n",
            "Epoch  5 Batch  427 / 447  Training Loss  5.161800800124183e-05\n",
            "Epoch  5 Batch  428 / 447  Training Loss  7.800863386364654e-05\n",
            "Epoch  5 Batch  429 / 447  Training Loss  5.224561027716845e-05\n",
            "Epoch  5 Batch  430 / 447  Training Loss  5.4173535318113863e-05\n",
            "Epoch  5 Batch  431 / 447  Training Loss  6.256419146666303e-05\n",
            "Epoch  5 Batch  432 / 447  Training Loss  7.711440412094817e-05\n",
            "Epoch  5 Batch  433 / 447  Training Loss  8.656048157718033e-05\n",
            "Epoch  5 Batch  434 / 447  Training Loss  8.30990175018087e-05\n",
            "Epoch  5 Batch  435 / 447  Training Loss  0.0001639675028854981\n",
            "Epoch  5 Batch  436 / 447  Training Loss  6.540599861182272e-05\n",
            "Epoch  5 Batch  437 / 447  Training Loss  0.00018676480976864696\n",
            "Epoch  5 Batch  438 / 447  Training Loss  5.8562505728332326e-05\n",
            "Epoch  5 Batch  439 / 447  Training Loss  7.298323907889426e-05\n",
            "Epoch  5 Batch  440 / 447  Training Loss  0.00010329522774554789\n",
            "Epoch  5 Batch  441 / 447  Training Loss  5.15596657351125e-05\n",
            "Epoch  5 Batch  442 / 447  Training Loss  5.882496043341234e-05\n",
            "Epoch  5 Batch  443 / 447  Training Loss  9.83464706223458e-05\n",
            "Epoch  5 Batch  444 / 447  Training Loss  8.146424079313874e-05\n",
            "Epoch  5 Batch  445 / 447  Training Loss  7.758543506497517e-05\n",
            "Epoch  5 Batch  446 / 447  Training Loss  0.00012674331082962453\n",
            "   6    |    -    |   0.000238   | 99.813871\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 6\n",
            "Epoch  6 Batch  0 / 447  Training Loss  5.4719439503969625e-05\n",
            "Epoch  6 Batch  1 / 447  Training Loss  6.345529982354492e-05\n",
            "Epoch  6 Batch  2 / 447  Training Loss  8.807869016891345e-05\n",
            "Epoch  6 Batch  3 / 447  Training Loss  0.00021890236530452967\n",
            "Epoch  6 Batch  4 / 447  Training Loss  5.8793411881197244e-05\n",
            "Epoch  6 Batch  5 / 447  Training Loss  0.00010984659456880763\n",
            "Epoch  6 Batch  6 / 447  Training Loss  9.481093002250418e-05\n",
            "Epoch  6 Batch  7 / 447  Training Loss  7.552799070253968e-05\n",
            "Epoch  6 Batch  8 / 447  Training Loss  7.075353641994298e-05\n",
            "Epoch  6 Batch  9 / 447  Training Loss  0.00010384807683294639\n",
            "Epoch  6 Batch  10 / 447  Training Loss  7.420600013574585e-05\n",
            "Epoch  6 Batch  11 / 447  Training Loss  0.00020779125043191016\n",
            "Epoch  6 Batch  12 / 447  Training Loss  7.203433779068291e-05\n",
            "Epoch  6 Batch  13 / 447  Training Loss  6.571320409420878e-05\n",
            "Epoch  6 Batch  14 / 447  Training Loss  6.212750304257497e-05\n",
            "Epoch  6 Batch  15 / 447  Training Loss  0.00010868274694075808\n",
            "Epoch  6 Batch  16 / 447  Training Loss  8.232400432461873e-05\n",
            "Epoch  6 Batch  17 / 447  Training Loss  5.127261101733893e-05\n",
            "Epoch  6 Batch  18 / 447  Training Loss  5.1815710321534425e-05\n",
            "Epoch  6 Batch  19 / 447  Training Loss  6.710565503453836e-05\n",
            "Epoch  6 Batch  20 / 447  Training Loss  8.061045809881762e-05\n",
            "Epoch  6 Batch  21 / 447  Training Loss  4.8690348194213584e-05\n",
            "Epoch  6 Batch  22 / 447  Training Loss  7.175472273956984e-05\n",
            "Epoch  6 Batch  23 / 447  Training Loss  7.502356311306357e-05\n",
            "Epoch  6 Batch  24 / 447  Training Loss  6.511015817523003e-05\n",
            "Epoch  6 Batch  25 / 447  Training Loss  7.346637721639127e-05\n",
            "Epoch  6 Batch  26 / 447  Training Loss  6.991482950979844e-05\n",
            "Epoch  6 Batch  27 / 447  Training Loss  8.582515874877572e-05\n",
            "Epoch  6 Batch  28 / 447  Training Loss  9.273571777157485e-05\n",
            "Epoch  6 Batch  29 / 447  Training Loss  5.6453187426086515e-05\n",
            "Epoch  6 Batch  30 / 447  Training Loss  7.136885687941685e-05\n",
            "Epoch  6 Batch  31 / 447  Training Loss  7.783745968481526e-05\n",
            "Epoch  6 Batch  32 / 447  Training Loss  5.6198692618636414e-05\n",
            "Epoch  6 Batch  33 / 447  Training Loss  8.497949602315202e-05\n",
            "Epoch  6 Batch  34 / 447  Training Loss  0.00010439886682434008\n",
            "Epoch  6 Batch  35 / 447  Training Loss  0.00010215105430688709\n",
            "Epoch  6 Batch  36 / 447  Training Loss  0.00010248376929666847\n",
            "Epoch  6 Batch  37 / 447  Training Loss  0.00011091802298324183\n",
            "Epoch  6 Batch  38 / 447  Training Loss  7.02277829986997e-05\n",
            "Epoch  6 Batch  39 / 447  Training Loss  7.959235517773777e-05\n",
            "Epoch  6 Batch  40 / 447  Training Loss  7.680830458411947e-05\n",
            "Epoch  6 Batch  41 / 447  Training Loss  9.072203101823106e-05\n",
            "Epoch  6 Batch  42 / 447  Training Loss  7.706778706051409e-05\n",
            "Epoch  6 Batch  43 / 447  Training Loss  6.889279757160693e-05\n",
            "Epoch  6 Batch  44 / 447  Training Loss  6.273231701925397e-05\n",
            "Epoch  6 Batch  45 / 447  Training Loss  6.795787339797243e-05\n",
            "Epoch  6 Batch  46 / 447  Training Loss  6.962849874980748e-05\n",
            "Epoch  6 Batch  47 / 447  Training Loss  5.245572901912965e-05\n",
            "Epoch  6 Batch  48 / 447  Training Loss  7.655687659280375e-05\n",
            "Epoch  6 Batch  49 / 447  Training Loss  5.8344412536825985e-05\n",
            "Epoch  6 Batch  50 / 447  Training Loss  6.951657996978611e-05\n",
            "Epoch  6 Batch  51 / 447  Training Loss  5.0286034820601344e-05\n",
            "Epoch  6 Batch  52 / 447  Training Loss  3.9830705645726994e-05\n",
            "Epoch  6 Batch  53 / 447  Training Loss  7.386786455754191e-05\n",
            "Epoch  6 Batch  54 / 447  Training Loss  5.989567216602154e-05\n",
            "Epoch  6 Batch  55 / 447  Training Loss  6.145538645796478e-05\n",
            "Epoch  6 Batch  56 / 447  Training Loss  4.712184454547241e-05\n",
            "Epoch  6 Batch  57 / 447  Training Loss  7.44433855288662e-05\n",
            "Epoch  6 Batch  58 / 447  Training Loss  4.9763708375394344e-05\n",
            "Epoch  6 Batch  59 / 447  Training Loss  7.564426050521433e-05\n",
            "Epoch  6 Batch  60 / 447  Training Loss  5.7355136959813535e-05\n",
            "Epoch  6 Batch  61 / 447  Training Loss  7.37892187316902e-05\n",
            "Epoch  6 Batch  62 / 447  Training Loss  9.38243028940633e-05\n",
            "Epoch  6 Batch  63 / 447  Training Loss  6.75466944812797e-05\n",
            "Epoch  6 Batch  64 / 447  Training Loss  5.8222962252330035e-05\n",
            "Epoch  6 Batch  65 / 447  Training Loss  6.338999082800001e-05\n",
            "Epoch  6 Batch  66 / 447  Training Loss  5.970045458525419e-05\n",
            "Epoch  6 Batch  67 / 447  Training Loss  6.914149707881734e-05\n",
            "Epoch  6 Batch  68 / 447  Training Loss  4.861272100242786e-05\n",
            "Epoch  6 Batch  69 / 447  Training Loss  8.731086563784629e-05\n",
            "Epoch  6 Batch  70 / 447  Training Loss  0.00040710336179472506\n",
            "Epoch  6 Batch  71 / 447  Training Loss  6.201914948178455e-05\n",
            "Epoch  6 Batch  72 / 447  Training Loss  6.796009256504476e-05\n",
            "Epoch  6 Batch  73 / 447  Training Loss  7.722723967162892e-05\n",
            "Epoch  6 Batch  74 / 447  Training Loss  6.599964399356395e-05\n",
            "Epoch  6 Batch  75 / 447  Training Loss  8.387201523873955e-05\n",
            "Epoch  6 Batch  76 / 447  Training Loss  5.487668022396974e-05\n",
            "Epoch  6 Batch  77 / 447  Training Loss  5.6028682593023404e-05\n",
            "Epoch  6 Batch  78 / 447  Training Loss  5.8454377722227946e-05\n",
            "Epoch  6 Batch  79 / 447  Training Loss  6.431168003473431e-05\n",
            "Epoch  6 Batch  80 / 447  Training Loss  4.607399023370817e-05\n",
            "Epoch  6 Batch  81 / 447  Training Loss  6.151635898277164e-05\n",
            "Epoch  6 Batch  82 / 447  Training Loss  5.3248091717250645e-05\n",
            "Epoch  6 Batch  83 / 447  Training Loss  7.23224293324165e-05\n",
            "Epoch  6 Batch  84 / 447  Training Loss  0.00011652710236376151\n",
            "Epoch  6 Batch  85 / 447  Training Loss  0.00014338498294819146\n",
            "Epoch  6 Batch  86 / 447  Training Loss  5.039236566517502e-05\n",
            "Epoch  6 Batch  87 / 447  Training Loss  7.053817535052076e-05\n",
            "Epoch  6 Batch  88 / 447  Training Loss  5.73755351069849e-05\n",
            "Epoch  6 Batch  89 / 447  Training Loss  9.145064541371539e-05\n",
            "Epoch  6 Batch  90 / 447  Training Loss  6.309585296548903e-05\n",
            "Epoch  6 Batch  91 / 447  Training Loss  6.920773739693686e-05\n",
            "Epoch  6 Batch  92 / 447  Training Loss  4.434675065567717e-05\n",
            "Epoch  6 Batch  93 / 447  Training Loss  0.00022009192616678774\n",
            "Epoch  6 Batch  94 / 447  Training Loss  6.919146107975394e-05\n",
            "Epoch  6 Batch  95 / 447  Training Loss  6.842743459856138e-05\n",
            "Epoch  6 Batch  96 / 447  Training Loss  8.445965795544907e-05\n",
            "Epoch  6 Batch  97 / 447  Training Loss  5.7426812418270856e-05\n",
            "Epoch  6 Batch  98 / 447  Training Loss  5.7599212595960125e-05\n",
            "Epoch  6 Batch  99 / 447  Training Loss  5.511411291081458e-05\n",
            "Epoch  6 Batch  100 / 447  Training Loss  7.02478937455453e-05\n",
            "Epoch  6 Batch  101 / 447  Training Loss  0.00010321303852833807\n",
            "Epoch  6 Batch  102 / 447  Training Loss  6.790865154471248e-05\n",
            "Epoch  6 Batch  103 / 447  Training Loss  5.1208349759690464e-05\n",
            "Epoch  6 Batch  104 / 447  Training Loss  4.7800116590224206e-05\n",
            "Epoch  6 Batch  105 / 447  Training Loss  5.7185116020264104e-05\n",
            "Epoch  6 Batch  106 / 447  Training Loss  6.76957715768367e-05\n",
            "Epoch  6 Batch  107 / 447  Training Loss  6.64742910885252e-05\n",
            "Epoch  6 Batch  108 / 447  Training Loss  4.533588798949495e-05\n",
            "Epoch  6 Batch  109 / 447  Training Loss  6.327435403363779e-05\n",
            "Epoch  6 Batch  110 / 447  Training Loss  9.052778477780521e-05\n",
            "Epoch  6 Batch  111 / 447  Training Loss  5.5031778174452484e-05\n",
            "Epoch  6 Batch  112 / 447  Training Loss  6.281887908698991e-05\n",
            "Epoch  6 Batch  113 / 447  Training Loss  5.083354699308984e-05\n",
            "Epoch  6 Batch  114 / 447  Training Loss  5.810987568111159e-05\n",
            "Epoch  6 Batch  115 / 447  Training Loss  6.506554927909747e-05\n",
            "Epoch  6 Batch  116 / 447  Training Loss  4.6968041715445e-05\n",
            "Epoch  6 Batch  117 / 447  Training Loss  7.188514427980408e-05\n",
            "Epoch  6 Batch  118 / 447  Training Loss  9.680959192337468e-05\n",
            "Epoch  6 Batch  119 / 447  Training Loss  6.029029827914201e-05\n",
            "Epoch  6 Batch  120 / 447  Training Loss  5.504667205968872e-05\n",
            "Epoch  6 Batch  121 / 447  Training Loss  6.543719791807234e-05\n",
            "Epoch  6 Batch  122 / 447  Training Loss  5.369535574573092e-05\n",
            "Epoch  6 Batch  123 / 447  Training Loss  0.00017697885050438344\n",
            "Epoch  6 Batch  124 / 447  Training Loss  4.671961869462393e-05\n",
            "Epoch  6 Batch  125 / 447  Training Loss  4.3594525777734816e-05\n",
            "Epoch  6 Batch  126 / 447  Training Loss  7.067951082717627e-05\n",
            "Epoch  6 Batch  127 / 447  Training Loss  0.0001460130006307736\n",
            "Epoch  6 Batch  128 / 447  Training Loss  0.00011776640894822776\n",
            "Epoch  6 Batch  129 / 447  Training Loss  5.0979433581233025e-05\n",
            "Epoch  6 Batch  130 / 447  Training Loss  6.035491242073476e-05\n",
            "Epoch  6 Batch  131 / 447  Training Loss  4.947939305566251e-05\n",
            "Epoch  6 Batch  132 / 447  Training Loss  6.135056173661724e-05\n",
            "Epoch  6 Batch  133 / 447  Training Loss  4.7779140004422516e-05\n",
            "Epoch  6 Batch  134 / 447  Training Loss  5.798230995424092e-05\n",
            "Epoch  6 Batch  135 / 447  Training Loss  5.407330172602087e-05\n",
            "Epoch  6 Batch  136 / 447  Training Loss  8.473168418277055e-05\n",
            "Epoch  6 Batch  137 / 447  Training Loss  4.345929846749641e-05\n",
            "Epoch  6 Batch  138 / 447  Training Loss  8.378936763620004e-05\n",
            "Epoch  6 Batch  139 / 447  Training Loss  5.463387424242683e-05\n",
            "Epoch  6 Batch  140 / 447  Training Loss  6.247164856176823e-05\n",
            "Epoch  6 Batch  141 / 447  Training Loss  5.985660391161218e-05\n",
            "Epoch  6 Batch  142 / 447  Training Loss  4.696920223068446e-05\n",
            "Epoch  6 Batch  143 / 447  Training Loss  8.174842514563352e-05\n",
            "Epoch  6 Batch  144 / 447  Training Loss  6.318902887869626e-05\n",
            "Epoch  6 Batch  145 / 447  Training Loss  6.877688429085538e-05\n",
            "Epoch  6 Batch  146 / 447  Training Loss  6.13704978604801e-05\n",
            "Epoch  6 Batch  147 / 447  Training Loss  5.00448550155852e-05\n",
            "Epoch  6 Batch  148 / 447  Training Loss  5.881878678337671e-05\n",
            "Epoch  6 Batch  149 / 447  Training Loss  3.975555227953009e-05\n",
            "Epoch  6 Batch  150 / 447  Training Loss  3.769322938751429e-05\n",
            "Epoch  6 Batch  151 / 447  Training Loss  5.6470362324034795e-05\n",
            "Epoch  6 Batch  152 / 447  Training Loss  5.462623812491074e-05\n",
            "Epoch  6 Batch  153 / 447  Training Loss  5.0296977860853076e-05\n",
            "Epoch  6 Batch  154 / 447  Training Loss  5.5672200687695295e-05\n",
            "Epoch  6 Batch  155 / 447  Training Loss  6.620313797611743e-05\n",
            "Epoch  6 Batch  156 / 447  Training Loss  6.198964547365904e-05\n",
            "Epoch  6 Batch  157 / 447  Training Loss  5.593632522504777e-05\n",
            "Epoch  6 Batch  158 / 447  Training Loss  6.393084186129272e-05\n",
            "Epoch  6 Batch  159 / 447  Training Loss  4.090854054084048e-05\n",
            "Epoch  6 Batch  160 / 447  Training Loss  4.610035102814436e-05\n",
            "Epoch  6 Batch  161 / 447  Training Loss  5.64059337193612e-05\n",
            "Epoch  6 Batch  162 / 447  Training Loss  5.1871877076337114e-05\n",
            "Epoch  6 Batch  163 / 447  Training Loss  9.168845281237736e-05\n",
            "Epoch  6 Batch  164 / 447  Training Loss  4.509392965701409e-05\n",
            "Epoch  6 Batch  165 / 447  Training Loss  5.487156522576697e-05\n",
            "Epoch  6 Batch  166 / 447  Training Loss  7.378129521384835e-05\n",
            "Epoch  6 Batch  167 / 447  Training Loss  4.003843423561193e-05\n",
            "Epoch  6 Batch  168 / 447  Training Loss  5.784397944808006e-05\n",
            "Epoch  6 Batch  169 / 447  Training Loss  4.609711322700605e-05\n",
            "Epoch  6 Batch  170 / 447  Training Loss  4.485661702346988e-05\n",
            "Epoch  6 Batch  171 / 447  Training Loss  6.156365270726383e-05\n",
            "Epoch  6 Batch  172 / 447  Training Loss  6.634867895627394e-05\n",
            "Epoch  6 Batch  173 / 447  Training Loss  6.59400611766614e-05\n",
            "Epoch  6 Batch  174 / 447  Training Loss  4.27630620833952e-05\n",
            "Epoch  6 Batch  175 / 447  Training Loss  6.0141104768263176e-05\n",
            "Epoch  6 Batch  176 / 447  Training Loss  6.0249585658311844e-05\n",
            "Epoch  6 Batch  177 / 447  Training Loss  4.679180710809305e-05\n",
            "Epoch  6 Batch  178 / 447  Training Loss  0.0021851996425539255\n",
            "Epoch  6 Batch  179 / 447  Training Loss  0.004014560952782631\n",
            "Epoch  6 Batch  180 / 447  Training Loss  7.574710616609082e-05\n",
            "Epoch  6 Batch  181 / 447  Training Loss  7.228799222502857e-05\n",
            "Epoch  6 Batch  182 / 447  Training Loss  0.00010809052037075162\n",
            "Epoch  6 Batch  183 / 447  Training Loss  4.0896771679399535e-05\n",
            "Epoch  6 Batch  184 / 447  Training Loss  7.015401206444949e-05\n",
            "Epoch  6 Batch  185 / 447  Training Loss  0.0001021626521833241\n",
            "Epoch  6 Batch  186 / 447  Training Loss  6.427725020330399e-05\n",
            "Epoch  6 Batch  187 / 447  Training Loss  5.580044671660289e-05\n",
            "Epoch  6 Batch  188 / 447  Training Loss  4.8280515329679474e-05\n",
            "Epoch  6 Batch  189 / 447  Training Loss  0.0002417662035441026\n",
            "Epoch  6 Batch  190 / 447  Training Loss  5.100135240354575e-05\n",
            "Epoch  6 Batch  191 / 447  Training Loss  8.904377318685874e-05\n",
            "Epoch  6 Batch  192 / 447  Training Loss  7.089119753800333e-05\n",
            "Epoch  6 Batch  193 / 447  Training Loss  7.531779556302354e-05\n",
            "Epoch  6 Batch  194 / 447  Training Loss  5.668298399541527e-05\n",
            "Epoch  6 Batch  195 / 447  Training Loss  5.264617357170209e-05\n",
            "Epoch  6 Batch  196 / 447  Training Loss  5.409974619396962e-05\n",
            "Epoch  6 Batch  197 / 447  Training Loss  3.9509446651209146e-05\n",
            "Epoch  6 Batch  198 / 447  Training Loss  6.532225961564109e-05\n",
            "Epoch  6 Batch  199 / 447  Training Loss  8.893111953511834e-05\n",
            "Epoch  6 Batch  200 / 447  Training Loss  5.435953789856285e-05\n",
            "Epoch  6 Batch  201 / 447  Training Loss  5.466768561745994e-05\n",
            "Epoch  6 Batch  202 / 447  Training Loss  5.0668772018980235e-05\n",
            "Epoch  6 Batch  203 / 447  Training Loss  0.00014847962302155793\n",
            "Epoch  6 Batch  204 / 447  Training Loss  5.74644873267971e-05\n",
            "Epoch  6 Batch  205 / 447  Training Loss  0.0002262921625515446\n",
            "Epoch  6 Batch  206 / 447  Training Loss  8.767450344748795e-05\n",
            "Epoch  6 Batch  207 / 447  Training Loss  8.206188067561015e-05\n",
            "Epoch  6 Batch  208 / 447  Training Loss  3.957805529353209e-05\n",
            "Epoch  6 Batch  209 / 447  Training Loss  6.447503983508795e-05\n",
            "Epoch  6 Batch  210 / 447  Training Loss  4.949756112182513e-05\n",
            "Epoch  6 Batch  211 / 447  Training Loss  0.004566896706819534\n",
            "Epoch  6 Batch  212 / 447  Training Loss  5.541689824895002e-05\n",
            "Epoch  6 Batch  213 / 447  Training Loss  6.655678589595482e-05\n",
            "Epoch  6 Batch  214 / 447  Training Loss  6.594480510102585e-05\n",
            "Epoch  6 Batch  215 / 447  Training Loss  9.315663191955537e-05\n",
            "Epoch  6 Batch  216 / 447  Training Loss  5.623688412015326e-05\n",
            "Epoch  6 Batch  217 / 447  Training Loss  5.4563290177611634e-05\n",
            "Epoch  6 Batch  218 / 447  Training Loss  5.5026623158482835e-05\n",
            "Epoch  6 Batch  219 / 447  Training Loss  7.8355755249504e-05\n",
            "Epoch  6 Batch  220 / 447  Training Loss  0.00011157872359035537\n",
            "Epoch  6 Batch  221 / 447  Training Loss  5.088407124276273e-05\n",
            "Epoch  6 Batch  222 / 447  Training Loss  0.00019647387671284378\n",
            "Epoch  6 Batch  223 / 447  Training Loss  5.938916001468897e-05\n",
            "Epoch  6 Batch  224 / 447  Training Loss  7.479410851374269e-05\n",
            "Epoch  6 Batch  225 / 447  Training Loss  0.0001932171144289896\n",
            "Epoch  6 Batch  226 / 447  Training Loss  8.664548658998683e-05\n",
            "Epoch  6 Batch  227 / 447  Training Loss  6.799467519158497e-05\n",
            "Epoch  6 Batch  228 / 447  Training Loss  7.643363642273471e-05\n",
            "Epoch  6 Batch  229 / 447  Training Loss  6.744136771885678e-05\n",
            "Epoch  6 Batch  230 / 447  Training Loss  6.469266372732818e-05\n",
            "Epoch  6 Batch  231 / 447  Training Loss  7.631070911884308e-05\n",
            "Epoch  6 Batch  232 / 447  Training Loss  5.139707718626596e-05\n",
            "Epoch  6 Batch  233 / 447  Training Loss  7.163090776884928e-05\n",
            "Epoch  6 Batch  234 / 447  Training Loss  4.146369974478148e-05\n",
            "Epoch  6 Batch  235 / 447  Training Loss  8.313059515785426e-05\n",
            "Epoch  6 Batch  236 / 447  Training Loss  6.482511525973678e-05\n",
            "Epoch  6 Batch  237 / 447  Training Loss  6.215840403456241e-05\n",
            "Epoch  6 Batch  238 / 447  Training Loss  6.164466321934015e-05\n",
            "Epoch  6 Batch  239 / 447  Training Loss  4.575365164782852e-05\n",
            "Epoch  6 Batch  240 / 447  Training Loss  4.7756675485288724e-05\n",
            "Epoch  6 Batch  241 / 447  Training Loss  4.6681725507369265e-05\n",
            "Epoch  6 Batch  242 / 447  Training Loss  6.812315405113623e-05\n",
            "Epoch  6 Batch  243 / 447  Training Loss  4.845075818593614e-05\n",
            "Epoch  6 Batch  244 / 447  Training Loss  0.0001639367255847901\n",
            "Epoch  6 Batch  245 / 447  Training Loss  4.483932934817858e-05\n",
            "Epoch  6 Batch  246 / 447  Training Loss  0.00016767866327427328\n",
            "Epoch  6 Batch  247 / 447  Training Loss  4.439764961716719e-05\n",
            "Epoch  6 Batch  248 / 447  Training Loss  5.790125942439772e-05\n",
            "Epoch  6 Batch  249 / 447  Training Loss  6.350290641421452e-05\n",
            "Epoch  6 Batch  250 / 447  Training Loss  0.0001929747377289459\n",
            "Epoch  6 Batch  251 / 447  Training Loss  3.974394712713547e-05\n",
            "Epoch  6 Batch  252 / 447  Training Loss  4.709049972007051e-05\n",
            "Epoch  6 Batch  253 / 447  Training Loss  4.781911047757603e-05\n",
            "Epoch  6 Batch  254 / 447  Training Loss  4.9619342462392524e-05\n",
            "Epoch  6 Batch  255 / 447  Training Loss  7.2393988375552e-05\n",
            "Epoch  6 Batch  256 / 447  Training Loss  5.519128535524942e-05\n",
            "Epoch  6 Batch  257 / 447  Training Loss  6.303175905486569e-05\n",
            "Epoch  6 Batch  258 / 447  Training Loss  4.805410571862012e-05\n",
            "Epoch  6 Batch  259 / 447  Training Loss  5.049194805906154e-05\n",
            "Epoch  6 Batch  260 / 447  Training Loss  6.843197479611263e-05\n",
            "Epoch  6 Batch  261 / 447  Training Loss  4.769301085616462e-05\n",
            "Epoch  6 Batch  262 / 447  Training Loss  9.639128256822005e-05\n",
            "Epoch  6 Batch  263 / 447  Training Loss  4.703479862655513e-05\n",
            "Epoch  6 Batch  264 / 447  Training Loss  6.206217949511483e-05\n",
            "Epoch  6 Batch  265 / 447  Training Loss  6.490733358077705e-05\n",
            "Epoch  6 Batch  266 / 447  Training Loss  4.6181939978851005e-05\n",
            "Epoch  6 Batch  267 / 447  Training Loss  5.63913636142388e-05\n",
            "Epoch  6 Batch  268 / 447  Training Loss  5.7098503020824865e-05\n",
            "Epoch  6 Batch  269 / 447  Training Loss  5.412099926616065e-05\n",
            "Epoch  6 Batch  270 / 447  Training Loss  6.284212577156723e-05\n",
            "Epoch  6 Batch  271 / 447  Training Loss  6.279549415921792e-05\n",
            "Epoch  6 Batch  272 / 447  Training Loss  7.998197543201968e-05\n",
            "Epoch  6 Batch  273 / 447  Training Loss  4.6329409087775275e-05\n",
            "Epoch  6 Batch  274 / 447  Training Loss  6.132388080004603e-05\n",
            "Epoch  6 Batch  275 / 447  Training Loss  8.59020437928848e-05\n",
            "Epoch  6 Batch  276 / 447  Training Loss  0.00046444914187304676\n",
            "Epoch  6 Batch  277 / 447  Training Loss  3.302360710222274e-05\n",
            "Epoch  6 Batch  278 / 447  Training Loss  0.0002612119715195149\n",
            "Epoch  6 Batch  279 / 447  Training Loss  5.394877734943293e-05\n",
            "Epoch  6 Batch  280 / 447  Training Loss  4.9728296289686114e-05\n",
            "Epoch  6 Batch  281 / 447  Training Loss  7.232416101032868e-05\n",
            "Epoch  6 Batch  282 / 447  Training Loss  7.991012535057962e-05\n",
            "Epoch  6 Batch  283 / 447  Training Loss  7.134061161195859e-05\n",
            "Epoch  6 Batch  284 / 447  Training Loss  5.469927054946311e-05\n",
            "Epoch  6 Batch  285 / 447  Training Loss  7.431908306898549e-05\n",
            "Epoch  6 Batch  286 / 447  Training Loss  5.257971497485414e-05\n",
            "Epoch  6 Batch  287 / 447  Training Loss  7.837764860596508e-05\n",
            "Epoch  6 Batch  288 / 447  Training Loss  0.0001284851459786296\n",
            "Epoch  6 Batch  289 / 447  Training Loss  6.019723150529899e-05\n",
            "Epoch  6 Batch  290 / 447  Training Loss  4.5073804358253255e-05\n",
            "Epoch  6 Batch  291 / 447  Training Loss  3.5603585274657235e-05\n",
            "Epoch  6 Batch  292 / 447  Training Loss  8.743299986235797e-05\n",
            "Epoch  6 Batch  293 / 447  Training Loss  6.372211646521464e-05\n",
            "Epoch  6 Batch  294 / 447  Training Loss  7.132390601327643e-05\n",
            "Epoch  6 Batch  295 / 447  Training Loss  5.131262878421694e-05\n",
            "Epoch  6 Batch  296 / 447  Training Loss  5.859367593075149e-05\n",
            "Epoch  6 Batch  297 / 447  Training Loss  4.136253483011387e-05\n",
            "Epoch  6 Batch  298 / 447  Training Loss  5.480360414367169e-05\n",
            "Epoch  6 Batch  299 / 447  Training Loss  4.928731868858449e-05\n",
            "Epoch  6 Batch  300 / 447  Training Loss  5.755673555540852e-05\n",
            "Epoch  6 Batch  301 / 447  Training Loss  4.5671597035834566e-05\n",
            "Epoch  6 Batch  302 / 447  Training Loss  6.65313345962204e-05\n",
            "Epoch  6 Batch  303 / 447  Training Loss  6.073509939596988e-05\n",
            "Epoch  6 Batch  304 / 447  Training Loss  4.502894444158301e-05\n",
            "Epoch  6 Batch  305 / 447  Training Loss  5.006489664083347e-05\n",
            "Epoch  6 Batch  306 / 447  Training Loss  6.677437340840697e-05\n",
            "Epoch  6 Batch  307 / 447  Training Loss  4.641207488020882e-05\n",
            "Epoch  6 Batch  308 / 447  Training Loss  5.3509855206357315e-05\n",
            "Epoch  6 Batch  309 / 447  Training Loss  4.839674511458725e-05\n",
            "Epoch  6 Batch  310 / 447  Training Loss  5.757283724960871e-05\n",
            "Epoch  6 Batch  311 / 447  Training Loss  7.927996921353042e-05\n",
            "Epoch  6 Batch  312 / 447  Training Loss  4.212858766550198e-05\n",
            "Epoch  6 Batch  313 / 447  Training Loss  6.14948439761065e-05\n",
            "Epoch  6 Batch  314 / 447  Training Loss  4.27690647484269e-05\n",
            "Epoch  6 Batch  315 / 447  Training Loss  5.210634844843298e-05\n",
            "Epoch  6 Batch  316 / 447  Training Loss  4.822752089239657e-05\n",
            "Epoch  6 Batch  317 / 447  Training Loss  4.9723628762876615e-05\n",
            "Epoch  6 Batch  318 / 447  Training Loss  8.571019134251401e-05\n",
            "Epoch  6 Batch  319 / 447  Training Loss  0.00014253325934987515\n",
            "Epoch  6 Batch  320 / 447  Training Loss  4.408290988067165e-05\n",
            "Epoch  6 Batch  321 / 447  Training Loss  5.0192993512609974e-05\n",
            "Epoch  6 Batch  322 / 447  Training Loss  4.256209285813384e-05\n",
            "Epoch  6 Batch  323 / 447  Training Loss  5.129513010615483e-05\n",
            "Epoch  6 Batch  324 / 447  Training Loss  3.7364028685260564e-05\n",
            "Epoch  6 Batch  325 / 447  Training Loss  4.0438328142045066e-05\n",
            "Epoch  6 Batch  326 / 447  Training Loss  5.310790584189817e-05\n",
            "Epoch  6 Batch  327 / 447  Training Loss  5.4210559028433636e-05\n",
            "Epoch  6 Batch  328 / 447  Training Loss  4.610940959537402e-05\n",
            "Epoch  6 Batch  329 / 447  Training Loss  4.044820161652751e-05\n",
            "Epoch  6 Batch  330 / 447  Training Loss  8.395409531658515e-05\n",
            "Epoch  6 Batch  331 / 447  Training Loss  4.9946553190238774e-05\n",
            "Epoch  6 Batch  332 / 447  Training Loss  6.135189323686063e-05\n",
            "Epoch  6 Batch  333 / 447  Training Loss  6.501093594124541e-05\n",
            "Epoch  6 Batch  334 / 447  Training Loss  3.9704715163679793e-05\n",
            "Epoch  6 Batch  335 / 447  Training Loss  4.617630838765763e-05\n",
            "Epoch  6 Batch  336 / 447  Training Loss  6.136312731541693e-05\n",
            "Epoch  6 Batch  337 / 447  Training Loss  0.004789534956216812\n",
            "Epoch  6 Batch  338 / 447  Training Loss  0.007069307379424572\n",
            "Epoch  6 Batch  339 / 447  Training Loss  0.0005192112294025719\n",
            "Epoch  6 Batch  340 / 447  Training Loss  6.783764547435567e-05\n",
            "Epoch  6 Batch  341 / 447  Training Loss  0.007555232848972082\n",
            "Epoch  6 Batch  342 / 447  Training Loss  0.00014735982404090464\n",
            "Epoch  6 Batch  343 / 447  Training Loss  5.8691850426839665e-05\n",
            "Epoch  6 Batch  344 / 447  Training Loss  0.00011641468154266477\n",
            "Epoch  6 Batch  345 / 447  Training Loss  0.00029808294493705034\n",
            "Epoch  6 Batch  346 / 447  Training Loss  6.265853153308854e-05\n",
            "Epoch  6 Batch  347 / 447  Training Loss  8.042321860557422e-05\n",
            "Epoch  6 Batch  348 / 447  Training Loss  4.242408977006562e-05\n",
            "Epoch  6 Batch  349 / 447  Training Loss  4.4432454160414636e-05\n",
            "Epoch  6 Batch  350 / 447  Training Loss  7.901321077952161e-05\n",
            "Epoch  6 Batch  351 / 447  Training Loss  8.536274253856391e-05\n",
            "Epoch  6 Batch  352 / 447  Training Loss  8.099346450762823e-05\n",
            "Epoch  6 Batch  353 / 447  Training Loss  0.00014602801820728928\n",
            "Epoch  6 Batch  354 / 447  Training Loss  0.0008227958460338414\n",
            "Epoch  6 Batch  355 / 447  Training Loss  9.161318303085864e-05\n",
            "Epoch  6 Batch  356 / 447  Training Loss  7.0520443841815e-05\n",
            "Epoch  6 Batch  357 / 447  Training Loss  6.535270949825644e-05\n",
            "Epoch  6 Batch  358 / 447  Training Loss  0.0016231259796768427\n",
            "Epoch  6 Batch  359 / 447  Training Loss  0.00014085696602705866\n",
            "Epoch  6 Batch  360 / 447  Training Loss  0.001881706528365612\n",
            "Epoch  6 Batch  361 / 447  Training Loss  0.0003731063334271312\n",
            "Epoch  6 Batch  362 / 447  Training Loss  0.00015395801165141165\n",
            "Epoch  6 Batch  363 / 447  Training Loss  0.00510578416287899\n",
            "Epoch  6 Batch  364 / 447  Training Loss  0.00032879956415854394\n",
            "Epoch  6 Batch  365 / 447  Training Loss  0.0001549717562738806\n",
            "Epoch  6 Batch  366 / 447  Training Loss  0.00014519831165671349\n",
            "Epoch  6 Batch  367 / 447  Training Loss  8.464900747640058e-05\n",
            "Epoch  6 Batch  368 / 447  Training Loss  7.196846854640171e-05\n",
            "Epoch  6 Batch  369 / 447  Training Loss  9.292060713050887e-05\n",
            "Epoch  6 Batch  370 / 447  Training Loss  0.00013022267376072705\n",
            "Epoch  6 Batch  371 / 447  Training Loss  0.00024631014093756676\n",
            "Epoch  6 Batch  372 / 447  Training Loss  0.0001717409904813394\n",
            "Epoch  6 Batch  373 / 447  Training Loss  5.42916968697682e-05\n",
            "Epoch  6 Batch  374 / 447  Training Loss  0.00041980668902397156\n",
            "Epoch  6 Batch  375 / 447  Training Loss  0.0016361731104552746\n",
            "Epoch  6 Batch  376 / 447  Training Loss  0.004000508692115545\n",
            "Epoch  6 Batch  377 / 447  Training Loss  0.00027982445317320526\n",
            "Epoch  6 Batch  378 / 447  Training Loss  0.0002033215423580259\n",
            "Epoch  6 Batch  379 / 447  Training Loss  0.004847976379096508\n",
            "Epoch  6 Batch  380 / 447  Training Loss  0.0021839935798197985\n",
            "Epoch  6 Batch  381 / 447  Training Loss  9.645405225455761e-05\n",
            "Epoch  6 Batch  382 / 447  Training Loss  0.007894729264080524\n",
            "Epoch  6 Batch  383 / 447  Training Loss  0.006190730258822441\n",
            "Epoch  6 Batch  384 / 447  Training Loss  0.01620972715318203\n",
            "Epoch  6 Batch  385 / 447  Training Loss  0.0001413775171386078\n",
            "Epoch  6 Batch  386 / 447  Training Loss  0.000143858851515688\n",
            "Epoch  6 Batch  387 / 447  Training Loss  0.00017679264419712126\n",
            "Epoch  6 Batch  388 / 447  Training Loss  0.000413342728279531\n",
            "Epoch  6 Batch  389 / 447  Training Loss  0.0012691873125731945\n",
            "Epoch  6 Batch  390 / 447  Training Loss  0.0011452424805611372\n",
            "Epoch  6 Batch  391 / 447  Training Loss  0.0010273506632074714\n",
            "Epoch  6 Batch  392 / 447  Training Loss  0.0003913894179277122\n",
            "Epoch  6 Batch  393 / 447  Training Loss  0.00018183852080255747\n",
            "Epoch  6 Batch  394 / 447  Training Loss  0.0004884855588898063\n",
            "Epoch  6 Batch  395 / 447  Training Loss  0.00024677260080352426\n",
            "Epoch  6 Batch  396 / 447  Training Loss  0.00048144880565814674\n",
            "Epoch  6 Batch  397 / 447  Training Loss  8.562199946027249e-05\n",
            "Epoch  6 Batch  398 / 447  Training Loss  0.0002990502107422799\n",
            "Epoch  6 Batch  399 / 447  Training Loss  0.0015036967815831304\n",
            "Epoch  6 Batch  400 / 447  Training Loss  0.00024158389715012163\n",
            "Epoch  6 Batch  401 / 447  Training Loss  0.00023619529383722693\n",
            "Epoch  6 Batch  402 / 447  Training Loss  6.65215338813141e-05\n",
            "Epoch  6 Batch  403 / 447  Training Loss  0.00010751464287750423\n",
            "Epoch  6 Batch  404 / 447  Training Loss  0.00041634446824900806\n",
            "Epoch  6 Batch  405 / 447  Training Loss  0.00024106039199978113\n",
            "Epoch  6 Batch  406 / 447  Training Loss  0.00015537279250565916\n",
            "Epoch  6 Batch  407 / 447  Training Loss  0.00011835264012916014\n",
            "Epoch  6 Batch  408 / 447  Training Loss  8.656378486193717e-05\n",
            "Epoch  6 Batch  409 / 447  Training Loss  0.0010430713882669806\n",
            "Epoch  6 Batch  410 / 447  Training Loss  8.871495811035857e-05\n",
            "Epoch  6 Batch  411 / 447  Training Loss  0.00027553169638849795\n",
            "Epoch  6 Batch  412 / 447  Training Loss  0.000313559896312654\n",
            "Epoch  6 Batch  413 / 447  Training Loss  7.684260344831273e-05\n",
            "Epoch  6 Batch  414 / 447  Training Loss  0.00021422345889732242\n",
            "Epoch  6 Batch  415 / 447  Training Loss  0.00021260036737658083\n",
            "Epoch  6 Batch  416 / 447  Training Loss  0.0001539045333629474\n",
            "Epoch  6 Batch  417 / 447  Training Loss  0.0033340975642204285\n",
            "Epoch  6 Batch  418 / 447  Training Loss  0.024100132286548615\n",
            "Epoch  6 Batch  419 / 447  Training Loss  0.0001916546607390046\n",
            "Epoch  6 Batch  420 / 447  Training Loss  0.00022494276345241815\n",
            "Epoch  6 Batch  421 / 447  Training Loss  0.0003993244026787579\n",
            "Epoch  6 Batch  422 / 447  Training Loss  0.00012288200377952307\n",
            "Epoch  6 Batch  423 / 447  Training Loss  0.00023508165031671524\n",
            "Epoch  6 Batch  424 / 447  Training Loss  0.0005516629316844046\n",
            "Epoch  6 Batch  425 / 447  Training Loss  0.00024188497627619654\n",
            "Epoch  6 Batch  426 / 447  Training Loss  0.0001473594893468544\n",
            "Epoch  6 Batch  427 / 447  Training Loss  0.00012483962927944958\n",
            "Epoch  6 Batch  428 / 447  Training Loss  0.0009080502204596996\n",
            "Epoch  6 Batch  429 / 447  Training Loss  0.000219537818338722\n",
            "Epoch  6 Batch  430 / 447  Training Loss  0.0001131757817347534\n",
            "Epoch  6 Batch  431 / 447  Training Loss  0.00023061783576849848\n",
            "Epoch  6 Batch  432 / 447  Training Loss  0.0003554409777279943\n",
            "Epoch  6 Batch  433 / 447  Training Loss  0.00012417190009728074\n",
            "Epoch  6 Batch  434 / 447  Training Loss  0.00018434913363307714\n",
            "Epoch  6 Batch  435 / 447  Training Loss  0.000633193994872272\n",
            "Epoch  6 Batch  436 / 447  Training Loss  0.0001160748943220824\n",
            "Epoch  6 Batch  437 / 447  Training Loss  0.00048371567390859127\n",
            "Epoch  6 Batch  438 / 447  Training Loss  0.0001640083792153746\n",
            "Epoch  6 Batch  439 / 447  Training Loss  9.107319783652201e-05\n",
            "Epoch  6 Batch  440 / 447  Training Loss  9.227720875060186e-05\n",
            "Epoch  6 Batch  441 / 447  Training Loss  0.00014399603242054582\n",
            "Epoch  6 Batch  442 / 447  Training Loss  0.0003460653533693403\n",
            "Epoch  6 Batch  443 / 447  Training Loss  0.00011561920109670609\n",
            "Epoch  6 Batch  444 / 447  Training Loss  0.0002480785478837788\n",
            "Epoch  6 Batch  445 / 447  Training Loss  0.00011960577830905095\n",
            "Epoch  6 Batch  446 / 447  Training Loss  0.00018955403356812894\n",
            "   7    |    -    |   0.000354   | 99.480799\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 7\n",
            "Epoch  7 Batch  0 / 447  Training Loss  0.00010347905481467023\n",
            "Epoch  7 Batch  1 / 447  Training Loss  0.00014920618559699506\n",
            "Epoch  7 Batch  2 / 447  Training Loss  0.0001826198713388294\n",
            "Epoch  7 Batch  3 / 447  Training Loss  9.070518717635423e-05\n",
            "Epoch  7 Batch  4 / 447  Training Loss  0.00010787397332023829\n",
            "Epoch  7 Batch  5 / 447  Training Loss  0.0001366973592666909\n",
            "Epoch  7 Batch  6 / 447  Training Loss  9.089838567888364e-05\n",
            "Epoch  7 Batch  7 / 447  Training Loss  0.0001131663957494311\n",
            "Epoch  7 Batch  8 / 447  Training Loss  9.535398567095399e-05\n",
            "Epoch  7 Batch  9 / 447  Training Loss  0.0001397102023474872\n",
            "Epoch  7 Batch  10 / 447  Training Loss  6.31281072855927e-05\n",
            "Epoch  7 Batch  11 / 447  Training Loss  0.00027045165188610554\n",
            "Epoch  7 Batch  12 / 447  Training Loss  0.00010685999586712569\n",
            "Epoch  7 Batch  13 / 447  Training Loss  9.404846787219867e-05\n",
            "Epoch  7 Batch  14 / 447  Training Loss  0.00017211020167451352\n",
            "Epoch  7 Batch  15 / 447  Training Loss  0.00013473379658535123\n",
            "Epoch  7 Batch  16 / 447  Training Loss  0.00015930640802253038\n",
            "Epoch  7 Batch  17 / 447  Training Loss  7.220816769404337e-05\n",
            "Epoch  7 Batch  18 / 447  Training Loss  0.0007115394691936672\n",
            "Epoch  7 Batch  19 / 447  Training Loss  0.00011848827853100374\n",
            "Epoch  7 Batch  20 / 447  Training Loss  8.278220047941431e-05\n",
            "Epoch  7 Batch  21 / 447  Training Loss  8.461026300210506e-05\n",
            "Epoch  7 Batch  22 / 447  Training Loss  0.0001229550107382238\n",
            "Epoch  7 Batch  23 / 447  Training Loss  8.292243001051247e-05\n",
            "Epoch  7 Batch  24 / 447  Training Loss  0.00013770062651019543\n",
            "Epoch  7 Batch  25 / 447  Training Loss  0.0001548587461002171\n",
            "Epoch  7 Batch  26 / 447  Training Loss  0.0001780290767783299\n",
            "Epoch  7 Batch  27 / 447  Training Loss  9.148815297521651e-05\n",
            "Epoch  7 Batch  28 / 447  Training Loss  0.00019052760035265237\n",
            "Epoch  7 Batch  29 / 447  Training Loss  5.9570196754066274e-05\n",
            "Epoch  7 Batch  30 / 447  Training Loss  0.00010587107681203634\n",
            "Epoch  7 Batch  31 / 447  Training Loss  8.825150143820792e-05\n",
            "Epoch  7 Batch  32 / 447  Training Loss  0.0003104073985014111\n",
            "Epoch  7 Batch  33 / 447  Training Loss  0.000411031098337844\n",
            "Epoch  7 Batch  34 / 447  Training Loss  6.225860852282494e-05\n",
            "Epoch  7 Batch  35 / 447  Training Loss  7.912403088994324e-05\n",
            "Epoch  7 Batch  36 / 447  Training Loss  8.469104795949534e-05\n",
            "Epoch  7 Batch  37 / 447  Training Loss  0.00011360123608028516\n",
            "Epoch  7 Batch  38 / 447  Training Loss  8.882202382665128e-05\n",
            "Epoch  7 Batch  39 / 447  Training Loss  6.489059160230681e-05\n",
            "Epoch  7 Batch  40 / 447  Training Loss  0.00011393259046599269\n",
            "Epoch  7 Batch  41 / 447  Training Loss  0.00012146309018135071\n",
            "Epoch  7 Batch  42 / 447  Training Loss  0.003678155830129981\n",
            "Epoch  7 Batch  43 / 447  Training Loss  0.0003819102421402931\n",
            "Epoch  7 Batch  44 / 447  Training Loss  0.016136670485138893\n",
            "Epoch  7 Batch  45 / 447  Training Loss  0.0030009327456355095\n",
            "Epoch  7 Batch  46 / 447  Training Loss  0.01135928649455309\n",
            "Epoch  7 Batch  47 / 447  Training Loss  0.0005768826231360435\n",
            "Epoch  7 Batch  48 / 447  Training Loss  9.928282088367268e-05\n",
            "Epoch  7 Batch  49 / 447  Training Loss  0.0003240249352529645\n",
            "Epoch  7 Batch  50 / 447  Training Loss  0.0001694726524874568\n",
            "Epoch  7 Batch  51 / 447  Training Loss  0.0002562151930760592\n",
            "Epoch  7 Batch  52 / 447  Training Loss  0.001570176682434976\n",
            "Epoch  7 Batch  53 / 447  Training Loss  0.00012049881479470059\n",
            "Epoch  7 Batch  54 / 447  Training Loss  0.0004448527470231056\n",
            "Epoch  7 Batch  55 / 447  Training Loss  0.0022604987025260925\n",
            "Epoch  7 Batch  56 / 447  Training Loss  0.0027111864183098078\n",
            "Epoch  7 Batch  57 / 447  Training Loss  0.0001236122625414282\n",
            "Epoch  7 Batch  58 / 447  Training Loss  0.00011600125435506925\n",
            "Epoch  7 Batch  59 / 447  Training Loss  0.00021269147691782564\n",
            "Epoch  7 Batch  60 / 447  Training Loss  0.000623083848040551\n",
            "Epoch  7 Batch  61 / 447  Training Loss  0.00011579991405596957\n",
            "Epoch  7 Batch  62 / 447  Training Loss  0.002730755601078272\n",
            "Epoch  7 Batch  63 / 447  Training Loss  0.0002818995853886008\n",
            "Epoch  7 Batch  64 / 447  Training Loss  0.0001844166254159063\n",
            "Epoch  7 Batch  65 / 447  Training Loss  0.00032813302823342383\n",
            "Epoch  7 Batch  66 / 447  Training Loss  0.00013137712085153908\n",
            "Epoch  7 Batch  67 / 447  Training Loss  0.00014662242028862238\n",
            "Epoch  7 Batch  68 / 447  Training Loss  7.888711843406782e-05\n",
            "Epoch  7 Batch  69 / 447  Training Loss  0.007254060357809067\n",
            "Epoch  7 Batch  70 / 447  Training Loss  0.00014432775788009167\n",
            "Epoch  7 Batch  71 / 447  Training Loss  0.00011691769759636372\n",
            "Epoch  7 Batch  72 / 447  Training Loss  0.00011784417438320816\n",
            "Epoch  7 Batch  73 / 447  Training Loss  0.00014980976993683726\n",
            "Epoch  7 Batch  74 / 447  Training Loss  6.48022978566587e-05\n",
            "Epoch  7 Batch  75 / 447  Training Loss  0.00019004038767889142\n",
            "Epoch  7 Batch  76 / 447  Training Loss  0.0001470885326853022\n",
            "Epoch  7 Batch  77 / 447  Training Loss  9.952930122381076e-05\n",
            "Epoch  7 Batch  78 / 447  Training Loss  0.00044256620458327234\n",
            "Epoch  7 Batch  79 / 447  Training Loss  0.0008860148373059928\n",
            "Epoch  7 Batch  80 / 447  Training Loss  0.00019178845104761422\n",
            "Epoch  7 Batch  81 / 447  Training Loss  0.00023917490034364164\n",
            "Epoch  7 Batch  82 / 447  Training Loss  0.00012177233293186873\n",
            "Epoch  7 Batch  83 / 447  Training Loss  0.00016656963271088898\n",
            "Epoch  7 Batch  84 / 447  Training Loss  0.003004941390827298\n",
            "Epoch  7 Batch  85 / 447  Training Loss  0.0013470598496496677\n",
            "Epoch  7 Batch  86 / 447  Training Loss  0.00029877436463721097\n",
            "Epoch  7 Batch  87 / 447  Training Loss  0.00022639695089310408\n",
            "Epoch  7 Batch  88 / 447  Training Loss  0.0002911558549385518\n",
            "Epoch  7 Batch  89 / 447  Training Loss  0.0006905226618982852\n",
            "Epoch  7 Batch  90 / 447  Training Loss  0.0004430502303875983\n",
            "Epoch  7 Batch  91 / 447  Training Loss  0.00017351422866340727\n",
            "Epoch  7 Batch  92 / 447  Training Loss  7.628299499629065e-05\n",
            "Epoch  7 Batch  93 / 447  Training Loss  0.00012400574632920325\n",
            "Epoch  7 Batch  94 / 447  Training Loss  0.00015911573427729309\n",
            "Epoch  7 Batch  95 / 447  Training Loss  8.809657447272912e-05\n",
            "Epoch  7 Batch  96 / 447  Training Loss  8.550273923901841e-05\n",
            "Epoch  7 Batch  97 / 447  Training Loss  0.0003046756610274315\n",
            "Epoch  7 Batch  98 / 447  Training Loss  7.440461195074022e-05\n",
            "Epoch  7 Batch  99 / 447  Training Loss  0.00023010886798147112\n",
            "Epoch  7 Batch  100 / 447  Training Loss  9.099418093683198e-05\n",
            "Epoch  7 Batch  101 / 447  Training Loss  0.0003934925189241767\n",
            "Epoch  7 Batch  102 / 447  Training Loss  0.0011869273148477077\n",
            "Epoch  7 Batch  103 / 447  Training Loss  0.0002986233157571405\n",
            "Epoch  7 Batch  104 / 447  Training Loss  0.00016703178698662668\n",
            "Epoch  7 Batch  105 / 447  Training Loss  0.0001713695819489658\n",
            "Epoch  7 Batch  106 / 447  Training Loss  0.00016760035941842943\n",
            "Epoch  7 Batch  107 / 447  Training Loss  0.004874950274825096\n",
            "Epoch  7 Batch  108 / 447  Training Loss  0.011036748997867107\n",
            "Epoch  7 Batch  109 / 447  Training Loss  0.003197271842509508\n",
            "Epoch  7 Batch  110 / 447  Training Loss  0.004594991449266672\n",
            "Epoch  7 Batch  111 / 447  Training Loss  0.010934083722531796\n",
            "Epoch  7 Batch  112 / 447  Training Loss  0.00040255647036246955\n",
            "Epoch  7 Batch  113 / 447  Training Loss  0.005256721284240484\n",
            "Epoch  7 Batch  114 / 447  Training Loss  0.0011849674629047513\n",
            "Epoch  7 Batch  115 / 447  Training Loss  0.0005102671566419303\n",
            "Epoch  7 Batch  116 / 447  Training Loss  0.0007119175861589611\n",
            "Epoch  7 Batch  117 / 447  Training Loss  0.00028946573729626834\n",
            "Epoch  7 Batch  118 / 447  Training Loss  0.0002565402246546\n",
            "Epoch  7 Batch  119 / 447  Training Loss  0.000392249901778996\n",
            "Epoch  7 Batch  120 / 447  Training Loss  0.0005272614071145654\n",
            "Epoch  7 Batch  121 / 447  Training Loss  0.00047869878471828997\n",
            "Epoch  7 Batch  122 / 447  Training Loss  0.0001536087947897613\n",
            "Epoch  7 Batch  123 / 447  Training Loss  0.0007078333874233067\n",
            "Epoch  7 Batch  124 / 447  Training Loss  0.0002918218669947237\n",
            "Epoch  7 Batch  125 / 447  Training Loss  0.0008232917170971632\n",
            "Epoch  7 Batch  126 / 447  Training Loss  0.0004443458456080407\n",
            "Epoch  7 Batch  127 / 447  Training Loss  0.00018622844072524458\n",
            "Epoch  7 Batch  128 / 447  Training Loss  0.0007901089848019183\n",
            "Epoch  7 Batch  129 / 447  Training Loss  0.00013343297177925706\n",
            "Epoch  7 Batch  130 / 447  Training Loss  0.0001632003695704043\n",
            "Epoch  7 Batch  131 / 447  Training Loss  0.00017276909784413874\n",
            "Epoch  7 Batch  132 / 447  Training Loss  0.00010987606219714507\n",
            "Epoch  7 Batch  133 / 447  Training Loss  0.0001233645307365805\n",
            "Epoch  7 Batch  134 / 447  Training Loss  0.00013146069250069559\n",
            "Epoch  7 Batch  135 / 447  Training Loss  0.00029391140560619533\n",
            "Epoch  7 Batch  136 / 447  Training Loss  0.0006281147361733019\n",
            "Epoch  7 Batch  137 / 447  Training Loss  0.0005968905752524734\n",
            "Epoch  7 Batch  138 / 447  Training Loss  0.0003056492714677006\n",
            "Epoch  7 Batch  139 / 447  Training Loss  0.0022362787276506424\n",
            "Epoch  7 Batch  140 / 447  Training Loss  0.00031922978814691305\n",
            "Epoch  7 Batch  141 / 447  Training Loss  0.0008432833128608763\n",
            "Epoch  7 Batch  142 / 447  Training Loss  0.0010550691513344646\n",
            "Epoch  7 Batch  143 / 447  Training Loss  0.002935956232249737\n",
            "Epoch  7 Batch  144 / 447  Training Loss  0.0021225260570645332\n",
            "Epoch  7 Batch  145 / 447  Training Loss  0.0003555194998625666\n",
            "Epoch  7 Batch  146 / 447  Training Loss  0.0004230712365824729\n",
            "Epoch  7 Batch  147 / 447  Training Loss  0.00020913001208100468\n",
            "Epoch  7 Batch  148 / 447  Training Loss  0.0003014924586750567\n",
            "Epoch  7 Batch  149 / 447  Training Loss  0.00016527704428881407\n",
            "Epoch  7 Batch  150 / 447  Training Loss  0.00016805129416752607\n",
            "Epoch  7 Batch  151 / 447  Training Loss  0.0001461550418753177\n",
            "Epoch  7 Batch  152 / 447  Training Loss  0.0003105289943050593\n",
            "Epoch  7 Batch  153 / 447  Training Loss  0.00013130609295330942\n",
            "Epoch  7 Batch  154 / 447  Training Loss  0.00011660554446280003\n",
            "Epoch  7 Batch  155 / 447  Training Loss  0.0001607874291948974\n",
            "Epoch  7 Batch  156 / 447  Training Loss  0.000353140669176355\n",
            "Epoch  7 Batch  157 / 447  Training Loss  0.00011012053437298164\n",
            "Epoch  7 Batch  158 / 447  Training Loss  0.00028799459687434137\n",
            "Epoch  7 Batch  159 / 447  Training Loss  0.00022882111079525203\n",
            "Epoch  7 Batch  160 / 447  Training Loss  0.00045100777060724795\n",
            "Epoch  7 Batch  161 / 447  Training Loss  0.0015798824606463313\n",
            "Epoch  7 Batch  162 / 447  Training Loss  0.00032986982841975987\n",
            "Epoch  7 Batch  163 / 447  Training Loss  0.0004662464198190719\n",
            "Epoch  7 Batch  164 / 447  Training Loss  9.121988114202395e-05\n",
            "Epoch  7 Batch  165 / 447  Training Loss  0.00025200474192388356\n",
            "Epoch  7 Batch  166 / 447  Training Loss  0.00012961897300556302\n",
            "Epoch  7 Batch  167 / 447  Training Loss  0.00018177861056756228\n",
            "Epoch  7 Batch  168 / 447  Training Loss  0.00024108534853439778\n",
            "Epoch  7 Batch  169 / 447  Training Loss  0.00012847635662183166\n",
            "Epoch  7 Batch  170 / 447  Training Loss  0.00023605572641827166\n",
            "Epoch  7 Batch  171 / 447  Training Loss  0.0001072077066055499\n",
            "Epoch  7 Batch  172 / 447  Training Loss  0.00010883931099670008\n",
            "Epoch  7 Batch  173 / 447  Training Loss  0.0001112530444515869\n",
            "Epoch  7 Batch  174 / 447  Training Loss  0.0009524652268737555\n",
            "Epoch  7 Batch  175 / 447  Training Loss  0.00013892259448766708\n",
            "Epoch  7 Batch  176 / 447  Training Loss  0.0001964012044481933\n",
            "Epoch  7 Batch  177 / 447  Training Loss  0.0002465741417836398\n",
            "Epoch  7 Batch  178 / 447  Training Loss  0.0002952894428744912\n",
            "Epoch  7 Batch  179 / 447  Training Loss  0.0016351124504581094\n",
            "Epoch  7 Batch  180 / 447  Training Loss  0.00012674258323386312\n",
            "Epoch  7 Batch  181 / 447  Training Loss  9.54552524490282e-05\n",
            "Epoch  7 Batch  182 / 447  Training Loss  9.485163900535554e-05\n",
            "Epoch  7 Batch  183 / 447  Training Loss  9.256139310309663e-05\n",
            "Epoch  7 Batch  184 / 447  Training Loss  0.0001821412006393075\n",
            "Epoch  7 Batch  185 / 447  Training Loss  0.00013658360694535077\n",
            "Epoch  7 Batch  186 / 447  Training Loss  0.00014285038923844695\n",
            "Epoch  7 Batch  187 / 447  Training Loss  9.314119233749807e-05\n",
            "Epoch  7 Batch  188 / 447  Training Loss  0.00010410579852759838\n",
            "Epoch  7 Batch  189 / 447  Training Loss  0.00014766569074708968\n",
            "Epoch  7 Batch  190 / 447  Training Loss  8.494456415064633e-05\n",
            "Epoch  7 Batch  191 / 447  Training Loss  6.121243495726958e-05\n",
            "Epoch  7 Batch  192 / 447  Training Loss  0.0012889974750578403\n",
            "Epoch  7 Batch  193 / 447  Training Loss  0.0005431818426586688\n",
            "Epoch  7 Batch  194 / 447  Training Loss  0.0005225411150604486\n",
            "Epoch  7 Batch  195 / 447  Training Loss  0.00020393749582581222\n",
            "Epoch  7 Batch  196 / 447  Training Loss  0.0002497521636541933\n",
            "Epoch  7 Batch  197 / 447  Training Loss  7.026317325653508e-05\n",
            "Epoch  7 Batch  198 / 447  Training Loss  0.0005905303987674415\n",
            "Epoch  7 Batch  199 / 447  Training Loss  0.00010805764031829312\n",
            "Epoch  7 Batch  200 / 447  Training Loss  0.0001317425922024995\n",
            "Epoch  7 Batch  201 / 447  Training Loss  0.00011331532732583582\n",
            "Epoch  7 Batch  202 / 447  Training Loss  6.835487874923274e-05\n",
            "Epoch  7 Batch  203 / 447  Training Loss  9.540531027596444e-05\n",
            "Epoch  7 Batch  204 / 447  Training Loss  9.14524935069494e-05\n",
            "Epoch  7 Batch  205 / 447  Training Loss  8.186049672076479e-05\n",
            "Epoch  7 Batch  206 / 447  Training Loss  9.01437015272677e-05\n",
            "Epoch  7 Batch  207 / 447  Training Loss  9.9338183645159e-05\n",
            "Epoch  7 Batch  208 / 447  Training Loss  0.0001130154196289368\n",
            "Epoch  7 Batch  209 / 447  Training Loss  0.00017633003881201148\n",
            "Epoch  7 Batch  210 / 447  Training Loss  0.000302485772408545\n",
            "Epoch  7 Batch  211 / 447  Training Loss  0.00015750575403217226\n",
            "Epoch  7 Batch  212 / 447  Training Loss  0.00014216499403119087\n",
            "Epoch  7 Batch  213 / 447  Training Loss  0.00012130694813095033\n",
            "Epoch  7 Batch  214 / 447  Training Loss  0.0001966508716577664\n",
            "Epoch  7 Batch  215 / 447  Training Loss  0.00015510903904214501\n",
            "Epoch  7 Batch  216 / 447  Training Loss  7.574158371426165e-05\n",
            "Epoch  7 Batch  217 / 447  Training Loss  8.930199692258611e-05\n",
            "Epoch  7 Batch  218 / 447  Training Loss  0.00019112623704131693\n",
            "Epoch  7 Batch  219 / 447  Training Loss  8.206596248783171e-05\n",
            "Epoch  7 Batch  220 / 447  Training Loss  7.285226456588134e-05\n",
            "Epoch  7 Batch  221 / 447  Training Loss  0.00016830122331157327\n",
            "Epoch  7 Batch  222 / 447  Training Loss  6.073494296288118e-05\n",
            "Epoch  7 Batch  223 / 447  Training Loss  0.0014356663450598717\n",
            "Epoch  7 Batch  224 / 447  Training Loss  0.00016835116548463702\n",
            "Epoch  7 Batch  225 / 447  Training Loss  0.0001291893277084455\n",
            "Epoch  7 Batch  226 / 447  Training Loss  9.874414536170661e-05\n",
            "Epoch  7 Batch  227 / 447  Training Loss  6.853638478787616e-05\n",
            "Epoch  7 Batch  228 / 447  Training Loss  7.499885396100581e-05\n",
            "Epoch  7 Batch  229 / 447  Training Loss  0.0003175748570356518\n",
            "Epoch  7 Batch  230 / 447  Training Loss  6.944491906324401e-05\n",
            "Epoch  7 Batch  231 / 447  Training Loss  0.00035203484003432095\n",
            "Epoch  7 Batch  232 / 447  Training Loss  9.702318493509665e-05\n",
            "Epoch  7 Batch  233 / 447  Training Loss  0.00014603781164623797\n",
            "Epoch  7 Batch  234 / 447  Training Loss  0.0001265608734684065\n",
            "Epoch  7 Batch  235 / 447  Training Loss  0.0001087645287043415\n",
            "Epoch  7 Batch  236 / 447  Training Loss  0.0001551439199829474\n",
            "Epoch  7 Batch  237 / 447  Training Loss  7.468646799679846e-05\n",
            "Epoch  7 Batch  238 / 447  Training Loss  0.00010325486800866202\n",
            "Epoch  7 Batch  239 / 447  Training Loss  0.00010292149090673774\n",
            "Epoch  7 Batch  240 / 447  Training Loss  0.00010278451372869313\n",
            "Epoch  7 Batch  241 / 447  Training Loss  6.865083560114726e-05\n",
            "Epoch  7 Batch  242 / 447  Training Loss  0.0001121486711781472\n",
            "Epoch  7 Batch  243 / 447  Training Loss  8.625499322079122e-05\n",
            "Epoch  7 Batch  244 / 447  Training Loss  8.798204362392426e-05\n",
            "Epoch  7 Batch  245 / 447  Training Loss  0.00018791866023093462\n",
            "Epoch  7 Batch  246 / 447  Training Loss  5.3362175094662234e-05\n",
            "Epoch  7 Batch  247 / 447  Training Loss  8.385945693589747e-05\n",
            "Epoch  7 Batch  248 / 447  Training Loss  8.336089376825839e-05\n",
            "Epoch  7 Batch  249 / 447  Training Loss  8.693021663930267e-05\n",
            "Epoch  7 Batch  250 / 447  Training Loss  7.16645154170692e-05\n",
            "Epoch  7 Batch  251 / 447  Training Loss  0.0002151060471078381\n",
            "Epoch  7 Batch  252 / 447  Training Loss  8.832439925754443e-05\n",
            "Epoch  7 Batch  253 / 447  Training Loss  5.151626100996509e-05\n",
            "Epoch  7 Batch  254 / 447  Training Loss  8.916601655073464e-05\n",
            "Epoch  7 Batch  255 / 447  Training Loss  5.8310768508818e-05\n",
            "Epoch  7 Batch  256 / 447  Training Loss  9.583486098563299e-05\n",
            "Epoch  7 Batch  257 / 447  Training Loss  0.00016235145449172705\n",
            "Epoch  7 Batch  258 / 447  Training Loss  0.00010989252768922597\n",
            "Epoch  7 Batch  259 / 447  Training Loss  7.922705844976008e-05\n",
            "Epoch  7 Batch  260 / 447  Training Loss  7.909275882411748e-05\n",
            "Epoch  7 Batch  261 / 447  Training Loss  6.85944251017645e-05\n",
            "Epoch  7 Batch  262 / 447  Training Loss  7.626523438375443e-05\n",
            "Epoch  7 Batch  263 / 447  Training Loss  9.892465459415689e-05\n",
            "Epoch  7 Batch  264 / 447  Training Loss  5.6607950682519004e-05\n",
            "Epoch  7 Batch  265 / 447  Training Loss  9.738942026160657e-05\n",
            "Epoch  7 Batch  266 / 447  Training Loss  6.708240107400343e-05\n",
            "Epoch  7 Batch  267 / 447  Training Loss  0.00010284095333190635\n",
            "Epoch  7 Batch  268 / 447  Training Loss  8.69735304149799e-05\n",
            "Epoch  7 Batch  269 / 447  Training Loss  7.865079533075914e-05\n",
            "Epoch  7 Batch  270 / 447  Training Loss  5.7456971262581646e-05\n",
            "Epoch  7 Batch  271 / 447  Training Loss  7.418595487251878e-05\n",
            "Epoch  7 Batch  272 / 447  Training Loss  0.00010751022637123242\n",
            "Epoch  7 Batch  273 / 447  Training Loss  7.376771100098267e-05\n",
            "Epoch  7 Batch  274 / 447  Training Loss  6.81483288644813e-05\n",
            "Epoch  7 Batch  275 / 447  Training Loss  7.079996430547908e-05\n",
            "Epoch  7 Batch  276 / 447  Training Loss  6.592681893380359e-05\n",
            "Epoch  7 Batch  277 / 447  Training Loss  8.517483365722e-05\n",
            "Epoch  7 Batch  278 / 447  Training Loss  6.070330709917471e-05\n",
            "Epoch  7 Batch  279 / 447  Training Loss  7.972314051585272e-05\n",
            "Epoch  7 Batch  280 / 447  Training Loss  5.2794352086493745e-05\n",
            "Epoch  7 Batch  281 / 447  Training Loss  7.96359236119315e-05\n",
            "Epoch  7 Batch  282 / 447  Training Loss  6.911901436978951e-05\n",
            "Epoch  7 Batch  283 / 447  Training Loss  8.036228973651305e-05\n",
            "Epoch  7 Batch  284 / 447  Training Loss  0.0001256910036318004\n",
            "Epoch  7 Batch  285 / 447  Training Loss  8.608551434008405e-05\n",
            "Epoch  7 Batch  286 / 447  Training Loss  7.86937671364285e-05\n",
            "Epoch  7 Batch  287 / 447  Training Loss  5.104552838020027e-05\n",
            "Epoch  7 Batch  288 / 447  Training Loss  0.00014815659960731864\n",
            "Epoch  7 Batch  289 / 447  Training Loss  5.404100375017151e-05\n",
            "Epoch  7 Batch  290 / 447  Training Loss  5.5648208217462525e-05\n",
            "Epoch  7 Batch  291 / 447  Training Loss  7.974207255756482e-05\n",
            "Epoch  7 Batch  292 / 447  Training Loss  5.9250753110973164e-05\n",
            "Epoch  7 Batch  293 / 447  Training Loss  5.196098209125921e-05\n",
            "Epoch  7 Batch  294 / 447  Training Loss  6.177050090627745e-05\n",
            "Epoch  7 Batch  295 / 447  Training Loss  7.32103071641177e-05\n",
            "Epoch  7 Batch  296 / 447  Training Loss  4.4540556700667366e-05\n",
            "Epoch  7 Batch  297 / 447  Training Loss  8.432284084847197e-05\n",
            "Epoch  7 Batch  298 / 447  Training Loss  6.13881929893978e-05\n",
            "Epoch  7 Batch  299 / 447  Training Loss  6.60909281577915e-05\n",
            "Epoch  7 Batch  300 / 447  Training Loss  7.324746547965333e-05\n",
            "Epoch  7 Batch  301 / 447  Training Loss  8.020750101422891e-05\n",
            "Epoch  7 Batch  302 / 447  Training Loss  7.963565440149978e-05\n",
            "Epoch  7 Batch  303 / 447  Training Loss  5.83367764193099e-05\n",
            "Epoch  7 Batch  304 / 447  Training Loss  5.1259208703413606e-05\n",
            "Epoch  7 Batch  305 / 447  Training Loss  5.946019518887624e-05\n",
            "Epoch  7 Batch  306 / 447  Training Loss  7.761100278003141e-05\n",
            "Epoch  7 Batch  307 / 447  Training Loss  7.992218888830394e-05\n",
            "Epoch  7 Batch  308 / 447  Training Loss  4.751361848320812e-05\n",
            "Epoch  7 Batch  309 / 447  Training Loss  0.00023213500389829278\n",
            "Epoch  7 Batch  310 / 447  Training Loss  0.00010436065349495038\n",
            "Epoch  7 Batch  311 / 447  Training Loss  6.36135446256958e-05\n",
            "Epoch  7 Batch  312 / 447  Training Loss  3.9580972952535376e-05\n",
            "Epoch  7 Batch  313 / 447  Training Loss  8.464526035822928e-05\n",
            "Epoch  7 Batch  314 / 447  Training Loss  8.097547834040597e-05\n",
            "Epoch  7 Batch  315 / 447  Training Loss  6.533692794619128e-05\n",
            "Epoch  7 Batch  316 / 447  Training Loss  4.440783231984824e-05\n",
            "Epoch  7 Batch  317 / 447  Training Loss  0.00010175503848586231\n",
            "Epoch  7 Batch  318 / 447  Training Loss  5.8340945543022826e-05\n",
            "Epoch  7 Batch  319 / 447  Training Loss  7.990658923517913e-05\n",
            "Epoch  7 Batch  320 / 447  Training Loss  0.00014096905943006277\n",
            "Epoch  7 Batch  321 / 447  Training Loss  4.9393802328268066e-05\n",
            "Epoch  7 Batch  322 / 447  Training Loss  6.800348637625575e-05\n",
            "Epoch  7 Batch  323 / 447  Training Loss  5.732279169023968e-05\n",
            "Epoch  7 Batch  324 / 447  Training Loss  9.409091580891982e-05\n",
            "Epoch  7 Batch  325 / 447  Training Loss  6.484199548140168e-05\n",
            "Epoch  7 Batch  326 / 447  Training Loss  6.991999543970451e-05\n",
            "Epoch  7 Batch  327 / 447  Training Loss  0.00011717263259924948\n",
            "Epoch  7 Batch  328 / 447  Training Loss  5.9483209042809904e-05\n",
            "Epoch  7 Batch  329 / 447  Training Loss  5.351326035452075e-05\n",
            "Epoch  7 Batch  330 / 447  Training Loss  0.00010472074791323394\n",
            "Epoch  7 Batch  331 / 447  Training Loss  5.979687921353616e-05\n",
            "Epoch  7 Batch  332 / 447  Training Loss  5.973948282189667e-05\n",
            "Epoch  7 Batch  333 / 447  Training Loss  8.671806426718831e-05\n",
            "Epoch  7 Batch  334 / 447  Training Loss  5.181822780286893e-05\n",
            "Epoch  7 Batch  335 / 447  Training Loss  0.0001209732799907215\n",
            "Epoch  7 Batch  336 / 447  Training Loss  5.190949377720244e-05\n",
            "Epoch  7 Batch  337 / 447  Training Loss  5.1602706662379205e-05\n",
            "Epoch  7 Batch  338 / 447  Training Loss  5.759932901128195e-05\n",
            "Epoch  7 Batch  339 / 447  Training Loss  6.306130671873689e-05\n",
            "Epoch  7 Batch  340 / 447  Training Loss  6.392903014784679e-05\n",
            "Epoch  7 Batch  341 / 447  Training Loss  4.88141413370613e-05\n",
            "Epoch  7 Batch  342 / 447  Training Loss  0.0005822067032568157\n",
            "Epoch  7 Batch  343 / 447  Training Loss  7.52022533561103e-05\n",
            "Epoch  7 Batch  344 / 447  Training Loss  9.144077921519056e-05\n",
            "Epoch  7 Batch  345 / 447  Training Loss  0.0004881892236880958\n",
            "Epoch  7 Batch  346 / 447  Training Loss  0.000159935443662107\n",
            "Epoch  7 Batch  347 / 447  Training Loss  7.359652954619378e-05\n",
            "Epoch  7 Batch  348 / 447  Training Loss  6.420353020075709e-05\n",
            "Epoch  7 Batch  349 / 447  Training Loss  7.7520810009446e-05\n",
            "Epoch  7 Batch  350 / 447  Training Loss  7.456915045622736e-05\n",
            "Epoch  7 Batch  351 / 447  Training Loss  0.0001035445457091555\n",
            "Epoch  7 Batch  352 / 447  Training Loss  9.368339669890702e-05\n",
            "Epoch  7 Batch  353 / 447  Training Loss  5.181010055821389e-05\n",
            "Epoch  7 Batch  354 / 447  Training Loss  0.0009810547344386578\n",
            "Epoch  7 Batch  355 / 447  Training Loss  0.0001678103144513443\n",
            "Epoch  7 Batch  356 / 447  Training Loss  7.311732770176604e-05\n",
            "Epoch  7 Batch  357 / 447  Training Loss  6.305475835688412e-05\n",
            "Epoch  7 Batch  358 / 447  Training Loss  5.80322666792199e-05\n",
            "Epoch  7 Batch  359 / 447  Training Loss  7.558915240224451e-05\n",
            "Epoch  7 Batch  360 / 447  Training Loss  0.00013568333815783262\n",
            "Epoch  7 Batch  361 / 447  Training Loss  9.75775474216789e-05\n",
            "Epoch  7 Batch  362 / 447  Training Loss  7.888914842624217e-05\n",
            "Epoch  7 Batch  363 / 447  Training Loss  0.00012509975931607187\n",
            "Epoch  7 Batch  364 / 447  Training Loss  0.00013583479449152946\n",
            "Epoch  7 Batch  365 / 447  Training Loss  8.445180719718337e-05\n",
            "Epoch  7 Batch  366 / 447  Training Loss  6.532345287268981e-05\n",
            "Epoch  7 Batch  367 / 447  Training Loss  5.208947550272569e-05\n",
            "Epoch  7 Batch  368 / 447  Training Loss  7.880629709688947e-05\n",
            "Epoch  7 Batch  369 / 447  Training Loss  7.241762068588287e-05\n",
            "Epoch  7 Batch  370 / 447  Training Loss  6.460802978836e-05\n",
            "Epoch  7 Batch  371 / 447  Training Loss  5.845376290380955e-05\n",
            "Epoch  7 Batch  372 / 447  Training Loss  0.001541825826279819\n",
            "Epoch  7 Batch  373 / 447  Training Loss  8.274046558653936e-05\n",
            "Epoch  7 Batch  374 / 447  Training Loss  0.00026181209250353277\n",
            "Epoch  7 Batch  375 / 447  Training Loss  8.296530722873285e-05\n",
            "Epoch  7 Batch  376 / 447  Training Loss  8.876892388798296e-05\n",
            "Epoch  7 Batch  377 / 447  Training Loss  6.977951852604747e-05\n",
            "Epoch  7 Batch  378 / 447  Training Loss  0.0001071355291060172\n",
            "Epoch  7 Batch  379 / 447  Training Loss  9.069997759070247e-05\n",
            "Epoch  7 Batch  380 / 447  Training Loss  8.678917947690934e-05\n",
            "Epoch  7 Batch  381 / 447  Training Loss  8.064145367825404e-05\n",
            "Epoch  7 Batch  382 / 447  Training Loss  6.789041799493134e-05\n",
            "Epoch  7 Batch  383 / 447  Training Loss  5.741944187320769e-05\n",
            "Epoch  7 Batch  384 / 447  Training Loss  6.20562641415745e-05\n",
            "Epoch  7 Batch  385 / 447  Training Loss  5.843553662998602e-05\n",
            "Epoch  7 Batch  386 / 447  Training Loss  0.0006599146872758865\n",
            "Epoch  7 Batch  387 / 447  Training Loss  8.253720443462953e-05\n",
            "Epoch  7 Batch  388 / 447  Training Loss  0.000725741614587605\n",
            "Epoch  7 Batch  389 / 447  Training Loss  9.842224244493991e-05\n",
            "Epoch  7 Batch  390 / 447  Training Loss  7.86411837907508e-05\n",
            "Epoch  7 Batch  391 / 447  Training Loss  0.00012159426114521921\n",
            "Epoch  7 Batch  392 / 447  Training Loss  5.820569640491158e-05\n",
            "Epoch  7 Batch  393 / 447  Training Loss  0.0014691422693431377\n",
            "Epoch  7 Batch  394 / 447  Training Loss  0.0005680369795300066\n",
            "Epoch  7 Batch  395 / 447  Training Loss  8.952038479037583e-05\n",
            "Epoch  7 Batch  396 / 447  Training Loss  0.00010915954771917313\n",
            "Epoch  7 Batch  397 / 447  Training Loss  0.008759727701544762\n",
            "Epoch  7 Batch  398 / 447  Training Loss  0.000996854156255722\n",
            "Epoch  7 Batch  399 / 447  Training Loss  9.046573541127145e-05\n",
            "Epoch  7 Batch  400 / 447  Training Loss  0.00014734455908183008\n",
            "Epoch  7 Batch  401 / 447  Training Loss  0.00011642187018878758\n",
            "Epoch  7 Batch  402 / 447  Training Loss  7.492847362300381e-05\n",
            "Epoch  7 Batch  403 / 447  Training Loss  9.158506145467982e-05\n",
            "Epoch  7 Batch  404 / 447  Training Loss  7.643287972314283e-05\n",
            "Epoch  7 Batch  405 / 447  Training Loss  0.00013368828513193876\n",
            "Epoch  7 Batch  406 / 447  Training Loss  6.230364670045674e-05\n",
            "Epoch  7 Batch  407 / 447  Training Loss  0.0004522224480751902\n",
            "Epoch  7 Batch  408 / 447  Training Loss  3.5442419175524265e-05\n",
            "Epoch  7 Batch  409 / 447  Training Loss  0.00015344865096267313\n",
            "Epoch  7 Batch  410 / 447  Training Loss  6.457199197029695e-05\n",
            "Epoch  7 Batch  411 / 447  Training Loss  0.000470107450382784\n",
            "Epoch  7 Batch  412 / 447  Training Loss  7.913086301414296e-05\n",
            "Epoch  7 Batch  413 / 447  Training Loss  9.053528629010543e-05\n",
            "Epoch  7 Batch  414 / 447  Training Loss  0.00011244912457186729\n",
            "Epoch  7 Batch  415 / 447  Training Loss  7.009439286775887e-05\n",
            "Epoch  7 Batch  416 / 447  Training Loss  0.00010772162931971252\n",
            "Epoch  7 Batch  417 / 447  Training Loss  5.4549527703784406e-05\n",
            "Epoch  7 Batch  418 / 447  Training Loss  0.00013209524331614375\n",
            "Epoch  7 Batch  419 / 447  Training Loss  7.701658614678308e-05\n",
            "Epoch  7 Batch  420 / 447  Training Loss  7.73731226217933e-05\n",
            "Epoch  7 Batch  421 / 447  Training Loss  0.00012449159112293273\n",
            "Epoch  7 Batch  422 / 447  Training Loss  6.13237643847242e-05\n",
            "Epoch  7 Batch  423 / 447  Training Loss  6.022821253282018e-05\n",
            "Epoch  7 Batch  424 / 447  Training Loss  7.106961129466072e-05\n",
            "Epoch  7 Batch  425 / 447  Training Loss  0.00011917702795471996\n",
            "Epoch  7 Batch  426 / 447  Training Loss  7.357323920587078e-05\n",
            "Epoch  7 Batch  427 / 447  Training Loss  6.730191671522334e-05\n",
            "Epoch  7 Batch  428 / 447  Training Loss  4.7935889597283676e-05\n",
            "Epoch  7 Batch  429 / 447  Training Loss  0.0001231423084391281\n",
            "Epoch  7 Batch  430 / 447  Training Loss  9.987855446524918e-05\n",
            "Epoch  7 Batch  431 / 447  Training Loss  0.00018513420945964754\n",
            "Epoch  7 Batch  432 / 447  Training Loss  0.00014815633767284453\n",
            "Epoch  7 Batch  433 / 447  Training Loss  7.627116428920999e-05\n",
            "Epoch  7 Batch  434 / 447  Training Loss  6.740178650943562e-05\n",
            "Epoch  7 Batch  435 / 447  Training Loss  8.508475730195642e-05\n",
            "Epoch  7 Batch  436 / 447  Training Loss  9.176426829071715e-05\n",
            "Epoch  7 Batch  437 / 447  Training Loss  5.612869790638797e-05\n",
            "Epoch  7 Batch  438 / 447  Training Loss  0.0010743208695203066\n",
            "Epoch  7 Batch  439 / 447  Training Loss  0.0001059566784533672\n",
            "Epoch  7 Batch  440 / 447  Training Loss  9.20576712815091e-05\n",
            "Epoch  7 Batch  441 / 447  Training Loss  4.174879359197803e-05\n",
            "Epoch  7 Batch  442 / 447  Training Loss  6.349015166051686e-05\n",
            "Epoch  7 Batch  443 / 447  Training Loss  6.54736504657194e-05\n",
            "Epoch  7 Batch  444 / 447  Training Loss  8.795095345703885e-05\n",
            "Epoch  7 Batch  445 / 447  Training Loss  7.17592120054178e-05\n",
            "Epoch  7 Batch  446 / 447  Training Loss  0.00012879673158749938\n",
            "   8    |    -    |   0.000431   | 99.872649\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 8\n",
            "Epoch  8 Batch  0 / 447  Training Loss  6.533684063469991e-05\n",
            "Epoch  8 Batch  1 / 447  Training Loss  8.433134644292295e-05\n",
            "Epoch  8 Batch  2 / 447  Training Loss  9.639963536756113e-05\n",
            "Epoch  8 Batch  3 / 447  Training Loss  6.196223694132641e-05\n",
            "Epoch  8 Batch  4 / 447  Training Loss  7.75340959080495e-05\n",
            "Epoch  8 Batch  5 / 447  Training Loss  5.200091618462466e-05\n",
            "Epoch  8 Batch  6 / 447  Training Loss  5.9493751905392855e-05\n",
            "Epoch  8 Batch  7 / 447  Training Loss  6.62459569866769e-05\n",
            "Epoch  8 Batch  8 / 447  Training Loss  6.594273872906342e-05\n",
            "Epoch  8 Batch  9 / 447  Training Loss  4.658326361095533e-05\n",
            "Epoch  8 Batch  10 / 447  Training Loss  4.936677578371018e-05\n",
            "Epoch  8 Batch  11 / 447  Training Loss  5.530348062166013e-05\n",
            "Epoch  8 Batch  12 / 447  Training Loss  5.377823981689289e-05\n",
            "Epoch  8 Batch  13 / 447  Training Loss  5.063238859293051e-05\n",
            "Epoch  8 Batch  14 / 447  Training Loss  6.804650911362842e-05\n",
            "Epoch  8 Batch  15 / 447  Training Loss  4.8735080781625584e-05\n",
            "Epoch  8 Batch  16 / 447  Training Loss  8.275846630567685e-05\n",
            "Epoch  8 Batch  17 / 447  Training Loss  7.109379657777026e-05\n",
            "Epoch  8 Batch  18 / 447  Training Loss  4.41661431977991e-05\n",
            "Epoch  8 Batch  19 / 447  Training Loss  5.5985186918405816e-05\n",
            "Epoch  8 Batch  20 / 447  Training Loss  5.3040017519379035e-05\n",
            "Epoch  8 Batch  21 / 447  Training Loss  4.695332609117031e-05\n",
            "Epoch  8 Batch  22 / 447  Training Loss  6.136370939202607e-05\n",
            "Epoch  8 Batch  23 / 447  Training Loss  0.0001082350208889693\n",
            "Epoch  8 Batch  24 / 447  Training Loss  5.125157622387633e-05\n",
            "Epoch  8 Batch  25 / 447  Training Loss  5.3664665756514296e-05\n",
            "Epoch  8 Batch  26 / 447  Training Loss  0.00015443771553691477\n",
            "Epoch  8 Batch  27 / 447  Training Loss  8.079945837380365e-05\n",
            "Epoch  8 Batch  28 / 447  Training Loss  3.1731469789519906e-05\n",
            "Epoch  8 Batch  29 / 447  Training Loss  7.16794456820935e-05\n",
            "Epoch  8 Batch  30 / 447  Training Loss  4.261865251464769e-05\n",
            "Epoch  8 Batch  31 / 447  Training Loss  6.50720321573317e-05\n",
            "Epoch  8 Batch  32 / 447  Training Loss  6.747762381564826e-05\n",
            "Epoch  8 Batch  33 / 447  Training Loss  6.473399844253436e-05\n",
            "Epoch  8 Batch  34 / 447  Training Loss  8.484658610541373e-05\n",
            "Epoch  8 Batch  35 / 447  Training Loss  3.8484871765831485e-05\n",
            "Epoch  8 Batch  36 / 447  Training Loss  3.909690349246375e-05\n",
            "Epoch  8 Batch  37 / 447  Training Loss  4.883825749857351e-05\n",
            "Epoch  8 Batch  38 / 447  Training Loss  6.900709558976814e-05\n",
            "Epoch  8 Batch  39 / 447  Training Loss  4.2719784687506035e-05\n",
            "Epoch  8 Batch  40 / 447  Training Loss  7.586020365124568e-05\n",
            "Epoch  8 Batch  41 / 447  Training Loss  4.33771638199687e-05\n",
            "Epoch  8 Batch  42 / 447  Training Loss  4.670160706155002e-05\n",
            "Epoch  8 Batch  43 / 447  Training Loss  9.293961920775473e-05\n",
            "Epoch  8 Batch  44 / 447  Training Loss  5.591272929450497e-05\n",
            "Epoch  8 Batch  45 / 447  Training Loss  7.181308319559321e-05\n",
            "Epoch  8 Batch  46 / 447  Training Loss  4.334779441705905e-05\n",
            "Epoch  8 Batch  47 / 447  Training Loss  6.0972703067818657e-05\n",
            "Epoch  8 Batch  48 / 447  Training Loss  4.8149493522942066e-05\n",
            "Epoch  8 Batch  49 / 447  Training Loss  6.605882663279772e-05\n",
            "Epoch  8 Batch  50 / 447  Training Loss  6.469309300882742e-05\n",
            "Epoch  8 Batch  51 / 447  Training Loss  6.104816566221416e-05\n",
            "Epoch  8 Batch  52 / 447  Training Loss  6.7513705289457e-05\n",
            "Epoch  8 Batch  53 / 447  Training Loss  4.860391345573589e-05\n",
            "Epoch  8 Batch  54 / 447  Training Loss  4.9910402594832703e-05\n",
            "Epoch  8 Batch  55 / 447  Training Loss  5.9491874708328396e-05\n",
            "Epoch  8 Batch  56 / 447  Training Loss  5.8605812228051946e-05\n",
            "Epoch  8 Batch  57 / 447  Training Loss  7.086375990184024e-05\n",
            "Epoch  8 Batch  58 / 447  Training Loss  5.076358502265066e-05\n",
            "Epoch  8 Batch  59 / 447  Training Loss  6.0009449953213334e-05\n",
            "Epoch  8 Batch  60 / 447  Training Loss  4.390187677927315e-05\n",
            "Epoch  8 Batch  61 / 447  Training Loss  7.83637078711763e-05\n",
            "Epoch  8 Batch  62 / 447  Training Loss  5.820208752993494e-05\n",
            "Epoch  8 Batch  63 / 447  Training Loss  7.262922736117616e-05\n",
            "Epoch  8 Batch  64 / 447  Training Loss  4.2090494389412925e-05\n",
            "Epoch  8 Batch  65 / 447  Training Loss  6.941612082300708e-05\n",
            "Epoch  8 Batch  66 / 447  Training Loss  4.762355092680082e-05\n",
            "Epoch  8 Batch  67 / 447  Training Loss  5.80726882617455e-05\n",
            "Epoch  8 Batch  68 / 447  Training Loss  5.2264909754740074e-05\n",
            "Epoch  8 Batch  69 / 447  Training Loss  3.22779560519848e-05\n",
            "Epoch  8 Batch  70 / 447  Training Loss  5.16267646162305e-05\n",
            "Epoch  8 Batch  71 / 447  Training Loss  6.218453199835494e-05\n",
            "Epoch  8 Batch  72 / 447  Training Loss  4.410572728374973e-05\n",
            "Epoch  8 Batch  73 / 447  Training Loss  4.689737397711724e-05\n",
            "Epoch  8 Batch  74 / 447  Training Loss  4.847442323807627e-05\n",
            "Epoch  8 Batch  75 / 447  Training Loss  6.792390922782943e-05\n",
            "Epoch  8 Batch  76 / 447  Training Loss  5.728424366679974e-05\n",
            "Epoch  8 Batch  77 / 447  Training Loss  3.8191672501852736e-05\n",
            "Epoch  8 Batch  78 / 447  Training Loss  5.423809852800332e-05\n",
            "Epoch  8 Batch  79 / 447  Training Loss  4.038086990476586e-05\n",
            "Epoch  8 Batch  80 / 447  Training Loss  0.00016951160796452314\n",
            "Epoch  8 Batch  81 / 447  Training Loss  4.8639136366546154e-05\n",
            "Epoch  8 Batch  82 / 447  Training Loss  4.0799550333758816e-05\n",
            "Epoch  8 Batch  83 / 447  Training Loss  4.359117156127468e-05\n",
            "Epoch  8 Batch  84 / 447  Training Loss  6.495814159279689e-05\n",
            "Epoch  8 Batch  85 / 447  Training Loss  4.164524216321297e-05\n",
            "Epoch  8 Batch  86 / 447  Training Loss  4.707777770818211e-05\n",
            "Epoch  8 Batch  87 / 447  Training Loss  5.733210491598584e-05\n",
            "Epoch  8 Batch  88 / 447  Training Loss  5.4125986935105175e-05\n",
            "Epoch  8 Batch  89 / 447  Training Loss  4.8530506319366395e-05\n",
            "Epoch  8 Batch  90 / 447  Training Loss  3.813775765593164e-05\n",
            "Epoch  8 Batch  91 / 447  Training Loss  4.335098492447287e-05\n",
            "Epoch  8 Batch  92 / 447  Training Loss  5.024029087508097e-05\n",
            "Epoch  8 Batch  93 / 447  Training Loss  3.7775047530885786e-05\n",
            "Epoch  8 Batch  94 / 447  Training Loss  9.8826625617221e-05\n",
            "Epoch  8 Batch  95 / 447  Training Loss  4.3437750719022006e-05\n",
            "Epoch  8 Batch  96 / 447  Training Loss  5.285177758196369e-05\n",
            "Epoch  8 Batch  97 / 447  Training Loss  5.7622190070105717e-05\n",
            "Epoch  8 Batch  98 / 447  Training Loss  4.714533861260861e-05\n",
            "Epoch  8 Batch  99 / 447  Training Loss  4.8996265832101926e-05\n",
            "Epoch  8 Batch  100 / 447  Training Loss  5.036227958044037e-05\n",
            "Epoch  8 Batch  101 / 447  Training Loss  3.278259828221053e-05\n",
            "Epoch  8 Batch  102 / 447  Training Loss  4.7674766392447054e-05\n",
            "Epoch  8 Batch  103 / 447  Training Loss  6.558271707035601e-05\n",
            "Epoch  8 Batch  104 / 447  Training Loss  0.0005720427725464106\n",
            "Epoch  8 Batch  105 / 447  Training Loss  6.14616583334282e-05\n",
            "Epoch  8 Batch  106 / 447  Training Loss  3.06178662867751e-05\n",
            "Epoch  8 Batch  107 / 447  Training Loss  2.9917182473582216e-05\n",
            "Epoch  8 Batch  108 / 447  Training Loss  3.44802720064763e-05\n",
            "Epoch  8 Batch  109 / 447  Training Loss  4.49818980996497e-05\n",
            "Epoch  8 Batch  110 / 447  Training Loss  6.061966996639967e-05\n",
            "Epoch  8 Batch  111 / 447  Training Loss  5.689730096491985e-05\n",
            "Epoch  8 Batch  112 / 447  Training Loss  3.602135620894842e-05\n",
            "Epoch  8 Batch  113 / 447  Training Loss  7.181063119787723e-05\n",
            "Epoch  8 Batch  114 / 447  Training Loss  4.958795034326613e-05\n",
            "Epoch  8 Batch  115 / 447  Training Loss  5.560404679272324e-05\n",
            "Epoch  8 Batch  116 / 447  Training Loss  5.5625292588956654e-05\n",
            "Epoch  8 Batch  117 / 447  Training Loss  3.0360401069629006e-05\n",
            "Epoch  8 Batch  118 / 447  Training Loss  3.406474206713028e-05\n",
            "Epoch  8 Batch  119 / 447  Training Loss  5.155347389518283e-05\n",
            "Epoch  8 Batch  120 / 447  Training Loss  6.287183350650594e-05\n",
            "Epoch  8 Batch  121 / 447  Training Loss  4.3190237192902714e-05\n",
            "Epoch  8 Batch  122 / 447  Training Loss  5.866681385668926e-05\n",
            "Epoch  8 Batch  123 / 447  Training Loss  0.00010657768871169537\n",
            "Epoch  8 Batch  124 / 447  Training Loss  4.485616955207661e-05\n",
            "Epoch  8 Batch  125 / 447  Training Loss  0.00026774441357702017\n",
            "Epoch  8 Batch  126 / 447  Training Loss  4.495417306316085e-05\n",
            "Epoch  8 Batch  127 / 447  Training Loss  6.911184755153954e-05\n",
            "Epoch  8 Batch  128 / 447  Training Loss  6.339325045701116e-05\n",
            "Epoch  8 Batch  129 / 447  Training Loss  2.8165597541374154e-05\n",
            "Epoch  8 Batch  130 / 447  Training Loss  6.1008369812043384e-05\n",
            "Epoch  8 Batch  131 / 447  Training Loss  0.0001025381134240888\n",
            "Epoch  8 Batch  132 / 447  Training Loss  0.00014612619997933507\n",
            "Epoch  8 Batch  133 / 447  Training Loss  0.00012241295189596713\n",
            "Epoch  8 Batch  134 / 447  Training Loss  4.759160219691694e-05\n",
            "Epoch  8 Batch  135 / 447  Training Loss  7.397795707220212e-05\n",
            "Epoch  8 Batch  136 / 447  Training Loss  7.440867193508893e-05\n",
            "Epoch  8 Batch  137 / 447  Training Loss  5.372930900193751e-05\n",
            "Epoch  8 Batch  138 / 447  Training Loss  5.99890154262539e-05\n",
            "Epoch  8 Batch  139 / 447  Training Loss  4.592619734467007e-05\n",
            "Epoch  8 Batch  140 / 447  Training Loss  4.107924542040564e-05\n",
            "Epoch  8 Batch  141 / 447  Training Loss  7.699692650930956e-05\n",
            "Epoch  8 Batch  142 / 447  Training Loss  4.055759563925676e-05\n",
            "Epoch  8 Batch  143 / 447  Training Loss  5.328808765625581e-05\n",
            "Epoch  8 Batch  144 / 447  Training Loss  4.316345803090371e-05\n",
            "Epoch  8 Batch  145 / 447  Training Loss  3.385030504432507e-05\n",
            "Epoch  8 Batch  146 / 447  Training Loss  3.202467996743508e-05\n",
            "Epoch  8 Batch  147 / 447  Training Loss  5.269995745038614e-05\n",
            "Epoch  8 Batch  148 / 447  Training Loss  7.061303767841309e-05\n",
            "Epoch  8 Batch  149 / 447  Training Loss  7.515084143960848e-05\n",
            "Epoch  8 Batch  150 / 447  Training Loss  4.852842539548874e-05\n",
            "Epoch  8 Batch  151 / 447  Training Loss  4.6098743041511625e-05\n",
            "Epoch  8 Batch  152 / 447  Training Loss  8.489845640724525e-05\n",
            "Epoch  8 Batch  153 / 447  Training Loss  4.592160621541552e-05\n",
            "Epoch  8 Batch  154 / 447  Training Loss  3.66066160495393e-05\n",
            "Epoch  8 Batch  155 / 447  Training Loss  3.8887501432327554e-05\n",
            "Epoch  8 Batch  156 / 447  Training Loss  3.836849646177143e-05\n",
            "Epoch  8 Batch  157 / 447  Training Loss  4.929116403218359e-05\n",
            "Epoch  8 Batch  158 / 447  Training Loss  5.371820225263946e-05\n",
            "Epoch  8 Batch  159 / 447  Training Loss  6.462923920480534e-05\n",
            "Epoch  8 Batch  160 / 447  Training Loss  4.380145401228219e-05\n",
            "Epoch  8 Batch  161 / 447  Training Loss  3.679030487546697e-05\n",
            "Epoch  8 Batch  162 / 447  Training Loss  2.7950040021096356e-05\n",
            "Epoch  8 Batch  163 / 447  Training Loss  4.935356628266163e-05\n",
            "Epoch  8 Batch  164 / 447  Training Loss  4.985024133929983e-05\n",
            "Epoch  8 Batch  165 / 447  Training Loss  6.391337228706107e-05\n",
            "Epoch  8 Batch  166 / 447  Training Loss  5.7240580645157024e-05\n",
            "Epoch  8 Batch  167 / 447  Training Loss  4.192485357634723e-05\n",
            "Epoch  8 Batch  168 / 447  Training Loss  4.726292536361143e-05\n",
            "Epoch  8 Batch  169 / 447  Training Loss  3.600730633479543e-05\n",
            "Epoch  8 Batch  170 / 447  Training Loss  5.160438013263047e-05\n",
            "Epoch  8 Batch  171 / 447  Training Loss  4.1044389945454895e-05\n",
            "Epoch  8 Batch  172 / 447  Training Loss  4.5206237700767815e-05\n",
            "Epoch  8 Batch  173 / 447  Training Loss  5.173785029910505e-05\n",
            "Epoch  8 Batch  174 / 447  Training Loss  5.306983075570315e-05\n",
            "Epoch  8 Batch  175 / 447  Training Loss  5.9183152188779786e-05\n",
            "Epoch  8 Batch  176 / 447  Training Loss  5.0475984608056024e-05\n",
            "Epoch  8 Batch  177 / 447  Training Loss  3.7257399526424706e-05\n",
            "Epoch  8 Batch  178 / 447  Training Loss  3.169897536281496e-05\n",
            "Epoch  8 Batch  179 / 447  Training Loss  7.78418907430023e-05\n",
            "Epoch  8 Batch  180 / 447  Training Loss  5.267369851935655e-05\n",
            "Epoch  8 Batch  181 / 447  Training Loss  4.500858631217852e-05\n",
            "Epoch  8 Batch  182 / 447  Training Loss  4.997511496185325e-05\n",
            "Epoch  8 Batch  183 / 447  Training Loss  3.476479105302133e-05\n",
            "Epoch  8 Batch  184 / 447  Training Loss  3.785750232054852e-05\n",
            "Epoch  8 Batch  185 / 447  Training Loss  6.1290011217352e-05\n",
            "Epoch  8 Batch  186 / 447  Training Loss  4.1783587221289054e-05\n",
            "Epoch  8 Batch  187 / 447  Training Loss  6.703408143948764e-05\n",
            "Epoch  8 Batch  188 / 447  Training Loss  5.788777343695983e-05\n",
            "Epoch  8 Batch  189 / 447  Training Loss  4.035600431961939e-05\n",
            "Epoch  8 Batch  190 / 447  Training Loss  5.5883865570649505e-05\n",
            "Epoch  8 Batch  191 / 447  Training Loss  0.00029402607469819486\n",
            "Epoch  8 Batch  192 / 447  Training Loss  2.9061786335660145e-05\n",
            "Epoch  8 Batch  193 / 447  Training Loss  4.709548375103623e-05\n",
            "Epoch  8 Batch  194 / 447  Training Loss  4.0606460970593616e-05\n",
            "Epoch  8 Batch  195 / 447  Training Loss  4.338028884376399e-05\n",
            "Epoch  8 Batch  196 / 447  Training Loss  4.307266863179393e-05\n",
            "Epoch  8 Batch  197 / 447  Training Loss  4.2232546547893435e-05\n",
            "Epoch  8 Batch  198 / 447  Training Loss  4.495177563512698e-05\n",
            "Epoch  8 Batch  199 / 447  Training Loss  7.011126581346616e-05\n",
            "Epoch  8 Batch  200 / 447  Training Loss  5.945819430053234e-05\n",
            "Epoch  8 Batch  201 / 447  Training Loss  6.086688154027797e-05\n",
            "Epoch  8 Batch  202 / 447  Training Loss  3.4094533475581557e-05\n",
            "Epoch  8 Batch  203 / 447  Training Loss  5.411838719737716e-05\n",
            "Epoch  8 Batch  204 / 447  Training Loss  4.2601881432347e-05\n",
            "Epoch  8 Batch  205 / 447  Training Loss  4.1533956391504034e-05\n",
            "Epoch  8 Batch  206 / 447  Training Loss  3.459796425886452e-05\n",
            "Epoch  8 Batch  207 / 447  Training Loss  8.894113125279546e-05\n",
            "Epoch  8 Batch  208 / 447  Training Loss  3.8690221117576584e-05\n",
            "Epoch  8 Batch  209 / 447  Training Loss  5.364279422792606e-05\n",
            "Epoch  8 Batch  210 / 447  Training Loss  5.1384962716838345e-05\n",
            "Epoch  8 Batch  211 / 447  Training Loss  6.148911779746413e-05\n",
            "Epoch  8 Batch  212 / 447  Training Loss  5.209667142480612e-05\n",
            "Epoch  8 Batch  213 / 447  Training Loss  3.700181332533248e-05\n",
            "Epoch  8 Batch  214 / 447  Training Loss  4.7139456000877544e-05\n",
            "Epoch  8 Batch  215 / 447  Training Loss  4.967654240317643e-05\n",
            "Epoch  8 Batch  216 / 447  Training Loss  3.47727291227784e-05\n",
            "Epoch  8 Batch  217 / 447  Training Loss  4.872553108725697e-05\n",
            "Epoch  8 Batch  218 / 447  Training Loss  5.246658474789001e-05\n",
            "Epoch  8 Batch  219 / 447  Training Loss  4.1976523789344355e-05\n",
            "Epoch  8 Batch  220 / 447  Training Loss  5.658244845108129e-05\n",
            "Epoch  8 Batch  221 / 447  Training Loss  3.2612766517559066e-05\n",
            "Epoch  8 Batch  222 / 447  Training Loss  5.6756529374979436e-05\n",
            "Epoch  8 Batch  223 / 447  Training Loss  4.6977576857898384e-05\n",
            "Epoch  8 Batch  224 / 447  Training Loss  3.3287378755630925e-05\n",
            "Epoch  8 Batch  225 / 447  Training Loss  5.00739406561479e-05\n",
            "Epoch  8 Batch  226 / 447  Training Loss  5.915157817071304e-05\n",
            "Epoch  8 Batch  227 / 447  Training Loss  3.4524120565038174e-05\n",
            "Epoch  8 Batch  228 / 447  Training Loss  4.782449104823172e-05\n",
            "Epoch  8 Batch  229 / 447  Training Loss  2.8459962777560577e-05\n",
            "Epoch  8 Batch  230 / 447  Training Loss  8.138781413435936e-05\n",
            "Epoch  8 Batch  231 / 447  Training Loss  4.286669718567282e-05\n",
            "Epoch  8 Batch  232 / 447  Training Loss  4.495682514971122e-05\n",
            "Epoch  8 Batch  233 / 447  Training Loss  5.0994018238270655e-05\n",
            "Epoch  8 Batch  234 / 447  Training Loss  4.342299507698044e-05\n",
            "Epoch  8 Batch  235 / 447  Training Loss  3.786749948631041e-05\n",
            "Epoch  8 Batch  236 / 447  Training Loss  4.101928789168596e-05\n",
            "Epoch  8 Batch  237 / 447  Training Loss  3.2927993743214756e-05\n",
            "Epoch  8 Batch  238 / 447  Training Loss  4.012549470644444e-05\n",
            "Epoch  8 Batch  239 / 447  Training Loss  2.685351500986144e-05\n",
            "Epoch  8 Batch  240 / 447  Training Loss  6.210589344846085e-05\n",
            "Epoch  8 Batch  241 / 447  Training Loss  3.0262464861152694e-05\n",
            "Epoch  8 Batch  242 / 447  Training Loss  3.4875971323344857e-05\n",
            "Epoch  8 Batch  243 / 447  Training Loss  3.30730399582535e-05\n",
            "Epoch  8 Batch  244 / 447  Training Loss  6.529309030156583e-05\n",
            "Epoch  8 Batch  245 / 447  Training Loss  3.075163112953305e-05\n",
            "Epoch  8 Batch  246 / 447  Training Loss  4.184532735962421e-05\n",
            "Epoch  8 Batch  247 / 447  Training Loss  5.488242459250614e-05\n",
            "Epoch  8 Batch  248 / 447  Training Loss  2.7664058507070877e-05\n",
            "Epoch  8 Batch  249 / 447  Training Loss  4.0394523239228874e-05\n",
            "Epoch  8 Batch  250 / 447  Training Loss  3.255325282225385e-05\n",
            "Epoch  8 Batch  251 / 447  Training Loss  3.062891846639104e-05\n",
            "Epoch  8 Batch  252 / 447  Training Loss  6.859414133941755e-05\n",
            "Epoch  8 Batch  253 / 447  Training Loss  5.022947152610868e-05\n",
            "Epoch  8 Batch  254 / 447  Training Loss  4.1308318031951785e-05\n",
            "Epoch  8 Batch  255 / 447  Training Loss  3.151022974634543e-05\n",
            "Epoch  8 Batch  256 / 447  Training Loss  4.68980724690482e-05\n",
            "Epoch  8 Batch  257 / 447  Training Loss  3.6799385270569474e-05\n",
            "Epoch  8 Batch  258 / 447  Training Loss  3.9886432205094025e-05\n",
            "Epoch  8 Batch  259 / 447  Training Loss  4.908493792754598e-05\n",
            "Epoch  8 Batch  260 / 447  Training Loss  3.998339161626063e-05\n",
            "Epoch  8 Batch  261 / 447  Training Loss  3.333327185828239e-05\n",
            "Epoch  8 Batch  262 / 447  Training Loss  4.325946429162286e-05\n",
            "Epoch  8 Batch  263 / 447  Training Loss  3.255940828239545e-05\n",
            "Epoch  8 Batch  264 / 447  Training Loss  4.8471851187059656e-05\n",
            "Epoch  8 Batch  265 / 447  Training Loss  3.870583896059543e-05\n",
            "Epoch  8 Batch  266 / 447  Training Loss  4.265009920345619e-05\n",
            "Epoch  8 Batch  267 / 447  Training Loss  5.957704343018122e-05\n",
            "Epoch  8 Batch  268 / 447  Training Loss  2.21237769437721e-05\n",
            "Epoch  8 Batch  269 / 447  Training Loss  4.865995651925914e-05\n",
            "Epoch  8 Batch  270 / 447  Training Loss  2.6031080778921023e-05\n",
            "Epoch  8 Batch  271 / 447  Training Loss  4.9578360631130636e-05\n",
            "Epoch  8 Batch  272 / 447  Training Loss  3.7679994420614094e-05\n",
            "Epoch  8 Batch  273 / 447  Training Loss  3.350221231812611e-05\n",
            "Epoch  8 Batch  274 / 447  Training Loss  4.966237611370161e-05\n",
            "Epoch  8 Batch  275 / 447  Training Loss  4.174064451945014e-05\n",
            "Epoch  8 Batch  276 / 447  Training Loss  4.694284871220589e-05\n",
            "Epoch  8 Batch  277 / 447  Training Loss  4.7145407734205946e-05\n",
            "Epoch  8 Batch  278 / 447  Training Loss  3.833364098682068e-05\n",
            "Epoch  8 Batch  279 / 447  Training Loss  4.6664139517815784e-05\n",
            "Epoch  8 Batch  280 / 447  Training Loss  5.838936704094522e-05\n",
            "Epoch  8 Batch  281 / 447  Training Loss  3.061410825466737e-05\n",
            "Epoch  8 Batch  282 / 447  Training Loss  3.68428482033778e-05\n",
            "Epoch  8 Batch  283 / 447  Training Loss  5.6592511100461707e-05\n",
            "Epoch  8 Batch  284 / 447  Training Loss  4.501126386458054e-05\n",
            "Epoch  8 Batch  285 / 447  Training Loss  4.778908987645991e-05\n",
            "Epoch  8 Batch  286 / 447  Training Loss  2.9187638574512675e-05\n",
            "Epoch  8 Batch  287 / 447  Training Loss  7.254997035488486e-05\n",
            "Epoch  8 Batch  288 / 447  Training Loss  2.743595359788742e-05\n",
            "Epoch  8 Batch  289 / 447  Training Loss  3.7738358514616266e-05\n",
            "Epoch  8 Batch  290 / 447  Training Loss  2.8609545552171767e-05\n",
            "Epoch  8 Batch  291 / 447  Training Loss  4.1896742914104834e-05\n",
            "Epoch  8 Batch  292 / 447  Training Loss  3.175251549691893e-05\n",
            "Epoch  8 Batch  293 / 447  Training Loss  4.560613160720095e-05\n",
            "Epoch  8 Batch  294 / 447  Training Loss  4.5839267841074616e-05\n",
            "Epoch  8 Batch  295 / 447  Training Loss  2.8662516342592426e-05\n",
            "Epoch  8 Batch  296 / 447  Training Loss  3.264531915192492e-05\n",
            "Epoch  8 Batch  297 / 447  Training Loss  3.2826774258865044e-05\n",
            "Epoch  8 Batch  298 / 447  Training Loss  3.511997783789411e-05\n",
            "Epoch  8 Batch  299 / 447  Training Loss  3.695135092129931e-05\n",
            "Epoch  8 Batch  300 / 447  Training Loss  3.7233960028970614e-05\n",
            "Epoch  8 Batch  301 / 447  Training Loss  3.939638554584235e-05\n",
            "Epoch  8 Batch  302 / 447  Training Loss  2.8416505301720463e-05\n",
            "Epoch  8 Batch  303 / 447  Training Loss  3.089405799983069e-05\n",
            "Epoch  8 Batch  304 / 447  Training Loss  3.462654058239423e-05\n",
            "Epoch  8 Batch  305 / 447  Training Loss  3.2591189665254205e-05\n",
            "Epoch  8 Batch  306 / 447  Training Loss  4.1237453842768446e-05\n",
            "Epoch  8 Batch  307 / 447  Training Loss  2.580409636721015e-05\n",
            "Epoch  8 Batch  308 / 447  Training Loss  4.009205804322846e-05\n",
            "Epoch  8 Batch  309 / 447  Training Loss  3.010713044204749e-05\n",
            "Epoch  8 Batch  310 / 447  Training Loss  3.56310629285872e-05\n",
            "Epoch  8 Batch  311 / 447  Training Loss  5.153613165020943e-05\n",
            "Epoch  8 Batch  312 / 447  Training Loss  3.600935815484263e-05\n",
            "Epoch  8 Batch  313 / 447  Training Loss  3.752950215130113e-05\n",
            "Epoch  8 Batch  314 / 447  Training Loss  3.822064172709361e-05\n",
            "Epoch  8 Batch  315 / 447  Training Loss  3.959249079343863e-05\n",
            "Epoch  8 Batch  316 / 447  Training Loss  3.7804256862727925e-05\n",
            "Epoch  8 Batch  317 / 447  Training Loss  2.855013190128375e-05\n",
            "Epoch  8 Batch  318 / 447  Training Loss  3.553442002157681e-05\n",
            "Epoch  8 Batch  319 / 447  Training Loss  3.353014835738577e-05\n",
            "Epoch  8 Batch  320 / 447  Training Loss  0.0001865940575953573\n",
            "Epoch  8 Batch  321 / 447  Training Loss  3.200151331839152e-05\n",
            "Epoch  8 Batch  322 / 447  Training Loss  3.2929183362284675e-05\n",
            "Epoch  8 Batch  323 / 447  Training Loss  2.718784162425436e-05\n",
            "Epoch  8 Batch  324 / 447  Training Loss  4.671869828598574e-05\n",
            "Epoch  8 Batch  325 / 447  Training Loss  2.8791406293748878e-05\n",
            "Epoch  8 Batch  326 / 447  Training Loss  5.2872834203299135e-05\n",
            "Epoch  8 Batch  327 / 447  Training Loss  3.5511795431375504e-05\n",
            "Epoch  8 Batch  328 / 447  Training Loss  2.9729673769907095e-05\n",
            "Epoch  8 Batch  329 / 447  Training Loss  4.2611554817995057e-05\n",
            "Epoch  8 Batch  330 / 447  Training Loss  3.352005296619609e-05\n",
            "Epoch  8 Batch  331 / 447  Training Loss  3.2121126423589885e-05\n",
            "Epoch  8 Batch  332 / 447  Training Loss  3.4195734770037234e-05\n",
            "Epoch  8 Batch  333 / 447  Training Loss  3.345060395076871e-05\n",
            "Epoch  8 Batch  334 / 447  Training Loss  0.00012795129441656172\n",
            "Epoch  8 Batch  335 / 447  Training Loss  3.783751526498236e-05\n",
            "Epoch  8 Batch  336 / 447  Training Loss  2.4068054699455388e-05\n",
            "Epoch  8 Batch  337 / 447  Training Loss  3.254338298575021e-05\n",
            "Epoch  8 Batch  338 / 447  Training Loss  3.7625828554155305e-05\n",
            "Epoch  8 Batch  339 / 447  Training Loss  4.975265983375721e-05\n",
            "Epoch  8 Batch  340 / 447  Training Loss  4.095811891602352e-05\n",
            "Epoch  8 Batch  341 / 447  Training Loss  4.011342389276251e-05\n",
            "Epoch  8 Batch  342 / 447  Training Loss  4.855448423768394e-05\n",
            "Epoch  8 Batch  343 / 447  Training Loss  3.748206654563546e-05\n",
            "Epoch  8 Batch  344 / 447  Training Loss  3.821193604380824e-05\n",
            "Epoch  8 Batch  345 / 447  Training Loss  2.9405817258520983e-05\n",
            "Epoch  8 Batch  346 / 447  Training Loss  3.7034438719274476e-05\n",
            "Epoch  8 Batch  347 / 447  Training Loss  4.7952929890016094e-05\n",
            "Epoch  8 Batch  348 / 447  Training Loss  4.554373299470171e-05\n",
            "Epoch  8 Batch  349 / 447  Training Loss  2.8182625101180747e-05\n",
            "Epoch  8 Batch  350 / 447  Training Loss  5.679911555489525e-05\n",
            "Epoch  8 Batch  351 / 447  Training Loss  5.306648381520063e-05\n",
            "Epoch  8 Batch  352 / 447  Training Loss  4.797127257916145e-05\n",
            "Epoch  8 Batch  353 / 447  Training Loss  5.745224189013243e-05\n",
            "Epoch  8 Batch  354 / 447  Training Loss  4.286038165446371e-05\n",
            "Epoch  8 Batch  355 / 447  Training Loss  4.9014855903806165e-05\n",
            "Epoch  8 Batch  356 / 447  Training Loss  2.5499148250673898e-05\n",
            "Epoch  8 Batch  357 / 447  Training Loss  4.146146966377273e-05\n",
            "Epoch  8 Batch  358 / 447  Training Loss  3.897478018188849e-05\n",
            "Epoch  8 Batch  359 / 447  Training Loss  3.01298096019309e-05\n",
            "Epoch  8 Batch  360 / 447  Training Loss  7.911690772743896e-05\n",
            "Epoch  8 Batch  361 / 447  Training Loss  3.3355139748891816e-05\n",
            "Epoch  8 Batch  362 / 447  Training Loss  4.962931416230276e-05\n",
            "Epoch  8 Batch  363 / 447  Training Loss  3.97205694753211e-05\n",
            "Epoch  8 Batch  364 / 447  Training Loss  4.42444761574734e-05\n",
            "Epoch  8 Batch  365 / 447  Training Loss  3.481496241874993e-05\n",
            "Epoch  8 Batch  366 / 447  Training Loss  3.927552097593434e-05\n",
            "Epoch  8 Batch  367 / 447  Training Loss  3.6762758099939674e-05\n",
            "Epoch  8 Batch  368 / 447  Training Loss  2.7146794309373945e-05\n",
            "Epoch  8 Batch  369 / 447  Training Loss  2.9049162549199536e-05\n",
            "Epoch  8 Batch  370 / 447  Training Loss  0.00010173658665735275\n",
            "Epoch  8 Batch  371 / 447  Training Loss  0.0017256238497793674\n",
            "Epoch  8 Batch  372 / 447  Training Loss  4.0433475078316405e-05\n",
            "Epoch  8 Batch  373 / 447  Training Loss  4.0028928196989e-05\n",
            "Epoch  8 Batch  374 / 447  Training Loss  3.122925409115851e-05\n",
            "Epoch  8 Batch  375 / 447  Training Loss  3.863468373310752e-05\n",
            "Epoch  8 Batch  376 / 447  Training Loss  3.50040354533121e-05\n",
            "Epoch  8 Batch  377 / 447  Training Loss  4.338630606071092e-05\n",
            "Epoch  8 Batch  378 / 447  Training Loss  2.6492258257349022e-05\n",
            "Epoch  8 Batch  379 / 447  Training Loss  3.4037242585327476e-05\n",
            "Epoch  8 Batch  380 / 447  Training Loss  3.355095032020472e-05\n",
            "Epoch  8 Batch  381 / 447  Training Loss  0.00011088120663771406\n",
            "Epoch  8 Batch  382 / 447  Training Loss  4.6795234084129333e-05\n",
            "Epoch  8 Batch  383 / 447  Training Loss  2.5644447305239737e-05\n",
            "Epoch  8 Batch  384 / 447  Training Loss  3.8802114431746304e-05\n",
            "Epoch  8 Batch  385 / 447  Training Loss  0.00014296831795945764\n",
            "Epoch  8 Batch  386 / 447  Training Loss  4.233619256410748e-05\n",
            "Epoch  8 Batch  387 / 447  Training Loss  5.1875078497687355e-05\n",
            "Epoch  8 Batch  388 / 447  Training Loss  4.582840847433545e-05\n",
            "Epoch  8 Batch  389 / 447  Training Loss  5.177256025490351e-05\n",
            "Epoch  8 Batch  390 / 447  Training Loss  2.9595685191452503e-05\n",
            "Epoch  8 Batch  391 / 447  Training Loss  4.6918794396333396e-05\n",
            "Epoch  8 Batch  392 / 447  Training Loss  3.5146429581800476e-05\n",
            "Epoch  8 Batch  393 / 447  Training Loss  3.455026671872474e-05\n",
            "Epoch  8 Batch  394 / 447  Training Loss  9.055930422618985e-05\n",
            "Epoch  8 Batch  395 / 447  Training Loss  5.530334237846546e-05\n",
            "Epoch  8 Batch  396 / 447  Training Loss  0.00026053478359244764\n",
            "Epoch  8 Batch  397 / 447  Training Loss  3.4442797186784446e-05\n",
            "Epoch  8 Batch  398 / 447  Training Loss  2.5400750018889084e-05\n",
            "Epoch  8 Batch  399 / 447  Training Loss  3.2588588510407135e-05\n",
            "Epoch  8 Batch  400 / 447  Training Loss  3.122684211120941e-05\n",
            "Epoch  8 Batch  401 / 447  Training Loss  6.80408556945622e-05\n",
            "Epoch  8 Batch  402 / 447  Training Loss  2.5004448616527952e-05\n",
            "Epoch  8 Batch  403 / 447  Training Loss  4.451698259799741e-05\n",
            "Epoch  8 Batch  404 / 447  Training Loss  4.7253688535420224e-05\n",
            "Epoch  8 Batch  405 / 447  Training Loss  3.880475560436025e-05\n",
            "Epoch  8 Batch  406 / 447  Training Loss  4.002905552624725e-05\n",
            "Epoch  8 Batch  407 / 447  Training Loss  5.266605512588285e-05\n",
            "Epoch  8 Batch  408 / 447  Training Loss  4.223599171382375e-05\n",
            "Epoch  8 Batch  409 / 447  Training Loss  0.006924380082637072\n",
            "Epoch  8 Batch  410 / 447  Training Loss  3.587851097108796e-05\n",
            "Epoch  8 Batch  411 / 447  Training Loss  5.5455631809309125e-05\n",
            "Epoch  8 Batch  412 / 447  Training Loss  3.769103568629362e-05\n",
            "Epoch  8 Batch  413 / 447  Training Loss  3.809131885645911e-05\n",
            "Epoch  8 Batch  414 / 447  Training Loss  3.305783320683986e-05\n",
            "Epoch  8 Batch  415 / 447  Training Loss  6.558339373441413e-05\n",
            "Epoch  8 Batch  416 / 447  Training Loss  4.0378774428972974e-05\n",
            "Epoch  8 Batch  417 / 447  Training Loss  4.511631414061412e-05\n",
            "Epoch  8 Batch  418 / 447  Training Loss  4.642346902983263e-05\n",
            "Epoch  8 Batch  419 / 447  Training Loss  0.0001834206486819312\n",
            "Epoch  8 Batch  420 / 447  Training Loss  3.129644028376788e-05\n",
            "Epoch  8 Batch  421 / 447  Training Loss  4.8407830036012456e-05\n",
            "Epoch  8 Batch  422 / 447  Training Loss  4.0312690543942153e-05\n",
            "Epoch  8 Batch  423 / 447  Training Loss  6.241566006792709e-05\n",
            "Epoch  8 Batch  424 / 447  Training Loss  2.8087031751056202e-05\n",
            "Epoch  8 Batch  425 / 447  Training Loss  0.001022009877488017\n",
            "Epoch  8 Batch  426 / 447  Training Loss  7.641359843546525e-05\n",
            "Epoch  8 Batch  427 / 447  Training Loss  2.8129210477345623e-05\n",
            "Epoch  8 Batch  428 / 447  Training Loss  3.239273064536974e-05\n",
            "Epoch  8 Batch  429 / 447  Training Loss  3.1937008316162974e-05\n",
            "Epoch  8 Batch  430 / 447  Training Loss  3.8605045119766146e-05\n",
            "Epoch  8 Batch  431 / 447  Training Loss  2.8057547751814127e-05\n",
            "Epoch  8 Batch  432 / 447  Training Loss  4.0562445065006614e-05\n",
            "Epoch  8 Batch  433 / 447  Training Loss  4.9119495088234544e-05\n",
            "Epoch  8 Batch  434 / 447  Training Loss  4.8336467443732545e-05\n",
            "Epoch  8 Batch  435 / 447  Training Loss  2.9471928428392857e-05\n",
            "Epoch  8 Batch  436 / 447  Training Loss  0.00011380830983398482\n",
            "Epoch  8 Batch  437 / 447  Training Loss  3.9531751099275425e-05\n",
            "Epoch  8 Batch  438 / 447  Training Loss  2.6604682716424577e-05\n",
            "Epoch  8 Batch  439 / 447  Training Loss  3.8414684240706265e-05\n",
            "Epoch  8 Batch  440 / 447  Training Loss  3.602595461416058e-05\n",
            "Epoch  8 Batch  441 / 447  Training Loss  3.8763737393310294e-05\n",
            "Epoch  8 Batch  442 / 447  Training Loss  5.793380842078477e-05\n",
            "Epoch  8 Batch  443 / 447  Training Loss  4.726433689938858e-05\n",
            "Epoch  8 Batch  444 / 447  Training Loss  0.0005432835314422846\n",
            "Epoch  8 Batch  445 / 447  Training Loss  4.766753045259975e-05\n",
            "Epoch  8 Batch  446 / 447  Training Loss  2.260077053506393e-05\n",
            "   9    |    -    |   0.000075   | 99.951019\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 9\n",
            "Epoch  9 Batch  0 / 447  Training Loss  5.574234819505364e-05\n",
            "Epoch  9 Batch  1 / 447  Training Loss  3.877769995597191e-05\n",
            "Epoch  9 Batch  2 / 447  Training Loss  3.23836829920765e-05\n",
            "Epoch  9 Batch  3 / 447  Training Loss  4.4013540900778025e-05\n",
            "Epoch  9 Batch  4 / 447  Training Loss  4.346687637735158e-05\n",
            "Epoch  9 Batch  5 / 447  Training Loss  4.996171992388554e-05\n",
            "Epoch  9 Batch  6 / 447  Training Loss  2.7033847800339572e-05\n",
            "Epoch  9 Batch  7 / 447  Training Loss  3.1665247661294416e-05\n",
            "Epoch  9 Batch  8 / 447  Training Loss  3.125083094346337e-05\n",
            "Epoch  9 Batch  9 / 447  Training Loss  4.075582910445519e-05\n",
            "Epoch  9 Batch  10 / 447  Training Loss  2.875617974495981e-05\n",
            "Epoch  9 Batch  11 / 447  Training Loss  3.53901123162359e-05\n",
            "Epoch  9 Batch  12 / 447  Training Loss  5.787449117633514e-05\n",
            "Epoch  9 Batch  13 / 447  Training Loss  3.8100195524748415e-05\n",
            "Epoch  9 Batch  14 / 447  Training Loss  0.00011933562927879393\n",
            "Epoch  9 Batch  15 / 447  Training Loss  4.14402165915817e-05\n",
            "Epoch  9 Batch  16 / 447  Training Loss  2.7011781639885157e-05\n",
            "Epoch  9 Batch  17 / 447  Training Loss  2.3614242309122346e-05\n",
            "Epoch  9 Batch  18 / 447  Training Loss  4.7968878789106384e-05\n",
            "Epoch  9 Batch  19 / 447  Training Loss  3.81943209504243e-05\n",
            "Epoch  9 Batch  20 / 447  Training Loss  4.798200461664237e-05\n",
            "Epoch  9 Batch  21 / 447  Training Loss  2.636781027831603e-05\n",
            "Epoch  9 Batch  22 / 447  Training Loss  4.592866753228009e-05\n",
            "Epoch  9 Batch  23 / 447  Training Loss  3.154723526677117e-05\n",
            "Epoch  9 Batch  24 / 447  Training Loss  3.6627647205023095e-05\n",
            "Epoch  9 Batch  25 / 447  Training Loss  4.030900163343176e-05\n",
            "Epoch  9 Batch  26 / 447  Training Loss  3.248066786909476e-05\n",
            "Epoch  9 Batch  27 / 447  Training Loss  3.7689937016693875e-05\n",
            "Epoch  9 Batch  28 / 447  Training Loss  2.6477420760784298e-05\n",
            "Epoch  9 Batch  29 / 447  Training Loss  3.2909894798649475e-05\n",
            "Epoch  9 Batch  30 / 447  Training Loss  6.56258562230505e-05\n",
            "Epoch  9 Batch  31 / 447  Training Loss  4.081578299519606e-05\n",
            "Epoch  9 Batch  32 / 447  Training Loss  3.6655717849498615e-05\n",
            "Epoch  9 Batch  33 / 447  Training Loss  4.1621104173827916e-05\n",
            "Epoch  9 Batch  34 / 447  Training Loss  5.446763680083677e-05\n",
            "Epoch  9 Batch  35 / 447  Training Loss  3.567812018445693e-05\n",
            "Epoch  9 Batch  36 / 447  Training Loss  4.335717676440254e-05\n",
            "Epoch  9 Batch  37 / 447  Training Loss  5.625815174425952e-05\n",
            "Epoch  9 Batch  38 / 447  Training Loss  5.1386352424742654e-05\n",
            "Epoch  9 Batch  39 / 447  Training Loss  3.943117189919576e-05\n",
            "Epoch  9 Batch  40 / 447  Training Loss  3.217178527847864e-05\n",
            "Epoch  9 Batch  41 / 447  Training Loss  2.1848156393389218e-05\n",
            "Epoch  9 Batch  42 / 447  Training Loss  3.1002484320197254e-05\n",
            "Epoch  9 Batch  43 / 447  Training Loss  3.559940887498669e-05\n",
            "Epoch  9 Batch  44 / 447  Training Loss  2.5926981834345497e-05\n",
            "Epoch  9 Batch  45 / 447  Training Loss  3.315381036372855e-05\n",
            "Epoch  9 Batch  46 / 447  Training Loss  3.505863423924893e-05\n",
            "Epoch  9 Batch  47 / 447  Training Loss  2.9455743060680106e-05\n",
            "Epoch  9 Batch  48 / 447  Training Loss  2.5793542590690777e-05\n",
            "Epoch  9 Batch  49 / 447  Training Loss  2.7401458282838576e-05\n",
            "Epoch  9 Batch  50 / 447  Training Loss  2.379924808337819e-05\n",
            "Epoch  9 Batch  51 / 447  Training Loss  3.4485292417230085e-05\n",
            "Epoch  9 Batch  52 / 447  Training Loss  2.1877272956771776e-05\n",
            "Epoch  9 Batch  53 / 447  Training Loss  3.581867349566892e-05\n",
            "Epoch  9 Batch  54 / 447  Training Loss  4.069565693498589e-05\n",
            "Epoch  9 Batch  55 / 447  Training Loss  2.9145423468435183e-05\n",
            "Epoch  9 Batch  56 / 447  Training Loss  4.2843235860345885e-05\n",
            "Epoch  9 Batch  57 / 447  Training Loss  5.779560160590336e-05\n",
            "Epoch  9 Batch  58 / 447  Training Loss  3.562212077667937e-05\n",
            "Epoch  9 Batch  59 / 447  Training Loss  4.277627886040136e-05\n",
            "Epoch  9 Batch  60 / 447  Training Loss  2.9721770260948688e-05\n",
            "Epoch  9 Batch  61 / 447  Training Loss  2.5864463168545626e-05\n",
            "Epoch  9 Batch  62 / 447  Training Loss  3.231565642636269e-05\n",
            "Epoch  9 Batch  63 / 447  Training Loss  2.8933487556059845e-05\n",
            "Epoch  9 Batch  64 / 447  Training Loss  2.5195804482791573e-05\n",
            "Epoch  9 Batch  65 / 447  Training Loss  3.266828571213409e-05\n",
            "Epoch  9 Batch  66 / 447  Training Loss  4.6914930862840265e-05\n",
            "Epoch  9 Batch  67 / 447  Training Loss  4.930551949655637e-05\n",
            "Epoch  9 Batch  68 / 447  Training Loss  2.810990008583758e-05\n",
            "Epoch  9 Batch  69 / 447  Training Loss  3.685395131469704e-05\n",
            "Epoch  9 Batch  70 / 447  Training Loss  3.211208968423307e-05\n",
            "Epoch  9 Batch  71 / 447  Training Loss  3.5066299460595474e-05\n",
            "Epoch  9 Batch  72 / 447  Training Loss  2.802050585160032e-05\n",
            "Epoch  9 Batch  73 / 447  Training Loss  2.734773079282604e-05\n",
            "Epoch  9 Batch  74 / 447  Training Loss  4.3932181142736226e-05\n",
            "Epoch  9 Batch  75 / 447  Training Loss  4.048353366670199e-05\n",
            "Epoch  9 Batch  76 / 447  Training Loss  3.206923429388553e-05\n",
            "Epoch  9 Batch  77 / 447  Training Loss  3.9027425373205915e-05\n",
            "Epoch  9 Batch  78 / 447  Training Loss  3.680897134472616e-05\n",
            "Epoch  9 Batch  79 / 447  Training Loss  2.73796176770702e-05\n",
            "Epoch  9 Batch  80 / 447  Training Loss  2.9576536690001376e-05\n",
            "Epoch  9 Batch  81 / 447  Training Loss  3.8158814277267084e-05\n",
            "Epoch  9 Batch  82 / 447  Training Loss  3.903691322193481e-05\n",
            "Epoch  9 Batch  83 / 447  Training Loss  2.6597668693284504e-05\n",
            "Epoch  9 Batch  84 / 447  Training Loss  4.190511390333995e-05\n",
            "Epoch  9 Batch  85 / 447  Training Loss  2.6566453016130254e-05\n",
            "Epoch  9 Batch  86 / 447  Training Loss  3.335902511025779e-05\n",
            "Epoch  9 Batch  87 / 447  Training Loss  4.216266461298801e-05\n",
            "Epoch  9 Batch  88 / 447  Training Loss  3.9609883970115334e-05\n",
            "Epoch  9 Batch  89 / 447  Training Loss  3.368301986483857e-05\n",
            "Epoch  9 Batch  90 / 447  Training Loss  3.506022403598763e-05\n",
            "Epoch  9 Batch  91 / 447  Training Loss  2.594805118860677e-05\n",
            "Epoch  9 Batch  92 / 447  Training Loss  3.818828918156214e-05\n",
            "Epoch  9 Batch  93 / 447  Training Loss  2.775659777398687e-05\n",
            "Epoch  9 Batch  94 / 447  Training Loss  2.971999128931202e-05\n",
            "Epoch  9 Batch  95 / 447  Training Loss  3.245002881158143e-05\n",
            "Epoch  9 Batch  96 / 447  Training Loss  1.9900768165825866e-05\n",
            "Epoch  9 Batch  97 / 447  Training Loss  5.891062392038293e-05\n",
            "Epoch  9 Batch  98 / 447  Training Loss  3.215153992641717e-05\n",
            "Epoch  9 Batch  99 / 447  Training Loss  3.5099696106044576e-05\n",
            "Epoch  9 Batch  100 / 447  Training Loss  2.7245641831541434e-05\n",
            "Epoch  9 Batch  101 / 447  Training Loss  3.2203886803472415e-05\n",
            "Epoch  9 Batch  102 / 447  Training Loss  3.0014400181244127e-05\n",
            "Epoch  9 Batch  103 / 447  Training Loss  2.542620677559171e-05\n",
            "Epoch  9 Batch  104 / 447  Training Loss  3.0187375159584917e-05\n",
            "Epoch  9 Batch  105 / 447  Training Loss  3.0184943170752376e-05\n",
            "Epoch  9 Batch  106 / 447  Training Loss  4.219247057335451e-05\n",
            "Epoch  9 Batch  107 / 447  Training Loss  2.1662172002834268e-05\n",
            "Epoch  9 Batch  108 / 447  Training Loss  3.4646509448066354e-05\n",
            "Epoch  9 Batch  109 / 447  Training Loss  3.160792766720988e-05\n",
            "Epoch  9 Batch  110 / 447  Training Loss  2.0125487935729325e-05\n",
            "Epoch  9 Batch  111 / 447  Training Loss  3.454261604929343e-05\n",
            "Epoch  9 Batch  112 / 447  Training Loss  2.8786247639800422e-05\n",
            "Epoch  9 Batch  113 / 447  Training Loss  2.9488224754459225e-05\n",
            "Epoch  9 Batch  114 / 447  Training Loss  2.6764806534629315e-05\n",
            "Epoch  9 Batch  115 / 447  Training Loss  2.7250691346125677e-05\n",
            "Epoch  9 Batch  116 / 447  Training Loss  2.5770585125428624e-05\n",
            "Epoch  9 Batch  117 / 447  Training Loss  3.352545536472462e-05\n",
            "Epoch  9 Batch  118 / 447  Training Loss  2.2020381948095746e-05\n",
            "Epoch  9 Batch  119 / 447  Training Loss  2.9834529414074495e-05\n",
            "Epoch  9 Batch  120 / 447  Training Loss  3.496537829050794e-05\n",
            "Epoch  9 Batch  121 / 447  Training Loss  2.238736669823993e-05\n",
            "Epoch  9 Batch  122 / 447  Training Loss  2.656962715263944e-05\n",
            "Epoch  9 Batch  123 / 447  Training Loss  3.708294752868824e-05\n",
            "Epoch  9 Batch  124 / 447  Training Loss  4.353535041445866e-05\n",
            "Epoch  9 Batch  125 / 447  Training Loss  3.477723657852039e-05\n",
            "Epoch  9 Batch  126 / 447  Training Loss  2.454011882946361e-05\n",
            "Epoch  9 Batch  127 / 447  Training Loss  2.2355501641868614e-05\n",
            "Epoch  9 Batch  128 / 447  Training Loss  3.8258822314674035e-05\n",
            "Epoch  9 Batch  129 / 447  Training Loss  1.82986659638118e-05\n",
            "Epoch  9 Batch  130 / 447  Training Loss  2.2877049559610896e-05\n",
            "Epoch  9 Batch  131 / 447  Training Loss  2.738388502621092e-05\n",
            "Epoch  9 Batch  132 / 447  Training Loss  3.502498657326214e-05\n",
            "Epoch  9 Batch  133 / 447  Training Loss  3.132534766336903e-05\n",
            "Epoch  9 Batch  134 / 447  Training Loss  1.6174406482605264e-05\n",
            "Epoch  9 Batch  135 / 447  Training Loss  3.285915227024816e-05\n",
            "Epoch  9 Batch  136 / 447  Training Loss  2.6352174245403148e-05\n",
            "Epoch  9 Batch  137 / 447  Training Loss  3.17533704219386e-05\n",
            "Epoch  9 Batch  138 / 447  Training Loss  2.6785699446918443e-05\n",
            "Epoch  9 Batch  139 / 447  Training Loss  2.9277811336214654e-05\n",
            "Epoch  9 Batch  140 / 447  Training Loss  2.2247364540817216e-05\n",
            "Epoch  9 Batch  141 / 447  Training Loss  2.005209717026446e-05\n",
            "Epoch  9 Batch  142 / 447  Training Loss  2.598775245132856e-05\n",
            "Epoch  9 Batch  143 / 447  Training Loss  3.059512528125197e-05\n",
            "Epoch  9 Batch  144 / 447  Training Loss  3.3527328923810273e-05\n",
            "Epoch  9 Batch  145 / 447  Training Loss  2.6875251933233812e-05\n",
            "Epoch  9 Batch  146 / 447  Training Loss  3.0155559215927497e-05\n",
            "Epoch  9 Batch  147 / 447  Training Loss  2.718387622735463e-05\n",
            "Epoch  9 Batch  148 / 447  Training Loss  2.9180337151046842e-05\n",
            "Epoch  9 Batch  149 / 447  Training Loss  4.140610690228641e-05\n",
            "Epoch  9 Batch  150 / 447  Training Loss  2.245120776933618e-05\n",
            "Epoch  9 Batch  151 / 447  Training Loss  5.511518247658387e-05\n",
            "Epoch  9 Batch  152 / 447  Training Loss  2.4633418433950283e-05\n",
            "Epoch  9 Batch  153 / 447  Training Loss  2.6948991944664158e-05\n",
            "Epoch  9 Batch  154 / 447  Training Loss  3.127643867628649e-05\n",
            "Epoch  9 Batch  155 / 447  Training Loss  3.1489020329900086e-05\n",
            "Epoch  9 Batch  156 / 447  Training Loss  2.3895756385172717e-05\n",
            "Epoch  9 Batch  157 / 447  Training Loss  3.7005793274147436e-05\n",
            "Epoch  9 Batch  158 / 447  Training Loss  2.0282566765672527e-05\n",
            "Epoch  9 Batch  159 / 447  Training Loss  2.536034480726812e-05\n",
            "Epoch  9 Batch  160 / 447  Training Loss  1.7966767700272612e-05\n",
            "Epoch  9 Batch  161 / 447  Training Loss  1.946235897776205e-05\n",
            "Epoch  9 Batch  162 / 447  Training Loss  2.9849365091649815e-05\n",
            "Epoch  9 Batch  163 / 447  Training Loss  3.0633782444056123e-05\n",
            "Epoch  9 Batch  164 / 447  Training Loss  2.3310398319154046e-05\n",
            "Epoch  9 Batch  165 / 447  Training Loss  3.1290390325011685e-05\n",
            "Epoch  9 Batch  166 / 447  Training Loss  2.8680153263849206e-05\n",
            "Epoch  9 Batch  167 / 447  Training Loss  2.8486765586421825e-05\n",
            "Epoch  9 Batch  168 / 447  Training Loss  2.8099000701331533e-05\n",
            "Epoch  9 Batch  169 / 447  Training Loss  2.581907028798014e-05\n",
            "Epoch  9 Batch  170 / 447  Training Loss  2.1524703697650693e-05\n",
            "Epoch  9 Batch  171 / 447  Training Loss  2.236848922620993e-05\n",
            "Epoch  9 Batch  172 / 447  Training Loss  2.4636241505504586e-05\n",
            "Epoch  9 Batch  173 / 447  Training Loss  3.243675746489316e-05\n",
            "Epoch  9 Batch  174 / 447  Training Loss  3.629239654401317e-05\n",
            "Epoch  9 Batch  175 / 447  Training Loss  3.911052772309631e-05\n",
            "Epoch  9 Batch  176 / 447  Training Loss  2.5215093046426773e-05\n",
            "Epoch  9 Batch  177 / 447  Training Loss  2.288078576384578e-05\n",
            "Epoch  9 Batch  178 / 447  Training Loss  3.56732307409402e-05\n",
            "Epoch  9 Batch  179 / 447  Training Loss  3.5058190405834466e-05\n",
            "Epoch  9 Batch  180 / 447  Training Loss  1.7801417925511487e-05\n",
            "Epoch  9 Batch  181 / 447  Training Loss  2.7508765924721956e-05\n",
            "Epoch  9 Batch  182 / 447  Training Loss  2.750793646555394e-05\n",
            "Epoch  9 Batch  183 / 447  Training Loss  2.787702214845922e-05\n",
            "Epoch  9 Batch  184 / 447  Training Loss  2.527196738810744e-05\n",
            "Epoch  9 Batch  185 / 447  Training Loss  3.135365841444582e-05\n",
            "Epoch  9 Batch  186 / 447  Training Loss  2.647968904057052e-05\n",
            "Epoch  9 Batch  187 / 447  Training Loss  2.614281402202323e-05\n",
            "Epoch  9 Batch  188 / 447  Training Loss  3.223983003408648e-05\n",
            "Epoch  9 Batch  189 / 447  Training Loss  1.8955721316160634e-05\n",
            "Epoch  9 Batch  190 / 447  Training Loss  2.534508894314058e-05\n",
            "Epoch  9 Batch  191 / 447  Training Loss  5.3208565077511594e-05\n",
            "Epoch  9 Batch  192 / 447  Training Loss  3.3940385037567466e-05\n",
            "Epoch  9 Batch  193 / 447  Training Loss  2.5955352612072602e-05\n",
            "Epoch  9 Batch  194 / 447  Training Loss  2.7295413019601256e-05\n",
            "Epoch  9 Batch  195 / 447  Training Loss  2.5000295863719657e-05\n",
            "Epoch  9 Batch  196 / 447  Training Loss  6.123475031927228e-05\n",
            "Epoch  9 Batch  197 / 447  Training Loss  2.755183777480852e-05\n",
            "Epoch  9 Batch  198 / 447  Training Loss  2.28962890105322e-05\n",
            "Epoch  9 Batch  199 / 447  Training Loss  3.083356932620518e-05\n",
            "Epoch  9 Batch  200 / 447  Training Loss  4.091925438842736e-05\n",
            "Epoch  9 Batch  201 / 447  Training Loss  2.8816151825594716e-05\n",
            "Epoch  9 Batch  202 / 447  Training Loss  3.19431783282198e-05\n",
            "Epoch  9 Batch  203 / 447  Training Loss  2.6605517632560804e-05\n",
            "Epoch  9 Batch  204 / 447  Training Loss  2.660620702954475e-05\n",
            "Epoch  9 Batch  205 / 447  Training Loss  3.05540525005199e-05\n",
            "Epoch  9 Batch  206 / 447  Training Loss  2.415286326140631e-05\n",
            "Epoch  9 Batch  207 / 447  Training Loss  2.7873547878698446e-05\n",
            "Epoch  9 Batch  208 / 447  Training Loss  2.6593923394102603e-05\n",
            "Epoch  9 Batch  209 / 447  Training Loss  3.116613515885547e-05\n",
            "Epoch  9 Batch  210 / 447  Training Loss  4.960113074048422e-05\n",
            "Epoch  9 Batch  211 / 447  Training Loss  3.6313009331934154e-05\n",
            "Epoch  9 Batch  212 / 447  Training Loss  2.3354161385213956e-05\n",
            "Epoch  9 Batch  213 / 447  Training Loss  2.3869686629041098e-05\n",
            "Epoch  9 Batch  214 / 447  Training Loss  2.879546445910819e-05\n",
            "Epoch  9 Batch  215 / 447  Training Loss  2.7367112124920823e-05\n",
            "Epoch  9 Batch  216 / 447  Training Loss  2.905428846133873e-05\n",
            "Epoch  9 Batch  217 / 447  Training Loss  2.710863554966636e-05\n",
            "Epoch  9 Batch  218 / 447  Training Loss  2.2645324861514382e-05\n",
            "Epoch  9 Batch  219 / 447  Training Loss  2.74857557087671e-05\n",
            "Epoch  9 Batch  220 / 447  Training Loss  3.176980681018904e-05\n",
            "Epoch  9 Batch  221 / 447  Training Loss  2.0002898963866755e-05\n",
            "Epoch  9 Batch  222 / 447  Training Loss  2.357089215365704e-05\n",
            "Epoch  9 Batch  223 / 447  Training Loss  2.2689086108584888e-05\n",
            "Epoch  9 Batch  224 / 447  Training Loss  9.227227565133944e-05\n",
            "Epoch  9 Batch  225 / 447  Training Loss  3.283746991655789e-05\n",
            "Epoch  9 Batch  226 / 447  Training Loss  2.6359215553384274e-05\n",
            "Epoch  9 Batch  227 / 447  Training Loss  3.532571645337157e-05\n",
            "Epoch  9 Batch  228 / 447  Training Loss  2.7210169719182886e-05\n",
            "Epoch  9 Batch  229 / 447  Training Loss  1.835877810663078e-05\n",
            "Epoch  9 Batch  230 / 447  Training Loss  3.264705810579471e-05\n",
            "Epoch  9 Batch  231 / 447  Training Loss  2.9263801479828544e-05\n",
            "Epoch  9 Batch  232 / 447  Training Loss  1.842533310991712e-05\n",
            "Epoch  9 Batch  233 / 447  Training Loss  3.556153751560487e-05\n",
            "Epoch  9 Batch  234 / 447  Training Loss  2.649381713126786e-05\n",
            "Epoch  9 Batch  235 / 447  Training Loss  2.605129702715203e-05\n",
            "Epoch  9 Batch  236 / 447  Training Loss  3.154014120809734e-05\n",
            "Epoch  9 Batch  237 / 447  Training Loss  4.434895163285546e-05\n",
            "Epoch  9 Batch  238 / 447  Training Loss  2.89029412670061e-05\n",
            "Epoch  9 Batch  239 / 447  Training Loss  3.106336953351274e-05\n",
            "Epoch  9 Batch  240 / 447  Training Loss  2.855215461750049e-05\n",
            "Epoch  9 Batch  241 / 447  Training Loss  2.5256096705561504e-05\n",
            "Epoch  9 Batch  242 / 447  Training Loss  3.7377772969193757e-05\n",
            "Epoch  9 Batch  243 / 447  Training Loss  3.795524753513746e-05\n",
            "Epoch  9 Batch  244 / 447  Training Loss  3.2842406653799117e-05\n",
            "Epoch  9 Batch  245 / 447  Training Loss  2.0695701095974073e-05\n",
            "Epoch  9 Batch  246 / 447  Training Loss  2.4097766072372906e-05\n",
            "Epoch  9 Batch  247 / 447  Training Loss  2.0547571693896316e-05\n",
            "Epoch  9 Batch  248 / 447  Training Loss  3.1230134482029825e-05\n",
            "Epoch  9 Batch  249 / 447  Training Loss  3.6921621358487755e-05\n",
            "Epoch  9 Batch  250 / 447  Training Loss  2.9485496270353906e-05\n",
            "Epoch  9 Batch  251 / 447  Training Loss  2.40362041949993e-05\n",
            "Epoch  9 Batch  252 / 447  Training Loss  3.427336196182296e-05\n",
            "Epoch  9 Batch  253 / 447  Training Loss  2.564736860222183e-05\n",
            "Epoch  9 Batch  254 / 447  Training Loss  2.7538420908967964e-05\n",
            "Epoch  9 Batch  255 / 447  Training Loss  1.8397269741399214e-05\n",
            "Epoch  9 Batch  256 / 447  Training Loss  3.222060695406981e-05\n",
            "Epoch  9 Batch  257 / 447  Training Loss  2.187912105000578e-05\n",
            "Epoch  9 Batch  258 / 447  Training Loss  2.4841448976076208e-05\n",
            "Epoch  9 Batch  259 / 447  Training Loss  2.6544486900093034e-05\n",
            "Epoch  9 Batch  260 / 447  Training Loss  3.4324802982155234e-05\n",
            "Epoch  9 Batch  261 / 447  Training Loss  3.534202551236376e-05\n",
            "Epoch  9 Batch  262 / 447  Training Loss  2.868884985218756e-05\n",
            "Epoch  9 Batch  263 / 447  Training Loss  1.9083225197391585e-05\n",
            "Epoch  9 Batch  264 / 447  Training Loss  2.8933287467225455e-05\n",
            "Epoch  9 Batch  265 / 447  Training Loss  2.5629897209000774e-05\n",
            "Epoch  9 Batch  266 / 447  Training Loss  4.484218152356334e-05\n",
            "Epoch  9 Batch  267 / 447  Training Loss  2.8610735171241686e-05\n",
            "Epoch  9 Batch  268 / 447  Training Loss  2.6894691472989507e-05\n",
            "Epoch  9 Batch  269 / 447  Training Loss  1.9933628209400922e-05\n",
            "Epoch  9 Batch  270 / 447  Training Loss  3.038962859136518e-05\n",
            "Epoch  9 Batch  271 / 447  Training Loss  1.687459553068038e-05\n",
            "Epoch  9 Batch  272 / 447  Training Loss  2.6273823095834814e-05\n",
            "Epoch  9 Batch  273 / 447  Training Loss  2.692516500246711e-05\n",
            "Epoch  9 Batch  274 / 447  Training Loss  3.221736551495269e-05\n",
            "Epoch  9 Batch  275 / 447  Training Loss  1.9324243112350814e-05\n",
            "Epoch  9 Batch  276 / 447  Training Loss  2.785087053780444e-05\n",
            "Epoch  9 Batch  277 / 447  Training Loss  2.133951238647569e-05\n",
            "Epoch  9 Batch  278 / 447  Training Loss  4.346114656073041e-05\n",
            "Epoch  9 Batch  279 / 447  Training Loss  2.3858230633777566e-05\n",
            "Epoch  9 Batch  280 / 447  Training Loss  2.4320675947819836e-05\n",
            "Epoch  9 Batch  281 / 447  Training Loss  2.045944711426273e-05\n",
            "Epoch  9 Batch  282 / 447  Training Loss  8.905041613616049e-05\n",
            "Epoch  9 Batch  283 / 447  Training Loss  3.5017914342461154e-05\n",
            "Epoch  9 Batch  284 / 447  Training Loss  6.764900899725035e-05\n",
            "Epoch  9 Batch  285 / 447  Training Loss  2.537974614824634e-05\n",
            "Epoch  9 Batch  286 / 447  Training Loss  2.4248005502158776e-05\n",
            "Epoch  9 Batch  287 / 447  Training Loss  2.9507235012715682e-05\n",
            "Epoch  9 Batch  288 / 447  Training Loss  2.7155867428518832e-05\n",
            "Epoch  9 Batch  289 / 447  Training Loss  3.150590055156499e-05\n",
            "Epoch  9 Batch  290 / 447  Training Loss  5.39038228453137e-05\n",
            "Epoch  9 Batch  291 / 447  Training Loss  5.0470665883040056e-05\n",
            "Epoch  9 Batch  292 / 447  Training Loss  2.0525974832708016e-05\n",
            "Epoch  9 Batch  293 / 447  Training Loss  2.5688368623377755e-05\n",
            "Epoch  9 Batch  294 / 447  Training Loss  2.1727501007262617e-05\n",
            "Epoch  9 Batch  295 / 447  Training Loss  4.027987961308099e-05\n",
            "Epoch  9 Batch  296 / 447  Training Loss  3.168617331539281e-05\n",
            "Epoch  9 Batch  297 / 447  Training Loss  2.2325759346131235e-05\n",
            "Epoch  9 Batch  298 / 447  Training Loss  2.7879281333298422e-05\n",
            "Epoch  9 Batch  299 / 447  Training Loss  5.42931848031003e-05\n",
            "Epoch  9 Batch  300 / 447  Training Loss  4.3749725591624156e-05\n",
            "Epoch  9 Batch  301 / 447  Training Loss  3.1046864023664966e-05\n",
            "Epoch  9 Batch  302 / 447  Training Loss  2.3093678464647382e-05\n",
            "Epoch  9 Batch  303 / 447  Training Loss  2.332883741473779e-05\n",
            "Epoch  9 Batch  304 / 447  Training Loss  1.6628393495921046e-05\n",
            "Epoch  9 Batch  305 / 447  Training Loss  2.153177592845168e-05\n",
            "Epoch  9 Batch  306 / 447  Training Loss  1.737381171551533e-05\n",
            "Epoch  9 Batch  307 / 447  Training Loss  2.8716140150208957e-05\n",
            "Epoch  9 Batch  308 / 447  Training Loss  2.8663860575761646e-05\n",
            "Epoch  9 Batch  309 / 447  Training Loss  3.562520578270778e-05\n",
            "Epoch  9 Batch  310 / 447  Training Loss  2.0920808310620487e-05\n",
            "Epoch  9 Batch  311 / 447  Training Loss  2.6465317205293104e-05\n",
            "Epoch  9 Batch  312 / 447  Training Loss  1.9363880710443482e-05\n",
            "Epoch  9 Batch  313 / 447  Training Loss  3.627584737841971e-05\n",
            "Epoch  9 Batch  314 / 447  Training Loss  3.039303919649683e-05\n",
            "Epoch  9 Batch  315 / 447  Training Loss  2.0658182620536536e-05\n",
            "Epoch  9 Batch  316 / 447  Training Loss  2.0060057067894377e-05\n",
            "Epoch  9 Batch  317 / 447  Training Loss  2.3548460376332514e-05\n",
            "Epoch  9 Batch  318 / 447  Training Loss  1.6677926396369003e-05\n",
            "Epoch  9 Batch  319 / 447  Training Loss  2.721184318943415e-05\n",
            "Epoch  9 Batch  320 / 447  Training Loss  3.500971070025116e-05\n",
            "Epoch  9 Batch  321 / 447  Training Loss  2.9403505322989076e-05\n",
            "Epoch  9 Batch  322 / 447  Training Loss  2.7675758246914484e-05\n",
            "Epoch  9 Batch  323 / 447  Training Loss  3.4604840038809925e-05\n",
            "Epoch  9 Batch  324 / 447  Training Loss  2.4567782020312734e-05\n",
            "Epoch  9 Batch  325 / 447  Training Loss  2.490412225597538e-05\n",
            "Epoch  9 Batch  326 / 447  Training Loss  2.5927913156920113e-05\n",
            "Epoch  9 Batch  327 / 447  Training Loss  2.2688480385113508e-05\n",
            "Epoch  9 Batch  328 / 447  Training Loss  2.1634310542140156e-05\n",
            "Epoch  9 Batch  329 / 447  Training Loss  3.1850519008003175e-05\n",
            "Epoch  9 Batch  330 / 447  Training Loss  3.0474011509795673e-05\n",
            "Epoch  9 Batch  331 / 447  Training Loss  2.4247252440545708e-05\n",
            "Epoch  9 Batch  332 / 447  Training Loss  2.107657928718254e-05\n",
            "Epoch  9 Batch  333 / 447  Training Loss  2.3581995264976285e-05\n",
            "Epoch  9 Batch  334 / 447  Training Loss  1.610707477084361e-05\n",
            "Epoch  9 Batch  335 / 447  Training Loss  1.642012102820445e-05\n",
            "Epoch  9 Batch  336 / 447  Training Loss  2.6907937353826128e-05\n",
            "Epoch  9 Batch  337 / 447  Training Loss  2.5120320060523227e-05\n",
            "Epoch  9 Batch  338 / 447  Training Loss  2.4141707399394363e-05\n",
            "Epoch  9 Batch  339 / 447  Training Loss  2.7036214305553585e-05\n",
            "Epoch  9 Batch  340 / 447  Training Loss  2.798753121169284e-05\n",
            "Epoch  9 Batch  341 / 447  Training Loss  2.1916963305557147e-05\n",
            "Epoch  9 Batch  342 / 447  Training Loss  2.3572109057568014e-05\n",
            "Epoch  9 Batch  343 / 447  Training Loss  2.767393561953213e-05\n",
            "Epoch  9 Batch  344 / 447  Training Loss  2.355441938561853e-05\n",
            "Epoch  9 Batch  345 / 447  Training Loss  4.304222602513619e-05\n",
            "Epoch  9 Batch  346 / 447  Training Loss  3.13294367515482e-05\n",
            "Epoch  9 Batch  347 / 447  Training Loss  2.1955553165753372e-05\n",
            "Epoch  9 Batch  348 / 447  Training Loss  2.70719774562167e-05\n",
            "Epoch  9 Batch  349 / 447  Training Loss  2.1277483028825372e-05\n",
            "Epoch  9 Batch  350 / 447  Training Loss  2.4429340555798262e-05\n",
            "Epoch  9 Batch  351 / 447  Training Loss  2.2019215975888073e-05\n",
            "Epoch  9 Batch  352 / 447  Training Loss  3.071490937145427e-05\n",
            "Epoch  9 Batch  353 / 447  Training Loss  2.199291884608101e-05\n",
            "Epoch  9 Batch  354 / 447  Training Loss  2.4810671675368212e-05\n",
            "Epoch  9 Batch  355 / 447  Training Loss  1.8207874745712616e-05\n",
            "Epoch  9 Batch  356 / 447  Training Loss  2.351217517571058e-05\n",
            "Epoch  9 Batch  357 / 447  Training Loss  2.7738195058191195e-05\n",
            "Epoch  9 Batch  358 / 447  Training Loss  1.8721173546509817e-05\n",
            "Epoch  9 Batch  359 / 447  Training Loss  2.7214551664656028e-05\n",
            "Epoch  9 Batch  360 / 447  Training Loss  2.7811569452751428e-05\n",
            "Epoch  9 Batch  361 / 447  Training Loss  2.1114086848683655e-05\n",
            "Epoch  9 Batch  362 / 447  Training Loss  2.8878681405331008e-05\n",
            "Epoch  9 Batch  363 / 447  Training Loss  3.007810482813511e-05\n",
            "Epoch  9 Batch  364 / 447  Training Loss  2.1933577954769135e-05\n",
            "Epoch  9 Batch  365 / 447  Training Loss  1.6339383364538662e-05\n",
            "Epoch  9 Batch  366 / 447  Training Loss  3.33306124957744e-05\n",
            "Epoch  9 Batch  367 / 447  Training Loss  4.0659699152456596e-05\n",
            "Epoch  9 Batch  368 / 447  Training Loss  2.25858784688171e-05\n",
            "Epoch  9 Batch  369 / 447  Training Loss  2.7217869501328096e-05\n",
            "Epoch  9 Batch  370 / 447  Training Loss  3.3318901841994375e-05\n",
            "Epoch  9 Batch  371 / 447  Training Loss  4.3746586015913635e-05\n",
            "Epoch  9 Batch  372 / 447  Training Loss  3.454269972280599e-05\n",
            "Epoch  9 Batch  373 / 447  Training Loss  2.2655818611383438e-05\n",
            "Epoch  9 Batch  374 / 447  Training Loss  2.2923792130313814e-05\n",
            "Epoch  9 Batch  375 / 447  Training Loss  1.755282391968649e-05\n",
            "Epoch  9 Batch  376 / 447  Training Loss  1.6448138921987265e-05\n",
            "Epoch  9 Batch  377 / 447  Training Loss  1.9238250388298184e-05\n",
            "Epoch  9 Batch  378 / 447  Training Loss  2.202149698860012e-05\n",
            "Epoch  9 Batch  379 / 447  Training Loss  3.172566357534379e-05\n",
            "Epoch  9 Batch  380 / 447  Training Loss  2.4068236598395742e-05\n",
            "Epoch  9 Batch  381 / 447  Training Loss  2.6564521249383688e-05\n",
            "Epoch  9 Batch  382 / 447  Training Loss  2.107649197569117e-05\n",
            "Epoch  9 Batch  383 / 447  Training Loss  4.029133197036572e-05\n",
            "Epoch  9 Batch  384 / 447  Training Loss  3.3555897971382365e-05\n",
            "Epoch  9 Batch  385 / 447  Training Loss  2.072876122838352e-05\n",
            "Epoch  9 Batch  386 / 447  Training Loss  1.698360574664548e-05\n",
            "Epoch  9 Batch  387 / 447  Training Loss  1.77559231815394e-05\n",
            "Epoch  9 Batch  388 / 447  Training Loss  3.33537464030087e-05\n",
            "Epoch  9 Batch  389 / 447  Training Loss  2.443668017804157e-05\n",
            "Epoch  9 Batch  390 / 447  Training Loss  2.906127156165894e-05\n",
            "Epoch  9 Batch  391 / 447  Training Loss  3.035767440451309e-05\n",
            "Epoch  9 Batch  392 / 447  Training Loss  2.322698856005445e-05\n",
            "Epoch  9 Batch  393 / 447  Training Loss  4.274025195627473e-05\n",
            "Epoch  9 Batch  394 / 447  Training Loss  3.234118048567325e-05\n",
            "Epoch  9 Batch  395 / 447  Training Loss  2.1945044863969088e-05\n",
            "Epoch  9 Batch  396 / 447  Training Loss  2.7431324269855395e-05\n",
            "Epoch  9 Batch  397 / 447  Training Loss  3.905562698491849e-05\n",
            "Epoch  9 Batch  398 / 447  Training Loss  1.883526965684723e-05\n",
            "Epoch  9 Batch  399 / 447  Training Loss  2.9888920835219324e-05\n",
            "Epoch  9 Batch  400 / 447  Training Loss  3.4778728149831295e-05\n",
            "Epoch  9 Batch  401 / 447  Training Loss  1.8514565454097465e-05\n",
            "Epoch  9 Batch  402 / 447  Training Loss  2.3829827114241198e-05\n",
            "Epoch  9 Batch  403 / 447  Training Loss  3.626726902439259e-05\n",
            "Epoch  9 Batch  404 / 447  Training Loss  3.9472870412282646e-05\n",
            "Epoch  9 Batch  405 / 447  Training Loss  2.1409470718936063e-05\n",
            "Epoch  9 Batch  406 / 447  Training Loss  3.155236117891036e-05\n",
            "Epoch  9 Batch  407 / 447  Training Loss  2.7202675482840277e-05\n",
            "Epoch  9 Batch  408 / 447  Training Loss  3.482771717244759e-05\n",
            "Epoch  9 Batch  409 / 447  Training Loss  2.6208828785456717e-05\n",
            "Epoch  9 Batch  410 / 447  Training Loss  1.587694714544341e-05\n",
            "Epoch  9 Batch  411 / 447  Training Loss  1.8994853235199116e-05\n",
            "Epoch  9 Batch  412 / 447  Training Loss  3.069477679673582e-05\n",
            "Epoch  9 Batch  413 / 447  Training Loss  2.4166982257156633e-05\n",
            "Epoch  9 Batch  414 / 447  Training Loss  2.602100357762538e-05\n",
            "Epoch  9 Batch  415 / 447  Training Loss  1.7737349480739795e-05\n",
            "Epoch  9 Batch  416 / 447  Training Loss  1.9341519873705693e-05\n",
            "Epoch  9 Batch  417 / 447  Training Loss  2.1642947103828192e-05\n",
            "Epoch  9 Batch  418 / 447  Training Loss  1.742700260365382e-05\n",
            "Epoch  9 Batch  419 / 447  Training Loss  2.3116483134799637e-05\n",
            "Epoch  9 Batch  420 / 447  Training Loss  2.1094690964673646e-05\n",
            "Epoch  9 Batch  421 / 447  Training Loss  2.814284925989341e-05\n",
            "Epoch  9 Batch  422 / 447  Training Loss  2.8262989872018807e-05\n",
            "Epoch  9 Batch  423 / 447  Training Loss  1.9072858776780777e-05\n",
            "Epoch  9 Batch  424 / 447  Training Loss  1.790949136193376e-05\n",
            "Epoch  9 Batch  425 / 447  Training Loss  2.0320518160588108e-05\n",
            "Epoch  9 Batch  426 / 447  Training Loss  2.2740628992323764e-05\n",
            "Epoch  9 Batch  427 / 447  Training Loss  2.6615089154802263e-05\n",
            "Epoch  9 Batch  428 / 447  Training Loss  2.081788807117846e-05\n",
            "Epoch  9 Batch  429 / 447  Training Loss  2.1531177480937913e-05\n",
            "Epoch  9 Batch  430 / 447  Training Loss  2.6472833269508556e-05\n",
            "Epoch  9 Batch  431 / 447  Training Loss  2.6983010684489273e-05\n",
            "Epoch  9 Batch  432 / 447  Training Loss  2.626428431540262e-05\n",
            "Epoch  9 Batch  433 / 447  Training Loss  1.6394838894484565e-05\n",
            "Epoch  9 Batch  434 / 447  Training Loss  2.0648603822337463e-05\n",
            "Epoch  9 Batch  435 / 447  Training Loss  3.294909038231708e-05\n",
            "Epoch  9 Batch  436 / 447  Training Loss  1.6188621884793974e-05\n",
            "Epoch  9 Batch  437 / 447  Training Loss  2.1691210349672474e-05\n",
            "Epoch  9 Batch  438 / 447  Training Loss  2.0906731151626445e-05\n",
            "Epoch  9 Batch  439 / 447  Training Loss  1.79728667717427e-05\n",
            "Epoch  9 Batch  440 / 447  Training Loss  2.7694886739482172e-05\n",
            "Epoch  9 Batch  441 / 447  Training Loss  2.0031435269629583e-05\n",
            "Epoch  9 Batch  442 / 447  Training Loss  1.908808008010965e-05\n",
            "Epoch  9 Batch  443 / 447  Training Loss  3.270512752351351e-05\n",
            "Epoch  9 Batch  444 / 447  Training Loss  2.9622437068610452e-05\n",
            "Epoch  9 Batch  445 / 447  Training Loss  2.222535476903431e-05\n",
            "Epoch  9 Batch  446 / 447  Training Loss  1.8746057321550325e-05\n",
            "  10    |    -    |   0.000030   | 99.960815\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 10\n",
            "Epoch  10 Batch  0 / 447  Training Loss  2.420909731881693e-05\n",
            "Epoch  10 Batch  1 / 447  Training Loss  1.5831441487534903e-05\n",
            "Epoch  10 Batch  2 / 447  Training Loss  1.9152837921865284e-05\n",
            "Epoch  10 Batch  3 / 447  Training Loss  2.444888377794996e-05\n",
            "Epoch  10 Batch  4 / 447  Training Loss  2.8962798751308583e-05\n",
            "Epoch  10 Batch  5 / 447  Training Loss  2.3041302483761683e-05\n",
            "Epoch  10 Batch  6 / 447  Training Loss  1.6576756024733186e-05\n",
            "Epoch  10 Batch  7 / 447  Training Loss  1.640179289097432e-05\n",
            "Epoch  10 Batch  8 / 447  Training Loss  1.9592014723457396e-05\n",
            "Epoch  10 Batch  9 / 447  Training Loss  2.353548916289583e-05\n",
            "Epoch  10 Batch  10 / 447  Training Loss  2.03212330234237e-05\n",
            "Epoch  10 Batch  11 / 447  Training Loss  1.8873326553148217e-05\n",
            "Epoch  10 Batch  12 / 447  Training Loss  1.9027094822376966e-05\n",
            "Epoch  10 Batch  13 / 447  Training Loss  2.5130568246822804e-05\n",
            "Epoch  10 Batch  14 / 447  Training Loss  2.235799729533028e-05\n",
            "Epoch  10 Batch  15 / 447  Training Loss  1.919551141327247e-05\n",
            "Epoch  10 Batch  16 / 447  Training Loss  2.7394844437367283e-05\n",
            "Epoch  10 Batch  17 / 447  Training Loss  2.4064436729531735e-05\n",
            "Epoch  10 Batch  18 / 447  Training Loss  2.696097180887591e-05\n",
            "Epoch  10 Batch  19 / 447  Training Loss  2.2723854272044264e-05\n",
            "Epoch  10 Batch  20 / 447  Training Loss  1.560024611535482e-05\n",
            "Epoch  10 Batch  21 / 447  Training Loss  2.3654198230360635e-05\n",
            "Epoch  10 Batch  22 / 447  Training Loss  1.830050314310938e-05\n",
            "Epoch  10 Batch  23 / 447  Training Loss  1.7545726223033853e-05\n",
            "Epoch  10 Batch  24 / 447  Training Loss  2.1152089175302535e-05\n",
            "Epoch  10 Batch  25 / 447  Training Loss  2.4252252842416056e-05\n",
            "Epoch  10 Batch  26 / 447  Training Loss  1.668835466261953e-05\n",
            "Epoch  10 Batch  27 / 447  Training Loss  2.865940041374415e-05\n",
            "Epoch  10 Batch  28 / 447  Training Loss  2.1034105884609744e-05\n",
            "Epoch  10 Batch  29 / 447  Training Loss  2.4889623091439717e-05\n",
            "Epoch  10 Batch  30 / 447  Training Loss  2.1909410861553624e-05\n",
            "Epoch  10 Batch  31 / 447  Training Loss  2.6185296519543044e-05\n",
            "Epoch  10 Batch  32 / 447  Training Loss  2.5853421902866103e-05\n",
            "Epoch  10 Batch  33 / 447  Training Loss  2.0982501155231148e-05\n",
            "Epoch  10 Batch  34 / 447  Training Loss  2.6829093258129433e-05\n",
            "Epoch  10 Batch  35 / 447  Training Loss  2.8969003324164078e-05\n",
            "Epoch  10 Batch  36 / 447  Training Loss  1.4616292901337147e-05\n",
            "Epoch  10 Batch  37 / 447  Training Loss  2.4287868654937483e-05\n",
            "Epoch  10 Batch  38 / 447  Training Loss  2.2399974113795906e-05\n",
            "Epoch  10 Batch  39 / 447  Training Loss  2.504042276996188e-05\n",
            "Epoch  10 Batch  40 / 447  Training Loss  2.3762768250890076e-05\n",
            "Epoch  10 Batch  41 / 447  Training Loss  2.02219180209795e-05\n",
            "Epoch  10 Batch  42 / 447  Training Loss  2.4448916519759223e-05\n",
            "Epoch  10 Batch  43 / 447  Training Loss  1.866202183009591e-05\n",
            "Epoch  10 Batch  44 / 447  Training Loss  1.7763923096936196e-05\n",
            "Epoch  10 Batch  45 / 447  Training Loss  1.562329998705536e-05\n",
            "Epoch  10 Batch  46 / 447  Training Loss  3.336431836942211e-05\n",
            "Epoch  10 Batch  47 / 447  Training Loss  1.7394280803273432e-05\n",
            "Epoch  10 Batch  48 / 447  Training Loss  2.0565001250361092e-05\n",
            "Epoch  10 Batch  49 / 447  Training Loss  2.2713586076861247e-05\n",
            "Epoch  10 Batch  50 / 447  Training Loss  2.6077548682224005e-05\n",
            "Epoch  10 Batch  51 / 447  Training Loss  1.977231295313686e-05\n",
            "Epoch  10 Batch  52 / 447  Training Loss  2.17260039789835e-05\n",
            "Epoch  10 Batch  53 / 447  Training Loss  3.004199970746413e-05\n",
            "Epoch  10 Batch  54 / 447  Training Loss  2.2104906747699715e-05\n",
            "Epoch  10 Batch  55 / 447  Training Loss  2.4497821868862957e-05\n",
            "Epoch  10 Batch  56 / 447  Training Loss  3.177818871336058e-05\n",
            "Epoch  10 Batch  57 / 447  Training Loss  2.2533402443514206e-05\n",
            "Epoch  10 Batch  58 / 447  Training Loss  4.204122524242848e-05\n",
            "Epoch  10 Batch  59 / 447  Training Loss  1.795342905097641e-05\n",
            "Epoch  10 Batch  60 / 447  Training Loss  2.2048237951821648e-05\n",
            "Epoch  10 Batch  61 / 447  Training Loss  1.3900126759835985e-05\n",
            "Epoch  10 Batch  62 / 447  Training Loss  2.1843599824933335e-05\n",
            "Epoch  10 Batch  63 / 447  Training Loss  2.37981257669162e-05\n",
            "Epoch  10 Batch  64 / 447  Training Loss  2.3267590222530998e-05\n",
            "Epoch  10 Batch  65 / 447  Training Loss  2.009264972002711e-05\n",
            "Epoch  10 Batch  66 / 447  Training Loss  3.019672294612974e-05\n",
            "Epoch  10 Batch  67 / 447  Training Loss  2.6887995772995055e-05\n",
            "Epoch  10 Batch  68 / 447  Training Loss  2.78703955700621e-05\n",
            "Epoch  10 Batch  69 / 447  Training Loss  2.749608756857924e-05\n",
            "Epoch  10 Batch  70 / 447  Training Loss  5.0662903959164396e-05\n",
            "Epoch  10 Batch  71 / 447  Training Loss  2.156197115255054e-05\n",
            "Epoch  10 Batch  72 / 447  Training Loss  1.866167258413043e-05\n",
            "Epoch  10 Batch  73 / 447  Training Loss  2.0867271814495325e-05\n",
            "Epoch  10 Batch  74 / 447  Training Loss  2.6056955903186463e-05\n",
            "Epoch  10 Batch  75 / 447  Training Loss  2.1335517885745503e-05\n",
            "Epoch  10 Batch  76 / 447  Training Loss  2.6949017410515808e-05\n",
            "Epoch  10 Batch  77 / 447  Training Loss  2.4357486836379394e-05\n",
            "Epoch  10 Batch  78 / 447  Training Loss  2.3741966288071126e-05\n",
            "Epoch  10 Batch  79 / 447  Training Loss  2.9471711968653835e-05\n",
            "Epoch  10 Batch  80 / 447  Training Loss  3.734714846359566e-05\n",
            "Epoch  10 Batch  81 / 447  Training Loss  1.885230994957965e-05\n",
            "Epoch  10 Batch  82 / 447  Training Loss  1.6473351934109814e-05\n",
            "Epoch  10 Batch  83 / 447  Training Loss  2.3153193978942e-05\n",
            "Epoch  10 Batch  84 / 447  Training Loss  2.799305548251141e-05\n",
            "Epoch  10 Batch  85 / 447  Training Loss  1.8756891222437844e-05\n",
            "Epoch  10 Batch  86 / 447  Training Loss  1.9588829673011787e-05\n",
            "Epoch  10 Batch  87 / 447  Training Loss  1.6040105037973262e-05\n",
            "Epoch  10 Batch  88 / 447  Training Loss  2.1950598238618113e-05\n",
            "Epoch  10 Batch  89 / 447  Training Loss  2.70533564616926e-05\n",
            "Epoch  10 Batch  90 / 447  Training Loss  2.1358320736908354e-05\n",
            "Epoch  10 Batch  91 / 447  Training Loss  3.2227591873379424e-05\n",
            "Epoch  10 Batch  92 / 447  Training Loss  2.2571208319277503e-05\n",
            "Epoch  10 Batch  93 / 447  Training Loss  2.5907498638844118e-05\n",
            "Epoch  10 Batch  94 / 447  Training Loss  1.92315892491024e-05\n",
            "Epoch  10 Batch  95 / 447  Training Loss  2.005435999308247e-05\n",
            "Epoch  10 Batch  96 / 447  Training Loss  2.1493338863365352e-05\n",
            "Epoch  10 Batch  97 / 447  Training Loss  1.6191161194001324e-05\n",
            "Epoch  10 Batch  98 / 447  Training Loss  1.9078659533988684e-05\n",
            "Epoch  10 Batch  99 / 447  Training Loss  2.5039696993189864e-05\n",
            "Epoch  10 Batch  100 / 447  Training Loss  2.693502392503433e-05\n",
            "Epoch  10 Batch  101 / 447  Training Loss  2.0314389985287562e-05\n",
            "Epoch  10 Batch  102 / 447  Training Loss  1.877813701867126e-05\n",
            "Epoch  10 Batch  103 / 447  Training Loss  2.3476175556425005e-05\n",
            "Epoch  10 Batch  104 / 447  Training Loss  2.902032429119572e-05\n",
            "Epoch  10 Batch  105 / 447  Training Loss  2.041762854787521e-05\n",
            "Epoch  10 Batch  106 / 447  Training Loss  2.4189466785173863e-05\n",
            "Epoch  10 Batch  107 / 447  Training Loss  2.041360676230397e-05\n",
            "Epoch  10 Batch  108 / 447  Training Loss  1.6477517419843934e-05\n",
            "Epoch  10 Batch  109 / 447  Training Loss  1.9944771338487044e-05\n",
            "Epoch  10 Batch  110 / 447  Training Loss  1.6442019841633737e-05\n",
            "Epoch  10 Batch  111 / 447  Training Loss  1.3633952221425716e-05\n",
            "Epoch  10 Batch  112 / 447  Training Loss  1.8692742742132396e-05\n",
            "Epoch  10 Batch  113 / 447  Training Loss  1.8625210941536352e-05\n",
            "Epoch  10 Batch  114 / 447  Training Loss  2.8158759960206226e-05\n",
            "Epoch  10 Batch  115 / 447  Training Loss  1.868019717221614e-05\n",
            "Epoch  10 Batch  116 / 447  Training Loss  2.1611873307847418e-05\n",
            "Epoch  10 Batch  117 / 447  Training Loss  2.6141891794395633e-05\n",
            "Epoch  10 Batch  118 / 447  Training Loss  1.7635111362324096e-05\n",
            "Epoch  10 Batch  119 / 447  Training Loss  2.4899152776924893e-05\n",
            "Epoch  10 Batch  120 / 447  Training Loss  2.2249314497457817e-05\n",
            "Epoch  10 Batch  121 / 447  Training Loss  1.654697916819714e-05\n",
            "Epoch  10 Batch  122 / 447  Training Loss  2.4762035536696203e-05\n",
            "Epoch  10 Batch  123 / 447  Training Loss  2.3058000806486234e-05\n",
            "Epoch  10 Batch  124 / 447  Training Loss  2.2578893549507484e-05\n",
            "Epoch  10 Batch  125 / 447  Training Loss  1.4180395737639628e-05\n",
            "Epoch  10 Batch  126 / 447  Training Loss  1.950516161741689e-05\n",
            "Epoch  10 Batch  127 / 447  Training Loss  2.488119571353309e-05\n",
            "Epoch  10 Batch  128 / 447  Training Loss  2.126064100593794e-05\n",
            "Epoch  10 Batch  129 / 447  Training Loss  2.5360064682899974e-05\n",
            "Epoch  10 Batch  130 / 447  Training Loss  1.6133963072206825e-05\n",
            "Epoch  10 Batch  131 / 447  Training Loss  1.895717650768347e-05\n",
            "Epoch  10 Batch  132 / 447  Training Loss  2.1590840333374217e-05\n",
            "Epoch  10 Batch  133 / 447  Training Loss  1.6130417861859314e-05\n",
            "Epoch  10 Batch  134 / 447  Training Loss  1.876624082797207e-05\n",
            "Epoch  10 Batch  135 / 447  Training Loss  2.3144215447246097e-05\n",
            "Epoch  10 Batch  136 / 447  Training Loss  2.3759370378684253e-05\n",
            "Epoch  10 Batch  137 / 447  Training Loss  1.5040582184155937e-05\n",
            "Epoch  10 Batch  138 / 447  Training Loss  2.4206423404393718e-05\n",
            "Epoch  10 Batch  139 / 447  Training Loss  1.4654418919235468e-05\n",
            "Epoch  10 Batch  140 / 447  Training Loss  1.880812669696752e-05\n",
            "Epoch  10 Batch  141 / 447  Training Loss  1.86420620593708e-05\n",
            "Epoch  10 Batch  142 / 447  Training Loss  1.876906208053697e-05\n",
            "Epoch  10 Batch  143 / 447  Training Loss  2.3406691980198957e-05\n",
            "Epoch  10 Batch  144 / 447  Training Loss  1.9455656001809984e-05\n",
            "Epoch  10 Batch  145 / 447  Training Loss  2.2707306925440207e-05\n",
            "Epoch  10 Batch  146 / 447  Training Loss  1.84934488061117e-05\n",
            "Epoch  10 Batch  147 / 447  Training Loss  2.5885530703817494e-05\n",
            "Epoch  10 Batch  148 / 447  Training Loss  1.805524698283989e-05\n",
            "Epoch  10 Batch  149 / 447  Training Loss  1.6075460735009983e-05\n",
            "Epoch  10 Batch  150 / 447  Training Loss  1.656124004512094e-05\n",
            "Epoch  10 Batch  151 / 447  Training Loss  2.276258237543516e-05\n",
            "Epoch  10 Batch  152 / 447  Training Loss  3.4307562600588426e-05\n",
            "Epoch  10 Batch  153 / 447  Training Loss  2.4841097911121324e-05\n",
            "Epoch  10 Batch  154 / 447  Training Loss  1.5165420336415991e-05\n",
            "Epoch  10 Batch  155 / 447  Training Loss  2.1057530830148607e-05\n",
            "Epoch  10 Batch  156 / 447  Training Loss  2.0279689124436118e-05\n",
            "Epoch  10 Batch  157 / 447  Training Loss  2.0589523046510294e-05\n",
            "Epoch  10 Batch  158 / 447  Training Loss  1.991286626434885e-05\n",
            "Epoch  10 Batch  159 / 447  Training Loss  2.408900581940543e-05\n",
            "Epoch  10 Batch  160 / 447  Training Loss  2.492420571797993e-05\n",
            "Epoch  10 Batch  161 / 447  Training Loss  1.706119473965373e-05\n",
            "Epoch  10 Batch  162 / 447  Training Loss  2.4176544684451073e-05\n",
            "Epoch  10 Batch  163 / 447  Training Loss  1.8210464986623265e-05\n",
            "Epoch  10 Batch  164 / 447  Training Loss  2.0802064682357013e-05\n",
            "Epoch  10 Batch  165 / 447  Training Loss  3.046955134777818e-05\n",
            "Epoch  10 Batch  166 / 447  Training Loss  2.2601867385674268e-05\n",
            "Epoch  10 Batch  167 / 447  Training Loss  1.685299321252387e-05\n",
            "Epoch  10 Batch  168 / 447  Training Loss  2.4102453608065844e-05\n",
            "Epoch  10 Batch  169 / 447  Training Loss  2.205335295002442e-05\n",
            "Epoch  10 Batch  170 / 447  Training Loss  2.2464950234279968e-05\n",
            "Epoch  10 Batch  171 / 447  Training Loss  2.4837814635247923e-05\n",
            "Epoch  10 Batch  172 / 447  Training Loss  1.3313565432326868e-05\n",
            "Epoch  10 Batch  173 / 447  Training Loss  2.0627478079404682e-05\n",
            "Epoch  10 Batch  174 / 447  Training Loss  1.7541633496875875e-05\n",
            "Epoch  10 Batch  175 / 447  Training Loss  2.7687645342666656e-05\n",
            "Epoch  10 Batch  176 / 447  Training Loss  2.9870534490328282e-05\n",
            "Epoch  10 Batch  177 / 447  Training Loss  1.9169116058037616e-05\n",
            "Epoch  10 Batch  178 / 447  Training Loss  1.4074346836423501e-05\n",
            "Epoch  10 Batch  179 / 447  Training Loss  1.4253007975639775e-05\n",
            "Epoch  10 Batch  180 / 447  Training Loss  2.1911657313467003e-05\n",
            "Epoch  10 Batch  181 / 447  Training Loss  2.4482349544996396e-05\n",
            "Epoch  10 Batch  182 / 447  Training Loss  1.8132204786525108e-05\n",
            "Epoch  10 Batch  183 / 447  Training Loss  1.9699058611877263e-05\n",
            "Epoch  10 Batch  184 / 447  Training Loss  2.7481975848786533e-05\n",
            "Epoch  10 Batch  185 / 447  Training Loss  2.7851079721585847e-05\n",
            "Epoch  10 Batch  186 / 447  Training Loss  2.1801755792694166e-05\n",
            "Epoch  10 Batch  187 / 447  Training Loss  1.553410584165249e-05\n",
            "Epoch  10 Batch  188 / 447  Training Loss  1.980538036150392e-05\n",
            "Epoch  10 Batch  189 / 447  Training Loss  1.9166505808243528e-05\n",
            "Epoch  10 Batch  190 / 447  Training Loss  2.0184246750432067e-05\n",
            "Epoch  10 Batch  191 / 447  Training Loss  1.7827995179686695e-05\n",
            "Epoch  10 Batch  192 / 447  Training Loss  1.8048780475510284e-05\n",
            "Epoch  10 Batch  193 / 447  Training Loss  2.0958610548404977e-05\n",
            "Epoch  10 Batch  194 / 447  Training Loss  1.684507333266083e-05\n",
            "Epoch  10 Batch  195 / 447  Training Loss  2.6504390916670673e-05\n",
            "Epoch  10 Batch  196 / 447  Training Loss  2.5594459657440893e-05\n",
            "Epoch  10 Batch  197 / 447  Training Loss  2.1716066839871928e-05\n",
            "Epoch  10 Batch  198 / 447  Training Loss  2.3452925233868882e-05\n",
            "Epoch  10 Batch  199 / 447  Training Loss  1.4459070371231064e-05\n",
            "Epoch  10 Batch  200 / 447  Training Loss  2.784357093332801e-05\n",
            "Epoch  10 Batch  201 / 447  Training Loss  2.447916631354019e-05\n",
            "Epoch  10 Batch  202 / 447  Training Loss  1.6683568901498802e-05\n",
            "Epoch  10 Batch  203 / 447  Training Loss  1.8602449927129783e-05\n",
            "Epoch  10 Batch  204 / 447  Training Loss  3.0718740163138136e-05\n",
            "Epoch  10 Batch  205 / 447  Training Loss  1.6318927009706385e-05\n",
            "Epoch  10 Batch  206 / 447  Training Loss  1.2694147699221503e-05\n",
            "Epoch  10 Batch  207 / 447  Training Loss  2.0356732420623302e-05\n",
            "Epoch  10 Batch  208 / 447  Training Loss  2.5106528482865542e-05\n",
            "Epoch  10 Batch  209 / 447  Training Loss  1.86927445611218e-05\n",
            "Epoch  10 Batch  210 / 447  Training Loss  1.70688745129155e-05\n",
            "Epoch  10 Batch  211 / 447  Training Loss  2.652582224982325e-05\n",
            "Epoch  10 Batch  212 / 447  Training Loss  1.7911530449055135e-05\n",
            "Epoch  10 Batch  213 / 447  Training Loss  1.7921447579283267e-05\n",
            "Epoch  10 Batch  214 / 447  Training Loss  1.760304621711839e-05\n",
            "Epoch  10 Batch  215 / 447  Training Loss  1.4952491255826317e-05\n",
            "Epoch  10 Batch  216 / 447  Training Loss  3.4428114304319024e-05\n",
            "Epoch  10 Batch  217 / 447  Training Loss  2.9681323212571442e-05\n",
            "Epoch  10 Batch  218 / 447  Training Loss  1.4111816199147142e-05\n",
            "Epoch  10 Batch  219 / 447  Training Loss  2.7199186661164276e-05\n",
            "Epoch  10 Batch  220 / 447  Training Loss  2.5293171347584575e-05\n",
            "Epoch  10 Batch  221 / 447  Training Loss  2.0205574401188642e-05\n",
            "Epoch  10 Batch  222 / 447  Training Loss  2.009676245506853e-05\n",
            "Epoch  10 Batch  223 / 447  Training Loss  1.547623105579987e-05\n",
            "Epoch  10 Batch  224 / 447  Training Loss  2.4459206542815082e-05\n",
            "Epoch  10 Batch  225 / 447  Training Loss  2.020206011366099e-05\n",
            "Epoch  10 Batch  226 / 447  Training Loss  1.624724973225966e-05\n",
            "Epoch  10 Batch  227 / 447  Training Loss  1.5429144696099684e-05\n",
            "Epoch  10 Batch  228 / 447  Training Loss  3.396241299924441e-05\n",
            "Epoch  10 Batch  229 / 447  Training Loss  1.949912439158652e-05\n",
            "Epoch  10 Batch  230 / 447  Training Loss  2.424092781438958e-05\n",
            "Epoch  10 Batch  231 / 447  Training Loss  1.985491871892009e-05\n",
            "Epoch  10 Batch  232 / 447  Training Loss  1.7870954252430238e-05\n",
            "Epoch  10 Batch  233 / 447  Training Loss  2.2432024707086384e-05\n",
            "Epoch  10 Batch  234 / 447  Training Loss  1.8582470147521235e-05\n",
            "Epoch  10 Batch  235 / 447  Training Loss  1.8765613276627846e-05\n",
            "Epoch  10 Batch  236 / 447  Training Loss  2.5522971554892138e-05\n",
            "Epoch  10 Batch  237 / 447  Training Loss  2.5538663976476528e-05\n",
            "Epoch  10 Batch  238 / 447  Training Loss  2.2333442757371813e-05\n",
            "Epoch  10 Batch  239 / 447  Training Loss  2.0944604329997674e-05\n",
            "Epoch  10 Batch  240 / 447  Training Loss  2.137822775694076e-05\n",
            "Epoch  10 Batch  241 / 447  Training Loss  1.7545167793286964e-05\n",
            "Epoch  10 Batch  242 / 447  Training Loss  2.250827856187243e-05\n",
            "Epoch  10 Batch  243 / 447  Training Loss  2.536585998313967e-05\n",
            "Epoch  10 Batch  244 / 447  Training Loss  1.684121525613591e-05\n",
            "Epoch  10 Batch  245 / 447  Training Loss  2.3594409867655486e-05\n",
            "Epoch  10 Batch  246 / 447  Training Loss  2.579248757683672e-05\n",
            "Epoch  10 Batch  247 / 447  Training Loss  3.0166942451614887e-05\n",
            "Epoch  10 Batch  248 / 447  Training Loss  1.9502000213833526e-05\n",
            "Epoch  10 Batch  249 / 447  Training Loss  1.5940859157126397e-05\n",
            "Epoch  10 Batch  250 / 447  Training Loss  2.0665947886300273e-05\n",
            "Epoch  10 Batch  251 / 447  Training Loss  1.7675174603937194e-05\n",
            "Epoch  10 Batch  252 / 447  Training Loss  2.4318309442605823e-05\n",
            "Epoch  10 Batch  253 / 447  Training Loss  2.032797783613205e-05\n",
            "Epoch  10 Batch  254 / 447  Training Loss  1.3883402971259784e-05\n",
            "Epoch  10 Batch  255 / 447  Training Loss  2.4031434804783203e-05\n",
            "Epoch  10 Batch  256 / 447  Training Loss  2.3265545678441413e-05\n",
            "Epoch  10 Batch  257 / 447  Training Loss  1.9913919459213503e-05\n",
            "Epoch  10 Batch  258 / 447  Training Loss  2.553166814323049e-05\n",
            "Epoch  10 Batch  259 / 447  Training Loss  1.593074921402149e-05\n",
            "Epoch  10 Batch  260 / 447  Training Loss  1.4637555068475194e-05\n",
            "Epoch  10 Batch  261 / 447  Training Loss  1.9334123862790875e-05\n",
            "Epoch  10 Batch  262 / 447  Training Loss  2.0863095414824784e-05\n",
            "Epoch  10 Batch  263 / 447  Training Loss  2.0959791072527878e-05\n",
            "Epoch  10 Batch  264 / 447  Training Loss  1.7896145436679944e-05\n",
            "Epoch  10 Batch  265 / 447  Training Loss  2.0598716218955815e-05\n",
            "Epoch  10 Batch  266 / 447  Training Loss  2.7955837140325457e-05\n",
            "Epoch  10 Batch  267 / 447  Training Loss  2.1432335415738635e-05\n",
            "Epoch  10 Batch  268 / 447  Training Loss  2.4002785721677355e-05\n",
            "Epoch  10 Batch  269 / 447  Training Loss  2.2195856217877008e-05\n",
            "Epoch  10 Batch  270 / 447  Training Loss  1.8050341168418527e-05\n",
            "Epoch  10 Batch  271 / 447  Training Loss  1.926208278746344e-05\n",
            "Epoch  10 Batch  272 / 447  Training Loss  1.4238619769457728e-05\n",
            "Epoch  10 Batch  273 / 447  Training Loss  2.0318193492130376e-05\n",
            "Epoch  10 Batch  274 / 447  Training Loss  2.879522980947513e-05\n",
            "Epoch  10 Batch  275 / 447  Training Loss  1.826685002015438e-05\n",
            "Epoch  10 Batch  276 / 447  Training Loss  1.7306736481259577e-05\n",
            "Epoch  10 Batch  277 / 447  Training Loss  2.904549728555139e-05\n",
            "Epoch  10 Batch  278 / 447  Training Loss  1.4430746887228452e-05\n",
            "Epoch  10 Batch  279 / 447  Training Loss  2.9150058253435418e-05\n",
            "Epoch  10 Batch  280 / 447  Training Loss  1.805746251193341e-05\n",
            "Epoch  10 Batch  281 / 447  Training Loss  2.1719049982493743e-05\n",
            "Epoch  10 Batch  282 / 447  Training Loss  2.1125812054378912e-05\n",
            "Epoch  10 Batch  283 / 447  Training Loss  1.596998299646657e-05\n",
            "Epoch  10 Batch  284 / 447  Training Loss  1.924941170727834e-05\n",
            "Epoch  10 Batch  285 / 447  Training Loss  1.778677869879175e-05\n",
            "Epoch  10 Batch  286 / 447  Training Loss  2.059883627225645e-05\n",
            "Epoch  10 Batch  287 / 447  Training Loss  1.775404416548554e-05\n",
            "Epoch  10 Batch  288 / 447  Training Loss  1.740013794915285e-05\n",
            "Epoch  10 Batch  289 / 447  Training Loss  3.1581927032675594e-05\n",
            "Epoch  10 Batch  290 / 447  Training Loss  1.749543116602581e-05\n",
            "Epoch  10 Batch  291 / 447  Training Loss  2.2549314962816425e-05\n",
            "Epoch  10 Batch  292 / 447  Training Loss  1.2853057341999374e-05\n",
            "Epoch  10 Batch  293 / 447  Training Loss  1.4494157767330762e-05\n",
            "Epoch  10 Batch  294 / 447  Training Loss  1.4673531950393226e-05\n",
            "Epoch  10 Batch  295 / 447  Training Loss  2.1534062398131937e-05\n",
            "Epoch  10 Batch  296 / 447  Training Loss  2.2887166778673418e-05\n",
            "Epoch  10 Batch  297 / 447  Training Loss  1.9507631805026904e-05\n",
            "Epoch  10 Batch  298 / 447  Training Loss  1.3613484043162316e-05\n",
            "Epoch  10 Batch  299 / 447  Training Loss  1.992928810068406e-05\n",
            "Epoch  10 Batch  300 / 447  Training Loss  2.1956129785394296e-05\n",
            "Epoch  10 Batch  301 / 447  Training Loss  2.6992105631507002e-05\n",
            "Epoch  10 Batch  302 / 447  Training Loss  1.851595334301237e-05\n",
            "Epoch  10 Batch  303 / 447  Training Loss  2.1243864466669038e-05\n",
            "Epoch  10 Batch  304 / 447  Training Loss  1.66267800523201e-05\n",
            "Epoch  10 Batch  305 / 447  Training Loss  2.030446739809122e-05\n",
            "Epoch  10 Batch  306 / 447  Training Loss  1.8853630535886623e-05\n",
            "Epoch  10 Batch  307 / 447  Training Loss  1.9964045350207016e-05\n",
            "Epoch  10 Batch  308 / 447  Training Loss  3.7166220863582566e-05\n",
            "Epoch  10 Batch  309 / 447  Training Loss  2.69793308689259e-05\n",
            "Epoch  10 Batch  310 / 447  Training Loss  1.9065291780862026e-05\n",
            "Epoch  10 Batch  311 / 447  Training Loss  2.1318812287063338e-05\n",
            "Epoch  10 Batch  312 / 447  Training Loss  1.3937088624516036e-05\n",
            "Epoch  10 Batch  313 / 447  Training Loss  1.893597982416395e-05\n",
            "Epoch  10 Batch  314 / 447  Training Loss  1.689026612439193e-05\n",
            "Epoch  10 Batch  315 / 447  Training Loss  1.416485793015454e-05\n",
            "Epoch  10 Batch  316 / 447  Training Loss  1.92264596989844e-05\n",
            "Epoch  10 Batch  317 / 447  Training Loss  2.391055204498116e-05\n",
            "Epoch  10 Batch  318 / 447  Training Loss  1.5766494470881298e-05\n",
            "Epoch  10 Batch  319 / 447  Training Loss  1.7929660316440277e-05\n",
            "Epoch  10 Batch  320 / 447  Training Loss  2.020363899646327e-05\n",
            "Epoch  10 Batch  321 / 447  Training Loss  2.3027680072118528e-05\n",
            "Epoch  10 Batch  322 / 447  Training Loss  1.5140412870096043e-05\n",
            "Epoch  10 Batch  323 / 447  Training Loss  1.7905422282638028e-05\n",
            "Epoch  10 Batch  324 / 447  Training Loss  2.4226335881394334e-05\n",
            "Epoch  10 Batch  325 / 447  Training Loss  1.8298120266990736e-05\n",
            "Epoch  10 Batch  326 / 447  Training Loss  2.3749815227347426e-05\n",
            "Epoch  10 Batch  327 / 447  Training Loss  3.0663202778669074e-05\n",
            "Epoch  10 Batch  328 / 447  Training Loss  1.889644772745669e-05\n",
            "Epoch  10 Batch  329 / 447  Training Loss  2.068210596917197e-05\n",
            "Epoch  10 Batch  330 / 447  Training Loss  2.0361572751426138e-05\n",
            "Epoch  10 Batch  331 / 447  Training Loss  1.626325502002146e-05\n",
            "Epoch  10 Batch  332 / 447  Training Loss  1.4087344425206538e-05\n",
            "Epoch  10 Batch  333 / 447  Training Loss  1.740614970913157e-05\n",
            "Epoch  10 Batch  334 / 447  Training Loss  1.4557809663529042e-05\n",
            "Epoch  10 Batch  335 / 447  Training Loss  4.5565095206256956e-05\n",
            "Epoch  10 Batch  336 / 447  Training Loss  1.5986164726200514e-05\n",
            "Epoch  10 Batch  337 / 447  Training Loss  1.4847752936475445e-05\n",
            "Epoch  10 Batch  338 / 447  Training Loss  2.6486324713914655e-05\n",
            "Epoch  10 Batch  339 / 447  Training Loss  2.227670483989641e-05\n",
            "Epoch  10 Batch  340 / 447  Training Loss  2.1662730432581156e-05\n",
            "Epoch  10 Batch  341 / 447  Training Loss  1.905355202325154e-05\n",
            "Epoch  10 Batch  342 / 447  Training Loss  1.901945870486088e-05\n",
            "Epoch  10 Batch  343 / 447  Training Loss  1.7119386029662564e-05\n",
            "Epoch  10 Batch  344 / 447  Training Loss  1.2728748515655752e-05\n",
            "Epoch  10 Batch  345 / 447  Training Loss  2.4352628315682523e-05\n",
            "Epoch  10 Batch  346 / 447  Training Loss  1.7440122974221595e-05\n",
            "Epoch  10 Batch  347 / 447  Training Loss  2.0578816474881023e-05\n",
            "Epoch  10 Batch  348 / 447  Training Loss  2.8668742743320763e-05\n",
            "Epoch  10 Batch  349 / 447  Training Loss  1.8260447177453898e-05\n",
            "Epoch  10 Batch  350 / 447  Training Loss  2.0241546735633165e-05\n",
            "Epoch  10 Batch  351 / 447  Training Loss  1.3354099792195484e-05\n",
            "Epoch  10 Batch  352 / 447  Training Loss  2.8180198569316417e-05\n",
            "Epoch  10 Batch  353 / 447  Training Loss  1.616208282939624e-05\n",
            "Epoch  10 Batch  354 / 447  Training Loss  1.885808705992531e-05\n",
            "Epoch  10 Batch  355 / 447  Training Loss  2.2609567167819478e-05\n",
            "Epoch  10 Batch  356 / 447  Training Loss  2.8096170353819616e-05\n",
            "Epoch  10 Batch  357 / 447  Training Loss  2.026403853960801e-05\n",
            "Epoch  10 Batch  358 / 447  Training Loss  2.6288043954991736e-05\n",
            "Epoch  10 Batch  359 / 447  Training Loss  2.00366757781012e-05\n",
            "Epoch  10 Batch  360 / 447  Training Loss  1.627056917641312e-05\n",
            "Epoch  10 Batch  361 / 447  Training Loss  1.987742871278897e-05\n",
            "Epoch  10 Batch  362 / 447  Training Loss  2.114745984727051e-05\n",
            "Epoch  10 Batch  363 / 447  Training Loss  2.3386597604257986e-05\n",
            "Epoch  10 Batch  364 / 447  Training Loss  2.046127337962389e-05\n",
            "Epoch  10 Batch  365 / 447  Training Loss  1.5331906979554333e-05\n",
            "Epoch  10 Batch  366 / 447  Training Loss  1.4417612874240149e-05\n",
            "Epoch  10 Batch  367 / 447  Training Loss  1.2648257325054146e-05\n",
            "Epoch  10 Batch  368 / 447  Training Loss  2.2784772227169015e-05\n",
            "Epoch  10 Batch  369 / 447  Training Loss  1.4843802091490943e-05\n",
            "Epoch  10 Batch  370 / 447  Training Loss  1.5486919437535107e-05\n",
            "Epoch  10 Batch  371 / 447  Training Loss  1.8612803614814766e-05\n",
            "Epoch  10 Batch  372 / 447  Training Loss  1.3428631064016372e-05\n",
            "Epoch  10 Batch  373 / 447  Training Loss  1.3722234143642709e-05\n",
            "Epoch  10 Batch  374 / 447  Training Loss  1.6433838027296588e-05\n",
            "Epoch  10 Batch  375 / 447  Training Loss  2.1819725589011796e-05\n",
            "Epoch  10 Batch  376 / 447  Training Loss  1.2885987416666467e-05\n",
            "Epoch  10 Batch  377 / 447  Training Loss  1.8771028408082202e-05\n",
            "Epoch  10 Batch  378 / 447  Training Loss  1.5921570593491197e-05\n",
            "Epoch  10 Batch  379 / 447  Training Loss  1.7036267308867536e-05\n",
            "Epoch  10 Batch  380 / 447  Training Loss  1.6787918866612017e-05\n",
            "Epoch  10 Batch  381 / 447  Training Loss  1.3466379641613457e-05\n",
            "Epoch  10 Batch  382 / 447  Training Loss  1.4334384104586206e-05\n",
            "Epoch  10 Batch  383 / 447  Training Loss  2.023424895014614e-05\n",
            "Epoch  10 Batch  384 / 447  Training Loss  1.768694528436754e-05\n",
            "Epoch  10 Batch  385 / 447  Training Loss  1.798094490368385e-05\n",
            "Epoch  10 Batch  386 / 447  Training Loss  2.216406210209243e-05\n",
            "Epoch  10 Batch  387 / 447  Training Loss  1.8799013560055755e-05\n",
            "Epoch  10 Batch  388 / 447  Training Loss  1.7385387764079496e-05\n",
            "Epoch  10 Batch  389 / 447  Training Loss  1.771868483047001e-05\n",
            "Epoch  10 Batch  390 / 447  Training Loss  1.8340086171519943e-05\n",
            "Epoch  10 Batch  391 / 447  Training Loss  2.396082527411636e-05\n",
            "Epoch  10 Batch  392 / 447  Training Loss  2.485864570189733e-05\n",
            "Epoch  10 Batch  393 / 447  Training Loss  1.7709151507006027e-05\n",
            "Epoch  10 Batch  394 / 447  Training Loss  2.059490725514479e-05\n",
            "Epoch  10 Batch  395 / 447  Training Loss  1.6175201380974613e-05\n",
            "Epoch  10 Batch  396 / 447  Training Loss  2.9399625418591313e-05\n",
            "Epoch  10 Batch  397 / 447  Training Loss  2.665811734914314e-05\n",
            "Epoch  10 Batch  398 / 447  Training Loss  3.72689391952008e-05\n",
            "Epoch  10 Batch  399 / 447  Training Loss  1.9837489162455313e-05\n",
            "Epoch  10 Batch  400 / 447  Training Loss  2.828507604135666e-05\n",
            "Epoch  10 Batch  401 / 447  Training Loss  1.9483972209854983e-05\n",
            "Epoch  10 Batch  402 / 447  Training Loss  1.434788828191813e-05\n",
            "Epoch  10 Batch  403 / 447  Training Loss  1.6928419427131303e-05\n",
            "Epoch  10 Batch  404 / 447  Training Loss  2.6125204385607503e-05\n",
            "Epoch  10 Batch  405 / 447  Training Loss  2.2943677322473377e-05\n",
            "Epoch  10 Batch  406 / 447  Training Loss  1.5352816262748092e-05\n",
            "Epoch  10 Batch  407 / 447  Training Loss  1.555148082843516e-05\n",
            "Epoch  10 Batch  408 / 447  Training Loss  1.617606176296249e-05\n",
            "Epoch  10 Batch  409 / 447  Training Loss  2.188067628594581e-05\n",
            "Epoch  10 Batch  410 / 447  Training Loss  1.3355497685552109e-05\n",
            "Epoch  10 Batch  411 / 447  Training Loss  1.6238125681411475e-05\n",
            "Epoch  10 Batch  412 / 447  Training Loss  1.688351585471537e-05\n",
            "Epoch  10 Batch  413 / 447  Training Loss  1.6652225895086303e-05\n",
            "Epoch  10 Batch  414 / 447  Training Loss  2.0393808881635778e-05\n",
            "Epoch  10 Batch  415 / 447  Training Loss  2.597522507130634e-05\n",
            "Epoch  10 Batch  416 / 447  Training Loss  1.9701505152625032e-05\n",
            "Epoch  10 Batch  417 / 447  Training Loss  1.784596315701492e-05\n",
            "Epoch  10 Batch  418 / 447  Training Loss  2.123577542079147e-05\n",
            "Epoch  10 Batch  419 / 447  Training Loss  2.4966930141090415e-05\n",
            "Epoch  10 Batch  420 / 447  Training Loss  1.2943507499585394e-05\n",
            "Epoch  10 Batch  421 / 447  Training Loss  1.5780286048538983e-05\n",
            "Epoch  10 Batch  422 / 447  Training Loss  2.4033317458815873e-05\n",
            "Epoch  10 Batch  423 / 447  Training Loss  1.8579925381345674e-05\n",
            "Epoch  10 Batch  424 / 447  Training Loss  1.9317219994263723e-05\n",
            "Epoch  10 Batch  425 / 447  Training Loss  1.873652399808634e-05\n",
            "Epoch  10 Batch  426 / 447  Training Loss  2.1095296688145027e-05\n",
            "Epoch  10 Batch  427 / 447  Training Loss  1.8117156287189573e-05\n",
            "Epoch  10 Batch  428 / 447  Training Loss  1.7121208657044917e-05\n",
            "Epoch  10 Batch  429 / 447  Training Loss  2.0937422959832475e-05\n",
            "Epoch  10 Batch  430 / 447  Training Loss  2.2170523152453825e-05\n",
            "Epoch  10 Batch  431 / 447  Training Loss  1.8712415112531744e-05\n",
            "Epoch  10 Batch  432 / 447  Training Loss  2.2975758838583715e-05\n",
            "Epoch  10 Batch  433 / 447  Training Loss  1.9234414139646105e-05\n",
            "Epoch  10 Batch  434 / 447  Training Loss  2.0924948330502957e-05\n",
            "Epoch  10 Batch  435 / 447  Training Loss  2.4189324904000387e-05\n",
            "Epoch  10 Batch  436 / 447  Training Loss  2.7023790607927367e-05\n",
            "Epoch  10 Batch  437 / 447  Training Loss  2.4128761651809327e-05\n",
            "Epoch  10 Batch  438 / 447  Training Loss  1.7610534996492788e-05\n",
            "Epoch  10 Batch  439 / 447  Training Loss  3.3556596463313326e-05\n",
            "Epoch  10 Batch  440 / 447  Training Loss  2.6182604415225796e-05\n",
            "Epoch  10 Batch  441 / 447  Training Loss  1.4747196473763324e-05\n",
            "Epoch  10 Batch  442 / 447  Training Loss  1.975284430955071e-05\n",
            "Epoch  10 Batch  443 / 447  Training Loss  1.882326068880502e-05\n",
            "Epoch  10 Batch  444 / 447  Training Loss  3.005090547958389e-05\n",
            "Epoch  10 Batch  445 / 447  Training Loss  1.4071243640501052e-05\n",
            "Epoch  10 Batch  446 / 447  Training Loss  6.594295882678125e-06\n",
            "  11    |    -    |   0.000021   | 99.960815\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 11\n",
            "Epoch  11 Batch  0 / 447  Training Loss  1.9219418391003273e-05\n",
            "Epoch  11 Batch  1 / 447  Training Loss  1.5151176739891525e-05\n",
            "Epoch  11 Batch  2 / 447  Training Loss  2.437817420286592e-05\n",
            "Epoch  11 Batch  3 / 447  Training Loss  1.6933841834543273e-05\n",
            "Epoch  11 Batch  4 / 447  Training Loss  1.2835629604524001e-05\n",
            "Epoch  11 Batch  5 / 447  Training Loss  1.777460056473501e-05\n",
            "Epoch  11 Batch  6 / 447  Training Loss  1.941972914210055e-05\n",
            "Epoch  11 Batch  7 / 447  Training Loss  2.5966684916056693e-05\n",
            "Epoch  11 Batch  8 / 447  Training Loss  1.6592710380791686e-05\n",
            "Epoch  11 Batch  9 / 447  Training Loss  1.6758973288233392e-05\n",
            "Epoch  11 Batch  10 / 447  Training Loss  2.678876080608461e-05\n",
            "Epoch  11 Batch  11 / 447  Training Loss  2.235249849036336e-05\n",
            "Epoch  11 Batch  12 / 447  Training Loss  1.4889371414028574e-05\n",
            "Epoch  11 Batch  13 / 447  Training Loss  2.217937435489148e-05\n",
            "Epoch  11 Batch  14 / 447  Training Loss  1.2873942068836186e-05\n",
            "Epoch  11 Batch  15 / 447  Training Loss  2.5493052817182615e-05\n",
            "Epoch  11 Batch  16 / 447  Training Loss  2.4605798898846842e-05\n",
            "Epoch  11 Batch  17 / 447  Training Loss  1.4360039131133817e-05\n",
            "Epoch  11 Batch  18 / 447  Training Loss  1.3281352948979475e-05\n",
            "Epoch  11 Batch  19 / 447  Training Loss  2.5155270122922957e-05\n",
            "Epoch  11 Batch  20 / 447  Training Loss  1.809040259104222e-05\n",
            "Epoch  11 Batch  21 / 447  Training Loss  2.1600724721793085e-05\n",
            "Epoch  11 Batch  22 / 447  Training Loss  1.6140169464051723e-05\n",
            "Epoch  11 Batch  23 / 447  Training Loss  1.7521764675620943e-05\n",
            "Epoch  11 Batch  24 / 447  Training Loss  1.743330722092651e-05\n",
            "Epoch  11 Batch  25 / 447  Training Loss  1.4186673979565967e-05\n",
            "Epoch  11 Batch  26 / 447  Training Loss  1.790629539755173e-05\n",
            "Epoch  11 Batch  27 / 447  Training Loss  1.6365032934118062e-05\n",
            "Epoch  11 Batch  28 / 447  Training Loss  1.740281550155487e-05\n",
            "Epoch  11 Batch  29 / 447  Training Loss  2.3168926418293267e-05\n",
            "Epoch  11 Batch  30 / 447  Training Loss  2.4309214495588094e-05\n",
            "Epoch  11 Batch  31 / 447  Training Loss  1.751280069584027e-05\n",
            "Epoch  11 Batch  32 / 447  Training Loss  2.1203120923019014e-05\n",
            "Epoch  11 Batch  33 / 447  Training Loss  1.6026968296500854e-05\n",
            "Epoch  11 Batch  34 / 447  Training Loss  1.6900246919249184e-05\n",
            "Epoch  11 Batch  35 / 447  Training Loss  3.1658968509873375e-05\n",
            "Epoch  11 Batch  36 / 447  Training Loss  1.4682885193906259e-05\n",
            "Epoch  11 Batch  37 / 447  Training Loss  1.5203861948975828e-05\n",
            "Epoch  11 Batch  38 / 447  Training Loss  1.6206427972065285e-05\n",
            "Epoch  11 Batch  39 / 447  Training Loss  2.390598092461005e-05\n",
            "Epoch  11 Batch  40 / 447  Training Loss  2.2378315406967886e-05\n",
            "Epoch  11 Batch  41 / 447  Training Loss  2.5190794985974208e-05\n",
            "Epoch  11 Batch  42 / 447  Training Loss  2.0180279534542933e-05\n",
            "Epoch  11 Batch  43 / 447  Training Loss  1.4656469829787966e-05\n",
            "Epoch  11 Batch  44 / 447  Training Loss  2.2810016162111424e-05\n",
            "Epoch  11 Batch  45 / 447  Training Loss  1.8278247807756998e-05\n",
            "Epoch  11 Batch  46 / 447  Training Loss  2.4570817913627252e-05\n",
            "Epoch  11 Batch  47 / 447  Training Loss  1.94104177353438e-05\n",
            "Epoch  11 Batch  48 / 447  Training Loss  1.652760329307057e-05\n",
            "Epoch  11 Batch  49 / 447  Training Loss  1.8171960618929006e-05\n",
            "Epoch  11 Batch  50 / 447  Training Loss  1.3474671504809521e-05\n",
            "Epoch  11 Batch  51 / 447  Training Loss  1.4314156942418776e-05\n",
            "Epoch  11 Batch  52 / 447  Training Loss  1.4906749129295349e-05\n",
            "Epoch  11 Batch  53 / 447  Training Loss  1.7583492081030272e-05\n",
            "Epoch  11 Batch  54 / 447  Training Loss  2.6527421141508967e-05\n",
            "Epoch  11 Batch  55 / 447  Training Loss  1.6986736227408983e-05\n",
            "Epoch  11 Batch  56 / 447  Training Loss  2.425900129310321e-05\n",
            "Epoch  11 Batch  57 / 447  Training Loss  2.5377148631378077e-05\n",
            "Epoch  11 Batch  58 / 447  Training Loss  1.8413102225167677e-05\n",
            "Epoch  11 Batch  59 / 447  Training Loss  2.1442157958517782e-05\n",
            "Epoch  11 Batch  60 / 447  Training Loss  1.904108103190083e-05\n",
            "Epoch  11 Batch  61 / 447  Training Loss  1.9498496840242296e-05\n",
            "Epoch  11 Batch  62 / 447  Training Loss  1.989393786061555e-05\n",
            "Epoch  11 Batch  63 / 447  Training Loss  1.7587857655598782e-05\n",
            "Epoch  11 Batch  64 / 447  Training Loss  2.5213637854903936e-05\n",
            "Epoch  11 Batch  65 / 447  Training Loss  2.3309257812798023e-05\n",
            "Epoch  11 Batch  66 / 447  Training Loss  1.4236104107112624e-05\n",
            "Epoch  11 Batch  67 / 447  Training Loss  1.5546764188911766e-05\n",
            "Epoch  11 Batch  68 / 447  Training Loss  1.8364233255852014e-05\n",
            "Epoch  11 Batch  69 / 447  Training Loss  1.5939571312628686e-05\n",
            "Epoch  11 Batch  70 / 447  Training Loss  1.631723171158228e-05\n",
            "Epoch  11 Batch  71 / 447  Training Loss  1.5131943655433133e-05\n",
            "Epoch  11 Batch  72 / 447  Training Loss  1.954867911990732e-05\n",
            "Epoch  11 Batch  73 / 447  Training Loss  1.4596412256651092e-05\n",
            "Epoch  11 Batch  74 / 447  Training Loss  1.60057534230873e-05\n",
            "Epoch  11 Batch  75 / 447  Training Loss  1.3197087355365511e-05\n",
            "Epoch  11 Batch  76 / 447  Training Loss  1.7056023352779448e-05\n",
            "Epoch  11 Batch  77 / 447  Training Loss  1.7468528312747367e-05\n",
            "Epoch  11 Batch  78 / 447  Training Loss  2.00145659619011e-05\n",
            "Epoch  11 Batch  79 / 447  Training Loss  1.5017804798844736e-05\n",
            "Epoch  11 Batch  80 / 447  Training Loss  1.7207952623721212e-05\n",
            "Epoch  11 Batch  81 / 447  Training Loss  1.3196434338169638e-05\n",
            "Epoch  11 Batch  82 / 447  Training Loss  1.8478989659342915e-05\n",
            "Epoch  11 Batch  83 / 447  Training Loss  2.3080890969140455e-05\n",
            "Epoch  11 Batch  84 / 447  Training Loss  1.955132211151067e-05\n",
            "Epoch  11 Batch  85 / 447  Training Loss  1.5058982171467505e-05\n",
            "Epoch  11 Batch  86 / 447  Training Loss  1.7027135982061736e-05\n",
            "Epoch  11 Batch  87 / 447  Training Loss  1.6989513824228197e-05\n",
            "Epoch  11 Batch  88 / 447  Training Loss  2.129816493834369e-05\n",
            "Epoch  11 Batch  89 / 447  Training Loss  2.5333660232718103e-05\n",
            "Epoch  11 Batch  90 / 447  Training Loss  1.9310060451971367e-05\n",
            "Epoch  11 Batch  91 / 447  Training Loss  1.0293225386703853e-05\n",
            "Epoch  11 Batch  92 / 447  Training Loss  2.238066190329846e-05\n",
            "Epoch  11 Batch  93 / 447  Training Loss  1.9660421457956545e-05\n",
            "Epoch  11 Batch  94 / 447  Training Loss  1.3149468031770084e-05\n",
            "Epoch  11 Batch  95 / 447  Training Loss  2.4511229639756493e-05\n",
            "Epoch  11 Batch  96 / 447  Training Loss  1.9064051230088808e-05\n",
            "Epoch  11 Batch  97 / 447  Training Loss  1.5052996786835138e-05\n",
            "Epoch  11 Batch  98 / 447  Training Loss  2.004611087613739e-05\n",
            "Epoch  11 Batch  99 / 447  Training Loss  3.198039121343754e-05\n",
            "Epoch  11 Batch  100 / 447  Training Loss  1.7403950550942682e-05\n",
            "Epoch  11 Batch  101 / 447  Training Loss  1.9558170606615022e-05\n",
            "Epoch  11 Batch  102 / 447  Training Loss  1.5326722859754227e-05\n",
            "Epoch  11 Batch  103 / 447  Training Loss  1.6971363947959617e-05\n",
            "Epoch  11 Batch  104 / 447  Training Loss  1.583699486218393e-05\n",
            "Epoch  11 Batch  105 / 447  Training Loss  1.5997282389434986e-05\n",
            "Epoch  11 Batch  106 / 447  Training Loss  1.6682226487318985e-05\n",
            "Epoch  11 Batch  107 / 447  Training Loss  1.6262954886769876e-05\n",
            "Epoch  11 Batch  108 / 447  Training Loss  1.666313619352877e-05\n",
            "Epoch  11 Batch  109 / 447  Training Loss  1.3426597433863208e-05\n",
            "Epoch  11 Batch  110 / 447  Training Loss  1.165478170150891e-05\n",
            "Epoch  11 Batch  111 / 447  Training Loss  1.5193101717159152e-05\n",
            "Epoch  11 Batch  112 / 447  Training Loss  2.854244849004317e-05\n",
            "Epoch  11 Batch  113 / 447  Training Loss  1.8143920897273347e-05\n",
            "Epoch  11 Batch  114 / 447  Training Loss  1.6593616237514652e-05\n",
            "Epoch  11 Batch  115 / 447  Training Loss  1.8491229639039375e-05\n",
            "Epoch  11 Batch  116 / 447  Training Loss  1.4922889022273012e-05\n",
            "Epoch  11 Batch  117 / 447  Training Loss  1.4607941011490766e-05\n",
            "Epoch  11 Batch  118 / 447  Training Loss  1.60254021466244e-05\n",
            "Epoch  11 Batch  119 / 447  Training Loss  1.9375074771232903e-05\n",
            "Epoch  11 Batch  120 / 447  Training Loss  1.18294501589844e-05\n",
            "Epoch  11 Batch  121 / 447  Training Loss  1.6472731658723205e-05\n",
            "Epoch  11 Batch  122 / 447  Training Loss  2.6064511985168792e-05\n",
            "Epoch  11 Batch  123 / 447  Training Loss  2.1764359189546667e-05\n",
            "Epoch  11 Batch  124 / 447  Training Loss  1.4211484995030332e-05\n",
            "Epoch  11 Batch  125 / 447  Training Loss  1.706425246084109e-05\n",
            "Epoch  11 Batch  126 / 447  Training Loss  1.4982879292801954e-05\n",
            "Epoch  11 Batch  127 / 447  Training Loss  1.4006524907017592e-05\n",
            "Epoch  11 Batch  128 / 447  Training Loss  1.4971593373047654e-05\n",
            "Epoch  11 Batch  129 / 447  Training Loss  2.3152420908445492e-05\n",
            "Epoch  11 Batch  130 / 447  Training Loss  1.810507455957122e-05\n",
            "Epoch  11 Batch  131 / 447  Training Loss  1.640164373384323e-05\n",
            "Epoch  11 Batch  132 / 447  Training Loss  1.2862456060247496e-05\n",
            "Epoch  11 Batch  133 / 447  Training Loss  1.1644304322544485e-05\n",
            "Epoch  11 Batch  134 / 447  Training Loss  1.6437063095509075e-05\n",
            "Epoch  11 Batch  135 / 447  Training Loss  1.2979465282114688e-05\n",
            "Epoch  11 Batch  136 / 447  Training Loss  1.5849624105612747e-05\n",
            "Epoch  11 Batch  137 / 447  Training Loss  1.7735965229803696e-05\n",
            "Epoch  11 Batch  138 / 447  Training Loss  1.5811107004992664e-05\n",
            "Epoch  11 Batch  139 / 447  Training Loss  1.7302205378655344e-05\n",
            "Epoch  11 Batch  140 / 447  Training Loss  1.5158444512053393e-05\n",
            "Epoch  11 Batch  141 / 447  Training Loss  1.7233145626960322e-05\n",
            "Epoch  11 Batch  142 / 447  Training Loss  1.9648668967420235e-05\n",
            "Epoch  11 Batch  143 / 447  Training Loss  1.749476359691471e-05\n",
            "Epoch  11 Batch  144 / 447  Training Loss  1.5011334653536323e-05\n",
            "Epoch  11 Batch  145 / 447  Training Loss  1.501623682997888e-05\n",
            "Epoch  11 Batch  146 / 447  Training Loss  1.755787161528133e-05\n",
            "Epoch  11 Batch  147 / 447  Training Loss  1.3060055607638787e-05\n",
            "Epoch  11 Batch  148 / 447  Training Loss  1.984379741770681e-05\n",
            "Epoch  11 Batch  149 / 447  Training Loss  1.7600310457055457e-05\n",
            "Epoch  11 Batch  150 / 447  Training Loss  1.5817135135876015e-05\n",
            "Epoch  11 Batch  151 / 447  Training Loss  1.4078055755817331e-05\n",
            "Epoch  11 Batch  152 / 447  Training Loss  1.7215541447512805e-05\n",
            "Epoch  11 Batch  153 / 447  Training Loss  2.339197635592427e-05\n",
            "Epoch  11 Batch  154 / 447  Training Loss  2.190309351135511e-05\n",
            "Epoch  11 Batch  155 / 447  Training Loss  2.055757249763701e-05\n",
            "Epoch  11 Batch  156 / 447  Training Loss  1.3397672773862723e-05\n",
            "Epoch  11 Batch  157 / 447  Training Loss  1.9156661437591538e-05\n",
            "Epoch  11 Batch  158 / 447  Training Loss  1.5060408259159885e-05\n",
            "Epoch  11 Batch  159 / 447  Training Loss  1.8433267541695386e-05\n",
            "Epoch  11 Batch  160 / 447  Training Loss  1.1259734492341522e-05\n",
            "Epoch  11 Batch  161 / 447  Training Loss  1.3308324923855253e-05\n",
            "Epoch  11 Batch  162 / 447  Training Loss  1.96041746676201e-05\n",
            "Epoch  11 Batch  163 / 447  Training Loss  1.301319025515113e-05\n",
            "Epoch  11 Batch  164 / 447  Training Loss  2.3319020328926854e-05\n",
            "Epoch  11 Batch  165 / 447  Training Loss  2.086139102175366e-05\n",
            "Epoch  11 Batch  166 / 447  Training Loss  1.8696606275625527e-05\n",
            "Epoch  11 Batch  167 / 447  Training Loss  1.2819542462239042e-05\n",
            "Epoch  11 Batch  168 / 447  Training Loss  1.3533943274524063e-05\n",
            "Epoch  11 Batch  169 / 447  Training Loss  2.2256264855968766e-05\n",
            "Epoch  11 Batch  170 / 447  Training Loss  1.5046547559904866e-05\n",
            "Epoch  11 Batch  171 / 447  Training Loss  2.4229428163380362e-05\n",
            "Epoch  11 Batch  172 / 447  Training Loss  2.126684194081463e-05\n",
            "Epoch  11 Batch  173 / 447  Training Loss  1.6095356841105968e-05\n",
            "Epoch  11 Batch  174 / 447  Training Loss  1.4919065506546758e-05\n",
            "Epoch  11 Batch  175 / 447  Training Loss  1.202564817504026e-05\n",
            "Epoch  11 Batch  176 / 447  Training Loss  1.6194737327168696e-05\n",
            "Epoch  11 Batch  177 / 447  Training Loss  2.0167526599834673e-05\n",
            "Epoch  11 Batch  178 / 447  Training Loss  1.5966796127031557e-05\n",
            "Epoch  11 Batch  179 / 447  Training Loss  1.3645442777487915e-05\n",
            "Epoch  11 Batch  180 / 447  Training Loss  1.2673769560933579e-05\n",
            "Epoch  11 Batch  181 / 447  Training Loss  1.722334673104342e-05\n",
            "Epoch  11 Batch  182 / 447  Training Loss  1.2086497918062378e-05\n",
            "Epoch  11 Batch  183 / 447  Training Loss  1.8669446944841184e-05\n",
            "Epoch  11 Batch  184 / 447  Training Loss  2.3684640837018378e-05\n",
            "Epoch  11 Batch  185 / 447  Training Loss  1.82875846803654e-05\n",
            "Epoch  11 Batch  186 / 447  Training Loss  1.4841199117654469e-05\n",
            "Epoch  11 Batch  187 / 447  Training Loss  1.4475846910499968e-05\n",
            "Epoch  11 Batch  188 / 447  Training Loss  1.5057738892210182e-05\n",
            "Epoch  11 Batch  189 / 447  Training Loss  1.0506993021408562e-05\n",
            "Epoch  11 Batch  190 / 447  Training Loss  1.814353345253039e-05\n",
            "Epoch  11 Batch  191 / 447  Training Loss  2.547242729633581e-05\n",
            "Epoch  11 Batch  192 / 447  Training Loss  2.54738588409964e-05\n",
            "Epoch  11 Batch  193 / 447  Training Loss  1.4430405826715287e-05\n",
            "Epoch  11 Batch  194 / 447  Training Loss  1.7432474123779684e-05\n",
            "Epoch  11 Batch  195 / 447  Training Loss  1.9196484572603367e-05\n",
            "Epoch  11 Batch  196 / 447  Training Loss  1.667501601332333e-05\n",
            "Epoch  11 Batch  197 / 447  Training Loss  1.448517923563486e-05\n",
            "Epoch  11 Batch  198 / 447  Training Loss  1.6112788216560148e-05\n",
            "Epoch  11 Batch  199 / 447  Training Loss  1.4976143575040624e-05\n",
            "Epoch  11 Batch  200 / 447  Training Loss  1.5685034668422304e-05\n",
            "Epoch  11 Batch  201 / 447  Training Loss  1.2752615475619677e-05\n",
            "Epoch  11 Batch  202 / 447  Training Loss  1.7187874618684873e-05\n",
            "Epoch  11 Batch  203 / 447  Training Loss  1.4621040463680401e-05\n",
            "Epoch  11 Batch  204 / 447  Training Loss  1.4988526345405262e-05\n",
            "Epoch  11 Batch  205 / 447  Training Loss  1.808626257115975e-05\n",
            "Epoch  11 Batch  206 / 447  Training Loss  1.4992398973845411e-05\n",
            "Epoch  11 Batch  207 / 447  Training Loss  1.6571220839978196e-05\n",
            "Epoch  11 Batch  208 / 447  Training Loss  2.201564893766772e-05\n",
            "Epoch  11 Batch  209 / 447  Training Loss  2.017912447627168e-05\n",
            "Epoch  11 Batch  210 / 447  Training Loss  1.6688878531567752e-05\n",
            "Epoch  11 Batch  211 / 447  Training Loss  1.1962103599216789e-05\n",
            "Epoch  11 Batch  212 / 447  Training Loss  2.1465173631440848e-05\n",
            "Epoch  11 Batch  213 / 447  Training Loss  1.64923767442815e-05\n",
            "Epoch  11 Batch  214 / 447  Training Loss  1.6710861018509604e-05\n",
            "Epoch  11 Batch  215 / 447  Training Loss  2.008311275858432e-05\n",
            "Epoch  11 Batch  216 / 447  Training Loss  2.1036548787378706e-05\n",
            "Epoch  11 Batch  217 / 447  Training Loss  1.888867154775653e-05\n",
            "Epoch  11 Batch  218 / 447  Training Loss  1.2036715816066135e-05\n",
            "Epoch  11 Batch  219 / 447  Training Loss  1.599924871698022e-05\n",
            "Epoch  11 Batch  220 / 447  Training Loss  1.2188639630039688e-05\n",
            "Epoch  11 Batch  221 / 447  Training Loss  2.2343599994201213e-05\n",
            "Epoch  11 Batch  222 / 447  Training Loss  1.7496297004981898e-05\n",
            "Epoch  11 Batch  223 / 447  Training Loss  1.06838406281895e-05\n",
            "Epoch  11 Batch  224 / 447  Training Loss  2.5648725568316877e-05\n",
            "Epoch  11 Batch  225 / 447  Training Loss  1.883838740468491e-05\n",
            "Epoch  11 Batch  226 / 447  Training Loss  2.4833369025145657e-05\n",
            "Epoch  11 Batch  227 / 447  Training Loss  2.0267951185815036e-05\n",
            "Epoch  11 Batch  228 / 447  Training Loss  1.5305682609323412e-05\n",
            "Epoch  11 Batch  229 / 447  Training Loss  2.1326706701074727e-05\n",
            "Epoch  11 Batch  230 / 447  Training Loss  1.699473796179518e-05\n",
            "Epoch  11 Batch  231 / 447  Training Loss  1.9904302462236956e-05\n",
            "Epoch  11 Batch  232 / 447  Training Loss  1.7854239558801055e-05\n",
            "Epoch  11 Batch  233 / 447  Training Loss  1.5510910088778473e-05\n",
            "Epoch  11 Batch  234 / 447  Training Loss  9.661065632826649e-06\n",
            "Epoch  11 Batch  235 / 447  Training Loss  2.1524325347854756e-05\n",
            "Epoch  11 Batch  236 / 447  Training Loss  1.4531060514855199e-05\n",
            "Epoch  11 Batch  237 / 447  Training Loss  1.750735282257665e-05\n",
            "Epoch  11 Batch  238 / 447  Training Loss  1.3943939848104492e-05\n",
            "Epoch  11 Batch  239 / 447  Training Loss  1.6573369066463783e-05\n",
            "Epoch  11 Batch  240 / 447  Training Loss  1.211961807712214e-05\n",
            "Epoch  11 Batch  241 / 447  Training Loss  1.6223961210926063e-05\n",
            "Epoch  11 Batch  242 / 447  Training Loss  2.1474104869412258e-05\n",
            "Epoch  11 Batch  243 / 447  Training Loss  1.0109946742886677e-05\n",
            "Epoch  11 Batch  244 / 447  Training Loss  2.0246410713298246e-05\n",
            "Epoch  11 Batch  245 / 447  Training Loss  1.409722062817309e-05\n",
            "Epoch  11 Batch  246 / 447  Training Loss  1.376188902213471e-05\n",
            "Epoch  11 Batch  247 / 447  Training Loss  1.3127564670867287e-05\n",
            "Epoch  11 Batch  248 / 447  Training Loss  1.322942080150824e-05\n",
            "Epoch  11 Batch  249 / 447  Training Loss  1.3737322660745122e-05\n",
            "Epoch  11 Batch  250 / 447  Training Loss  1.2358726053207647e-05\n",
            "Epoch  11 Batch  251 / 447  Training Loss  1.2948381481692195e-05\n",
            "Epoch  11 Batch  252 / 447  Training Loss  1.1987002835667226e-05\n",
            "Epoch  11 Batch  253 / 447  Training Loss  1.4587816622224636e-05\n",
            "Epoch  11 Batch  254 / 447  Training Loss  1.5382087440229952e-05\n",
            "Epoch  11 Batch  255 / 447  Training Loss  2.3349410184891894e-05\n",
            "Epoch  11 Batch  256 / 447  Training Loss  1.6393765690736473e-05\n",
            "Epoch  11 Batch  257 / 447  Training Loss  2.0884173864033073e-05\n",
            "Epoch  11 Batch  258 / 447  Training Loss  1.7557100363774225e-05\n",
            "Epoch  11 Batch  259 / 447  Training Loss  1.9007473383680917e-05\n",
            "Epoch  11 Batch  260 / 447  Training Loss  1.5064836588862818e-05\n",
            "Epoch  11 Batch  261 / 447  Training Loss  1.4827283848717343e-05\n",
            "Epoch  11 Batch  262 / 447  Training Loss  1.6817362848087214e-05\n",
            "Epoch  11 Batch  263 / 447  Training Loss  2.8917598683619872e-05\n",
            "Epoch  11 Batch  264 / 447  Training Loss  1.9142838937113993e-05\n",
            "Epoch  11 Batch  265 / 447  Training Loss  1.2373130630294327e-05\n",
            "Epoch  11 Batch  266 / 447  Training Loss  1.5408872059197165e-05\n",
            "Epoch  11 Batch  267 / 447  Training Loss  1.8415796148474328e-05\n",
            "Epoch  11 Batch  268 / 447  Training Loss  2.2047956008464098e-05\n",
            "Epoch  11 Batch  269 / 447  Training Loss  1.5298917787731625e-05\n",
            "Epoch  11 Batch  270 / 447  Training Loss  2.8926915547344834e-05\n",
            "Epoch  11 Batch  271 / 447  Training Loss  2.1436717361211777e-05\n",
            "Epoch  11 Batch  272 / 447  Training Loss  1.670555502641946e-05\n",
            "Epoch  11 Batch  273 / 447  Training Loss  2.938167381216772e-05\n",
            "Epoch  11 Batch  274 / 447  Training Loss  1.8176278899773024e-05\n",
            "Epoch  11 Batch  275 / 447  Training Loss  2.2381134840543382e-05\n",
            "Epoch  11 Batch  276 / 447  Training Loss  1.55088710016571e-05\n",
            "Epoch  11 Batch  277 / 447  Training Loss  2.728244726313278e-05\n",
            "Epoch  11 Batch  278 / 447  Training Loss  1.607405829417985e-05\n",
            "Epoch  11 Batch  279 / 447  Training Loss  1.2669834177359007e-05\n",
            "Epoch  11 Batch  280 / 447  Training Loss  1.5193369108601473e-05\n",
            "Epoch  11 Batch  281 / 447  Training Loss  1.6254911315627396e-05\n",
            "Epoch  11 Batch  282 / 447  Training Loss  1.851460365287494e-05\n",
            "Epoch  11 Batch  283 / 447  Training Loss  2.4506274712621234e-05\n",
            "Epoch  11 Batch  284 / 447  Training Loss  1.6661100744386204e-05\n",
            "Epoch  11 Batch  285 / 447  Training Loss  1.4642522728536278e-05\n",
            "Epoch  11 Batch  286 / 447  Training Loss  1.9573521058191545e-05\n",
            "Epoch  11 Batch  287 / 447  Training Loss  1.6517404219484888e-05\n",
            "Epoch  11 Batch  288 / 447  Training Loss  1.186405097541865e-05\n",
            "Epoch  11 Batch  289 / 447  Training Loss  1.8524619008530863e-05\n",
            "Epoch  11 Batch  290 / 447  Training Loss  1.7599129932932556e-05\n",
            "Epoch  11 Batch  291 / 447  Training Loss  1.2213912668812554e-05\n",
            "Epoch  11 Batch  292 / 447  Training Loss  1.2005944881821051e-05\n",
            "Epoch  11 Batch  293 / 447  Training Loss  2.1260946596157737e-05\n",
            "Epoch  11 Batch  294 / 447  Training Loss  1.18809539344511e-05\n",
            "Epoch  11 Batch  295 / 447  Training Loss  1.5164971046033315e-05\n",
            "Epoch  11 Batch  296 / 447  Training Loss  1.6772040908108465e-05\n",
            "Epoch  11 Batch  297 / 447  Training Loss  1.4615826330555137e-05\n",
            "Epoch  11 Batch  298 / 447  Training Loss  1.550663546368014e-05\n",
            "Epoch  11 Batch  299 / 447  Training Loss  1.5447387340827845e-05\n",
            "Epoch  11 Batch  300 / 447  Training Loss  1.37540864670882e-05\n",
            "Epoch  11 Batch  301 / 447  Training Loss  1.5618526958860457e-05\n",
            "Epoch  11 Batch  302 / 447  Training Loss  1.0511339496588334e-05\n",
            "Epoch  11 Batch  303 / 447  Training Loss  1.6794479961390607e-05\n",
            "Epoch  11 Batch  304 / 447  Training Loss  1.5470181097043678e-05\n",
            "Epoch  11 Batch  305 / 447  Training Loss  1.6269019397441298e-05\n",
            "Epoch  11 Batch  306 / 447  Training Loss  1.5256330698321108e-05\n",
            "Epoch  11 Batch  307 / 447  Training Loss  1.1889925190189388e-05\n",
            "Epoch  11 Batch  308 / 447  Training Loss  1.8111288227373734e-05\n",
            "Epoch  11 Batch  309 / 447  Training Loss  1.5386613085865974e-05\n",
            "Epoch  11 Batch  310 / 447  Training Loss  1.4753183677385096e-05\n",
            "Epoch  11 Batch  311 / 447  Training Loss  1.7554668374941684e-05\n",
            "Epoch  11 Batch  312 / 447  Training Loss  1.2535399037005845e-05\n",
            "Epoch  11 Batch  313 / 447  Training Loss  1.6063859220594168e-05\n",
            "Epoch  11 Batch  314 / 447  Training Loss  1.4615763575420715e-05\n",
            "Epoch  11 Batch  315 / 447  Training Loss  1.860009433585219e-05\n",
            "Epoch  11 Batch  316 / 447  Training Loss  1.7214049876201898e-05\n",
            "Epoch  11 Batch  317 / 447  Training Loss  1.5421757780131884e-05\n",
            "Epoch  11 Batch  318 / 447  Training Loss  1.0008467143052258e-05\n",
            "Epoch  11 Batch  319 / 447  Training Loss  1.6279625924653374e-05\n",
            "Epoch  11 Batch  320 / 447  Training Loss  1.6129110008478165e-05\n",
            "Epoch  11 Batch  321 / 447  Training Loss  1.7311491319560446e-05\n",
            "Epoch  11 Batch  322 / 447  Training Loss  1.2557433365145698e-05\n",
            "Epoch  11 Batch  323 / 447  Training Loss  2.042435335170012e-05\n",
            "Epoch  11 Batch  324 / 447  Training Loss  2.112411857524421e-05\n",
            "Epoch  11 Batch  325 / 447  Training Loss  2.4061240765149705e-05\n",
            "Epoch  11 Batch  326 / 447  Training Loss  1.875027191999834e-05\n",
            "Epoch  11 Batch  327 / 447  Training Loss  1.5667268598917872e-05\n",
            "Epoch  11 Batch  328 / 447  Training Loss  2.436919567117002e-05\n",
            "Epoch  11 Batch  329 / 447  Training Loss  1.2947329196322244e-05\n",
            "Epoch  11 Batch  330 / 447  Training Loss  1.0614061466185376e-05\n",
            "Epoch  11 Batch  331 / 447  Training Loss  1.5645324310753495e-05\n",
            "Epoch  11 Batch  332 / 447  Training Loss  1.5690789950895123e-05\n",
            "Epoch  11 Batch  333 / 447  Training Loss  1.679064916970674e-05\n",
            "Epoch  11 Batch  334 / 447  Training Loss  1.2205767234263476e-05\n",
            "Epoch  11 Batch  335 / 447  Training Loss  1.1935169823118486e-05\n",
            "Epoch  11 Batch  336 / 447  Training Loss  1.9241271729697473e-05\n",
            "Epoch  11 Batch  337 / 447  Training Loss  2.0138924810453318e-05\n",
            "Epoch  11 Batch  338 / 447  Training Loss  1.6503538063261658e-05\n",
            "Epoch  11 Batch  339 / 447  Training Loss  1.872562279459089e-05\n",
            "Epoch  11 Batch  340 / 447  Training Loss  1.2696529665845446e-05\n",
            "Epoch  11 Batch  341 / 447  Training Loss  1.9684486687765457e-05\n",
            "Epoch  11 Batch  342 / 447  Training Loss  1.1834770702989772e-05\n",
            "Epoch  11 Batch  343 / 447  Training Loss  1.8569691746961325e-05\n",
            "Epoch  11 Batch  344 / 447  Training Loss  1.6663911083014682e-05\n",
            "Epoch  11 Batch  345 / 447  Training Loss  1.8503393221180886e-05\n",
            "Epoch  11 Batch  346 / 447  Training Loss  1.9473989596008323e-05\n",
            "Epoch  11 Batch  347 / 447  Training Loss  1.6644869901938364e-05\n",
            "Epoch  11 Batch  348 / 447  Training Loss  1.072601844498422e-05\n",
            "Epoch  11 Batch  349 / 447  Training Loss  1.6224481441895477e-05\n",
            "Epoch  11 Batch  350 / 447  Training Loss  1.1541997992026154e-05\n",
            "Epoch  11 Batch  351 / 447  Training Loss  1.3268493603391107e-05\n",
            "Epoch  11 Batch  352 / 447  Training Loss  2.025930371019058e-05\n",
            "Epoch  11 Batch  353 / 447  Training Loss  1.6746333130868152e-05\n",
            "Epoch  11 Batch  354 / 447  Training Loss  1.2102512300771195e-05\n",
            "Epoch  11 Batch  355 / 447  Training Loss  1.3669129657500889e-05\n",
            "Epoch  11 Batch  356 / 447  Training Loss  1.2984535715077072e-05\n",
            "Epoch  11 Batch  357 / 447  Training Loss  1.621482806513086e-05\n",
            "Epoch  11 Batch  358 / 447  Training Loss  1.4137342077447101e-05\n",
            "Epoch  11 Batch  359 / 447  Training Loss  1.4326277778309304e-05\n",
            "Epoch  11 Batch  360 / 447  Training Loss  1.2550138308142778e-05\n",
            "Epoch  11 Batch  361 / 447  Training Loss  2.164443321817089e-05\n",
            "Epoch  11 Batch  362 / 447  Training Loss  1.1046696272387635e-05\n",
            "Epoch  11 Batch  363 / 447  Training Loss  1.8375032595940866e-05\n",
            "Epoch  11 Batch  364 / 447  Training Loss  1.131436692958232e-05\n",
            "Epoch  11 Batch  365 / 447  Training Loss  1.2452526789274998e-05\n",
            "Epoch  11 Batch  366 / 447  Training Loss  1.6206402506213635e-05\n",
            "Epoch  11 Batch  367 / 447  Training Loss  1.9229211829951964e-05\n",
            "Epoch  11 Batch  368 / 447  Training Loss  1.7271066099056043e-05\n",
            "Epoch  11 Batch  369 / 447  Training Loss  1.903377051348798e-05\n",
            "Epoch  11 Batch  370 / 447  Training Loss  1.9535042156348936e-05\n",
            "Epoch  11 Batch  371 / 447  Training Loss  1.6824831618578173e-05\n",
            "Epoch  11 Batch  372 / 447  Training Loss  1.3801766726828646e-05\n",
            "Epoch  11 Batch  373 / 447  Training Loss  1.682482434262056e-05\n",
            "Epoch  11 Batch  374 / 447  Training Loss  1.676009560469538e-05\n",
            "Epoch  11 Batch  375 / 447  Training Loss  1.2116937796236016e-05\n",
            "Epoch  11 Batch  376 / 447  Training Loss  2.296543425472919e-05\n",
            "Epoch  11 Batch  377 / 447  Training Loss  9.89715954347048e-06\n",
            "Epoch  11 Batch  378 / 447  Training Loss  1.2162094208179042e-05\n",
            "Epoch  11 Batch  379 / 447  Training Loss  1.5182277820713352e-05\n",
            "Epoch  11 Batch  380 / 447  Training Loss  1.1426934179326054e-05\n",
            "Epoch  11 Batch  381 / 447  Training Loss  1.3198515262047295e-05\n",
            "Epoch  11 Batch  382 / 447  Training Loss  1.8609049220685847e-05\n",
            "Epoch  11 Batch  383 / 447  Training Loss  1.7534781363792717e-05\n",
            "Epoch  11 Batch  384 / 447  Training Loss  1.5028090274427086e-05\n",
            "Epoch  11 Batch  385 / 447  Training Loss  1.310994503000984e-05\n",
            "Epoch  11 Batch  386 / 447  Training Loss  1.5076369891176e-05\n",
            "Epoch  11 Batch  387 / 447  Training Loss  1.9173023247276433e-05\n",
            "Epoch  11 Batch  388 / 447  Training Loss  1.2569817045005038e-05\n",
            "Epoch  11 Batch  389 / 447  Training Loss  1.4439829101320356e-05\n",
            "Epoch  11 Batch  390 / 447  Training Loss  1.480116043239832e-05\n",
            "Epoch  11 Batch  391 / 447  Training Loss  1.4919603927410208e-05\n",
            "Epoch  11 Batch  392 / 447  Training Loss  1.414207144989632e-05\n",
            "Epoch  11 Batch  393 / 447  Training Loss  1.1750234989449382e-05\n",
            "Epoch  11 Batch  394 / 447  Training Loss  1.1754820661735721e-05\n",
            "Epoch  11 Batch  395 / 447  Training Loss  1.4185386135068256e-05\n",
            "Epoch  11 Batch  396 / 447  Training Loss  1.735233854560647e-05\n",
            "Epoch  11 Batch  397 / 447  Training Loss  1.53088567458326e-05\n",
            "Epoch  11 Batch  398 / 447  Training Loss  1.8120441382052377e-05\n",
            "Epoch  11 Batch  399 / 447  Training Loss  1.6103071175166406e-05\n",
            "Epoch  11 Batch  400 / 447  Training Loss  1.2993795280635823e-05\n",
            "Epoch  11 Batch  401 / 447  Training Loss  1.4220920093066525e-05\n",
            "Epoch  11 Batch  402 / 447  Training Loss  1.676752435741946e-05\n",
            "Epoch  11 Batch  403 / 447  Training Loss  1.2478372809709981e-05\n",
            "Epoch  11 Batch  404 / 447  Training Loss  1.5929663277347572e-05\n",
            "Epoch  11 Batch  405 / 447  Training Loss  1.4614239262300543e-05\n",
            "Epoch  11 Batch  406 / 447  Training Loss  1.2155902368249372e-05\n",
            "Epoch  11 Batch  407 / 447  Training Loss  1.7634603864280507e-05\n",
            "Epoch  11 Batch  408 / 447  Training Loss  1.5639398043276742e-05\n",
            "Epoch  11 Batch  409 / 447  Training Loss  1.6583762771915644e-05\n",
            "Epoch  11 Batch  410 / 447  Training Loss  1.9419536329223774e-05\n",
            "Epoch  11 Batch  411 / 447  Training Loss  1.1102019016107079e-05\n",
            "Epoch  11 Batch  412 / 447  Training Loss  1.7638762074057013e-05\n",
            "Epoch  11 Batch  413 / 447  Training Loss  1.7303602362517267e-05\n",
            "Epoch  11 Batch  414 / 447  Training Loss  1.132742272602627e-05\n",
            "Epoch  11 Batch  415 / 447  Training Loss  1.1392146916477941e-05\n",
            "Epoch  11 Batch  416 / 447  Training Loss  1.2875065294792876e-05\n",
            "Epoch  11 Batch  417 / 447  Training Loss  1.783359402907081e-05\n",
            "Epoch  11 Batch  418 / 447  Training Loss  1.3486876014212612e-05\n",
            "Epoch  11 Batch  419 / 447  Training Loss  1.6285966921714135e-05\n",
            "Epoch  11 Batch  420 / 447  Training Loss  1.5253449419105891e-05\n",
            "Epoch  11 Batch  421 / 447  Training Loss  1.4788051885261666e-05\n",
            "Epoch  11 Batch  422 / 447  Training Loss  1.4568164260708727e-05\n",
            "Epoch  11 Batch  423 / 447  Training Loss  2.041032348643057e-05\n",
            "Epoch  11 Batch  424 / 447  Training Loss  1.2556735782709438e-05\n",
            "Epoch  11 Batch  425 / 447  Training Loss  1.4062052287044935e-05\n",
            "Epoch  11 Batch  426 / 447  Training Loss  1.342019095318392e-05\n",
            "Epoch  11 Batch  427 / 447  Training Loss  1.0293021659890655e-05\n",
            "Epoch  11 Batch  428 / 447  Training Loss  1.558024450787343e-05\n",
            "Epoch  11 Batch  429 / 447  Training Loss  1.284150766878156e-05\n",
            "Epoch  11 Batch  430 / 447  Training Loss  2.4926701371441595e-05\n",
            "Epoch  11 Batch  431 / 447  Training Loss  1.6136225895024836e-05\n",
            "Epoch  11 Batch  432 / 447  Training Loss  1.3450646292767487e-05\n",
            "Epoch  11 Batch  433 / 447  Training Loss  1.5169685866567306e-05\n",
            "Epoch  11 Batch  434 / 447  Training Loss  1.480409991927445e-05\n",
            "Epoch  11 Batch  435 / 447  Training Loss  1.828508720791433e-05\n",
            "Epoch  11 Batch  436 / 447  Training Loss  1.3223442692833487e-05\n",
            "Epoch  11 Batch  437 / 447  Training Loss  1.3436731933325063e-05\n",
            "Epoch  11 Batch  438 / 447  Training Loss  2.50250959652476e-05\n",
            "Epoch  11 Batch  439 / 447  Training Loss  1.681933645159006e-05\n",
            "Epoch  11 Batch  440 / 447  Training Loss  1.852545392466709e-05\n",
            "Epoch  11 Batch  441 / 447  Training Loss  1.997131585085299e-05\n",
            "Epoch  11 Batch  442 / 447  Training Loss  1.2496044291765429e-05\n",
            "Epoch  11 Batch  443 / 447  Training Loss  1.8566614016890526e-05\n",
            "Epoch  11 Batch  444 / 447  Training Loss  1.3009956091991626e-05\n",
            "Epoch  11 Batch  445 / 447  Training Loss  1.5779554814798757e-05\n",
            "Epoch  11 Batch  446 / 447  Training Loss  1.1074926078435965e-05\n",
            "  12    |    -    |   0.000017   | 99.951019\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 12\n",
            "Epoch  12 Batch  0 / 447  Training Loss  1.762314968800638e-05\n",
            "Epoch  12 Batch  1 / 447  Training Loss  1.673756014497485e-05\n",
            "Epoch  12 Batch  2 / 447  Training Loss  1.2951591088494752e-05\n",
            "Epoch  12 Batch  3 / 447  Training Loss  1.4255255337047856e-05\n",
            "Epoch  12 Batch  4 / 447  Training Loss  1.2410887393343728e-05\n",
            "Epoch  12 Batch  5 / 447  Training Loss  1.8368757082498632e-05\n",
            "Epoch  12 Batch  6 / 447  Training Loss  1.5815734514035285e-05\n",
            "Epoch  12 Batch  7 / 447  Training Loss  1.7147312973975204e-05\n",
            "Epoch  12 Batch  8 / 447  Training Loss  1.3405244317254983e-05\n",
            "Epoch  12 Batch  9 / 447  Training Loss  1.6418245650129393e-05\n",
            "Epoch  12 Batch  10 / 447  Training Loss  1.4193592505762354e-05\n",
            "Epoch  12 Batch  11 / 447  Training Loss  1.6837748262332752e-05\n",
            "Epoch  12 Batch  12 / 447  Training Loss  1.2954771591466852e-05\n",
            "Epoch  12 Batch  13 / 447  Training Loss  1.9466886442387477e-05\n",
            "Epoch  12 Batch  14 / 447  Training Loss  1.0577005923551042e-05\n",
            "Epoch  12 Batch  15 / 447  Training Loss  1.2932842764712404e-05\n",
            "Epoch  12 Batch  16 / 447  Training Loss  1.1004670341208111e-05\n",
            "Epoch  12 Batch  17 / 447  Training Loss  1.7080199540941976e-05\n",
            "Epoch  12 Batch  18 / 447  Training Loss  1.3033731192990672e-05\n",
            "Epoch  12 Batch  19 / 447  Training Loss  1.4759725672774948e-05\n",
            "Epoch  12 Batch  20 / 447  Training Loss  1.5167881429078989e-05\n",
            "Epoch  12 Batch  21 / 447  Training Loss  1.6548176063224673e-05\n",
            "Epoch  12 Batch  22 / 447  Training Loss  1.5443161828443408e-05\n",
            "Epoch  12 Batch  23 / 447  Training Loss  2.011356809816789e-05\n",
            "Epoch  12 Batch  24 / 447  Training Loss  1.283405890717404e-05\n",
            "Epoch  12 Batch  25 / 447  Training Loss  1.3912765098211821e-05\n",
            "Epoch  12 Batch  26 / 447  Training Loss  1.4799625205341727e-05\n",
            "Epoch  12 Batch  27 / 447  Training Loss  1.5681009244872257e-05\n",
            "Epoch  12 Batch  28 / 447  Training Loss  2.090003363264259e-05\n",
            "Epoch  12 Batch  29 / 447  Training Loss  1.923993659147527e-05\n",
            "Epoch  12 Batch  30 / 447  Training Loss  1.1970089872193057e-05\n",
            "Epoch  12 Batch  31 / 447  Training Loss  1.598012931935955e-05\n",
            "Epoch  12 Batch  32 / 447  Training Loss  1.3949469575891271e-05\n",
            "Epoch  12 Batch  33 / 447  Training Loss  1.7046868379111402e-05\n",
            "Epoch  12 Batch  34 / 447  Training Loss  2.510438753233757e-05\n",
            "Epoch  12 Batch  35 / 447  Training Loss  1.4478842786047608e-05\n",
            "Epoch  12 Batch  36 / 447  Training Loss  1.3574882359534968e-05\n",
            "Epoch  12 Batch  37 / 447  Training Loss  1.5080660887178965e-05\n",
            "Epoch  12 Batch  38 / 447  Training Loss  1.8329266822547652e-05\n",
            "Epoch  12 Batch  39 / 447  Training Loss  1.9090975911240093e-05\n",
            "Epoch  12 Batch  40 / 447  Training Loss  1.4283916243584827e-05\n",
            "Epoch  12 Batch  41 / 447  Training Loss  1.4130263480183203e-05\n",
            "Epoch  12 Batch  42 / 447  Training Loss  1.1252277545281686e-05\n",
            "Epoch  12 Batch  43 / 447  Training Loss  1.2619866538443603e-05\n",
            "Epoch  12 Batch  44 / 447  Training Loss  1.653973595239222e-05\n",
            "Epoch  12 Batch  45 / 447  Training Loss  1.66511981660733e-05\n",
            "Epoch  12 Batch  46 / 447  Training Loss  1.5789930330356583e-05\n",
            "Epoch  12 Batch  47 / 447  Training Loss  1.4365962670126464e-05\n",
            "Epoch  12 Batch  48 / 447  Training Loss  1.2699868420895655e-05\n",
            "Epoch  12 Batch  49 / 447  Training Loss  1.4207445929059759e-05\n",
            "Epoch  12 Batch  50 / 447  Training Loss  1.1293855095573235e-05\n",
            "Epoch  12 Batch  51 / 447  Training Loss  1.7869379007606767e-05\n",
            "Epoch  12 Batch  52 / 447  Training Loss  1.1810683645308018e-05\n",
            "Epoch  12 Batch  53 / 447  Training Loss  1.6254480215138756e-05\n",
            "Epoch  12 Batch  54 / 447  Training Loss  1.3336790289031342e-05\n",
            "Epoch  12 Batch  55 / 447  Training Loss  1.1059115422540344e-05\n",
            "Epoch  12 Batch  56 / 447  Training Loss  1.4979863408370875e-05\n",
            "Epoch  12 Batch  57 / 447  Training Loss  1.7950758774532005e-05\n",
            "Epoch  12 Batch  58 / 447  Training Loss  2.3320053514908068e-05\n",
            "Epoch  12 Batch  59 / 447  Training Loss  1.9639475794974715e-05\n",
            "Epoch  12 Batch  60 / 447  Training Loss  1.2460122889024206e-05\n",
            "Epoch  12 Batch  61 / 447  Training Loss  2.1787345758639276e-05\n",
            "Epoch  12 Batch  62 / 447  Training Loss  1.47196878970135e-05\n",
            "Epoch  12 Batch  63 / 447  Training Loss  1.199838243337581e-05\n",
            "Epoch  12 Batch  64 / 447  Training Loss  1.2992294614377897e-05\n",
            "Epoch  12 Batch  65 / 447  Training Loss  2.3712373149464838e-05\n",
            "Epoch  12 Batch  66 / 447  Training Loss  1.5104392332432326e-05\n",
            "Epoch  12 Batch  67 / 447  Training Loss  1.234188584930962e-05\n",
            "Epoch  12 Batch  68 / 447  Training Loss  1.350325510429684e-05\n",
            "Epoch  12 Batch  69 / 447  Training Loss  1.5754920241306536e-05\n",
            "Epoch  12 Batch  70 / 447  Training Loss  1.4404334251594264e-05\n",
            "Epoch  12 Batch  71 / 447  Training Loss  1.4794287380937021e-05\n",
            "Epoch  12 Batch  72 / 447  Training Loss  1.3007196685066447e-05\n",
            "Epoch  12 Batch  73 / 447  Training Loss  1.2389862604322843e-05\n",
            "Epoch  12 Batch  74 / 447  Training Loss  9.849717571341898e-06\n",
            "Epoch  12 Batch  75 / 447  Training Loss  1.2305011296120938e-05\n",
            "Epoch  12 Batch  76 / 447  Training Loss  1.2179053555883002e-05\n",
            "Epoch  12 Batch  77 / 447  Training Loss  1.6998839782900177e-05\n",
            "Epoch  12 Batch  78 / 447  Training Loss  1.3218769709055778e-05\n",
            "Epoch  12 Batch  79 / 447  Training Loss  1.5707479178672656e-05\n",
            "Epoch  12 Batch  80 / 447  Training Loss  1.9262401110609062e-05\n",
            "Epoch  12 Batch  81 / 447  Training Loss  1.782495019142516e-05\n",
            "Epoch  12 Batch  82 / 447  Training Loss  2.4031109205679968e-05\n",
            "Epoch  12 Batch  83 / 447  Training Loss  1.3146972378308419e-05\n",
            "Epoch  12 Batch  84 / 447  Training Loss  1.74707001860952e-05\n",
            "Epoch  12 Batch  85 / 447  Training Loss  1.4906090655131266e-05\n",
            "Epoch  12 Batch  86 / 447  Training Loss  1.820267607399728e-05\n",
            "Epoch  12 Batch  87 / 447  Training Loss  1.599685856490396e-05\n",
            "Epoch  12 Batch  88 / 447  Training Loss  1.3185141142457724e-05\n",
            "Epoch  12 Batch  89 / 447  Training Loss  1.2944422451255377e-05\n",
            "Epoch  12 Batch  90 / 447  Training Loss  1.630965925869532e-05\n",
            "Epoch  12 Batch  91 / 447  Training Loss  1.510247602709569e-05\n",
            "Epoch  12 Batch  92 / 447  Training Loss  1.2412613614287693e-05\n",
            "Epoch  12 Batch  93 / 447  Training Loss  1.450811487302417e-05\n",
            "Epoch  12 Batch  94 / 447  Training Loss  1.1300201549602207e-05\n",
            "Epoch  12 Batch  95 / 447  Training Loss  1.446002261218382e-05\n",
            "Epoch  12 Batch  96 / 447  Training Loss  1.4600682334275916e-05\n",
            "Epoch  12 Batch  97 / 447  Training Loss  1.690910357865505e-05\n",
            "Epoch  12 Batch  98 / 447  Training Loss  1.7591439245734364e-05\n",
            "Epoch  12 Batch  99 / 447  Training Loss  1.743603206705302e-05\n",
            "Epoch  12 Batch  100 / 447  Training Loss  1.1803876077465247e-05\n",
            "Epoch  12 Batch  101 / 447  Training Loss  1.7116584785981104e-05\n",
            "Epoch  12 Batch  102 / 447  Training Loss  1.2026368494844064e-05\n",
            "Epoch  12 Batch  103 / 447  Training Loss  1.5735184206278063e-05\n",
            "Epoch  12 Batch  104 / 447  Training Loss  1.255614188266918e-05\n",
            "Epoch  12 Batch  105 / 447  Training Loss  1.84427644853713e-05\n",
            "Epoch  12 Batch  106 / 447  Training Loss  1.3403189768723678e-05\n",
            "Epoch  12 Batch  107 / 447  Training Loss  1.0952295269817114e-05\n",
            "Epoch  12 Batch  108 / 447  Training Loss  1.2444481399143115e-05\n",
            "Epoch  12 Batch  109 / 447  Training Loss  1.3434323591354769e-05\n",
            "Epoch  12 Batch  110 / 447  Training Loss  1.3451284758048132e-05\n",
            "Epoch  12 Batch  111 / 447  Training Loss  1.3288550690049306e-05\n",
            "Epoch  12 Batch  112 / 447  Training Loss  1.2542172044049948e-05\n",
            "Epoch  12 Batch  113 / 447  Training Loss  1.345495184068568e-05\n",
            "Epoch  12 Batch  114 / 447  Training Loss  1.3594701158581302e-05\n",
            "Epoch  12 Batch  115 / 447  Training Loss  1.1119329428765923e-05\n",
            "Epoch  12 Batch  116 / 447  Training Loss  2.303979817952495e-05\n",
            "Epoch  12 Batch  117 / 447  Training Loss  1.3942136320110876e-05\n",
            "Epoch  12 Batch  118 / 447  Training Loss  1.368756147712702e-05\n",
            "Epoch  12 Batch  119 / 447  Training Loss  1.3740513168158941e-05\n",
            "Epoch  12 Batch  120 / 447  Training Loss  1.2109103408874944e-05\n",
            "Epoch  12 Batch  121 / 447  Training Loss  1.3730303180636838e-05\n",
            "Epoch  12 Batch  122 / 447  Training Loss  1.4501973055303097e-05\n",
            "Epoch  12 Batch  123 / 447  Training Loss  1.3166311873646919e-05\n",
            "Epoch  12 Batch  124 / 447  Training Loss  1.4629396901000291e-05\n",
            "Epoch  12 Batch  125 / 447  Training Loss  2.4814855350996368e-05\n",
            "Epoch  12 Batch  126 / 447  Training Loss  1.810384310374502e-05\n",
            "Epoch  12 Batch  127 / 447  Training Loss  1.3200653484091163e-05\n",
            "Epoch  12 Batch  128 / 447  Training Loss  1.157618135039229e-05\n",
            "Epoch  12 Batch  129 / 447  Training Loss  1.1558612641238142e-05\n",
            "Epoch  12 Batch  130 / 447  Training Loss  1.3275352102937177e-05\n",
            "Epoch  12 Batch  131 / 447  Training Loss  1.9549010175978765e-05\n",
            "Epoch  12 Batch  132 / 447  Training Loss  1.3218417734606192e-05\n",
            "Epoch  12 Batch  133 / 447  Training Loss  1.2349212738627102e-05\n",
            "Epoch  12 Batch  134 / 447  Training Loss  1.268255800823681e-05\n",
            "Epoch  12 Batch  135 / 447  Training Loss  1.8511333109927364e-05\n",
            "Epoch  12 Batch  136 / 447  Training Loss  1.5288833310478367e-05\n",
            "Epoch  12 Batch  137 / 447  Training Loss  1.5620782505720854e-05\n",
            "Epoch  12 Batch  138 / 447  Training Loss  2.0127841708017513e-05\n",
            "Epoch  12 Batch  139 / 447  Training Loss  2.047052294074092e-05\n",
            "Epoch  12 Batch  140 / 447  Training Loss  1.3225416296336334e-05\n",
            "Epoch  12 Batch  141 / 447  Training Loss  1.7794051018427126e-05\n",
            "Epoch  12 Batch  142 / 447  Training Loss  1.3217038031143602e-05\n",
            "Epoch  12 Batch  143 / 447  Training Loss  1.1933468158531468e-05\n",
            "Epoch  12 Batch  144 / 447  Training Loss  1.3221495464676991e-05\n",
            "Epoch  12 Batch  145 / 447  Training Loss  1.4699526218464598e-05\n",
            "Epoch  12 Batch  146 / 447  Training Loss  1.1593046110647265e-05\n",
            "Epoch  12 Batch  147 / 447  Training Loss  1.808643173717428e-05\n",
            "Epoch  12 Batch  148 / 447  Training Loss  9.799498002394103e-06\n",
            "Epoch  12 Batch  149 / 447  Training Loss  1.9122762751067057e-05\n",
            "Epoch  12 Batch  150 / 447  Training Loss  1.2049251381540671e-05\n",
            "Epoch  12 Batch  151 / 447  Training Loss  1.4923510207154322e-05\n",
            "Epoch  12 Batch  152 / 447  Training Loss  1.5115807400434278e-05\n",
            "Epoch  12 Batch  153 / 447  Training Loss  1.9498695110087283e-05\n",
            "Epoch  12 Batch  154 / 447  Training Loss  1.323803371633403e-05\n",
            "Epoch  12 Batch  155 / 447  Training Loss  1.0661061423888896e-05\n",
            "Epoch  12 Batch  156 / 447  Training Loss  2.001185384870041e-05\n",
            "Epoch  12 Batch  157 / 447  Training Loss  1.4454029951593839e-05\n",
            "Epoch  12 Batch  158 / 447  Training Loss  1.5184611584118102e-05\n",
            "Epoch  12 Batch  159 / 447  Training Loss  1.779266312951222e-05\n",
            "Epoch  12 Batch  160 / 447  Training Loss  1.155886820924934e-05\n",
            "Epoch  12 Batch  161 / 447  Training Loss  1.1864098269143142e-05\n",
            "Epoch  12 Batch  162 / 447  Training Loss  1.1758547771023586e-05\n",
            "Epoch  12 Batch  163 / 447  Training Loss  1.8815158910001628e-05\n",
            "Epoch  12 Batch  164 / 447  Training Loss  1.1148603334731888e-05\n",
            "Epoch  12 Batch  165 / 447  Training Loss  1.6019344911910594e-05\n",
            "Epoch  12 Batch  166 / 447  Training Loss  1.0909542652370874e-05\n",
            "Epoch  12 Batch  167 / 447  Training Loss  1.2337612133705989e-05\n",
            "Epoch  12 Batch  168 / 447  Training Loss  1.1710541912179906e-05\n",
            "Epoch  12 Batch  169 / 447  Training Loss  1.601398253114894e-05\n",
            "Epoch  12 Batch  170 / 447  Training Loss  1.320927549386397e-05\n",
            "Epoch  12 Batch  171 / 447  Training Loss  1.4718109014211223e-05\n",
            "Epoch  12 Batch  172 / 447  Training Loss  1.4393011042557191e-05\n",
            "Epoch  12 Batch  173 / 447  Training Loss  1.5473378880415112e-05\n",
            "Epoch  12 Batch  174 / 447  Training Loss  1.5154966604313813e-05\n",
            "Epoch  12 Batch  175 / 447  Training Loss  1.9504615920595825e-05\n",
            "Epoch  12 Batch  176 / 447  Training Loss  1.5154721950239036e-05\n",
            "Epoch  12 Batch  177 / 447  Training Loss  1.2564089956867974e-05\n",
            "Epoch  12 Batch  178 / 447  Training Loss  1.1698279195115902e-05\n",
            "Epoch  12 Batch  179 / 447  Training Loss  1.5721985619165935e-05\n",
            "Epoch  12 Batch  180 / 447  Training Loss  1.6725476598367095e-05\n",
            "Epoch  12 Batch  181 / 447  Training Loss  1.5972396795405075e-05\n",
            "Epoch  12 Batch  182 / 447  Training Loss  1.3985509212943725e-05\n",
            "Epoch  12 Batch  183 / 447  Training Loss  1.233733109984314e-05\n",
            "Epoch  12 Batch  184 / 447  Training Loss  1.4733785974385682e-05\n",
            "Epoch  12 Batch  185 / 447  Training Loss  1.1538992112036794e-05\n",
            "Epoch  12 Batch  186 / 447  Training Loss  1.337079538643593e-05\n",
            "Epoch  12 Batch  187 / 447  Training Loss  1.4136893696559127e-05\n",
            "Epoch  12 Batch  188 / 447  Training Loss  1.1931731023651082e-05\n",
            "Epoch  12 Batch  189 / 447  Training Loss  1.0263321200909559e-05\n",
            "Epoch  12 Batch  190 / 447  Training Loss  1.6562706150580198e-05\n",
            "Epoch  12 Batch  191 / 447  Training Loss  1.7768210454960354e-05\n",
            "Epoch  12 Batch  192 / 447  Training Loss  1.0539362847339362e-05\n",
            "Epoch  12 Batch  193 / 447  Training Loss  1.2347765732556581e-05\n",
            "Epoch  12 Batch  194 / 447  Training Loss  1.3658432180818636e-05\n",
            "Epoch  12 Batch  195 / 447  Training Loss  1.4450508388108574e-05\n",
            "Epoch  12 Batch  196 / 447  Training Loss  1.645709380682092e-05\n",
            "Epoch  12 Batch  197 / 447  Training Loss  1.6745601897127926e-05\n",
            "Epoch  12 Batch  198 / 447  Training Loss  1.7099491742555983e-05\n",
            "Epoch  12 Batch  199 / 447  Training Loss  1.2817411516152788e-05\n",
            "Epoch  12 Batch  200 / 447  Training Loss  1.1537896170921158e-05\n",
            "Epoch  12 Batch  201 / 447  Training Loss  1.7868353097583167e-05\n",
            "Epoch  12 Batch  202 / 447  Training Loss  1.3220200344221666e-05\n",
            "Epoch  12 Batch  203 / 447  Training Loss  1.2405054803821258e-05\n",
            "Epoch  12 Batch  204 / 447  Training Loss  1.3511978067981545e-05\n",
            "Epoch  12 Batch  205 / 447  Training Loss  1.2718694051727653e-05\n",
            "Epoch  12 Batch  206 / 447  Training Loss  2.1292818928486668e-05\n",
            "Epoch  12 Batch  207 / 447  Training Loss  1.205998250952689e-05\n",
            "Epoch  12 Batch  208 / 447  Training Loss  1.286762380914297e-05\n",
            "Epoch  12 Batch  209 / 447  Training Loss  1.2355058061075397e-05\n",
            "Epoch  12 Batch  210 / 447  Training Loss  1.1788066331064329e-05\n",
            "Epoch  12 Batch  211 / 447  Training Loss  1.6196050637518056e-05\n",
            "Epoch  12 Batch  212 / 447  Training Loss  1.0213238056167029e-05\n",
            "Epoch  12 Batch  213 / 447  Training Loss  1.629079270060174e-05\n",
            "Epoch  12 Batch  214 / 447  Training Loss  1.475714634580072e-05\n",
            "Epoch  12 Batch  215 / 447  Training Loss  1.3920926903665531e-05\n",
            "Epoch  12 Batch  216 / 447  Training Loss  1.8972594261867926e-05\n",
            "Epoch  12 Batch  217 / 447  Training Loss  1.6733012671465985e-05\n",
            "Epoch  12 Batch  218 / 447  Training Loss  1.3406647667579819e-05\n",
            "Epoch  12 Batch  219 / 447  Training Loss  1.735188197926618e-05\n",
            "Epoch  12 Batch  220 / 447  Training Loss  9.619101547286846e-06\n",
            "Epoch  12 Batch  221 / 447  Training Loss  1.5131879081309307e-05\n",
            "Epoch  12 Batch  222 / 447  Training Loss  1.2831151252612472e-05\n",
            "Epoch  12 Batch  223 / 447  Training Loss  2.20609872485511e-05\n",
            "Epoch  12 Batch  224 / 447  Training Loss  1.4158103113004472e-05\n",
            "Epoch  12 Batch  225 / 447  Training Loss  1.2322191651037429e-05\n",
            "Epoch  12 Batch  226 / 447  Training Loss  1.4225152881408576e-05\n",
            "Epoch  12 Batch  227 / 447  Training Loss  1.2481626981752925e-05\n",
            "Epoch  12 Batch  228 / 447  Training Loss  1.2747237633448094e-05\n",
            "Epoch  12 Batch  229 / 447  Training Loss  1.1692754924297333e-05\n",
            "Epoch  12 Batch  230 / 447  Training Loss  1.349257399851922e-05\n",
            "Epoch  12 Batch  231 / 447  Training Loss  1.1765098861360457e-05\n",
            "Epoch  12 Batch  232 / 447  Training Loss  1.4075132639845833e-05\n",
            "Epoch  12 Batch  233 / 447  Training Loss  1.85564440471353e-05\n",
            "Epoch  12 Batch  234 / 447  Training Loss  1.6591742678429e-05\n",
            "Epoch  12 Batch  235 / 447  Training Loss  1.3537949598685373e-05\n",
            "Epoch  12 Batch  236 / 447  Training Loss  1.59072078531608e-05\n",
            "Epoch  12 Batch  237 / 447  Training Loss  1.4643447684647981e-05\n",
            "Epoch  12 Batch  238 / 447  Training Loss  9.443047019885853e-06\n",
            "Epoch  12 Batch  239 / 447  Training Loss  1.2531683751149103e-05\n",
            "Epoch  12 Batch  240 / 447  Training Loss  1.3491306162904948e-05\n",
            "Epoch  12 Batch  241 / 447  Training Loss  1.3301158105605282e-05\n",
            "Epoch  12 Batch  242 / 447  Training Loss  1.3711567589780316e-05\n",
            "Epoch  12 Batch  243 / 447  Training Loss  1.19649303087499e-05\n",
            "Epoch  12 Batch  244 / 447  Training Loss  1.2790691471309401e-05\n",
            "Epoch  12 Batch  245 / 447  Training Loss  1.1709434147633146e-05\n",
            "Epoch  12 Batch  246 / 447  Training Loss  1.0535962246649433e-05\n",
            "Epoch  12 Batch  247 / 447  Training Loss  1.401092504238477e-05\n",
            "Epoch  12 Batch  248 / 447  Training Loss  1.4801587894908153e-05\n",
            "Epoch  12 Batch  249 / 447  Training Loss  1.1784955859184265e-05\n",
            "Epoch  12 Batch  250 / 447  Training Loss  1.558549229230266e-05\n",
            "Epoch  12 Batch  251 / 447  Training Loss  1.4860432202112861e-05\n",
            "Epoch  12 Batch  252 / 447  Training Loss  1.584690653544385e-05\n",
            "Epoch  12 Batch  253 / 447  Training Loss  2.5938588805729523e-05\n",
            "Epoch  12 Batch  254 / 447  Training Loss  1.078566401702119e-05\n",
            "Epoch  12 Batch  255 / 447  Training Loss  1.305843579757493e-05\n",
            "Epoch  12 Batch  256 / 447  Training Loss  1.0011791346187238e-05\n",
            "Epoch  12 Batch  257 / 447  Training Loss  1.2663570487347897e-05\n",
            "Epoch  12 Batch  258 / 447  Training Loss  1.549766238895245e-05\n",
            "Epoch  12 Batch  259 / 447  Training Loss  1.0298399502062239e-05\n",
            "Epoch  12 Batch  260 / 447  Training Loss  1.2532304026535712e-05\n",
            "Epoch  12 Batch  261 / 447  Training Loss  1.1684285709634423e-05\n",
            "Epoch  12 Batch  262 / 447  Training Loss  1.6352867532987148e-05\n",
            "Epoch  12 Batch  263 / 447  Training Loss  2.0514586140052415e-05\n",
            "Epoch  12 Batch  264 / 447  Training Loss  1.7025140550686046e-05\n",
            "Epoch  12 Batch  265 / 447  Training Loss  1.4095762708166149e-05\n",
            "Epoch  12 Batch  266 / 447  Training Loss  1.2900604815513361e-05\n",
            "Epoch  12 Batch  267 / 447  Training Loss  1.6016243534977548e-05\n",
            "Epoch  12 Batch  268 / 447  Training Loss  1.2879158930445556e-05\n",
            "Epoch  12 Batch  269 / 447  Training Loss  1.7801077774493024e-05\n",
            "Epoch  12 Batch  270 / 447  Training Loss  1.0416292752779555e-05\n",
            "Epoch  12 Batch  271 / 447  Training Loss  1.1856712262670044e-05\n",
            "Epoch  12 Batch  272 / 447  Training Loss  1.192785202874802e-05\n",
            "Epoch  12 Batch  273 / 447  Training Loss  1.3717749425268266e-05\n",
            "Epoch  12 Batch  274 / 447  Training Loss  8.885269380698446e-06\n",
            "Epoch  12 Batch  275 / 447  Training Loss  1.986530151043553e-05\n",
            "Epoch  12 Batch  276 / 447  Training Loss  1.1153440937050618e-05\n",
            "Epoch  12 Batch  277 / 447  Training Loss  7.901072422100697e-06\n",
            "Epoch  12 Batch  278 / 447  Training Loss  1.7189800928463228e-05\n",
            "Epoch  12 Batch  279 / 447  Training Loss  1.1780317436205223e-05\n",
            "Epoch  12 Batch  280 / 447  Training Loss  1.3195536666898988e-05\n",
            "Epoch  12 Batch  281 / 447  Training Loss  1.0774133443192113e-05\n",
            "Epoch  12 Batch  282 / 447  Training Loss  1.1923201782337856e-05\n",
            "Epoch  12 Batch  283 / 447  Training Loss  1.6860465620993637e-05\n",
            "Epoch  12 Batch  284 / 447  Training Loss  2.0825476894970052e-05\n",
            "Epoch  12 Batch  285 / 447  Training Loss  1.4203380487742834e-05\n",
            "Epoch  12 Batch  286 / 447  Training Loss  1.1490005817904603e-05\n",
            "Epoch  12 Batch  287 / 447  Training Loss  1.4987941540312022e-05\n",
            "Epoch  12 Batch  288 / 447  Training Loss  1.3371583918342367e-05\n",
            "Epoch  12 Batch  289 / 447  Training Loss  1.0450125955685508e-05\n",
            "Epoch  12 Batch  290 / 447  Training Loss  1.5015035387477838e-05\n",
            "Epoch  12 Batch  291 / 447  Training Loss  1.2357466403045692e-05\n",
            "Epoch  12 Batch  292 / 447  Training Loss  1.7600093997316435e-05\n",
            "Epoch  12 Batch  293 / 447  Training Loss  1.4580121387552936e-05\n",
            "Epoch  12 Batch  294 / 447  Training Loss  1.0526394362386782e-05\n",
            "Epoch  12 Batch  295 / 447  Training Loss  1.0783186553453561e-05\n",
            "Epoch  12 Batch  296 / 447  Training Loss  1.607617741683498e-05\n",
            "Epoch  12 Batch  297 / 447  Training Loss  9.31379236135399e-06\n",
            "Epoch  12 Batch  298 / 447  Training Loss  1.0013248356699478e-05\n",
            "Epoch  12 Batch  299 / 447  Training Loss  1.1655357411655132e-05\n",
            "Epoch  12 Batch  300 / 447  Training Loss  2.1156562070245855e-05\n",
            "Epoch  12 Batch  301 / 447  Training Loss  1.680022614891641e-05\n",
            "Epoch  12 Batch  302 / 447  Training Loss  9.23170864552958e-06\n",
            "Epoch  12 Batch  303 / 447  Training Loss  1.1690600331348833e-05\n",
            "Epoch  12 Batch  304 / 447  Training Loss  1.29145264509134e-05\n",
            "Epoch  12 Batch  305 / 447  Training Loss  1.0059872693091165e-05\n",
            "Epoch  12 Batch  306 / 447  Training Loss  1.4185045984049793e-05\n",
            "Epoch  12 Batch  307 / 447  Training Loss  1.0705240129027516e-05\n",
            "Epoch  12 Batch  308 / 447  Training Loss  1.5844676454435103e-05\n",
            "Epoch  12 Batch  309 / 447  Training Loss  1.456773497920949e-05\n",
            "Epoch  12 Batch  310 / 447  Training Loss  1.3001322258787695e-05\n",
            "Epoch  12 Batch  311 / 447  Training Loss  1.4849039871478453e-05\n",
            "Epoch  12 Batch  312 / 447  Training Loss  1.540295488666743e-05\n",
            "Epoch  12 Batch  313 / 447  Training Loss  9.192863217322156e-06\n",
            "Epoch  12 Batch  314 / 447  Training Loss  1.5209997400233988e-05\n",
            "Epoch  12 Batch  315 / 447  Training Loss  1.2650845746975392e-05\n",
            "Epoch  12 Batch  316 / 447  Training Loss  1.4165994798531756e-05\n",
            "Epoch  12 Batch  317 / 447  Training Loss  1.085182429960696e-05\n",
            "Epoch  12 Batch  318 / 447  Training Loss  1.870863161457237e-05\n",
            "Epoch  12 Batch  319 / 447  Training Loss  1.0342831046727952e-05\n",
            "Epoch  12 Batch  320 / 447  Training Loss  1.7350366761093028e-05\n",
            "Epoch  12 Batch  321 / 447  Training Loss  1.996139508264605e-05\n",
            "Epoch  12 Batch  322 / 447  Training Loss  1.2298689398448914e-05\n",
            "Epoch  12 Batch  323 / 447  Training Loss  1.7314307115157135e-05\n",
            "Epoch  12 Batch  324 / 447  Training Loss  1.9279663320048712e-05\n",
            "Epoch  12 Batch  325 / 447  Training Loss  1.2606128620973323e-05\n",
            "Epoch  12 Batch  326 / 447  Training Loss  1.4577858564734925e-05\n",
            "Epoch  12 Batch  327 / 447  Training Loss  1.1515790902194567e-05\n",
            "Epoch  12 Batch  328 / 447  Training Loss  1.038403388520237e-05\n",
            "Epoch  12 Batch  329 / 447  Training Loss  1.2164578038209584e-05\n",
            "Epoch  12 Batch  330 / 447  Training Loss  1.288742532778997e-05\n",
            "Epoch  12 Batch  331 / 447  Training Loss  9.269942893297412e-06\n",
            "Epoch  12 Batch  332 / 447  Training Loss  1.1866455679410137e-05\n",
            "Epoch  12 Batch  333 / 447  Training Loss  1.0335456863685977e-05\n",
            "Epoch  12 Batch  334 / 447  Training Loss  1.8710328731685877e-05\n",
            "Epoch  12 Batch  335 / 447  Training Loss  1.179314585897373e-05\n",
            "Epoch  12 Batch  336 / 447  Training Loss  1.5271252777893096e-05\n",
            "Epoch  12 Batch  337 / 447  Training Loss  1.3705735000257846e-05\n",
            "Epoch  12 Batch  338 / 447  Training Loss  1.156678899860708e-05\n",
            "Epoch  12 Batch  339 / 447  Training Loss  2.4185022994061e-05\n",
            "Epoch  12 Batch  340 / 447  Training Loss  1.793339652067516e-05\n",
            "Epoch  12 Batch  341 / 447  Training Loss  1.628715472179465e-05\n",
            "Epoch  12 Batch  342 / 447  Training Loss  1.1355778951838147e-05\n",
            "Epoch  12 Batch  343 / 447  Training Loss  1.1137054571008775e-05\n",
            "Epoch  12 Batch  344 / 447  Training Loss  1.165625417343108e-05\n",
            "Epoch  12 Batch  345 / 447  Training Loss  1.0639444553817157e-05\n",
            "Epoch  12 Batch  346 / 447  Training Loss  1.190188959299121e-05\n",
            "Epoch  12 Batch  347 / 447  Training Loss  1.1117877875221893e-05\n",
            "Epoch  12 Batch  348 / 447  Training Loss  1.4833358363830484e-05\n",
            "Epoch  12 Batch  349 / 447  Training Loss  1.2884893294540234e-05\n",
            "Epoch  12 Batch  350 / 447  Training Loss  1.2017461813229602e-05\n",
            "Epoch  12 Batch  351 / 447  Training Loss  1.3203080015955493e-05\n",
            "Epoch  12 Batch  352 / 447  Training Loss  1.6008876627893187e-05\n",
            "Epoch  12 Batch  353 / 447  Training Loss  1.4628662029281259e-05\n",
            "Epoch  12 Batch  354 / 447  Training Loss  1.645069096412044e-05\n",
            "Epoch  12 Batch  355 / 447  Training Loss  1.3054898772679735e-05\n",
            "Epoch  12 Batch  356 / 447  Training Loss  1.3458372450259048e-05\n",
            "Epoch  12 Batch  357 / 447  Training Loss  1.3161301467334852e-05\n",
            "Epoch  12 Batch  358 / 447  Training Loss  9.942170436261222e-06\n",
            "Epoch  12 Batch  359 / 447  Training Loss  1.5127712686080486e-05\n",
            "Epoch  12 Batch  360 / 447  Training Loss  1.56431215145858e-05\n",
            "Epoch  12 Batch  361 / 447  Training Loss  1.092993170459522e-05\n",
            "Epoch  12 Batch  362 / 447  Training Loss  1.1283198546152562e-05\n",
            "Epoch  12 Batch  363 / 447  Training Loss  1.6192090697586536e-05\n",
            "Epoch  12 Batch  364 / 447  Training Loss  1.5688599887653254e-05\n",
            "Epoch  12 Batch  365 / 447  Training Loss  2.1756752175861038e-05\n",
            "Epoch  12 Batch  366 / 447  Training Loss  1.2346223229542375e-05\n",
            "Epoch  12 Batch  367 / 447  Training Loss  1.695375431154389e-05\n",
            "Epoch  12 Batch  368 / 447  Training Loss  1.2107017937523779e-05\n",
            "Epoch  12 Batch  369 / 447  Training Loss  1.4501188161375467e-05\n",
            "Epoch  12 Batch  370 / 447  Training Loss  1.4839773029962089e-05\n",
            "Epoch  12 Batch  371 / 447  Training Loss  1.915500615723431e-05\n",
            "Epoch  12 Batch  372 / 447  Training Loss  1.5476120097446255e-05\n",
            "Epoch  12 Batch  373 / 447  Training Loss  1.4709679817315191e-05\n",
            "Epoch  12 Batch  374 / 447  Training Loss  1.7561204003868625e-05\n",
            "Epoch  12 Batch  375 / 447  Training Loss  1.315538065682631e-05\n",
            "Epoch  12 Batch  376 / 447  Training Loss  1.5964038539095782e-05\n",
            "Epoch  12 Batch  377 / 447  Training Loss  9.082892574951984e-06\n",
            "Epoch  12 Batch  378 / 447  Training Loss  1.4816621842328459e-05\n",
            "Epoch  12 Batch  379 / 447  Training Loss  1.0857819688681047e-05\n",
            "Epoch  12 Batch  380 / 447  Training Loss  1.031727788358694e-05\n",
            "Epoch  12 Batch  381 / 447  Training Loss  1.1428299330873415e-05\n",
            "Epoch  12 Batch  382 / 447  Training Loss  1.6683037756592967e-05\n",
            "Epoch  12 Batch  383 / 447  Training Loss  1.428683663107222e-05\n",
            "Epoch  12 Batch  384 / 447  Training Loss  1.4519177057081833e-05\n",
            "Epoch  12 Batch  385 / 447  Training Loss  1.35517630042159e-05\n",
            "Epoch  12 Batch  386 / 447  Training Loss  1.492521187174134e-05\n",
            "Epoch  12 Batch  387 / 447  Training Loss  1.233446801052196e-05\n",
            "Epoch  12 Batch  388 / 447  Training Loss  1.0289812053088099e-05\n",
            "Epoch  12 Batch  389 / 447  Training Loss  1.3093879715597723e-05\n",
            "Epoch  12 Batch  390 / 447  Training Loss  1.3349282198760193e-05\n",
            "Epoch  12 Batch  391 / 447  Training Loss  9.602119462215342e-06\n",
            "Epoch  12 Batch  392 / 447  Training Loss  1.7125003068940714e-05\n",
            "Epoch  12 Batch  393 / 447  Training Loss  1.641608469071798e-05\n",
            "Epoch  12 Batch  394 / 447  Training Loss  1.2366680493869353e-05\n",
            "Epoch  12 Batch  395 / 447  Training Loss  1.5423172953887843e-05\n",
            "Epoch  12 Batch  396 / 447  Training Loss  8.923301720642485e-06\n",
            "Epoch  12 Batch  397 / 447  Training Loss  1.3337094969756436e-05\n",
            "Epoch  12 Batch  398 / 447  Training Loss  1.6366027921321802e-05\n",
            "Epoch  12 Batch  399 / 447  Training Loss  1.132221586885862e-05\n",
            "Epoch  12 Batch  400 / 447  Training Loss  1.9514736777637154e-05\n",
            "Epoch  12 Batch  401 / 447  Training Loss  1.1440473826951347e-05\n",
            "Epoch  12 Batch  402 / 447  Training Loss  1.4072840713197365e-05\n",
            "Epoch  12 Batch  403 / 447  Training Loss  1.248789340024814e-05\n",
            "Epoch  12 Batch  404 / 447  Training Loss  1.5254548998200335e-05\n",
            "Epoch  12 Batch  405 / 447  Training Loss  1.0898645996348932e-05\n",
            "Epoch  12 Batch  406 / 447  Training Loss  1.2603919458342716e-05\n",
            "Epoch  12 Batch  407 / 447  Training Loss  1.3474687875714153e-05\n",
            "Epoch  12 Batch  408 / 447  Training Loss  1.261387387785362e-05\n",
            "Epoch  12 Batch  409 / 447  Training Loss  1.7998090697801672e-05\n",
            "Epoch  12 Batch  410 / 447  Training Loss  8.276718290289864e-06\n",
            "Epoch  12 Batch  411 / 447  Training Loss  1.1011905371560715e-05\n",
            "Epoch  12 Batch  412 / 447  Training Loss  1.7446205674787052e-05\n",
            "Epoch  12 Batch  413 / 447  Training Loss  1.5068222637637518e-05\n",
            "Epoch  12 Batch  414 / 447  Training Loss  1.695062019280158e-05\n",
            "Epoch  12 Batch  415 / 447  Training Loss  1.713037272565998e-05\n",
            "Epoch  12 Batch  416 / 447  Training Loss  1.552217327116523e-05\n",
            "Epoch  12 Batch  417 / 447  Training Loss  1.1234257726755459e-05\n",
            "Epoch  12 Batch  418 / 447  Training Loss  1.4232339708541986e-05\n",
            "Epoch  12 Batch  419 / 447  Training Loss  1.0261795068799984e-05\n",
            "Epoch  12 Batch  420 / 447  Training Loss  1.2528671504696831e-05\n",
            "Epoch  12 Batch  421 / 447  Training Loss  9.423297342436854e-06\n",
            "Epoch  12 Batch  422 / 447  Training Loss  1.709210300759878e-05\n",
            "Epoch  12 Batch  423 / 447  Training Loss  1.301578595302999e-05\n",
            "Epoch  12 Batch  424 / 447  Training Loss  9.491831406194251e-06\n",
            "Epoch  12 Batch  425 / 447  Training Loss  1.237321430380689e-05\n",
            "Epoch  12 Batch  426 / 447  Training Loss  1.2750376299663913e-05\n",
            "Epoch  12 Batch  427 / 447  Training Loss  1.5383033314719796e-05\n",
            "Epoch  12 Batch  428 / 447  Training Loss  9.105158824240789e-06\n",
            "Epoch  12 Batch  429 / 447  Training Loss  1.0433096576889511e-05\n",
            "Epoch  12 Batch  430 / 447  Training Loss  1.5149391401791945e-05\n",
            "Epoch  12 Batch  431 / 447  Training Loss  2.0139957996434532e-05\n",
            "Epoch  12 Batch  432 / 447  Training Loss  1.9859087842633016e-05\n",
            "Epoch  12 Batch  433 / 447  Training Loss  1.0399584425613284e-05\n",
            "Epoch  12 Batch  434 / 447  Training Loss  1.4611588994739577e-05\n",
            "Epoch  12 Batch  435 / 447  Training Loss  1.3938356460130308e-05\n",
            "Epoch  12 Batch  436 / 447  Training Loss  9.73871283349581e-06\n",
            "Epoch  12 Batch  437 / 447  Training Loss  9.878424862108659e-06\n",
            "Epoch  12 Batch  438 / 447  Training Loss  1.5432788131874986e-05\n",
            "Epoch  12 Batch  439 / 447  Training Loss  1.0252515494357795e-05\n",
            "Epoch  12 Batch  440 / 447  Training Loss  1.5446212273673154e-05\n",
            "Epoch  12 Batch  441 / 447  Training Loss  9.766817129275296e-06\n",
            "Epoch  12 Batch  442 / 447  Training Loss  1.6436120858998038e-05\n",
            "Epoch  12 Batch  443 / 447  Training Loss  1.0560616829025093e-05\n",
            "Epoch  12 Batch  444 / 447  Training Loss  1.5304263797588646e-05\n",
            "Epoch  12 Batch  445 / 447  Training Loss  1.1036817340936977e-05\n",
            "Epoch  12 Batch  446 / 447  Training Loss  1.4877072317176498e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTiNbM081LlD",
        "outputId": "54b8c61e-f850-44d5-ce7b-c63a2fdbaef1"
      },
      "source": [
        "# Changing the directory to store the model there.\n",
        "# print(os.getcwd())\n",
        "# os.chdir('/content/drive/My Drive/Colab Notebooks/new/')\n",
        "\n",
        "# print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/NLP-Project\n",
            "/content/drive/My Drive/Colab Notebooks/NLP-Project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3bq66KD1eg7"
      },
      "source": [
        "#### Saving the Model (creating checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fJcvYcG1nd3"
      },
      "source": [
        "PATH = \"fine_tune_10e_25eph.pt\"\n",
        "torch.save({\n",
        "            'epoch': num_of_epochs,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': running_loss,\n",
        "            }, PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuyVxTpoGouA"
      },
      "source": [
        "model_load = T5ForConditionalGeneration.from_pretrained('Best_Masked_Number_Prediction_Model_5e_4.bin', return_dict=True, config='t5-base-config.json')"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elq45fvZvxKR",
        "outputId": "3003d9d6-6bbe-4e67-9d5f-9c31be80fe5c"
      },
      "source": [
        "model_load.to(device)\n",
        "evaluate(model_load, test_dataloader)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.95805369127517"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWLx5DywGtHI"
      },
      "source": [
        "# Function to generate sentences from symptoms on the test dataset\n",
        "def generateText(text):\n",
        "  model_load.eval()\n",
        "  input_ids = tokenizer.encode(text, return_tensors=\"pt\")  # Batch size 1\n",
        "  # input_ids.to(dev)\n",
        "  s = time.time()\n",
        "  outputs = model_load.generate(input_ids)\n",
        "  gen_text=tokenizer.decode(outputs[0]).replace('<pad>','').replace('</s>','')\n",
        "  elapsed = time.time() - s\n",
        "  print('Generated in {} seconds'.format(str(elapsed)[:4]))\n",
        "\n",
        "  return gen_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "KllLHDqnJEUV",
        "outputId": "0bf86bb8-df3f-4712-c294-ee27a06c7a96"
      },
      "source": [
        "data_valid\n",
        "# testing on this for now"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>inputs</th>\n",
              "      <th>target</th>\n",
              "      <th>target_str</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The sum of &lt;extra_id_0&gt;58 and 806 is 1564</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The sum of 755 and 2&lt;extra_id_0&gt;1 is 1016</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The sum of 864 and 95 is &lt;extra_id_0&gt;9</td>\n",
              "      <td>95</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The sum of 263 and 915 is &lt;extra_id_0&gt;178</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The sum of 569 and 460 is 1&lt;extra_id_0&gt;29</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8995</th>\n",
              "      <td>The sum of 745 and 755 is 1&lt;extra_id_0&gt;0</td>\n",
              "      <td>50</td>\n",
              "      <td>50</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8996</th>\n",
              "      <td>The sum of 131 and &lt;extra_id_0&gt;91 is 922</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8997</th>\n",
              "      <td>The sum of 732 and 735 is &lt;extra_id_0&gt;467</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8998</th>\n",
              "      <td>The sum of 4&lt;extra_id_0&gt;5 and 175 is 640</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8999</th>\n",
              "      <td>The sum of 1&lt;extra_id_0&gt;1 and 318 is 419</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>9000 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                         inputs  target target_str\n",
              "0     The sum of <extra_id_0>58 and 806 is 1564       7          7\n",
              "1     The sum of 755 and 2<extra_id_0>1 is 1016       6          6\n",
              "2        The sum of 864 and 95 is <extra_id_0>9      95         95\n",
              "3     The sum of 263 and 915 is <extra_id_0>178       1          1\n",
              "4     The sum of 569 and 460 is 1<extra_id_0>29       0          0\n",
              "...                                         ...     ...        ...\n",
              "8995   The sum of 745 and 755 is 1<extra_id_0>0      50         50\n",
              "8996   The sum of 131 and <extra_id_0>91 is 922       7          7\n",
              "8997  The sum of 732 and 735 is <extra_id_0>467       1          1\n",
              "8998   The sum of 4<extra_id_0>5 and 175 is 640       6          6\n",
              "8999   The sum of 1<extra_id_0>1 and 318 is 419       0          0\n",
              "\n",
              "[9000 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "mMFAn0nSGy2y",
        "outputId": "452f7da0-ce5d-4318-8469-50cfa5a640ec"
      },
      "source": [
        "generateText(\"The sum of 732 and 73<extra_id_0> is 1467\") # example"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated in 0.24 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' 7'"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj7E2DPhfwpf"
      },
      "source": [
        "data_test['predictions'] = data_valid.apply(lambda x: generateText(x))\n",
        "# should try this."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-qnPQqYmxBz"
      },
      "source": [
        "\n",
        "def convert_to_10ebased(number: str, split_type: str, invert_number: bool) -> str:\n",
        "    signal = None\n",
        "    if number[0] == '-':\n",
        "        signal = '-'\n",
        "        number = number[1:]\n",
        "\n",
        "    output = []\n",
        "    for i, digit in enumerate(number[::-1]):\n",
        "        if split_type is None:\n",
        "            output.append('10e' + str(i))\n",
        "        elif split_type == 'underscore':\n",
        "            output.append('10e' + '_'.join(str(i)))\n",
        "        elif split_type == 'character':\n",
        "            output.append(' '.join('D' + str(i) + 'E'))\n",
        "        else:\n",
        "            raise Exception(f'Wrong split_type: {split_type}')\n",
        "        output.append(digit)\n",
        "\n",
        "    if signal:\n",
        "        output.append(signal)\n",
        "\n",
        "    # The output is already inverted. If we want it to _not_ be inverted, then we invert it.\n",
        "    if not invert_number:\n",
        "        output = output[::-1]\n",
        "\n",
        "    return ' '.join(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LSKVJpXRrqj3",
        "outputId": "415fbab8-533d-447a-d54f-c711453e8f3f"
      },
      "source": [
        "convert_to_10ebased(\n",
        "                \"932\", split_type=None, invert_number=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'9 10e2 3 10e1 2 10e0'"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdFMBWGPYcLx"
      },
      "source": [
        "Testing with OOD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5HnhuzcrudM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c120dea9-c3d6-4962-d71f-e636554fb4af"
      },
      "source": [
        "data_ood = pd.read_csv('data_10e_ood_full.csv')\n",
        "print(data_ood.head(5))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              inputs                              target\n",
            "0  The sum of 9 10e3 8 10e2 2 10e1 8 10e0 and 2 1...  1 10e4 2 10e3 2 10e2 6 10e1 5 10e0\n",
            "1  The sum of 3 10e3 9 10e2 0 10e1 4 10e0 and 4 1...         8 10e3 1 10e2 0 10e1 4 10e0\n",
            "2  The sum of 3 10e3 8 10e2 6 10e1 4 10e0 and 2 1...         6 10e3 1 10e2 9 10e1 5 10e0\n",
            "3  The sum of 2 10e3 1 10e2 7 10e1 6 10e0 and 2 1...         4 10e3 2 10e2 2 10e1 7 10e0\n",
            "4  The sum of 8 10e3 2 10e2 2 10e1 1 10e0 and 4 1...  1 10e4 2 10e3 8 10e2 0 10e1 0 10e0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "n1aW3PfjYmf9",
        "outputId": "4322b8e8-a108-49a5-e1ae-186ee0c362e9"
      },
      "source": [
        "seq_len = [len(i.split()) for i in data_ood['inputs']]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa662e2aa90>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnqaqPnAY7pC",
        "outputId": "0be258c4-c95b-44bd-b5bf-101ad329ac35"
      },
      "source": [
        "token_lens_ood = []\n",
        "\n",
        "for txt in data_ood.inputs:\n",
        "  # doubt\n",
        "  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
        "  token_lens_ood.append(len(tokens))\n",
        "\n",
        "max(token_lens_ood)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALXua3mtZJ2B",
        "outputId": "211b44f7-b5ab-4267-de5f-1328b457877e"
      },
      "source": [
        "token_lens_target_ood = []\n",
        "\n",
        "for txt in data_ood.target:\n",
        "  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
        "  token_lens_target_ood.append(len(tokens))\n",
        "\n",
        "max(token_lens_target_ood)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZyxTIlsZW2v",
        "outputId": "59a5297f-74a3-4223-f041-7e0433416747"
      },
      "source": [
        "ood_inputs, ood_masks = get_word_embeddings(data_ood['inputs'], 55)\n",
        "data_ood['target_str'] = data_ood['target'].astype(str)\n",
        "ood_labels = get_word_embeddings(data_ood['target_str'], 30)[0]"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fd0mq9gLaBPF"
      },
      "source": [
        "ood_data = TensorDataset(ood_inputs, ood_masks, ood_labels)\n",
        "ood_dataloader = DataLoader(ood_data, shuffle = True, batch_size = batch_size)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "plFASDdCaiqR",
        "outputId": "79a9edc7-d374-4ed6-add5-a5069def879e"
      },
      "source": [
        "model_load.to(device)\n",
        "evaluate(model_load, ood_dataloader)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6103567625133121"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlpL5uElayw6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}