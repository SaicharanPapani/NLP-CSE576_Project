{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Project_model_finetune_10e (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f039e71570714282be5dc0d9d447b8c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_faee50cd492e48ae8518b92f38126edb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ee708deebf034c00872d6ea63c39c913",
              "IPY_MODEL_b057786eecc9455cbe8aaa84d73dd4c1",
              "IPY_MODEL_ea975d44a6714de8b6fa237184086d7a"
            ]
          }
        },
        "faee50cd492e48ae8518b92f38126edb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ee708deebf034c00872d6ea63c39c913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_dd1b07f4de204424ab18885ac80d265e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6f5fea9d57fa45f3875a7832bd4fa273"
          }
        },
        "b057786eecc9455cbe8aaa84d73dd4c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bed8c02859654233b74b5bcd80bc26ab",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 791656,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 791656,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_71c3580503d64fe487331a31284ab6f3"
          }
        },
        "ea975d44a6714de8b6fa237184086d7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_684c4485990146b182f95b91434b7509",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 773k/773k [00:00&lt;00:00, 1.32MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d30f793d49984fd687c1886c23689d2f"
          }
        },
        "dd1b07f4de204424ab18885ac80d265e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6f5fea9d57fa45f3875a7832bd4fa273": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bed8c02859654233b74b5bcd80bc26ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "71c3580503d64fe487331a31284ab6f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "684c4485990146b182f95b91434b7509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d30f793d49984fd687c1886c23689d2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "63d045c36d28421bb7167847fcf7e9ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_983599e960ba4a8a8629eb69ad49a424",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_42dc56b7d13c4baf95f380ecbcc38bc2",
              "IPY_MODEL_74d8b0d95db34110bdca1d6035934ef8",
              "IPY_MODEL_f1623b1e7ede40d79aad8ff66db340cc"
            ]
          }
        },
        "983599e960ba4a8a8629eb69ad49a424": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "42dc56b7d13c4baf95f380ecbcc38bc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_333c6b2af11a4e998009ddc40682e5ef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2d1b44335d5c46c2a51733ed74fd5152"
          }
        },
        "74d8b0d95db34110bdca1d6035934ef8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bc87379b6b5d47269090da7c8c31e408",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1389353,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1389353,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d5cd24581f040b587c8170f5f1b4095"
          }
        },
        "f1623b1e7ede40d79aad8ff66db340cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0e3555dd090645e0becc8e2f16f46aa8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.32M/1.32M [00:00&lt;00:00, 1.12MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b5631da2769b4f56969948a7205516ae"
          }
        },
        "333c6b2af11a4e998009ddc40682e5ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2d1b44335d5c46c2a51733ed74fd5152": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc87379b6b5d47269090da7c8c31e408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d5cd24581f040b587c8170f5f1b4095": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0e3555dd090645e0becc8e2f16f46aa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b5631da2769b4f56969948a7205516ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "af9048dc086b4390845e631ac8183322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1d01cb4c9f6b41668e021810ba9c9d92",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d16f6de7b5cb403b8fdaca9cb5f5aa21",
              "IPY_MODEL_2c3729898e2c4f6e83d635fae38d3f19",
              "IPY_MODEL_0126777b4de5467c84a90bc444b6a3ff"
            ]
          }
        },
        "1d01cb4c9f6b41668e021810ba9c9d92": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d16f6de7b5cb403b8fdaca9cb5f5aa21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_aadca138c5b645138c72a159cda77569",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb4f19f8ff4a4a28a3a6304897451ad4"
          }
        },
        "2c3729898e2c4f6e83d635fae38d3f19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ebb6628f052e49a7bae8d774c5c77711",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1199,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1199,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f6c1145ff5a424f883620602147e801"
          }
        },
        "0126777b4de5467c84a90bc444b6a3ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b3c9c0c7288141098aa799f8db841582",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.17k/1.17k [00:00&lt;00:00, 21.0kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c38bd18ea28d4667a2227fc67648aac1"
          }
        },
        "aadca138c5b645138c72a159cda77569": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb4f19f8ff4a4a28a3a6304897451ad4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ebb6628f052e49a7bae8d774c5c77711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f6c1145ff5a424f883620602147e801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b3c9c0c7288141098aa799f8db841582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c38bd18ea28d4667a2227fc67648aac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "517ff9c0064c4cb68b8289c89aa84bec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_11316a1bf53d492aa90081b4aeb50719",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_da2e511c593543adb0a2cd2cd76d9083",
              "IPY_MODEL_505211370db34aaf88eb677ae07af859",
              "IPY_MODEL_729bb275b02d4a429aac0415151437f5"
            ]
          }
        },
        "11316a1bf53d492aa90081b4aeb50719": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "da2e511c593543adb0a2cd2cd76d9083": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5cbf15178d344c259aaa8609bef54dc2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "Downloading: 100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8e1dd03ddfba4f7f8426c26fe38f273f"
          }
        },
        "505211370db34aaf88eb677ae07af859": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_42af1183e18948fbb9369a5704a42af5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 891691430,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 891691430,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7698db9afbf9487390537b3ffd0966c8"
          }
        },
        "729bb275b02d4a429aac0415151437f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_457d14b98daf406aa3aaf5f625bef9ad",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 850M/850M [00:29&lt;00:00, 31.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f90abbb0e5e24bf1b9afa0f058b4c704"
          }
        },
        "5cbf15178d344c259aaa8609bef54dc2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8e1dd03ddfba4f7f8426c26fe38f273f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "42af1183e18948fbb9369a5704a42af5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7698db9afbf9487390537b3ffd0966c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "457d14b98daf406aa3aaf5f625bef9ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f90abbb0e5e24bf1b9afa0f058b4c704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFErrTqq_ybT"
      },
      "source": [
        "### Installing neccessary packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3OMB-x8_0vK",
        "outputId": "01f7ed59-1d64-41f9-a560-46cde5e05d08"
      },
      "source": [
        "!pip install transformers\n",
        "# https://huggingface.co/transformers/installation.html\n",
        "!pip install sentencepiece\n",
        "# https://pypi.org/project/sentencepiece/\n",
        "# Python wrapper for SentencePiece. This API will offer the encoding, decoding and training of Sentencepiece.\n",
        "!pip install Cython\n",
        "# https://pypi.org/project/Cython/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 4.4 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 33.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n",
            "\u001b[K     |████████████████████████████████| 61 kB 442 kB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 51.3 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 35.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (0.29.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-AVcK4gBhW7"
      },
      "source": [
        "## Checking the GPU availabilty"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5BIx7Mj1x9M",
        "outputId": "94d8d772-6eaf-4806-abf2-4a0bc7192ace"
      },
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\") \n",
        "    print(\"GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CPU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1wUPLeYJ6GO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49ef1944-5d3c-4290-817a-1519d7290d91"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU-UZe2cBPpq"
      },
      "source": [
        "## Importing the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrGEtltY6SIa"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mlokiVxO7jy0"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "from transformers.optimization import Adafactor \n",
        "import time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "import torch\n",
        "import random\n",
        "import re\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/NLP_Final_Project')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdoJ8keH8pLP",
        "outputId": "be4ca0ba-5398-4604-80d2-e1f1ebbaac80"
      },
      "source": [
        "import pandas as pd\n",
        "# Reading csv\n",
        "data = pd.read_csv('data_10e_full_v1.csv')\n",
        "print(data.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              inputs  ... sumlen\n",
            "0  The sum of 1 10e3 4 10e2 0 10e1 6 10e0 and 7 1...  ...      4\n",
            "1  The sum of 1 10e3 6 10e2 1 10e1 6 10e0 and 1 1...  ...      4\n",
            "2  The sum of 3 10e2 7 10e1 6 10e0 and 1 10e3 6 1...  ...      4\n",
            "3  The sum of 1 10e3 0 10e2 3 10e1 1 10e0 and 4 1...  ...      4\n",
            "4  The sum of 1 10e3 3 10e2 6 10e1 3 10e0 and 5 1...  ...      4\n",
            "\n",
            "[5 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uh3S1gzkGt7E",
        "outputId": "5b4c8b0f-74ad-46ad-cb73-3f1c4b47ac40"
      },
      "source": [
        "data.info"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<bound method DataFrame.info of                                                   inputs  ... sumlen\n",
              "0      The sum of 1 10e3 4 10e2 0 10e1 6 10e0 and 7 1...  ...      4\n",
              "1      The sum of 1 10e3 6 10e2 1 10e1 6 10e0 and 1 1...  ...      4\n",
              "2      The sum of 3 10e2 7 10e1 6 10e0 and 1 10e3 6 1...  ...      4\n",
              "3      The sum of 1 10e3 0 10e2 3 10e1 1 10e0 and 4 1...  ...      4\n",
              "4      The sum of 1 10e3 3 10e2 6 10e1 3 10e0 and 5 1...  ...      4\n",
              "...                                                  ...  ...    ...\n",
              "14964  The sum of 9 10e2 8 10e1 1 10e0 and 6 10e2 9 1...  ...      4\n",
              "14965  The sum of 1 10e3 3 10e2 0 10e1 0 10e0 and 6 1...  ...      4\n",
              "14966  The sum of 1 10e3 6 10e2 6 10e1 0 10e0 and 8 1...  ...      4\n",
              "14967  The sum of 5 10e2 7 10e1 3 10e0 and 1 10e3 0 1...  ...      4\n",
              "14968  The sum of 4 10e2 9 10e1 6 10e0 and 1 10e3 7 1...  ...      4\n",
              "\n",
              "[14969 rows x 5 columns]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YA_luy6aGn45",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4796f58e-16d5-43ad-8629-9be7149ee6b0"
      },
      "source": [
        "data = data.sample(n = 14900, random_state = 42).reset_index(drop=True)\n",
        "len(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14900"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcgHXMgv8606"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Test and validation split\n",
        "train, validation = train_test_split(data, test_size=0.2, random_state=42)\n",
        "train, test = train_test_split(train, test_size=0.3, random_state=42)\n",
        "\n",
        "data_train = train.reset_index(drop=True)\n",
        "data_valid = validation.reset_index(drop=True)\n",
        "data_test = test.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y7YLTIG87LGc",
        "outputId": "f61ab72c-7377-449e-fb3a-c8972319a863"
      },
      "source": [
        "data_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8344, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vohFU6vyHAiq",
        "outputId": "709ae863-3035-4913-bcfc-0b67ca313daa"
      },
      "source": [
        "data_valid.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2980, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGC14qe5OR4R",
        "outputId": "c3ba8714-5a4a-43e4-8fbd-ea926144773b"
      },
      "source": [
        "data_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3576, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ls6YKC19R07"
      },
      "source": [
        "# Initializing Parameters \n",
        "batch_size, num_of_epochs = 32, 32\n",
        "num_of_batches = int(len(data_train)/batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ye-gC3y2YI5g"
      },
      "source": [
        "# Reference\n",
        "# https://huggingface.co/transformers/model_doc/t5.html\n",
        "# https://medium.com/analytics-vidhya/t5-a-detailed-explanation-a0ac9bc53e51\n",
        "# https://towardsdatascience.com/data-to-text-generation-with-t5-building-a-simple-yet-advanced-nlg-model-b5cce5a6df45"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "HngN0xRX1Rk5",
        "outputId": "266fb57b-5318-4c65-b0d3-7b1f1f41414c"
      },
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in data_train['inputs']]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fa9668bbe10>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ad1Lt8c9iDX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f039e71570714282be5dc0d9d447b8c5",
            "faee50cd492e48ae8518b92f38126edb",
            "ee708deebf034c00872d6ea63c39c913",
            "b057786eecc9455cbe8aaa84d73dd4c1",
            "ea975d44a6714de8b6fa237184086d7a",
            "dd1b07f4de204424ab18885ac80d265e",
            "6f5fea9d57fa45f3875a7832bd4fa273",
            "bed8c02859654233b74b5bcd80bc26ab",
            "71c3580503d64fe487331a31284ab6f3",
            "684c4485990146b182f95b91434b7509",
            "d30f793d49984fd687c1886c23689d2f",
            "63d045c36d28421bb7167847fcf7e9ce",
            "983599e960ba4a8a8629eb69ad49a424",
            "42dc56b7d13c4baf95f380ecbcc38bc2",
            "74d8b0d95db34110bdca1d6035934ef8",
            "f1623b1e7ede40d79aad8ff66db340cc",
            "333c6b2af11a4e998009ddc40682e5ef",
            "2d1b44335d5c46c2a51733ed74fd5152",
            "bc87379b6b5d47269090da7c8c31e408",
            "4d5cd24581f040b587c8170f5f1b4095",
            "0e3555dd090645e0becc8e2f16f46aa8",
            "b5631da2769b4f56969948a7205516ae",
            "af9048dc086b4390845e631ac8183322",
            "1d01cb4c9f6b41668e021810ba9c9d92",
            "d16f6de7b5cb403b8fdaca9cb5f5aa21",
            "2c3729898e2c4f6e83d635fae38d3f19",
            "0126777b4de5467c84a90bc444b6a3ff",
            "aadca138c5b645138c72a159cda77569",
            "eb4f19f8ff4a4a28a3a6304897451ad4",
            "ebb6628f052e49a7bae8d774c5c77711",
            "8f6c1145ff5a424f883620602147e801",
            "b3c9c0c7288141098aa799f8db841582",
            "c38bd18ea28d4667a2227fc67648aac1",
            "517ff9c0064c4cb68b8289c89aa84bec",
            "11316a1bf53d492aa90081b4aeb50719",
            "da2e511c593543adb0a2cd2cd76d9083",
            "505211370db34aaf88eb677ae07af859",
            "729bb275b02d4a429aac0415151437f5",
            "5cbf15178d344c259aaa8609bef54dc2",
            "8e1dd03ddfba4f7f8426c26fe38f273f",
            "42af1183e18948fbb9369a5704a42af5",
            "7698db9afbf9487390537b3ffd0966c8",
            "457d14b98daf406aa3aaf5f625bef9ad",
            "f90abbb0e5e24bf1b9afa0f058b4c704"
          ]
        },
        "outputId": "e0c499f9-9c6d-487a-9d90-4a4841a683bb"
      },
      "source": [
        "# T5-base\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base', return_dict=True)\n",
        "# moving the model to device(GPU/CPU)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f039e71570714282be5dc0d9d447b8c5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "63d045c36d28421bb7167847fcf7e9ce",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af9048dc086b4390845e631ac8183322",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "517ff9c0064c4cb68b8289c89aa84bec",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/850M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNRQcyTh3GQf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f30c5c8-83f8-4f38-e8af-f368483cdd63"
      },
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in data_train.inputs:\n",
        "  # doubt\n",
        "  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
        "  token_lens.append(len(tokens))\n",
        "\n",
        "max(token_lens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Efo0J6kA-yt",
        "outputId": "41b56ebe-5c20-4539-89cb-e7dd67d73309"
      },
      "source": [
        "token_lens_target = []\n",
        "\n",
        "for txt in data_train.target:\n",
        "  # doubt\n",
        "  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
        "  token_lens_target.append(len(tokens))\n",
        "\n",
        "max(token_lens_target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "20"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "OT1Us38M2ULV",
        "outputId": "437a25ca-8925-42e8-b8d0-a7fb5ba1d91e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(1,len(token_lens)+1), token_lens)\n",
        "plt.ylabel('length of tokens')\n",
        "plt.show()\n",
        "\n",
        "MAX_LEN = max(token_lens)\n",
        "print(\"Maximum length is: \", MAX_LEN)\n",
        "# when sample with first 40k and last 40k we got the maximum length is 14"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Maximum length is:  42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6rUVKln1Ff5"
      },
      "source": [
        "MAX_LEN = 45"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FqnJxC4u8XO"
      },
      "source": [
        "def get_word_embeddings(data, MAX_LEN=45):\n",
        "  input_ids=[]\n",
        "  attention_masks = []\n",
        "  for sent in data:\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=sent,  \n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "  # Convert lists to tensors\n",
        "  input_ids = torch.tensor(input_ids)\n",
        "  attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "  return input_ids, attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAxmAoDrvrWO",
        "outputId": "76255e3a-849d-4c6a-9ff5-0ef12b4bd08d"
      },
      "source": [
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "train_inputs, train_masks = get_word_embeddings(data_train['inputs'])\n",
        "val_inputs, val_masks = get_word_embeddings(data_valid['inputs'])\n",
        "test_inputs, test_masks = get_word_embeddings(data_test['inputs'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbPJ2deo71p5"
      },
      "source": [
        "data_train['target_str'] = data_train['target'].astype(str)\n",
        "data_valid['target_str'] = data_valid['target'].astype(str)\n",
        "data_test['target_str'] = data_test['target'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3dHakeNv66S",
        "outputId": "5851a716-1444-481f-ddc1-f6ff9c76aba9"
      },
      "source": [
        "#convert lists to tensors\n",
        "train_labels = get_word_embeddings(data_train['target_str'], 25)[0]\n",
        "val_labels = get_word_embeddings(data_valid['target_str'], 25)[0]\n",
        "test_labels = get_word_embeddings(data_test['target_str'], 25)[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHUnt0aF8GQk",
        "outputId": "7a28761d-61c0-4bb9-9468-d24c19f0fcb9"
      },
      "source": [
        "train_labels.shape\n",
        "# doubt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8344, 25])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgegmxBtoqZV",
        "outputId": "35a017b5-fb05-44d6-9242-7b095122e7c4"
      },
      "source": [
        "train_labels"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[209, 335,  15,  ...,   0,   0,   0],\n",
              "        [668, 335,  15,  ...,   0,   0,   0],\n",
              "        [209, 335,  15,  ...,   0,   0,   0],\n",
              "        ...,\n",
              "        [209, 335,  15,  ...,   0,   0,   0],\n",
              "        [204, 335,  15,  ...,   0,   0,   0],\n",
              "        [209, 335,  15,  ...,   0,   0,   0]])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ezRHf2e7rPL",
        "outputId": "67268fe7-ad9a-422d-fbb1-f3112f3f00d6"
      },
      "source": [
        "data_train['target']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       1 10e3 4 10e2 8 10e1 3 10e0\n",
              "1              9 10e2 3 10e1 5 10e0\n",
              "2       1 10e3 3 10e2 6 10e1 4 10e0\n",
              "3       1 10e3 3 10e2 7 10e1 6 10e0\n",
              "4       1 10e3 7 10e2 5 10e1 9 10e0\n",
              "                   ...             \n",
              "8339    1 10e3 4 10e2 7 10e1 8 10e0\n",
              "8340    2 10e3 2 10e2 7 10e1 8 10e0\n",
              "8341    1 10e3 2 10e2 2 10e1 5 10e0\n",
              "8342    2 10e3 3 10e2 8 10e1 4 10e0\n",
              "8343    1 10e3 5 10e2 5 10e1 7 10e0\n",
              "Name: target, Length: 8344, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "osOohece1-tJ",
        "outputId": "6c135d68-beb5-4f56-a1b9-7708ef97c54f"
      },
      "source": [
        "train_inputs.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8344, 45])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjDbWMIlv8Iq"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(train_data, shuffle = True, batch_size = batch_size)\n",
        "\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_dataloader = DataLoader(val_data, shuffle = True, batch_size = batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_dataloader = DataLoader(test_data, shuffle = True, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q88he4D9Lw0L"
      },
      "source": [
        "#  Optimizer\n",
        "# https://huggingface.co/transformers/model_doc/t5.html#overview\n",
        "optimizer = Adafactor(\n",
        "    model.parameters(),\n",
        "    lr=3e-4, # Initializing the learning Rate as suggested in the T5 official documentation\n",
        "    eps=(1e-8, 1e-3),\n",
        "    clip_threshold=1.0,\n",
        "    decay_rate=-0.5,\n",
        "    beta1=None,\n",
        "    weight_decay=0.0,\n",
        "    relative_step=False,\n",
        "    scale_parameter=False,\n",
        "    warmup_init=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ9Bj35-56kQ"
      },
      "source": [
        "# Changing the directory to store the model there.\n",
        "# print(os.getcwd())\n",
        "# os.chdir('/content/drive/MyDrive/Colab Notebooks/new_run')\n",
        "# print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COfO6s0z3kit",
        "outputId": "703b2593-aa61-4816-a88e-641b6f07e7fa"
      },
      "source": [
        "# Loading the configuration file for 't5-base' model\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-12-04 15:21:33--  https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json\n",
            "Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.217.39.238\n",
            "Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.217.39.238|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1199 (1.2K) [application/json]\n",
            "Saving to: ‘t5-base-config.json.6’\n",
            "\n",
            "t5-base-config.json 100%[===================>]   1.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-12-04 15:21:34 (30.3 MB/s) - ‘t5-base-config.json.6’ saved [1199/1199]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYMdr0Xu92Yv"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "# Setting the progress, with html as UI.\n",
        "def progress(loss, value, max=100):\n",
        "    return HTML(\"\"\" Batch loss :{loss}\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(loss=loss,value=value, max=max))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C2w5YpsN7C1"
      },
      "source": [
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    v_accuracy = []\n",
        "    v_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        \n",
        "        v_input_ids, v_attn_mask, v_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # print(v_input_ids.shape, v_labels.shape)\n",
        "\n",
        "        val_outputs = model.generate(input_ids=v_input_ids, attention_mask=v_attn_mask)\n",
        "\n",
        "        val_preds = [ tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            for output in val_outputs]\n",
        "\n",
        "        val_labels = [ tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            for output in v_labels]\n",
        "        \n",
        "        # v_loss.append(val_outputs.loss)\n",
        "\n",
        "        # Get the predictions\n",
        "        # print(val_outputs.logits.shape)\n",
        "        # val_preds = torch.argmax(val_outputs.logits, dim=1).flatten()\n",
        "        # print(val_preds, val_labels)\n",
        "        # Calculate the accuracy rate\n",
        "\n",
        "        val_preds = np.array(val_preds)\n",
        "        val_labels = np.array(val_labels)\n",
        "        accuracy = ((val_preds == val_labels).sum() / len(val_labels)) * 100\n",
        "        v_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    # v_loss = np.mean(v_loss)\n",
        "    v_accuracy = np.mean(v_accuracy)\n",
        "\n",
        "    return v_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlCbmfAQ-CWJ",
        "outputId": "4f37542d-8589-46c0-e432-7f0516219fc7"
      },
      "source": [
        "import gc\n",
        "\n",
        "val_acc = 0\n",
        "train_accuracy = 0\n",
        "\n",
        "# Sets the module in training mode\n",
        "model.train()\n",
        "\n",
        "for epoch in range(1,num_of_epochs+1):\n",
        "  print('Running epoch: {}'.format(epoch))\n",
        "  running_loss=0\n",
        "  # out = display(progress(1, num_of_batches+1), display_id=True)\n",
        "  i =0 \n",
        "  for batch in train_dataloader:\n",
        "    \n",
        "    input_ids, attn_mask, labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    # clear out the gradients of all Variables \n",
        "    optimizer.zero_grad()\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Forward propogation\n",
        "    # print(model(input_ids=input_ids, attention_mask=attn_mask, labels=labels))\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n",
        "    \n",
        "    loss = outputs.loss\n",
        "    loss_num=loss.item()\n",
        "    logits = outputs.logits\n",
        "    running_loss+=loss_num\n",
        "    # out.update(progress(loss_num,i, num_of_batches+1))\n",
        "\n",
        "    # calculating the gradients\n",
        "    loss.backward()\n",
        "\n",
        "    # updating the params\n",
        "    optimizer.step()\n",
        "\n",
        "    print(\"Epoch \", epoch, \"Batch \", i, \"/\", len(train_dataloader), \" Training Loss \", loss_num)\n",
        "    i += 1\n",
        "\n",
        "  running_loss = running_loss/len(train_dataloader)\n",
        "  # v_input_ids, v_attn_mask, v_labels = tuple(t.to(device) for t in data_valid)\n",
        "  \n",
        "  curr_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "  # print('Epoch: {} , Running loss: {}'.format(epoch,running_loss))\n",
        "  print(f\"{epoch + 1:^7} | {'-':^7} | {running_loss:^12.6f} | {curr_accuracy:^9.6f}\")\n",
        "  print(\"-\"*70)\n",
        "\n",
        "  if curr_accuracy > val_acc:\n",
        "    val_acc = curr_accuracy\n",
        "    # Saving the best model\n",
        "    torch.save(model.state_dict(),'FineTune_10e_Model_32eph_5e_4_v3322.bin')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running epoch: 1\n",
            "Epoch  1 Batch  0 / 261  Training Loss  7.266964912414551\n",
            "Epoch  1 Batch  1 / 261  Training Loss  2.8265485763549805\n",
            "Epoch  1 Batch  2 / 261  Training Loss  1.3468682765960693\n",
            "Epoch  1 Batch  3 / 261  Training Loss  1.0255646705627441\n",
            "Epoch  1 Batch  4 / 261  Training Loss  0.966743528842926\n",
            "Epoch  1 Batch  5 / 261  Training Loss  1.2884800434112549\n",
            "Epoch  1 Batch  6 / 261  Training Loss  0.6770210862159729\n",
            "Epoch  1 Batch  7 / 261  Training Loss  0.5691906213760376\n",
            "Epoch  1 Batch  8 / 261  Training Loss  0.5051558017730713\n",
            "Epoch  1 Batch  9 / 261  Training Loss  0.4489794075489044\n",
            "Epoch  1 Batch  10 / 261  Training Loss  0.41639935970306396\n",
            "Epoch  1 Batch  11 / 261  Training Loss  0.40877223014831543\n",
            "Epoch  1 Batch  12 / 261  Training Loss  0.41290661692619324\n",
            "Epoch  1 Batch  13 / 261  Training Loss  0.39036670327186584\n",
            "Epoch  1 Batch  14 / 261  Training Loss  0.3921063542366028\n",
            "Epoch  1 Batch  15 / 261  Training Loss  0.38539278507232666\n",
            "Epoch  1 Batch  16 / 261  Training Loss  0.3806622326374054\n",
            "Epoch  1 Batch  17 / 261  Training Loss  0.35910138487815857\n",
            "Epoch  1 Batch  18 / 261  Training Loss  0.34057068824768066\n",
            "Epoch  1 Batch  19 / 261  Training Loss  0.3443588316440582\n",
            "Epoch  1 Batch  20 / 261  Training Loss  0.38954031467437744\n",
            "Epoch  1 Batch  21 / 261  Training Loss  0.37842559814453125\n",
            "Epoch  1 Batch  22 / 261  Training Loss  0.3708306849002838\n",
            "Epoch  1 Batch  23 / 261  Training Loss  0.3595220446586609\n",
            "Epoch  1 Batch  24 / 261  Training Loss  0.36207589507102966\n",
            "Epoch  1 Batch  25 / 261  Training Loss  0.3426307439804077\n",
            "Epoch  1 Batch  26 / 261  Training Loss  0.3543475270271301\n",
            "Epoch  1 Batch  27 / 261  Training Loss  0.35873162746429443\n",
            "Epoch  1 Batch  28 / 261  Training Loss  0.7933780550956726\n",
            "Epoch  1 Batch  29 / 261  Training Loss  1.5084987878799438\n",
            "Epoch  1 Batch  30 / 261  Training Loss  0.593761682510376\n",
            "Epoch  1 Batch  31 / 261  Training Loss  0.40928393602371216\n",
            "Epoch  1 Batch  32 / 261  Training Loss  0.37250107526779175\n",
            "Epoch  1 Batch  33 / 261  Training Loss  0.3679245114326477\n",
            "Epoch  1 Batch  34 / 261  Training Loss  0.33539748191833496\n",
            "Epoch  1 Batch  35 / 261  Training Loss  0.3491338789463043\n",
            "Epoch  1 Batch  36 / 261  Training Loss  0.3404937982559204\n",
            "Epoch  1 Batch  37 / 261  Training Loss  0.3507370352745056\n",
            "Epoch  1 Batch  38 / 261  Training Loss  0.34164854884147644\n",
            "Epoch  1 Batch  39 / 261  Training Loss  0.3328690230846405\n",
            "Epoch  1 Batch  40 / 261  Training Loss  0.3426368832588196\n",
            "Epoch  1 Batch  41 / 261  Training Loss  0.3484884202480316\n",
            "Epoch  1 Batch  42 / 261  Training Loss  0.33571383357048035\n",
            "Epoch  1 Batch  43 / 261  Training Loss  0.3131028711795807\n",
            "Epoch  1 Batch  44 / 261  Training Loss  0.3319465219974518\n",
            "Epoch  1 Batch  45 / 261  Training Loss  0.3305028975009918\n",
            "Epoch  1 Batch  46 / 261  Training Loss  0.3334755599498749\n",
            "Epoch  1 Batch  47 / 261  Training Loss  0.3309808075428009\n",
            "Epoch  1 Batch  48 / 261  Training Loss  0.31524476408958435\n",
            "Epoch  1 Batch  49 / 261  Training Loss  0.3271041214466095\n",
            "Epoch  1 Batch  50 / 261  Training Loss  0.3180469870567322\n",
            "Epoch  1 Batch  51 / 261  Training Loss  0.3097512125968933\n",
            "Epoch  1 Batch  52 / 261  Training Loss  0.31862977147102356\n",
            "Epoch  1 Batch  53 / 261  Training Loss  0.3214910626411438\n",
            "Epoch  1 Batch  54 / 261  Training Loss  0.3052782416343689\n",
            "Epoch  1 Batch  55 / 261  Training Loss  0.30881938338279724\n",
            "Epoch  1 Batch  56 / 261  Training Loss  0.33825889229774475\n",
            "Epoch  1 Batch  57 / 261  Training Loss  0.32564008235931396\n",
            "Epoch  1 Batch  58 / 261  Training Loss  0.32104742527008057\n",
            "Epoch  1 Batch  59 / 261  Training Loss  0.3572709262371063\n",
            "Epoch  1 Batch  60 / 261  Training Loss  0.3474125266075134\n",
            "Epoch  1 Batch  61 / 261  Training Loss  0.3314553499221802\n",
            "Epoch  1 Batch  62 / 261  Training Loss  0.312969446182251\n",
            "Epoch  1 Batch  63 / 261  Training Loss  0.3208600580692291\n",
            "Epoch  1 Batch  64 / 261  Training Loss  0.30753910541534424\n",
            "Epoch  1 Batch  65 / 261  Training Loss  0.3147180676460266\n",
            "Epoch  1 Batch  66 / 261  Training Loss  0.31160277128219604\n",
            "Epoch  1 Batch  67 / 261  Training Loss  0.3163967430591583\n",
            "Epoch  1 Batch  68 / 261  Training Loss  0.30065596103668213\n",
            "Epoch  1 Batch  69 / 261  Training Loss  0.32954826951026917\n",
            "Epoch  1 Batch  70 / 261  Training Loss  0.30071139335632324\n",
            "Epoch  1 Batch  71 / 261  Training Loss  0.3141343593597412\n",
            "Epoch  1 Batch  72 / 261  Training Loss  0.3074893653392792\n",
            "Epoch  1 Batch  73 / 261  Training Loss  0.3020673394203186\n",
            "Epoch  1 Batch  74 / 261  Training Loss  0.3000484108924866\n",
            "Epoch  1 Batch  75 / 261  Training Loss  0.2969007194042206\n",
            "Epoch  1 Batch  76 / 261  Training Loss  0.28375494480133057\n",
            "Epoch  1 Batch  77 / 261  Training Loss  0.3266237676143646\n",
            "Epoch  1 Batch  78 / 261  Training Loss  0.31432920694351196\n",
            "Epoch  1 Batch  79 / 261  Training Loss  0.3034598231315613\n",
            "Epoch  1 Batch  80 / 261  Training Loss  0.2903384864330292\n",
            "Epoch  1 Batch  81 / 261  Training Loss  0.2822749614715576\n",
            "Epoch  1 Batch  82 / 261  Training Loss  0.2984234094619751\n",
            "Epoch  1 Batch  83 / 261  Training Loss  0.30223309993743896\n",
            "Epoch  1 Batch  84 / 261  Training Loss  0.2942928373813629\n",
            "Epoch  1 Batch  85 / 261  Training Loss  0.2966676354408264\n",
            "Epoch  1 Batch  86 / 261  Training Loss  0.30084675550460815\n",
            "Epoch  1 Batch  87 / 261  Training Loss  0.26807111501693726\n",
            "Epoch  1 Batch  88 / 261  Training Loss  0.3044677972793579\n",
            "Epoch  1 Batch  89 / 261  Training Loss  0.2859548032283783\n",
            "Epoch  1 Batch  90 / 261  Training Loss  0.30774828791618347\n",
            "Epoch  1 Batch  91 / 261  Training Loss  0.3099907636642456\n",
            "Epoch  1 Batch  92 / 261  Training Loss  0.2961706519126892\n",
            "Epoch  1 Batch  93 / 261  Training Loss  0.29572033882141113\n",
            "Epoch  1 Batch  94 / 261  Training Loss  0.3191584646701813\n",
            "Epoch  1 Batch  95 / 261  Training Loss  0.6756267547607422\n",
            "Epoch  1 Batch  96 / 261  Training Loss  0.3269568383693695\n",
            "Epoch  1 Batch  97 / 261  Training Loss  0.3140801787376404\n",
            "Epoch  1 Batch  98 / 261  Training Loss  0.29446229338645935\n",
            "Epoch  1 Batch  99 / 261  Training Loss  0.28432610630989075\n",
            "Epoch  1 Batch  100 / 261  Training Loss  0.3204004168510437\n",
            "Epoch  1 Batch  101 / 261  Training Loss  0.29305458068847656\n",
            "Epoch  1 Batch  102 / 261  Training Loss  0.3058914244174957\n",
            "Epoch  1 Batch  103 / 261  Training Loss  0.28275370597839355\n",
            "Epoch  1 Batch  104 / 261  Training Loss  0.28501489758491516\n",
            "Epoch  1 Batch  105 / 261  Training Loss  0.2826710641384125\n",
            "Epoch  1 Batch  106 / 261  Training Loss  0.2982151210308075\n",
            "Epoch  1 Batch  107 / 261  Training Loss  0.2752145528793335\n",
            "Epoch  1 Batch  108 / 261  Training Loss  0.2923862636089325\n",
            "Epoch  1 Batch  109 / 261  Training Loss  0.2889709770679474\n",
            "Epoch  1 Batch  110 / 261  Training Loss  0.2681735157966614\n",
            "Epoch  1 Batch  111 / 261  Training Loss  0.2777312099933624\n",
            "Epoch  1 Batch  112 / 261  Training Loss  0.29341766238212585\n",
            "Epoch  1 Batch  113 / 261  Training Loss  0.2875213027000427\n",
            "Epoch  1 Batch  114 / 261  Training Loss  0.2535766065120697\n",
            "Epoch  1 Batch  115 / 261  Training Loss  0.27386102080345154\n",
            "Epoch  1 Batch  116 / 261  Training Loss  0.27770039439201355\n",
            "Epoch  1 Batch  117 / 261  Training Loss  0.28216832876205444\n",
            "Epoch  1 Batch  118 / 261  Training Loss  0.2823319733142853\n",
            "Epoch  1 Batch  119 / 261  Training Loss  0.2684779465198517\n",
            "Epoch  1 Batch  120 / 261  Training Loss  0.2642049491405487\n",
            "Epoch  1 Batch  121 / 261  Training Loss  0.2676752507686615\n",
            "Epoch  1 Batch  122 / 261  Training Loss  0.2529788017272949\n",
            "Epoch  1 Batch  123 / 261  Training Loss  0.2746547758579254\n",
            "Epoch  1 Batch  124 / 261  Training Loss  0.243323415517807\n",
            "Epoch  1 Batch  125 / 261  Training Loss  0.24786871671676636\n",
            "Epoch  1 Batch  126 / 261  Training Loss  0.28982335329055786\n",
            "Epoch  1 Batch  127 / 261  Training Loss  0.26736247539520264\n",
            "Epoch  1 Batch  128 / 261  Training Loss  0.256066232919693\n",
            "Epoch  1 Batch  129 / 261  Training Loss  0.29262134432792664\n",
            "Epoch  1 Batch  130 / 261  Training Loss  0.28265678882598877\n",
            "Epoch  1 Batch  131 / 261  Training Loss  0.2619113326072693\n",
            "Epoch  1 Batch  132 / 261  Training Loss  0.2275090366601944\n",
            "Epoch  1 Batch  133 / 261  Training Loss  0.25305435061454773\n",
            "Epoch  1 Batch  134 / 261  Training Loss  0.2731371223926544\n",
            "Epoch  1 Batch  135 / 261  Training Loss  0.2631377875804901\n",
            "Epoch  1 Batch  136 / 261  Training Loss  0.2390213906764984\n",
            "Epoch  1 Batch  137 / 261  Training Loss  0.24823440611362457\n",
            "Epoch  1 Batch  138 / 261  Training Loss  0.25474950671195984\n",
            "Epoch  1 Batch  139 / 261  Training Loss  0.2585482597351074\n",
            "Epoch  1 Batch  140 / 261  Training Loss  0.24986964464187622\n",
            "Epoch  1 Batch  141 / 261  Training Loss  0.23495331406593323\n",
            "Epoch  1 Batch  142 / 261  Training Loss  0.23713909089565277\n",
            "Epoch  1 Batch  143 / 261  Training Loss  0.25218045711517334\n",
            "Epoch  1 Batch  144 / 261  Training Loss  0.243313267827034\n",
            "Epoch  1 Batch  145 / 261  Training Loss  0.24386632442474365\n",
            "Epoch  1 Batch  146 / 261  Training Loss  0.2805807888507843\n",
            "Epoch  1 Batch  147 / 261  Training Loss  0.24562616646289825\n",
            "Epoch  1 Batch  148 / 261  Training Loss  0.24817770719528198\n",
            "Epoch  1 Batch  149 / 261  Training Loss  0.2579047381877899\n",
            "Epoch  1 Batch  150 / 261  Training Loss  0.24874180555343628\n",
            "Epoch  1 Batch  151 / 261  Training Loss  0.22453568875789642\n",
            "Epoch  1 Batch  152 / 261  Training Loss  0.2588130831718445\n",
            "Epoch  1 Batch  153 / 261  Training Loss  0.23388294875621796\n",
            "Epoch  1 Batch  154 / 261  Training Loss  0.24499697983264923\n",
            "Epoch  1 Batch  155 / 261  Training Loss  0.23466211557388306\n",
            "Epoch  1 Batch  156 / 261  Training Loss  0.22825759649276733\n",
            "Epoch  1 Batch  157 / 261  Training Loss  0.24082647264003754\n",
            "Epoch  1 Batch  158 / 261  Training Loss  0.2394266128540039\n",
            "Epoch  1 Batch  159 / 261  Training Loss  0.2423863410949707\n",
            "Epoch  1 Batch  160 / 261  Training Loss  0.22147975862026215\n",
            "Epoch  1 Batch  161 / 261  Training Loss  0.20733270049095154\n",
            "Epoch  1 Batch  162 / 261  Training Loss  0.20946598052978516\n",
            "Epoch  1 Batch  163 / 261  Training Loss  0.24813680350780487\n",
            "Epoch  1 Batch  164 / 261  Training Loss  0.23420052230358124\n",
            "Epoch  1 Batch  165 / 261  Training Loss  0.22029972076416016\n",
            "Epoch  1 Batch  166 / 261  Training Loss  0.24426241219043732\n",
            "Epoch  1 Batch  167 / 261  Training Loss  0.23373304307460785\n",
            "Epoch  1 Batch  168 / 261  Training Loss  0.22357942163944244\n",
            "Epoch  1 Batch  169 / 261  Training Loss  0.23874814808368683\n",
            "Epoch  1 Batch  170 / 261  Training Loss  0.2385057955980301\n",
            "Epoch  1 Batch  171 / 261  Training Loss  0.1975211203098297\n",
            "Epoch  1 Batch  172 / 261  Training Loss  0.22384457290172577\n",
            "Epoch  1 Batch  173 / 261  Training Loss  0.20197005569934845\n",
            "Epoch  1 Batch  174 / 261  Training Loss  0.20465165376663208\n",
            "Epoch  1 Batch  175 / 261  Training Loss  0.24502462148666382\n",
            "Epoch  1 Batch  176 / 261  Training Loss  0.22766602039337158\n",
            "Epoch  1 Batch  177 / 261  Training Loss  0.21622836589813232\n",
            "Epoch  1 Batch  178 / 261  Training Loss  0.22224174439907074\n",
            "Epoch  1 Batch  179 / 261  Training Loss  0.2342292219400406\n",
            "Epoch  1 Batch  180 / 261  Training Loss  0.20921017229557037\n",
            "Epoch  1 Batch  181 / 261  Training Loss  0.20685577392578125\n",
            "Epoch  1 Batch  182 / 261  Training Loss  0.22391995787620544\n",
            "Epoch  1 Batch  183 / 261  Training Loss  0.19117361307144165\n",
            "Epoch  1 Batch  184 / 261  Training Loss  0.21432693302631378\n",
            "Epoch  1 Batch  185 / 261  Training Loss  0.2014620006084442\n",
            "Epoch  1 Batch  186 / 261  Training Loss  0.2352946251630783\n",
            "Epoch  1 Batch  187 / 261  Training Loss  0.20231562852859497\n",
            "Epoch  1 Batch  188 / 261  Training Loss  0.1902192234992981\n",
            "Epoch  1 Batch  189 / 261  Training Loss  0.1963464766740799\n",
            "Epoch  1 Batch  190 / 261  Training Loss  0.18204936385154724\n",
            "Epoch  1 Batch  191 / 261  Training Loss  0.19120408594608307\n",
            "Epoch  1 Batch  192 / 261  Training Loss  0.20458726584911346\n",
            "Epoch  1 Batch  193 / 261  Training Loss  0.19770678877830505\n",
            "Epoch  1 Batch  194 / 261  Training Loss  0.17918334901332855\n",
            "Epoch  1 Batch  195 / 261  Training Loss  0.1874890923500061\n",
            "Epoch  1 Batch  196 / 261  Training Loss  0.1785544604063034\n",
            "Epoch  1 Batch  197 / 261  Training Loss  0.19648094475269318\n",
            "Epoch  1 Batch  198 / 261  Training Loss  0.19393640756607056\n",
            "Epoch  1 Batch  199 / 261  Training Loss  0.19459398090839386\n",
            "Epoch  1 Batch  200 / 261  Training Loss  0.22827163338661194\n",
            "Epoch  1 Batch  201 / 261  Training Loss  0.19445815682411194\n",
            "Epoch  1 Batch  202 / 261  Training Loss  0.18817681074142456\n",
            "Epoch  1 Batch  203 / 261  Training Loss  0.19454512000083923\n",
            "Epoch  1 Batch  204 / 261  Training Loss  0.18854081630706787\n",
            "Epoch  1 Batch  205 / 261  Training Loss  0.22107824683189392\n",
            "Epoch  1 Batch  206 / 261  Training Loss  0.1991361677646637\n",
            "Epoch  1 Batch  207 / 261  Training Loss  0.18274928629398346\n",
            "Epoch  1 Batch  208 / 261  Training Loss  0.1892479658126831\n",
            "Epoch  1 Batch  209 / 261  Training Loss  0.1748952716588974\n",
            "Epoch  1 Batch  210 / 261  Training Loss  0.17840120196342468\n",
            "Epoch  1 Batch  211 / 261  Training Loss  0.1597856879234314\n",
            "Epoch  1 Batch  212 / 261  Training Loss  0.18624359369277954\n",
            "Epoch  1 Batch  213 / 261  Training Loss  0.1891350895166397\n",
            "Epoch  1 Batch  214 / 261  Training Loss  0.21121037006378174\n",
            "Epoch  1 Batch  215 / 261  Training Loss  0.16513097286224365\n",
            "Epoch  1 Batch  216 / 261  Training Loss  0.16656450927257538\n",
            "Epoch  1 Batch  217 / 261  Training Loss  0.18307490646839142\n",
            "Epoch  1 Batch  218 / 261  Training Loss  0.1779281198978424\n",
            "Epoch  1 Batch  219 / 261  Training Loss  0.20005589723587036\n",
            "Epoch  1 Batch  220 / 261  Training Loss  0.1964433640241623\n",
            "Epoch  1 Batch  221 / 261  Training Loss  0.1860886961221695\n",
            "Epoch  1 Batch  222 / 261  Training Loss  0.1784614771604538\n",
            "Epoch  1 Batch  223 / 261  Training Loss  0.18927830457687378\n",
            "Epoch  1 Batch  224 / 261  Training Loss  0.18624648451805115\n",
            "Epoch  1 Batch  225 / 261  Training Loss  0.19535356760025024\n",
            "Epoch  1 Batch  226 / 261  Training Loss  0.17771221697330475\n",
            "Epoch  1 Batch  227 / 261  Training Loss  0.19164413213729858\n",
            "Epoch  1 Batch  228 / 261  Training Loss  0.20215415954589844\n",
            "Epoch  1 Batch  229 / 261  Training Loss  0.18056917190551758\n",
            "Epoch  1 Batch  230 / 261  Training Loss  0.18747542798519135\n",
            "Epoch  1 Batch  231 / 261  Training Loss  0.17144832015037537\n",
            "Epoch  1 Batch  232 / 261  Training Loss  0.15301883220672607\n",
            "Epoch  1 Batch  233 / 261  Training Loss  0.17289841175079346\n",
            "Epoch  1 Batch  234 / 261  Training Loss  0.16823747754096985\n",
            "Epoch  1 Batch  235 / 261  Training Loss  0.15312212705612183\n",
            "Epoch  1 Batch  236 / 261  Training Loss  0.16307485103607178\n",
            "Epoch  1 Batch  237 / 261  Training Loss  0.1721685528755188\n",
            "Epoch  1 Batch  238 / 261  Training Loss  0.16965803503990173\n",
            "Epoch  1 Batch  239 / 261  Training Loss  0.16319473087787628\n",
            "Epoch  1 Batch  240 / 261  Training Loss  0.15825173258781433\n",
            "Epoch  1 Batch  241 / 261  Training Loss  0.16450105607509613\n",
            "Epoch  1 Batch  242 / 261  Training Loss  0.17955850064754486\n",
            "Epoch  1 Batch  243 / 261  Training Loss  0.17396511137485504\n",
            "Epoch  1 Batch  244 / 261  Training Loss  0.15333348512649536\n",
            "Epoch  1 Batch  245 / 261  Training Loss  0.15959353744983673\n",
            "Epoch  1 Batch  246 / 261  Training Loss  0.16435950994491577\n",
            "Epoch  1 Batch  247 / 261  Training Loss  0.15504886209964752\n",
            "Epoch  1 Batch  248 / 261  Training Loss  0.16274233162403107\n",
            "Epoch  1 Batch  249 / 261  Training Loss  0.14784836769104004\n",
            "Epoch  1 Batch  250 / 261  Training Loss  0.18110941350460052\n",
            "Epoch  1 Batch  251 / 261  Training Loss  0.17649853229522705\n",
            "Epoch  1 Batch  252 / 261  Training Loss  0.15802820026874542\n",
            "Epoch  1 Batch  253 / 261  Training Loss  0.17698998749256134\n",
            "Epoch  1 Batch  254 / 261  Training Loss  0.14011549949645996\n",
            "Epoch  1 Batch  255 / 261  Training Loss  0.15489573776721954\n",
            "Epoch  1 Batch  256 / 261  Training Loss  0.15101923048496246\n",
            "Epoch  1 Batch  257 / 261  Training Loss  0.15757544338703156\n",
            "Epoch  1 Batch  258 / 261  Training Loss  0.14645491540431976\n",
            "Epoch  1 Batch  259 / 261  Training Loss  0.14656373858451843\n",
            "Epoch  1 Batch  260 / 261  Training Loss  0.14316147565841675\n",
            "   2    |    -    |   0.321243   | 23.503989\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 2\n",
            "Epoch  2 Batch  0 / 261  Training Loss  0.09423534572124481\n",
            "Epoch  2 Batch  1 / 261  Training Loss  0.09929367154836655\n",
            "Epoch  2 Batch  2 / 261  Training Loss  0.10512755066156387\n",
            "Epoch  2 Batch  3 / 261  Training Loss  0.08986032754182816\n",
            "Epoch  2 Batch  4 / 261  Training Loss  0.13433557748794556\n",
            "Epoch  2 Batch  5 / 261  Training Loss  0.14977657794952393\n",
            "Epoch  2 Batch  6 / 261  Training Loss  0.14713311195373535\n",
            "Epoch  2 Batch  7 / 261  Training Loss  0.1343502700328827\n",
            "Epoch  2 Batch  8 / 261  Training Loss  0.09283317625522614\n",
            "Epoch  2 Batch  9 / 261  Training Loss  0.09401562064886093\n",
            "Epoch  2 Batch  10 / 261  Training Loss  0.09748531132936478\n",
            "Epoch  2 Batch  11 / 261  Training Loss  0.08603544533252716\n",
            "Epoch  2 Batch  12 / 261  Training Loss  0.07833384722471237\n",
            "Epoch  2 Batch  13 / 261  Training Loss  0.07802850008010864\n",
            "Epoch  2 Batch  14 / 261  Training Loss  0.14594358205795288\n",
            "Epoch  2 Batch  15 / 261  Training Loss  0.12899746000766754\n",
            "Epoch  2 Batch  16 / 261  Training Loss  0.09486699104309082\n",
            "Epoch  2 Batch  17 / 261  Training Loss  0.09317609667778015\n",
            "Epoch  2 Batch  18 / 261  Training Loss  0.08913729339838028\n",
            "Epoch  2 Batch  19 / 261  Training Loss  0.08999533951282501\n",
            "Epoch  2 Batch  20 / 261  Training Loss  0.08691606670618057\n",
            "Epoch  2 Batch  21 / 261  Training Loss  0.07747206091880798\n",
            "Epoch  2 Batch  22 / 261  Training Loss  0.08355572074651718\n",
            "Epoch  2 Batch  23 / 261  Training Loss  0.07185700535774231\n",
            "Epoch  2 Batch  24 / 261  Training Loss  0.07325469702482224\n",
            "Epoch  2 Batch  25 / 261  Training Loss  0.07865128666162491\n",
            "Epoch  2 Batch  26 / 261  Training Loss  0.0841074287891388\n",
            "Epoch  2 Batch  27 / 261  Training Loss  0.11280990391969681\n",
            "Epoch  2 Batch  28 / 261  Training Loss  0.09324180334806442\n",
            "Epoch  2 Batch  29 / 261  Training Loss  0.1159278005361557\n",
            "Epoch  2 Batch  30 / 261  Training Loss  0.09884463250637054\n",
            "Epoch  2 Batch  31 / 261  Training Loss  0.06783732771873474\n",
            "Epoch  2 Batch  32 / 261  Training Loss  0.06457394361495972\n",
            "Epoch  2 Batch  33 / 261  Training Loss  0.07906890660524368\n",
            "Epoch  2 Batch  34 / 261  Training Loss  0.08959454298019409\n",
            "Epoch  2 Batch  35 / 261  Training Loss  0.09281216561794281\n",
            "Epoch  2 Batch  36 / 261  Training Loss  0.07419350743293762\n",
            "Epoch  2 Batch  37 / 261  Training Loss  0.07971720397472382\n",
            "Epoch  2 Batch  38 / 261  Training Loss  0.07958145439624786\n",
            "Epoch  2 Batch  39 / 261  Training Loss  0.06728341430425644\n",
            "Epoch  2 Batch  40 / 261  Training Loss  0.05365947261452675\n",
            "Epoch  2 Batch  41 / 261  Training Loss  0.07280006259679794\n",
            "Epoch  2 Batch  42 / 261  Training Loss  0.06858377903699875\n",
            "Epoch  2 Batch  43 / 261  Training Loss  0.06055950000882149\n",
            "Epoch  2 Batch  44 / 261  Training Loss  0.06960632652044296\n",
            "Epoch  2 Batch  45 / 261  Training Loss  0.0845235139131546\n",
            "Epoch  2 Batch  46 / 261  Training Loss  0.06875539571046829\n",
            "Epoch  2 Batch  47 / 261  Training Loss  0.07145480066537857\n",
            "Epoch  2 Batch  48 / 261  Training Loss  0.09596297889947891\n",
            "Epoch  2 Batch  49 / 261  Training Loss  0.10046248137950897\n",
            "Epoch  2 Batch  50 / 261  Training Loss  0.08333699405193329\n",
            "Epoch  2 Batch  51 / 261  Training Loss  0.13672474026679993\n",
            "Epoch  2 Batch  52 / 261  Training Loss  0.1108693853020668\n",
            "Epoch  2 Batch  53 / 261  Training Loss  0.10380171984434128\n",
            "Epoch  2 Batch  54 / 261  Training Loss  0.0864396020770073\n",
            "Epoch  2 Batch  55 / 261  Training Loss  0.06682054698467255\n",
            "Epoch  2 Batch  56 / 261  Training Loss  0.06593513488769531\n",
            "Epoch  2 Batch  57 / 261  Training Loss  0.06738190352916718\n",
            "Epoch  2 Batch  58 / 261  Training Loss  0.05601067468523979\n",
            "Epoch  2 Batch  59 / 261  Training Loss  0.049951039254665375\n",
            "Epoch  2 Batch  60 / 261  Training Loss  0.06540493667125702\n",
            "Epoch  2 Batch  61 / 261  Training Loss  0.05306804180145264\n",
            "Epoch  2 Batch  62 / 261  Training Loss  0.07763312011957169\n",
            "Epoch  2 Batch  63 / 261  Training Loss  0.05150223895907402\n",
            "Epoch  2 Batch  64 / 261  Training Loss  0.07724045217037201\n",
            "Epoch  2 Batch  65 / 261  Training Loss  0.08376748859882355\n",
            "Epoch  2 Batch  66 / 261  Training Loss  0.08704546838998795\n",
            "Epoch  2 Batch  67 / 261  Training Loss  0.07591211050748825\n",
            "Epoch  2 Batch  68 / 261  Training Loss  0.10156072676181793\n",
            "Epoch  2 Batch  69 / 261  Training Loss  0.06168970465660095\n",
            "Epoch  2 Batch  70 / 261  Training Loss  0.061365704983472824\n",
            "Epoch  2 Batch  71 / 261  Training Loss  0.05951569601893425\n",
            "Epoch  2 Batch  72 / 261  Training Loss  0.06735468655824661\n",
            "Epoch  2 Batch  73 / 261  Training Loss  0.07096341997385025\n",
            "Epoch  2 Batch  74 / 261  Training Loss  0.06640000641345978\n",
            "Epoch  2 Batch  75 / 261  Training Loss  0.04827318340539932\n",
            "Epoch  2 Batch  76 / 261  Training Loss  0.04562543332576752\n",
            "Epoch  2 Batch  77 / 261  Training Loss  0.07024338096380234\n",
            "Epoch  2 Batch  78 / 261  Training Loss  0.05598260089755058\n",
            "Epoch  2 Batch  79 / 261  Training Loss  0.05824356898665428\n",
            "Epoch  2 Batch  80 / 261  Training Loss  0.058339543640613556\n",
            "Epoch  2 Batch  81 / 261  Training Loss  0.06298394501209259\n",
            "Epoch  2 Batch  82 / 261  Training Loss  0.04661016911268234\n",
            "Epoch  2 Batch  83 / 261  Training Loss  0.05126459524035454\n",
            "Epoch  2 Batch  84 / 261  Training Loss  0.06551220268011093\n",
            "Epoch  2 Batch  85 / 261  Training Loss  0.05945347622036934\n",
            "Epoch  2 Batch  86 / 261  Training Loss  0.058068159967660904\n",
            "Epoch  2 Batch  87 / 261  Training Loss  0.047254908829927444\n",
            "Epoch  2 Batch  88 / 261  Training Loss  0.04817046597599983\n",
            "Epoch  2 Batch  89 / 261  Training Loss  0.0611921064555645\n",
            "Epoch  2 Batch  90 / 261  Training Loss  0.0747489407658577\n",
            "Epoch  2 Batch  91 / 261  Training Loss  0.08907587826251984\n",
            "Epoch  2 Batch  92 / 261  Training Loss  0.08807756751775742\n",
            "Epoch  2 Batch  93 / 261  Training Loss  0.06145705282688141\n",
            "Epoch  2 Batch  94 / 261  Training Loss  0.05035911872982979\n",
            "Epoch  2 Batch  95 / 261  Training Loss  0.05287361145019531\n",
            "Epoch  2 Batch  96 / 261  Training Loss  0.04835361987352371\n",
            "Epoch  2 Batch  97 / 261  Training Loss  0.044216178357601166\n",
            "Epoch  2 Batch  98 / 261  Training Loss  0.05341971293091774\n",
            "Epoch  2 Batch  99 / 261  Training Loss  0.04209274798631668\n",
            "Epoch  2 Batch  100 / 261  Training Loss  0.05145175755023956\n",
            "Epoch  2 Batch  101 / 261  Training Loss  0.03515473008155823\n",
            "Epoch  2 Batch  102 / 261  Training Loss  0.04537248611450195\n",
            "Epoch  2 Batch  103 / 261  Training Loss  0.04502430558204651\n",
            "Epoch  2 Batch  104 / 261  Training Loss  0.0641789585351944\n",
            "Epoch  2 Batch  105 / 261  Training Loss  0.05982627347111702\n",
            "Epoch  2 Batch  106 / 261  Training Loss  0.051422324031591415\n",
            "Epoch  2 Batch  107 / 261  Training Loss  0.05832136049866676\n",
            "Epoch  2 Batch  108 / 261  Training Loss  0.051260385662317276\n",
            "Epoch  2 Batch  109 / 261  Training Loss  0.04741295427083969\n",
            "Epoch  2 Batch  110 / 261  Training Loss  0.08923778682947159\n",
            "Epoch  2 Batch  111 / 261  Training Loss  0.08768770098686218\n",
            "Epoch  2 Batch  112 / 261  Training Loss  0.06534327566623688\n",
            "Epoch  2 Batch  113 / 261  Training Loss  0.05471105873584747\n",
            "Epoch  2 Batch  114 / 261  Training Loss  0.0476340614259243\n",
            "Epoch  2 Batch  115 / 261  Training Loss  0.046647049486637115\n",
            "Epoch  2 Batch  116 / 261  Training Loss  0.03775215148925781\n",
            "Epoch  2 Batch  117 / 261  Training Loss  0.04186383634805679\n",
            "Epoch  2 Batch  118 / 261  Training Loss  0.051230162382125854\n",
            "Epoch  2 Batch  119 / 261  Training Loss  0.05487474799156189\n",
            "Epoch  2 Batch  120 / 261  Training Loss  0.049649108201265335\n",
            "Epoch  2 Batch  121 / 261  Training Loss  0.05116256698966026\n",
            "Epoch  2 Batch  122 / 261  Training Loss  0.041198521852493286\n",
            "Epoch  2 Batch  123 / 261  Training Loss  0.04209728166460991\n",
            "Epoch  2 Batch  124 / 261  Training Loss  0.04946744814515114\n",
            "Epoch  2 Batch  125 / 261  Training Loss  0.0707477256655693\n",
            "Epoch  2 Batch  126 / 261  Training Loss  0.04063819721341133\n",
            "Epoch  2 Batch  127 / 261  Training Loss  0.03542206063866615\n",
            "Epoch  2 Batch  128 / 261  Training Loss  0.034493062645196915\n",
            "Epoch  2 Batch  129 / 261  Training Loss  0.06187015399336815\n",
            "Epoch  2 Batch  130 / 261  Training Loss  0.0711003914475441\n",
            "Epoch  2 Batch  131 / 261  Training Loss  0.04322187602519989\n",
            "Epoch  2 Batch  132 / 261  Training Loss  0.0633038729429245\n",
            "Epoch  2 Batch  133 / 261  Training Loss  0.04171138256788254\n",
            "Epoch  2 Batch  134 / 261  Training Loss  0.03795256465673447\n",
            "Epoch  2 Batch  135 / 261  Training Loss  0.04135191813111305\n",
            "Epoch  2 Batch  136 / 261  Training Loss  0.03744453564286232\n",
            "Epoch  2 Batch  137 / 261  Training Loss  0.034493137151002884\n",
            "Epoch  2 Batch  138 / 261  Training Loss  0.036644868552684784\n",
            "Epoch  2 Batch  139 / 261  Training Loss  0.026465363800525665\n",
            "Epoch  2 Batch  140 / 261  Training Loss  0.025277044624090195\n",
            "Epoch  2 Batch  141 / 261  Training Loss  0.03339957445859909\n",
            "Epoch  2 Batch  142 / 261  Training Loss  0.0312045868486166\n",
            "Epoch  2 Batch  143 / 261  Training Loss  0.025533823296427727\n",
            "Epoch  2 Batch  144 / 261  Training Loss  0.03483971208333969\n",
            "Epoch  2 Batch  145 / 261  Training Loss  0.031379856169223785\n",
            "Epoch  2 Batch  146 / 261  Training Loss  0.03890766203403473\n",
            "Epoch  2 Batch  147 / 261  Training Loss  0.04163212701678276\n",
            "Epoch  2 Batch  148 / 261  Training Loss  0.04019615054130554\n",
            "Epoch  2 Batch  149 / 261  Training Loss  0.03434868901968002\n",
            "Epoch  2 Batch  150 / 261  Training Loss  0.03756794333457947\n",
            "Epoch  2 Batch  151 / 261  Training Loss  0.035466380417346954\n",
            "Epoch  2 Batch  152 / 261  Training Loss  0.047658003866672516\n",
            "Epoch  2 Batch  153 / 261  Training Loss  0.040409695357084274\n",
            "Epoch  2 Batch  154 / 261  Training Loss  0.04576963558793068\n",
            "Epoch  2 Batch  155 / 261  Training Loss  0.0585823580622673\n",
            "Epoch  2 Batch  156 / 261  Training Loss  0.038664016872644424\n",
            "Epoch  2 Batch  157 / 261  Training Loss  0.03764072060585022\n",
            "Epoch  2 Batch  158 / 261  Training Loss  0.028917817398905754\n",
            "Epoch  2 Batch  159 / 261  Training Loss  0.04194432124495506\n",
            "Epoch  2 Batch  160 / 261  Training Loss  0.08969131112098694\n",
            "Epoch  2 Batch  161 / 261  Training Loss  0.05579756200313568\n",
            "Epoch  2 Batch  162 / 261  Training Loss  0.05344036966562271\n",
            "Epoch  2 Batch  163 / 261  Training Loss  0.060193464159965515\n",
            "Epoch  2 Batch  164 / 261  Training Loss  0.047638531774282455\n",
            "Epoch  2 Batch  165 / 261  Training Loss  0.05073843151330948\n",
            "Epoch  2 Batch  166 / 261  Training Loss  0.047346655279397964\n",
            "Epoch  2 Batch  167 / 261  Training Loss  0.040489234030246735\n",
            "Epoch  2 Batch  168 / 261  Training Loss  0.04620833322405815\n",
            "Epoch  2 Batch  169 / 261  Training Loss  0.03755578026175499\n",
            "Epoch  2 Batch  170 / 261  Training Loss  0.034334491938352585\n",
            "Epoch  2 Batch  171 / 261  Training Loss  0.02587282657623291\n",
            "Epoch  2 Batch  172 / 261  Training Loss  0.02559189312160015\n",
            "Epoch  2 Batch  173 / 261  Training Loss  0.028470776975154877\n",
            "Epoch  2 Batch  174 / 261  Training Loss  0.02727022022008896\n",
            "Epoch  2 Batch  175 / 261  Training Loss  0.01930011436343193\n",
            "Epoch  2 Batch  176 / 261  Training Loss  0.024605315178632736\n",
            "Epoch  2 Batch  177 / 261  Training Loss  0.03840459883213043\n",
            "Epoch  2 Batch  178 / 261  Training Loss  0.029442649334669113\n",
            "Epoch  2 Batch  179 / 261  Training Loss  0.02764330618083477\n",
            "Epoch  2 Batch  180 / 261  Training Loss  0.024238917976617813\n",
            "Epoch  2 Batch  181 / 261  Training Loss  0.02530486509203911\n",
            "Epoch  2 Batch  182 / 261  Training Loss  0.018826903775334358\n",
            "Epoch  2 Batch  183 / 261  Training Loss  0.02939220517873764\n",
            "Epoch  2 Batch  184 / 261  Training Loss  0.032786961644887924\n",
            "Epoch  2 Batch  185 / 261  Training Loss  0.04952320083975792\n",
            "Epoch  2 Batch  186 / 261  Training Loss  0.029413603246212006\n",
            "Epoch  2 Batch  187 / 261  Training Loss  0.022487523034214973\n",
            "Epoch  2 Batch  188 / 261  Training Loss  0.02874959446489811\n",
            "Epoch  2 Batch  189 / 261  Training Loss  0.032430924475193024\n",
            "Epoch  2 Batch  190 / 261  Training Loss  0.03268890082836151\n",
            "Epoch  2 Batch  191 / 261  Training Loss  0.03783880174160004\n",
            "Epoch  2 Batch  192 / 261  Training Loss  0.032025519758462906\n",
            "Epoch  2 Batch  193 / 261  Training Loss  0.03427751362323761\n",
            "Epoch  2 Batch  194 / 261  Training Loss  0.03711443766951561\n",
            "Epoch  2 Batch  195 / 261  Training Loss  0.03352515399456024\n",
            "Epoch  2 Batch  196 / 261  Training Loss  0.029706429690122604\n",
            "Epoch  2 Batch  197 / 261  Training Loss  0.03905750811100006\n",
            "Epoch  2 Batch  198 / 261  Training Loss  0.03398716077208519\n",
            "Epoch  2 Batch  199 / 261  Training Loss  0.0447055920958519\n",
            "Epoch  2 Batch  200 / 261  Training Loss  0.04539557918906212\n",
            "Epoch  2 Batch  201 / 261  Training Loss  0.05814957991242409\n",
            "Epoch  2 Batch  202 / 261  Training Loss  0.04067223146557808\n",
            "Epoch  2 Batch  203 / 261  Training Loss  0.04645390808582306\n",
            "Epoch  2 Batch  204 / 261  Training Loss  0.027997717261314392\n",
            "Epoch  2 Batch  205 / 261  Training Loss  0.02726132608950138\n",
            "Epoch  2 Batch  206 / 261  Training Loss  0.022189054638147354\n",
            "Epoch  2 Batch  207 / 261  Training Loss  0.023081393912434578\n",
            "Epoch  2 Batch  208 / 261  Training Loss  0.020287638530135155\n",
            "Epoch  2 Batch  209 / 261  Training Loss  0.03167541325092316\n",
            "Epoch  2 Batch  210 / 261  Training Loss  0.03215448558330536\n",
            "Epoch  2 Batch  211 / 261  Training Loss  0.030489522963762283\n",
            "Epoch  2 Batch  212 / 261  Training Loss  0.03228414058685303\n",
            "Epoch  2 Batch  213 / 261  Training Loss  0.03976882994174957\n",
            "Epoch  2 Batch  214 / 261  Training Loss  0.031806591898202896\n",
            "Epoch  2 Batch  215 / 261  Training Loss  0.02829062193632126\n",
            "Epoch  2 Batch  216 / 261  Training Loss  0.03148600459098816\n",
            "Epoch  2 Batch  217 / 261  Training Loss  0.03215737268328667\n",
            "Epoch  2 Batch  218 / 261  Training Loss  0.02627657912671566\n",
            "Epoch  2 Batch  219 / 261  Training Loss  0.03599438816308975\n",
            "Epoch  2 Batch  220 / 261  Training Loss  0.022338243201375008\n",
            "Epoch  2 Batch  221 / 261  Training Loss  0.020464159548282623\n",
            "Epoch  2 Batch  222 / 261  Training Loss  0.02158479019999504\n",
            "Epoch  2 Batch  223 / 261  Training Loss  0.024174828082323074\n",
            "Epoch  2 Batch  224 / 261  Training Loss  0.022787582129240036\n",
            "Epoch  2 Batch  225 / 261  Training Loss  0.02550092525780201\n",
            "Epoch  2 Batch  226 / 261  Training Loss  0.0321878083050251\n",
            "Epoch  2 Batch  227 / 261  Training Loss  0.02655959688127041\n",
            "Epoch  2 Batch  228 / 261  Training Loss  0.029198219999670982\n",
            "Epoch  2 Batch  229 / 261  Training Loss  0.04052598029375076\n",
            "Epoch  2 Batch  230 / 261  Training Loss  0.03226089850068092\n",
            "Epoch  2 Batch  231 / 261  Training Loss  0.038809165358543396\n",
            "Epoch  2 Batch  232 / 261  Training Loss  0.026336712762713432\n",
            "Epoch  2 Batch  233 / 261  Training Loss  0.04255899786949158\n",
            "Epoch  2 Batch  234 / 261  Training Loss  0.03823001682758331\n",
            "Epoch  2 Batch  235 / 261  Training Loss  0.017582358792424202\n",
            "Epoch  2 Batch  236 / 261  Training Loss  0.025709422305226326\n",
            "Epoch  2 Batch  237 / 261  Training Loss  0.03145003318786621\n",
            "Epoch  2 Batch  238 / 261  Training Loss  0.05626281350851059\n",
            "Epoch  2 Batch  239 / 261  Training Loss  0.03850572556257248\n",
            "Epoch  2 Batch  240 / 261  Training Loss  0.03326048329472542\n",
            "Epoch  2 Batch  241 / 261  Training Loss  0.06374840438365936\n",
            "Epoch  2 Batch  242 / 261  Training Loss  0.028461065143346786\n",
            "Epoch  2 Batch  243 / 261  Training Loss  0.04369034618139267\n",
            "Epoch  2 Batch  244 / 261  Training Loss  0.026262881234288216\n",
            "Epoch  2 Batch  245 / 261  Training Loss  0.0214216485619545\n",
            "Epoch  2 Batch  246 / 261  Training Loss  0.024534983560442924\n",
            "Epoch  2 Batch  247 / 261  Training Loss  0.021238338202238083\n",
            "Epoch  2 Batch  248 / 261  Training Loss  0.02825702168047428\n",
            "Epoch  2 Batch  249 / 261  Training Loss  0.013470465317368507\n",
            "Epoch  2 Batch  250 / 261  Training Loss  0.018136724829673767\n",
            "Epoch  2 Batch  251 / 261  Training Loss  0.016518108546733856\n",
            "Epoch  2 Batch  252 / 261  Training Loss  0.027031105011701584\n",
            "Epoch  2 Batch  253 / 261  Training Loss  0.0256568044424057\n",
            "Epoch  2 Batch  254 / 261  Training Loss  0.02111440896987915\n",
            "Epoch  2 Batch  255 / 261  Training Loss  0.022893209010362625\n",
            "Epoch  2 Batch  256 / 261  Training Loss  0.02033689059317112\n",
            "Epoch  2 Batch  257 / 261  Training Loss  0.019412774592638016\n",
            "Epoch  2 Batch  258 / 261  Training Loss  0.026384055614471436\n",
            "Epoch  2 Batch  259 / 261  Training Loss  0.01793535426259041\n",
            "Epoch  2 Batch  260 / 261  Training Loss  0.022506294772028923\n",
            "   3    |    -    |   0.053233   | 80.352394\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 3\n",
            "Epoch  3 Batch  0 / 261  Training Loss  0.012317090295255184\n",
            "Epoch  3 Batch  1 / 261  Training Loss  0.01667027920484543\n",
            "Epoch  3 Batch  2 / 261  Training Loss  0.01909000612795353\n",
            "Epoch  3 Batch  3 / 261  Training Loss  0.013924960978329182\n",
            "Epoch  3 Batch  4 / 261  Training Loss  0.017356447875499725\n",
            "Epoch  3 Batch  5 / 261  Training Loss  0.012413556687533855\n",
            "Epoch  3 Batch  6 / 261  Training Loss  0.015955384820699692\n",
            "Epoch  3 Batch  7 / 261  Training Loss  0.009617469273507595\n",
            "Epoch  3 Batch  8 / 261  Training Loss  0.01995745114982128\n",
            "Epoch  3 Batch  9 / 261  Training Loss  0.018575172871351242\n",
            "Epoch  3 Batch  10 / 261  Training Loss  0.0143433203920722\n",
            "Epoch  3 Batch  11 / 261  Training Loss  0.020866746082901955\n",
            "Epoch  3 Batch  12 / 261  Training Loss  0.02719450555741787\n",
            "Epoch  3 Batch  13 / 261  Training Loss  0.04645911604166031\n",
            "Epoch  3 Batch  14 / 261  Training Loss  0.041903749108314514\n",
            "Epoch  3 Batch  15 / 261  Training Loss  0.029731309041380882\n",
            "Epoch  3 Batch  16 / 261  Training Loss  0.023721642792224884\n",
            "Epoch  3 Batch  17 / 261  Training Loss  0.020434211939573288\n",
            "Epoch  3 Batch  18 / 261  Training Loss  0.02263304777443409\n",
            "Epoch  3 Batch  19 / 261  Training Loss  0.020362816751003265\n",
            "Epoch  3 Batch  20 / 261  Training Loss  0.019920596852898598\n",
            "Epoch  3 Batch  21 / 261  Training Loss  0.019089851528406143\n",
            "Epoch  3 Batch  22 / 261  Training Loss  0.01872383803129196\n",
            "Epoch  3 Batch  23 / 261  Training Loss  0.011327486485242844\n",
            "Epoch  3 Batch  24 / 261  Training Loss  0.014955137856304646\n",
            "Epoch  3 Batch  25 / 261  Training Loss  0.02080140821635723\n",
            "Epoch  3 Batch  26 / 261  Training Loss  0.031159281730651855\n",
            "Epoch  3 Batch  27 / 261  Training Loss  0.04222070798277855\n",
            "Epoch  3 Batch  28 / 261  Training Loss  0.03781091421842575\n",
            "Epoch  3 Batch  29 / 261  Training Loss  0.0192808099091053\n",
            "Epoch  3 Batch  30 / 261  Training Loss  0.013617086224257946\n",
            "Epoch  3 Batch  31 / 261  Training Loss  0.019176030531525612\n",
            "Epoch  3 Batch  32 / 261  Training Loss  0.02191176451742649\n",
            "Epoch  3 Batch  33 / 261  Training Loss  0.029653554782271385\n",
            "Epoch  3 Batch  34 / 261  Training Loss  0.02213628776371479\n",
            "Epoch  3 Batch  35 / 261  Training Loss  0.017523158341646194\n",
            "Epoch  3 Batch  36 / 261  Training Loss  0.02488035522401333\n",
            "Epoch  3 Batch  37 / 261  Training Loss  0.014181053265929222\n",
            "Epoch  3 Batch  38 / 261  Training Loss  0.01816914975643158\n",
            "Epoch  3 Batch  39 / 261  Training Loss  0.022133491933345795\n",
            "Epoch  3 Batch  40 / 261  Training Loss  0.05658009648323059\n",
            "Epoch  3 Batch  41 / 261  Training Loss  0.042562175542116165\n",
            "Epoch  3 Batch  42 / 261  Training Loss  0.02712731994688511\n",
            "Epoch  3 Batch  43 / 261  Training Loss  0.02285105735063553\n",
            "Epoch  3 Batch  44 / 261  Training Loss  0.01759784109890461\n",
            "Epoch  3 Batch  45 / 261  Training Loss  0.011976607143878937\n",
            "Epoch  3 Batch  46 / 261  Training Loss  0.01998285949230194\n",
            "Epoch  3 Batch  47 / 261  Training Loss  0.026662185788154602\n",
            "Epoch  3 Batch  48 / 261  Training Loss  0.02312062308192253\n",
            "Epoch  3 Batch  49 / 261  Training Loss  0.01854102686047554\n",
            "Epoch  3 Batch  50 / 261  Training Loss  0.022074991837143898\n",
            "Epoch  3 Batch  51 / 261  Training Loss  0.013898189179599285\n",
            "Epoch  3 Batch  52 / 261  Training Loss  0.015776578336954117\n",
            "Epoch  3 Batch  53 / 261  Training Loss  0.018310243263840675\n",
            "Epoch  3 Batch  54 / 261  Training Loss  0.014711549505591393\n",
            "Epoch  3 Batch  55 / 261  Training Loss  0.010702511295676231\n",
            "Epoch  3 Batch  56 / 261  Training Loss  0.016226373612880707\n",
            "Epoch  3 Batch  57 / 261  Training Loss  0.016435805708169937\n",
            "Epoch  3 Batch  58 / 261  Training Loss  0.026260986924171448\n",
            "Epoch  3 Batch  59 / 261  Training Loss  0.012559388764202595\n",
            "Epoch  3 Batch  60 / 261  Training Loss  0.018344251438975334\n",
            "Epoch  3 Batch  61 / 261  Training Loss  0.01129651628434658\n",
            "Epoch  3 Batch  62 / 261  Training Loss  0.010688960552215576\n",
            "Epoch  3 Batch  63 / 261  Training Loss  0.023083914071321487\n",
            "Epoch  3 Batch  64 / 261  Training Loss  0.016509568318724632\n",
            "Epoch  3 Batch  65 / 261  Training Loss  0.02410712093114853\n",
            "Epoch  3 Batch  66 / 261  Training Loss  0.01481942180544138\n",
            "Epoch  3 Batch  67 / 261  Training Loss  0.02324279397726059\n",
            "Epoch  3 Batch  68 / 261  Training Loss  0.02222723700106144\n",
            "Epoch  3 Batch  69 / 261  Training Loss  0.015602469444274902\n",
            "Epoch  3 Batch  70 / 261  Training Loss  0.03127893805503845\n",
            "Epoch  3 Batch  71 / 261  Training Loss  0.030243419110774994\n",
            "Epoch  3 Batch  72 / 261  Training Loss  0.01867813430726528\n",
            "Epoch  3 Batch  73 / 261  Training Loss  0.01755419187247753\n",
            "Epoch  3 Batch  74 / 261  Training Loss  0.02067163772881031\n",
            "Epoch  3 Batch  75 / 261  Training Loss  0.01725572906434536\n",
            "Epoch  3 Batch  76 / 261  Training Loss  0.020711129531264305\n",
            "Epoch  3 Batch  77 / 261  Training Loss  0.014762715436518192\n",
            "Epoch  3 Batch  78 / 261  Training Loss  0.01635769195854664\n",
            "Epoch  3 Batch  79 / 261  Training Loss  0.013367938809096813\n",
            "Epoch  3 Batch  80 / 261  Training Loss  0.008595357649028301\n",
            "Epoch  3 Batch  81 / 261  Training Loss  0.011505327187478542\n",
            "Epoch  3 Batch  82 / 261  Training Loss  0.010898103937506676\n",
            "Epoch  3 Batch  83 / 261  Training Loss  0.015449907630681992\n",
            "Epoch  3 Batch  84 / 261  Training Loss  0.012960067950189114\n",
            "Epoch  3 Batch  85 / 261  Training Loss  0.009241153486073017\n",
            "Epoch  3 Batch  86 / 261  Training Loss  0.011576520279049873\n",
            "Epoch  3 Batch  87 / 261  Training Loss  0.00636573601514101\n",
            "Epoch  3 Batch  88 / 261  Training Loss  0.011102057993412018\n",
            "Epoch  3 Batch  89 / 261  Training Loss  0.037226203829050064\n",
            "Epoch  3 Batch  90 / 261  Training Loss  0.03440094739198685\n",
            "Epoch  3 Batch  91 / 261  Training Loss  0.01937880925834179\n",
            "Epoch  3 Batch  92 / 261  Training Loss  0.01736624725162983\n",
            "Epoch  3 Batch  93 / 261  Training Loss  0.0344490185379982\n",
            "Epoch  3 Batch  94 / 261  Training Loss  0.019555123522877693\n",
            "Epoch  3 Batch  95 / 261  Training Loss  0.015376473776996136\n",
            "Epoch  3 Batch  96 / 261  Training Loss  0.022271353751420975\n",
            "Epoch  3 Batch  97 / 261  Training Loss  0.01434181910008192\n",
            "Epoch  3 Batch  98 / 261  Training Loss  0.03353550285100937\n",
            "Epoch  3 Batch  99 / 261  Training Loss  0.016556210815906525\n",
            "Epoch  3 Batch  100 / 261  Training Loss  0.01328861340880394\n",
            "Epoch  3 Batch  101 / 261  Training Loss  0.012459048070013523\n",
            "Epoch  3 Batch  102 / 261  Training Loss  0.016585391014814377\n",
            "Epoch  3 Batch  103 / 261  Training Loss  0.020438354462385178\n",
            "Epoch  3 Batch  104 / 261  Training Loss  0.014504133723676205\n",
            "Epoch  3 Batch  105 / 261  Training Loss  0.011690148152410984\n",
            "Epoch  3 Batch  106 / 261  Training Loss  0.014558584429323673\n",
            "Epoch  3 Batch  107 / 261  Training Loss  0.01011948473751545\n",
            "Epoch  3 Batch  108 / 261  Training Loss  0.019877202808856964\n",
            "Epoch  3 Batch  109 / 261  Training Loss  0.018651649355888367\n",
            "Epoch  3 Batch  110 / 261  Training Loss  0.021131522953510284\n",
            "Epoch  3 Batch  111 / 261  Training Loss  0.023034829646348953\n",
            "Epoch  3 Batch  112 / 261  Training Loss  0.0206040907651186\n",
            "Epoch  3 Batch  113 / 261  Training Loss  0.03606404736638069\n",
            "Epoch  3 Batch  114 / 261  Training Loss  0.024166107177734375\n",
            "Epoch  3 Batch  115 / 261  Training Loss  0.0317067913711071\n",
            "Epoch  3 Batch  116 / 261  Training Loss  0.020140325650572777\n",
            "Epoch  3 Batch  117 / 261  Training Loss  0.010910732671618462\n",
            "Epoch  3 Batch  118 / 261  Training Loss  0.009674150496721268\n",
            "Epoch  3 Batch  119 / 261  Training Loss  0.013330468907952309\n",
            "Epoch  3 Batch  120 / 261  Training Loss  0.012671854346990585\n",
            "Epoch  3 Batch  121 / 261  Training Loss  0.01663612760603428\n",
            "Epoch  3 Batch  122 / 261  Training Loss  0.026911746710538864\n",
            "Epoch  3 Batch  123 / 261  Training Loss  0.01935003697872162\n",
            "Epoch  3 Batch  124 / 261  Training Loss  0.013385268859565258\n",
            "Epoch  3 Batch  125 / 261  Training Loss  0.01650644838809967\n",
            "Epoch  3 Batch  126 / 261  Training Loss  0.021666429936885834\n",
            "Epoch  3 Batch  127 / 261  Training Loss  0.029029114171862602\n",
            "Epoch  3 Batch  128 / 261  Training Loss  0.014615369029343128\n",
            "Epoch  3 Batch  129 / 261  Training Loss  0.020843882113695145\n",
            "Epoch  3 Batch  130 / 261  Training Loss  0.016040397807955742\n",
            "Epoch  3 Batch  131 / 261  Training Loss  0.008855769410729408\n",
            "Epoch  3 Batch  132 / 261  Training Loss  0.012708290480077267\n",
            "Epoch  3 Batch  133 / 261  Training Loss  0.012271047569811344\n",
            "Epoch  3 Batch  134 / 261  Training Loss  0.016802627593278885\n",
            "Epoch  3 Batch  135 / 261  Training Loss  0.02036227472126484\n",
            "Epoch  3 Batch  136 / 261  Training Loss  0.024314388632774353\n",
            "Epoch  3 Batch  137 / 261  Training Loss  0.012407676316797733\n",
            "Epoch  3 Batch  138 / 261  Training Loss  0.016551682725548744\n",
            "Epoch  3 Batch  139 / 261  Training Loss  0.016777122393250465\n",
            "Epoch  3 Batch  140 / 261  Training Loss  0.016745131462812424\n",
            "Epoch  3 Batch  141 / 261  Training Loss  0.007068746257573366\n",
            "Epoch  3 Batch  142 / 261  Training Loss  0.020745132118463516\n",
            "Epoch  3 Batch  143 / 261  Training Loss  0.015774397179484367\n",
            "Epoch  3 Batch  144 / 261  Training Loss  0.008755946531891823\n",
            "Epoch  3 Batch  145 / 261  Training Loss  0.008734548464417458\n",
            "Epoch  3 Batch  146 / 261  Training Loss  0.01054693479090929\n",
            "Epoch  3 Batch  147 / 261  Training Loss  0.01221325621008873\n",
            "Epoch  3 Batch  148 / 261  Training Loss  0.010603372938930988\n",
            "Epoch  3 Batch  149 / 261  Training Loss  0.0064768074080348015\n",
            "Epoch  3 Batch  150 / 261  Training Loss  0.010234185494482517\n",
            "Epoch  3 Batch  151 / 261  Training Loss  0.013318601064383984\n",
            "Epoch  3 Batch  152 / 261  Training Loss  0.05579681321978569\n",
            "Epoch  3 Batch  153 / 261  Training Loss  0.04952358826994896\n",
            "Epoch  3 Batch  154 / 261  Training Loss  0.09672533720731735\n",
            "Epoch  3 Batch  155 / 261  Training Loss  0.01850265823304653\n",
            "Epoch  3 Batch  156 / 261  Training Loss  0.030819712206721306\n",
            "Epoch  3 Batch  157 / 261  Training Loss  0.03501679748296738\n",
            "Epoch  3 Batch  158 / 261  Training Loss  0.027682339772582054\n",
            "Epoch  3 Batch  159 / 261  Training Loss  0.00912066176533699\n",
            "Epoch  3 Batch  160 / 261  Training Loss  0.022148415446281433\n",
            "Epoch  3 Batch  161 / 261  Training Loss  0.02346288599073887\n",
            "Epoch  3 Batch  162 / 261  Training Loss  0.016450969502329826\n",
            "Epoch  3 Batch  163 / 261  Training Loss  0.023671364411711693\n",
            "Epoch  3 Batch  164 / 261  Training Loss  0.02082318626344204\n",
            "Epoch  3 Batch  165 / 261  Training Loss  0.011480585671961308\n",
            "Epoch  3 Batch  166 / 261  Training Loss  0.017636915668845177\n",
            "Epoch  3 Batch  167 / 261  Training Loss  0.014401191845536232\n",
            "Epoch  3 Batch  168 / 261  Training Loss  0.019402604550123215\n",
            "Epoch  3 Batch  169 / 261  Training Loss  0.011694947257637978\n",
            "Epoch  3 Batch  170 / 261  Training Loss  0.011130902916193008\n",
            "Epoch  3 Batch  171 / 261  Training Loss  0.012476298026740551\n",
            "Epoch  3 Batch  172 / 261  Training Loss  0.013092930428683758\n",
            "Epoch  3 Batch  173 / 261  Training Loss  0.009573359042406082\n",
            "Epoch  3 Batch  174 / 261  Training Loss  0.01458799559623003\n",
            "Epoch  3 Batch  175 / 261  Training Loss  0.009477067738771439\n",
            "Epoch  3 Batch  176 / 261  Training Loss  0.009843278676271439\n",
            "Epoch  3 Batch  177 / 261  Training Loss  0.016361229121685028\n",
            "Epoch  3 Batch  178 / 261  Training Loss  0.013543426059186459\n",
            "Epoch  3 Batch  179 / 261  Training Loss  0.01262832060456276\n",
            "Epoch  3 Batch  180 / 261  Training Loss  0.02074611932039261\n",
            "Epoch  3 Batch  181 / 261  Training Loss  0.008156206458806992\n",
            "Epoch  3 Batch  182 / 261  Training Loss  0.008489319123327732\n",
            "Epoch  3 Batch  183 / 261  Training Loss  0.010532685555517673\n",
            "Epoch  3 Batch  184 / 261  Training Loss  0.011789950542151928\n",
            "Epoch  3 Batch  185 / 261  Training Loss  0.009438257664442062\n",
            "Epoch  3 Batch  186 / 261  Training Loss  0.00983332097530365\n",
            "Epoch  3 Batch  187 / 261  Training Loss  0.023235386237502098\n",
            "Epoch  3 Batch  188 / 261  Training Loss  0.010767973959445953\n",
            "Epoch  3 Batch  189 / 261  Training Loss  0.016684796661138535\n",
            "Epoch  3 Batch  190 / 261  Training Loss  0.012704103253781796\n",
            "Epoch  3 Batch  191 / 261  Training Loss  0.01371318381279707\n",
            "Epoch  3 Batch  192 / 261  Training Loss  0.016232186928391457\n",
            "Epoch  3 Batch  193 / 261  Training Loss  0.015070111490786076\n",
            "Epoch  3 Batch  194 / 261  Training Loss  0.010775146074593067\n",
            "Epoch  3 Batch  195 / 261  Training Loss  0.010134140029549599\n",
            "Epoch  3 Batch  196 / 261  Training Loss  0.009548193775117397\n",
            "Epoch  3 Batch  197 / 261  Training Loss  0.009091495536267757\n",
            "Epoch  3 Batch  198 / 261  Training Loss  0.01351295504719019\n",
            "Epoch  3 Batch  199 / 261  Training Loss  0.013143184594810009\n",
            "Epoch  3 Batch  200 / 261  Training Loss  0.008412102237343788\n",
            "Epoch  3 Batch  201 / 261  Training Loss  0.006337596569210291\n",
            "Epoch  3 Batch  202 / 261  Training Loss  0.009182867594063282\n",
            "Epoch  3 Batch  203 / 261  Training Loss  0.021505437791347504\n",
            "Epoch  3 Batch  204 / 261  Training Loss  0.015627935528755188\n",
            "Epoch  3 Batch  205 / 261  Training Loss  0.015616748481988907\n",
            "Epoch  3 Batch  206 / 261  Training Loss  0.007958941161632538\n",
            "Epoch  3 Batch  207 / 261  Training Loss  0.01482442393898964\n",
            "Epoch  3 Batch  208 / 261  Training Loss  0.010777391493320465\n",
            "Epoch  3 Batch  209 / 261  Training Loss  0.009075336158275604\n",
            "Epoch  3 Batch  210 / 261  Training Loss  0.019507654011249542\n",
            "Epoch  3 Batch  211 / 261  Training Loss  0.010178639553487301\n",
            "Epoch  3 Batch  212 / 261  Training Loss  0.009290967136621475\n",
            "Epoch  3 Batch  213 / 261  Training Loss  0.01614687591791153\n",
            "Epoch  3 Batch  214 / 261  Training Loss  0.010459547862410545\n",
            "Epoch  3 Batch  215 / 261  Training Loss  0.014732309617102146\n",
            "Epoch  3 Batch  216 / 261  Training Loss  0.016831038519740105\n",
            "Epoch  3 Batch  217 / 261  Training Loss  0.029467366635799408\n",
            "Epoch  3 Batch  218 / 261  Training Loss  0.014314685016870499\n",
            "Epoch  3 Batch  219 / 261  Training Loss  0.007688454352319241\n",
            "Epoch  3 Batch  220 / 261  Training Loss  0.021689701825380325\n",
            "Epoch  3 Batch  221 / 261  Training Loss  0.019918818026781082\n",
            "Epoch  3 Batch  222 / 261  Training Loss  0.017435597255825996\n",
            "Epoch  3 Batch  223 / 261  Training Loss  0.01345035806298256\n",
            "Epoch  3 Batch  224 / 261  Training Loss  0.013574447482824326\n",
            "Epoch  3 Batch  225 / 261  Training Loss  0.015749923884868622\n",
            "Epoch  3 Batch  226 / 261  Training Loss  0.01355119701474905\n",
            "Epoch  3 Batch  227 / 261  Training Loss  0.005172031931579113\n",
            "Epoch  3 Batch  228 / 261  Training Loss  0.011299289762973785\n",
            "Epoch  3 Batch  229 / 261  Training Loss  0.014669892378151417\n",
            "Epoch  3 Batch  230 / 261  Training Loss  0.004704928025603294\n",
            "Epoch  3 Batch  231 / 261  Training Loss  0.00854006689041853\n",
            "Epoch  3 Batch  232 / 261  Training Loss  0.007610383443534374\n",
            "Epoch  3 Batch  233 / 261  Training Loss  0.014032439328730106\n",
            "Epoch  3 Batch  234 / 261  Training Loss  0.015819180756807327\n",
            "Epoch  3 Batch  235 / 261  Training Loss  0.008080600760877132\n",
            "Epoch  3 Batch  236 / 261  Training Loss  0.0075251455418765545\n",
            "Epoch  3 Batch  237 / 261  Training Loss  0.0070904893800616264\n",
            "Epoch  3 Batch  238 / 261  Training Loss  0.005997964181005955\n",
            "Epoch  3 Batch  239 / 261  Training Loss  0.011874744668602943\n",
            "Epoch  3 Batch  240 / 261  Training Loss  0.009650520980358124\n",
            "Epoch  3 Batch  241 / 261  Training Loss  0.011032584123313427\n",
            "Epoch  3 Batch  242 / 261  Training Loss  0.02806907147169113\n",
            "Epoch  3 Batch  243 / 261  Training Loss  0.01661001704633236\n",
            "Epoch  3 Batch  244 / 261  Training Loss  0.005614097695797682\n",
            "Epoch  3 Batch  245 / 261  Training Loss  0.011240418069064617\n",
            "Epoch  3 Batch  246 / 261  Training Loss  0.006602105684578419\n",
            "Epoch  3 Batch  247 / 261  Training Loss  0.01148199662566185\n",
            "Epoch  3 Batch  248 / 261  Training Loss  0.006823759060353041\n",
            "Epoch  3 Batch  249 / 261  Training Loss  0.005380264017730951\n",
            "Epoch  3 Batch  250 / 261  Training Loss  0.006374401040375233\n",
            "Epoch  3 Batch  251 / 261  Training Loss  0.003748502116650343\n",
            "Epoch  3 Batch  252 / 261  Training Loss  0.014655028469860554\n",
            "Epoch  3 Batch  253 / 261  Training Loss  0.00931472983211279\n",
            "Epoch  3 Batch  254 / 261  Training Loss  0.003644461976364255\n",
            "Epoch  3 Batch  255 / 261  Training Loss  0.0066977860406041145\n",
            "Epoch  3 Batch  256 / 261  Training Loss  0.010783035308122635\n",
            "Epoch  3 Batch  257 / 261  Training Loss  0.02877308614552021\n",
            "Epoch  3 Batch  258 / 261  Training Loss  0.02035387232899666\n",
            "Epoch  3 Batch  259 / 261  Training Loss  0.010821224190294743\n",
            "Epoch  3 Batch  260 / 261  Training Loss  0.012471858412027359\n",
            "   4    |    -    |   0.017312   | 87.732713\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 4\n",
            "Epoch  4 Batch  0 / 261  Training Loss  0.010042847134172916\n",
            "Epoch  4 Batch  1 / 261  Training Loss  0.014273020438849926\n",
            "Epoch  4 Batch  2 / 261  Training Loss  0.010347739793360233\n",
            "Epoch  4 Batch  3 / 261  Training Loss  0.01029257196933031\n",
            "Epoch  4 Batch  4 / 261  Training Loss  0.010178571566939354\n",
            "Epoch  4 Batch  5 / 261  Training Loss  0.0035475105978548527\n",
            "Epoch  4 Batch  6 / 261  Training Loss  0.006085279397666454\n",
            "Epoch  4 Batch  7 / 261  Training Loss  0.0029479635413736105\n",
            "Epoch  4 Batch  8 / 261  Training Loss  0.006786431185901165\n",
            "Epoch  4 Batch  9 / 261  Training Loss  0.005666003096848726\n",
            "Epoch  4 Batch  10 / 261  Training Loss  0.008675478398799896\n",
            "Epoch  4 Batch  11 / 261  Training Loss  0.015283491462469101\n",
            "Epoch  4 Batch  12 / 261  Training Loss  0.010036135092377663\n",
            "Epoch  4 Batch  13 / 261  Training Loss  0.00538383424282074\n",
            "Epoch  4 Batch  14 / 261  Training Loss  0.004120372701436281\n",
            "Epoch  4 Batch  15 / 261  Training Loss  0.008476408198475838\n",
            "Epoch  4 Batch  16 / 261  Training Loss  0.009876341558992863\n",
            "Epoch  4 Batch  17 / 261  Training Loss  0.008127814158797264\n",
            "Epoch  4 Batch  18 / 261  Training Loss  0.00896461121737957\n",
            "Epoch  4 Batch  19 / 261  Training Loss  0.008042819797992706\n",
            "Epoch  4 Batch  20 / 261  Training Loss  0.009159481152892113\n",
            "Epoch  4 Batch  21 / 261  Training Loss  0.0037645259872078896\n",
            "Epoch  4 Batch  22 / 261  Training Loss  0.008529338985681534\n",
            "Epoch  4 Batch  23 / 261  Training Loss  0.004695639945566654\n",
            "Epoch  4 Batch  24 / 261  Training Loss  0.01379544660449028\n",
            "Epoch  4 Batch  25 / 261  Training Loss  0.006308994255959988\n",
            "Epoch  4 Batch  26 / 261  Training Loss  0.00294690765440464\n",
            "Epoch  4 Batch  27 / 261  Training Loss  0.006181707605719566\n",
            "Epoch  4 Batch  28 / 261  Training Loss  0.00918726809322834\n",
            "Epoch  4 Batch  29 / 261  Training Loss  0.005831429269164801\n",
            "Epoch  4 Batch  30 / 261  Training Loss  0.009066719561815262\n",
            "Epoch  4 Batch  31 / 261  Training Loss  0.006890063174068928\n",
            "Epoch  4 Batch  32 / 261  Training Loss  0.02541712485253811\n",
            "Epoch  4 Batch  33 / 261  Training Loss  0.028536377474665642\n",
            "Epoch  4 Batch  34 / 261  Training Loss  0.013181542046368122\n",
            "Epoch  4 Batch  35 / 261  Training Loss  0.013511044904589653\n",
            "Epoch  4 Batch  36 / 261  Training Loss  0.011413175612688065\n",
            "Epoch  4 Batch  37 / 261  Training Loss  0.010973109863698483\n",
            "Epoch  4 Batch  38 / 261  Training Loss  0.00941429939121008\n",
            "Epoch  4 Batch  39 / 261  Training Loss  0.011499961838126183\n",
            "Epoch  4 Batch  40 / 261  Training Loss  0.005559947341680527\n",
            "Epoch  4 Batch  41 / 261  Training Loss  0.008779079653322697\n",
            "Epoch  4 Batch  42 / 261  Training Loss  0.0087539441883564\n",
            "Epoch  4 Batch  43 / 261  Training Loss  0.005352085921913385\n",
            "Epoch  4 Batch  44 / 261  Training Loss  0.011651987209916115\n",
            "Epoch  4 Batch  45 / 261  Training Loss  0.004199404735118151\n",
            "Epoch  4 Batch  46 / 261  Training Loss  0.006262600421905518\n",
            "Epoch  4 Batch  47 / 261  Training Loss  0.004495072178542614\n",
            "Epoch  4 Batch  48 / 261  Training Loss  0.007570967543870211\n",
            "Epoch  4 Batch  49 / 261  Training Loss  0.006985273212194443\n",
            "Epoch  4 Batch  50 / 261  Training Loss  0.006845821626484394\n",
            "Epoch  4 Batch  51 / 261  Training Loss  0.004302064888179302\n",
            "Epoch  4 Batch  52 / 261  Training Loss  0.008109759539365768\n",
            "Epoch  4 Batch  53 / 261  Training Loss  0.005945445504039526\n",
            "Epoch  4 Batch  54 / 261  Training Loss  0.0070897843688726425\n",
            "Epoch  4 Batch  55 / 261  Training Loss  0.021894406527280807\n",
            "Epoch  4 Batch  56 / 261  Training Loss  0.01442774385213852\n",
            "Epoch  4 Batch  57 / 261  Training Loss  0.005871008615940809\n",
            "Epoch  4 Batch  58 / 261  Training Loss  0.005568294320255518\n",
            "Epoch  4 Batch  59 / 261  Training Loss  0.011908549815416336\n",
            "Epoch  4 Batch  60 / 261  Training Loss  0.010972823016345501\n",
            "Epoch  4 Batch  61 / 261  Training Loss  0.020640021190047264\n",
            "Epoch  4 Batch  62 / 261  Training Loss  0.0045124501921236515\n",
            "Epoch  4 Batch  63 / 261  Training Loss  0.012978272512555122\n",
            "Epoch  4 Batch  64 / 261  Training Loss  0.018045837059617043\n",
            "Epoch  4 Batch  65 / 261  Training Loss  0.007880059070885181\n",
            "Epoch  4 Batch  66 / 261  Training Loss  0.004255488980561495\n",
            "Epoch  4 Batch  67 / 261  Training Loss  0.004875278566032648\n",
            "Epoch  4 Batch  68 / 261  Training Loss  0.00332440878264606\n",
            "Epoch  4 Batch  69 / 261  Training Loss  0.0036136116832494736\n",
            "Epoch  4 Batch  70 / 261  Training Loss  0.0035702413879334927\n",
            "Epoch  4 Batch  71 / 261  Training Loss  0.005664571654051542\n",
            "Epoch  4 Batch  72 / 261  Training Loss  0.009223805740475655\n",
            "Epoch  4 Batch  73 / 261  Training Loss  0.00942476186901331\n",
            "Epoch  4 Batch  74 / 261  Training Loss  0.015607266686856747\n",
            "Epoch  4 Batch  75 / 261  Training Loss  0.024529803544282913\n",
            "Epoch  4 Batch  76 / 261  Training Loss  0.01396200805902481\n",
            "Epoch  4 Batch  77 / 261  Training Loss  0.012021479196846485\n",
            "Epoch  4 Batch  78 / 261  Training Loss  0.006128407549113035\n",
            "Epoch  4 Batch  79 / 261  Training Loss  0.006171406712383032\n",
            "Epoch  4 Batch  80 / 261  Training Loss  0.014380764216184616\n",
            "Epoch  4 Batch  81 / 261  Training Loss  0.012046829797327518\n",
            "Epoch  4 Batch  82 / 261  Training Loss  0.004608814604580402\n",
            "Epoch  4 Batch  83 / 261  Training Loss  0.004710434004664421\n",
            "Epoch  4 Batch  84 / 261  Training Loss  0.0044996836222708225\n",
            "Epoch  4 Batch  85 / 261  Training Loss  0.005406088661402464\n",
            "Epoch  4 Batch  86 / 261  Training Loss  0.004293167497962713\n",
            "Epoch  4 Batch  87 / 261  Training Loss  0.00347269419580698\n",
            "Epoch  4 Batch  88 / 261  Training Loss  0.0088871531188488\n",
            "Epoch  4 Batch  89 / 261  Training Loss  0.006025663111358881\n",
            "Epoch  4 Batch  90 / 261  Training Loss  0.006773529574275017\n",
            "Epoch  4 Batch  91 / 261  Training Loss  0.00482075335457921\n",
            "Epoch  4 Batch  92 / 261  Training Loss  0.005619505420327187\n",
            "Epoch  4 Batch  93 / 261  Training Loss  0.0055754100903868675\n",
            "Epoch  4 Batch  94 / 261  Training Loss  0.004036081489175558\n",
            "Epoch  4 Batch  95 / 261  Training Loss  0.008295286446809769\n",
            "Epoch  4 Batch  96 / 261  Training Loss  0.006018679589033127\n",
            "Epoch  4 Batch  97 / 261  Training Loss  0.008092474192380905\n",
            "Epoch  4 Batch  98 / 261  Training Loss  0.0032031142618507147\n",
            "Epoch  4 Batch  99 / 261  Training Loss  0.003512793453410268\n",
            "Epoch  4 Batch  100 / 261  Training Loss  0.0026895608752965927\n",
            "Epoch  4 Batch  101 / 261  Training Loss  0.002291038865223527\n",
            "Epoch  4 Batch  102 / 261  Training Loss  0.005135795567184687\n",
            "Epoch  4 Batch  103 / 261  Training Loss  0.004359944257885218\n",
            "Epoch  4 Batch  104 / 261  Training Loss  0.004287752788513899\n",
            "Epoch  4 Batch  105 / 261  Training Loss  0.0061789024621248245\n",
            "Epoch  4 Batch  106 / 261  Training Loss  0.004187407437711954\n",
            "Epoch  4 Batch  107 / 261  Training Loss  0.003680751658976078\n",
            "Epoch  4 Batch  108 / 261  Training Loss  0.0036318558268249035\n",
            "Epoch  4 Batch  109 / 261  Training Loss  0.005650967359542847\n",
            "Epoch  4 Batch  110 / 261  Training Loss  0.013741891831159592\n",
            "Epoch  4 Batch  111 / 261  Training Loss  0.00656053377315402\n",
            "Epoch  4 Batch  112 / 261  Training Loss  0.008979876525700092\n",
            "Epoch  4 Batch  113 / 261  Training Loss  0.011918666772544384\n",
            "Epoch  4 Batch  114 / 261  Training Loss  0.01901303045451641\n",
            "Epoch  4 Batch  115 / 261  Training Loss  0.007317769341170788\n",
            "Epoch  4 Batch  116 / 261  Training Loss  0.012044801376760006\n",
            "Epoch  4 Batch  117 / 261  Training Loss  0.0035262873861938715\n",
            "Epoch  4 Batch  118 / 261  Training Loss  0.002968662418425083\n",
            "Epoch  4 Batch  119 / 261  Training Loss  0.010085338726639748\n",
            "Epoch  4 Batch  120 / 261  Training Loss  0.007904964499175549\n",
            "Epoch  4 Batch  121 / 261  Training Loss  0.0034719095565378666\n",
            "Epoch  4 Batch  122 / 261  Training Loss  0.005534963216632605\n",
            "Epoch  4 Batch  123 / 261  Training Loss  0.003457489889115095\n",
            "Epoch  4 Batch  124 / 261  Training Loss  0.005176620092242956\n",
            "Epoch  4 Batch  125 / 261  Training Loss  0.006796475499868393\n",
            "Epoch  4 Batch  126 / 261  Training Loss  0.004912990145385265\n",
            "Epoch  4 Batch  127 / 261  Training Loss  0.00313186994753778\n",
            "Epoch  4 Batch  128 / 261  Training Loss  0.004426419269293547\n",
            "Epoch  4 Batch  129 / 261  Training Loss  0.0026442825328558683\n",
            "Epoch  4 Batch  130 / 261  Training Loss  0.016574371606111526\n",
            "Epoch  4 Batch  131 / 261  Training Loss  0.008656262420117855\n",
            "Epoch  4 Batch  132 / 261  Training Loss  0.020335929468274117\n",
            "Epoch  4 Batch  133 / 261  Training Loss  0.011572860181331635\n",
            "Epoch  4 Batch  134 / 261  Training Loss  0.036956872791051865\n",
            "Epoch  4 Batch  135 / 261  Training Loss  0.011213278397917747\n",
            "Epoch  4 Batch  136 / 261  Training Loss  0.00466540502384305\n",
            "Epoch  4 Batch  137 / 261  Training Loss  0.018002761527895927\n",
            "Epoch  4 Batch  138 / 261  Training Loss  0.01129245012998581\n",
            "Epoch  4 Batch  139 / 261  Training Loss  0.019806088879704475\n",
            "Epoch  4 Batch  140 / 261  Training Loss  0.07515081018209457\n",
            "Epoch  4 Batch  141 / 261  Training Loss  0.039341770112514496\n",
            "Epoch  4 Batch  142 / 261  Training Loss  0.03673085942864418\n",
            "Epoch  4 Batch  143 / 261  Training Loss  0.03573552519083023\n",
            "Epoch  4 Batch  144 / 261  Training Loss  0.010566925629973412\n",
            "Epoch  4 Batch  145 / 261  Training Loss  0.011749111115932465\n",
            "Epoch  4 Batch  146 / 261  Training Loss  0.008747034706175327\n",
            "Epoch  4 Batch  147 / 261  Training Loss  0.009106345474720001\n",
            "Epoch  4 Batch  148 / 261  Training Loss  0.007487595081329346\n",
            "Epoch  4 Batch  149 / 261  Training Loss  0.006329289637506008\n",
            "Epoch  4 Batch  150 / 261  Training Loss  0.00528967147693038\n",
            "Epoch  4 Batch  151 / 261  Training Loss  0.009571029804646969\n",
            "Epoch  4 Batch  152 / 261  Training Loss  0.006895165424793959\n",
            "Epoch  4 Batch  153 / 261  Training Loss  0.008788109757006168\n",
            "Epoch  4 Batch  154 / 261  Training Loss  0.005205180495977402\n",
            "Epoch  4 Batch  155 / 261  Training Loss  0.01542746089398861\n",
            "Epoch  4 Batch  156 / 261  Training Loss  0.006693780887871981\n",
            "Epoch  4 Batch  157 / 261  Training Loss  0.009916124865412712\n",
            "Epoch  4 Batch  158 / 261  Training Loss  0.015536641702055931\n",
            "Epoch  4 Batch  159 / 261  Training Loss  0.006625204812735319\n",
            "Epoch  4 Batch  160 / 261  Training Loss  0.006663980428129435\n",
            "Epoch  4 Batch  161 / 261  Training Loss  0.006790926679968834\n",
            "Epoch  4 Batch  162 / 261  Training Loss  0.0046487539075315\n",
            "Epoch  4 Batch  163 / 261  Training Loss  0.005068299826234579\n",
            "Epoch  4 Batch  164 / 261  Training Loss  0.004158382304012775\n",
            "Epoch  4 Batch  165 / 261  Training Loss  0.02048608846962452\n",
            "Epoch  4 Batch  166 / 261  Training Loss  0.007925615645945072\n",
            "Epoch  4 Batch  167 / 261  Training Loss  0.004452284425497055\n",
            "Epoch  4 Batch  168 / 261  Training Loss  0.009378775022923946\n",
            "Epoch  4 Batch  169 / 261  Training Loss  0.006950130686163902\n",
            "Epoch  4 Batch  170 / 261  Training Loss  0.007113987114280462\n",
            "Epoch  4 Batch  171 / 261  Training Loss  0.01132657565176487\n",
            "Epoch  4 Batch  172 / 261  Training Loss  0.010139849968254566\n",
            "Epoch  4 Batch  173 / 261  Training Loss  0.009756545536220074\n",
            "Epoch  4 Batch  174 / 261  Training Loss  0.006661178078502417\n",
            "Epoch  4 Batch  175 / 261  Training Loss  0.01646340638399124\n",
            "Epoch  4 Batch  176 / 261  Training Loss  0.008330218493938446\n",
            "Epoch  4 Batch  177 / 261  Training Loss  0.0046614063903689384\n",
            "Epoch  4 Batch  178 / 261  Training Loss  0.004267038777470589\n",
            "Epoch  4 Batch  179 / 261  Training Loss  0.00468286732211709\n",
            "Epoch  4 Batch  180 / 261  Training Loss  0.0074636260978877544\n",
            "Epoch  4 Batch  181 / 261  Training Loss  0.00583513081073761\n",
            "Epoch  4 Batch  182 / 261  Training Loss  0.002858387539163232\n",
            "Epoch  4 Batch  183 / 261  Training Loss  0.010815867222845554\n",
            "Epoch  4 Batch  184 / 261  Training Loss  0.004514144733548164\n",
            "Epoch  4 Batch  185 / 261  Training Loss  0.014040295965969563\n",
            "Epoch  4 Batch  186 / 261  Training Loss  0.00805972795933485\n",
            "Epoch  4 Batch  187 / 261  Training Loss  0.005429578945040703\n",
            "Epoch  4 Batch  188 / 261  Training Loss  0.0026185554452240467\n",
            "Epoch  4 Batch  189 / 261  Training Loss  0.011487461626529694\n",
            "Epoch  4 Batch  190 / 261  Training Loss  0.01699506677687168\n",
            "Epoch  4 Batch  191 / 261  Training Loss  0.0052651637233793736\n",
            "Epoch  4 Batch  192 / 261  Training Loss  0.0031813951209187508\n",
            "Epoch  4 Batch  193 / 261  Training Loss  0.007248444948345423\n",
            "Epoch  4 Batch  194 / 261  Training Loss  0.0029567936435341835\n",
            "Epoch  4 Batch  195 / 261  Training Loss  0.0032749148085713387\n",
            "Epoch  4 Batch  196 / 261  Training Loss  0.005020508076995611\n",
            "Epoch  4 Batch  197 / 261  Training Loss  0.0048082382418215275\n",
            "Epoch  4 Batch  198 / 261  Training Loss  0.00722290575504303\n",
            "Epoch  4 Batch  199 / 261  Training Loss  0.002908819355070591\n",
            "Epoch  4 Batch  200 / 261  Training Loss  0.0042264750227332115\n",
            "Epoch  4 Batch  201 / 261  Training Loss  0.004777705762535334\n",
            "Epoch  4 Batch  202 / 261  Training Loss  0.005694427061825991\n",
            "Epoch  4 Batch  203 / 261  Training Loss  0.013060481287539005\n",
            "Epoch  4 Batch  204 / 261  Training Loss  0.012936906889081001\n",
            "Epoch  4 Batch  205 / 261  Training Loss  0.010979161597788334\n",
            "Epoch  4 Batch  206 / 261  Training Loss  0.002778762485831976\n",
            "Epoch  4 Batch  207 / 261  Training Loss  0.005304272286593914\n",
            "Epoch  4 Batch  208 / 261  Training Loss  0.005220457911491394\n",
            "Epoch  4 Batch  209 / 261  Training Loss  0.003912833984941244\n",
            "Epoch  4 Batch  210 / 261  Training Loss  0.0025106968823820353\n",
            "Epoch  4 Batch  211 / 261  Training Loss  0.004951148293912411\n",
            "Epoch  4 Batch  212 / 261  Training Loss  0.002781348302960396\n",
            "Epoch  4 Batch  213 / 261  Training Loss  0.0042182751931250095\n",
            "Epoch  4 Batch  214 / 261  Training Loss  0.0022164969705045223\n",
            "Epoch  4 Batch  215 / 261  Training Loss  0.004778806120157242\n",
            "Epoch  4 Batch  216 / 261  Training Loss  0.004899397026747465\n",
            "Epoch  4 Batch  217 / 261  Training Loss  0.005296132527291775\n",
            "Epoch  4 Batch  218 / 261  Training Loss  0.006428899243474007\n",
            "Epoch  4 Batch  219 / 261  Training Loss  0.006664838641881943\n",
            "Epoch  4 Batch  220 / 261  Training Loss  0.0033268483821302652\n",
            "Epoch  4 Batch  221 / 261  Training Loss  0.0038308408111333847\n",
            "Epoch  4 Batch  222 / 261  Training Loss  0.0023388939443975687\n",
            "Epoch  4 Batch  223 / 261  Training Loss  0.0035576631780713797\n",
            "Epoch  4 Batch  224 / 261  Training Loss  0.006899914238601923\n",
            "Epoch  4 Batch  225 / 261  Training Loss  0.008710918948054314\n",
            "Epoch  4 Batch  226 / 261  Training Loss  0.006357321050018072\n",
            "Epoch  4 Batch  227 / 261  Training Loss  0.006498694885522127\n",
            "Epoch  4 Batch  228 / 261  Training Loss  0.0069391969591379166\n",
            "Epoch  4 Batch  229 / 261  Training Loss  0.002089382614940405\n",
            "Epoch  4 Batch  230 / 261  Training Loss  0.002346463268622756\n",
            "Epoch  4 Batch  231 / 261  Training Loss  0.0018143695779144764\n",
            "Epoch  4 Batch  232 / 261  Training Loss  0.002812526421621442\n",
            "Epoch  4 Batch  233 / 261  Training Loss  0.004032645374536514\n",
            "Epoch  4 Batch  234 / 261  Training Loss  0.002419116673991084\n",
            "Epoch  4 Batch  235 / 261  Training Loss  0.0040751053020358086\n",
            "Epoch  4 Batch  236 / 261  Training Loss  0.0011402093805372715\n",
            "Epoch  4 Batch  237 / 261  Training Loss  0.0033363327383995056\n",
            "Epoch  4 Batch  238 / 261  Training Loss  0.004194758366793394\n",
            "Epoch  4 Batch  239 / 261  Training Loss  0.002443765988573432\n",
            "Epoch  4 Batch  240 / 261  Training Loss  0.00239087943919003\n",
            "Epoch  4 Batch  241 / 261  Training Loss  0.0065327538177371025\n",
            "Epoch  4 Batch  242 / 261  Training Loss  0.0015029088826850057\n",
            "Epoch  4 Batch  243 / 261  Training Loss  0.005437878426164389\n",
            "Epoch  4 Batch  244 / 261  Training Loss  0.003511678194627166\n",
            "Epoch  4 Batch  245 / 261  Training Loss  0.001769412774592638\n",
            "Epoch  4 Batch  246 / 261  Training Loss  0.0019180669914931059\n",
            "Epoch  4 Batch  247 / 261  Training Loss  0.0025950088165700436\n",
            "Epoch  4 Batch  248 / 261  Training Loss  0.002111972076818347\n",
            "Epoch  4 Batch  249 / 261  Training Loss  0.003347135381773114\n",
            "Epoch  4 Batch  250 / 261  Training Loss  0.0032501209061592817\n",
            "Epoch  4 Batch  251 / 261  Training Loss  0.002482033334672451\n",
            "Epoch  4 Batch  252 / 261  Training Loss  0.0018572744447737932\n",
            "Epoch  4 Batch  253 / 261  Training Loss  0.004414195194840431\n",
            "Epoch  4 Batch  254 / 261  Training Loss  0.003039364470168948\n",
            "Epoch  4 Batch  255 / 261  Training Loss  0.0030327760614454746\n",
            "Epoch  4 Batch  256 / 261  Training Loss  0.002691983012482524\n",
            "Epoch  4 Batch  257 / 261  Training Loss  0.005655620712786913\n",
            "Epoch  4 Batch  258 / 261  Training Loss  0.005572427995502949\n",
            "Epoch  4 Batch  259 / 261  Training Loss  0.008785544894635677\n",
            "Epoch  4 Batch  260 / 261  Training Loss  0.004527091048657894\n",
            "   5    |    -    |   0.008037   | 96.442819\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 5\n",
            "Epoch  5 Batch  0 / 261  Training Loss  0.005254658404737711\n",
            "Epoch  5 Batch  1 / 261  Training Loss  0.0014416004996746778\n",
            "Epoch  5 Batch  2 / 261  Training Loss  0.0012813182547688484\n",
            "Epoch  5 Batch  3 / 261  Training Loss  0.007120975758880377\n",
            "Epoch  5 Batch  4 / 261  Training Loss  0.002842921996489167\n",
            "Epoch  5 Batch  5 / 261  Training Loss  0.005128946155309677\n",
            "Epoch  5 Batch  6 / 261  Training Loss  0.001614776556380093\n",
            "Epoch  5 Batch  7 / 261  Training Loss  0.00221655098721385\n",
            "Epoch  5 Batch  8 / 261  Training Loss  0.0013573834439739585\n",
            "Epoch  5 Batch  9 / 261  Training Loss  0.004032132215797901\n",
            "Epoch  5 Batch  10 / 261  Training Loss  0.007479310501366854\n",
            "Epoch  5 Batch  11 / 261  Training Loss  0.006516273133456707\n",
            "Epoch  5 Batch  12 / 261  Training Loss  0.012561543844640255\n",
            "Epoch  5 Batch  13 / 261  Training Loss  0.0014547419268637896\n",
            "Epoch  5 Batch  14 / 261  Training Loss  0.002023262670263648\n",
            "Epoch  5 Batch  15 / 261  Training Loss  0.003568840678781271\n",
            "Epoch  5 Batch  16 / 261  Training Loss  0.002130575943738222\n",
            "Epoch  5 Batch  17 / 261  Training Loss  0.0039018639363348484\n",
            "Epoch  5 Batch  18 / 261  Training Loss  0.0018073896644636989\n",
            "Epoch  5 Batch  19 / 261  Training Loss  0.008261092938482761\n",
            "Epoch  5 Batch  20 / 261  Training Loss  0.006791756022721529\n",
            "Epoch  5 Batch  21 / 261  Training Loss  0.007107771001756191\n",
            "Epoch  5 Batch  22 / 261  Training Loss  0.0027171727269887924\n",
            "Epoch  5 Batch  23 / 261  Training Loss  0.001423333422280848\n",
            "Epoch  5 Batch  24 / 261  Training Loss  0.0018657608889043331\n",
            "Epoch  5 Batch  25 / 261  Training Loss  0.00635376013815403\n",
            "Epoch  5 Batch  26 / 261  Training Loss  0.0017698679585009813\n",
            "Epoch  5 Batch  27 / 261  Training Loss  0.0035602704156190157\n",
            "Epoch  5 Batch  28 / 261  Training Loss  0.0015082800528034568\n",
            "Epoch  5 Batch  29 / 261  Training Loss  0.0016524484381079674\n",
            "Epoch  5 Batch  30 / 261  Training Loss  0.003169604577124119\n",
            "Epoch  5 Batch  31 / 261  Training Loss  0.004323969129472971\n",
            "Epoch  5 Batch  32 / 261  Training Loss  0.0019432936096563935\n",
            "Epoch  5 Batch  33 / 261  Training Loss  0.004062403459101915\n",
            "Epoch  5 Batch  34 / 261  Training Loss  0.007587467320263386\n",
            "Epoch  5 Batch  35 / 261  Training Loss  0.18435974419116974\n",
            "Epoch  5 Batch  36 / 261  Training Loss  0.3751859962940216\n",
            "Epoch  5 Batch  37 / 261  Training Loss  0.08594322204589844\n",
            "Epoch  5 Batch  38 / 261  Training Loss  0.04122211039066315\n",
            "Epoch  5 Batch  39 / 261  Training Loss  0.023862473666667938\n",
            "Epoch  5 Batch  40 / 261  Training Loss  0.030193399637937546\n",
            "Epoch  5 Batch  41 / 261  Training Loss  0.010900421999394894\n",
            "Epoch  5 Batch  42 / 261  Training Loss  0.023930810391902924\n",
            "Epoch  5 Batch  43 / 261  Training Loss  0.023267406970262527\n",
            "Epoch  5 Batch  44 / 261  Training Loss  0.015128008089959621\n",
            "Epoch  5 Batch  45 / 261  Training Loss  0.006385481916368008\n",
            "Epoch  5 Batch  46 / 261  Training Loss  0.005097519606351852\n",
            "Epoch  5 Batch  47 / 261  Training Loss  0.007719747256487608\n",
            "Epoch  5 Batch  48 / 261  Training Loss  0.007211831398308277\n",
            "Epoch  5 Batch  49 / 261  Training Loss  0.006198377814143896\n",
            "Epoch  5 Batch  50 / 261  Training Loss  0.009047581814229488\n",
            "Epoch  5 Batch  51 / 261  Training Loss  0.004367728717625141\n",
            "Epoch  5 Batch  52 / 261  Training Loss  0.00511445477604866\n",
            "Epoch  5 Batch  53 / 261  Training Loss  0.017510538920760155\n",
            "Epoch  5 Batch  54 / 261  Training Loss  0.006202301476150751\n",
            "Epoch  5 Batch  55 / 261  Training Loss  0.0031823718454688787\n",
            "Epoch  5 Batch  56 / 261  Training Loss  0.004479061812162399\n",
            "Epoch  5 Batch  57 / 261  Training Loss  0.004996966570615768\n",
            "Epoch  5 Batch  58 / 261  Training Loss  0.008662299253046513\n",
            "Epoch  5 Batch  59 / 261  Training Loss  0.003373613115400076\n",
            "Epoch  5 Batch  60 / 261  Training Loss  0.004884351044893265\n",
            "Epoch  5 Batch  61 / 261  Training Loss  0.006011837162077427\n",
            "Epoch  5 Batch  62 / 261  Training Loss  0.005754494573920965\n",
            "Epoch  5 Batch  63 / 261  Training Loss  0.002800176851451397\n",
            "Epoch  5 Batch  64 / 261  Training Loss  0.002974726026877761\n",
            "Epoch  5 Batch  65 / 261  Training Loss  0.0025619440712034702\n",
            "Epoch  5 Batch  66 / 261  Training Loss  0.0033888996113091707\n",
            "Epoch  5 Batch  67 / 261  Training Loss  0.002788247773423791\n",
            "Epoch  5 Batch  68 / 261  Training Loss  0.002692680573090911\n",
            "Epoch  5 Batch  69 / 261  Training Loss  0.0015728081343695521\n",
            "Epoch  5 Batch  70 / 261  Training Loss  0.0027652352582663298\n",
            "Epoch  5 Batch  71 / 261  Training Loss  0.004014474805444479\n",
            "Epoch  5 Batch  72 / 261  Training Loss  0.002768491394817829\n",
            "Epoch  5 Batch  73 / 261  Training Loss  0.002038063481450081\n",
            "Epoch  5 Batch  74 / 261  Training Loss  0.0021646309178322554\n",
            "Epoch  5 Batch  75 / 261  Training Loss  0.0015599060570821166\n",
            "Epoch  5 Batch  76 / 261  Training Loss  0.0014705698704347014\n",
            "Epoch  5 Batch  77 / 261  Training Loss  0.0038345667999237776\n",
            "Epoch  5 Batch  78 / 261  Training Loss  0.0016489543486386538\n",
            "Epoch  5 Batch  79 / 261  Training Loss  0.004525260534137487\n",
            "Epoch  5 Batch  80 / 261  Training Loss  0.0018685641698539257\n",
            "Epoch  5 Batch  81 / 261  Training Loss  0.004746818449348211\n",
            "Epoch  5 Batch  82 / 261  Training Loss  0.0020071854814887047\n",
            "Epoch  5 Batch  83 / 261  Training Loss  0.004668230190873146\n",
            "Epoch  5 Batch  84 / 261  Training Loss  0.0037694836501032114\n",
            "Epoch  5 Batch  85 / 261  Training Loss  0.005155095364898443\n",
            "Epoch  5 Batch  86 / 261  Training Loss  0.0028084763325750828\n",
            "Epoch  5 Batch  87 / 261  Training Loss  0.002454518573358655\n",
            "Epoch  5 Batch  88 / 261  Training Loss  0.002420743927359581\n",
            "Epoch  5 Batch  89 / 261  Training Loss  0.0048181721940636635\n",
            "Epoch  5 Batch  90 / 261  Training Loss  0.004636817146092653\n",
            "Epoch  5 Batch  91 / 261  Training Loss  0.00139021931681782\n",
            "Epoch  5 Batch  92 / 261  Training Loss  0.0018196499440819025\n",
            "Epoch  5 Batch  93 / 261  Training Loss  0.0020627020858228207\n",
            "Epoch  5 Batch  94 / 261  Training Loss  0.002119731390848756\n",
            "Epoch  5 Batch  95 / 261  Training Loss  0.002174518071115017\n",
            "Epoch  5 Batch  96 / 261  Training Loss  0.00272944918833673\n",
            "Epoch  5 Batch  97 / 261  Training Loss  0.007620128337293863\n",
            "Epoch  5 Batch  98 / 261  Training Loss  0.007425053045153618\n",
            "Epoch  5 Batch  99 / 261  Training Loss  0.005611278582364321\n",
            "Epoch  5 Batch  100 / 261  Training Loss  0.003771265037357807\n",
            "Epoch  5 Batch  101 / 261  Training Loss  0.0031536156311631203\n",
            "Epoch  5 Batch  102 / 261  Training Loss  0.004192018881440163\n",
            "Epoch  5 Batch  103 / 261  Training Loss  0.00449691666290164\n",
            "Epoch  5 Batch  104 / 261  Training Loss  0.004795496817678213\n",
            "Epoch  5 Batch  105 / 261  Training Loss  0.0045110066421329975\n",
            "Epoch  5 Batch  106 / 261  Training Loss  0.002373573137447238\n",
            "Epoch  5 Batch  107 / 261  Training Loss  0.006519982125610113\n",
            "Epoch  5 Batch  108 / 261  Training Loss  0.002190177096053958\n",
            "Epoch  5 Batch  109 / 261  Training Loss  0.0017481936374679208\n",
            "Epoch  5 Batch  110 / 261  Training Loss  0.0032378246542066336\n",
            "Epoch  5 Batch  111 / 261  Training Loss  0.010185846127569675\n",
            "Epoch  5 Batch  112 / 261  Training Loss  0.013983516953885555\n",
            "Epoch  5 Batch  113 / 261  Training Loss  0.01670605130493641\n",
            "Epoch  5 Batch  114 / 261  Training Loss  0.00915979128330946\n",
            "Epoch  5 Batch  115 / 261  Training Loss  0.003391494043171406\n",
            "Epoch  5 Batch  116 / 261  Training Loss  0.0019980676006525755\n",
            "Epoch  5 Batch  117 / 261  Training Loss  0.002647747751325369\n",
            "Epoch  5 Batch  118 / 261  Training Loss  0.005072576459497213\n",
            "Epoch  5 Batch  119 / 261  Training Loss  0.003926335368305445\n",
            "Epoch  5 Batch  120 / 261  Training Loss  0.0012525987112894654\n",
            "Epoch  5 Batch  121 / 261  Training Loss  0.001073226216249168\n",
            "Epoch  5 Batch  122 / 261  Training Loss  0.002622167579829693\n",
            "Epoch  5 Batch  123 / 261  Training Loss  0.002298183273524046\n",
            "Epoch  5 Batch  124 / 261  Training Loss  0.0015948223881423473\n",
            "Epoch  5 Batch  125 / 261  Training Loss  0.0018400989938527346\n",
            "Epoch  5 Batch  126 / 261  Training Loss  0.0037818467244505882\n",
            "Epoch  5 Batch  127 / 261  Training Loss  0.002660224447026849\n",
            "Epoch  5 Batch  128 / 261  Training Loss  0.0026253231335431337\n",
            "Epoch  5 Batch  129 / 261  Training Loss  0.0018832497298717499\n",
            "Epoch  5 Batch  130 / 261  Training Loss  0.012236136011779308\n",
            "Epoch  5 Batch  131 / 261  Training Loss  0.0021557894069701433\n",
            "Epoch  5 Batch  132 / 261  Training Loss  0.0026788199320435524\n",
            "Epoch  5 Batch  133 / 261  Training Loss  0.0011865386040881276\n",
            "Epoch  5 Batch  134 / 261  Training Loss  0.0028769290074706078\n",
            "Epoch  5 Batch  135 / 261  Training Loss  0.003908497281372547\n",
            "Epoch  5 Batch  136 / 261  Training Loss  0.0038539310917258263\n",
            "Epoch  5 Batch  137 / 261  Training Loss  0.003146910108625889\n",
            "Epoch  5 Batch  138 / 261  Training Loss  0.004661057610064745\n",
            "Epoch  5 Batch  139 / 261  Training Loss  0.0035351235419511795\n",
            "Epoch  5 Batch  140 / 261  Training Loss  0.012126299552619457\n",
            "Epoch  5 Batch  141 / 261  Training Loss  0.003702149959281087\n",
            "Epoch  5 Batch  142 / 261  Training Loss  0.0013576661003753543\n",
            "Epoch  5 Batch  143 / 261  Training Loss  0.005839217454195023\n",
            "Epoch  5 Batch  144 / 261  Training Loss  0.004873408004641533\n",
            "Epoch  5 Batch  145 / 261  Training Loss  0.006159400567412376\n",
            "Epoch  5 Batch  146 / 261  Training Loss  0.0018080775626003742\n",
            "Epoch  5 Batch  147 / 261  Training Loss  0.016768867149949074\n",
            "Epoch  5 Batch  148 / 261  Training Loss  0.006305126007646322\n",
            "Epoch  5 Batch  149 / 261  Training Loss  0.004493351094424725\n",
            "Epoch  5 Batch  150 / 261  Training Loss  0.002270445926114917\n",
            "Epoch  5 Batch  151 / 261  Training Loss  0.0072683305479586124\n",
            "Epoch  5 Batch  152 / 261  Training Loss  0.003638005582615733\n",
            "Epoch  5 Batch  153 / 261  Training Loss  0.003014632500708103\n",
            "Epoch  5 Batch  154 / 261  Training Loss  0.0011515287915244699\n",
            "Epoch  5 Batch  155 / 261  Training Loss  0.004341142252087593\n",
            "Epoch  5 Batch  156 / 261  Training Loss  0.005501814652234316\n",
            "Epoch  5 Batch  157 / 261  Training Loss  0.008384248241782188\n",
            "Epoch  5 Batch  158 / 261  Training Loss  0.0028457152657210827\n",
            "Epoch  5 Batch  159 / 261  Training Loss  0.0027687654364854097\n",
            "Epoch  5 Batch  160 / 261  Training Loss  0.008323177695274353\n",
            "Epoch  5 Batch  161 / 261  Training Loss  0.004774876404553652\n",
            "Epoch  5 Batch  162 / 261  Training Loss  0.0018439872656017542\n",
            "Epoch  5 Batch  163 / 261  Training Loss  0.002661684760823846\n",
            "Epoch  5 Batch  164 / 261  Training Loss  0.002967672422528267\n",
            "Epoch  5 Batch  165 / 261  Training Loss  0.0013859623577445745\n",
            "Epoch  5 Batch  166 / 261  Training Loss  0.0029436468612402678\n",
            "Epoch  5 Batch  167 / 261  Training Loss  0.003335021436214447\n",
            "Epoch  5 Batch  168 / 261  Training Loss  0.0015176136512309313\n",
            "Epoch  5 Batch  169 / 261  Training Loss  0.003598471637815237\n",
            "Epoch  5 Batch  170 / 261  Training Loss  0.0033403211273252964\n",
            "Epoch  5 Batch  171 / 261  Training Loss  0.0030725037213414907\n",
            "Epoch  5 Batch  172 / 261  Training Loss  0.0020519637037068605\n",
            "Epoch  5 Batch  173 / 261  Training Loss  0.004274685867130756\n",
            "Epoch  5 Batch  174 / 261  Training Loss  0.009448093362152576\n",
            "Epoch  5 Batch  175 / 261  Training Loss  0.0036875975783914328\n",
            "Epoch  5 Batch  176 / 261  Training Loss  0.004875293001532555\n",
            "Epoch  5 Batch  177 / 261  Training Loss  0.0026247715577483177\n",
            "Epoch  5 Batch  178 / 261  Training Loss  0.0043530841358006\n",
            "Epoch  5 Batch  179 / 261  Training Loss  0.002749108709394932\n",
            "Epoch  5 Batch  180 / 261  Training Loss  0.0023727372754365206\n",
            "Epoch  5 Batch  181 / 261  Training Loss  0.002877160906791687\n",
            "Epoch  5 Batch  182 / 261  Training Loss  0.004044887609779835\n",
            "Epoch  5 Batch  183 / 261  Training Loss  0.0019400283927097917\n",
            "Epoch  5 Batch  184 / 261  Training Loss  0.000937433447688818\n",
            "Epoch  5 Batch  185 / 261  Training Loss  0.0014737413730472326\n",
            "Epoch  5 Batch  186 / 261  Training Loss  0.002398769836872816\n",
            "Epoch  5 Batch  187 / 261  Training Loss  0.0013018564786762\n",
            "Epoch  5 Batch  188 / 261  Training Loss  0.0011060195975005627\n",
            "Epoch  5 Batch  189 / 261  Training Loss  0.0011450080201029778\n",
            "Epoch  5 Batch  190 / 261  Training Loss  0.0029425444081425667\n",
            "Epoch  5 Batch  191 / 261  Training Loss  0.001418549451045692\n",
            "Epoch  5 Batch  192 / 261  Training Loss  0.007362670730799437\n",
            "Epoch  5 Batch  193 / 261  Training Loss  0.0031986951362341642\n",
            "Epoch  5 Batch  194 / 261  Training Loss  0.003835882293060422\n",
            "Epoch  5 Batch  195 / 261  Training Loss  0.002157253446057439\n",
            "Epoch  5 Batch  196 / 261  Training Loss  0.0018015061505138874\n",
            "Epoch  5 Batch  197 / 261  Training Loss  0.002745532663539052\n",
            "Epoch  5 Batch  198 / 261  Training Loss  0.005487332120537758\n",
            "Epoch  5 Batch  199 / 261  Training Loss  0.002589485142379999\n",
            "Epoch  5 Batch  200 / 261  Training Loss  0.0026599103584885597\n",
            "Epoch  5 Batch  201 / 261  Training Loss  0.002928317990154028\n",
            "Epoch  5 Batch  202 / 261  Training Loss  0.0035367822274565697\n",
            "Epoch  5 Batch  203 / 261  Training Loss  0.0016105097020044923\n",
            "Epoch  5 Batch  204 / 261  Training Loss  0.0015751938335597515\n",
            "Epoch  5 Batch  205 / 261  Training Loss  0.002107388572767377\n",
            "Epoch  5 Batch  206 / 261  Training Loss  0.002112995833158493\n",
            "Epoch  5 Batch  207 / 261  Training Loss  0.001966397976502776\n",
            "Epoch  5 Batch  208 / 261  Training Loss  0.004452626220881939\n",
            "Epoch  5 Batch  209 / 261  Training Loss  0.007115636952221394\n",
            "Epoch  5 Batch  210 / 261  Training Loss  0.0014208839274942875\n",
            "Epoch  5 Batch  211 / 261  Training Loss  0.002622391330078244\n",
            "Epoch  5 Batch  212 / 261  Training Loss  0.011418848298490047\n",
            "Epoch  5 Batch  213 / 261  Training Loss  0.003666640492156148\n",
            "Epoch  5 Batch  214 / 261  Training Loss  0.0031419622246176004\n",
            "Epoch  5 Batch  215 / 261  Training Loss  0.0024603656493127346\n",
            "Epoch  5 Batch  216 / 261  Training Loss  0.0028965536039322615\n",
            "Epoch  5 Batch  217 / 261  Training Loss  0.006250032689422369\n",
            "Epoch  5 Batch  218 / 261  Training Loss  0.00092738913372159\n",
            "Epoch  5 Batch  219 / 261  Training Loss  0.004065569490194321\n",
            "Epoch  5 Batch  220 / 261  Training Loss  0.003494763746857643\n",
            "Epoch  5 Batch  221 / 261  Training Loss  0.0017095698276534677\n",
            "Epoch  5 Batch  222 / 261  Training Loss  0.0025495332665741444\n",
            "Epoch  5 Batch  223 / 261  Training Loss  0.005432413890957832\n",
            "Epoch  5 Batch  224 / 261  Training Loss  0.0054027605801820755\n",
            "Epoch  5 Batch  225 / 261  Training Loss  0.002673487411811948\n",
            "Epoch  5 Batch  226 / 261  Training Loss  0.0015956353163346648\n",
            "Epoch  5 Batch  227 / 261  Training Loss  0.005122147034853697\n",
            "Epoch  5 Batch  228 / 261  Training Loss  0.013012166135013103\n",
            "Epoch  5 Batch  229 / 261  Training Loss  0.001693048165179789\n",
            "Epoch  5 Batch  230 / 261  Training Loss  0.0015706397825852036\n",
            "Epoch  5 Batch  231 / 261  Training Loss  0.000945215520914644\n",
            "Epoch  5 Batch  232 / 261  Training Loss  0.0010119173675775528\n",
            "Epoch  5 Batch  233 / 261  Training Loss  0.0009402100695297122\n",
            "Epoch  5 Batch  234 / 261  Training Loss  0.00465815607458353\n",
            "Epoch  5 Batch  235 / 261  Training Loss  0.0055357604287564754\n",
            "Epoch  5 Batch  236 / 261  Training Loss  0.0016623309347778559\n",
            "Epoch  5 Batch  237 / 261  Training Loss  0.0011273743584752083\n",
            "Epoch  5 Batch  238 / 261  Training Loss  0.0017932695336639881\n",
            "Epoch  5 Batch  239 / 261  Training Loss  0.001022609299980104\n",
            "Epoch  5 Batch  240 / 261  Training Loss  0.003228600602596998\n",
            "Epoch  5 Batch  241 / 261  Training Loss  0.002426081569865346\n",
            "Epoch  5 Batch  242 / 261  Training Loss  0.0011476243380457163\n",
            "Epoch  5 Batch  243 / 261  Training Loss  0.002026826608926058\n",
            "Epoch  5 Batch  244 / 261  Training Loss  0.007046420592814684\n",
            "Epoch  5 Batch  245 / 261  Training Loss  0.009899470023810863\n",
            "Epoch  5 Batch  246 / 261  Training Loss  0.0115785738453269\n",
            "Epoch  5 Batch  247 / 261  Training Loss  0.0033415767829865217\n",
            "Epoch  5 Batch  248 / 261  Training Loss  0.001953685889020562\n",
            "Epoch  5 Batch  249 / 261  Training Loss  0.002058879006654024\n",
            "Epoch  5 Batch  250 / 261  Training Loss  0.004974608775228262\n",
            "Epoch  5 Batch  251 / 261  Training Loss  0.0011079312535002828\n",
            "Epoch  5 Batch  252 / 261  Training Loss  0.0010002924827858806\n",
            "Epoch  5 Batch  253 / 261  Training Loss  0.005966627039015293\n",
            "Epoch  5 Batch  254 / 261  Training Loss  0.013218733482062817\n",
            "Epoch  5 Batch  255 / 261  Training Loss  0.009268112480640411\n",
            "Epoch  5 Batch  256 / 261  Training Loss  0.001657058484852314\n",
            "Epoch  5 Batch  257 / 261  Training Loss  0.002486977493390441\n",
            "Epoch  5 Batch  258 / 261  Training Loss  0.007421331480145454\n",
            "Epoch  5 Batch  259 / 261  Training Loss  0.004012647550553083\n",
            "Epoch  5 Batch  260 / 261  Training Loss  0.009443303570151329\n",
            "   6    |    -    |   0.006991   | 91.722074\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 6\n",
            "Epoch  6 Batch  0 / 261  Training Loss  0.010502572171390057\n",
            "Epoch  6 Batch  1 / 261  Training Loss  0.008259921334683895\n",
            "Epoch  6 Batch  2 / 261  Training Loss  0.0018309886800125241\n",
            "Epoch  6 Batch  3 / 261  Training Loss  0.001216706819832325\n",
            "Epoch  6 Batch  4 / 261  Training Loss  0.0008498792885802686\n",
            "Epoch  6 Batch  5 / 261  Training Loss  0.0014087584568187594\n",
            "Epoch  6 Batch  6 / 261  Training Loss  0.007490796502679586\n",
            "Epoch  6 Batch  7 / 261  Training Loss  0.0015817169332876801\n",
            "Epoch  6 Batch  8 / 261  Training Loss  0.011693988926708698\n",
            "Epoch  6 Batch  9 / 261  Training Loss  0.014137204736471176\n",
            "Epoch  6 Batch  10 / 261  Training Loss  0.00526816863566637\n",
            "Epoch  6 Batch  11 / 261  Training Loss  0.00588279590010643\n",
            "Epoch  6 Batch  12 / 261  Training Loss  0.01074012741446495\n",
            "Epoch  6 Batch  13 / 261  Training Loss  0.003336680820211768\n",
            "Epoch  6 Batch  14 / 261  Training Loss  0.007891356013715267\n",
            "Epoch  6 Batch  15 / 261  Training Loss  0.005288660526275635\n",
            "Epoch  6 Batch  16 / 261  Training Loss  0.00155741348862648\n",
            "Epoch  6 Batch  17 / 261  Training Loss  0.0040484145283699036\n",
            "Epoch  6 Batch  18 / 261  Training Loss  0.008937213569879532\n",
            "Epoch  6 Batch  19 / 261  Training Loss  0.002704136073589325\n",
            "Epoch  6 Batch  20 / 261  Training Loss  0.002882848959416151\n",
            "Epoch  6 Batch  21 / 261  Training Loss  0.004382077604532242\n",
            "Epoch  6 Batch  22 / 261  Training Loss  0.010041087865829468\n",
            "Epoch  6 Batch  23 / 261  Training Loss  0.0020343707874417305\n",
            "Epoch  6 Batch  24 / 261  Training Loss  0.005959987174719572\n",
            "Epoch  6 Batch  25 / 261  Training Loss  0.0030885213054716587\n",
            "Epoch  6 Batch  26 / 261  Training Loss  0.0019763268064707518\n",
            "Epoch  6 Batch  27 / 261  Training Loss  0.0017372035654261708\n",
            "Epoch  6 Batch  28 / 261  Training Loss  0.0015110170934349298\n",
            "Epoch  6 Batch  29 / 261  Training Loss  0.0018708284478634596\n",
            "Epoch  6 Batch  30 / 261  Training Loss  0.001264420454390347\n",
            "Epoch  6 Batch  31 / 261  Training Loss  0.002558663487434387\n",
            "Epoch  6 Batch  32 / 261  Training Loss  0.003598901443183422\n",
            "Epoch  6 Batch  33 / 261  Training Loss  0.0020152986980974674\n",
            "Epoch  6 Batch  34 / 261  Training Loss  0.0008806681144051254\n",
            "Epoch  6 Batch  35 / 261  Training Loss  0.0014410725561901927\n",
            "Epoch  6 Batch  36 / 261  Training Loss  0.0008823176613077521\n",
            "Epoch  6 Batch  37 / 261  Training Loss  0.0011466022115200758\n",
            "Epoch  6 Batch  38 / 261  Training Loss  0.0023098899982869625\n",
            "Epoch  6 Batch  39 / 261  Training Loss  0.0011828116839751601\n",
            "Epoch  6 Batch  40 / 261  Training Loss  0.002605483867228031\n",
            "Epoch  6 Batch  41 / 261  Training Loss  0.0018265588441863656\n",
            "Epoch  6 Batch  42 / 261  Training Loss  0.005322454497218132\n",
            "Epoch  6 Batch  43 / 261  Training Loss  0.0010894974693655968\n",
            "Epoch  6 Batch  44 / 261  Training Loss  0.02050420269370079\n",
            "Epoch  6 Batch  45 / 261  Training Loss  0.010102692991495132\n",
            "Epoch  6 Batch  46 / 261  Training Loss  0.002159379655495286\n",
            "Epoch  6 Batch  47 / 261  Training Loss  0.008104654029011726\n",
            "Epoch  6 Batch  48 / 261  Training Loss  0.0031672788318246603\n",
            "Epoch  6 Batch  49 / 261  Training Loss  0.004037895705550909\n",
            "Epoch  6 Batch  50 / 261  Training Loss  0.010308976285159588\n",
            "Epoch  6 Batch  51 / 261  Training Loss  0.0022858588490635157\n",
            "Epoch  6 Batch  52 / 261  Training Loss  0.001182609936222434\n",
            "Epoch  6 Batch  53 / 261  Training Loss  0.0009016031981445849\n",
            "Epoch  6 Batch  54 / 261  Training Loss  0.0035907316487282515\n",
            "Epoch  6 Batch  55 / 261  Training Loss  0.0033300938084721565\n",
            "Epoch  6 Batch  56 / 261  Training Loss  0.001833467511460185\n",
            "Epoch  6 Batch  57 / 261  Training Loss  0.001505872467532754\n",
            "Epoch  6 Batch  58 / 261  Training Loss  0.0018838249379768968\n",
            "Epoch  6 Batch  59 / 261  Training Loss  0.0021825344301760197\n",
            "Epoch  6 Batch  60 / 261  Training Loss  0.0013071632711216807\n",
            "Epoch  6 Batch  61 / 261  Training Loss  0.0043495167046785355\n",
            "Epoch  6 Batch  62 / 261  Training Loss  0.0021410135086625814\n",
            "Epoch  6 Batch  63 / 261  Training Loss  0.0027603295166045427\n",
            "Epoch  6 Batch  64 / 261  Training Loss  0.0019914417061954737\n",
            "Epoch  6 Batch  65 / 261  Training Loss  0.0026931532192975283\n",
            "Epoch  6 Batch  66 / 261  Training Loss  0.0025757048279047012\n",
            "Epoch  6 Batch  67 / 261  Training Loss  0.008443212136626244\n",
            "Epoch  6 Batch  68 / 261  Training Loss  0.0010845515644177794\n",
            "Epoch  6 Batch  69 / 261  Training Loss  0.0019269383046776056\n",
            "Epoch  6 Batch  70 / 261  Training Loss  0.002333478070795536\n",
            "Epoch  6 Batch  71 / 261  Training Loss  0.003770919516682625\n",
            "Epoch  6 Batch  72 / 261  Training Loss  0.0037497251760214567\n",
            "Epoch  6 Batch  73 / 261  Training Loss  0.001007855636999011\n",
            "Epoch  6 Batch  74 / 261  Training Loss  0.003492149990051985\n",
            "Epoch  6 Batch  75 / 261  Training Loss  0.006006820593029261\n",
            "Epoch  6 Batch  76 / 261  Training Loss  0.008070689626038074\n",
            "Epoch  6 Batch  77 / 261  Training Loss  0.000969615182839334\n",
            "Epoch  6 Batch  78 / 261  Training Loss  0.0012107875663787127\n",
            "Epoch  6 Batch  79 / 261  Training Loss  0.001510191592387855\n",
            "Epoch  6 Batch  80 / 261  Training Loss  0.0012325231218710542\n",
            "Epoch  6 Batch  81 / 261  Training Loss  0.0015724460827186704\n",
            "Epoch  6 Batch  82 / 261  Training Loss  0.0012106753420084715\n",
            "Epoch  6 Batch  83 / 261  Training Loss  0.0008136352989822626\n",
            "Epoch  6 Batch  84 / 261  Training Loss  0.000703920959495008\n",
            "Epoch  6 Batch  85 / 261  Training Loss  0.0009067285573109984\n",
            "Epoch  6 Batch  86 / 261  Training Loss  0.00292290560901165\n",
            "Epoch  6 Batch  87 / 261  Training Loss  0.010808728635311127\n",
            "Epoch  6 Batch  88 / 261  Training Loss  0.0030484271701425314\n",
            "Epoch  6 Batch  89 / 261  Training Loss  0.003973757848143578\n",
            "Epoch  6 Batch  90 / 261  Training Loss  0.014306212775409222\n",
            "Epoch  6 Batch  91 / 261  Training Loss  0.004056088160723448\n",
            "Epoch  6 Batch  92 / 261  Training Loss  0.0058108787052333355\n",
            "Epoch  6 Batch  93 / 261  Training Loss  0.0028125671669840813\n",
            "Epoch  6 Batch  94 / 261  Training Loss  0.011185028590261936\n",
            "Epoch  6 Batch  95 / 261  Training Loss  0.006351554300636053\n",
            "Epoch  6 Batch  96 / 261  Training Loss  0.0022592474706470966\n",
            "Epoch  6 Batch  97 / 261  Training Loss  0.007301079574972391\n",
            "Epoch  6 Batch  98 / 261  Training Loss  0.0017266581999137998\n",
            "Epoch  6 Batch  99 / 261  Training Loss  0.0024440980050712824\n",
            "Epoch  6 Batch  100 / 261  Training Loss  0.0017026221612468362\n",
            "Epoch  6 Batch  101 / 261  Training Loss  0.0010662372224032879\n",
            "Epoch  6 Batch  102 / 261  Training Loss  0.0015898612327873707\n",
            "Epoch  6 Batch  103 / 261  Training Loss  0.0008497275412082672\n",
            "Epoch  6 Batch  104 / 261  Training Loss  0.0035476982593536377\n",
            "Epoch  6 Batch  105 / 261  Training Loss  0.006831860635429621\n",
            "Epoch  6 Batch  106 / 261  Training Loss  0.011455714702606201\n",
            "Epoch  6 Batch  107 / 261  Training Loss  0.00509268743917346\n",
            "Epoch  6 Batch  108 / 261  Training Loss  0.003317096270620823\n",
            "Epoch  6 Batch  109 / 261  Training Loss  0.005417953711003065\n",
            "Epoch  6 Batch  110 / 261  Training Loss  0.03756185621023178\n",
            "Epoch  6 Batch  111 / 261  Training Loss  0.0226987786591053\n",
            "Epoch  6 Batch  112 / 261  Training Loss  0.018483499065041542\n",
            "Epoch  6 Batch  113 / 261  Training Loss  0.006901110522449017\n",
            "Epoch  6 Batch  114 / 261  Training Loss  0.0016089160926640034\n",
            "Epoch  6 Batch  115 / 261  Training Loss  0.005037963390350342\n",
            "Epoch  6 Batch  116 / 261  Training Loss  0.0033191002439707518\n",
            "Epoch  6 Batch  117 / 261  Training Loss  0.016779493540525436\n",
            "Epoch  6 Batch  118 / 261  Training Loss  0.013356635347008705\n",
            "Epoch  6 Batch  119 / 261  Training Loss  0.007184131536632776\n",
            "Epoch  6 Batch  120 / 261  Training Loss  0.0028051985427737236\n",
            "Epoch  6 Batch  121 / 261  Training Loss  0.006495273672044277\n",
            "Epoch  6 Batch  122 / 261  Training Loss  0.00274071772582829\n",
            "Epoch  6 Batch  123 / 261  Training Loss  0.0009683480020612478\n",
            "Epoch  6 Batch  124 / 261  Training Loss  0.001057369983755052\n",
            "Epoch  6 Batch  125 / 261  Training Loss  0.005781078711152077\n",
            "Epoch  6 Batch  126 / 261  Training Loss  0.0030176863074302673\n",
            "Epoch  6 Batch  127 / 261  Training Loss  0.0023628221824765205\n",
            "Epoch  6 Batch  128 / 261  Training Loss  0.0013529089046642184\n",
            "Epoch  6 Batch  129 / 261  Training Loss  0.0018200438935309649\n",
            "Epoch  6 Batch  130 / 261  Training Loss  0.0020263041369616985\n",
            "Epoch  6 Batch  131 / 261  Training Loss  0.0014166668988764286\n",
            "Epoch  6 Batch  132 / 261  Training Loss  0.0010588953737169504\n",
            "Epoch  6 Batch  133 / 261  Training Loss  0.0011089500039815903\n",
            "Epoch  6 Batch  134 / 261  Training Loss  0.0015584912616759539\n",
            "Epoch  6 Batch  135 / 261  Training Loss  0.002028197981417179\n",
            "Epoch  6 Batch  136 / 261  Training Loss  0.0014313921565189958\n",
            "Epoch  6 Batch  137 / 261  Training Loss  0.0008221925818361342\n",
            "Epoch  6 Batch  138 / 261  Training Loss  0.0013067200779914856\n",
            "Epoch  6 Batch  139 / 261  Training Loss  0.00106734037399292\n",
            "Epoch  6 Batch  140 / 261  Training Loss  0.0010000849142670631\n",
            "Epoch  6 Batch  141 / 261  Training Loss  0.0009526016074232757\n",
            "Epoch  6 Batch  142 / 261  Training Loss  0.000978793133981526\n",
            "Epoch  6 Batch  143 / 261  Training Loss  0.0030714913737028837\n",
            "Epoch  6 Batch  144 / 261  Training Loss  0.0052216327749192715\n",
            "Epoch  6 Batch  145 / 261  Training Loss  0.0024247393012046814\n",
            "Epoch  6 Batch  146 / 261  Training Loss  0.001464280067011714\n",
            "Epoch  6 Batch  147 / 261  Training Loss  0.0019350104266777635\n",
            "Epoch  6 Batch  148 / 261  Training Loss  0.0013208967866376042\n",
            "Epoch  6 Batch  149 / 261  Training Loss  0.00247934483923018\n",
            "Epoch  6 Batch  150 / 261  Training Loss  0.0016007982194423676\n",
            "Epoch  6 Batch  151 / 261  Training Loss  0.0012328604934737086\n",
            "Epoch  6 Batch  152 / 261  Training Loss  0.0010284719755873084\n",
            "Epoch  6 Batch  153 / 261  Training Loss  0.0023196495603770018\n",
            "Epoch  6 Batch  154 / 261  Training Loss  0.0013696185778826475\n",
            "Epoch  6 Batch  155 / 261  Training Loss  0.0006385698216035962\n",
            "Epoch  6 Batch  156 / 261  Training Loss  0.0011106823803856969\n",
            "Epoch  6 Batch  157 / 261  Training Loss  0.0019642338156700134\n",
            "Epoch  6 Batch  158 / 261  Training Loss  0.0014893459156155586\n",
            "Epoch  6 Batch  159 / 261  Training Loss  0.001040950301103294\n",
            "Epoch  6 Batch  160 / 261  Training Loss  0.0018710674485191703\n",
            "Epoch  6 Batch  161 / 261  Training Loss  0.0008907075389288366\n",
            "Epoch  6 Batch  162 / 261  Training Loss  0.0008692627889104187\n",
            "Epoch  6 Batch  163 / 261  Training Loss  0.0007271306239999831\n",
            "Epoch  6 Batch  164 / 261  Training Loss  0.0009409833000972867\n",
            "Epoch  6 Batch  165 / 261  Training Loss  0.000801355519797653\n",
            "Epoch  6 Batch  166 / 261  Training Loss  0.0008175071561709046\n",
            "Epoch  6 Batch  167 / 261  Training Loss  0.000744994671549648\n",
            "Epoch  6 Batch  168 / 261  Training Loss  0.0008100054692476988\n",
            "Epoch  6 Batch  169 / 261  Training Loss  0.0009654011810198426\n",
            "Epoch  6 Batch  170 / 261  Training Loss  0.0012145573273301125\n",
            "Epoch  6 Batch  171 / 261  Training Loss  0.001174723613075912\n",
            "Epoch  6 Batch  172 / 261  Training Loss  0.001307081081904471\n",
            "Epoch  6 Batch  173 / 261  Training Loss  0.001240496407262981\n",
            "Epoch  6 Batch  174 / 261  Training Loss  0.0009126396616920829\n",
            "Epoch  6 Batch  175 / 261  Training Loss  0.0009164263028651476\n",
            "Epoch  6 Batch  176 / 261  Training Loss  0.0008494791109114885\n",
            "Epoch  6 Batch  177 / 261  Training Loss  0.0012005046010017395\n",
            "Epoch  6 Batch  178 / 261  Training Loss  0.0013774547260254622\n",
            "Epoch  6 Batch  179 / 261  Training Loss  0.000996604678221047\n",
            "Epoch  6 Batch  180 / 261  Training Loss  0.0007648846367374063\n",
            "Epoch  6 Batch  181 / 261  Training Loss  0.0013929243432357907\n",
            "Epoch  6 Batch  182 / 261  Training Loss  0.0008262034971266985\n",
            "Epoch  6 Batch  183 / 261  Training Loss  0.0007656975067220628\n",
            "Epoch  6 Batch  184 / 261  Training Loss  0.0013556963531300426\n",
            "Epoch  6 Batch  185 / 261  Training Loss  0.0005876516806893051\n",
            "Epoch  6 Batch  186 / 261  Training Loss  0.002852369798347354\n",
            "Epoch  6 Batch  187 / 261  Training Loss  0.0014292234554886818\n",
            "Epoch  6 Batch  188 / 261  Training Loss  0.001241091056726873\n",
            "Epoch  6 Batch  189 / 261  Training Loss  0.0006481959135271609\n",
            "Epoch  6 Batch  190 / 261  Training Loss  0.0014925997238606215\n",
            "Epoch  6 Batch  191 / 261  Training Loss  0.001364719239063561\n",
            "Epoch  6 Batch  192 / 261  Training Loss  0.0008691309485584497\n",
            "Epoch  6 Batch  193 / 261  Training Loss  0.0006915958365425467\n",
            "Epoch  6 Batch  194 / 261  Training Loss  0.0005595083930529654\n",
            "Epoch  6 Batch  195 / 261  Training Loss  0.0005410931771621108\n",
            "Epoch  6 Batch  196 / 261  Training Loss  0.000679458724334836\n",
            "Epoch  6 Batch  197 / 261  Training Loss  0.00099445809610188\n",
            "Epoch  6 Batch  198 / 261  Training Loss  0.0008664935012347996\n",
            "Epoch  6 Batch  199 / 261  Training Loss  0.0011972890933975577\n",
            "Epoch  6 Batch  200 / 261  Training Loss  0.0007211145129986107\n",
            "Epoch  6 Batch  201 / 261  Training Loss  0.0006385017186403275\n",
            "Epoch  6 Batch  202 / 261  Training Loss  0.0010280561400577426\n",
            "Epoch  6 Batch  203 / 261  Training Loss  0.0030308885034173727\n",
            "Epoch  6 Batch  204 / 261  Training Loss  0.0017850588774308562\n",
            "Epoch  6 Batch  205 / 261  Training Loss  0.0013453422579914331\n",
            "Epoch  6 Batch  206 / 261  Training Loss  0.0006068351794965565\n",
            "Epoch  6 Batch  207 / 261  Training Loss  0.001207961468026042\n",
            "Epoch  6 Batch  208 / 261  Training Loss  0.0006623464869335294\n",
            "Epoch  6 Batch  209 / 261  Training Loss  0.0007886916282586753\n",
            "Epoch  6 Batch  210 / 261  Training Loss  0.0016044946387410164\n",
            "Epoch  6 Batch  211 / 261  Training Loss  0.0007318509160540998\n",
            "Epoch  6 Batch  212 / 261  Training Loss  0.001830407534725964\n",
            "Epoch  6 Batch  213 / 261  Training Loss  0.001327310805208981\n",
            "Epoch  6 Batch  214 / 261  Training Loss  0.0006238195928744972\n",
            "Epoch  6 Batch  215 / 261  Training Loss  0.0007355574634857476\n",
            "Epoch  6 Batch  216 / 261  Training Loss  0.0016964158276095986\n",
            "Epoch  6 Batch  217 / 261  Training Loss  0.0006471025408245623\n",
            "Epoch  6 Batch  218 / 261  Training Loss  0.0005526833119802177\n",
            "Epoch  6 Batch  219 / 261  Training Loss  0.0006359732360579073\n",
            "Epoch  6 Batch  220 / 261  Training Loss  0.0009102122276090086\n",
            "Epoch  6 Batch  221 / 261  Training Loss  0.0005433899932540953\n",
            "Epoch  6 Batch  222 / 261  Training Loss  0.0005875531933270395\n",
            "Epoch  6 Batch  223 / 261  Training Loss  0.006853739731013775\n",
            "Epoch  6 Batch  224 / 261  Training Loss  0.00044168264139443636\n",
            "Epoch  6 Batch  225 / 261  Training Loss  0.0010612185578793287\n",
            "Epoch  6 Batch  226 / 261  Training Loss  0.0005001106765121222\n",
            "Epoch  6 Batch  227 / 261  Training Loss  0.0009493491961620748\n",
            "Epoch  6 Batch  228 / 261  Training Loss  0.0005476863589137793\n",
            "Epoch  6 Batch  229 / 261  Training Loss  0.0006855538231320679\n",
            "Epoch  6 Batch  230 / 261  Training Loss  0.0005351884756237268\n",
            "Epoch  6 Batch  231 / 261  Training Loss  0.0010752970119938254\n",
            "Epoch  6 Batch  232 / 261  Training Loss  0.0006728586740791798\n",
            "Epoch  6 Batch  233 / 261  Training Loss  0.005933600477874279\n",
            "Epoch  6 Batch  234 / 261  Training Loss  0.0010307185584679246\n",
            "Epoch  6 Batch  235 / 261  Training Loss  0.001121478620916605\n",
            "Epoch  6 Batch  236 / 261  Training Loss  0.0006903363973833621\n",
            "Epoch  6 Batch  237 / 261  Training Loss  0.0009180026245303452\n",
            "Epoch  6 Batch  238 / 261  Training Loss  0.000778372457716614\n",
            "Epoch  6 Batch  239 / 261  Training Loss  0.005880217533558607\n",
            "Epoch  6 Batch  240 / 261  Training Loss  0.025808362290263176\n",
            "Epoch  6 Batch  241 / 261  Training Loss  0.01075943373143673\n",
            "Epoch  6 Batch  242 / 261  Training Loss  0.004691220819950104\n",
            "Epoch  6 Batch  243 / 261  Training Loss  0.002187343779951334\n",
            "Epoch  6 Batch  244 / 261  Training Loss  0.0025526664685457945\n",
            "Epoch  6 Batch  245 / 261  Training Loss  0.0010732546215876937\n",
            "Epoch  6 Batch  246 / 261  Training Loss  0.003005770966410637\n",
            "Epoch  6 Batch  247 / 261  Training Loss  0.002204855438321829\n",
            "Epoch  6 Batch  248 / 261  Training Loss  0.0016810232773423195\n",
            "Epoch  6 Batch  249 / 261  Training Loss  0.0007780392188578844\n",
            "Epoch  6 Batch  250 / 261  Training Loss  0.0022036905866116285\n",
            "Epoch  6 Batch  251 / 261  Training Loss  0.0030452096834778786\n",
            "Epoch  6 Batch  252 / 261  Training Loss  0.0043967957608401775\n",
            "Epoch  6 Batch  253 / 261  Training Loss  0.0032099285162985325\n",
            "Epoch  6 Batch  254 / 261  Training Loss  0.004645249806344509\n",
            "Epoch  6 Batch  255 / 261  Training Loss  0.013906218111515045\n",
            "Epoch  6 Batch  256 / 261  Training Loss  0.018718402832746506\n",
            "Epoch  6 Batch  257 / 261  Training Loss  0.007134384009987116\n",
            "Epoch  6 Batch  258 / 261  Training Loss  0.010043413378298283\n",
            "Epoch  6 Batch  259 / 261  Training Loss  0.008825748227536678\n",
            "Epoch  6 Batch  260 / 261  Training Loss  0.005137101747095585\n",
            "   7    |    -    |   0.003389   | 93.317819\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 7\n",
            "Epoch  7 Batch  0 / 261  Training Loss  0.002780492650344968\n",
            "Epoch  7 Batch  1 / 261  Training Loss  0.00708864675834775\n",
            "Epoch  7 Batch  2 / 261  Training Loss  0.0016808472573757172\n",
            "Epoch  7 Batch  3 / 261  Training Loss  0.002225408097729087\n",
            "Epoch  7 Batch  4 / 261  Training Loss  0.004520407412201166\n",
            "Epoch  7 Batch  5 / 261  Training Loss  0.0028264925349503756\n",
            "Epoch  7 Batch  6 / 261  Training Loss  0.006216306239366531\n",
            "Epoch  7 Batch  7 / 261  Training Loss  0.002236610744148493\n",
            "Epoch  7 Batch  8 / 261  Training Loss  0.001964641036465764\n",
            "Epoch  7 Batch  9 / 261  Training Loss  0.0011794206220656633\n",
            "Epoch  7 Batch  10 / 261  Training Loss  0.008211623877286911\n",
            "Epoch  7 Batch  11 / 261  Training Loss  0.0011076499940827489\n",
            "Epoch  7 Batch  12 / 261  Training Loss  0.001094035105779767\n",
            "Epoch  7 Batch  13 / 261  Training Loss  0.0011482792906463146\n",
            "Epoch  7 Batch  14 / 261  Training Loss  0.0020649477373808622\n",
            "Epoch  7 Batch  15 / 261  Training Loss  0.0012503255857154727\n",
            "Epoch  7 Batch  16 / 261  Training Loss  0.0021626960951834917\n",
            "Epoch  7 Batch  17 / 261  Training Loss  0.004948677960783243\n",
            "Epoch  7 Batch  18 / 261  Training Loss  0.0013306173495948315\n",
            "Epoch  7 Batch  19 / 261  Training Loss  0.0006795491790398955\n",
            "Epoch  7 Batch  20 / 261  Training Loss  0.0014136940008029342\n",
            "Epoch  7 Batch  21 / 261  Training Loss  0.000713198387529701\n",
            "Epoch  7 Batch  22 / 261  Training Loss  0.0008108415640890598\n",
            "Epoch  7 Batch  23 / 261  Training Loss  0.0007029189146123827\n",
            "Epoch  7 Batch  24 / 261  Training Loss  0.0006591089768335223\n",
            "Epoch  7 Batch  25 / 261  Training Loss  0.0020077743101865053\n",
            "Epoch  7 Batch  26 / 261  Training Loss  0.0013398953014984727\n",
            "Epoch  7 Batch  27 / 261  Training Loss  0.001023347256705165\n",
            "Epoch  7 Batch  28 / 261  Training Loss  0.0008238158188760281\n",
            "Epoch  7 Batch  29 / 261  Training Loss  0.0009744245326146483\n",
            "Epoch  7 Batch  30 / 261  Training Loss  0.000519610766787082\n",
            "Epoch  7 Batch  31 / 261  Training Loss  0.0005937356618233025\n",
            "Epoch  7 Batch  32 / 261  Training Loss  0.0008128634071908891\n",
            "Epoch  7 Batch  33 / 261  Training Loss  0.002174842869862914\n",
            "Epoch  7 Batch  34 / 261  Training Loss  0.0011347196996212006\n",
            "Epoch  7 Batch  35 / 261  Training Loss  0.0024817483499646187\n",
            "Epoch  7 Batch  36 / 261  Training Loss  0.0005742844077758491\n",
            "Epoch  7 Batch  37 / 261  Training Loss  0.0009389304323121905\n",
            "Epoch  7 Batch  38 / 261  Training Loss  0.0007821088074706495\n",
            "Epoch  7 Batch  39 / 261  Training Loss  0.0011229574447497725\n",
            "Epoch  7 Batch  40 / 261  Training Loss  0.000443988450570032\n",
            "Epoch  7 Batch  41 / 261  Training Loss  0.0024515523109585047\n",
            "Epoch  7 Batch  42 / 261  Training Loss  0.0005361538496799767\n",
            "Epoch  7 Batch  43 / 261  Training Loss  0.0012452449882403016\n",
            "Epoch  7 Batch  44 / 261  Training Loss  0.000890080991666764\n",
            "Epoch  7 Batch  45 / 261  Training Loss  0.0007142953108996153\n",
            "Epoch  7 Batch  46 / 261  Training Loss  0.00826979335397482\n",
            "Epoch  7 Batch  47 / 261  Training Loss  0.0031750949565321207\n",
            "Epoch  7 Batch  48 / 261  Training Loss  0.0012794982176274061\n",
            "Epoch  7 Batch  49 / 261  Training Loss  0.000931602728087455\n",
            "Epoch  7 Batch  50 / 261  Training Loss  0.003701959503814578\n",
            "Epoch  7 Batch  51 / 261  Training Loss  0.0013694310327991843\n",
            "Epoch  7 Batch  52 / 261  Training Loss  0.00043957974412478507\n",
            "Epoch  7 Batch  53 / 261  Training Loss  0.0010988309513777494\n",
            "Epoch  7 Batch  54 / 261  Training Loss  0.000853712554089725\n",
            "Epoch  7 Batch  55 / 261  Training Loss  0.0003461828746367246\n",
            "Epoch  7 Batch  56 / 261  Training Loss  0.0006482335738837719\n",
            "Epoch  7 Batch  57 / 261  Training Loss  0.0005772433942183852\n",
            "Epoch  7 Batch  58 / 261  Training Loss  0.0026313704438507557\n",
            "Epoch  7 Batch  59 / 261  Training Loss  0.001882161246612668\n",
            "Epoch  7 Batch  60 / 261  Training Loss  0.0005805580294691026\n",
            "Epoch  7 Batch  61 / 261  Training Loss  0.0010435672011226416\n",
            "Epoch  7 Batch  62 / 261  Training Loss  0.0010156368371099234\n",
            "Epoch  7 Batch  63 / 261  Training Loss  0.0028200726956129074\n",
            "Epoch  7 Batch  64 / 261  Training Loss  0.0018920934526249766\n",
            "Epoch  7 Batch  65 / 261  Training Loss  0.0005691326805390418\n",
            "Epoch  7 Batch  66 / 261  Training Loss  0.00303073157556355\n",
            "Epoch  7 Batch  67 / 261  Training Loss  0.000767958932556212\n",
            "Epoch  7 Batch  68 / 261  Training Loss  0.0013027815148234367\n",
            "Epoch  7 Batch  69 / 261  Training Loss  0.0008447342552244663\n",
            "Epoch  7 Batch  70 / 261  Training Loss  0.0006199986673891544\n",
            "Epoch  7 Batch  71 / 261  Training Loss  0.0007640753174200654\n",
            "Epoch  7 Batch  72 / 261  Training Loss  0.0007754992111586034\n",
            "Epoch  7 Batch  73 / 261  Training Loss  0.0007276543765328825\n",
            "Epoch  7 Batch  74 / 261  Training Loss  0.0006702888058498502\n",
            "Epoch  7 Batch  75 / 261  Training Loss  0.0006272275932133198\n",
            "Epoch  7 Batch  76 / 261  Training Loss  0.00048394204350188375\n",
            "Epoch  7 Batch  77 / 261  Training Loss  0.0006248998106457293\n",
            "Epoch  7 Batch  78 / 261  Training Loss  0.0005506767192855477\n",
            "Epoch  7 Batch  79 / 261  Training Loss  0.00040039673331193626\n",
            "Epoch  7 Batch  80 / 261  Training Loss  0.001811741036362946\n",
            "Epoch  7 Batch  81 / 261  Training Loss  0.0030327674467116594\n",
            "Epoch  7 Batch  82 / 261  Training Loss  0.0009200677159242332\n",
            "Epoch  7 Batch  83 / 261  Training Loss  0.0005840386147610843\n",
            "Epoch  7 Batch  84 / 261  Training Loss  0.0003630301798693836\n",
            "Epoch  7 Batch  85 / 261  Training Loss  0.000998132862150669\n",
            "Epoch  7 Batch  86 / 261  Training Loss  0.0025222294498234987\n",
            "Epoch  7 Batch  87 / 261  Training Loss  0.0016075825551524758\n",
            "Epoch  7 Batch  88 / 261  Training Loss  0.00085231609409675\n",
            "Epoch  7 Batch  89 / 261  Training Loss  0.0005882904515601695\n",
            "Epoch  7 Batch  90 / 261  Training Loss  0.0011003906838595867\n",
            "Epoch  7 Batch  91 / 261  Training Loss  0.000682053214404732\n",
            "Epoch  7 Batch  92 / 261  Training Loss  0.0007829508977010846\n",
            "Epoch  7 Batch  93 / 261  Training Loss  0.0007004617946222425\n",
            "Epoch  7 Batch  94 / 261  Training Loss  0.0006256284541450441\n",
            "Epoch  7 Batch  95 / 261  Training Loss  0.00043821055442094803\n",
            "Epoch  7 Batch  96 / 261  Training Loss  0.0004985129926353693\n",
            "Epoch  7 Batch  97 / 261  Training Loss  0.0007616651128046215\n",
            "Epoch  7 Batch  98 / 261  Training Loss  0.0007332144887186587\n",
            "Epoch  7 Batch  99 / 261  Training Loss  0.0005429789307527244\n",
            "Epoch  7 Batch  100 / 261  Training Loss  0.00036607353831641376\n",
            "Epoch  7 Batch  101 / 261  Training Loss  0.00044024240924045444\n",
            "Epoch  7 Batch  102 / 261  Training Loss  0.0005027669831179082\n",
            "Epoch  7 Batch  103 / 261  Training Loss  0.0035370734985917807\n",
            "Epoch  7 Batch  104 / 261  Training Loss  0.0006417070399038494\n",
            "Epoch  7 Batch  105 / 261  Training Loss  0.00046390332863666117\n",
            "Epoch  7 Batch  106 / 261  Training Loss  0.0004906660178676248\n",
            "Epoch  7 Batch  107 / 261  Training Loss  0.0005611737724393606\n",
            "Epoch  7 Batch  108 / 261  Training Loss  0.0006036682170815766\n",
            "Epoch  7 Batch  109 / 261  Training Loss  0.0010527302511036396\n",
            "Epoch  7 Batch  110 / 261  Training Loss  0.0005642281612381339\n",
            "Epoch  7 Batch  111 / 261  Training Loss  0.0007978762732818723\n",
            "Epoch  7 Batch  112 / 261  Training Loss  0.000595356454141438\n",
            "Epoch  7 Batch  113 / 261  Training Loss  0.00025471876142546535\n",
            "Epoch  7 Batch  114 / 261  Training Loss  0.0006119867321103811\n",
            "Epoch  7 Batch  115 / 261  Training Loss  0.00031222664983943105\n",
            "Epoch  7 Batch  116 / 261  Training Loss  0.0005098404944874346\n",
            "Epoch  7 Batch  117 / 261  Training Loss  0.0007988905417732894\n",
            "Epoch  7 Batch  118 / 261  Training Loss  0.0007519060163758695\n",
            "Epoch  7 Batch  119 / 261  Training Loss  0.0006279631634242833\n",
            "Epoch  7 Batch  120 / 261  Training Loss  0.0006081209285184741\n",
            "Epoch  7 Batch  121 / 261  Training Loss  0.0014404228422790766\n",
            "Epoch  7 Batch  122 / 261  Training Loss  0.0009093705448321998\n",
            "Epoch  7 Batch  123 / 261  Training Loss  0.0004367284127511084\n",
            "Epoch  7 Batch  124 / 261  Training Loss  0.000400249264203012\n",
            "Epoch  7 Batch  125 / 261  Training Loss  0.002355690812692046\n",
            "Epoch  7 Batch  126 / 261  Training Loss  0.00040725109283812344\n",
            "Epoch  7 Batch  127 / 261  Training Loss  0.0014252950204536319\n",
            "Epoch  7 Batch  128 / 261  Training Loss  0.0007355054258368909\n",
            "Epoch  7 Batch  129 / 261  Training Loss  0.00266423262655735\n",
            "Epoch  7 Batch  130 / 261  Training Loss  0.003961303737014532\n",
            "Epoch  7 Batch  131 / 261  Training Loss  0.0004819529131054878\n",
            "Epoch  7 Batch  132 / 261  Training Loss  0.006988027133047581\n",
            "Epoch  7 Batch  133 / 261  Training Loss  0.022922782227396965\n",
            "Epoch  7 Batch  134 / 261  Training Loss  0.0042906953021883965\n",
            "Epoch  7 Batch  135 / 261  Training Loss  0.009846245869994164\n",
            "Epoch  7 Batch  136 / 261  Training Loss  0.007057362236082554\n",
            "Epoch  7 Batch  137 / 261  Training Loss  0.003774386364966631\n",
            "Epoch  7 Batch  138 / 261  Training Loss  0.0064233471639454365\n",
            "Epoch  7 Batch  139 / 261  Training Loss  0.004937662277370691\n",
            "Epoch  7 Batch  140 / 261  Training Loss  0.002092806389555335\n",
            "Epoch  7 Batch  141 / 261  Training Loss  0.006389848422259092\n",
            "Epoch  7 Batch  142 / 261  Training Loss  0.0030744923278689384\n",
            "Epoch  7 Batch  143 / 261  Training Loss  0.006901334039866924\n",
            "Epoch  7 Batch  144 / 261  Training Loss  0.0018931369995698333\n",
            "Epoch  7 Batch  145 / 261  Training Loss  0.008535441942512989\n",
            "Epoch  7 Batch  146 / 261  Training Loss  0.008224998600780964\n",
            "Epoch  7 Batch  147 / 261  Training Loss  0.003831491805613041\n",
            "Epoch  7 Batch  148 / 261  Training Loss  0.0017751328414306045\n",
            "Epoch  7 Batch  149 / 261  Training Loss  0.0032144382130354643\n",
            "Epoch  7 Batch  150 / 261  Training Loss  0.01507836114615202\n",
            "Epoch  7 Batch  151 / 261  Training Loss  0.0014170726062729955\n",
            "Epoch  7 Batch  152 / 261  Training Loss  0.002279833657667041\n",
            "Epoch  7 Batch  153 / 261  Training Loss  0.0013064579106867313\n",
            "Epoch  7 Batch  154 / 261  Training Loss  0.005903355311602354\n",
            "Epoch  7 Batch  155 / 261  Training Loss  0.006003012880682945\n",
            "Epoch  7 Batch  156 / 261  Training Loss  0.009405934251844883\n",
            "Epoch  7 Batch  157 / 261  Training Loss  0.008512905798852444\n",
            "Epoch  7 Batch  158 / 261  Training Loss  0.022774608805775642\n",
            "Epoch  7 Batch  159 / 261  Training Loss  0.02011920139193535\n",
            "Epoch  7 Batch  160 / 261  Training Loss  0.0031920834444463253\n",
            "Epoch  7 Batch  161 / 261  Training Loss  0.004142020829021931\n",
            "Epoch  7 Batch  162 / 261  Training Loss  0.007747814524918795\n",
            "Epoch  7 Batch  163 / 261  Training Loss  0.014735358767211437\n",
            "Epoch  7 Batch  164 / 261  Training Loss  0.00214325706474483\n",
            "Epoch  7 Batch  165 / 261  Training Loss  0.005278636701405048\n",
            "Epoch  7 Batch  166 / 261  Training Loss  0.004569052252918482\n",
            "Epoch  7 Batch  167 / 261  Training Loss  0.0020555173978209496\n",
            "Epoch  7 Batch  168 / 261  Training Loss  0.001136019709520042\n",
            "Epoch  7 Batch  169 / 261  Training Loss  0.0015206432435661554\n",
            "Epoch  7 Batch  170 / 261  Training Loss  0.003927403595298529\n",
            "Epoch  7 Batch  171 / 261  Training Loss  0.0019478811882436275\n",
            "Epoch  7 Batch  172 / 261  Training Loss  0.0027129938825964928\n",
            "Epoch  7 Batch  173 / 261  Training Loss  0.002688864478841424\n",
            "Epoch  7 Batch  174 / 261  Training Loss  0.0011423671385273337\n",
            "Epoch  7 Batch  175 / 261  Training Loss  0.001478981226682663\n",
            "Epoch  7 Batch  176 / 261  Training Loss  0.0025428312364965677\n",
            "Epoch  7 Batch  177 / 261  Training Loss  0.0008073397912085056\n",
            "Epoch  7 Batch  178 / 261  Training Loss  0.0010929066920652986\n",
            "Epoch  7 Batch  179 / 261  Training Loss  0.003676207270473242\n",
            "Epoch  7 Batch  180 / 261  Training Loss  0.002889992669224739\n",
            "Epoch  7 Batch  181 / 261  Training Loss  0.0013744353782385588\n",
            "Epoch  7 Batch  182 / 261  Training Loss  0.0017029175069183111\n",
            "Epoch  7 Batch  183 / 261  Training Loss  0.0010253363288939\n",
            "Epoch  7 Batch  184 / 261  Training Loss  0.0012463803868740797\n",
            "Epoch  7 Batch  185 / 261  Training Loss  0.0008789006387814879\n",
            "Epoch  7 Batch  186 / 261  Training Loss  0.005301984958350658\n",
            "Epoch  7 Batch  187 / 261  Training Loss  0.012852762825787067\n",
            "Epoch  7 Batch  188 / 261  Training Loss  0.002639529062435031\n",
            "Epoch  7 Batch  189 / 261  Training Loss  0.004480911884456873\n",
            "Epoch  7 Batch  190 / 261  Training Loss  0.0005002642283216119\n",
            "Epoch  7 Batch  191 / 261  Training Loss  0.0007200808031484485\n",
            "Epoch  7 Batch  192 / 261  Training Loss  0.0030644231010228395\n",
            "Epoch  7 Batch  193 / 261  Training Loss  0.0010645411675795913\n",
            "Epoch  7 Batch  194 / 261  Training Loss  0.0009574725991114974\n",
            "Epoch  7 Batch  195 / 261  Training Loss  0.002715823706239462\n",
            "Epoch  7 Batch  196 / 261  Training Loss  0.00031427870271727443\n",
            "Epoch  7 Batch  197 / 261  Training Loss  0.0005424317205324769\n",
            "Epoch  7 Batch  198 / 261  Training Loss  0.002875668928027153\n",
            "Epoch  7 Batch  199 / 261  Training Loss  0.0009443472372367978\n",
            "Epoch  7 Batch  200 / 261  Training Loss  0.0014430300798267126\n",
            "Epoch  7 Batch  201 / 261  Training Loss  0.0008014613995328546\n",
            "Epoch  7 Batch  202 / 261  Training Loss  0.0006364606088027358\n",
            "Epoch  7 Batch  203 / 261  Training Loss  0.0009069708175957203\n",
            "Epoch  7 Batch  204 / 261  Training Loss  0.002533550374209881\n",
            "Epoch  7 Batch  205 / 261  Training Loss  0.002002051332965493\n",
            "Epoch  7 Batch  206 / 261  Training Loss  0.004166372120380402\n",
            "Epoch  7 Batch  207 / 261  Training Loss  0.004752625245600939\n",
            "Epoch  7 Batch  208 / 261  Training Loss  0.004993635229766369\n",
            "Epoch  7 Batch  209 / 261  Training Loss  0.0004910466959699988\n",
            "Epoch  7 Batch  210 / 261  Training Loss  0.0025771758519113064\n",
            "Epoch  7 Batch  211 / 261  Training Loss  0.01926802285015583\n",
            "Epoch  7 Batch  212 / 261  Training Loss  0.003458302700892091\n",
            "Epoch  7 Batch  213 / 261  Training Loss  0.0029593734070658684\n",
            "Epoch  7 Batch  214 / 261  Training Loss  0.0040538678877055645\n",
            "Epoch  7 Batch  215 / 261  Training Loss  0.017572179436683655\n",
            "Epoch  7 Batch  216 / 261  Training Loss  0.03795583173632622\n",
            "Epoch  7 Batch  217 / 261  Training Loss  0.041395559906959534\n",
            "Epoch  7 Batch  218 / 261  Training Loss  0.013087821193039417\n",
            "Epoch  7 Batch  219 / 261  Training Loss  0.006768335588276386\n",
            "Epoch  7 Batch  220 / 261  Training Loss  0.006329919211566448\n",
            "Epoch  7 Batch  221 / 261  Training Loss  0.00708552822470665\n",
            "Epoch  7 Batch  222 / 261  Training Loss  0.003234169678762555\n",
            "Epoch  7 Batch  223 / 261  Training Loss  0.0033723118249326944\n",
            "Epoch  7 Batch  224 / 261  Training Loss  0.003680095076560974\n",
            "Epoch  7 Batch  225 / 261  Training Loss  0.0019424792844802141\n",
            "Epoch  7 Batch  226 / 261  Training Loss  0.003951655235141516\n",
            "Epoch  7 Batch  227 / 261  Training Loss  0.008598576299846172\n",
            "Epoch  7 Batch  228 / 261  Training Loss  0.003692983416840434\n",
            "Epoch  7 Batch  229 / 261  Training Loss  0.002352745272219181\n",
            "Epoch  7 Batch  230 / 261  Training Loss  0.004322946071624756\n",
            "Epoch  7 Batch  231 / 261  Training Loss  0.0021191388368606567\n",
            "Epoch  7 Batch  232 / 261  Training Loss  0.008823982439935207\n",
            "Epoch  7 Batch  233 / 261  Training Loss  0.01886422373354435\n",
            "Epoch  7 Batch  234 / 261  Training Loss  0.0021866620518267155\n",
            "Epoch  7 Batch  235 / 261  Training Loss  0.0044651092030107975\n",
            "Epoch  7 Batch  236 / 261  Training Loss  0.012362786568701267\n",
            "Epoch  7 Batch  237 / 261  Training Loss  0.006886547431349754\n",
            "Epoch  7 Batch  238 / 261  Training Loss  0.007417978253215551\n",
            "Epoch  7 Batch  239 / 261  Training Loss  0.0021386826410889626\n",
            "Epoch  7 Batch  240 / 261  Training Loss  0.0011222686152905226\n",
            "Epoch  7 Batch  241 / 261  Training Loss  0.0011995312524959445\n",
            "Epoch  7 Batch  242 / 261  Training Loss  0.001913739601150155\n",
            "Epoch  7 Batch  243 / 261  Training Loss  0.002829041099175811\n",
            "Epoch  7 Batch  244 / 261  Training Loss  0.0008734941366128623\n",
            "Epoch  7 Batch  245 / 261  Training Loss  0.0009924739133566618\n",
            "Epoch  7 Batch  246 / 261  Training Loss  0.004628758877515793\n",
            "Epoch  7 Batch  247 / 261  Training Loss  0.004521092399954796\n",
            "Epoch  7 Batch  248 / 261  Training Loss  0.004087917506694794\n",
            "Epoch  7 Batch  249 / 261  Training Loss  0.0019176614005118608\n",
            "Epoch  7 Batch  250 / 261  Training Loss  0.0012089352821931243\n",
            "Epoch  7 Batch  251 / 261  Training Loss  0.0017177639529109001\n",
            "Epoch  7 Batch  252 / 261  Training Loss  0.0032207872718572617\n",
            "Epoch  7 Batch  253 / 261  Training Loss  0.0017004763940349221\n",
            "Epoch  7 Batch  254 / 261  Training Loss  0.0038286775816231966\n",
            "Epoch  7 Batch  255 / 261  Training Loss  0.0009134145802818239\n",
            "Epoch  7 Batch  256 / 261  Training Loss  0.0019028803799301386\n",
            "Epoch  7 Batch  257 / 261  Training Loss  0.0013449374819174409\n",
            "Epoch  7 Batch  258 / 261  Training Loss  0.0030931485816836357\n",
            "Epoch  7 Batch  259 / 261  Training Loss  0.0006880929577164352\n",
            "Epoch  7 Batch  260 / 261  Training Loss  0.0011588159250095487\n",
            "   8    |    -    |   0.003197   | 98.503989\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 8\n",
            "Epoch  8 Batch  0 / 261  Training Loss  0.00043686694698408246\n",
            "Epoch  8 Batch  1 / 261  Training Loss  0.0005302975769154727\n",
            "Epoch  8 Batch  2 / 261  Training Loss  0.0005742337089031935\n",
            "Epoch  8 Batch  3 / 261  Training Loss  0.0009833454387262464\n",
            "Epoch  8 Batch  4 / 261  Training Loss  0.000860927626490593\n",
            "Epoch  8 Batch  5 / 261  Training Loss  0.0006475281552411616\n",
            "Epoch  8 Batch  6 / 261  Training Loss  0.0037450999952852726\n",
            "Epoch  8 Batch  7 / 261  Training Loss  0.0005015219212509692\n",
            "Epoch  8 Batch  8 / 261  Training Loss  0.0027124506887048483\n",
            "Epoch  8 Batch  9 / 261  Training Loss  0.0008785661775618792\n",
            "Epoch  8 Batch  10 / 261  Training Loss  0.0006089668604545295\n",
            "Epoch  8 Batch  11 / 261  Training Loss  0.0007749803480692208\n",
            "Epoch  8 Batch  12 / 261  Training Loss  0.0011567192850634456\n",
            "Epoch  8 Batch  13 / 261  Training Loss  0.0012539442395791411\n",
            "Epoch  8 Batch  14 / 261  Training Loss  0.00100477144587785\n",
            "Epoch  8 Batch  15 / 261  Training Loss  0.0007695624371990561\n",
            "Epoch  8 Batch  16 / 261  Training Loss  0.0029851612634956837\n",
            "Epoch  8 Batch  17 / 261  Training Loss  0.0006500607705675066\n",
            "Epoch  8 Batch  18 / 261  Training Loss  0.002015376929193735\n",
            "Epoch  8 Batch  19 / 261  Training Loss  0.0009058458963409066\n",
            "Epoch  8 Batch  20 / 261  Training Loss  0.0010356620186939836\n",
            "Epoch  8 Batch  21 / 261  Training Loss  0.0011530068004503846\n",
            "Epoch  8 Batch  22 / 261  Training Loss  0.0012255082838237286\n",
            "Epoch  8 Batch  23 / 261  Training Loss  0.0006645903922617435\n",
            "Epoch  8 Batch  24 / 261  Training Loss  0.0005819585057906806\n",
            "Epoch  8 Batch  25 / 261  Training Loss  0.0005166300688870251\n",
            "Epoch  8 Batch  26 / 261  Training Loss  0.0008144322200678289\n",
            "Epoch  8 Batch  27 / 261  Training Loss  0.0008850321173667908\n",
            "Epoch  8 Batch  28 / 261  Training Loss  0.0017743324860930443\n",
            "Epoch  8 Batch  29 / 261  Training Loss  0.0008667550864629447\n",
            "Epoch  8 Batch  30 / 261  Training Loss  0.0035080835223197937\n",
            "Epoch  8 Batch  31 / 261  Training Loss  0.0033080345019698143\n",
            "Epoch  8 Batch  32 / 261  Training Loss  0.0019457407761365175\n",
            "Epoch  8 Batch  33 / 261  Training Loss  0.0077608730643987656\n",
            "Epoch  8 Batch  34 / 261  Training Loss  0.0007626281585544348\n",
            "Epoch  8 Batch  35 / 261  Training Loss  0.0006039412692189217\n",
            "Epoch  8 Batch  36 / 261  Training Loss  0.0008461193647235632\n",
            "Epoch  8 Batch  37 / 261  Training Loss  0.00458813551813364\n",
            "Epoch  8 Batch  38 / 261  Training Loss  0.0010423309868201613\n",
            "Epoch  8 Batch  39 / 261  Training Loss  0.0012246450642123818\n",
            "Epoch  8 Batch  40 / 261  Training Loss  0.0025262190029025078\n",
            "Epoch  8 Batch  41 / 261  Training Loss  0.0011387758422642946\n",
            "Epoch  8 Batch  42 / 261  Training Loss  0.001997217768803239\n",
            "Epoch  8 Batch  43 / 261  Training Loss  0.0008755869348533452\n",
            "Epoch  8 Batch  44 / 261  Training Loss  0.0013735160464420915\n",
            "Epoch  8 Batch  45 / 261  Training Loss  0.0005772863514721394\n",
            "Epoch  8 Batch  46 / 261  Training Loss  0.0009369730832986534\n",
            "Epoch  8 Batch  47 / 261  Training Loss  0.0006097242003306746\n",
            "Epoch  8 Batch  48 / 261  Training Loss  0.0007776189595460892\n",
            "Epoch  8 Batch  49 / 261  Training Loss  0.0005924001452513039\n",
            "Epoch  8 Batch  50 / 261  Training Loss  0.0008404997061006725\n",
            "Epoch  8 Batch  51 / 261  Training Loss  0.0011017679935321212\n",
            "Epoch  8 Batch  52 / 261  Training Loss  0.001115129911340773\n",
            "Epoch  8 Batch  53 / 261  Training Loss  0.0006877001142129302\n",
            "Epoch  8 Batch  54 / 261  Training Loss  0.0014881788520142436\n",
            "Epoch  8 Batch  55 / 261  Training Loss  0.00047003847430460155\n",
            "Epoch  8 Batch  56 / 261  Training Loss  0.006740069482475519\n",
            "Epoch  8 Batch  57 / 261  Training Loss  0.0013090568827465177\n",
            "Epoch  8 Batch  58 / 261  Training Loss  0.005470023024827242\n",
            "Epoch  8 Batch  59 / 261  Training Loss  0.0007962040835991502\n",
            "Epoch  8 Batch  60 / 261  Training Loss  0.0009160724002867937\n",
            "Epoch  8 Batch  61 / 261  Training Loss  0.000821746711153537\n",
            "Epoch  8 Batch  62 / 261  Training Loss  0.000533108483068645\n",
            "Epoch  8 Batch  63 / 261  Training Loss  0.0005307241808623075\n",
            "Epoch  8 Batch  64 / 261  Training Loss  0.0007085102843120694\n",
            "Epoch  8 Batch  65 / 261  Training Loss  0.00040409903158433735\n",
            "Epoch  8 Batch  66 / 261  Training Loss  0.0011331266723573208\n",
            "Epoch  8 Batch  67 / 261  Training Loss  0.0005108348559588194\n",
            "Epoch  8 Batch  68 / 261  Training Loss  0.0013451138511300087\n",
            "Epoch  8 Batch  69 / 261  Training Loss  0.0009709782898426056\n",
            "Epoch  8 Batch  70 / 261  Training Loss  0.00044048510608263314\n",
            "Epoch  8 Batch  71 / 261  Training Loss  0.0005081412964500487\n",
            "Epoch  8 Batch  72 / 261  Training Loss  0.0004451969580259174\n",
            "Epoch  8 Batch  73 / 261  Training Loss  0.0003762462001759559\n",
            "Epoch  8 Batch  74 / 261  Training Loss  0.0003475678968243301\n",
            "Epoch  8 Batch  75 / 261  Training Loss  0.001227158005349338\n",
            "Epoch  8 Batch  76 / 261  Training Loss  0.0004329568473622203\n",
            "Epoch  8 Batch  77 / 261  Training Loss  0.0004788683436345309\n",
            "Epoch  8 Batch  78 / 261  Training Loss  0.00037860259180888534\n",
            "Epoch  8 Batch  79 / 261  Training Loss  0.0004895775346085429\n",
            "Epoch  8 Batch  80 / 261  Training Loss  0.0014949314063414931\n",
            "Epoch  8 Batch  81 / 261  Training Loss  0.0007790254894644022\n",
            "Epoch  8 Batch  82 / 261  Training Loss  0.00033394727506674826\n",
            "Epoch  8 Batch  83 / 261  Training Loss  0.0005037583759985864\n",
            "Epoch  8 Batch  84 / 261  Training Loss  0.00047473981976509094\n",
            "Epoch  8 Batch  85 / 261  Training Loss  0.0006453015375882387\n",
            "Epoch  8 Batch  86 / 261  Training Loss  0.0005889253225177526\n",
            "Epoch  8 Batch  87 / 261  Training Loss  0.0005004341946914792\n",
            "Epoch  8 Batch  88 / 261  Training Loss  0.00030129338847473264\n",
            "Epoch  8 Batch  89 / 261  Training Loss  0.0004963916726410389\n",
            "Epoch  8 Batch  90 / 261  Training Loss  0.00048094530939124525\n",
            "Epoch  8 Batch  91 / 261  Training Loss  0.00045308138942345977\n",
            "Epoch  8 Batch  92 / 261  Training Loss  0.0008438141085207462\n",
            "Epoch  8 Batch  93 / 261  Training Loss  0.0007759364671073854\n",
            "Epoch  8 Batch  94 / 261  Training Loss  0.0004195580550003797\n",
            "Epoch  8 Batch  95 / 261  Training Loss  0.0005078694084659219\n",
            "Epoch  8 Batch  96 / 261  Training Loss  0.00043834696407429874\n",
            "Epoch  8 Batch  97 / 261  Training Loss  0.00035661295987665653\n",
            "Epoch  8 Batch  98 / 261  Training Loss  0.0005455384962260723\n",
            "Epoch  8 Batch  99 / 261  Training Loss  0.0004663471190724522\n",
            "Epoch  8 Batch  100 / 261  Training Loss  0.00041252822848036885\n",
            "Epoch  8 Batch  101 / 261  Training Loss  0.0006102981860749424\n",
            "Epoch  8 Batch  102 / 261  Training Loss  0.008052757009863853\n",
            "Epoch  8 Batch  103 / 261  Training Loss  0.0004382302286103368\n",
            "Epoch  8 Batch  104 / 261  Training Loss  0.0038813818246126175\n",
            "Epoch  8 Batch  105 / 261  Training Loss  0.0010205833241343498\n",
            "Epoch  8 Batch  106 / 261  Training Loss  0.0016890940023586154\n",
            "Epoch  8 Batch  107 / 261  Training Loss  0.0007188291056081653\n",
            "Epoch  8 Batch  108 / 261  Training Loss  0.0006595462327823043\n",
            "Epoch  8 Batch  109 / 261  Training Loss  0.001942810253240168\n",
            "Epoch  8 Batch  110 / 261  Training Loss  0.001470317249186337\n",
            "Epoch  8 Batch  111 / 261  Training Loss  0.0008381916559301317\n",
            "Epoch  8 Batch  112 / 261  Training Loss  0.00046515281428582966\n",
            "Epoch  8 Batch  113 / 261  Training Loss  0.0008076115045696497\n",
            "Epoch  8 Batch  114 / 261  Training Loss  0.002590364310890436\n",
            "Epoch  8 Batch  115 / 261  Training Loss  0.00040498957969248295\n",
            "Epoch  8 Batch  116 / 261  Training Loss  0.0006153930444270372\n",
            "Epoch  8 Batch  117 / 261  Training Loss  0.00036875810474157333\n",
            "Epoch  8 Batch  118 / 261  Training Loss  0.0004508316924329847\n",
            "Epoch  8 Batch  119 / 261  Training Loss  0.0012124773347750306\n",
            "Epoch  8 Batch  120 / 261  Training Loss  0.0006821139832027256\n",
            "Epoch  8 Batch  121 / 261  Training Loss  0.000755556218791753\n",
            "Epoch  8 Batch  122 / 261  Training Loss  0.0007286933832801878\n",
            "Epoch  8 Batch  123 / 261  Training Loss  0.00042024190770462155\n",
            "Epoch  8 Batch  124 / 261  Training Loss  0.0004584006092045456\n",
            "Epoch  8 Batch  125 / 261  Training Loss  0.0004431698762346059\n",
            "Epoch  8 Batch  126 / 261  Training Loss  0.00037873571272939444\n",
            "Epoch  8 Batch  127 / 261  Training Loss  0.0003643425297923386\n",
            "Epoch  8 Batch  128 / 261  Training Loss  0.00057100725825876\n",
            "Epoch  8 Batch  129 / 261  Training Loss  0.00046175465104170144\n",
            "Epoch  8 Batch  130 / 261  Training Loss  0.00048599368892610073\n",
            "Epoch  8 Batch  131 / 261  Training Loss  0.0002689330722205341\n",
            "Epoch  8 Batch  132 / 261  Training Loss  0.000534104008693248\n",
            "Epoch  8 Batch  133 / 261  Training Loss  0.00025628277217037976\n",
            "Epoch  8 Batch  134 / 261  Training Loss  0.000400492746848613\n",
            "Epoch  8 Batch  135 / 261  Training Loss  0.00046332660713233054\n",
            "Epoch  8 Batch  136 / 261  Training Loss  0.00041145755676552653\n",
            "Epoch  8 Batch  137 / 261  Training Loss  0.00071817880962044\n",
            "Epoch  8 Batch  138 / 261  Training Loss  0.0003336871159262955\n",
            "Epoch  8 Batch  139 / 261  Training Loss  0.0005294425645843148\n",
            "Epoch  8 Batch  140 / 261  Training Loss  0.000386022322345525\n",
            "Epoch  8 Batch  141 / 261  Training Loss  0.0004643273714464158\n",
            "Epoch  8 Batch  142 / 261  Training Loss  0.00029870372964069247\n",
            "Epoch  8 Batch  143 / 261  Training Loss  0.0003788379253819585\n",
            "Epoch  8 Batch  144 / 261  Training Loss  0.0004996623029001057\n",
            "Epoch  8 Batch  145 / 261  Training Loss  0.000464149983599782\n",
            "Epoch  8 Batch  146 / 261  Training Loss  0.00023135058290790766\n",
            "Epoch  8 Batch  147 / 261  Training Loss  0.0003003164893016219\n",
            "Epoch  8 Batch  148 / 261  Training Loss  0.00021142273908481002\n",
            "Epoch  8 Batch  149 / 261  Training Loss  0.0005180228035897017\n",
            "Epoch  8 Batch  150 / 261  Training Loss  0.00045522983418777585\n",
            "Epoch  8 Batch  151 / 261  Training Loss  0.00038019855855964124\n",
            "Epoch  8 Batch  152 / 261  Training Loss  0.0004484397650230676\n",
            "Epoch  8 Batch  153 / 261  Training Loss  0.004468688275665045\n",
            "Epoch  8 Batch  154 / 261  Training Loss  0.0003251465386711061\n",
            "Epoch  8 Batch  155 / 261  Training Loss  0.000510615122038871\n",
            "Epoch  8 Batch  156 / 261  Training Loss  0.0010906244860962033\n",
            "Epoch  8 Batch  157 / 261  Training Loss  0.00034976183087565005\n",
            "Epoch  8 Batch  158 / 261  Training Loss  0.00029668115894310176\n",
            "Epoch  8 Batch  159 / 261  Training Loss  0.00031572359148412943\n",
            "Epoch  8 Batch  160 / 261  Training Loss  0.0006521519972011447\n",
            "Epoch  8 Batch  161 / 261  Training Loss  0.0007441463530994952\n",
            "Epoch  8 Batch  162 / 261  Training Loss  0.002047839807346463\n",
            "Epoch  8 Batch  163 / 261  Training Loss  0.0018214398296549916\n",
            "Epoch  8 Batch  164 / 261  Training Loss  0.0002606878988444805\n",
            "Epoch  8 Batch  165 / 261  Training Loss  0.00026374522713012993\n",
            "Epoch  8 Batch  166 / 261  Training Loss  0.001049419748596847\n",
            "Epoch  8 Batch  167 / 261  Training Loss  0.0003380118578206748\n",
            "Epoch  8 Batch  168 / 261  Training Loss  0.00045017979573458433\n",
            "Epoch  8 Batch  169 / 261  Training Loss  0.0003536191943567246\n",
            "Epoch  8 Batch  170 / 261  Training Loss  0.0006305829156190157\n",
            "Epoch  8 Batch  171 / 261  Training Loss  0.00035780746839009225\n",
            "Epoch  8 Batch  172 / 261  Training Loss  0.0003127408854197711\n",
            "Epoch  8 Batch  173 / 261  Training Loss  0.00027352149481885135\n",
            "Epoch  8 Batch  174 / 261  Training Loss  0.00021940635633654892\n",
            "Epoch  8 Batch  175 / 261  Training Loss  0.0003409805940464139\n",
            "Epoch  8 Batch  176 / 261  Training Loss  0.00026682676980271935\n",
            "Epoch  8 Batch  177 / 261  Training Loss  0.0007225724402815104\n",
            "Epoch  8 Batch  178 / 261  Training Loss  0.0005367026315070689\n",
            "Epoch  8 Batch  179 / 261  Training Loss  0.00025039861793629825\n",
            "Epoch  8 Batch  180 / 261  Training Loss  0.00045569639769382775\n",
            "Epoch  8 Batch  181 / 261  Training Loss  0.0004228799371048808\n",
            "Epoch  8 Batch  182 / 261  Training Loss  0.00025449664099141955\n",
            "Epoch  8 Batch  183 / 261  Training Loss  0.0002194928383687511\n",
            "Epoch  8 Batch  184 / 261  Training Loss  0.00026582033024169505\n",
            "Epoch  8 Batch  185 / 261  Training Loss  0.0009505412890575826\n",
            "Epoch  8 Batch  186 / 261  Training Loss  0.0008474526694044471\n",
            "Epoch  8 Batch  187 / 261  Training Loss  0.00021249409473966807\n",
            "Epoch  8 Batch  188 / 261  Training Loss  0.0003898883005604148\n",
            "Epoch  8 Batch  189 / 261  Training Loss  0.0007864067447371781\n",
            "Epoch  8 Batch  190 / 261  Training Loss  0.00040475680725649\n",
            "Epoch  8 Batch  191 / 261  Training Loss  0.000430547894211486\n",
            "Epoch  8 Batch  192 / 261  Training Loss  0.00033160197199322283\n",
            "Epoch  8 Batch  193 / 261  Training Loss  0.0004271673387847841\n",
            "Epoch  8 Batch  194 / 261  Training Loss  0.0003267083375249058\n",
            "Epoch  8 Batch  195 / 261  Training Loss  0.0007818319136276841\n",
            "Epoch  8 Batch  196 / 261  Training Loss  0.0005155315157026052\n",
            "Epoch  8 Batch  197 / 261  Training Loss  0.00674894405528903\n",
            "Epoch  8 Batch  198 / 261  Training Loss  0.0007813730044290423\n",
            "Epoch  8 Batch  199 / 261  Training Loss  0.0002777898916974664\n",
            "Epoch  8 Batch  200 / 261  Training Loss  0.0008965185843408108\n",
            "Epoch  8 Batch  201 / 261  Training Loss  0.0008392054587602615\n",
            "Epoch  8 Batch  202 / 261  Training Loss  0.0004765407065860927\n",
            "Epoch  8 Batch  203 / 261  Training Loss  0.0005995381507091224\n",
            "Epoch  8 Batch  204 / 261  Training Loss  0.00025856311549432576\n",
            "Epoch  8 Batch  205 / 261  Training Loss  0.0006966497167013586\n",
            "Epoch  8 Batch  206 / 261  Training Loss  0.00148634088691324\n",
            "Epoch  8 Batch  207 / 261  Training Loss  0.0007929124403744936\n",
            "Epoch  8 Batch  208 / 261  Training Loss  0.0006656870828010142\n",
            "Epoch  8 Batch  209 / 261  Training Loss  0.0007741919253021479\n",
            "Epoch  8 Batch  210 / 261  Training Loss  0.0029011594597250223\n",
            "Epoch  8 Batch  211 / 261  Training Loss  0.0007906300015747547\n",
            "Epoch  8 Batch  212 / 261  Training Loss  0.0005228818044997752\n",
            "Epoch  8 Batch  213 / 261  Training Loss  0.00030118649010546505\n",
            "Epoch  8 Batch  214 / 261  Training Loss  0.0014800748322159052\n",
            "Epoch  8 Batch  215 / 261  Training Loss  0.00026728733791969717\n",
            "Epoch  8 Batch  216 / 261  Training Loss  0.00047388835810124874\n",
            "Epoch  8 Batch  217 / 261  Training Loss  0.00041427582618780434\n",
            "Epoch  8 Batch  218 / 261  Training Loss  0.00045233219861984253\n",
            "Epoch  8 Batch  219 / 261  Training Loss  0.00033838243689388037\n",
            "Epoch  8 Batch  220 / 261  Training Loss  0.00023184505698736757\n",
            "Epoch  8 Batch  221 / 261  Training Loss  0.00030406241421587765\n",
            "Epoch  8 Batch  222 / 261  Training Loss  0.0008512453059665859\n",
            "Epoch  8 Batch  223 / 261  Training Loss  0.0010716388933360577\n",
            "Epoch  8 Batch  224 / 261  Training Loss  0.00032217917032539845\n",
            "Epoch  8 Batch  225 / 261  Training Loss  0.000500345544423908\n",
            "Epoch  8 Batch  226 / 261  Training Loss  0.000661041762214154\n",
            "Epoch  8 Batch  227 / 261  Training Loss  0.00039484642911702394\n",
            "Epoch  8 Batch  228 / 261  Training Loss  0.00026621046708896756\n",
            "Epoch  8 Batch  229 / 261  Training Loss  0.00042907276656478643\n",
            "Epoch  8 Batch  230 / 261  Training Loss  0.0002541988214943558\n",
            "Epoch  8 Batch  231 / 261  Training Loss  0.0007459598709829152\n",
            "Epoch  8 Batch  232 / 261  Training Loss  0.0002423417172394693\n",
            "Epoch  8 Batch  233 / 261  Training Loss  0.00020060688257217407\n",
            "Epoch  8 Batch  234 / 261  Training Loss  0.0016485470114275813\n",
            "Epoch  8 Batch  235 / 261  Training Loss  0.006058869883418083\n",
            "Epoch  8 Batch  236 / 261  Training Loss  0.0013586492277681828\n",
            "Epoch  8 Batch  237 / 261  Training Loss  0.0003607104590628296\n",
            "Epoch  8 Batch  238 / 261  Training Loss  0.001066461205482483\n",
            "Epoch  8 Batch  239 / 261  Training Loss  0.0003697877109516412\n",
            "Epoch  8 Batch  240 / 261  Training Loss  0.0004822663322556764\n",
            "Epoch  8 Batch  241 / 261  Training Loss  0.0005359354545362294\n",
            "Epoch  8 Batch  242 / 261  Training Loss  0.00031415768899023533\n",
            "Epoch  8 Batch  243 / 261  Training Loss  0.00037382415030151606\n",
            "Epoch  8 Batch  244 / 261  Training Loss  0.0003334531211294234\n",
            "Epoch  8 Batch  245 / 261  Training Loss  0.00036796764470636845\n",
            "Epoch  8 Batch  246 / 261  Training Loss  0.00036153095425106585\n",
            "Epoch  8 Batch  247 / 261  Training Loss  0.0003382616851013154\n",
            "Epoch  8 Batch  248 / 261  Training Loss  0.0005219972226768732\n",
            "Epoch  8 Batch  249 / 261  Training Loss  0.00047187026939354837\n",
            "Epoch  8 Batch  250 / 261  Training Loss  0.0004182494303677231\n",
            "Epoch  8 Batch  251 / 261  Training Loss  0.000980159966275096\n",
            "Epoch  8 Batch  252 / 261  Training Loss  0.00031449011294171214\n",
            "Epoch  8 Batch  253 / 261  Training Loss  0.00019698355754371732\n",
            "Epoch  8 Batch  254 / 261  Training Loss  0.0003398820699658245\n",
            "Epoch  8 Batch  255 / 261  Training Loss  0.00031574786407873034\n",
            "Epoch  8 Batch  256 / 261  Training Loss  0.00064130499958992\n",
            "Epoch  8 Batch  257 / 261  Training Loss  0.00028289417969062924\n",
            "Epoch  8 Batch  258 / 261  Training Loss  0.0002677057927940041\n",
            "Epoch  8 Batch  259 / 261  Training Loss  0.0002364723914070055\n",
            "Epoch  8 Batch  260 / 261  Training Loss  0.000459827744634822\n",
            "   9    |    -    |   0.000899   | 99.534574\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 9\n",
            "Epoch  9 Batch  0 / 261  Training Loss  0.0003227358974982053\n",
            "Epoch  9 Batch  1 / 261  Training Loss  0.000265802547801286\n",
            "Epoch  9 Batch  2 / 261  Training Loss  0.00023250895901583135\n",
            "Epoch  9 Batch  3 / 261  Training Loss  0.0003828260814771056\n",
            "Epoch  9 Batch  4 / 261  Training Loss  0.00026558604440651834\n",
            "Epoch  9 Batch  5 / 261  Training Loss  0.0003933252301067114\n",
            "Epoch  9 Batch  6 / 261  Training Loss  0.00032320560421794653\n",
            "Epoch  9 Batch  7 / 261  Training Loss  0.00030764020630158484\n",
            "Epoch  9 Batch  8 / 261  Training Loss  0.0002885960857383907\n",
            "Epoch  9 Batch  9 / 261  Training Loss  0.00032370633562095463\n",
            "Epoch  9 Batch  10 / 261  Training Loss  0.00017875406774692237\n",
            "Epoch  9 Batch  11 / 261  Training Loss  0.00019569334108382463\n",
            "Epoch  9 Batch  12 / 261  Training Loss  0.00027074466925114393\n",
            "Epoch  9 Batch  13 / 261  Training Loss  0.0002214273263234645\n",
            "Epoch  9 Batch  14 / 261  Training Loss  0.00019178491493221372\n",
            "Epoch  9 Batch  15 / 261  Training Loss  0.0003236941702198237\n",
            "Epoch  9 Batch  16 / 261  Training Loss  0.00020700672757811844\n",
            "Epoch  9 Batch  17 / 261  Training Loss  0.000267268274910748\n",
            "Epoch  9 Batch  18 / 261  Training Loss  0.0001957196363946423\n",
            "Epoch  9 Batch  19 / 261  Training Loss  0.000197079119971022\n",
            "Epoch  9 Batch  20 / 261  Training Loss  0.0006188382976688445\n",
            "Epoch  9 Batch  21 / 261  Training Loss  0.0002846629358828068\n",
            "Epoch  9 Batch  22 / 261  Training Loss  0.00017530738841742277\n",
            "Epoch  9 Batch  23 / 261  Training Loss  0.00024340493837371469\n",
            "Epoch  9 Batch  24 / 261  Training Loss  0.00024973266408778727\n",
            "Epoch  9 Batch  25 / 261  Training Loss  0.0004055944737046957\n",
            "Epoch  9 Batch  26 / 261  Training Loss  0.00028346068575046957\n",
            "Epoch  9 Batch  27 / 261  Training Loss  0.00022788820206187665\n",
            "Epoch  9 Batch  28 / 261  Training Loss  0.00016961425717454404\n",
            "Epoch  9 Batch  29 / 261  Training Loss  0.0003021935117430985\n",
            "Epoch  9 Batch  30 / 261  Training Loss  0.00021032645599916577\n",
            "Epoch  9 Batch  31 / 261  Training Loss  0.00016008163220249116\n",
            "Epoch  9 Batch  32 / 261  Training Loss  0.0003200253704562783\n",
            "Epoch  9 Batch  33 / 261  Training Loss  0.00025737364194355905\n",
            "Epoch  9 Batch  34 / 261  Training Loss  0.00027023005532100797\n",
            "Epoch  9 Batch  35 / 261  Training Loss  0.00016920696361921728\n",
            "Epoch  9 Batch  36 / 261  Training Loss  0.00022826492204330862\n",
            "Epoch  9 Batch  37 / 261  Training Loss  0.00045383640099316835\n",
            "Epoch  9 Batch  38 / 261  Training Loss  0.00019111997971776873\n",
            "Epoch  9 Batch  39 / 261  Training Loss  0.00025140622165054083\n",
            "Epoch  9 Batch  40 / 261  Training Loss  0.00020008542924188077\n",
            "Epoch  9 Batch  41 / 261  Training Loss  0.00023225355835165828\n",
            "Epoch  9 Batch  42 / 261  Training Loss  0.002486499957740307\n",
            "Epoch  9 Batch  43 / 261  Training Loss  0.0013933369191363454\n",
            "Epoch  9 Batch  44 / 261  Training Loss  0.0005863921251147985\n",
            "Epoch  9 Batch  45 / 261  Training Loss  0.00018002245633397251\n",
            "Epoch  9 Batch  46 / 261  Training Loss  0.0003095124557148665\n",
            "Epoch  9 Batch  47 / 261  Training Loss  0.00015272165182977915\n",
            "Epoch  9 Batch  48 / 261  Training Loss  0.00041743984911590815\n",
            "Epoch  9 Batch  49 / 261  Training Loss  0.00016927678370848298\n",
            "Epoch  9 Batch  50 / 261  Training Loss  0.0002873363846447319\n",
            "Epoch  9 Batch  51 / 261  Training Loss  0.00031093796133063734\n",
            "Epoch  9 Batch  52 / 261  Training Loss  0.0003470508090686053\n",
            "Epoch  9 Batch  53 / 261  Training Loss  0.0004355435085017234\n",
            "Epoch  9 Batch  54 / 261  Training Loss  0.0002227185177616775\n",
            "Epoch  9 Batch  55 / 261  Training Loss  0.0003583320649340749\n",
            "Epoch  9 Batch  56 / 261  Training Loss  0.0005066129961051047\n",
            "Epoch  9 Batch  57 / 261  Training Loss  0.0002698611933737993\n",
            "Epoch  9 Batch  58 / 261  Training Loss  0.00024686360848136246\n",
            "Epoch  9 Batch  59 / 261  Training Loss  0.00022548690321855247\n",
            "Epoch  9 Batch  60 / 261  Training Loss  0.000297025078907609\n",
            "Epoch  9 Batch  61 / 261  Training Loss  0.00026586023159325123\n",
            "Epoch  9 Batch  62 / 261  Training Loss  0.0003272201865911484\n",
            "Epoch  9 Batch  63 / 261  Training Loss  0.0003223659878131002\n",
            "Epoch  9 Batch  64 / 261  Training Loss  0.00028036092408001423\n",
            "Epoch  9 Batch  65 / 261  Training Loss  0.000187744852155447\n",
            "Epoch  9 Batch  66 / 261  Training Loss  0.00020489201415330172\n",
            "Epoch  9 Batch  67 / 261  Training Loss  0.00017250070231966674\n",
            "Epoch  9 Batch  68 / 261  Training Loss  0.00014783347432967275\n",
            "Epoch  9 Batch  69 / 261  Training Loss  0.0002157159906346351\n",
            "Epoch  9 Batch  70 / 261  Training Loss  0.0002659360761754215\n",
            "Epoch  9 Batch  71 / 261  Training Loss  0.00018375903891865164\n",
            "Epoch  9 Batch  72 / 261  Training Loss  0.000300017767585814\n",
            "Epoch  9 Batch  73 / 261  Training Loss  0.0002044494467554614\n",
            "Epoch  9 Batch  74 / 261  Training Loss  0.00025491704582236707\n",
            "Epoch  9 Batch  75 / 261  Training Loss  0.0001661814603721723\n",
            "Epoch  9 Batch  76 / 261  Training Loss  0.00024141406174749136\n",
            "Epoch  9 Batch  77 / 261  Training Loss  0.00024882639991119504\n",
            "Epoch  9 Batch  78 / 261  Training Loss  0.00020566201419569552\n",
            "Epoch  9 Batch  79 / 261  Training Loss  0.00011463918053777888\n",
            "Epoch  9 Batch  80 / 261  Training Loss  0.00022004797938279808\n",
            "Epoch  9 Batch  81 / 261  Training Loss  0.00019939304911531508\n",
            "Epoch  9 Batch  82 / 261  Training Loss  0.0001729931536829099\n",
            "Epoch  9 Batch  83 / 261  Training Loss  0.0002193864929722622\n",
            "Epoch  9 Batch  84 / 261  Training Loss  0.0005331280990503728\n",
            "Epoch  9 Batch  85 / 261  Training Loss  0.00018772788462229073\n",
            "Epoch  9 Batch  86 / 261  Training Loss  0.00021844549337401986\n",
            "Epoch  9 Batch  87 / 261  Training Loss  0.00012842606520280242\n",
            "Epoch  9 Batch  88 / 261  Training Loss  0.00014916165673639625\n",
            "Epoch  9 Batch  89 / 261  Training Loss  0.00028231870965100825\n",
            "Epoch  9 Batch  90 / 261  Training Loss  0.0003283541009295732\n",
            "Epoch  9 Batch  91 / 261  Training Loss  0.00016156380297616124\n",
            "Epoch  9 Batch  92 / 261  Training Loss  0.000336489814799279\n",
            "Epoch  9 Batch  93 / 261  Training Loss  0.0002476947265677154\n",
            "Epoch  9 Batch  94 / 261  Training Loss  0.0004179583047516644\n",
            "Epoch  9 Batch  95 / 261  Training Loss  0.00016113514720927924\n",
            "Epoch  9 Batch  96 / 261  Training Loss  0.0002619149745441973\n",
            "Epoch  9 Batch  97 / 261  Training Loss  0.0001656688255025074\n",
            "Epoch  9 Batch  98 / 261  Training Loss  0.0002890500472858548\n",
            "Epoch  9 Batch  99 / 261  Training Loss  0.0001996028731809929\n",
            "Epoch  9 Batch  100 / 261  Training Loss  0.00016275685629807413\n",
            "Epoch  9 Batch  101 / 261  Training Loss  0.00024087578640319407\n",
            "Epoch  9 Batch  102 / 261  Training Loss  0.0002235603897133842\n",
            "Epoch  9 Batch  103 / 261  Training Loss  0.00017777930770535022\n",
            "Epoch  9 Batch  104 / 261  Training Loss  0.00016980180225800723\n",
            "Epoch  9 Batch  105 / 261  Training Loss  0.00018951085803564638\n",
            "Epoch  9 Batch  106 / 261  Training Loss  0.0002582274319138378\n",
            "Epoch  9 Batch  107 / 261  Training Loss  0.00013656230294145644\n",
            "Epoch  9 Batch  108 / 261  Training Loss  0.0001650143531151116\n",
            "Epoch  9 Batch  109 / 261  Training Loss  0.00019188830628991127\n",
            "Epoch  9 Batch  110 / 261  Training Loss  0.0002365259570069611\n",
            "Epoch  9 Batch  111 / 261  Training Loss  0.0002289421099703759\n",
            "Epoch  9 Batch  112 / 261  Training Loss  0.0001383621129207313\n",
            "Epoch  9 Batch  113 / 261  Training Loss  0.0001953576720552519\n",
            "Epoch  9 Batch  114 / 261  Training Loss  0.00016108772251755\n",
            "Epoch  9 Batch  115 / 261  Training Loss  0.0003051075618714094\n",
            "Epoch  9 Batch  116 / 261  Training Loss  0.0017054760828614235\n",
            "Epoch  9 Batch  117 / 261  Training Loss  0.009711455553770065\n",
            "Epoch  9 Batch  118 / 261  Training Loss  0.00027522770687937737\n",
            "Epoch  9 Batch  119 / 261  Training Loss  0.00029079639352858067\n",
            "Epoch  9 Batch  120 / 261  Training Loss  0.0002075717056868598\n",
            "Epoch  9 Batch  121 / 261  Training Loss  0.00016551764565519989\n",
            "Epoch  9 Batch  122 / 261  Training Loss  0.0010335086844861507\n",
            "Epoch  9 Batch  123 / 261  Training Loss  0.0002641047758515924\n",
            "Epoch  9 Batch  124 / 261  Training Loss  0.0004930750001221895\n",
            "Epoch  9 Batch  125 / 261  Training Loss  0.00017282305634580553\n",
            "Epoch  9 Batch  126 / 261  Training Loss  0.0003456599952187389\n",
            "Epoch  9 Batch  127 / 261  Training Loss  0.00023807268007658422\n",
            "Epoch  9 Batch  128 / 261  Training Loss  0.00022682026610709727\n",
            "Epoch  9 Batch  129 / 261  Training Loss  0.0002945821324829012\n",
            "Epoch  9 Batch  130 / 261  Training Loss  0.0002194933913415298\n",
            "Epoch  9 Batch  131 / 261  Training Loss  0.00014547552564181387\n",
            "Epoch  9 Batch  132 / 261  Training Loss  0.00019256063387729228\n",
            "Epoch  9 Batch  133 / 261  Training Loss  0.0001948783319676295\n",
            "Epoch  9 Batch  134 / 261  Training Loss  0.00023432643502019346\n",
            "Epoch  9 Batch  135 / 261  Training Loss  0.00030966068152338266\n",
            "Epoch  9 Batch  136 / 261  Training Loss  0.00018494590767659247\n",
            "Epoch  9 Batch  137 / 261  Training Loss  0.0002652177063282579\n",
            "Epoch  9 Batch  138 / 261  Training Loss  0.00017771909188013524\n",
            "Epoch  9 Batch  139 / 261  Training Loss  0.00017072838090825826\n",
            "Epoch  9 Batch  140 / 261  Training Loss  0.00019562219677027315\n",
            "Epoch  9 Batch  141 / 261  Training Loss  0.00023890228476375341\n",
            "Epoch  9 Batch  142 / 261  Training Loss  0.0002298793406225741\n",
            "Epoch  9 Batch  143 / 261  Training Loss  0.00025221460964530706\n",
            "Epoch  9 Batch  144 / 261  Training Loss  0.00021299567015375942\n",
            "Epoch  9 Batch  145 / 261  Training Loss  0.00038845051312819123\n",
            "Epoch  9 Batch  146 / 261  Training Loss  0.00046509047388099134\n",
            "Epoch  9 Batch  147 / 261  Training Loss  0.00018385032308287919\n",
            "Epoch  9 Batch  148 / 261  Training Loss  0.00017955225484911352\n",
            "Epoch  9 Batch  149 / 261  Training Loss  0.0003227342676836997\n",
            "Epoch  9 Batch  150 / 261  Training Loss  0.0007771458476781845\n",
            "Epoch  9 Batch  151 / 261  Training Loss  0.0003634068707469851\n",
            "Epoch  9 Batch  152 / 261  Training Loss  0.0003125819202978164\n",
            "Epoch  9 Batch  153 / 261  Training Loss  0.0003209164133295417\n",
            "Epoch  9 Batch  154 / 261  Training Loss  0.003386126132681966\n",
            "Epoch  9 Batch  155 / 261  Training Loss  0.0007531597511842847\n",
            "Epoch  9 Batch  156 / 261  Training Loss  0.00013379113806877285\n",
            "Epoch  9 Batch  157 / 261  Training Loss  0.00025630046729929745\n",
            "Epoch  9 Batch  158 / 261  Training Loss  0.00018600486509967595\n",
            "Epoch  9 Batch  159 / 261  Training Loss  0.00022424831695389003\n",
            "Epoch  9 Batch  160 / 261  Training Loss  0.00022119372442830354\n",
            "Epoch  9 Batch  161 / 261  Training Loss  0.000256590370554477\n",
            "Epoch  9 Batch  162 / 261  Training Loss  0.00024107804347295314\n",
            "Epoch  9 Batch  163 / 261  Training Loss  0.0007257365505211055\n",
            "Epoch  9 Batch  164 / 261  Training Loss  0.0002904876309912652\n",
            "Epoch  9 Batch  165 / 261  Training Loss  0.00021439697593450546\n",
            "Epoch  9 Batch  166 / 261  Training Loss  0.00020246322674211115\n",
            "Epoch  9 Batch  167 / 261  Training Loss  0.0002272420679219067\n",
            "Epoch  9 Batch  168 / 261  Training Loss  0.0001853172288974747\n",
            "Epoch  9 Batch  169 / 261  Training Loss  0.00022788373462390155\n",
            "Epoch  9 Batch  170 / 261  Training Loss  0.00014798881602473557\n",
            "Epoch  9 Batch  171 / 261  Training Loss  0.0001960180525202304\n",
            "Epoch  9 Batch  172 / 261  Training Loss  0.00014827401901129633\n",
            "Epoch  9 Batch  173 / 261  Training Loss  0.00033152580726891756\n",
            "Epoch  9 Batch  174 / 261  Training Loss  0.00023419744684360921\n",
            "Epoch  9 Batch  175 / 261  Training Loss  0.0003186345857102424\n",
            "Epoch  9 Batch  176 / 261  Training Loss  0.00025856817956082523\n",
            "Epoch  9 Batch  177 / 261  Training Loss  0.005876136943697929\n",
            "Epoch  9 Batch  178 / 261  Training Loss  0.010783705860376358\n",
            "Epoch  9 Batch  179 / 261  Training Loss  0.0003127481904812157\n",
            "Epoch  9 Batch  180 / 261  Training Loss  0.0003787142923101783\n",
            "Epoch  9 Batch  181 / 261  Training Loss  0.002740767551586032\n",
            "Epoch  9 Batch  182 / 261  Training Loss  0.018826378509402275\n",
            "Epoch  9 Batch  183 / 261  Training Loss  0.006137731485068798\n",
            "Epoch  9 Batch  184 / 261  Training Loss  0.0006788830505684018\n",
            "Epoch  9 Batch  185 / 261  Training Loss  0.0010059011401608586\n",
            "Epoch  9 Batch  186 / 261  Training Loss  0.011913971975445747\n",
            "Epoch  9 Batch  187 / 261  Training Loss  0.030029140412807465\n",
            "Epoch  9 Batch  188 / 261  Training Loss  0.01130565907806158\n",
            "Epoch  9 Batch  189 / 261  Training Loss  0.08056505024433136\n",
            "Epoch  9 Batch  190 / 261  Training Loss  0.016480257734656334\n",
            "Epoch  9 Batch  191 / 261  Training Loss  0.009564007632434368\n",
            "Epoch  9 Batch  192 / 261  Training Loss  0.04901055246591568\n",
            "Epoch  9 Batch  193 / 261  Training Loss  0.017397968098521233\n",
            "Epoch  9 Batch  194 / 261  Training Loss  0.019885901361703873\n",
            "Epoch  9 Batch  195 / 261  Training Loss  0.009924599900841713\n",
            "Epoch  9 Batch  196 / 261  Training Loss  0.014150326140224934\n",
            "Epoch  9 Batch  197 / 261  Training Loss  0.002145434031262994\n",
            "Epoch  9 Batch  198 / 261  Training Loss  0.0011822623200714588\n",
            "Epoch  9 Batch  199 / 261  Training Loss  0.00339107564650476\n",
            "Epoch  9 Batch  200 / 261  Training Loss  0.0010105703258886933\n",
            "Epoch  9 Batch  201 / 261  Training Loss  0.0008946233429014683\n",
            "Epoch  9 Batch  202 / 261  Training Loss  0.002163055818527937\n",
            "Epoch  9 Batch  203 / 261  Training Loss  0.001701944973319769\n",
            "Epoch  9 Batch  204 / 261  Training Loss  0.0009525243658572435\n",
            "Epoch  9 Batch  205 / 261  Training Loss  0.0035425021778792143\n",
            "Epoch  9 Batch  206 / 261  Training Loss  0.001223867293447256\n",
            "Epoch  9 Batch  207 / 261  Training Loss  0.003419340355321765\n",
            "Epoch  9 Batch  208 / 261  Training Loss  0.00755473505705595\n",
            "Epoch  9 Batch  209 / 261  Training Loss  0.0011527669848874211\n",
            "Epoch  9 Batch  210 / 261  Training Loss  0.0020588012412190437\n",
            "Epoch  9 Batch  211 / 261  Training Loss  0.0008117899997159839\n",
            "Epoch  9 Batch  212 / 261  Training Loss  0.0006595147424377501\n",
            "Epoch  9 Batch  213 / 261  Training Loss  0.0014057325897738338\n",
            "Epoch  9 Batch  214 / 261  Training Loss  0.0015509106451645494\n",
            "Epoch  9 Batch  215 / 261  Training Loss  0.001388963428325951\n",
            "Epoch  9 Batch  216 / 261  Training Loss  0.0036024751607328653\n",
            "Epoch  9 Batch  217 / 261  Training Loss  0.0016228477470576763\n",
            "Epoch  9 Batch  218 / 261  Training Loss  0.002019844949245453\n",
            "Epoch  9 Batch  219 / 261  Training Loss  0.0010553402826189995\n",
            "Epoch  9 Batch  220 / 261  Training Loss  0.004549297504127026\n",
            "Epoch  9 Batch  221 / 261  Training Loss  0.002171197207644582\n",
            "Epoch  9 Batch  222 / 261  Training Loss  0.0009205878595821559\n",
            "Epoch  9 Batch  223 / 261  Training Loss  0.0006561827613040805\n",
            "Epoch  9 Batch  224 / 261  Training Loss  0.00052311469335109\n",
            "Epoch  9 Batch  225 / 261  Training Loss  0.0004299384308978915\n",
            "Epoch  9 Batch  226 / 261  Training Loss  0.0029721208848059177\n",
            "Epoch  9 Batch  227 / 261  Training Loss  0.0017259124433621764\n",
            "Epoch  9 Batch  228 / 261  Training Loss  0.0004644996370188892\n",
            "Epoch  9 Batch  229 / 261  Training Loss  0.0004191958869341761\n",
            "Epoch  9 Batch  230 / 261  Training Loss  0.0006823890144005418\n",
            "Epoch  9 Batch  231 / 261  Training Loss  0.00115200353320688\n",
            "Epoch  9 Batch  232 / 261  Training Loss  0.0020149066112935543\n",
            "Epoch  9 Batch  233 / 261  Training Loss  0.0013491840800270438\n",
            "Epoch  9 Batch  234 / 261  Training Loss  0.005567196756601334\n",
            "Epoch  9 Batch  235 / 261  Training Loss  0.0007739776629023254\n",
            "Epoch  9 Batch  236 / 261  Training Loss  0.00042267944081686437\n",
            "Epoch  9 Batch  237 / 261  Training Loss  0.0004904732340946794\n",
            "Epoch  9 Batch  238 / 261  Training Loss  0.0003013253153767437\n",
            "Epoch  9 Batch  239 / 261  Training Loss  0.0009500202722847462\n",
            "Epoch  9 Batch  240 / 261  Training Loss  0.0006926285568624735\n",
            "Epoch  9 Batch  241 / 261  Training Loss  0.0022378633730113506\n",
            "Epoch  9 Batch  242 / 261  Training Loss  0.00926976278424263\n",
            "Epoch  9 Batch  243 / 261  Training Loss  0.0009322515106759965\n",
            "Epoch  9 Batch  244 / 261  Training Loss  0.0003608950355555862\n",
            "Epoch  9 Batch  245 / 261  Training Loss  0.0005958881229162216\n",
            "Epoch  9 Batch  246 / 261  Training Loss  0.0012753286864608526\n",
            "Epoch  9 Batch  247 / 261  Training Loss  0.0004642907588277012\n",
            "Epoch  9 Batch  248 / 261  Training Loss  0.0007099835784174502\n",
            "Epoch  9 Batch  249 / 261  Training Loss  0.00029993796488270164\n",
            "Epoch  9 Batch  250 / 261  Training Loss  0.0005235280259512365\n",
            "Epoch  9 Batch  251 / 261  Training Loss  0.0005094294901937246\n",
            "Epoch  9 Batch  252 / 261  Training Loss  0.0005595951224677265\n",
            "Epoch  9 Batch  253 / 261  Training Loss  0.000367856293451041\n",
            "Epoch  9 Batch  254 / 261  Training Loss  0.0009449286735616624\n",
            "Epoch  9 Batch  255 / 261  Training Loss  0.0008716955198906362\n",
            "Epoch  9 Batch  256 / 261  Training Loss  0.0006038222345523536\n",
            "Epoch  9 Batch  257 / 261  Training Loss  0.0002741820062510669\n",
            "Epoch  9 Batch  258 / 261  Training Loss  0.0005915291840210557\n",
            "Epoch  9 Batch  259 / 261  Training Loss  0.0009317921940237284\n",
            "Epoch  9 Batch  260 / 261  Training Loss  0.0006319714593701065\n",
            "  10    |    -    |   0.001838   | 99.301862\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 10\n",
            "Epoch  10 Batch  0 / 261  Training Loss  0.00019870178948622197\n",
            "Epoch  10 Batch  1 / 261  Training Loss  0.0003860725846607238\n",
            "Epoch  10 Batch  2 / 261  Training Loss  0.0014756072778254747\n",
            "Epoch  10 Batch  3 / 261  Training Loss  0.0005564088351093233\n",
            "Epoch  10 Batch  4 / 261  Training Loss  0.00034033108386211097\n",
            "Epoch  10 Batch  5 / 261  Training Loss  0.0006791034829802811\n",
            "Epoch  10 Batch  6 / 261  Training Loss  0.0009141761111095548\n",
            "Epoch  10 Batch  7 / 261  Training Loss  0.0003939209273084998\n",
            "Epoch  10 Batch  8 / 261  Training Loss  0.000463476637378335\n",
            "Epoch  10 Batch  9 / 261  Training Loss  0.0002975323877763003\n",
            "Epoch  10 Batch  10 / 261  Training Loss  0.000249282835284248\n",
            "Epoch  10 Batch  11 / 261  Training Loss  0.00021510724036488682\n",
            "Epoch  10 Batch  12 / 261  Training Loss  0.0003164302615914494\n",
            "Epoch  10 Batch  13 / 261  Training Loss  0.00024941490846686065\n",
            "Epoch  10 Batch  14 / 261  Training Loss  0.0002635204291436821\n",
            "Epoch  10 Batch  15 / 261  Training Loss  0.0003057204303331673\n",
            "Epoch  10 Batch  16 / 261  Training Loss  0.0012267224956303835\n",
            "Epoch  10 Batch  17 / 261  Training Loss  0.00028704540454782546\n",
            "Epoch  10 Batch  18 / 261  Training Loss  0.0003458231803961098\n",
            "Epoch  10 Batch  19 / 261  Training Loss  0.00045123364543542266\n",
            "Epoch  10 Batch  20 / 261  Training Loss  0.00028632301837205887\n",
            "Epoch  10 Batch  21 / 261  Training Loss  0.0005861925892531872\n",
            "Epoch  10 Batch  22 / 261  Training Loss  0.00034745849552564323\n",
            "Epoch  10 Batch  23 / 261  Training Loss  0.00028386327903717756\n",
            "Epoch  10 Batch  24 / 261  Training Loss  0.0002661488833837211\n",
            "Epoch  10 Batch  25 / 261  Training Loss  0.00022634789638686925\n",
            "Epoch  10 Batch  26 / 261  Training Loss  0.00034435783163644373\n",
            "Epoch  10 Batch  27 / 261  Training Loss  0.000298027676763013\n",
            "Epoch  10 Batch  28 / 261  Training Loss  0.00021674086747225374\n",
            "Epoch  10 Batch  29 / 261  Training Loss  0.0002986093459185213\n",
            "Epoch  10 Batch  30 / 261  Training Loss  0.000283860630588606\n",
            "Epoch  10 Batch  31 / 261  Training Loss  0.00030396939837373793\n",
            "Epoch  10 Batch  32 / 261  Training Loss  0.0002640487509779632\n",
            "Epoch  10 Batch  33 / 261  Training Loss  0.00023720324679743499\n",
            "Epoch  10 Batch  34 / 261  Training Loss  0.0006905259797349572\n",
            "Epoch  10 Batch  35 / 261  Training Loss  0.00021281153021845967\n",
            "Epoch  10 Batch  36 / 261  Training Loss  0.0002614747209008783\n",
            "Epoch  10 Batch  37 / 261  Training Loss  0.0003003627934958786\n",
            "Epoch  10 Batch  38 / 261  Training Loss  0.0002632235991768539\n",
            "Epoch  10 Batch  39 / 261  Training Loss  0.0005313282599672675\n",
            "Epoch  10 Batch  40 / 261  Training Loss  0.00029805241501890123\n",
            "Epoch  10 Batch  41 / 261  Training Loss  0.00019015822908841074\n",
            "Epoch  10 Batch  42 / 261  Training Loss  0.0001768909569364041\n",
            "Epoch  10 Batch  43 / 261  Training Loss  0.00018763708067126572\n",
            "Epoch  10 Batch  44 / 261  Training Loss  0.00022029616229701787\n",
            "Epoch  10 Batch  45 / 261  Training Loss  0.00024507424677722156\n",
            "Epoch  10 Batch  46 / 261  Training Loss  0.000279729429166764\n",
            "Epoch  10 Batch  47 / 261  Training Loss  0.00014671772078145295\n",
            "Epoch  10 Batch  48 / 261  Training Loss  0.00033777207136154175\n",
            "Epoch  10 Batch  49 / 261  Training Loss  0.0002221022587036714\n",
            "Epoch  10 Batch  50 / 261  Training Loss  0.00019844748021569103\n",
            "Epoch  10 Batch  51 / 261  Training Loss  0.00017339996702503413\n",
            "Epoch  10 Batch  52 / 261  Training Loss  0.00023864521062932909\n",
            "Epoch  10 Batch  53 / 261  Training Loss  0.0002117904950864613\n",
            "Epoch  10 Batch  54 / 261  Training Loss  0.00024910218780860305\n",
            "Epoch  10 Batch  55 / 261  Training Loss  0.00040385161992162466\n",
            "Epoch  10 Batch  56 / 261  Training Loss  0.00023780569608788937\n",
            "Epoch  10 Batch  57 / 261  Training Loss  0.00021308234136085957\n",
            "Epoch  10 Batch  58 / 261  Training Loss  0.0003789500624407083\n",
            "Epoch  10 Batch  59 / 261  Training Loss  0.0006765518337488174\n",
            "Epoch  10 Batch  60 / 261  Training Loss  0.00023650206276215613\n",
            "Epoch  10 Batch  61 / 261  Training Loss  0.0002512579085305333\n",
            "Epoch  10 Batch  62 / 261  Training Loss  0.00021832015772815794\n",
            "Epoch  10 Batch  63 / 261  Training Loss  0.00020693628175649792\n",
            "Epoch  10 Batch  64 / 261  Training Loss  0.00021511194063350558\n",
            "Epoch  10 Batch  65 / 261  Training Loss  0.00018573107081465423\n",
            "Epoch  10 Batch  66 / 261  Training Loss  0.00019303342560306191\n",
            "Epoch  10 Batch  67 / 261  Training Loss  0.000287120696157217\n",
            "Epoch  10 Batch  68 / 261  Training Loss  0.0009212680743075907\n",
            "Epoch  10 Batch  69 / 261  Training Loss  0.00020889454754069448\n",
            "Epoch  10 Batch  70 / 261  Training Loss  0.0002787166740745306\n",
            "Epoch  10 Batch  71 / 261  Training Loss  0.00026475207414478064\n",
            "Epoch  10 Batch  72 / 261  Training Loss  0.00021612831915263087\n",
            "Epoch  10 Batch  73 / 261  Training Loss  0.00020726845832541585\n",
            "Epoch  10 Batch  74 / 261  Training Loss  0.00026123292627744377\n",
            "Epoch  10 Batch  75 / 261  Training Loss  0.00016489552217535675\n",
            "Epoch  10 Batch  76 / 261  Training Loss  0.00025734902010299265\n",
            "Epoch  10 Batch  77 / 261  Training Loss  0.0001815427967812866\n",
            "Epoch  10 Batch  78 / 261  Training Loss  0.00019195482309442014\n",
            "Epoch  10 Batch  79 / 261  Training Loss  0.00022207973233889788\n",
            "Epoch  10 Batch  80 / 261  Training Loss  0.00018055136024486274\n",
            "Epoch  10 Batch  81 / 261  Training Loss  0.00024130233214236796\n",
            "Epoch  10 Batch  82 / 261  Training Loss  0.0001773256663000211\n",
            "Epoch  10 Batch  83 / 261  Training Loss  0.00024758154177106917\n",
            "Epoch  10 Batch  84 / 261  Training Loss  0.0006359118269756436\n",
            "Epoch  10 Batch  85 / 261  Training Loss  0.0002446900762151927\n",
            "Epoch  10 Batch  86 / 261  Training Loss  0.00019846615032292902\n",
            "Epoch  10 Batch  87 / 261  Training Loss  0.0011628977954387665\n",
            "Epoch  10 Batch  88 / 261  Training Loss  0.007199867162853479\n",
            "Epoch  10 Batch  89 / 261  Training Loss  0.000219610970816575\n",
            "Epoch  10 Batch  90 / 261  Training Loss  0.00020157132530584931\n",
            "Epoch  10 Batch  91 / 261  Training Loss  0.0002883028064388782\n",
            "Epoch  10 Batch  92 / 261  Training Loss  0.00025136469048447907\n",
            "Epoch  10 Batch  93 / 261  Training Loss  0.0003038399154320359\n",
            "Epoch  10 Batch  94 / 261  Training Loss  0.0002535370003897697\n",
            "Epoch  10 Batch  95 / 261  Training Loss  0.00021030842617619783\n",
            "Epoch  10 Batch  96 / 261  Training Loss  0.00018230221758130938\n",
            "Epoch  10 Batch  97 / 261  Training Loss  0.0001918222551466897\n",
            "Epoch  10 Batch  98 / 261  Training Loss  0.00018890720093622804\n",
            "Epoch  10 Batch  99 / 261  Training Loss  0.0002740788331720978\n",
            "Epoch  10 Batch  100 / 261  Training Loss  0.0028032546397298574\n",
            "Epoch  10 Batch  101 / 261  Training Loss  0.0077050235122442245\n",
            "Epoch  10 Batch  102 / 261  Training Loss  0.0018417858518660069\n",
            "Epoch  10 Batch  103 / 261  Training Loss  0.015942959114909172\n",
            "Epoch  10 Batch  104 / 261  Training Loss  0.0006444985629059374\n",
            "Epoch  10 Batch  105 / 261  Training Loss  0.022321516647934914\n",
            "Epoch  10 Batch  106 / 261  Training Loss  0.00039231579285115004\n",
            "Epoch  10 Batch  107 / 261  Training Loss  0.0012840969720855355\n",
            "Epoch  10 Batch  108 / 261  Training Loss  0.002541878493502736\n",
            "Epoch  10 Batch  109 / 261  Training Loss  0.0002690984692890197\n",
            "Epoch  10 Batch  110 / 261  Training Loss  0.00026814360171556473\n",
            "Epoch  10 Batch  111 / 261  Training Loss  0.00023320494801737368\n",
            "Epoch  10 Batch  112 / 261  Training Loss  0.00033098753192462027\n",
            "Epoch  10 Batch  113 / 261  Training Loss  0.0003096079744864255\n",
            "Epoch  10 Batch  114 / 261  Training Loss  0.00032947128056548536\n",
            "Epoch  10 Batch  115 / 261  Training Loss  0.00022213274496607482\n",
            "Epoch  10 Batch  116 / 261  Training Loss  0.0003820202546194196\n",
            "Epoch  10 Batch  117 / 261  Training Loss  0.00021817417291458696\n",
            "Epoch  10 Batch  118 / 261  Training Loss  0.0006228266283869743\n",
            "Epoch  10 Batch  119 / 261  Training Loss  0.0008913989295251667\n",
            "Epoch  10 Batch  120 / 261  Training Loss  0.0008525294833816588\n",
            "Epoch  10 Batch  121 / 261  Training Loss  0.0038238167762756348\n",
            "Epoch  10 Batch  122 / 261  Training Loss  0.000399578595533967\n",
            "Epoch  10 Batch  123 / 261  Training Loss  0.003155091078951955\n",
            "Epoch  10 Batch  124 / 261  Training Loss  0.003383416449651122\n",
            "Epoch  10 Batch  125 / 261  Training Loss  0.00023424284881912172\n",
            "Epoch  10 Batch  126 / 261  Training Loss  0.0001856887392932549\n",
            "Epoch  10 Batch  127 / 261  Training Loss  0.0013065203092992306\n",
            "Epoch  10 Batch  128 / 261  Training Loss  0.00022612664906773716\n",
            "Epoch  10 Batch  129 / 261  Training Loss  0.0005643857293762267\n",
            "Epoch  10 Batch  130 / 261  Training Loss  0.00021442929573822767\n",
            "Epoch  10 Batch  131 / 261  Training Loss  0.00031821735319681466\n",
            "Epoch  10 Batch  132 / 261  Training Loss  0.00024015123199205846\n",
            "Epoch  10 Batch  133 / 261  Training Loss  0.0003874169779010117\n",
            "Epoch  10 Batch  134 / 261  Training Loss  0.00016958288324531168\n",
            "Epoch  10 Batch  135 / 261  Training Loss  0.0005887093138881028\n",
            "Epoch  10 Batch  136 / 261  Training Loss  0.0005487771122716367\n",
            "Epoch  10 Batch  137 / 261  Training Loss  0.00026259207515977323\n",
            "Epoch  10 Batch  138 / 261  Training Loss  0.0005824202089570463\n",
            "Epoch  10 Batch  139 / 261  Training Loss  0.00022517198522109538\n",
            "Epoch  10 Batch  140 / 261  Training Loss  0.0008783508674241602\n",
            "Epoch  10 Batch  141 / 261  Training Loss  0.0002524165320210159\n",
            "Epoch  10 Batch  142 / 261  Training Loss  0.0002670625108294189\n",
            "Epoch  10 Batch  143 / 261  Training Loss  0.00019586643611546606\n",
            "Epoch  10 Batch  144 / 261  Training Loss  0.00019586093549150974\n",
            "Epoch  10 Batch  145 / 261  Training Loss  0.0005991504876874387\n",
            "Epoch  10 Batch  146 / 261  Training Loss  0.0002516763343010098\n",
            "Epoch  10 Batch  147 / 261  Training Loss  0.00015867681941017509\n",
            "Epoch  10 Batch  148 / 261  Training Loss  0.0002090336784021929\n",
            "Epoch  10 Batch  149 / 261  Training Loss  0.0002838244254235178\n",
            "Epoch  10 Batch  150 / 261  Training Loss  0.0005559625569730997\n",
            "Epoch  10 Batch  151 / 261  Training Loss  0.00032563458080403507\n",
            "Epoch  10 Batch  152 / 261  Training Loss  0.00026387753314338624\n",
            "Epoch  10 Batch  153 / 261  Training Loss  0.0004280883586034179\n",
            "Epoch  10 Batch  154 / 261  Training Loss  0.00019625767890829593\n",
            "Epoch  10 Batch  155 / 261  Training Loss  0.0002503447176422924\n",
            "Epoch  10 Batch  156 / 261  Training Loss  0.00046544766519218683\n",
            "Epoch  10 Batch  157 / 261  Training Loss  0.00015971448738127947\n",
            "Epoch  10 Batch  158 / 261  Training Loss  0.00020011633750982583\n",
            "Epoch  10 Batch  159 / 261  Training Loss  0.0002013687917497009\n",
            "Epoch  10 Batch  160 / 261  Training Loss  0.0005224106716923416\n",
            "Epoch  10 Batch  161 / 261  Training Loss  0.0003106560616288334\n",
            "Epoch  10 Batch  162 / 261  Training Loss  0.00025683012790977955\n",
            "Epoch  10 Batch  163 / 261  Training Loss  0.00022344237368088216\n",
            "Epoch  10 Batch  164 / 261  Training Loss  0.00018856572569347918\n",
            "Epoch  10 Batch  165 / 261  Training Loss  0.00021300619118846953\n",
            "Epoch  10 Batch  166 / 261  Training Loss  0.0004271113430149853\n",
            "Epoch  10 Batch  167 / 261  Training Loss  0.0002195574197685346\n",
            "Epoch  10 Batch  168 / 261  Training Loss  0.00027380840037949383\n",
            "Epoch  10 Batch  169 / 261  Training Loss  0.00016299608978442848\n",
            "Epoch  10 Batch  170 / 261  Training Loss  0.0001611024490557611\n",
            "Epoch  10 Batch  171 / 261  Training Loss  0.00028676894726231694\n",
            "Epoch  10 Batch  172 / 261  Training Loss  0.0002346465043956414\n",
            "Epoch  10 Batch  173 / 261  Training Loss  0.0004962382954545319\n",
            "Epoch  10 Batch  174 / 261  Training Loss  0.00019341122242622077\n",
            "Epoch  10 Batch  175 / 261  Training Loss  0.0002254574792459607\n",
            "Epoch  10 Batch  176 / 261  Training Loss  0.0002272473502671346\n",
            "Epoch  10 Batch  177 / 261  Training Loss  0.001136850449256599\n",
            "Epoch  10 Batch  178 / 261  Training Loss  0.00029712144169025123\n",
            "Epoch  10 Batch  179 / 261  Training Loss  0.00021038699196651578\n",
            "Epoch  10 Batch  180 / 261  Training Loss  0.000293393328320235\n",
            "Epoch  10 Batch  181 / 261  Training Loss  0.00017355081217829138\n",
            "Epoch  10 Batch  182 / 261  Training Loss  0.0002880029787775129\n",
            "Epoch  10 Batch  183 / 261  Training Loss  0.000566048373002559\n",
            "Epoch  10 Batch  184 / 261  Training Loss  0.00020226446213200688\n",
            "Epoch  10 Batch  185 / 261  Training Loss  0.0008537682006135583\n",
            "Epoch  10 Batch  186 / 261  Training Loss  0.000645628955680877\n",
            "Epoch  10 Batch  187 / 261  Training Loss  0.0006542981136590242\n",
            "Epoch  10 Batch  188 / 261  Training Loss  0.00019676488591358066\n",
            "Epoch  10 Batch  189 / 261  Training Loss  0.0003160856431350112\n",
            "Epoch  10 Batch  190 / 261  Training Loss  0.00016445659275632352\n",
            "Epoch  10 Batch  191 / 261  Training Loss  0.0002522116992622614\n",
            "Epoch  10 Batch  192 / 261  Training Loss  0.0001919484930112958\n",
            "Epoch  10 Batch  193 / 261  Training Loss  0.0002024569985223934\n",
            "Epoch  10 Batch  194 / 261  Training Loss  0.00012557476293295622\n",
            "Epoch  10 Batch  195 / 261  Training Loss  0.00019187839643564075\n",
            "Epoch  10 Batch  196 / 261  Training Loss  0.0001947862037923187\n",
            "Epoch  10 Batch  197 / 261  Training Loss  0.0002671473193913698\n",
            "Epoch  10 Batch  198 / 261  Training Loss  0.002053602831438184\n",
            "Epoch  10 Batch  199 / 261  Training Loss  0.0007247307221405208\n",
            "Epoch  10 Batch  200 / 261  Training Loss  0.0008094037184491754\n",
            "Epoch  10 Batch  201 / 261  Training Loss  0.00019193865591660142\n",
            "Epoch  10 Batch  202 / 261  Training Loss  0.0003524695639498532\n",
            "Epoch  10 Batch  203 / 261  Training Loss  0.0004630784678738564\n",
            "Epoch  10 Batch  204 / 261  Training Loss  0.00023749069077894092\n",
            "Epoch  10 Batch  205 / 261  Training Loss  0.00019630990573205054\n",
            "Epoch  10 Batch  206 / 261  Training Loss  0.00024845238658599555\n",
            "Epoch  10 Batch  207 / 261  Training Loss  0.00014349393313750625\n",
            "Epoch  10 Batch  208 / 261  Training Loss  0.00022187037393450737\n",
            "Epoch  10 Batch  209 / 261  Training Loss  0.00023423659149557352\n",
            "Epoch  10 Batch  210 / 261  Training Loss  0.00028513334109447896\n",
            "Epoch  10 Batch  211 / 261  Training Loss  0.0004180930263828486\n",
            "Epoch  10 Batch  212 / 261  Training Loss  0.00014632270904257894\n",
            "Epoch  10 Batch  213 / 261  Training Loss  0.00022114708553999662\n",
            "Epoch  10 Batch  214 / 261  Training Loss  0.0001930754806380719\n",
            "Epoch  10 Batch  215 / 261  Training Loss  0.00018930603982880712\n",
            "Epoch  10 Batch  216 / 261  Training Loss  0.00020722599583677948\n",
            "Epoch  10 Batch  217 / 261  Training Loss  0.00022830642410553992\n",
            "Epoch  10 Batch  218 / 261  Training Loss  0.00015408385661430657\n",
            "Epoch  10 Batch  219 / 261  Training Loss  0.00016951588622760028\n",
            "Epoch  10 Batch  220 / 261  Training Loss  0.00012562442861963063\n",
            "Epoch  10 Batch  221 / 261  Training Loss  0.0001319422444794327\n",
            "Epoch  10 Batch  222 / 261  Training Loss  0.00010952755837934092\n",
            "Epoch  10 Batch  223 / 261  Training Loss  0.00011257759615546092\n",
            "Epoch  10 Batch  224 / 261  Training Loss  0.00020308019884396344\n",
            "Epoch  10 Batch  225 / 261  Training Loss  0.00035010979627259076\n",
            "Epoch  10 Batch  226 / 261  Training Loss  0.00015187948883976787\n",
            "Epoch  10 Batch  227 / 261  Training Loss  0.0001546597486594692\n",
            "Epoch  10 Batch  228 / 261  Training Loss  0.00013344413309823722\n",
            "Epoch  10 Batch  229 / 261  Training Loss  0.00014347299293149263\n",
            "Epoch  10 Batch  230 / 261  Training Loss  0.0004901160136796534\n",
            "Epoch  10 Batch  231 / 261  Training Loss  0.00027625280199572444\n",
            "Epoch  10 Batch  232 / 261  Training Loss  0.00015390214684884995\n",
            "Epoch  10 Batch  233 / 261  Training Loss  0.00019407602667342871\n",
            "Epoch  10 Batch  234 / 261  Training Loss  0.00010201761324424297\n",
            "Epoch  10 Batch  235 / 261  Training Loss  0.0008934797951951623\n",
            "Epoch  10 Batch  236 / 261  Training Loss  0.00012537585280369967\n",
            "Epoch  10 Batch  237 / 261  Training Loss  0.00012525025522336364\n",
            "Epoch  10 Batch  238 / 261  Training Loss  0.011760026216506958\n",
            "Epoch  10 Batch  239 / 261  Training Loss  0.024930298328399658\n",
            "Epoch  10 Batch  240 / 261  Training Loss  0.0025838816072791815\n",
            "Epoch  10 Batch  241 / 261  Training Loss  0.0013274523662403226\n",
            "Epoch  10 Batch  242 / 261  Training Loss  0.0033995446283370256\n",
            "Epoch  10 Batch  243 / 261  Training Loss  0.0008641699096187949\n",
            "Epoch  10 Batch  244 / 261  Training Loss  0.0009283279650844634\n",
            "Epoch  10 Batch  245 / 261  Training Loss  0.0017787415999919176\n",
            "Epoch  10 Batch  246 / 261  Training Loss  0.00042961869621649384\n",
            "Epoch  10 Batch  247 / 261  Training Loss  0.00038716220296919346\n",
            "Epoch  10 Batch  248 / 261  Training Loss  0.00029979724786244333\n",
            "Epoch  10 Batch  249 / 261  Training Loss  0.007079754490405321\n",
            "Epoch  10 Batch  250 / 261  Training Loss  0.0017464515985921025\n",
            "Epoch  10 Batch  251 / 261  Training Loss  0.0003037775168195367\n",
            "Epoch  10 Batch  252 / 261  Training Loss  0.001135701430030167\n",
            "Epoch  10 Batch  253 / 261  Training Loss  0.0039270236156880856\n",
            "Epoch  10 Batch  254 / 261  Training Loss  0.006191466003656387\n",
            "Epoch  10 Batch  255 / 261  Training Loss  0.015421905554831028\n",
            "Epoch  10 Batch  256 / 261  Training Loss  0.00868102815002203\n",
            "Epoch  10 Batch  257 / 261  Training Loss  0.014902987517416477\n",
            "Epoch  10 Batch  258 / 261  Training Loss  0.0011323151411488652\n",
            "Epoch  10 Batch  259 / 261  Training Loss  0.0006285160197876394\n",
            "Epoch  10 Batch  260 / 261  Training Loss  0.006899949628859758\n",
            "  11    |    -    |   0.001011   | 91.023936\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 11\n",
            "Epoch  11 Batch  0 / 261  Training Loss  0.010761311277747154\n",
            "Epoch  11 Batch  1 / 261  Training Loss  0.0029280048329383135\n",
            "Epoch  11 Batch  2 / 261  Training Loss  0.0012400512350723147\n",
            "Epoch  11 Batch  3 / 261  Training Loss  0.003536938689649105\n",
            "Epoch  11 Batch  4 / 261  Training Loss  0.0014900651294738054\n",
            "Epoch  11 Batch  5 / 261  Training Loss  0.0007400555768981576\n",
            "Epoch  11 Batch  6 / 261  Training Loss  0.0006377412937581539\n",
            "Epoch  11 Batch  7 / 261  Training Loss  0.0007802272448316216\n",
            "Epoch  11 Batch  8 / 261  Training Loss  0.0008264344651252031\n",
            "Epoch  11 Batch  9 / 261  Training Loss  0.0008878886001184583\n",
            "Epoch  11 Batch  10 / 261  Training Loss  0.0007540556835010648\n",
            "Epoch  11 Batch  11 / 261  Training Loss  0.002017255639657378\n",
            "Epoch  11 Batch  12 / 261  Training Loss  0.006387445144355297\n",
            "Epoch  11 Batch  13 / 261  Training Loss  0.0023728052619844675\n",
            "Epoch  11 Batch  14 / 261  Training Loss  0.0013076908653602004\n",
            "Epoch  11 Batch  15 / 261  Training Loss  0.0009921425953507423\n",
            "Epoch  11 Batch  16 / 261  Training Loss  0.0010252606589347124\n",
            "Epoch  11 Batch  17 / 261  Training Loss  0.0004191455664113164\n",
            "Epoch  11 Batch  18 / 261  Training Loss  0.0010081040672957897\n",
            "Epoch  11 Batch  19 / 261  Training Loss  0.000389714608900249\n",
            "Epoch  11 Batch  20 / 261  Training Loss  0.0010226317681372166\n",
            "Epoch  11 Batch  21 / 261  Training Loss  0.0008313233265653253\n",
            "Epoch  11 Batch  22 / 261  Training Loss  0.00033772457391023636\n",
            "Epoch  11 Batch  23 / 261  Training Loss  0.0009105593198910356\n",
            "Epoch  11 Batch  24 / 261  Training Loss  0.0020632476080209017\n",
            "Epoch  11 Batch  25 / 261  Training Loss  0.0004225975717417896\n",
            "Epoch  11 Batch  26 / 261  Training Loss  0.0012317958753556013\n",
            "Epoch  11 Batch  27 / 261  Training Loss  0.00032933286274783313\n",
            "Epoch  11 Batch  28 / 261  Training Loss  0.0016205002320930362\n",
            "Epoch  11 Batch  29 / 261  Training Loss  0.0007625767611898482\n",
            "Epoch  11 Batch  30 / 261  Training Loss  0.0038599190302193165\n",
            "Epoch  11 Batch  31 / 261  Training Loss  0.0002954310330096632\n",
            "Epoch  11 Batch  32 / 261  Training Loss  0.0021764389239251614\n",
            "Epoch  11 Batch  33 / 261  Training Loss  0.0007496707257814705\n",
            "Epoch  11 Batch  34 / 261  Training Loss  0.000602210289798677\n",
            "Epoch  11 Batch  35 / 261  Training Loss  0.0010208355961367488\n",
            "Epoch  11 Batch  36 / 261  Training Loss  0.0009878261480480433\n",
            "Epoch  11 Batch  37 / 261  Training Loss  0.001269414322450757\n",
            "Epoch  11 Batch  38 / 261  Training Loss  0.0018231933936476707\n",
            "Epoch  11 Batch  39 / 261  Training Loss  0.00500333309173584\n",
            "Epoch  11 Batch  40 / 261  Training Loss  0.013137643225491047\n",
            "Epoch  11 Batch  41 / 261  Training Loss  0.001482084859162569\n",
            "Epoch  11 Batch  42 / 261  Training Loss  0.0015152905834838748\n",
            "Epoch  11 Batch  43 / 261  Training Loss  0.003311824519187212\n",
            "Epoch  11 Batch  44 / 261  Training Loss  0.0006993024726398289\n",
            "Epoch  11 Batch  45 / 261  Training Loss  0.011969316750764847\n",
            "Epoch  11 Batch  46 / 261  Training Loss  0.0008168749045580626\n",
            "Epoch  11 Batch  47 / 261  Training Loss  0.00040371212526224554\n",
            "Epoch  11 Batch  48 / 261  Training Loss  0.005058613605797291\n",
            "Epoch  11 Batch  49 / 261  Training Loss  0.0010087111731991172\n",
            "Epoch  11 Batch  50 / 261  Training Loss  0.0029136878438293934\n",
            "Epoch  11 Batch  51 / 261  Training Loss  0.010137306526303291\n",
            "Epoch  11 Batch  52 / 261  Training Loss  0.0041513326577842236\n",
            "Epoch  11 Batch  53 / 261  Training Loss  0.0024285712279379368\n",
            "Epoch  11 Batch  54 / 261  Training Loss  0.001247971085831523\n",
            "Epoch  11 Batch  55 / 261  Training Loss  0.0019000819884240627\n",
            "Epoch  11 Batch  56 / 261  Training Loss  0.0005569512723013759\n",
            "Epoch  11 Batch  57 / 261  Training Loss  0.0009176328894682229\n",
            "Epoch  11 Batch  58 / 261  Training Loss  0.00025875086430460215\n",
            "Epoch  11 Batch  59 / 261  Training Loss  0.001434182282537222\n",
            "Epoch  11 Batch  60 / 261  Training Loss  0.0005373039748519659\n",
            "Epoch  11 Batch  61 / 261  Training Loss  0.0004619112587533891\n",
            "Epoch  11 Batch  62 / 261  Training Loss  0.00478105153888464\n",
            "Epoch  11 Batch  63 / 261  Training Loss  0.0011677169241011143\n",
            "Epoch  11 Batch  64 / 261  Training Loss  0.0004798669833689928\n",
            "Epoch  11 Batch  65 / 261  Training Loss  0.0003988543467130512\n",
            "Epoch  11 Batch  66 / 261  Training Loss  0.0004264919843990356\n",
            "Epoch  11 Batch  67 / 261  Training Loss  0.00033250433625653386\n",
            "Epoch  11 Batch  68 / 261  Training Loss  0.0002725134545471519\n",
            "Epoch  11 Batch  69 / 261  Training Loss  0.0007536893244832754\n",
            "Epoch  11 Batch  70 / 261  Training Loss  0.006053304299712181\n",
            "Epoch  11 Batch  71 / 261  Training Loss  0.0003343256830703467\n",
            "Epoch  11 Batch  72 / 261  Training Loss  0.0004265101160854101\n",
            "Epoch  11 Batch  73 / 261  Training Loss  0.0005159370484761894\n",
            "Epoch  11 Batch  74 / 261  Training Loss  0.00047259367420338094\n",
            "Epoch  11 Batch  75 / 261  Training Loss  0.0003904955228790641\n",
            "Epoch  11 Batch  76 / 261  Training Loss  0.00038044535904191434\n",
            "Epoch  11 Batch  77 / 261  Training Loss  0.0002958816767204553\n",
            "Epoch  11 Batch  78 / 261  Training Loss  0.0002237236622022465\n",
            "Epoch  11 Batch  79 / 261  Training Loss  0.000698701711371541\n",
            "Epoch  11 Batch  80 / 261  Training Loss  0.0005890437169000506\n",
            "Epoch  11 Batch  81 / 261  Training Loss  0.0006023947498761117\n",
            "Epoch  11 Batch  82 / 261  Training Loss  0.00029354143771342933\n",
            "Epoch  11 Batch  83 / 261  Training Loss  0.00048587960191071033\n",
            "Epoch  11 Batch  84 / 261  Training Loss  0.00023457786301150918\n",
            "Epoch  11 Batch  85 / 261  Training Loss  0.00045793692697770894\n",
            "Epoch  11 Batch  86 / 261  Training Loss  0.000277401995845139\n",
            "Epoch  11 Batch  87 / 261  Training Loss  0.0003515637945383787\n",
            "Epoch  11 Batch  88 / 261  Training Loss  0.00044207501923665404\n",
            "Epoch  11 Batch  89 / 261  Training Loss  0.0004997100331820548\n",
            "Epoch  11 Batch  90 / 261  Training Loss  0.00025548075791448355\n",
            "Epoch  11 Batch  91 / 261  Training Loss  0.0002693370042834431\n",
            "Epoch  11 Batch  92 / 261  Training Loss  0.00027075770776718855\n",
            "Epoch  11 Batch  93 / 261  Training Loss  0.0006488821818493307\n",
            "Epoch  11 Batch  94 / 261  Training Loss  0.0001587686565471813\n",
            "Epoch  11 Batch  95 / 261  Training Loss  0.0005264890496619046\n",
            "Epoch  11 Batch  96 / 261  Training Loss  0.00023534375941380858\n",
            "Epoch  11 Batch  97 / 261  Training Loss  0.00022275833180174232\n",
            "Epoch  11 Batch  98 / 261  Training Loss  0.0005430823075585067\n",
            "Epoch  11 Batch  99 / 261  Training Loss  0.000175083740032278\n",
            "Epoch  11 Batch  100 / 261  Training Loss  0.0002484350698068738\n",
            "Epoch  11 Batch  101 / 261  Training Loss  0.0002852351462934166\n",
            "Epoch  11 Batch  102 / 261  Training Loss  0.0002828335855156183\n",
            "Epoch  11 Batch  103 / 261  Training Loss  0.0001957815111381933\n",
            "Epoch  11 Batch  104 / 261  Training Loss  0.000567120558116585\n",
            "Epoch  11 Batch  105 / 261  Training Loss  0.00026199533022008836\n",
            "Epoch  11 Batch  106 / 261  Training Loss  0.0005398595239967108\n",
            "Epoch  11 Batch  107 / 261  Training Loss  0.0001618373062228784\n",
            "Epoch  11 Batch  108 / 261  Training Loss  0.0003255125193390995\n",
            "Epoch  11 Batch  109 / 261  Training Loss  0.0006050422671250999\n",
            "Epoch  11 Batch  110 / 261  Training Loss  0.00018859314150176942\n",
            "Epoch  11 Batch  111 / 261  Training Loss  0.00017977556854020804\n",
            "Epoch  11 Batch  112 / 261  Training Loss  0.00020336674060672522\n",
            "Epoch  11 Batch  113 / 261  Training Loss  0.004144163336604834\n",
            "Epoch  11 Batch  114 / 261  Training Loss  0.01543758250772953\n",
            "Epoch  11 Batch  115 / 261  Training Loss  0.05171341076493263\n",
            "Epoch  11 Batch  116 / 261  Training Loss  0.02720647305250168\n",
            "Epoch  11 Batch  117 / 261  Training Loss  0.03797569125890732\n",
            "Epoch  11 Batch  118 / 261  Training Loss  0.01225267443805933\n",
            "Epoch  11 Batch  119 / 261  Training Loss  0.019475283101201057\n",
            "Epoch  11 Batch  120 / 261  Training Loss  0.012437598779797554\n",
            "Epoch  11 Batch  121 / 261  Training Loss  0.016557157039642334\n",
            "Epoch  11 Batch  122 / 261  Training Loss  0.015070546418428421\n",
            "Epoch  11 Batch  123 / 261  Training Loss  0.005462998058646917\n",
            "Epoch  11 Batch  124 / 261  Training Loss  0.0028025286737829447\n",
            "Epoch  11 Batch  125 / 261  Training Loss  0.006710411980748177\n",
            "Epoch  11 Batch  126 / 261  Training Loss  0.0043737804517149925\n",
            "Epoch  11 Batch  127 / 261  Training Loss  0.0015714592300355434\n",
            "Epoch  11 Batch  128 / 261  Training Loss  0.0012160491896793246\n",
            "Epoch  11 Batch  129 / 261  Training Loss  0.011735689826309681\n",
            "Epoch  11 Batch  130 / 261  Training Loss  0.028937704861164093\n",
            "Epoch  11 Batch  131 / 261  Training Loss  0.004567645024508238\n",
            "Epoch  11 Batch  132 / 261  Training Loss  0.0022886733058840036\n",
            "Epoch  11 Batch  133 / 261  Training Loss  0.000995898968540132\n",
            "Epoch  11 Batch  134 / 261  Training Loss  0.0031292506027966738\n",
            "Epoch  11 Batch  135 / 261  Training Loss  0.003712352132424712\n",
            "Epoch  11 Batch  136 / 261  Training Loss  0.0019032200798392296\n",
            "Epoch  11 Batch  137 / 261  Training Loss  0.0005709948600269854\n",
            "Epoch  11 Batch  138 / 261  Training Loss  0.0021996567957103252\n",
            "Epoch  11 Batch  139 / 261  Training Loss  0.0010284120216965675\n",
            "Epoch  11 Batch  140 / 261  Training Loss  0.0005001001991331577\n",
            "Epoch  11 Batch  141 / 261  Training Loss  0.0015362007543444633\n",
            "Epoch  11 Batch  142 / 261  Training Loss  0.004026579670608044\n",
            "Epoch  11 Batch  143 / 261  Training Loss  0.016767822206020355\n",
            "Epoch  11 Batch  144 / 261  Training Loss  0.0034831634256988764\n",
            "Epoch  11 Batch  145 / 261  Training Loss  0.0022395190317183733\n",
            "Epoch  11 Batch  146 / 261  Training Loss  0.0012608664110302925\n",
            "Epoch  11 Batch  147 / 261  Training Loss  0.0014574718661606312\n",
            "Epoch  11 Batch  148 / 261  Training Loss  0.003230920061469078\n",
            "Epoch  11 Batch  149 / 261  Training Loss  0.0043337224051356316\n",
            "Epoch  11 Batch  150 / 261  Training Loss  0.00045697615132667124\n",
            "Epoch  11 Batch  151 / 261  Training Loss  0.0011681793257594109\n",
            "Epoch  11 Batch  152 / 261  Training Loss  0.0003816296812146902\n",
            "Epoch  11 Batch  153 / 261  Training Loss  0.0012301000533625484\n",
            "Epoch  11 Batch  154 / 261  Training Loss  0.005977964494377375\n",
            "Epoch  11 Batch  155 / 261  Training Loss  0.02139783464372158\n",
            "Epoch  11 Batch  156 / 261  Training Loss  0.000904100714251399\n",
            "Epoch  11 Batch  157 / 261  Training Loss  0.002375612035393715\n",
            "Epoch  11 Batch  158 / 261  Training Loss  0.0026275257114320993\n",
            "Epoch  11 Batch  159 / 261  Training Loss  0.002712061395868659\n",
            "Epoch  11 Batch  160 / 261  Training Loss  0.0030944254249334335\n",
            "Epoch  11 Batch  161 / 261  Training Loss  0.0026553014758974314\n",
            "Epoch  11 Batch  162 / 261  Training Loss  0.003976038191467524\n",
            "Epoch  11 Batch  163 / 261  Training Loss  0.008008535020053387\n",
            "Epoch  11 Batch  164 / 261  Training Loss  0.005620175506919622\n",
            "Epoch  11 Batch  165 / 261  Training Loss  0.002823661547154188\n",
            "Epoch  11 Batch  166 / 261  Training Loss  0.0022652067709714174\n",
            "Epoch  11 Batch  167 / 261  Training Loss  0.005732072051614523\n",
            "Epoch  11 Batch  168 / 261  Training Loss  0.0018584654899314046\n",
            "Epoch  11 Batch  169 / 261  Training Loss  0.005079220049083233\n",
            "Epoch  11 Batch  170 / 261  Training Loss  0.002073527779430151\n",
            "Epoch  11 Batch  171 / 261  Training Loss  0.0030460632406175137\n",
            "Epoch  11 Batch  172 / 261  Training Loss  0.0006666353438049555\n",
            "Epoch  11 Batch  173 / 261  Training Loss  0.001448341179639101\n",
            "Epoch  11 Batch  174 / 261  Training Loss  0.0006272678729146719\n",
            "Epoch  11 Batch  175 / 261  Training Loss  0.0039036881644278765\n",
            "Epoch  11 Batch  176 / 261  Training Loss  0.00037924942444078624\n",
            "Epoch  11 Batch  177 / 261  Training Loss  0.007908388040959835\n",
            "Epoch  11 Batch  178 / 261  Training Loss  0.0063663991168141365\n",
            "Epoch  11 Batch  179 / 261  Training Loss  0.0010302289156243205\n",
            "Epoch  11 Batch  180 / 261  Training Loss  0.0009724634001031518\n",
            "Epoch  11 Batch  181 / 261  Training Loss  0.00045546170440502465\n",
            "Epoch  11 Batch  182 / 261  Training Loss  0.002184999408200383\n",
            "Epoch  11 Batch  183 / 261  Training Loss  0.0006155335577204823\n",
            "Epoch  11 Batch  184 / 261  Training Loss  0.0007911641150712967\n",
            "Epoch  11 Batch  185 / 261  Training Loss  0.00103679858148098\n",
            "Epoch  11 Batch  186 / 261  Training Loss  0.0031581276562064886\n",
            "Epoch  11 Batch  187 / 261  Training Loss  0.003501581260934472\n",
            "Epoch  11 Batch  188 / 261  Training Loss  0.0005828149733133614\n",
            "Epoch  11 Batch  189 / 261  Training Loss  0.00041239947313442826\n",
            "Epoch  11 Batch  190 / 261  Training Loss  0.0005918830865994096\n",
            "Epoch  11 Batch  191 / 261  Training Loss  0.0011866186978295445\n",
            "Epoch  11 Batch  192 / 261  Training Loss  0.0005539676640182734\n",
            "Epoch  11 Batch  193 / 261  Training Loss  0.0015683056553825736\n",
            "Epoch  11 Batch  194 / 261  Training Loss  0.0004660428676288575\n",
            "Epoch  11 Batch  195 / 261  Training Loss  0.0005692694103345275\n",
            "Epoch  11 Batch  196 / 261  Training Loss  0.0021358795929700136\n",
            "Epoch  11 Batch  197 / 261  Training Loss  0.0008371116127818823\n",
            "Epoch  11 Batch  198 / 261  Training Loss  0.0009255681652575731\n",
            "Epoch  11 Batch  199 / 261  Training Loss  0.00048257142771035433\n",
            "Epoch  11 Batch  200 / 261  Training Loss  0.0008186898194253445\n",
            "Epoch  11 Batch  201 / 261  Training Loss  0.0004271626239642501\n",
            "Epoch  11 Batch  202 / 261  Training Loss  0.0035712984390556812\n",
            "Epoch  11 Batch  203 / 261  Training Loss  0.0021080172155052423\n",
            "Epoch  11 Batch  204 / 261  Training Loss  0.0004958540666848421\n",
            "Epoch  11 Batch  205 / 261  Training Loss  0.002410203916952014\n",
            "Epoch  11 Batch  206 / 261  Training Loss  0.004962209612131119\n",
            "Epoch  11 Batch  207 / 261  Training Loss  0.0010271104983985424\n",
            "Epoch  11 Batch  208 / 261  Training Loss  0.000600988685619086\n",
            "Epoch  11 Batch  209 / 261  Training Loss  0.0004896115860901773\n",
            "Epoch  11 Batch  210 / 261  Training Loss  0.0013493045698851347\n",
            "Epoch  11 Batch  211 / 261  Training Loss  0.0017563030123710632\n",
            "Epoch  11 Batch  212 / 261  Training Loss  0.0007356948335655034\n",
            "Epoch  11 Batch  213 / 261  Training Loss  0.0004508736601565033\n",
            "Epoch  11 Batch  214 / 261  Training Loss  0.0006167255342006683\n",
            "Epoch  11 Batch  215 / 261  Training Loss  0.0005683096242137253\n",
            "Epoch  11 Batch  216 / 261  Training Loss  0.00039879168616607785\n",
            "Epoch  11 Batch  217 / 261  Training Loss  0.0003849011554848403\n",
            "Epoch  11 Batch  218 / 261  Training Loss  0.0005940510309301317\n",
            "Epoch  11 Batch  219 / 261  Training Loss  0.0002768223639577627\n",
            "Epoch  11 Batch  220 / 261  Training Loss  0.0013640380930155516\n",
            "Epoch  11 Batch  221 / 261  Training Loss  0.0004954840405844152\n",
            "Epoch  11 Batch  222 / 261  Training Loss  0.0022466701921075583\n",
            "Epoch  11 Batch  223 / 261  Training Loss  0.001281582866795361\n",
            "Epoch  11 Batch  224 / 261  Training Loss  0.00066517194500193\n",
            "Epoch  11 Batch  225 / 261  Training Loss  0.0007783288601785898\n",
            "Epoch  11 Batch  226 / 261  Training Loss  0.0035080714151263237\n",
            "Epoch  11 Batch  227 / 261  Training Loss  0.0040748645551502705\n",
            "Epoch  11 Batch  228 / 261  Training Loss  0.0002870420867111534\n",
            "Epoch  11 Batch  229 / 261  Training Loss  0.0003268854634370655\n",
            "Epoch  11 Batch  230 / 261  Training Loss  0.00040085200453177094\n",
            "Epoch  11 Batch  231 / 261  Training Loss  0.00046826034667901695\n",
            "Epoch  11 Batch  232 / 261  Training Loss  0.0003582637873478234\n",
            "Epoch  11 Batch  233 / 261  Training Loss  0.0010222343262284994\n",
            "Epoch  11 Batch  234 / 261  Training Loss  0.0005794360768049955\n",
            "Epoch  11 Batch  235 / 261  Training Loss  0.00022294634254649282\n",
            "Epoch  11 Batch  236 / 261  Training Loss  0.0005283934879116714\n",
            "Epoch  11 Batch  237 / 261  Training Loss  0.00019215930660720915\n",
            "Epoch  11 Batch  238 / 261  Training Loss  0.00040381678263656795\n",
            "Epoch  11 Batch  239 / 261  Training Loss  0.0004350612289272249\n",
            "Epoch  11 Batch  240 / 261  Training Loss  0.0002228740049758926\n",
            "Epoch  11 Batch  241 / 261  Training Loss  0.0003509894304443151\n",
            "Epoch  11 Batch  242 / 261  Training Loss  0.0006326730363070965\n",
            "Epoch  11 Batch  243 / 261  Training Loss  0.0002315596502739936\n",
            "Epoch  11 Batch  244 / 261  Training Loss  0.00016895441513042897\n",
            "Epoch  11 Batch  245 / 261  Training Loss  0.0003459209983702749\n",
            "Epoch  11 Batch  246 / 261  Training Loss  0.0002439825184410438\n",
            "Epoch  11 Batch  247 / 261  Training Loss  0.0003435844264458865\n",
            "Epoch  11 Batch  248 / 261  Training Loss  0.000239087050431408\n",
            "Epoch  11 Batch  249 / 261  Training Loss  0.000484615535242483\n",
            "Epoch  11 Batch  250 / 261  Training Loss  0.0003133798309136182\n",
            "Epoch  11 Batch  251 / 261  Training Loss  0.00019903811335097998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTiNbM081LlD"
      },
      "source": [
        "# Changing the directory to store the model there.\n",
        "# print(os.getcwd())\n",
        "# os.chdir('/content/drive/My Drive/Colab Notebooks/new/')\n",
        "\n",
        "# print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3bq66KD1eg7"
      },
      "source": [
        "#### Saving the Model (creating checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fJcvYcG1nd3"
      },
      "source": [
        "# PATH = \"fine_tune_10e_20eph.pt\"\n",
        "# torch.save({\n",
        "#             'epoch': num_of_epochs,\n",
        "#             'model_state_dict': model.state_dict(),\n",
        "#             'optimizer_state_dict': optimizer.state_dict(),\n",
        "#             'loss': running_loss,\n",
        "#             }, PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuyVxTpoGouA"
      },
      "source": [
        "model_load = T5ForConditionalGeneration.from_pretrained('FineTune_10e_Model_32eph_5e_4_v3322.bin', return_dict=True, config='t5-base-config.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ud531fFI122"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bzZYxgxLhMd",
        "outputId": "fe92ceab-643f-49a8-eccb-f0012f6839e8"
      },
      "source": [
        "model_load.to('cpu')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUQx6vLt9zNN"
      },
      "source": [
        "# Function to generate sentences from symptoms on the test dataset\n",
        "def generateText(text):\n",
        "  model_load.eval()\n",
        "  input_ids = tokenizer.encode(text, return_tensors=\"pt\")  # Batch size 1\n",
        "  # s = time.time()\n",
        "  outputs = model_load.generate(input_ids)\n",
        "  prediction=tokenizer.decode(outputs[0]).replace('<pad>','').replace('</s>','')\n",
        "  return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Kd_klpg90NF",
        "outputId": "81f35057-27e3-48c9-fdd8-fd87527459f8"
      },
      "source": [
        "data_ood = pd.read_csv('data_10e_ood_full.csv')\n",
        "print(data_ood.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              inputs                              target\n",
            "0  The sum of 9 10e3 8 10e2 2 10e1 8 10e0 and 2 1...  1 10e4 2 10e3 2 10e2 6 10e1 5 10e0\n",
            "1  The sum of 3 10e3 9 10e2 0 10e1 4 10e0 and 4 1...         8 10e3 1 10e2 0 10e1 4 10e0\n",
            "2  The sum of 3 10e3 8 10e2 6 10e1 4 10e0 and 2 1...         6 10e3 1 10e2 9 10e1 5 10e0\n",
            "3  The sum of 2 10e3 1 10e2 7 10e1 6 10e0 and 2 1...         4 10e3 2 10e2 2 10e1 7 10e0\n",
            "4  The sum of 8 10e3 2 10e2 2 10e1 1 10e0 and 4 1...  1 10e4 2 10e3 8 10e2 0 10e1 0 10e0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuMlx-4m95OI"
      },
      "source": [
        "data_ood['type'] = \"OOD\"\n",
        "\n",
        "data_test['type'] = 'iid'\n",
        "data_ood['target_str'] = data_ood['target'].astype(str)\n",
        "\n",
        "bigdata = data_test.append(data_ood, ignore_index=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uV1HrVNGZUYa"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwRB2G8A-Dha"
      },
      "source": [
        "bigdata['inputs']\n",
        "bigdata['predictions'] = bigdata.apply(lambda x: generateText(x['inputs']), axis=1)\n",
        "bigdata.to_csv('results_infer_10e.csv', index = False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elq45fvZvxKR",
        "outputId": "60c753f8-7f42-4966-c325-4f604f756901"
      },
      "source": [
        "evaluate(model_load, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "99.84270134228188"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}