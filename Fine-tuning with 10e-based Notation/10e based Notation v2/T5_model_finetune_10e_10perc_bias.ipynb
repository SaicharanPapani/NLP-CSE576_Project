{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NLP_Project_model_finetune_10e.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"a434a90742e64edf86daba294425e0bf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_1ebc718a7b3043919b468a2768685acb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_929fe1a6979049eb8d6f369780f09d9e","IPY_MODEL_c71a44ff77a74098b54cd86bb3485334","IPY_MODEL_a36f75f654144fbfa9046021921ffd6a"]}},"1ebc718a7b3043919b468a2768685acb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"929fe1a6979049eb8d6f369780f09d9e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0e06d5e7db294ae68a15dbcf9c22c6fb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e59e9b4cf12246efa2baea5d703078be"}},"c71a44ff77a74098b54cd86bb3485334":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_47d0902b2a2f445e81c3fd44146f30e5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":791656,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":791656,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0a722395b12b431c90752b5b8cc3151d"}},"a36f75f654144fbfa9046021921ffd6a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_29fffd4d4c2e4e5cb7cb3c41c3e7bb0b","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 773k/773k [00:00&lt;00:00, 2.02MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f15c2bdeed6a4e7f9cdcdf4d329b4893"}},"0e06d5e7db294ae68a15dbcf9c22c6fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e59e9b4cf12246efa2baea5d703078be":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"47d0902b2a2f445e81c3fd44146f30e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"0a722395b12b431c90752b5b8cc3151d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"29fffd4d4c2e4e5cb7cb3c41c3e7bb0b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f15c2bdeed6a4e7f9cdcdf4d329b4893":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"808b34c4f08b4913b4b7b08ce6e6e951":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_7a4b24a684b249faa847eee82a64488a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_621343c2c9474884a7b2f19f7a68faaf","IPY_MODEL_75989c155d604fb192ff8eb1f4f3e6fe","IPY_MODEL_3e711f4a2421473ba6cbf1630aa65acc"]}},"7a4b24a684b249faa847eee82a64488a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"621343c2c9474884a7b2f19f7a68faaf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3cf6be1cf9b842a4b62c9812f688f52c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8bc13c84395242e8b5d1e933d588f0c6"}},"75989c155d604fb192ff8eb1f4f3e6fe":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3203214762cd4cdcaf10464ea27059c6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1389353,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1389353,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8d8cedd2ba9845a297518790ffe8e140"}},"3e711f4a2421473ba6cbf1630aa65acc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e62809d2f11b4a92b6f5d825aaabf2af","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.32M/1.32M [00:00&lt;00:00, 2.16MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_38f98e84daf64b029690b7e9a74472bd"}},"3cf6be1cf9b842a4b62c9812f688f52c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8bc13c84395242e8b5d1e933d588f0c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3203214762cd4cdcaf10464ea27059c6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8d8cedd2ba9845a297518790ffe8e140":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e62809d2f11b4a92b6f5d825aaabf2af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"38f98e84daf64b029690b7e9a74472bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"882e40f1996d4fba8824e95b7c16f733":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d6e260a71af841838dab6b89e7c0be5e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2894ae4361c3406fb39b8a243e4ae4c8","IPY_MODEL_00ecffe11b49461fa2b23899b5f8808f","IPY_MODEL_84269ccdafcc4ab3a65e585e3985799e"]}},"d6e260a71af841838dab6b89e7c0be5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2894ae4361c3406fb39b8a243e4ae4c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_0b290681b4d4418fbf592d86f18c94cc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8d2ea7406fd740a6b38e357376536463"}},"00ecffe11b49461fa2b23899b5f8808f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0aad011a7dab49ffbee56c8702fa3a77","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1199,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1199,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a537e5346c6e47738fb87f3bc450af9e"}},"84269ccdafcc4ab3a65e585e3985799e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6a25230a39af4e7a87b47d27e281e838","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1.17k/1.17k [00:00&lt;00:00, 29.4kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_f48b27236e7942c1abfbc1d971be8142"}},"0b290681b4d4418fbf592d86f18c94cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"8d2ea7406fd740a6b38e357376536463":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0aad011a7dab49ffbee56c8702fa3a77":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a537e5346c6e47738fb87f3bc450af9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6a25230a39af4e7a87b47d27e281e838":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"f48b27236e7942c1abfbc1d971be8142":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9989be52718d4c358820426395a82ce5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a33b3496c8484916aa455c23641ebbf0","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f754ca44e3774e2ebebcb3cdfc78e008","IPY_MODEL_f1fca81e27dd49358f74e31f5c840546","IPY_MODEL_5054fdb07c9848cd95b870d48343ae32"]}},"a33b3496c8484916aa455c23641ebbf0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f754ca44e3774e2ebebcb3cdfc78e008":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_608cc7f730664f88a21bca5fd1790e01","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_07eec2d5d9e2455989052b2a9e7bf287"}},"f1fca81e27dd49358f74e31f5c840546":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_1ee89f19ad7b4c3f9ee70150b64b0321","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":891691430,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":891691430,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_1dffaa7b88094f4487b2ffd251f427c3"}},"5054fdb07c9848cd95b870d48343ae32":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c4aa0657ad1d4207b0ceff2890737aa7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 850M/850M [00:46&lt;00:00, 34.9MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_552e0f02581b4923acf31a1f5c00c70e"}},"608cc7f730664f88a21bca5fd1790e01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"07eec2d5d9e2455989052b2a9e7bf287":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"1ee89f19ad7b4c3f9ee70150b64b0321":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"1dffaa7b88094f4487b2ffd251f427c3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c4aa0657ad1d4207b0ceff2890737aa7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"552e0f02581b4923acf31a1f5c00c70e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"qFErrTqq_ybT"},"source":["### Installing neccessary packages:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3OMB-x8_0vK","executionInfo":{"status":"ok","timestamp":1638663600638,"user_tz":420,"elapsed":16300,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"8f911a3f-8087-4ff5-d752-57ae0ba8a128"},"source":["!pip install transformers\n","# https://huggingface.co/transformers/installation.html\n","!pip install sentencepiece\n","# https://pypi.org/project/sentencepiece/\n","# Python wrapper for SentencePiece. This API will offer the encoding, decoding and training of Sentencepiece.\n","!pip install Cython\n","# https://pypi.org/project/Cython/"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 12.0 MB/s \n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 31.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 38.7 MB/s \n","\u001b[?25hCollecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 41.4 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Collecting huggingface-hub<1.0,>=0.1.0\n","  Downloading huggingface_hub-0.2.1-py3-none-any.whl (61 kB)\n","\u001b[K     |████████████████████████████████| 61 kB 508 kB/s \n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed huggingface-hub-0.2.1 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 12.5 MB/s \n","\u001b[?25hInstalling collected packages: sentencepiece\n","Successfully installed sentencepiece-0.1.96\n","Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (0.29.24)\n"]}]},{"cell_type":"markdown","metadata":{"id":"T-AVcK4gBhW7"},"source":["## Checking the GPU availabilty"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K5BIx7Mj1x9M","executionInfo":{"status":"ok","timestamp":1638663606849,"user_tz":420,"elapsed":6218,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"08c2f92e-61b7-4aa7-e856-101cc82fd751"},"source":["import torch\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda:0\") \n","    print(\"GPU\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"CPU\")"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["GPU\n"]}]},{"cell_type":"code","metadata":{"id":"z1wUPLeYJ6GO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638663701227,"user_tz":420,"elapsed":94387,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"a8b30763-3744-4e93-f5d8-bb114e47b606"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","metadata":{"id":"bU-UZe2cBPpq"},"source":["## Importing the required packages:"]},{"cell_type":"code","metadata":{"id":"UrGEtltY6SIa","executionInfo":{"status":"ok","timestamp":1638663701230,"user_tz":420,"elapsed":19,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"mlokiVxO7jy0","executionInfo":{"status":"ok","timestamp":1638663703811,"user_tz":420,"elapsed":2597,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["import os\n","import sys\n","from transformers.optimization import Adafactor \n","import time\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    get_linear_schedule_with_warmup\n",")\n","import torch\n","import random\n","import re\n","\n","os.chdir('/content/drive/MyDrive/NLP_Final_Project')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xdoJ8keH8pLP","executionInfo":{"status":"ok","timestamp":1638666376481,"user_tz":420,"elapsed":247,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"aab08c4b-23f2-48ec-d21a-6a542a1dd6ed"},"source":["import pandas as pd\n","# Reading csv\n","data = pd.read_csv('data_10e_full_v1.csv')\n","print(data.head(5))"],"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["                                              inputs  ... sumlen\n","0  The sum of 1 10e3 4 10e2 0 10e1 6 10e0 and 7 1...  ...      4\n","1  The sum of 1 10e3 6 10e2 1 10e1 6 10e0 and 1 1...  ...      4\n","2  The sum of 3 10e2 7 10e1 6 10e0 and 1 10e3 6 1...  ...      4\n","3  The sum of 1 10e3 0 10e2 3 10e1 1 10e0 and 4 1...  ...      4\n","4  The sum of 1 10e3 3 10e2 6 10e1 3 10e0 and 5 1...  ...      4\n","\n","[5 rows x 5 columns]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l1CoyxqkGAEf","executionInfo":{"status":"ok","timestamp":1638666450081,"user_tz":420,"elapsed":427,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"bef05c80-372f-4471-f93d-93f7f831d892"},"source":["data_ood1 = pd.read_csv('10e_5digits_ood_v1.csv')\n","print(data.head(5))"],"execution_count":60,"outputs":[{"output_type":"stream","name":"stdout","text":["                                              inputs  ... target_str\n","0  The sum of 3 10e1 4 10e0 and 6 10e2 3 10e1 2 1...  ...        NaN\n","1  The sum of 6 10e2 9 10e1 5 10e0 and 9 10e2 7 1...  ...        NaN\n","2  The sum of 2 10e2 7 10e1 4 10e0 and 1 10e3 6 1...  ...        NaN\n","3  The sum of 1 10e2 2 10e1 8 10e0 and 4 10e2 4 1...  ...        NaN\n","4  The sum of 1 10e3 5 10e2 4 10e1 7 10e0 and 9 1...  ...        NaN\n","\n","[5 rows x 7 columns]\n"]}]},{"cell_type":"code","metadata":{"id":"YA_luy6aGn45","executionInfo":{"status":"ok","timestamp":1638666457762,"user_tz":420,"elapsed":222,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["data = data.sample(n = 11700, random_state = 42).reset_index(drop=True)\n","data_ood = data_ood1.sample(n = 1300, random_state = 42).reset_index(drop=True)"],"execution_count":61,"outputs":[]},{"cell_type":"code","metadata":{"id":"WMyEkxa9Gixi","executionInfo":{"status":"ok","timestamp":1638666387966,"user_tz":420,"elapsed":2,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["data = data.append(data_ood, ignore_index=True)"],"execution_count":55,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BffDkzj5HIZ6","executionInfo":{"status":"ok","timestamp":1638666389067,"user_tz":420,"elapsed":4,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"29285a69-5df1-4d42-8217-477305699985"},"source":["data = data.sample(frac=1).reset_index(drop=True)\n","print(len(data))"],"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["13000\n"]}]},{"cell_type":"code","metadata":{"id":"rcgHXMgv8606","executionInfo":{"status":"ok","timestamp":1638666389843,"user_tz":420,"elapsed":6,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["from sklearn.model_selection import train_test_split\n","\n","# Test and validation split\n","train, validation = train_test_split(data, test_size=0.2, random_state=42)\n","train, test = train_test_split(train, test_size=0.3, random_state=42)\n","\n","data_train = train.reset_index(drop=True)\n","data_valid = validation.reset_index(drop=True)\n","data_test = test.reset_index(drop=True)"],"execution_count":57,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y7YLTIG87LGc","executionInfo":{"status":"ok","timestamp":1638663859584,"user_tz":420,"elapsed":3,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"798d991c-726e-40ef-a457-f154cbff18dd"},"source":["data_train.shape"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7280, 5)"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vohFU6vyHAiq","executionInfo":{"status":"ok","timestamp":1638663860568,"user_tz":420,"elapsed":4,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"dad4418b-bf2d-4a1b-d6eb-2571f66089d6"},"source":["data_valid.shape"],"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2600, 5)"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NGC14qe5OR4R","executionInfo":{"status":"ok","timestamp":1638663860908,"user_tz":420,"elapsed":5,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"cd4c044e-56aa-498a-b311-52b7d29c46c6"},"source":["data_test.shape"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(3120, 5)"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","metadata":{"id":"7ls6YKC19R07","executionInfo":{"status":"ok","timestamp":1638663862071,"user_tz":420,"elapsed":2,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["# Initializing Parameters \n","batch_size, num_of_epochs = 32, 32\n","num_of_batches = int(len(data_train)/batch_size)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"ye-gC3y2YI5g","executionInfo":{"status":"ok","timestamp":1638663862317,"user_tz":420,"elapsed":5,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["# Reference\n","# https://huggingface.co/transformers/model_doc/t5.html\n","# https://medium.com/analytics-vidhya/t5-a-detailed-explanation-a0ac9bc53e51\n","# https://towardsdatascience.com/data-to-text-generation-with-t5-building-a-simple-yet-advanced-nlg-model-b5cce5a6df45"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":284},"id":"HngN0xRX1Rk5","executionInfo":{"status":"ok","timestamp":1638663862571,"user_tz":420,"elapsed":259,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"3f9e49fd-ba67-4b98-a43f-acd28a6673ce"},"source":["# get length of all the messages in the train set\n","seq_len = [len(i.split()) for i in data_train['inputs']]\n","\n","pd.Series(seq_len).hist(bins = 30)"],"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.axes._subplots.AxesSubplot at 0x7fadbf760350>"]},"metadata":{},"execution_count":22},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"_ad1Lt8c9iDX","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["a434a90742e64edf86daba294425e0bf","1ebc718a7b3043919b468a2768685acb","929fe1a6979049eb8d6f369780f09d9e","c71a44ff77a74098b54cd86bb3485334","a36f75f654144fbfa9046021921ffd6a","0e06d5e7db294ae68a15dbcf9c22c6fb","e59e9b4cf12246efa2baea5d703078be","47d0902b2a2f445e81c3fd44146f30e5","0a722395b12b431c90752b5b8cc3151d","29fffd4d4c2e4e5cb7cb3c41c3e7bb0b","f15c2bdeed6a4e7f9cdcdf4d329b4893","808b34c4f08b4913b4b7b08ce6e6e951","7a4b24a684b249faa847eee82a64488a","621343c2c9474884a7b2f19f7a68faaf","75989c155d604fb192ff8eb1f4f3e6fe","3e711f4a2421473ba6cbf1630aa65acc","3cf6be1cf9b842a4b62c9812f688f52c","8bc13c84395242e8b5d1e933d588f0c6","3203214762cd4cdcaf10464ea27059c6","8d8cedd2ba9845a297518790ffe8e140","e62809d2f11b4a92b6f5d825aaabf2af","38f98e84daf64b029690b7e9a74472bd","882e40f1996d4fba8824e95b7c16f733","d6e260a71af841838dab6b89e7c0be5e","2894ae4361c3406fb39b8a243e4ae4c8","00ecffe11b49461fa2b23899b5f8808f","84269ccdafcc4ab3a65e585e3985799e","0b290681b4d4418fbf592d86f18c94cc","8d2ea7406fd740a6b38e357376536463","0aad011a7dab49ffbee56c8702fa3a77","a537e5346c6e47738fb87f3bc450af9e","6a25230a39af4e7a87b47d27e281e838","f48b27236e7942c1abfbc1d971be8142","9989be52718d4c358820426395a82ce5","a33b3496c8484916aa455c23641ebbf0","f754ca44e3774e2ebebcb3cdfc78e008","f1fca81e27dd49358f74e31f5c840546","5054fdb07c9848cd95b870d48343ae32","608cc7f730664f88a21bca5fd1790e01","07eec2d5d9e2455989052b2a9e7bf287","1ee89f19ad7b4c3f9ee70150b64b0321","1dffaa7b88094f4487b2ffd251f427c3","c4aa0657ad1d4207b0ceff2890737aa7","552e0f02581b4923acf31a1f5c00c70e"]},"executionInfo":{"status":"ok","timestamp":1638663919829,"user_tz":420,"elapsed":56496,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"6d7e5db1-1153-480a-9c55-53a09c48d8bd"},"source":["# T5-base\n","tokenizer = T5Tokenizer.from_pretrained('t5-base')\n","\n","model = T5ForConditionalGeneration.from_pretrained('t5-base', return_dict=True)\n","# moving the model to device(GPU/CPU)\n","model.to(device)"],"execution_count":23,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a434a90742e64edf86daba294425e0bf","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"808b34c4f08b4913b4b7b08ce6e6e951","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"882e40f1996d4fba8824e95b7c16f733","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9989be52718d4c358820426395a82ce5","version_minor":0,"version_major":2},"text/plain":["Downloading:   0%|          | 0.00/850M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"uNRQcyTh3GQf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1638663921946,"user_tz":420,"elapsed":2126,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"cb2f633e-f1ea-4f2f-ce76-4ae434067443"},"source":["token_lens = []\n","\n","for txt in data_train.inputs:\n","  # doubt\n","  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n","  token_lens.append(len(tokens))\n","\n","max(token_lens)"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["50"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Efo0J6kA-yt","executionInfo":{"status":"ok","timestamp":1638663923363,"user_tz":420,"elapsed":1420,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"c33ca036-e66c-4950-e32b-e4943a57c9be"},"source":["token_lens_target = []\n","\n","for txt in data_train.target:\n","  # doubt\n","  tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n","  token_lens_target.append(len(tokens))\n","\n","max(token_lens_target)"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["23"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":284},"id":"OT1Us38M2ULV","executionInfo":{"status":"ok","timestamp":1638663923676,"user_tz":420,"elapsed":322,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"e74afab1-c995-4042-a530-00552d9ce7e8"},"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(range(1,len(token_lens)+1), token_lens)\n","plt.ylabel('length of tokens')\n","plt.show()\n","\n","MAX_LEN = max(token_lens)\n","print(\"Maximum length is: \", MAX_LEN)\n","# when sample with first 40k and last 40k we got the maximum length is 14"],"execution_count":26,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3deXwU9fnA8c9DuMMNESIgAQQUQRAiglClKCDSetSjWmup1WKtrVgVC/5q7fkrtl49rEprlV+LiopWK1pEAVtF0XCriCCES46AhPtK8vz+mNlkk2yS2c3O7uzu83699rU7s3M8O8czs9/5zndEVTHGGJM5GiQ7AGOMMYllid8YYzKMJX5jjMkwlviNMSbDWOI3xpgM0zDZAXjRoUMHzcvLS3YYxhiTUpYsWbJLVXOq9k+JxJ+Xl0dBQUGywzDGmJQiIhsj9beiHmOMyTCW+I0xJsNY4jfGmAxjid8YYzKMJX5jjMkwvtbqEZFCYD9QCpSoar6ItANmAXlAIXClqu7xMw5jjDEVEnHG/2VVHaiq+W73FOBNVe0FvOl2G2OMSZBkFPVcDMxwP88ALklCDMZkFFXluYLNHC0pTXYoJgD8TvwKvC4iS0Rkotuvo6pucz9vBzpGGlFEJopIgYgUFBUV+RymMelt3sc7mPz8Sh54/dNkh2ICwO87d0eo6lYROQGYJyKfhH+pqioiEZ8Eo6rTgekA+fn59rQYY+ph35ESAIoOHE1yJCYIfD3jV9Wt7vtO4EVgCLBDRHIB3PedfsZgjDGmMt8Sv4hki0jL0GdgDPAh8DIwwR1sAvCSXzEYY4ypzs+ino7AiyISms9TqvpvEfkAeFZErgc2Alf6GIMxBufirjEhvp3xq+p6VR3gvk5T1V+7/Xer6nmq2ktVz1fVL/yKIdyo+xYy8f+qt/C5Zc8h8qbMofdPXiNvyhzypsxha/Fhfvj0MoZPm8+m3c73iz7bFdN886bM4YHX10Q1zqurtpE3ZQ5fHDwW0zyT5WhJKXlT5vD3dwvrPa3+98wlb8ocet71atTjTvy/Akbdt7C8+765a8ibMqfeMcVi+LT53PL0Mk/DXvynt7l6+nvl3c8v2ULelDkcOOqUz2/cfZC8KXN4b/3umON5YelWT8vi6fc3kTdlDoeP1VwLKG/KHP745try7p/8cxX97plb57Q/3bGfvClzWLG5uFL/19ztfneV6xD975nLXS+uqjad9UUHyJsyh4LC6FLI22t3kTdlDlv2HIpqvHSSMXfurt91kNc/3lGt/5KNzr1jx0rKyvst27SHf634nK3Fh3lvg7OTvbB0a8zz/sP8dVEN/8Q7GwBYt/NAzPNMhr2HjgPR/95I9rvJrrQs+jPV1z/ewfpdB8u7/7Sg/vHEamvxYV5e8bmnYVds2cu7YUn9sbc+c6ax5zBAecKfvWRLnKOs7k/uOtx9sPaLwffPq6gl9I/3NpUfpGoz/xPnst6rq7ZV6v/EokIA1lbZ7vcfLeGpxZuqTeftdc7J2EvLvS3fkFkFm4GKfT8TZUzij4b9KzZBkNVAgIqDX1YDZ3ctTfEN1P1ZlMXpdyipvTySwRJ/BJE2oxTf10wKaphVOfE3rHIgiEaQNt8GznU/YvgZlbjHD9s3Y2CJP4LwC2FSy3DG+Cl0hl9SVuZ2i9ud2plOyhN/fTN/bHunXehOkUcvxtPDC9bx/ZE9ERFeXvE5z7rlfXVZtbWYJRv3MLhbWzZ/cYg12/dzft+O3PvvTzi9c2vG9c8FYMXmYkrKlMHd2tY6vSPHS5m91CmrvTK/K42yKo7BBW7ZY+Gugwzp3q68/0vLt1KmyorNe/nZRacB8OKyLXy5zwm0ad4YcDbq0Q/+h+wmDXni22ey4JOdnN+3I3M/3M6ug0e5fHAX3v1sNweOlvD1/K40zKr72L9gzU46tWrKis3FDO3Rng27DjLopLa8+ckOvjaoi6flBzD/kx2cnNOSk9o35/0NXzD9P5/x1QEncqykjPW7DrJ8UzH3XNSXme9VL899ddU21u44wI3n9mDZpmJEYNvew1x6RhdeWr6VL/XKYe2O/bRoWrFJP/vB5krFAEeOl/LPZVv5fO8ReuZkk9OiCR1aNqFT66ZMenoZv7tiAB1aNIkY+7JNe5i5eBMDu7Zh/5ESbhrZkwWf7KR7h2zyOmRHHGfv4eOVuj8o/IJmjbLo17l1rctp7kfbGXtap/Iz/JeWf87Evy9hygWnALB8U3G1cfYdOc4LS7bQomkjLh9csU427j7IRX96h3N7V3vsKtv2HmbF5r3kdWjOnoPHGdazPW99WoSqsrXYua7w5DuF/OQrfauNe/2TH5R/nvrCSs7Mq9hOV2wuZkDXNpSUlnHn7JW0btaIW0b1om22s42GinreWlMEX6V8fwp5e61Tdr9m+/5K031p+VZKSpXLBnfhWEkZ9811Kk3M+mAzXx1wIq2bNeLQsVJWb9tH744tWbfzAF8/sysvLN1CTssmdGjRpNKyDx2Adu47wssrPqdru+aMPa1T+ff7jxzn/tc/pXuHbPYcOsb4/rkcKy3jw6176dS6Gef2zuHFZVsY1acjzxZs5tCxUnrkOMOWlCrfGdGd0jJl1gebuSK/C7OXbOGSMzrTtFFWteWZaBmX+H83dw1Durcjr312jbUtwk8IQhvHpzsOcNkjiyicNp4xD/6Hw8dLKZw2nkcWOhfgCqeNB+Dih9+p1F2T3/57DX9zL+LuO+wkkqrzv3P2Sq48syvg7KSTnllePsxNI3ty8GgJP5q1gpF9cnjyuiEAzP1oR/lF4VH3L6T40HHG9O1YfmF71geb2bjbqc1wrKSM64Z3rzVOgOue+KBav/NPPYE3Vu+kX+fW9O7Yss5pAHznyQKyGgif/e+FXPnYuwC8sbry/XsXPPTfauMdKynj+zOXAlB8+BhPvFNY/t0JLZsy6ZnlnNW9HYs3VK7dcefslZW67/33J5XGDRmS1473C7/g7Gnz+fRX4yLGfumfFwFOTRuAb5+dx3Vu8qtpXd/x3IpK3Vc8+m6tw4fc+PclFE4bX54gn3Qvek55wfk9oaQcbuoLq5iz0rlY2qdjS/p3cRLcub9bCBDxAvNlf17E53uPlHcXThvPhL+9X2mYv769gcsGd+HU3Fbl/VZt2cubn1Sst6ff38zT71ecQF388DsUThvP/727sbxSxKc79jPzhqFARVFP6AL8+Q+8xdGSsvKTnD8tWBfxgnxo+z81txX//nBb+YG1pEy5Kqw2VLglG/eUn2CFfmNV3/jr4vJ9Jvz7u178kH+FLbeH3lhbabw3bjuXH81aQdvmjdhzqPJBHuA7I7oz64PN3PXiKmYv3cKSjXtYu/MAd0c4kCZaRhb1HDpWyvHSshq/r+ti0eHj9W/oKry2RPHhuqtthtc6AjheWsaR406/7WE7776ws8xid2Pcsb9iXlv2VCSNqmek0di+z5nn0eM1L8dIYiufrhinahXX/Ufc37jvCHXZdSDycg5V66u6jGvjpZTBS0y1zqNKQePx0pqX3c6weXndPsOTfm2OVJme1+kXh21f4dtogyrL7mgUyx3gSEkpuz1Wdf48wkGy6lLcuqf6MFD3+gstl0hJPyS0j23+wtnGglJFOyMTPwTrYlciVd3pUkH4P7CgFM/GWLwclaDWVomljDx8DEnEwnPVdh0hBXeFuMnYxF+boCSXcF5jqitZVD2LjFUQl1EmS6X10SCBiT9oiyUoF5Yt8UcQkHVTq1hjjPc+l8B9GPBnR07kGWg6iGkdhI1U33+d0Yxen0Qbj60idCIWtE1MgnIEqk1+fr4WFFRvbsGLZws2c+fzK+seMEzf3FZ8vG1fVOMM7ta21jsBv9SrA/9du4tXfjiCfp1bM+mZZeV3HE4Y1o2fX9yPkb9bwNbiw7WW5cZiQNc21W6PB7j1/F7cen5vLn9kUXlNovoI/cZImjRsEHVZbiRjT+vI3I+q34EdL6ELvdFYeMdIRoY1ERGazpGSUlZu2Vvn+KELitE0K1E4bTyPvvUZ0177pNp3zRtncaiWphai9eL3z2bi35dQtN97k86PfnMQH2/bzx/CmnT45JcX0LRRFk8t3hSxCQavRvftyLwId+F7UThtPDfPXMqcKncNVx1my55DjLh3QawhAtCsUVbE6yFrfz2OgT9/nbGndWLl1r20z27MrBuHUXzoGAN/MY8Jw7ox492N5cO/O3UUua2bxRSDiCwJe/phubQ/43/8vxuiHifapA913/4dSohvrK6+wYYuDhXuPhT3pA/U+fcgHkkfqDHpQ/QX8GpSnwvSXkSb9KGi6YD6Tidav69SyyQknkk/JJqkDzBz8aZqZ8z73Avx9X0KWKxJPxrLIlSZjVZNF8EPHy/l4LFSXli2lXU7D5TXRtvg1nIKT/rg1KKKt7RP/Ca9pMAfVFOD0LpL+v1nASt2SQZL/AkW2vgTuu0FrYAxzdS0eG2pVxaqYZP04uVkH3gCIO0Tf2CrxCU7gBSVSscwW8eVhc70k533TQbcufvpjuA1bXyspKxSU7LLNxfz9PvVmymIl0gXdgHe3/AFMxdvjPhdUAUxadTUTLKXC7vg3NQW7Xq49ZllcbmR0IvQXcvR+KDwi0o3bYHzbIL7rxgQt1Y5Y3H6z+aWP3+4Jg/O+5SeJ7TwLYaqywXg5qeWlt95nQhpf8YfRH9eWPl29E1fHGLqC7HXcojVos928z8vfpjw+aabpfW8EPj0+5v46UsfRTXOP6Nsgz7Rjhwvq9auPsDtz61I6j+hupI+wO/fXMsrHp+hEIsxD/6nWr/akr4f1Y0t8SdBcS23eJvaBfCEv972e0hG6SQVSuv8qBkVKz+uiVjiN8YkVCpcp0mFGOvD98QvIlkiskxEXnG7nxSRDSKy3H0N9DsGk0bS8ZTfmARLxMXdScBqoFVYv8mq+nwC5m3STFBraRnjl5Qr4xeRLsB44K9+zscYY+Ip3dtv8ruo5yHgTqDq/fq/FpGVIvKgiER85JGITBSRAhEpKCoq8jnMxLHzVWNMsvmW+EXkK8BOVV1S5aupwCnAmUA74MeRxlfV6aqar6r5OTnVHxtnTLpI85NLU0+pVqtnOHCRiBQCzwCjROQfqrpNHUeBJ4AhPsZgjAmYeD0Twk/Bj7B+fEv8qjpVVbuoah5wFTBfVb8pIrkA4hSiXQLYHUTGsyDeuWvST5D+hflxvSEZTTbMFJEcnIPqcuB7SYjBpKh0zPt2MAueAOV9XyQk8avqQmCh+3lUIuZpjDEmMrtz16SUpDfpa0wasMRvTJIFqTw5EewmvOj4sXlY4jcpxVKGSYQGAToa+7HNW+JPNCuqMMYkmSV+Y4ypIkAn/L6wxJ9o6b5FGZMW0ns/tcSfaFbUUy+2+EymsYu7xpiUZwfv5LPEn2C2zddPOi4/S4TBE6QSWavVY0waZslMq9cepKSaqSzxJ5ht88YEX5D2UyvjTwOZdW4Xf+m4/NLwT0zKS/d/JZb4jTEmw1jiN8YklP3DST5L/ElgLUyaTJYKW38qPCWsPizxJ9j+IyXMeHdjssNIWXbMNIlgZfwmrooOHE12CCkt06o+pqNUyKmW+E1cpfn25Lt0POO3oj+TaL4nfhHJEpFlIvKK291dRBaLyDoRmSUijf2OIUiC1M53KrIcaUz9JeKMfxKwOqz7XuBBVT0Z2ANcn4AYAqOB5f16Sce8bwez4AnSxV0/zhV9Tfwi0gUYD/zV7RZgFPC8O8gM4BI/YwgasTP+elm9bV+yQzD19N+1u5IdQt3SfDf1+4z/IeBOoMztbg8Uq2qJ270F6BxpRBGZKCIFIlJQVFTkc5iJk+bbk4lBpp3wv70uBRJ/gPjxj9C3xC8iXwF2quqSWMZX1emqmq+q+Tk5OXGOLnnsjN+Y4Ev3vbShj9MeDlwkIhcCTYFWwO+BNiLS0D3r7wJs9TGGwLG8b6qyMv7gSfcTNN/O+FV1qqp2UdU84CpgvqpeAywALncHmwC85FcMQZTem5MxJt5S7uJuDX4M3CYi63DK/B/3a0ZBrB99tKSs7oFMRikts20iaNJ9ndRZ1CMi2cBhVS0Tkd7AKcBrqnrc60xUdSGw0P28HhgSU7RR+sHTyxIxm6i8vOLzZIdgAuYP89clOwRTxaurtic7BF95OeP/D9BURDoDrwPXAk/6GVS8zFm5LdkhGGNM4HhJ/KKqh4CvAX9W1SuA0/wNyxhjjF88JX4RGQZcA8xx+2X5F5Ixxhg/eUn8k4CpwIuq+pGI9MCpmWOMMSYF1XlxV1X/g1POH+peD9ziZ1DGGGP846VWT2/gDiAvfHhVHeVfWMYYY/zi5c7d54BHcRpaK/U3HGOMMeH8uIHLS+IvUdVH4j9rY4wxdUlWI23/EpHvi0iuiLQLveIfijHGmETwcsY/wX2fHNZPgR7xD8cYY4zfvNTq6Z6IQIwxxiRGnUU9ItJcRH4iItPd7l5uW/vGGGNSkJcy/ieAY8DZbvdW4Fe+RWSMMcZXXhJ/T1X9LXAcwG23x5qVN8aYFOUl8R8TkWa4jwYVkZ7AUV+jMsYY4xsvtXruAf4NdBWRmTiPVPy2n0EZY4zxj5fEvwSnSeahOEU8k4CWfgZljDHGP55u4AKOq+ocVX0FyHH7GWOMSUFeEv//4ty9my0ig4HngW/WNZKINBWR90VkhYh8JCI/d/s/KSIbRGS5+xpYv59gjDEmGl5u4JojIo2AeThFPJeq6qcepn0UGKWqB9zx3xaR19zvJqvq8zFHbYwxJmY1Jn4R+SNuTR5Xa+Az4AcigqrW2ia/qipwwO1s5L58aG7IGGPSV6IbaSvAubAbev0WmB3WXScRyRKR5cBOYJ6qLna/+rWIrBSRB0WkSczRG2NMmvug8Iu4T7PGM35VnRH6LCKNgd5u5xpVPe5l4qpaCgwUkTbAiyLSD+cxjtuBxsB04MfAL6qOKyITgYkAJ510kqcfY4wx6caPxO+lrZ6RwFrgYeDPwKcick40M1HVYpzn9F6gqtvUcRSnOYghNYwzXVXzVTU/JycnmtkZY0zaEB+exOKlVs/9wBhVPVdVzwHGAg/WNZKI5Lhn+rh3/o4GPhGRXLefAJcAH8YavDHGpDv1oZDfyw1cjVR1TVgQn7q1dOqSC8wQkSycA8yzqvqKiMwXkRycm8GWA9+LJXBjjMkEftSI8ZL4C0Tkr8A/3O5rcC781kpVVwJnROhvD2k3xpgk8pL4bwJuBkLVN/+LU95vjDHGZ+P758Z9ml4S//dU9QHggVAPEZkE/D7u0RhjjKlkTN9OcZ+ml4u7EyL0+3ac4zDGGJMgtd25ezXwDaC7iLwc9lVLIP4VS40xxiREbUU9i4BtQAecKp0h+4GVfgZljDHGP7XdubsR2AgMS1w4xhhjwvlw/5anMn5jjDFJkuhG2owxxqShGhO/iLzpvt+buHCMMcb4rbaLu7kicjZwkYg8g9PEQjlVXeprZMYYY3xRW+L/KXA30IWwm7dcCljTC8YYk4Jqq9XzPPC8iNytqr9MYEzGGGN85OWZu78UkYuAUBv8C1X1FX/DMsYYA6A+tM/p5UEsvwEmAR+7r0ki8r9xj8QYY0xCeGmkbTwwUFXLAERkBrAMuMvPwIwxxoCQnCdwAbQJ+9w67lEYY4xJGC9n/L8BlonIApwqnecAU3yNyhhjjG+8XNx9WkQWAme6vX6sqtt9jcoYY4xvPBX1qOo2VX3ZfWVc0v/qgBOTHYIxJkMlpVZPrESkqYi8LyIrROQjEfm527+7iCwWkXUiMktEGvsVgzHGmOr8bKTtKDBKVQcAA4ELRGQocC/woKqeDOwBrvcxhrjwoVVUY4xJGk+JX0SyROREETkp9KprHHUccDsbua9QUw/Pu/1nAJfEEHdC+dEetjHGJIuXG7h+COwA5gFz3JenO3fdA8ZyYKc7/mdAsaqWuINsATrXMO5EESkQkYKioiIvs/PFzV/uaWf8xpi04uWMfxLQR1VPU9X+7ut0LxNX1VJVHYjT0NsQ4BSvganqdFXNV9X8nJwcr6PV6kfn96bXCS08D//by09n8ti6Q/74F2PrE1a5Fk0qKlkVThtf6XN4d8hFA07k9tG9AecAFWmYujRt5E9pX+OGsU23c5tm1frV9PuHn9w+pnn0zMn2NM+6/PQrfWNa5nVpUmXZXZnfJeJwN57TIy7zy2nZJC7TqUn7bG+X8Wpan29NHhnTfPvmtoppPC/isd7bZTfm6iFd6xwuWTdwbQb21mcmqloMLMB5jGMbEQlluC7A1vpMOxpWZJNAPjw1qNosEjCPIPBjxw/n93K0/S6yBh6Xix+1emqsxy8it7kf1wMLRWQOzgVbJxjVqk01Vx0/BziuqsUi0gwYjXNhdwFwOfAMMAF4qV6/wEeh9SIJ2nKjnUs8wvI7qUQrCEkiCDFkopq2xaBto/EjJKvqSG03cLV03ze5r8buC7ydz+UCM0QkC+efxbOq+oqIfAw8IyK/wmnz5/GYIo+BEOyduj7H9Vh3Dj/OJhIl2evSryXXINk/LE0Efcv2esbvh9ra4w/Vu79CVZ8L/05Erqhrwqq6EjgjQv/1OOX9CRft/pSoM/36COzGHeOiS8YirzpPryGoT2Uk1eKpKaA4LasU2MzTkkjylr2XMv6pHvsFnoik7d/GWDcg38p3A3tEqjs0rwd8v04Mqk61xtnEaRn7XcZf3+mn64Epmf/saivjHwdcCHQWkT+EfdUKKIk8lqmPqMv4Ce4FzliLkKI5MPt1EE/TPGMCJpnbWW1l/J8DBcBFwJKw/vuBH/kZlJ/8OMjGKwHVr4w/8fOsdbqJqNUT88ElTvP3rajHY4R2hEppTglEctRWxr8CWCEiT6nq8QTG5Juoy/irvAeNiKT0xdlIAlHGn+QVXn32NQSUhFXfQKAsvTa5jOSlPf6lIlJ1Ve/F+TfwK1XdHf+w/JGu5ftA8rNVnCTiV9RZxp/O24kJjAYNkrfbekn8rwGlwFNu91VAc2A78CTwVV8iC4BEr5RYZlfv0gY7ewvedRKvG0KKHJ+87keKctvo3pzYphl3PLei3vP1qyguXpJ5guEl8Z+vqoPCuleJyFJVHSQi3/QrMD/UtQGKBDAJmPgL+Dr2XKsnCRqIUBblThLN4Lec1wsgLok/6JK5Xr1U58wSkfJ69yJyJpDldqZU7Z66lnPVDVQSXMgfbT4KUD6Im4TcO1HXLJJdxl9lGQRpPfu5ejKtiM25bzd4d+6G3AD8TURa4MS6D7hBRLJxnsebMpwbJtJz44q9Vo8/p78BP6muJNYbuNKH97XlJKpUWrvB5TUX+VEK4eWZux8A/UWktdsd3mDbs/EPKfXEK3lGnXAk/XbBVEq6fhULBvrcJMixpZhkNiFTZ+IXkSbAZUAe0DB0lFLVX/gamQ/8um83WdcFBnRpw+6Dx4DgJYt+nVuzYnNx9CPG8DuyGgil0dQxrGPQZC/L7MYNKT5UUYO6pnj8bHY4HZzYphmfbN+f7DBqlMxril7K+F8CLsYpzz8Y9ko50dfjl0rvNYll3S2+67x6T+dbw7ol/Wp0Tcv0isGR25Cvc3oeh1vyk/PLPz/2zcExzavmGKLbUMJj8eKJb5/Jj87vXeP3rZo1qhbP0rtHV2uX/uKBnXnjtnMq9Vt692iyom79Kz5HukZZwuybzo7LtKJ1SqeWlbrfvP1c+lTpN/70XM/PBkiEZBY7eynj76KqF/geSYL4saxjqTbWsVXTes83fMOJuXXOeh43mjfK4uCx0mr9/d6m27eoeHhI00ZZtQxZt6rLLtrYw2PxokXThmRF+ZyadtmNaRchaZ18Qstqw3Vt24zC3YeimHp8/i2dfEJLurSt/lCbRDipXfNKZ/c9c6o/cOmUji1577Pg3HbUwGMjbX7sS142v0Ui0j/+sw4+7/WP4zS/GMZJtzL+VFKfazu1HXADVmpXSW2/uUHAq0MHLbSg1+MfAXxbRDbgPIjFaRvM4+MXg0REfDrjj/80oxVz65zxDSOhfLu46s9ky6nWvtyD1oREuNqWeTJbm/TcvFGAlqWIt20tKbV6gHHxn21yxLrO69xYUjl7mmr8fgZtXepbvdTPsuPaNvVkPljEi6CFl8wy/jqLelR1I9AVGOV+PuRlvCD4Uq8OANwy6mTG9evEFfldeOjrZ3DpGZ0jDj/7pmHln4d0b8eYvp0AmDy2D53bNIs43u+vGkirZg25eshJfG1QZ+69rKJU7JzeOXRq1ZSrh3Tlnq/2rTbuzV/uWan7HzecRbf2zblhRPeI8f3lW/nccl4v/n79kPIHbV8/ojvj+nViwrA8AKaOO4XzT+1YKYavnJ4LQL/OFbVAxp+ey7h+nVhw+0jAeaB3pIeO1+SSgScytEc7/nHDWZX6j+/vTPeSgZ3pUeWh5qG7MgEen5Bf/nlM347l8T18zSC+fXYeD1w5gGaNsvjeuRXL6I4xzgXRW893phO+34SWjRcdWjap9KDyx64dTP/OrfnaGZ25ZOCJ/O6KAZWGn3hOD/p3bk2nVk357pcq1s01Z3Ur//xo2AXmk09owXeGVww3ddwpjO7bkfGn53LxwBM546Q25esrpH/n1gw/uT0XnNaJh74+sNJ3PxhV8bu+P9JZHveHxVjbQ+d/e9npPDNxKINOasOF/TtV+/5fPxjBk9cNYXC3tpzbO4eclk04t3cOANmNs7h6yElMHtuH0e46evbGoXxrWDdmTRzK5YO70Kqpc+7YuU0z7r9yICe0bELTRg0Y07cjZ3VvVz6fP159BqeG1UJ6+BtOYwC3j+7N5LF9uGjAifz60n7V4hvWoz2d2zSr8drBTe7y+OXF/fjN1/rTqVVT/nyNM+2JX6r8MHoFnrxuSKX9I+TK/C7ccl4vLg+rlNDYvRDzk/GnctGAE+nQwrnGElo/k87rxfRrq1csuHxwlxpPFkf37Ui39s0B+Mu3BvNDd5vNc/uFhOblF6nrwqSI3APkA31UtbeInAg8p6rDfY0sTH5+vhYUFEQ93g0zCnhj9Q4eu3YwY0+rvNHnTZlTbfjCaePrnGbV8SKNExqmru/2HjrOgF+8XuO0apuO1zjDxz1eWkav/3kNEdjwm5qnWXXcUPcto07mD/PXVZvub15bzWNvrYRba/UAAA9jSURBVOfHF5xSviOGlJUpPe56FYBFU0Zx9rT5dGrVlPci1GqK1jV/fY931u3mH9efxQj3IN/vnrkcOFr9hvLCaeNZ9NkuvvGXxZzVvR2zbhxW5/KNdvmP+/1/Wb1tH3NuGcFpJ7bm1Lv/zeHjpXz8i7E0bxz5z3VN8wj1/83X+nP1kJNqne9rq7Zx08ylXHBaJx69djCj7lvI+l0HefP2c6td5Pz7u4Xc/dJHfHPoSfzqEn8v3e06cJT8X71B++zGLLl7dL2n98LSLdz27AouPaMzD1Y5ONbmvrlr+NOCddw2urfnk4NoeMkJXq0vOsCo+9+ie4dsylTZuPsQC+8YSV6H7LpHjkBElqhqftX+Xs7cL8Vpk/8ggKp+TsXzeGubYVcRWSAiH4vIRyIyye3/MxHZKiLL3deF0f2UNBK0/54Jkqx/uH5fTPOjUTAvk8yUksYgXEtLF17K+I+pqoaaZnabavCiBLhdVZeKSEtgiYjMc797UFXviyHemGRofo2/OjJ2EJ4NEIR1nazaGlVXT1ASZbLDCNIFXa/8blnUyxn/syLyGNBGRL4LvAH8pa6RVHWbqi51P+8HVgORC9d9luwNryap0OxzPMV7PUTaN4K6rv1UbTl4WNGJOCjEe3sL3HOlfZCoC75eLu7eBzwPzAb6AD9V1T9GMxMRyQPOABa7vX4gIitF5G8i0jaqiKOar19TNpFEOtONtA6SnRD8zgPJ+ufjaTnYTuGrVFm8nmrnqOo8VZ2sqneo6ry6x6jgtuo5G7hVVfcBjwA9gYHANuD+GsabKCIFIlJQVFQUzSwjxF+v0Y3xJHgtv9qGD6mTjBOpxsQvIvtFZF+E134R2edl4iLSCCfpz1TVFwBUdYeqlqpqGU6R0ZBI46rqdFXNV9X8nJyc6H8ZyS/aCKp0Wy52YK8sKOs3KKslFbcPv0Ou7WHrddbcqY04pz+PA6tV9YGw/rmqus3tvBT4sD7z8SYF13wABSWhhEQ6k6vtoli6nvnFUrSUyD0iTRe7LxK1rLzU6onVcOBanEc1Lnf73QVcLSIDcba9QuBGH2MwJmGS/YxXL7WJMjEJJ/KAnyr/LnxL/Kr6NpG3s1f9mmeqycSdMJ4i7WRBKGdPdHXOoCebZIcX9OVTFz/CT4mmF2IVgBwQSLEmx/pugH6dEUf9c1I8EdQoinr8qVidM5Oo+rv80jrxm8iSXiRhR+S4qlaNv5blm4mLPpV+c6JizYjEn+p/9eKlvgk3FfafZB/UIIn1+JMyV+MXP7eitE78yXzQQRAFISn6wevP8ntrCNo/mUiLJU03gbTmx1aV1onfRJbIBBU+r3RPOr400ubhvK/qfIN1+DGxCF/vdnHXBNKALm0AODW3Xrd+xCz8OBbNQa1jK38euBL6pznmNKfd94YNat7NOkT5vN5oRK715NvsqmnW2HkW8nmnnJC4mSbZBadVf+ZBNMJLKfxcVX7W4w+MVDjRXBaH9srrEq/l8K1h3Sp1X9g/l//e+WW6tmtewxiJEzr7nTVxKF+f/h7NG2exaMqoysO4S2L+7SM5WlLmWyy/u3wAU8edSuOGNSf+hZNHcixOMYQOekEpcWreuCHvTT2P9j4/VCQoFt91Hm2aN0p2GJ6kdeIPyg5Qk9COmt04i7bZids56vsIyjbNq8cahKQfrr17Jt2yacOI8QJkN2lIto9PWWzcsAGdWjetdZgWTRpCnGII4uZe1+9PJx1bpc5vzYiinnQvW041QT8gp5qatu/arw/YThFkfuestE78lmBMuKDVuom30M+rrTab1XQLNqvHHwe2kcdX0P45hXaS8LiSmdxD8050Pf6qPznSckkHsa7aVM0C5duTDysyrcv477moL62aNWR0347VvrtjTG/eWbebyRf04Wt/XuR5mpPH9mFwt7aUqbJi896Iw9xyXi++3CdyU9K/vKQfHVs6hbrZjbO48dweXDwg8oPJHv7GIA4cPe45tnCPT8hna/HhSv0aNhC+P7InF/bPrXXcyWP7kN+t4vk4s28axjvrdlNaVr8NsEvbZlw/ojtXD+lar+mE/O7yATy8YB1De7Qr7/fsjcN4acVWeuZk890vdeeK/Ip5De7WlmuHdmPiOT08Tf+hrw+kLIqd7pFrBvHkokJO7dTK+4+owcPfGMTNTy3lskFd6hz2gn6duOrMrtw+po8TxzcHM2NRIX061lzLKhUPChf2z2Xx+i+4Y2yfqMb77jk92L7vCN8Z0d2XuJ664SxWbo2cC6IVvo+ICDPf20T3GB+0XhtJhZt68vPztaCgwLfp502ZA0DhtPG+zSMdPPTGpzz0xlpuOa8Xt43u7Xk8W77B8fT7m5j6wiquOrMr0y47PdnhGJ+JyBJVza/aP62LeowxxlRnid8YYzKMJX5jjMkwlviNMSbDWOI30UuBCgHGmJpZ4jee2X0R6cOO3ZnNt8QvIl1FZIGIfCwiH4nIJLd/OxGZJyJr3fe2dU3LGBMfdug24O8Zfwlwu6r2BYYCN4tIX2AK8Kaq9gLedLuNMcYkiG+JX1W3qepS9/N+YDXQGbgYmOEONgO4xK8YTHwl65GCxpj4SkgZv4jkAWcAi4GOqrrN/Wo7UL09BWeciSJSICIFRUVFiQjTmIxhB/HM5nviF5EWwGzgVlXdF/6dOu1FRNwCVXW6quaran5OTuR2b0xi2cXd1JfmDZQaj3xN/CLSCCfpz1TVF9zeO0Qk1/0+F9jpZwzGGGMq87NWjwCPA6tV9YGwr14GJrifJwAv+RWDMcaY6vxslnk4cC2wSkSWu/3uAqYBz4rI9cBG4EofYzDGRGD1+DObb4lfVd+m5mrD5/k1X+M/yxmpy67TGLA7d00U7MJg6rPaPAYs8RtjTMaxxG9MBrGiHgOW+I0xJuNY4jeeZTVwzhbFCvuNSWl+VudMGTNvOIsNuw4mO4zAu254Hjv2HeHGc3pENd7ksX0Y2cfuvg4Su8Sb2SzxA8NP7sDwkzskO4zAa964Ib+4uF/U49385ZN9iMbExP6sGayoxxhjMo4lfmMykN25m9ks8RuTQaykx4AlfmOMyTiW+I0xJsNY4jcmA1mbPZnNEr8xxmQYS/zGZCBrsyezWeI3xpgMY4nfmAxkZfyZzRK/MRnEGtgz4O/D1v8mIjtF5MOwfj8Tka0istx9XejX/I0xxkTm5xn/k8AFEfo/qKoD3derPs7fGFMTK+nJaL4lflX9D/CFX9M3xkTPCnoMJKeM/wcistItCmpb00AiMlFECkSkoKioKJHxGWNMWkt04n8E6AkMBLYB99c0oKpOV9V8Vc3PybGHeBhjTLwkNPGr6g5VLVXVMuAvwJBEzt8Y47Ai/syW0MQvIrlhnZcCH9Y0rDEm/qw2pwEfH70oIk8DI4EOIrIFuAcYKSIDcU44CoEb/Zq/McaYyHxL/Kp6dYTej/s1P2NM3bIaOKf8jbPs3s1MZg9bNyaDjO+fy+pt+7np3J7JDsUkkSV+YzJIw6wGTBl3SrLDMElm//eMMSbDWOI3xpgMY4nfGGMyjCV+Y4zJMJb4jTEmw1jiN8aYDGOJ3xhjMowlfmOMyTCiGvx2+kSkCNgY4+gdgF1xDMcvFmd8pUKcqRAjWJzxlsg4u6lqtXbtUyLx14eIFKhqfrLjqIvFGV+pEGcqxAgWZ7wFIU4r6jHGmAxjid8YYzJMJiT+6ckOwCOLM75SIc5UiBEsznhLepxpX8ZvjDGmskw44zfGGBPGEr8xxmSYtE78InKBiKwRkXUiMiUJ8/+biOwUkQ/D+rUTkXkistZ9b+v2FxH5gxvrShEZFDbOBHf4tSIyIc4xdhWRBSLysYh8JCKTAhpnUxF5X0RWuHH+3O3fXUQWu/HMEpHGbv8mbvc69/u8sGlNdfuvEZGx8YzTnX6WiCwTkVcCHGOhiKwSkeUiUuD2C9Q6d6ffRkSeF5FPRGS1iAwLWpwi0sddjqHXPhG5NWhxVqKqafkCsoDPgB5AY2AF0DfBMZwDDAI+DOv3W2CK+3kKcK/7+ULgNUCAocBit387YL373tb93DaOMeYCg9zPLYFPgb4BjFOAFu7nRsBid/7PAle5/R8FbnI/fx941P18FTDL/dzX3RaaAN3dbSQrzuv9NuAp4BW3O4gxFgIdqvQL1Dp35zEDuMH93BhoE8Q4w+LNArYD3QIdpx8TDcILGAbMDeueCkxNQhx5VE78a4Bc93MusMb9/BhwddXhgKuBx8L6VxrOh3hfAkYHOU6gObAUOAvnDsiGVdc5MBcY5n5u6A4nVbeD8OHiFFsX4E1gFPCKO89AxehOs5DqiT9Q6xxoDWzArYQS1DirxDYGeCfocaZzUU9nYHNY9xa3X7J1VNVt7uftQEf3c03xJux3uEUNZ+CcTQcuTrcIZTmwE5iHcyZcrKolEeZZHo/7/V6gfQLifAi4Eyhzu9sHMEYABV4XkSUiMtHtF7R13h0oAp5wi87+KiLZAYwz3FXA0+7nwMaZzok/8NQ5rAeiPq2ItABmA7eq6r7w74ISp6qWqupAnLPqIUCgnhouIl8BdqrqkmTH4sEIVR0EjANuFpFzwr8MyDpviFNU+oiqngEcxCkyKReQOAFwr91cBDxX9bsgxQnpnfi3Al3Duru4/ZJth4jkArjvO93+NcXr++8QkUY4SX+mqr4Q1DhDVLUYWIBTbNJGRBpGmGd5PO73rYHdPsc5HLhIRAqBZ3CKe34fsBgBUNWt7vtO4EWcA2nQ1vkWYIuqLna7n8c5EAQtzpBxwFJV3eF2BzXOtE78HwC93BoVjXH+gr2c5JjAiSF0tX4CTpl6qP+33Cv+Q4G97t/EucAYEWnr1goY4/aLCxER4HFgtao+EOA4c0Skjfu5Gc51iNU4B4DLa4gzFP/lwHz3rOtl4Cq3Rk13oBfwfjxiVNWpqtpFVfNwtrf5qnpNkGIEEJFsEWkZ+oyzrj4kYOtcVbcDm0Wkj9vrPODjoMUZ5moqinlC8QQxzvS9uOteHLkQp5bKZ8D/JGH+TwPbgOM4Zy/X45ThvgmsBd4A2rnDCvCwG+sqID9sOt8B1rmv6+Ic4wicv6ArgeXu68IAxnk6sMyN80Pgp27/HjhJcR3OX+wmbv+mbvc69/seYdP6Hzf+NcA4n9b9SCpq9QQqRjeeFe7ro9C+EbR17k5/IFDgrvd/4tR2CWKc2Tj/1lqH9QtcnKGXNdlgjDEZJp2LeowxxkRgid8YYzKMJX5jjMkwlviNMSbDWOI3xpgMY4nfGGMyjCV+Y4zJMP8PirymqzzOqX4AAAAASUVORK5CYII=\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}},{"output_type":"stream","name":"stdout","text":["Maximum length is:  50\n"]}]},{"cell_type":"code","metadata":{"id":"k6rUVKln1Ff5","executionInfo":{"status":"ok","timestamp":1638663923677,"user_tz":420,"elapsed":4,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["MAX_LEN = 55"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"_FqnJxC4u8XO","executionInfo":{"status":"ok","timestamp":1638663923677,"user_tz":420,"elapsed":4,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["def get_word_embeddings(data, MAX_LEN=55):\n","  input_ids=[]\n","  attention_masks = []\n","  for sent in data:\n","        encoded_sent = tokenizer.encode_plus(\n","            text=sent,  \n","            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n","            max_length=MAX_LEN,                  # Max length to truncate/pad\n","            pad_to_max_length=True,         # Pad sentence to max length\n","            #return_tensors='pt',           # Return PyTorch tensor\n","            return_attention_mask=True      # Return attention mask\n","            )\n","        \n","        # Add the outputs to the lists\n","        input_ids.append(encoded_sent.get('input_ids'))\n","        attention_masks.append(encoded_sent.get('attention_mask'))\n","\n","  # Convert lists to tensors\n","  input_ids = torch.tensor(input_ids)\n","  attention_masks = torch.tensor(attention_masks)\n","\n","  return input_ids, attention_masks"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jAxmAoDrvrWO","executionInfo":{"status":"ok","timestamp":1638663927446,"user_tz":420,"elapsed":3772,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"05b39cfa-f595-45f4-bde7-c33cb9c1857f"},"source":["# Run function `preprocessing_for_bert` on the train set and the validation set\n","train_inputs, train_masks = get_word_embeddings(data_train['inputs'])\n","val_inputs, val_masks = get_word_embeddings(data_valid['inputs'])\n","test_inputs, test_masks = get_word_embeddings(data_test['inputs'])"],"execution_count":29,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"id":"XbPJ2deo71p5","executionInfo":{"status":"ok","timestamp":1638663927447,"user_tz":420,"elapsed":20,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["data_train['target_str'] = data_train['target'].astype(str)\n","data_valid['target_str'] = data_valid['target'].astype(str)\n","data_test['target_str'] = data_test['target'].astype(str)"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j3dHakeNv66S","executionInfo":{"status":"ok","timestamp":1638663930300,"user_tz":420,"elapsed":2871,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"a0809909-8252-40e1-f3f8-0a5b3fe67aa6"},"source":["#convert lists to tensors\n","train_labels = get_word_embeddings(data_train['target_str'], 30)[0]\n","val_labels = get_word_embeddings(data_valid['target_str'], 30)[0]\n","test_labels = get_word_embeddings(data_test['target_str'], 30)[0]"],"execution_count":31,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EHUnt0aF8GQk","executionInfo":{"status":"ok","timestamp":1638663930300,"user_tz":420,"elapsed":7,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"63370a95-2822-445f-e5c0-ecbc1c519e57"},"source":["train_labels.shape\n","# doubt"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([7280, 30])"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MgegmxBtoqZV","executionInfo":{"status":"ok","timestamp":1638663930848,"user_tz":420,"elapsed":551,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"1d07e326-3982-4316-dfa3-53e7d8c9e28b"},"source":["train_labels"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[209, 335,  15,  ...,   0,   0,   0],\n","        [209, 335,  15,  ...,   0,   0,   0],\n","        [204, 335,  15,  ...,   0,   0,   0],\n","        ...,\n","        [204, 335,  15,  ...,   0,   0,   0],\n","        [209, 335,  15,  ...,   0,   0,   0],\n","        [209, 335,  15,  ...,   0,   0,   0]])"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ezRHf2e7rPL","executionInfo":{"status":"ok","timestamp":1638663930849,"user_tz":420,"elapsed":15,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"18236587-e8ea-4a61-bf53-4b7eebcad18c"},"source":["data_train['target']"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0              1 10e3 8 10e2 8 10e1 4 10e0\n","1              1 10e3 7 10e2 2 10e1 1 10e0\n","2              2 10e3 9 10e2 4 10e1 8 10e0\n","3              2 10e3 6 10e2 1 10e1 2 10e0\n","4              1 10e3 2 10e2 7 10e1 3 10e0\n","                       ...                \n","7275           2 10e3 7 10e2 5 10e1 0 10e0\n","7276    5 10e4 7 10e3 8 10e2 6 10e1 3 10e0\n","7277           2 10e3 3 10e2 5 10e1 3 10e0\n","7278           1 10e3 7 10e2 2 10e1 4 10e0\n","7279           1 10e3 7 10e2 7 10e1 3 10e0\n","Name: target, Length: 7280, dtype: object"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"osOohece1-tJ","executionInfo":{"status":"ok","timestamp":1638663930849,"user_tz":420,"elapsed":11,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"f3c47f0e-1a5b-40b9-de46-23e0521afb4e"},"source":["train_inputs.shape"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([7280, 55])"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","metadata":{"id":"GjDbWMIlv8Iq","executionInfo":{"status":"ok","timestamp":1638663930850,"user_tz":420,"elapsed":9,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n","\n","batch_size = 32\n","\n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_dataloader = DataLoader(train_data, shuffle = True, batch_size = batch_size)\n","\n","val_data = TensorDataset(val_inputs, val_masks, val_labels)\n","val_dataloader = DataLoader(val_data, shuffle = True, batch_size = batch_size)\n","\n","test_data = TensorDataset(test_inputs, test_masks, test_labels)\n","test_dataloader = DataLoader(test_data, shuffle = True, batch_size = batch_size)"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"q88he4D9Lw0L","executionInfo":{"status":"ok","timestamp":1638663930850,"user_tz":420,"elapsed":8,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["#  Optimizer\n","# https://huggingface.co/transformers/model_doc/t5.html#overview\n","optimizer = Adafactor(\n","    model.parameters(),\n","    lr=3e-4, # Initializing the learning Rate as suggested in the T5 official documentation\n","    eps=(1e-8, 1e-3),\n","    clip_threshold=1.0,\n","    decay_rate=-0.5,\n","    beta1=None,\n","    weight_decay=0.0,\n","    relative_step=False,\n","    scale_parameter=False,\n","    warmup_init=False\n",")"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"vQ9Bj35-56kQ","executionInfo":{"status":"ok","timestamp":1638649695121,"user_tz":420,"elapsed":3,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["# Changing the directory to store the model there.\n","# print(os.getcwd())\n","# os.chdir('/content/drive/MyDrive/Colab Notebooks/new_run')\n","# print(os.getcwd())"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"COfO6s0z3kit","executionInfo":{"status":"ok","timestamp":1638663574764,"user_tz":420,"elapsed":995,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"b55c9654-a749-4b0d-dcf5-796aec019525"},"source":["# Loading the configuration file for 't5-base' model\n","!wget https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-12-05 00:19:40--  https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json\n","Resolving s3.amazonaws.com (s3.amazonaws.com)... 52.216.105.197\n","Connecting to s3.amazonaws.com (s3.amazonaws.com)|52.216.105.197|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1199 (1.2K) [application/json]\n","Saving to: ‘t5-base-config.json’\n","\n","t5-base-config.json 100%[===================>]   1.17K  --.-KB/s    in 0s      \n","\n","2021-12-05 00:19:40 (35.7 MB/s) - ‘t5-base-config.json’ saved [1199/1199]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"MYMdr0Xu92Yv","executionInfo":{"status":"ok","timestamp":1638663945416,"user_tz":420,"elapsed":467,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["from IPython.display import HTML, display\n","\n","# Setting the progress, with html as UI.\n","def progress(loss, value, max=100):\n","    return HTML(\"\"\" Batch loss :{loss}\n","        <progress\n","            value='{value}'\n","            max='{max}',\n","            style='width: 100%'\n","        >\n","            {value}\n","        </progress>\n","    \"\"\".format(loss=loss,value=value, max=max))"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"_C2w5YpsN7C1","executionInfo":{"status":"ok","timestamp":1638663942237,"user_tz":420,"elapsed":217,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["def evaluate(model, val_dataloader):\n","    \"\"\"After the completion of each training epoch, measure the model's performance\n","    on our validation set.\n","    \"\"\"\n","    # Put the model into the evaluation mode. The dropout layers are disabled during\n","    # the test time.\n","    model.eval()\n","\n","    # Tracking variables\n","    v_accuracy = []\n","    v_loss = []\n","\n","    # For each batch in our validation set...\n","    for batch in val_dataloader:\n","        # Load batch to GPU\n","        \n","        v_input_ids, v_attn_mask, v_labels = tuple(t.to(device) for t in batch)\n","\n","        # print(v_input_ids.shape, v_labels.shape)\n","\n","        val_outputs = model.generate(input_ids=v_input_ids, attention_mask=v_attn_mask)\n","\n","        val_preds = [ tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","            for output in val_outputs]\n","\n","        val_labels = [ tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n","            for output in v_labels]\n","        \n","        # v_loss.append(val_outputs.loss)\n","\n","        # Get the predictions\n","        # print(val_outputs.logits.shape)\n","        # val_preds = torch.argmax(val_outputs.logits, dim=1).flatten()\n","        # print(val_preds, val_labels)\n","        # Calculate the accuracy rate\n","\n","        val_preds = np.array(val_preds)\n","        val_labels = np.array(val_labels)\n","        accuracy = ((val_preds == val_labels).sum() / len(val_labels)) * 100\n","        v_accuracy.append(accuracy)\n","\n","    # Compute the average accuracy and loss over the validation set.\n","    # v_loss = np.mean(v_loss)\n","    v_accuracy = np.mean(v_accuracy)\n","\n","    return v_accuracy"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wlCbmfAQ-CWJ","outputId":"568d5906-b28e-46b7-e6e8-9275be5854f1"},"source":["import gc\n","\n","val_acc = 0\n","train_accuracy = 0\n","\n","# Sets the module in training mode\n","model.train()\n","\n","for epoch in range(1,num_of_epochs+1):\n","  print('Running epoch: {}'.format(epoch))\n","  running_loss=0\n","  # out = display(progress(1, num_of_batches+1), display_id=True)\n","  i =0 \n","  for batch in train_dataloader:\n","    \n","    input_ids, attn_mask, labels = tuple(t.to(device) for t in batch)\n","\n","    # clear out the gradients of all Variables \n","    optimizer.zero_grad()\n","    gc.collect()\n","    torch.cuda.empty_cache()\n","\n","    # Forward propogation\n","    # print(model(input_ids=input_ids, attention_mask=attn_mask, labels=labels))\n","    outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n","    \n","    loss = outputs.loss\n","    loss_num=loss.item()\n","    logits = outputs.logits\n","    running_loss+=loss_num\n","    # out.update(progress(loss_num,i, num_of_batches+1))\n","\n","    # calculating the gradients\n","    loss.backward()\n","\n","    # updating the params\n","    optimizer.step()\n","\n","    print(\"Epoch \", epoch, \"Batch \", i, \"/\", len(train_dataloader), \" Training Loss \", loss_num)\n","    i += 1\n","\n","  running_loss = running_loss/len(train_dataloader)\n","  # v_input_ids, v_attn_mask, v_labels = tuple(t.to(device) for t in data_valid)\n","  \n","  curr_accuracy = evaluate(model, val_dataloader)\n","\n","  # print('Epoch: {} , Running loss: {}'.format(epoch,running_loss))\n","  print(f\"{epoch + 1:^7} | {'-':^7} | {running_loss:^12.6f} | {curr_accuracy:^9.6f}\")\n","  print(\"-\"*70)\n","\n","  if curr_accuracy > val_acc:\n","    val_acc = curr_accuracy\n","    # Saving the best model\n","    torch.save(model.state_dict(),'FineTune_10e_Model_32eph_v2_10perc_bias.bin')\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n","Epoch  4 Batch  78 / 228  Training Loss  0.014985546469688416\n","Epoch  4 Batch  79 / 228  Training Loss  0.004447310231626034\n","Epoch  4 Batch  80 / 228  Training Loss  0.03369537740945816\n","Epoch  4 Batch  81 / 228  Training Loss  0.017536189407110214\n","Epoch  4 Batch  82 / 228  Training Loss  0.0069676837883889675\n","Epoch  4 Batch  83 / 228  Training Loss  0.009135553613305092\n","Epoch  4 Batch  84 / 228  Training Loss  0.009853671304881573\n","Epoch  4 Batch  85 / 228  Training Loss  0.00746605871245265\n","Epoch  4 Batch  86 / 228  Training Loss  0.006411363836377859\n","Epoch  4 Batch  87 / 228  Training Loss  0.00914064422249794\n","Epoch  4 Batch  88 / 228  Training Loss  0.01623951643705368\n","Epoch  4 Batch  89 / 228  Training Loss  0.01534426398575306\n","Epoch  4 Batch  90 / 228  Training Loss  0.02411000244319439\n","Epoch  4 Batch  91 / 228  Training Loss  0.015629298985004425\n","Epoch  4 Batch  92 / 228  Training Loss  0.010866420343518257\n","Epoch  4 Batch  93 / 228  Training Loss  0.01972162537276745\n","Epoch  4 Batch  94 / 228  Training Loss  0.013864991255104542\n","Epoch  4 Batch  95 / 228  Training Loss  0.006345723289996386\n","Epoch  4 Batch  96 / 228  Training Loss  0.008941913023591042\n","Epoch  4 Batch  97 / 228  Training Loss  0.018449150025844574\n","Epoch  4 Batch  98 / 228  Training Loss  0.019081266596913338\n","Epoch  4 Batch  99 / 228  Training Loss  0.011997729539871216\n","Epoch  4 Batch  100 / 228  Training Loss  0.009532040916383266\n","Epoch  4 Batch  101 / 228  Training Loss  0.0056925672106444836\n","Epoch  4 Batch  102 / 228  Training Loss  0.010027983225882053\n","Epoch  4 Batch  103 / 228  Training Loss  0.01848016306757927\n","Epoch  4 Batch  104 / 228  Training Loss  0.012532761320471764\n","Epoch  4 Batch  105 / 228  Training Loss  0.015216600149869919\n","Epoch  4 Batch  106 / 228  Training Loss  0.013903154991567135\n","Epoch  4 Batch  107 / 228  Training Loss  0.015682341530919075\n","Epoch  4 Batch  108 / 228  Training Loss  0.010722717270255089\n","Epoch  4 Batch  109 / 228  Training Loss  0.004411480389535427\n","Epoch  4 Batch  110 / 228  Training Loss  0.007927472703158855\n","Epoch  4 Batch  111 / 228  Training Loss  0.01888364739716053\n","Epoch  4 Batch  112 / 228  Training Loss  0.013352029956877232\n","Epoch  4 Batch  113 / 228  Training Loss  0.010289224795997143\n","Epoch  4 Batch  114 / 228  Training Loss  0.008411970920860767\n","Epoch  4 Batch  115 / 228  Training Loss  0.010948275215923786\n","Epoch  4 Batch  116 / 228  Training Loss  0.01017796341329813\n","Epoch  4 Batch  117 / 228  Training Loss  0.025678912177681923\n","Epoch  4 Batch  118 / 228  Training Loss  0.02194925770163536\n","Epoch  4 Batch  119 / 228  Training Loss  0.008500291965901852\n","Epoch  4 Batch  120 / 228  Training Loss  0.009975564666092396\n","Epoch  4 Batch  121 / 228  Training Loss  0.015965377911925316\n","Epoch  4 Batch  122 / 228  Training Loss  0.01004783995449543\n","Epoch  4 Batch  123 / 228  Training Loss  0.010156581178307533\n","Epoch  4 Batch  124 / 228  Training Loss  0.00957195833325386\n","Epoch  4 Batch  125 / 228  Training Loss  0.011121238581836224\n","Epoch  4 Batch  126 / 228  Training Loss  0.007401329465210438\n","Epoch  4 Batch  127 / 228  Training Loss  0.007780434098094702\n","Epoch  4 Batch  128 / 228  Training Loss  0.01033281721174717\n","Epoch  4 Batch  129 / 228  Training Loss  0.005698561202734709\n","Epoch  4 Batch  130 / 228  Training Loss  0.006747566629201174\n","Epoch  4 Batch  131 / 228  Training Loss  0.007259828504174948\n","Epoch  4 Batch  132 / 228  Training Loss  0.007094590459018946\n","Epoch  4 Batch  133 / 228  Training Loss  0.009757312014698982\n","Epoch  4 Batch  134 / 228  Training Loss  0.011076360940933228\n","Epoch  4 Batch  135 / 228  Training Loss  0.010372363962233067\n","Epoch  4 Batch  136 / 228  Training Loss  0.005977095570415258\n","Epoch  4 Batch  137 / 228  Training Loss  0.00449996255338192\n","Epoch  4 Batch  138 / 228  Training Loss  0.010405107401311398\n","Epoch  4 Batch  139 / 228  Training Loss  0.013167861849069595\n","Epoch  4 Batch  140 / 228  Training Loss  0.0159537922590971\n","Epoch  4 Batch  141 / 228  Training Loss  0.01721571572124958\n","Epoch  4 Batch  142 / 228  Training Loss  0.004503053613007069\n","Epoch  4 Batch  143 / 228  Training Loss  0.004842692520469427\n","Epoch  4 Batch  144 / 228  Training Loss  0.006149470806121826\n","Epoch  4 Batch  145 / 228  Training Loss  0.022623073309659958\n","Epoch  4 Batch  146 / 228  Training Loss  0.027173979207873344\n","Epoch  4 Batch  147 / 228  Training Loss  0.012029164470732212\n","Epoch  4 Batch  148 / 228  Training Loss  0.009207477793097496\n","Epoch  4 Batch  149 / 228  Training Loss  0.0076449355110526085\n","Epoch  4 Batch  150 / 228  Training Loss  0.00575173320248723\n","Epoch  4 Batch  151 / 228  Training Loss  0.00410474743694067\n","Epoch  4 Batch  152 / 228  Training Loss  0.01472342200577259\n","Epoch  4 Batch  153 / 228  Training Loss  0.0047674113884568214\n","Epoch  4 Batch  154 / 228  Training Loss  0.012334110215306282\n","Epoch  4 Batch  155 / 228  Training Loss  0.010484701953828335\n","Epoch  4 Batch  156 / 228  Training Loss  0.016305500641465187\n","Epoch  4 Batch  157 / 228  Training Loss  0.019372018054127693\n","Epoch  4 Batch  158 / 228  Training Loss  0.009863987565040588\n","Epoch  4 Batch  159 / 228  Training Loss  0.01579217053949833\n","Epoch  4 Batch  160 / 228  Training Loss  0.010975630953907967\n","Epoch  4 Batch  161 / 228  Training Loss  0.0076050222851336\n","Epoch  4 Batch  162 / 228  Training Loss  0.009604905731976032\n","Epoch  4 Batch  163 / 228  Training Loss  0.00889009889215231\n","Epoch  4 Batch  164 / 228  Training Loss  0.006489675957709551\n","Epoch  4 Batch  165 / 228  Training Loss  0.02271685190498829\n","Epoch  4 Batch  166 / 228  Training Loss  0.005570858251303434\n","Epoch  4 Batch  167 / 228  Training Loss  0.010253921151161194\n","Epoch  4 Batch  168 / 228  Training Loss  0.01044544018805027\n","Epoch  4 Batch  169 / 228  Training Loss  0.007418563589453697\n","Epoch  4 Batch  170 / 228  Training Loss  0.013507620431482792\n","Epoch  4 Batch  171 / 228  Training Loss  0.005619794595986605\n","Epoch  4 Batch  172 / 228  Training Loss  0.017128579318523407\n","Epoch  4 Batch  173 / 228  Training Loss  0.007111871615052223\n","Epoch  4 Batch  174 / 228  Training Loss  0.005981564521789551\n","Epoch  4 Batch  175 / 228  Training Loss  0.007637078873813152\n","Epoch  4 Batch  176 / 228  Training Loss  0.007652171887457371\n","Epoch  4 Batch  177 / 228  Training Loss  0.01617741771042347\n","Epoch  4 Batch  178 / 228  Training Loss  0.008662763983011246\n","Epoch  4 Batch  179 / 228  Training Loss  0.007438712753355503\n","Epoch  4 Batch  180 / 228  Training Loss  0.005018909927457571\n","Epoch  4 Batch  181 / 228  Training Loss  0.00869970303028822\n","Epoch  4 Batch  182 / 228  Training Loss  0.006096147932112217\n","Epoch  4 Batch  183 / 228  Training Loss  0.0029170201160013676\n","Epoch  4 Batch  184 / 228  Training Loss  0.0041866423562169075\n","Epoch  4 Batch  185 / 228  Training Loss  0.005475261714309454\n","Epoch  4 Batch  186 / 228  Training Loss  0.010355567559599876\n","Epoch  4 Batch  187 / 228  Training Loss  0.006437809206545353\n","Epoch  4 Batch  188 / 228  Training Loss  0.008786752820014954\n","Epoch  4 Batch  189 / 228  Training Loss  0.004571331664919853\n","Epoch  4 Batch  190 / 228  Training Loss  0.007854231633245945\n","Epoch  4 Batch  191 / 228  Training Loss  0.005558773409575224\n","Epoch  4 Batch  192 / 228  Training Loss  0.005130758509039879\n","Epoch  4 Batch  193 / 228  Training Loss  0.006425939500331879\n","Epoch  4 Batch  194 / 228  Training Loss  0.009038487449288368\n","Epoch  4 Batch  195 / 228  Training Loss  0.008937995880842209\n","Epoch  4 Batch  196 / 228  Training Loss  0.017528150230646133\n","Epoch  4 Batch  197 / 228  Training Loss  0.01186061929911375\n","Epoch  4 Batch  198 / 228  Training Loss  0.0029206147883087397\n","Epoch  4 Batch  199 / 228  Training Loss  0.011641886085271835\n","Epoch  4 Batch  200 / 228  Training Loss  0.012342093512415886\n","Epoch  4 Batch  201 / 228  Training Loss  0.02462099678814411\n","Epoch  4 Batch  202 / 228  Training Loss  0.009880953468382359\n","Epoch  4 Batch  203 / 228  Training Loss  0.01165018044412136\n","Epoch  4 Batch  204 / 228  Training Loss  0.007396625354886055\n","Epoch  4 Batch  205 / 228  Training Loss  0.0075061568059027195\n","Epoch  4 Batch  206 / 228  Training Loss  0.009162068367004395\n","Epoch  4 Batch  207 / 228  Training Loss  0.005555140785872936\n","Epoch  4 Batch  208 / 228  Training Loss  0.014260984025895596\n","Epoch  4 Batch  209 / 228  Training Loss  0.013804074376821518\n","Epoch  4 Batch  210 / 228  Training Loss  0.01938127540051937\n","Epoch  4 Batch  211 / 228  Training Loss  0.04888518527150154\n","Epoch  4 Batch  212 / 228  Training Loss  0.05149432271718979\n","Epoch  4 Batch  213 / 228  Training Loss  0.018490860238671303\n","Epoch  4 Batch  214 / 228  Training Loss  0.015351940877735615\n","Epoch  4 Batch  215 / 228  Training Loss  0.020614154636859894\n","Epoch  4 Batch  216 / 228  Training Loss  0.016329413279891014\n","Epoch  4 Batch  217 / 228  Training Loss  0.013416120782494545\n","Epoch  4 Batch  218 / 228  Training Loss  0.011415434069931507\n","Epoch  4 Batch  219 / 228  Training Loss  0.00871679000556469\n","Epoch  4 Batch  220 / 228  Training Loss  0.008841860108077526\n","Epoch  4 Batch  221 / 228  Training Loss  0.00841583963483572\n","Epoch  4 Batch  222 / 228  Training Loss  0.007425662595778704\n","Epoch  4 Batch  223 / 228  Training Loss  0.003443817375227809\n","Epoch  4 Batch  224 / 228  Training Loss  0.009481156244874\n","Epoch  4 Batch  225 / 228  Training Loss  0.005968326702713966\n","Epoch  4 Batch  226 / 228  Training Loss  0.009787661954760551\n","Epoch  4 Batch  227 / 228  Training Loss  0.002377802273258567\n","   5    |    -    |   0.011818   | 87.195122\n","----------------------------------------------------------------------\n","Running epoch: 5\n","Epoch  5 Batch  0 / 228  Training Loss  0.006665317807346582\n","Epoch  5 Batch  1 / 228  Training Loss  0.005191165488213301\n","Epoch  5 Batch  2 / 228  Training Loss  0.007292300928384066\n","Epoch  5 Batch  3 / 228  Training Loss  0.007573510054498911\n","Epoch  5 Batch  4 / 228  Training Loss  0.0070443348959088326\n","Epoch  5 Batch  5 / 228  Training Loss  0.006309478543698788\n","Epoch  5 Batch  6 / 228  Training Loss  0.004526031203567982\n","Epoch  5 Batch  7 / 228  Training Loss  0.00499138655140996\n","Epoch  5 Batch  8 / 228  Training Loss  0.004455076064914465\n","Epoch  5 Batch  9 / 228  Training Loss  0.0032314297277480364\n","Epoch  5 Batch  10 / 228  Training Loss  0.006664103828370571\n","Epoch  5 Batch  11 / 228  Training Loss  0.002409270964562893\n","Epoch  5 Batch  12 / 228  Training Loss  0.007063179276883602\n","Epoch  5 Batch  13 / 228  Training Loss  0.008784487843513489\n","Epoch  5 Batch  14 / 228  Training Loss  0.002890106290578842\n","Epoch  5 Batch  15 / 228  Training Loss  0.005357012152671814\n","Epoch  5 Batch  16 / 228  Training Loss  0.002836658153682947\n","Epoch  5 Batch  17 / 228  Training Loss  0.002744738943874836\n","Epoch  5 Batch  18 / 228  Training Loss  0.002739198273047805\n","Epoch  5 Batch  19 / 228  Training Loss  0.004812229890376329\n","Epoch  5 Batch  20 / 228  Training Loss  0.0027626887895166874\n","Epoch  5 Batch  21 / 228  Training Loss  0.0037427362985908985\n","Epoch  5 Batch  22 / 228  Training Loss  0.002475273096933961\n","Epoch  5 Batch  23 / 228  Training Loss  0.0075776842422783375\n","Epoch  5 Batch  24 / 228  Training Loss  0.0015778351807966828\n","Epoch  5 Batch  25 / 228  Training Loss  0.010347520932555199\n","Epoch  5 Batch  26 / 228  Training Loss  0.0058715492486953735\n","Epoch  5 Batch  27 / 228  Training Loss  0.003685717238113284\n","Epoch  5 Batch  28 / 228  Training Loss  0.005967854987829924\n","Epoch  5 Batch  29 / 228  Training Loss  0.007173553574830294\n","Epoch  5 Batch  30 / 228  Training Loss  0.0023110497277230024\n","Epoch  5 Batch  31 / 228  Training Loss  0.005429768934845924\n","Epoch  5 Batch  32 / 228  Training Loss  0.005381026305258274\n","Epoch  5 Batch  33 / 228  Training Loss  0.002562848385423422\n","Epoch  5 Batch  34 / 228  Training Loss  0.0022393018007278442\n","Epoch  5 Batch  35 / 228  Training Loss  0.002908840775489807\n","Epoch  5 Batch  36 / 228  Training Loss  0.004912895150482655\n","Epoch  5 Batch  37 / 228  Training Loss  0.003048497950658202\n","Epoch  5 Batch  38 / 228  Training Loss  0.005902315955609083\n","Epoch  5 Batch  39 / 228  Training Loss  0.00900891050696373\n","Epoch  5 Batch  40 / 228  Training Loss  0.0030269704293459654\n","Epoch  5 Batch  41 / 228  Training Loss  0.003090063575655222\n","Epoch  5 Batch  42 / 228  Training Loss  0.0048248437233269215\n","Epoch  5 Batch  43 / 228  Training Loss  0.00423577893525362\n","Epoch  5 Batch  44 / 228  Training Loss  0.0031229034066200256\n","Epoch  5 Batch  45 / 228  Training Loss  0.0029505242127925158\n","Epoch  5 Batch  46 / 228  Training Loss  0.009558260440826416\n","Epoch  5 Batch  47 / 228  Training Loss  0.01091375108808279\n","Epoch  5 Batch  48 / 228  Training Loss  0.008220944553613663\n","Epoch  5 Batch  49 / 228  Training Loss  0.004317880608141422\n","Epoch  5 Batch  50 / 228  Training Loss  0.0062019857577979565\n","Epoch  5 Batch  51 / 228  Training Loss  0.0023076471406966448\n","Epoch  5 Batch  52 / 228  Training Loss  0.0040989825502038\n","Epoch  5 Batch  53 / 228  Training Loss  0.004985641222447157\n","Epoch  5 Batch  54 / 228  Training Loss  0.007203823421150446\n","Epoch  5 Batch  55 / 228  Training Loss  0.0034459431190043688\n","Epoch  5 Batch  56 / 228  Training Loss  0.0033177384175360203\n","Epoch  5 Batch  57 / 228  Training Loss  0.005445966497063637\n","Epoch  5 Batch  58 / 228  Training Loss  0.0032861174549907446\n","Epoch  5 Batch  59 / 228  Training Loss  0.009621083736419678\n","Epoch  5 Batch  60 / 228  Training Loss  0.0027438662946224213\n","Epoch  5 Batch  61 / 228  Training Loss  0.0054223849438130856\n","Epoch  5 Batch  62 / 228  Training Loss  0.0046942951157689095\n","Epoch  5 Batch  63 / 228  Training Loss  0.003981573041528463\n","Epoch  5 Batch  64 / 228  Training Loss  0.005135551560670137\n","Epoch  5 Batch  65 / 228  Training Loss  0.006357947830110788\n","Epoch  5 Batch  66 / 228  Training Loss  0.0025604155380278826\n","Epoch  5 Batch  67 / 228  Training Loss  0.004571651574224234\n","Epoch  5 Batch  68 / 228  Training Loss  0.006577745545655489\n","Epoch  5 Batch  69 / 228  Training Loss  0.01619873195886612\n","Epoch  5 Batch  70 / 228  Training Loss  0.016429899260401726\n","Epoch  5 Batch  71 / 228  Training Loss  0.011540502309799194\n","Epoch  5 Batch  72 / 228  Training Loss  0.011709659360349178\n","Epoch  5 Batch  73 / 228  Training Loss  0.003023032331839204\n","Epoch  5 Batch  74 / 228  Training Loss  0.005844911094754934\n","Epoch  5 Batch  75 / 228  Training Loss  0.005013244226574898\n","Epoch  5 Batch  76 / 228  Training Loss  0.001553937210701406\n","Epoch  5 Batch  77 / 228  Training Loss  0.002862734254449606\n","Epoch  5 Batch  78 / 228  Training Loss  0.004150027874857187\n","Epoch  5 Batch  79 / 228  Training Loss  0.00281306030228734\n","Epoch  5 Batch  80 / 228  Training Loss  0.0028850228991359472\n","Epoch  5 Batch  81 / 228  Training Loss  0.0022708994802087545\n","Epoch  5 Batch  82 / 228  Training Loss  0.014122986234724522\n","Epoch  5 Batch  83 / 228  Training Loss  0.006142272148281336\n","Epoch  5 Batch  84 / 228  Training Loss  0.0037153803277760744\n","Epoch  5 Batch  85 / 228  Training Loss  0.010173213668167591\n","Epoch  5 Batch  86 / 228  Training Loss  0.006261385045945644\n","Epoch  5 Batch  87 / 228  Training Loss  0.005739809479564428\n","Epoch  5 Batch  88 / 228  Training Loss  0.004381063859909773\n","Epoch  5 Batch  89 / 228  Training Loss  0.0032110235188156366\n","Epoch  5 Batch  90 / 228  Training Loss  0.012464449740946293\n","Epoch  5 Batch  91 / 228  Training Loss  0.0073792473413050175\n","Epoch  5 Batch  92 / 228  Training Loss  0.00464186305180192\n","Epoch  5 Batch  93 / 228  Training Loss  0.006993693299591541\n","Epoch  5 Batch  94 / 228  Training Loss  0.010012342594563961\n","Epoch  5 Batch  95 / 228  Training Loss  0.0033568255603313446\n","Epoch  5 Batch  96 / 228  Training Loss  0.01127009466290474\n","Epoch  5 Batch  97 / 228  Training Loss  0.02484804391860962\n","Epoch  5 Batch  98 / 228  Training Loss  0.016559580340981483\n","Epoch  5 Batch  99 / 228  Training Loss  0.014585049822926521\n","Epoch  5 Batch  100 / 228  Training Loss  0.004201097879558802\n","Epoch  5 Batch  101 / 228  Training Loss  0.009214543737471104\n","Epoch  5 Batch  102 / 228  Training Loss  0.009354442358016968\n","Epoch  5 Batch  103 / 228  Training Loss  0.005370176862925291\n","Epoch  5 Batch  104 / 228  Training Loss  0.0050571635365486145\n","Epoch  5 Batch  105 / 228  Training Loss  0.003926425240933895\n","Epoch  5 Batch  106 / 228  Training Loss  0.005775506142526865\n","Epoch  5 Batch  107 / 228  Training Loss  0.005540926940739155\n","Epoch  5 Batch  108 / 228  Training Loss  0.007194108795374632\n","Epoch  5 Batch  109 / 228  Training Loss  0.002609223360195756\n","Epoch  5 Batch  110 / 228  Training Loss  0.0027421601116657257\n","Epoch  5 Batch  111 / 228  Training Loss  0.004964002873748541\n","Epoch  5 Batch  112 / 228  Training Loss  0.002965443767607212\n","Epoch  5 Batch  113 / 228  Training Loss  0.007303762249648571\n","Epoch  5 Batch  114 / 228  Training Loss  0.008883089758455753\n","Epoch  5 Batch  115 / 228  Training Loss  0.003878992050886154\n","Epoch  5 Batch  116 / 228  Training Loss  0.0034621660597622395\n","Epoch  5 Batch  117 / 228  Training Loss  0.0028810619842261076\n","Epoch  5 Batch  118 / 228  Training Loss  0.00208795303478837\n","Epoch  5 Batch  119 / 228  Training Loss  0.004757944028824568\n","Epoch  5 Batch  120 / 228  Training Loss  0.0031666941940784454\n","Epoch  5 Batch  121 / 228  Training Loss  0.0015321833780035377\n","Epoch  5 Batch  122 / 228  Training Loss  0.0028265465516597033\n","Epoch  5 Batch  123 / 228  Training Loss  0.007747097872197628\n","Epoch  5 Batch  124 / 228  Training Loss  0.007093737833201885\n","Epoch  5 Batch  125 / 228  Training Loss  0.009990056045353413\n","Epoch  5 Batch  126 / 228  Training Loss  0.005433165933936834\n","Epoch  5 Batch  127 / 228  Training Loss  0.0037575617898255587\n","Epoch  5 Batch  128 / 228  Training Loss  0.005280415993183851\n","Epoch  5 Batch  129 / 228  Training Loss  0.0055724554695189\n","Epoch  5 Batch  130 / 228  Training Loss  0.005761966574937105\n","Epoch  5 Batch  131 / 228  Training Loss  0.0070901489816606045\n","Epoch  5 Batch  132 / 228  Training Loss  0.00272320955991745\n","Epoch  5 Batch  133 / 228  Training Loss  0.0066408272832632065\n","Epoch  5 Batch  134 / 228  Training Loss  0.01280844397842884\n","Epoch  5 Batch  135 / 228  Training Loss  0.0030918524134904146\n","Epoch  5 Batch  136 / 228  Training Loss  0.004446844570338726\n","Epoch  5 Batch  137 / 228  Training Loss  0.008778181858360767\n","Epoch  5 Batch  138 / 228  Training Loss  0.003053148975595832\n","Epoch  5 Batch  139 / 228  Training Loss  0.0038628445472568274\n","Epoch  5 Batch  140 / 228  Training Loss  0.005624566692858934\n","Epoch  5 Batch  141 / 228  Training Loss  0.002901938511058688\n","Epoch  5 Batch  142 / 228  Training Loss  0.0023991428315639496\n","Epoch  5 Batch  143 / 228  Training Loss  0.0028073114808648825\n","Epoch  5 Batch  144 / 228  Training Loss  0.003278044518083334\n","Epoch  5 Batch  145 / 228  Training Loss  0.0033451521303504705\n","Epoch  5 Batch  146 / 228  Training Loss  0.002499689580872655\n","Epoch  5 Batch  147 / 228  Training Loss  0.0022237305529415607\n","Epoch  5 Batch  148 / 228  Training Loss  0.0033120831940323114\n","Epoch  5 Batch  149 / 228  Training Loss  0.005163645837455988\n","Epoch  5 Batch  150 / 228  Training Loss  0.005178165156394243\n","Epoch  5 Batch  151 / 228  Training Loss  0.0029992631170898676\n","Epoch  5 Batch  152 / 228  Training Loss  0.005694943480193615\n","Epoch  5 Batch  153 / 228  Training Loss  0.0031641991809010506\n","Epoch  5 Batch  154 / 228  Training Loss  0.00225738063454628\n","Epoch  5 Batch  155 / 228  Training Loss  0.003815904725342989\n","Epoch  5 Batch  156 / 228  Training Loss  0.0024021121207624674\n","Epoch  5 Batch  157 / 228  Training Loss  0.0018870141357183456\n","Epoch  5 Batch  158 / 228  Training Loss  0.004208202939480543\n","Epoch  5 Batch  159 / 228  Training Loss  0.0023801943752914667\n","Epoch  5 Batch  160 / 228  Training Loss  0.002871088217943907\n","Epoch  5 Batch  161 / 228  Training Loss  0.006234701257199049\n","Epoch  5 Batch  162 / 228  Training Loss  0.0027042666915804148\n","Epoch  5 Batch  163 / 228  Training Loss  0.003459047991782427\n","Epoch  5 Batch  164 / 228  Training Loss  0.004948215093463659\n","Epoch  5 Batch  165 / 228  Training Loss  0.0027408571913838387\n","Epoch  5 Batch  166 / 228  Training Loss  0.0031176558695733547\n","Epoch  5 Batch  167 / 228  Training Loss  0.0021743751130998135\n","Epoch  5 Batch  168 / 228  Training Loss  0.0029377921018749475\n","Epoch  5 Batch  169 / 228  Training Loss  0.005233574192970991\n","Epoch  5 Batch  170 / 228  Training Loss  0.0095736188814044\n","Epoch  5 Batch  171 / 228  Training Loss  0.006166405975818634\n","Epoch  5 Batch  172 / 228  Training Loss  0.007449348922818899\n","Epoch  5 Batch  173 / 228  Training Loss  0.0025306486058980227\n","Epoch  5 Batch  174 / 228  Training Loss  0.0028582776430994272\n","Epoch  5 Batch  175 / 228  Training Loss  0.0026439267676323652\n","Epoch  5 Batch  176 / 228  Training Loss  0.0018816888332366943\n","Epoch  5 Batch  177 / 228  Training Loss  0.006006364244967699\n","Epoch  5 Batch  178 / 228  Training Loss  0.005598616786301136\n","Epoch  5 Batch  179 / 228  Training Loss  0.00783032551407814\n","Epoch  5 Batch  180 / 228  Training Loss  0.0033891499042510986\n","Epoch  5 Batch  181 / 228  Training Loss  0.003549872897565365\n","Epoch  5 Batch  182 / 228  Training Loss  0.0017350264824926853\n","Epoch  5 Batch  183 / 228  Training Loss  0.008678542450070381\n","Epoch  5 Batch  184 / 228  Training Loss  0.008082548156380653\n","Epoch  5 Batch  185 / 228  Training Loss  0.002943722065538168\n","Epoch  5 Batch  186 / 228  Training Loss  0.010543032549321651\n","Epoch  5 Batch  187 / 228  Training Loss  0.005192857701331377\n","Epoch  5 Batch  188 / 228  Training Loss  0.008325298316776752\n","Epoch  5 Batch  189 / 228  Training Loss  0.00814720056951046\n","Epoch  5 Batch  190 / 228  Training Loss  0.00743074668571353\n","Epoch  5 Batch  191 / 228  Training Loss  0.015488446690142155\n","Epoch  5 Batch  192 / 228  Training Loss  0.008732845075428486\n","Epoch  5 Batch  193 / 228  Training Loss  0.003321134950965643\n","Epoch  5 Batch  194 / 228  Training Loss  0.004178269766271114\n","Epoch  5 Batch  195 / 228  Training Loss  0.007275044918060303\n","Epoch  5 Batch  196 / 228  Training Loss  0.009828472509980202\n","Epoch  5 Batch  197 / 228  Training Loss  0.005080589558929205\n","Epoch  5 Batch  198 / 228  Training Loss  0.0033263040240854025\n","Epoch  5 Batch  199 / 228  Training Loss  0.006659542676061392\n","Epoch  5 Batch  200 / 228  Training Loss  0.0027634764555841684\n","Epoch  5 Batch  201 / 228  Training Loss  0.0036870280746370554\n","Epoch  5 Batch  202 / 228  Training Loss  0.002598167397081852\n","Epoch  5 Batch  203 / 228  Training Loss  0.002754566725343466\n","Epoch  5 Batch  204 / 228  Training Loss  0.003558016149327159\n","Epoch  5 Batch  205 / 228  Training Loss  0.004208125174045563\n","Epoch  5 Batch  206 / 228  Training Loss  0.0034671283792704344\n","Epoch  5 Batch  207 / 228  Training Loss  0.0042186821810901165\n","Epoch  5 Batch  208 / 228  Training Loss  0.003014638787135482\n","Epoch  5 Batch  209 / 228  Training Loss  0.0032890415750443935\n","Epoch  5 Batch  210 / 228  Training Loss  0.0027283020317554474\n","Epoch  5 Batch  211 / 228  Training Loss  0.005248823668807745\n","Epoch  5 Batch  212 / 228  Training Loss  0.003968102857470512\n","Epoch  5 Batch  213 / 228  Training Loss  0.0083775008097291\n","Epoch  5 Batch  214 / 228  Training Loss  0.0232057087123394\n","Epoch  5 Batch  215 / 228  Training Loss  0.006460412871092558\n","Epoch  5 Batch  216 / 228  Training Loss  0.0076362816616892815\n","Epoch  5 Batch  217 / 228  Training Loss  0.003166629932820797\n","Epoch  5 Batch  218 / 228  Training Loss  0.00415151659399271\n","Epoch  5 Batch  219 / 228  Training Loss  0.0027989824302494526\n","Epoch  5 Batch  220 / 228  Training Loss  0.0036171823740005493\n","Epoch  5 Batch  221 / 228  Training Loss  0.002896462567150593\n","Epoch  5 Batch  222 / 228  Training Loss  0.0026175514794886112\n","Epoch  5 Batch  223 / 228  Training Loss  0.006363882217556238\n","Epoch  5 Batch  224 / 228  Training Loss  0.0040540508925914764\n","Epoch  5 Batch  225 / 228  Training Loss  0.006245511583983898\n","Epoch  5 Batch  226 / 228  Training Loss  0.002115813549607992\n","Epoch  5 Batch  227 / 228  Training Loss  0.0015925815096125007\n","   6    |    -    |   0.005390   | 88.033537\n","----------------------------------------------------------------------\n","Running epoch: 6\n","Epoch  6 Batch  0 / 228  Training Loss  0.0016804151237010956\n","Epoch  6 Batch  1 / 228  Training Loss  0.002834841376170516\n","Epoch  6 Batch  2 / 228  Training Loss  0.0010260791750624776\n","Epoch  6 Batch  3 / 228  Training Loss  0.0024151804391294718\n","Epoch  6 Batch  4 / 228  Training Loss  0.0012667401460930705\n","Epoch  6 Batch  5 / 228  Training Loss  0.001060070819221437\n","Epoch  6 Batch  6 / 228  Training Loss  0.003500832011923194\n","Epoch  6 Batch  7 / 228  Training Loss  0.0010967307025566697\n","Epoch  6 Batch  8 / 228  Training Loss  0.0015603837091475725\n","Epoch  6 Batch  9 / 228  Training Loss  0.0028759019915014505\n","Epoch  6 Batch  10 / 228  Training Loss  0.0015068899374455214\n","Epoch  6 Batch  11 / 228  Training Loss  0.01237858273088932\n","Epoch  6 Batch  12 / 228  Training Loss  0.009941652417182922\n","Epoch  6 Batch  13 / 228  Training Loss  0.0025359615683555603\n","Epoch  6 Batch  14 / 228  Training Loss  0.006785473320633173\n","Epoch  6 Batch  15 / 228  Training Loss  0.004045562818646431\n","Epoch  6 Batch  16 / 228  Training Loss  0.008047264069318771\n","Epoch  6 Batch  17 / 228  Training Loss  0.001087576849386096\n","Epoch  6 Batch  18 / 228  Training Loss  0.0028073317371308804\n","Epoch  6 Batch  19 / 228  Training Loss  0.0017079405952244997\n","Epoch  6 Batch  20 / 228  Training Loss  0.002015504287555814\n","Epoch  6 Batch  21 / 228  Training Loss  0.003242071019485593\n","Epoch  6 Batch  22 / 228  Training Loss  0.0020189916249364614\n","Epoch  6 Batch  23 / 228  Training Loss  0.002365797059610486\n","Epoch  6 Batch  24 / 228  Training Loss  0.006043727044016123\n","Epoch  6 Batch  25 / 228  Training Loss  0.006897301878780127\n","Epoch  6 Batch  26 / 228  Training Loss  0.013583043590188026\n","Epoch  6 Batch  27 / 228  Training Loss  0.004804609343409538\n","Epoch  6 Batch  28 / 228  Training Loss  0.008533695712685585\n","Epoch  6 Batch  29 / 228  Training Loss  0.007188267074525356\n","Epoch  6 Batch  30 / 228  Training Loss  0.014000105671584606\n","Epoch  6 Batch  31 / 228  Training Loss  0.006504457909613848\n","Epoch  6 Batch  32 / 228  Training Loss  0.008003698661923409\n","Epoch  6 Batch  33 / 228  Training Loss  0.0023710126988589764\n","Epoch  6 Batch  34 / 228  Training Loss  0.001909711747430265\n","Epoch  6 Batch  35 / 228  Training Loss  0.0022994489409029484\n","Epoch  6 Batch  36 / 228  Training Loss  0.005570493638515472\n","Epoch  6 Batch  37 / 228  Training Loss  0.002324007684364915\n","Epoch  6 Batch  38 / 228  Training Loss  0.0032936674542725086\n","Epoch  6 Batch  39 / 228  Training Loss  0.0033870639745146036\n","Epoch  6 Batch  40 / 228  Training Loss  0.0035596771631389856\n","Epoch  6 Batch  41 / 228  Training Loss  0.0023430169094353914\n","Epoch  6 Batch  42 / 228  Training Loss  0.003226347966119647\n","Epoch  6 Batch  43 / 228  Training Loss  0.0033336083870381117\n","Epoch  6 Batch  44 / 228  Training Loss  0.0058487700298428535\n","Epoch  6 Batch  45 / 228  Training Loss  0.004200058989226818\n","Epoch  6 Batch  46 / 228  Training Loss  0.0022007632069289684\n","Epoch  6 Batch  47 / 228  Training Loss  0.0022009212989360094\n","Epoch  6 Batch  48 / 228  Training Loss  0.008807440288364887\n","Epoch  6 Batch  49 / 228  Training Loss  0.001645036623813212\n","Epoch  6 Batch  50 / 228  Training Loss  0.004559709690511227\n","Epoch  6 Batch  51 / 228  Training Loss  0.003185722278431058\n","Epoch  6 Batch  52 / 228  Training Loss  0.003189180511981249\n","Epoch  6 Batch  53 / 228  Training Loss  0.007200147025287151\n","Epoch  6 Batch  54 / 228  Training Loss  0.0021137557923793793\n","Epoch  6 Batch  55 / 228  Training Loss  0.0009403970907442272\n","Epoch  6 Batch  56 / 228  Training Loss  0.004005136899650097\n","Epoch  6 Batch  57 / 228  Training Loss  0.009967750869691372\n","Epoch  6 Batch  58 / 228  Training Loss  0.02133113704621792\n","Epoch  6 Batch  59 / 228  Training Loss  0.006478302646428347\n","Epoch  6 Batch  60 / 228  Training Loss  0.01738067902624607\n","Epoch  6 Batch  61 / 228  Training Loss  0.0031485005747526884\n","Epoch  6 Batch  62 / 228  Training Loss  0.00267427833750844\n","Epoch  6 Batch  63 / 228  Training Loss  0.004509958904236555\n","Epoch  6 Batch  64 / 228  Training Loss  0.0026903809048235416\n","Epoch  6 Batch  65 / 228  Training Loss  0.003165388945490122\n","Epoch  6 Batch  66 / 228  Training Loss  0.003237183205783367\n","Epoch  6 Batch  67 / 228  Training Loss  0.0020697491709142923\n","Epoch  6 Batch  68 / 228  Training Loss  0.0011616351548582315\n","Epoch  6 Batch  69 / 228  Training Loss  0.00677229231223464\n","Epoch  6 Batch  70 / 228  Training Loss  0.006022651679813862\n","Epoch  6 Batch  71 / 228  Training Loss  0.0016283284639939666\n","Epoch  6 Batch  72 / 228  Training Loss  0.003771408461034298\n","Epoch  6 Batch  73 / 228  Training Loss  0.002914571901783347\n","Epoch  6 Batch  74 / 228  Training Loss  0.0035862880758941174\n","Epoch  6 Batch  75 / 228  Training Loss  0.002529443008825183\n","Epoch  6 Batch  76 / 228  Training Loss  0.0010123091051355004\n","Epoch  6 Batch  77 / 228  Training Loss  0.005954198073595762\n","Epoch  6 Batch  78 / 228  Training Loss  0.0029513253830373287\n","Epoch  6 Batch  79 / 228  Training Loss  0.003802417078986764\n","Epoch  6 Batch  80 / 228  Training Loss  0.0038981700781732798\n","Epoch  6 Batch  81 / 228  Training Loss  0.004267564043402672\n","Epoch  6 Batch  82 / 228  Training Loss  0.005379298701882362\n","Epoch  6 Batch  83 / 228  Training Loss  0.006160744000226259\n","Epoch  6 Batch  84 / 228  Training Loss  0.0024050811771303415\n","Epoch  6 Batch  85 / 228  Training Loss  0.0013994334731251001\n","Epoch  6 Batch  86 / 228  Training Loss  0.0024539565201848745\n","Epoch  6 Batch  87 / 228  Training Loss  0.0020486635621637106\n","Epoch  6 Batch  88 / 228  Training Loss  0.0035565909929573536\n","Epoch  6 Batch  89 / 228  Training Loss  0.0031037493608891964\n","Epoch  6 Batch  90 / 228  Training Loss  0.0029679343570023775\n","Epoch  6 Batch  91 / 228  Training Loss  0.002898682374507189\n","Epoch  6 Batch  92 / 228  Training Loss  0.0024224589578807354\n","Epoch  6 Batch  93 / 228  Training Loss  0.0013039029436185956\n","Epoch  6 Batch  94 / 228  Training Loss  0.0024559928569942713\n","Epoch  6 Batch  95 / 228  Training Loss  0.0020431673619896173\n","Epoch  6 Batch  96 / 228  Training Loss  0.003926616162061691\n","Epoch  6 Batch  97 / 228  Training Loss  0.004483983386307955\n","Epoch  6 Batch  98 / 228  Training Loss  0.004587090574204922\n","Epoch  6 Batch  99 / 228  Training Loss  0.00396352494135499\n","Epoch  6 Batch  100 / 228  Training Loss  0.0018702391535043716\n","Epoch  6 Batch  101 / 228  Training Loss  0.0029713741969317198\n","Epoch  6 Batch  102 / 228  Training Loss  0.002054503420367837\n","Epoch  6 Batch  103 / 228  Training Loss  0.003326708683744073\n","Epoch  6 Batch  104 / 228  Training Loss  0.0015387807507067919\n","Epoch  6 Batch  105 / 228  Training Loss  0.004495216067880392\n","Epoch  6 Batch  106 / 228  Training Loss  0.003408802207559347\n","Epoch  6 Batch  107 / 228  Training Loss  0.0015872737858444452\n","Epoch  6 Batch  108 / 228  Training Loss  0.002534432802349329\n","Epoch  6 Batch  109 / 228  Training Loss  0.0011437191860750318\n","Epoch  6 Batch  110 / 228  Training Loss  0.004017799161374569\n","Epoch  6 Batch  111 / 228  Training Loss  0.007774905767291784\n","Epoch  6 Batch  112 / 228  Training Loss  0.006202451419085264\n","Epoch  6 Batch  113 / 228  Training Loss  0.0027120704762637615\n","Epoch  6 Batch  114 / 228  Training Loss  0.0017813522135838866\n","Epoch  6 Batch  115 / 228  Training Loss  0.0014002153184264898\n","Epoch  6 Batch  116 / 228  Training Loss  0.0012900244910269976\n","Epoch  6 Batch  117 / 228  Training Loss  0.002818915294483304\n","Epoch  6 Batch  118 / 228  Training Loss  0.0017744392389431596\n","Epoch  6 Batch  119 / 228  Training Loss  0.005485649220645428\n","Epoch  6 Batch  120 / 228  Training Loss  0.001779027166776359\n","Epoch  6 Batch  121 / 228  Training Loss  0.0012445233296602964\n","Epoch  6 Batch  122 / 228  Training Loss  0.0029545158613473177\n","Epoch  6 Batch  123 / 228  Training Loss  0.006141474936157465\n","Epoch  6 Batch  124 / 228  Training Loss  0.0027734115719795227\n","Epoch  6 Batch  125 / 228  Training Loss  0.0020185362081974745\n","Epoch  6 Batch  126 / 228  Training Loss  0.00058994215214625\n","Epoch  6 Batch  127 / 228  Training Loss  0.0019338260171934962\n","Epoch  6 Batch  128 / 228  Training Loss  0.0020832547452300787\n","Epoch  6 Batch  129 / 228  Training Loss  0.005477994214743376\n","Epoch  6 Batch  130 / 228  Training Loss  0.003759257961064577\n","Epoch  6 Batch  131 / 228  Training Loss  0.0025810529477894306\n","Epoch  6 Batch  132 / 228  Training Loss  0.00586437014862895\n","Epoch  6 Batch  133 / 228  Training Loss  0.0018670704448595643\n","Epoch  6 Batch  134 / 228  Training Loss  0.0014283788623288274\n","Epoch  6 Batch  135 / 228  Training Loss  0.001460649655200541\n","Epoch  6 Batch  136 / 228  Training Loss  0.007912897504866123\n","Epoch  6 Batch  137 / 228  Training Loss  0.0029215067625045776\n","Epoch  6 Batch  138 / 228  Training Loss  0.008827569894492626\n","Epoch  6 Batch  139 / 228  Training Loss  0.0033705872483551502\n","Epoch  6 Batch  140 / 228  Training Loss  0.0013439947506412864\n","Epoch  6 Batch  141 / 228  Training Loss  0.0071039083413779736\n","Epoch  6 Batch  142 / 228  Training Loss  0.005607272498309612\n","Epoch  6 Batch  143 / 228  Training Loss  0.0017938134260475636\n","Epoch  6 Batch  144 / 228  Training Loss  0.0037125349044799805\n","Epoch  6 Batch  145 / 228  Training Loss  0.002234036335721612\n","Epoch  6 Batch  146 / 228  Training Loss  0.002209836384281516\n","Epoch  6 Batch  147 / 228  Training Loss  0.002428710926324129\n","Epoch  6 Batch  148 / 228  Training Loss  0.0006284468690864742\n","Epoch  6 Batch  149 / 228  Training Loss  0.0016635198844596744\n","Epoch  6 Batch  150 / 228  Training Loss  0.0012967566726729274\n","Epoch  6 Batch  151 / 228  Training Loss  0.0019206347642466426\n","Epoch  6 Batch  152 / 228  Training Loss  0.0008671751711517572\n","Epoch  6 Batch  153 / 228  Training Loss  0.0015379554824903607\n","Epoch  6 Batch  154 / 228  Training Loss  0.0012701585656031966\n","Epoch  6 Batch  155 / 228  Training Loss  0.0013170096790418029\n","Epoch  6 Batch  156 / 228  Training Loss  0.0015411624917760491\n","Epoch  6 Batch  157 / 228  Training Loss  0.0022197470534592867\n","Epoch  6 Batch  158 / 228  Training Loss  0.0017402935773134232\n","Epoch  6 Batch  159 / 228  Training Loss  0.0012120966566726565\n","Epoch  6 Batch  160 / 228  Training Loss  0.0038343926426023245\n","Epoch  6 Batch  161 / 228  Training Loss  0.0026470297016203403\n","Epoch  6 Batch  162 / 228  Training Loss  0.001925982884131372\n","Epoch  6 Batch  163 / 228  Training Loss  0.002166117075830698\n","Epoch  6 Batch  164 / 228  Training Loss  0.003749145893380046\n","Epoch  6 Batch  165 / 228  Training Loss  0.003075731685385108\n","Epoch  6 Batch  166 / 228  Training Loss  0.005475166719406843\n","Epoch  6 Batch  167 / 228  Training Loss  0.004280185792595148\n","Epoch  6 Batch  168 / 228  Training Loss  0.0017213032115250826\n","Epoch  6 Batch  169 / 228  Training Loss  0.0013130911393091083\n","Epoch  6 Batch  170 / 228  Training Loss  0.0010558640351518989\n","Epoch  6 Batch  171 / 228  Training Loss  0.0040150415152311325\n","Epoch  6 Batch  172 / 228  Training Loss  0.0011086908634752035\n","Epoch  6 Batch  173 / 228  Training Loss  0.0062346309423446655\n","Epoch  6 Batch  174 / 228  Training Loss  0.00588432839140296\n","Epoch  6 Batch  175 / 228  Training Loss  0.0044504692777991295\n","Epoch  6 Batch  176 / 228  Training Loss  0.005714377388358116\n","Epoch  6 Batch  177 / 228  Training Loss  0.001608711783774197\n","Epoch  6 Batch  178 / 228  Training Loss  0.00619722343981266\n","Epoch  6 Batch  179 / 228  Training Loss  0.005103210918605328\n","Epoch  6 Batch  180 / 228  Training Loss  0.0031497529707849026\n","Epoch  6 Batch  181 / 228  Training Loss  0.0029373823199421167\n","Epoch  6 Batch  182 / 228  Training Loss  0.003235975978896022\n","Epoch  6 Batch  183 / 228  Training Loss  0.00224704178981483\n","Epoch  6 Batch  184 / 228  Training Loss  0.0032332325354218483\n","Epoch  6 Batch  185 / 228  Training Loss  0.003789902664721012\n","Epoch  6 Batch  186 / 228  Training Loss  0.001184904365800321\n","Epoch  6 Batch  187 / 228  Training Loss  0.001307320431806147\n","Epoch  6 Batch  188 / 228  Training Loss  0.002353536430746317\n","Epoch  6 Batch  189 / 228  Training Loss  0.002643752610310912\n","Epoch  6 Batch  190 / 228  Training Loss  0.0005335126188583672\n","Epoch  6 Batch  191 / 228  Training Loss  0.0028042246121913195\n","Epoch  6 Batch  192 / 228  Training Loss  0.008396615274250507\n","Epoch  6 Batch  193 / 228  Training Loss  0.006251089740544558\n","Epoch  6 Batch  194 / 228  Training Loss  0.004813246428966522\n","Epoch  6 Batch  195 / 228  Training Loss  0.00133424811065197\n","Epoch  6 Batch  196 / 228  Training Loss  0.011418825946748257\n","Epoch  6 Batch  197 / 228  Training Loss  0.0037273422349244356\n","Epoch  6 Batch  198 / 228  Training Loss  0.004239266738295555\n","Epoch  6 Batch  199 / 228  Training Loss  0.007366369944065809\n","Epoch  6 Batch  200 / 228  Training Loss  0.010587719269096851\n","Epoch  6 Batch  201 / 228  Training Loss  0.005871807225048542\n","Epoch  6 Batch  202 / 228  Training Loss  0.010828777216374874\n","Epoch  6 Batch  203 / 228  Training Loss  0.004019491840153933\n","Epoch  6 Batch  204 / 228  Training Loss  0.004406903404742479\n","Epoch  6 Batch  205 / 228  Training Loss  0.004690754693001509\n","Epoch  6 Batch  206 / 228  Training Loss  0.003082257229834795\n","Epoch  6 Batch  207 / 228  Training Loss  0.001306265825405717\n","Epoch  6 Batch  208 / 228  Training Loss  0.004341575782746077\n","Epoch  6 Batch  209 / 228  Training Loss  0.0015732909087091684\n","Epoch  6 Batch  210 / 228  Training Loss  0.006604800000786781\n","Epoch  6 Batch  211 / 228  Training Loss  0.003885878948494792\n","Epoch  6 Batch  212 / 228  Training Loss  0.003088162513449788\n","Epoch  6 Batch  213 / 228  Training Loss  0.009033966809511185\n","Epoch  6 Batch  214 / 228  Training Loss  0.001422516186721623\n","Epoch  6 Batch  215 / 228  Training Loss  0.0012581670889630914\n","Epoch  6 Batch  216 / 228  Training Loss  0.0014921239344403148\n","Epoch  6 Batch  217 / 228  Training Loss  0.005267214495688677\n","Epoch  6 Batch  218 / 228  Training Loss  0.003579953918233514\n","Epoch  6 Batch  219 / 228  Training Loss  0.018973369151353836\n","Epoch  6 Batch  220 / 228  Training Loss  0.006101040635257959\n","Epoch  6 Batch  221 / 228  Training Loss  0.016076205298304558\n","Epoch  6 Batch  222 / 228  Training Loss  0.027172375470399857\n","Epoch  6 Batch  223 / 228  Training Loss  0.014351004734635353\n","Epoch  6 Batch  224 / 228  Training Loss  0.009093787521123886\n","Epoch  6 Batch  225 / 228  Training Loss  0.007393882144242525\n","Epoch  6 Batch  226 / 228  Training Loss  0.010409633629024029\n","Epoch  6 Batch  227 / 228  Training Loss  0.0017492396291345358\n","   7    |    -    |   0.004125   | 86.890244\n","----------------------------------------------------------------------\n","Running epoch: 7\n","Epoch  7 Batch  0 / 228  Training Loss  0.003072642022743821\n","Epoch  7 Batch  1 / 228  Training Loss  0.0032667438499629498\n","Epoch  7 Batch  2 / 228  Training Loss  0.002693148097023368\n","Epoch  7 Batch  3 / 228  Training Loss  0.0033334812615066767\n","Epoch  7 Batch  4 / 228  Training Loss  0.0035234822425991297\n","Epoch  7 Batch  5 / 228  Training Loss  0.005606922321021557\n","Epoch  7 Batch  6 / 228  Training Loss  0.0033092310186475515\n","Epoch  7 Batch  7 / 228  Training Loss  0.005679652560502291\n","Epoch  7 Batch  8 / 228  Training Loss  0.001974045066162944\n","Epoch  7 Batch  9 / 228  Training Loss  0.00405347254127264\n","Epoch  7 Batch  10 / 228  Training Loss  0.0042466092854738235\n","Epoch  7 Batch  11 / 228  Training Loss  0.0018558490555733442\n","Epoch  7 Batch  12 / 228  Training Loss  0.004268787335604429\n","Epoch  7 Batch  13 / 228  Training Loss  0.004647637717425823\n","Epoch  7 Batch  14 / 228  Training Loss  0.0029812974389642477\n","Epoch  7 Batch  15 / 228  Training Loss  0.003276548348367214\n","Epoch  7 Batch  16 / 228  Training Loss  0.005865841172635555\n","Epoch  7 Batch  17 / 228  Training Loss  0.001627685152925551\n","Epoch  7 Batch  18 / 228  Training Loss  0.00112657540012151\n","Epoch  7 Batch  19 / 228  Training Loss  0.00471732672303915\n","Epoch  7 Batch  20 / 228  Training Loss  0.011541906744241714\n","Epoch  7 Batch  21 / 228  Training Loss  0.01128989178687334\n","Epoch  7 Batch  22 / 228  Training Loss  0.0024329377338290215\n","Epoch  7 Batch  23 / 228  Training Loss  0.0017254831036552787\n","Epoch  7 Batch  24 / 228  Training Loss  0.0026472597382962704\n","Epoch  7 Batch  25 / 228  Training Loss  0.0018599895993247628\n","Epoch  7 Batch  26 / 228  Training Loss  0.0019071499118581414\n","Epoch  7 Batch  27 / 228  Training Loss  0.002696755575016141\n","Epoch  7 Batch  28 / 228  Training Loss  0.0022222253028303385\n","Epoch  7 Batch  29 / 228  Training Loss  0.0007936893380247056\n","Epoch  7 Batch  30 / 228  Training Loss  0.0017075625946745276\n","Epoch  7 Batch  31 / 228  Training Loss  0.0011136146495118737\n","Epoch  7 Batch  32 / 228  Training Loss  0.004179478622972965\n","Epoch  7 Batch  33 / 228  Training Loss  0.0014116802485659719\n","Epoch  7 Batch  34 / 228  Training Loss  0.0008844101103022695\n","Epoch  7 Batch  35 / 228  Training Loss  0.005793129093945026\n","Epoch  7 Batch  36 / 228  Training Loss  0.008275040425360203\n","Epoch  7 Batch  37 / 228  Training Loss  0.001927744597196579\n","Epoch  7 Batch  38 / 228  Training Loss  0.0032711776439100504\n","Epoch  7 Batch  39 / 228  Training Loss  0.0018653777660802007\n","Epoch  7 Batch  40 / 228  Training Loss  0.0027346769347786903\n","Epoch  7 Batch  41 / 228  Training Loss  0.0012988096568733454\n","Epoch  7 Batch  42 / 228  Training Loss  0.0029453816823661327\n","Epoch  7 Batch  43 / 228  Training Loss  0.0025234506465494633\n","Epoch  7 Batch  44 / 228  Training Loss  0.0031573758460581303\n","Epoch  7 Batch  45 / 228  Training Loss  0.0027082464657723904\n","Epoch  7 Batch  46 / 228  Training Loss  0.003513467265293002\n","Epoch  7 Batch  47 / 228  Training Loss  0.0030771547462791204\n","Epoch  7 Batch  48 / 228  Training Loss  0.0038036489859223366\n","Epoch  7 Batch  49 / 228  Training Loss  0.0037736857775598764\n","Epoch  7 Batch  50 / 228  Training Loss  0.0010317254345864058\n","Epoch  7 Batch  51 / 228  Training Loss  0.004533668980002403\n","Epoch  7 Batch  52 / 228  Training Loss  0.015351783484220505\n","Epoch  7 Batch  53 / 228  Training Loss  0.0033483891747891903\n","Epoch  7 Batch  54 / 228  Training Loss  0.0011326909298077226\n","Epoch  7 Batch  55 / 228  Training Loss  0.0010324688628315926\n","Epoch  7 Batch  56 / 228  Training Loss  0.006756777875125408\n","Epoch  7 Batch  57 / 228  Training Loss  0.0010674659861251712\n","Epoch  7 Batch  58 / 228  Training Loss  0.0005075527587905526\n","Epoch  7 Batch  59 / 228  Training Loss  0.005661951377987862\n","Epoch  7 Batch  60 / 228  Training Loss  0.005486736074090004\n","Epoch  7 Batch  61 / 228  Training Loss  0.0017916280776262283\n","Epoch  7 Batch  62 / 228  Training Loss  0.0032249975483864546\n","Epoch  7 Batch  63 / 228  Training Loss  0.0019640056416392326\n","Epoch  7 Batch  64 / 228  Training Loss  0.0024361354298889637\n","Epoch  7 Batch  65 / 228  Training Loss  0.0034871718380600214\n","Epoch  7 Batch  66 / 228  Training Loss  0.0022701690904796124\n","Epoch  7 Batch  67 / 228  Training Loss  0.0012811205815523863\n","Epoch  7 Batch  68 / 228  Training Loss  0.0024438216350972652\n","Epoch  7 Batch  69 / 228  Training Loss  0.0042943088337779045\n","Epoch  7 Batch  70 / 228  Training Loss  0.0010152329923585057\n","Epoch  7 Batch  71 / 228  Training Loss  0.003840117482468486\n","Epoch  7 Batch  72 / 228  Training Loss  0.0023662238381803036\n","Epoch  7 Batch  73 / 228  Training Loss  0.0012135205324739218\n","Epoch  7 Batch  74 / 228  Training Loss  0.0009884122991934419\n","Epoch  7 Batch  75 / 228  Training Loss  0.0009092845721170306\n","Epoch  7 Batch  76 / 228  Training Loss  0.0008726955857127905\n","Epoch  7 Batch  77 / 228  Training Loss  0.0070704445242881775\n","Epoch  7 Batch  78 / 228  Training Loss  0.0012203827500343323\n","Epoch  7 Batch  79 / 228  Training Loss  0.0016032725106924772\n","Epoch  7 Batch  80 / 228  Training Loss  0.0019263201393187046\n","Epoch  7 Batch  81 / 228  Training Loss  0.0010409516980871558\n","Epoch  7 Batch  82 / 228  Training Loss  0.0011924528516829014\n","Epoch  7 Batch  83 / 228  Training Loss  0.0019857503939419985\n","Epoch  7 Batch  84 / 228  Training Loss  0.0007099797949194908\n","Epoch  7 Batch  85 / 228  Training Loss  0.001106032868847251\n","Epoch  7 Batch  86 / 228  Training Loss  0.001043521100655198\n","Epoch  7 Batch  87 / 228  Training Loss  0.0015220119385048747\n","Epoch  7 Batch  88 / 228  Training Loss  0.0008938175742514431\n","Epoch  7 Batch  89 / 228  Training Loss  0.001078854431398213\n","Epoch  7 Batch  90 / 228  Training Loss  0.0006262592505663633\n","Epoch  7 Batch  91 / 228  Training Loss  0.0058930921368300915\n","Epoch  7 Batch  92 / 228  Training Loss  0.0011789831332862377\n","Epoch  7 Batch  93 / 228  Training Loss  0.0007195599609985948\n","Epoch  7 Batch  94 / 228  Training Loss  0.0006873983656987548\n","Epoch  7 Batch  95 / 228  Training Loss  0.0009656911715865135\n","Epoch  7 Batch  96 / 228  Training Loss  0.0011680425377562642\n","Epoch  7 Batch  97 / 228  Training Loss  0.0010942675871774554\n","Epoch  7 Batch  98 / 228  Training Loss  0.0035310133825987577\n","Epoch  7 Batch  99 / 228  Training Loss  0.0017827574629336596\n","Epoch  7 Batch  100 / 228  Training Loss  0.004839861299842596\n","Epoch  7 Batch  101 / 228  Training Loss  0.0008328405092470348\n","Epoch  7 Batch  102 / 228  Training Loss  0.004793369676917791\n","Epoch  7 Batch  103 / 228  Training Loss  0.00800322461873293\n","Epoch  7 Batch  104 / 228  Training Loss  0.0026413651648908854\n","Epoch  7 Batch  105 / 228  Training Loss  0.0006423945887945592\n","Epoch  7 Batch  106 / 228  Training Loss  0.009937545284628868\n","Epoch  7 Batch  107 / 228  Training Loss  0.0015206269454210997\n","Epoch  7 Batch  108 / 228  Training Loss  0.0009196418686769903\n","Epoch  7 Batch  109 / 228  Training Loss  0.0009812151547521353\n","Epoch  7 Batch  110 / 228  Training Loss  0.0011079052928835154\n","Epoch  7 Batch  111 / 228  Training Loss  0.0011729688849300146\n","Epoch  7 Batch  112 / 228  Training Loss  0.0018212966388091445\n","Epoch  7 Batch  113 / 228  Training Loss  0.00117727171164006\n","Epoch  7 Batch  114 / 228  Training Loss  0.0008966768509708345\n","Epoch  7 Batch  115 / 228  Training Loss  0.0013528801500797272\n","Epoch  7 Batch  116 / 228  Training Loss  0.0012633528094738722\n","Epoch  7 Batch  117 / 228  Training Loss  0.002873661695048213\n","Epoch  7 Batch  118 / 228  Training Loss  0.0008234624983742833\n","Epoch  7 Batch  119 / 228  Training Loss  0.0013423733180388808\n","Epoch  7 Batch  120 / 228  Training Loss  0.004073737189173698\n","Epoch  7 Batch  121 / 228  Training Loss  0.0005024091806262732\n","Epoch  7 Batch  122 / 228  Training Loss  0.0015760903479531407\n","Epoch  7 Batch  123 / 228  Training Loss  0.0006286490242928267\n","Epoch  7 Batch  124 / 228  Training Loss  0.004579715896397829\n","Epoch  7 Batch  125 / 228  Training Loss  0.003296878654509783\n","Epoch  7 Batch  126 / 228  Training Loss  0.0009257551282644272\n","Epoch  7 Batch  127 / 228  Training Loss  0.0010380607564002275\n","Epoch  7 Batch  128 / 228  Training Loss  0.0005459124222397804\n","Epoch  7 Batch  129 / 228  Training Loss  0.0008165364270098507\n","Epoch  7 Batch  130 / 228  Training Loss  0.0016053422586992383\n","Epoch  7 Batch  131 / 228  Training Loss  0.0009214667370542884\n","Epoch  7 Batch  132 / 228  Training Loss  0.001240825280547142\n","Epoch  7 Batch  133 / 228  Training Loss  0.0008421666570939124\n","Epoch  7 Batch  134 / 228  Training Loss  0.0008610645891167223\n","Epoch  7 Batch  135 / 228  Training Loss  0.0013538346393033862\n","Epoch  7 Batch  136 / 228  Training Loss  0.0033131041564047337\n","Epoch  7 Batch  137 / 228  Training Loss  0.00676620053127408\n","Epoch  7 Batch  138 / 228  Training Loss  0.0008197123534046113\n","Epoch  7 Batch  139 / 228  Training Loss  0.0028786244802176952\n","Epoch  7 Batch  140 / 228  Training Loss  0.0012080412125214934\n","Epoch  7 Batch  141 / 228  Training Loss  0.006472786422818899\n","Epoch  7 Batch  142 / 228  Training Loss  0.06430027633905411\n","Epoch  7 Batch  143 / 228  Training Loss  0.026395926252007484\n","Epoch  7 Batch  144 / 228  Training Loss  0.01903368905186653\n","Epoch  7 Batch  145 / 228  Training Loss  0.03906869888305664\n","Epoch  7 Batch  146 / 228  Training Loss  0.00896239373832941\n","Epoch  7 Batch  147 / 228  Training Loss  0.007377682253718376\n","Epoch  7 Batch  148 / 228  Training Loss  0.005865846294909716\n","Epoch  7 Batch  149 / 228  Training Loss  0.00712903356179595\n","Epoch  7 Batch  150 / 228  Training Loss  0.0033663155045360327\n","Epoch  7 Batch  151 / 228  Training Loss  0.008195658214390278\n","Epoch  7 Batch  152 / 228  Training Loss  0.004623390268534422\n","Epoch  7 Batch  153 / 228  Training Loss  0.00711843091994524\n","Epoch  7 Batch  154 / 228  Training Loss  0.005039509851485491\n","Epoch  7 Batch  155 / 228  Training Loss  0.001981328707188368\n","Epoch  7 Batch  156 / 228  Training Loss  0.008201105520129204\n","Epoch  7 Batch  157 / 228  Training Loss  0.015875840559601784\n","Epoch  7 Batch  158 / 228  Training Loss  0.015065521001815796\n","Epoch  7 Batch  159 / 228  Training Loss  0.007627472747117281\n","Epoch  7 Batch  160 / 228  Training Loss  0.006520186550915241\n","Epoch  7 Batch  161 / 228  Training Loss  0.009499396197497845\n","Epoch  7 Batch  162 / 228  Training Loss  0.0027793548069894314\n","Epoch  7 Batch  163 / 228  Training Loss  0.002578859683126211\n","Epoch  7 Batch  164 / 228  Training Loss  0.002377601806074381\n","Epoch  7 Batch  165 / 228  Training Loss  0.003230897244066\n","Epoch  7 Batch  166 / 228  Training Loss  0.0020728197414427996\n","Epoch  7 Batch  167 / 228  Training Loss  0.0074286130256950855\n","Epoch  7 Batch  168 / 228  Training Loss  0.0013109494466334581\n","Epoch  7 Batch  169 / 228  Training Loss  0.0024516682606190443\n","Epoch  7 Batch  170 / 228  Training Loss  0.0010047054383903742\n","Epoch  7 Batch  171 / 228  Training Loss  0.0014117473037913442\n","Epoch  7 Batch  172 / 228  Training Loss  0.007010736968368292\n","Epoch  7 Batch  173 / 228  Training Loss  0.01776835322380066\n","Epoch  7 Batch  174 / 228  Training Loss  0.0011157733388245106\n","Epoch  7 Batch  175 / 228  Training Loss  0.0018218124751001596\n","Epoch  7 Batch  176 / 228  Training Loss  0.0036039480473846197\n","Epoch  7 Batch  177 / 228  Training Loss  0.001700777094811201\n","Epoch  7 Batch  178 / 228  Training Loss  0.0013843073975294828\n","Epoch  7 Batch  179 / 228  Training Loss  0.0014205583138391376\n","Epoch  7 Batch  180 / 228  Training Loss  0.0010629711905494332\n","Epoch  7 Batch  181 / 228  Training Loss  0.0019794167019426823\n","Epoch  7 Batch  182 / 228  Training Loss  0.0010625608265399933\n","Epoch  7 Batch  183 / 228  Training Loss  0.0022559347562491894\n","Epoch  7 Batch  184 / 228  Training Loss  0.0018352475017309189\n","Epoch  7 Batch  185 / 228  Training Loss  0.0033275156747549772\n","Epoch  7 Batch  186 / 228  Training Loss  0.0016028056852519512\n","Epoch  7 Batch  187 / 228  Training Loss  0.0029938172083348036\n","Epoch  7 Batch  188 / 228  Training Loss  0.008968126960098743\n","Epoch  7 Batch  189 / 228  Training Loss  0.0011972261127084494\n","Epoch  7 Batch  190 / 228  Training Loss  0.0067214821465313435\n","Epoch  7 Batch  191 / 228  Training Loss  0.0033342258539050817\n","Epoch  7 Batch  192 / 228  Training Loss  0.0026894507464021444\n","Epoch  7 Batch  193 / 228  Training Loss  0.002308059949427843\n","Epoch  7 Batch  194 / 228  Training Loss  0.0013576931087300181\n","Epoch  7 Batch  195 / 228  Training Loss  0.0013703833101317286\n","Epoch  7 Batch  196 / 228  Training Loss  0.0065362160094082355\n","Epoch  7 Batch  197 / 228  Training Loss  0.002141020493581891\n","Epoch  7 Batch  198 / 228  Training Loss  0.001984286354854703\n","Epoch  7 Batch  199 / 228  Training Loss  0.0015066062333062291\n","Epoch  7 Batch  200 / 228  Training Loss  0.0013303643791005015\n","Epoch  7 Batch  201 / 228  Training Loss  0.003174767829477787\n","Epoch  7 Batch  202 / 228  Training Loss  0.0009443105082027614\n","Epoch  7 Batch  203 / 228  Training Loss  0.0011658115545287728\n","Epoch  7 Batch  204 / 228  Training Loss  0.0023124036379158497\n","Epoch  7 Batch  205 / 228  Training Loss  0.0011971531203016639\n","Epoch  7 Batch  206 / 228  Training Loss  0.0013466756790876389\n","Epoch  7 Batch  207 / 228  Training Loss  0.0009229690185748041\n","Epoch  7 Batch  208 / 228  Training Loss  0.0013919251505285501\n","Epoch  7 Batch  209 / 228  Training Loss  0.0016065043164417148\n","Epoch  7 Batch  210 / 228  Training Loss  0.0011893748305737972\n","Epoch  7 Batch  211 / 228  Training Loss  0.0031260582618415356\n","Epoch  7 Batch  212 / 228  Training Loss  0.00171539222355932\n","Epoch  7 Batch  213 / 228  Training Loss  0.0012599453330039978\n","Epoch  7 Batch  214 / 228  Training Loss  0.0023970240727066994\n","Epoch  7 Batch  215 / 228  Training Loss  0.0014690686948597431\n","Epoch  7 Batch  216 / 228  Training Loss  0.0020719983149319887\n","Epoch  7 Batch  217 / 228  Training Loss  0.0028928734827786684\n","Epoch  7 Batch  218 / 228  Training Loss  0.001337916823104024\n","Epoch  7 Batch  219 / 228  Training Loss  0.0011308594839647412\n","Epoch  7 Batch  220 / 228  Training Loss  0.0020190668292343616\n","Epoch  7 Batch  221 / 228  Training Loss  0.003796235891059041\n","Epoch  7 Batch  222 / 228  Training Loss  0.0013444921933114529\n","Epoch  7 Batch  223 / 228  Training Loss  0.0032148845493793488\n","Epoch  7 Batch  224 / 228  Training Loss  0.005212748423218727\n","Epoch  7 Batch  225 / 228  Training Loss  0.001599751180037856\n","Epoch  7 Batch  226 / 228  Training Loss  0.0023358941543847322\n","Epoch  7 Batch  227 / 228  Training Loss  0.0009571660193614662\n","   8    |    -    |   0.003641   | 88.567073\n","----------------------------------------------------------------------\n","Running epoch: 8\n","Epoch  8 Batch  0 / 228  Training Loss  0.001074911910109222\n","Epoch  8 Batch  1 / 228  Training Loss  0.002350870054215193\n","Epoch  8 Batch  2 / 228  Training Loss  0.0025687185116112232\n","Epoch  8 Batch  3 / 228  Training Loss  0.0007982067763805389\n","Epoch  8 Batch  4 / 228  Training Loss  0.001174316625110805\n","Epoch  8 Batch  5 / 228  Training Loss  0.0011531453346833587\n","Epoch  8 Batch  6 / 228  Training Loss  0.0007160363020375371\n","Epoch  8 Batch  7 / 228  Training Loss  0.004226623103022575\n","Epoch  8 Batch  8 / 228  Training Loss  0.001277653849683702\n","Epoch  8 Batch  9 / 228  Training Loss  0.0008519830298610032\n","Epoch  8 Batch  10 / 228  Training Loss  0.0013749974314123392\n","Epoch  8 Batch  11 / 228  Training Loss  0.0007169078453443944\n","Epoch  8 Batch  12 / 228  Training Loss  0.0008915142389014363\n","Epoch  8 Batch  13 / 228  Training Loss  0.0009531074902042747\n","Epoch  8 Batch  14 / 228  Training Loss  0.0007051917491480708\n","Epoch  8 Batch  15 / 228  Training Loss  0.003474205732345581\n","Epoch  8 Batch  16 / 228  Training Loss  0.0008667788933962584\n","Epoch  8 Batch  17 / 228  Training Loss  0.000795419211499393\n","Epoch  8 Batch  18 / 228  Training Loss  0.0008792848093435168\n","Epoch  8 Batch  19 / 228  Training Loss  0.0009662721422500908\n","Epoch  8 Batch  20 / 228  Training Loss  0.00326228654012084\n","Epoch  8 Batch  21 / 228  Training Loss  0.0019737149123102427\n","Epoch  8 Batch  22 / 228  Training Loss  0.0019304523011669517\n","Epoch  8 Batch  23 / 228  Training Loss  0.0010958636412397027\n","Epoch  8 Batch  24 / 228  Training Loss  0.000780497444793582\n","Epoch  8 Batch  25 / 228  Training Loss  0.0009056378621608019\n","Epoch  8 Batch  26 / 228  Training Loss  0.0009413080406375229\n","Epoch  8 Batch  27 / 228  Training Loss  0.0005788896814920008\n","Epoch  8 Batch  28 / 228  Training Loss  0.0006599081680178642\n","Epoch  8 Batch  29 / 228  Training Loss  0.0003617210022639483\n","Epoch  8 Batch  30 / 228  Training Loss  0.0009472526144236326\n","Epoch  8 Batch  31 / 228  Training Loss  0.0006881010485813022\n","Epoch  8 Batch  32 / 228  Training Loss  0.0006399702979251742\n","Epoch  8 Batch  33 / 228  Training Loss  0.006442424841225147\n","Epoch  8 Batch  34 / 228  Training Loss  0.05375772342085838\n","Epoch  8 Batch  35 / 228  Training Loss  0.08256321400403976\n","Epoch  8 Batch  36 / 228  Training Loss  0.027200065553188324\n","Epoch  8 Batch  37 / 228  Training Loss  0.024908946827054024\n","Epoch  8 Batch  38 / 228  Training Loss  0.013627314008772373\n","Epoch  8 Batch  39 / 228  Training Loss  0.00764593668282032\n","Epoch  8 Batch  40 / 228  Training Loss  0.007083761505782604\n","Epoch  8 Batch  41 / 228  Training Loss  0.012975340709090233\n","Epoch  8 Batch  42 / 228  Training Loss  0.01094487588852644\n","Epoch  8 Batch  43 / 228  Training Loss  0.0037095181178301573\n","Epoch  8 Batch  44 / 228  Training Loss  0.0026430534198880196\n","Epoch  8 Batch  45 / 228  Training Loss  0.0034626557026058435\n","Epoch  8 Batch  46 / 228  Training Loss  0.001996269216760993\n","Epoch  8 Batch  47 / 228  Training Loss  0.0038413642905652523\n","Epoch  8 Batch  48 / 228  Training Loss  0.0019517230102792382\n","Epoch  8 Batch  49 / 228  Training Loss  0.0018732163589447737\n","Epoch  8 Batch  50 / 228  Training Loss  0.003644305281341076\n","Epoch  8 Batch  51 / 228  Training Loss  0.0012601998168975115\n","Epoch  8 Batch  52 / 228  Training Loss  0.001112723140977323\n","Epoch  8 Batch  53 / 228  Training Loss  0.0018963570473715663\n","Epoch  8 Batch  54 / 228  Training Loss  0.0010001679183915257\n","Epoch  8 Batch  55 / 228  Training Loss  0.0012515425914898515\n","Epoch  8 Batch  56 / 228  Training Loss  0.001279923366382718\n","Epoch  8 Batch  57 / 228  Training Loss  0.0016978804487735033\n","Epoch  8 Batch  58 / 228  Training Loss  0.0014965581940487027\n","Epoch  8 Batch  59 / 228  Training Loss  0.00102538603823632\n","Epoch  8 Batch  60 / 228  Training Loss  0.0016931258141994476\n","Epoch  8 Batch  61 / 228  Training Loss  0.0011440246598795056\n","Epoch  8 Batch  62 / 228  Training Loss  0.00344765349291265\n","Epoch  8 Batch  63 / 228  Training Loss  0.0019377615535631776\n","Epoch  8 Batch  64 / 228  Training Loss  0.0017462800024077296\n","Epoch  8 Batch  65 / 228  Training Loss  0.001140217063948512\n","Epoch  8 Batch  66 / 228  Training Loss  0.001082040136680007\n","Epoch  8 Batch  67 / 228  Training Loss  0.0012454661773517728\n","Epoch  8 Batch  68 / 228  Training Loss  0.0011647421633824706\n","Epoch  8 Batch  69 / 228  Training Loss  0.000772399187553674\n","Epoch  8 Batch  70 / 228  Training Loss  0.004279175773262978\n","Epoch  8 Batch  71 / 228  Training Loss  0.0010130807058885694\n","Epoch  8 Batch  72 / 228  Training Loss  0.009486058726906776\n","Epoch  8 Batch  73 / 228  Training Loss  0.004671940114349127\n","Epoch  8 Batch  74 / 228  Training Loss  0.0007280641584657133\n","Epoch  8 Batch  75 / 228  Training Loss  0.0015899412101134658\n","Epoch  8 Batch  76 / 228  Training Loss  0.0022248902823776007\n","Epoch  8 Batch  77 / 228  Training Loss  0.0012447743210941553\n","Epoch  8 Batch  78 / 228  Training Loss  0.0011005158303305507\n","Epoch  8 Batch  79 / 228  Training Loss  0.0008768318220973015\n","Epoch  8 Batch  80 / 228  Training Loss  0.0010221453849226236\n","Epoch  8 Batch  81 / 228  Training Loss  0.0011071381159126759\n","Epoch  8 Batch  82 / 228  Training Loss  0.0015891218790784478\n","Epoch  8 Batch  83 / 228  Training Loss  0.0034584603272378445\n","Epoch  8 Batch  84 / 228  Training Loss  0.0014170935610309243\n","Epoch  8 Batch  85 / 228  Training Loss  0.0016583569813519716\n","Epoch  8 Batch  86 / 228  Training Loss  0.0009483928442932665\n","Epoch  8 Batch  87 / 228  Training Loss  0.0028748621698468924\n","Epoch  8 Batch  88 / 228  Training Loss  0.0010382531909272075\n","Epoch  8 Batch  89 / 228  Training Loss  0.0008445783751085401\n","Epoch  8 Batch  90 / 228  Training Loss  0.0009080126765184104\n","Epoch  8 Batch  91 / 228  Training Loss  0.0014246479840949178\n","Epoch  8 Batch  92 / 228  Training Loss  0.0009323915001004934\n","Epoch  8 Batch  93 / 228  Training Loss  0.0008436658536083996\n","Epoch  8 Batch  94 / 228  Training Loss  0.0032196242827922106\n","Epoch  8 Batch  95 / 228  Training Loss  0.004936997313052416\n","Epoch  8 Batch  96 / 228  Training Loss  0.014944419264793396\n","Epoch  8 Batch  97 / 228  Training Loss  0.001822259509935975\n","Epoch  8 Batch  98 / 228  Training Loss  0.005320290569216013\n","Epoch  8 Batch  99 / 228  Training Loss  0.0011395791079849005\n","Epoch  8 Batch  100 / 228  Training Loss  0.0017679182346910238\n","Epoch  8 Batch  101 / 228  Training Loss  0.0014813679736107588\n","Epoch  8 Batch  102 / 228  Training Loss  0.0013647670857608318\n","Epoch  8 Batch  103 / 228  Training Loss  0.0010213287314400077\n","Epoch  8 Batch  104 / 228  Training Loss  0.002457426395267248\n","Epoch  8 Batch  105 / 228  Training Loss  0.0032263207249343395\n","Epoch  8 Batch  106 / 228  Training Loss  0.004598959814757109\n","Epoch  8 Batch  107 / 228  Training Loss  0.0012090805685147643\n","Epoch  8 Batch  108 / 228  Training Loss  0.0011873290641233325\n","Epoch  8 Batch  109 / 228  Training Loss  0.001344230375252664\n","Epoch  8 Batch  110 / 228  Training Loss  0.0012520862510427833\n","Epoch  8 Batch  111 / 228  Training Loss  0.0018374070059508085\n","Epoch  8 Batch  112 / 228  Training Loss  0.001220439444296062\n","Epoch  8 Batch  113 / 228  Training Loss  0.0007777336286380887\n","Epoch  8 Batch  114 / 228  Training Loss  0.0007178116939030588\n","Epoch  8 Batch  115 / 228  Training Loss  0.001088285236619413\n","Epoch  8 Batch  116 / 228  Training Loss  0.0006681556114926934\n","Epoch  8 Batch  117 / 228  Training Loss  0.0016572029562667012\n","Epoch  8 Batch  118 / 228  Training Loss  0.005009702872484922\n","Epoch  8 Batch  119 / 228  Training Loss  0.0015843573492020369\n","Epoch  8 Batch  120 / 228  Training Loss  0.0031919078901410103\n","Epoch  8 Batch  121 / 228  Training Loss  0.0011067654704675078\n","Epoch  8 Batch  122 / 228  Training Loss  0.0042441850528120995\n","Epoch  8 Batch  123 / 228  Training Loss  0.0007973179453983903\n","Epoch  8 Batch  124 / 228  Training Loss  0.0005424743285402656\n","Epoch  8 Batch  125 / 228  Training Loss  0.0006110191461630166\n","Epoch  8 Batch  126 / 228  Training Loss  0.0011648996733129025\n","Epoch  8 Batch  127 / 228  Training Loss  0.002164036501199007\n","Epoch  8 Batch  128 / 228  Training Loss  0.001009503030218184\n","Epoch  8 Batch  129 / 228  Training Loss  0.0021888839546591043\n","Epoch  8 Batch  130 / 228  Training Loss  0.0008001381065696478\n","Epoch  8 Batch  131 / 228  Training Loss  0.00259901094250381\n","Epoch  8 Batch  132 / 228  Training Loss  0.0015661095967516303\n","Epoch  8 Batch  133 / 228  Training Loss  0.0009598183096386492\n","Epoch  8 Batch  134 / 228  Training Loss  0.002664323663339019\n","Epoch  8 Batch  135 / 228  Training Loss  0.0013390009989961982\n","Epoch  8 Batch  136 / 228  Training Loss  0.0017243429319933057\n","Epoch  8 Batch  137 / 228  Training Loss  0.0005741487839259207\n","Epoch  8 Batch  138 / 228  Training Loss  0.0027488816995173693\n","Epoch  8 Batch  139 / 228  Training Loss  0.0015987175283953547\n","Epoch  8 Batch  140 / 228  Training Loss  0.002443864941596985\n","Epoch  8 Batch  141 / 228  Training Loss  0.0013600424863398075\n","Epoch  8 Batch  142 / 228  Training Loss  0.0024386034347116947\n","Epoch  8 Batch  143 / 228  Training Loss  0.0013388358056545258\n","Epoch  8 Batch  144 / 228  Training Loss  0.003213804680854082\n","Epoch  8 Batch  145 / 228  Training Loss  0.0011790853459388018\n","Epoch  8 Batch  146 / 228  Training Loss  0.0010430439142510295\n","Epoch  8 Batch  147 / 228  Training Loss  0.0006804036092944443\n","Epoch  8 Batch  148 / 228  Training Loss  0.003862424986436963\n","Epoch  8 Batch  149 / 228  Training Loss  0.0015862188301980495\n","Epoch  8 Batch  150 / 228  Training Loss  0.0052595073357224464\n","Epoch  8 Batch  151 / 228  Training Loss  0.0016236703377217054\n","Epoch  8 Batch  152 / 228  Training Loss  0.0005541332648135722\n","Epoch  8 Batch  153 / 228  Training Loss  0.003703863825649023\n","Epoch  8 Batch  154 / 228  Training Loss  0.00669400580227375\n","Epoch  8 Batch  155 / 228  Training Loss  0.016200697049498558\n","Epoch  8 Batch  156 / 228  Training Loss  0.007186068221926689\n","Epoch  8 Batch  157 / 228  Training Loss  0.002021495020017028\n","Epoch  8 Batch  158 / 228  Training Loss  0.0009678484057076275\n","Epoch  8 Batch  159 / 228  Training Loss  0.009241913445293903\n","Epoch  8 Batch  160 / 228  Training Loss  0.0035647402983158827\n","Epoch  8 Batch  161 / 228  Training Loss  0.0043636588379740715\n","Epoch  8 Batch  162 / 228  Training Loss  0.009542049840092659\n","Epoch  8 Batch  163 / 228  Training Loss  0.0009263010579161346\n","Epoch  8 Batch  164 / 228  Training Loss  0.004140130244195461\n","Epoch  8 Batch  165 / 228  Training Loss  0.0007056205649860203\n","Epoch  8 Batch  166 / 228  Training Loss  0.005044862162321806\n","Epoch  8 Batch  167 / 228  Training Loss  0.005324745085090399\n","Epoch  8 Batch  168 / 228  Training Loss  0.000959694036282599\n","Epoch  8 Batch  169 / 228  Training Loss  0.001342938863672316\n","Epoch  8 Batch  170 / 228  Training Loss  0.0016472353599965572\n","Epoch  8 Batch  171 / 228  Training Loss  0.0007548574358224869\n","Epoch  8 Batch  172 / 228  Training Loss  0.0014284306671470404\n","Epoch  8 Batch  173 / 228  Training Loss  0.0018458596896380186\n","Epoch  8 Batch  174 / 228  Training Loss  0.0006892017554491758\n","Epoch  8 Batch  175 / 228  Training Loss  0.0004478819319047034\n","Epoch  8 Batch  176 / 228  Training Loss  0.0008639618754386902\n","Epoch  8 Batch  177 / 228  Training Loss  0.0008278907043859363\n","Epoch  8 Batch  178 / 228  Training Loss  0.0005926751182414591\n","Epoch  8 Batch  179 / 228  Training Loss  0.000868122442625463\n","Epoch  8 Batch  180 / 228  Training Loss  0.0013048594119027257\n","Epoch  8 Batch  181 / 228  Training Loss  0.0010494741145521402\n","Epoch  8 Batch  182 / 228  Training Loss  0.0006117578013800085\n","Epoch  8 Batch  183 / 228  Training Loss  0.004967758897691965\n","Epoch  8 Batch  184 / 228  Training Loss  0.0021864501759409904\n","Epoch  8 Batch  185 / 228  Training Loss  0.0006086149951443076\n","Epoch  8 Batch  186 / 228  Training Loss  0.0004334571131039411\n","Epoch  8 Batch  187 / 228  Training Loss  0.0022412303369492292\n","Epoch  8 Batch  188 / 228  Training Loss  0.014310055412352085\n","Epoch  8 Batch  189 / 228  Training Loss  0.0027120287995785475\n","Epoch  8 Batch  190 / 228  Training Loss  0.0005436054198071361\n","Epoch  8 Batch  191 / 228  Training Loss  0.0006315745995379984\n","Epoch  8 Batch  192 / 228  Training Loss  0.001382047776132822\n","Epoch  8 Batch  193 / 228  Training Loss  0.0009928367799147964\n","Epoch  8 Batch  194 / 228  Training Loss  0.0010355331469327211\n","Epoch  8 Batch  195 / 228  Training Loss  0.0008780266507528722\n","Epoch  8 Batch  196 / 228  Training Loss  0.005059175658971071\n","Epoch  8 Batch  197 / 228  Training Loss  0.0008113157236948609\n","Epoch  8 Batch  198 / 228  Training Loss  0.0015523723559454083\n","Epoch  8 Batch  199 / 228  Training Loss  0.0007971918676048517\n","Epoch  8 Batch  200 / 228  Training Loss  0.002106746891513467\n","Epoch  8 Batch  201 / 228  Training Loss  0.0018507461063563824\n","Epoch  8 Batch  202 / 228  Training Loss  0.0032849840354174376\n","Epoch  8 Batch  203 / 228  Training Loss  0.001374012092128396\n","Epoch  8 Batch  204 / 228  Training Loss  0.0005110079073347151\n","Epoch  8 Batch  205 / 228  Training Loss  0.0006420311401598155\n","Epoch  8 Batch  206 / 228  Training Loss  0.0016281813150271773\n","Epoch  8 Batch  207 / 228  Training Loss  0.0010794082190841436\n","Epoch  8 Batch  208 / 228  Training Loss  0.004571244120597839\n","Epoch  8 Batch  209 / 228  Training Loss  0.007792304269969463\n","Epoch  8 Batch  210 / 228  Training Loss  0.003892206819728017\n","Epoch  8 Batch  211 / 228  Training Loss  0.0016150822630152106\n","Epoch  8 Batch  212 / 228  Training Loss  0.0029797048773616552\n","Epoch  8 Batch  213 / 228  Training Loss  0.011164105497300625\n","Epoch  8 Batch  214 / 228  Training Loss  0.0048035080544650555\n","Epoch  8 Batch  215 / 228  Training Loss  0.0007931767613627017\n","Epoch  8 Batch  216 / 228  Training Loss  0.0013461573980748653\n","Epoch  8 Batch  217 / 228  Training Loss  0.001629458973184228\n","Epoch  8 Batch  218 / 228  Training Loss  0.0010472222929820418\n","Epoch  8 Batch  219 / 228  Training Loss  0.0017070345347747207\n","Epoch  8 Batch  220 / 228  Training Loss  0.0011448226869106293\n","Epoch  8 Batch  221 / 228  Training Loss  0.0032249426003545523\n","Epoch  8 Batch  222 / 228  Training Loss  0.0009373438078910112\n","Epoch  8 Batch  223 / 228  Training Loss  0.006634249817579985\n","Epoch  8 Batch  224 / 228  Training Loss  0.007142723072320223\n","Epoch  8 Batch  225 / 228  Training Loss  0.009170402772724628\n","Epoch  8 Batch  226 / 228  Training Loss  0.002492995234206319\n","Epoch  8 Batch  227 / 228  Training Loss  0.0012554592685773969\n","   9    |    -    |   0.003239   | 87.690549\n","----------------------------------------------------------------------\n","Running epoch: 9\n","Epoch  9 Batch  0 / 228  Training Loss  0.007466188631951809\n","Epoch  9 Batch  1 / 228  Training Loss  0.006973785348236561\n","Epoch  9 Batch  2 / 228  Training Loss  0.004027434624731541\n","Epoch  9 Batch  3 / 228  Training Loss  0.002289044437929988\n","Epoch  9 Batch  4 / 228  Training Loss  0.013576003722846508\n","Epoch  9 Batch  5 / 228  Training Loss  0.002754415851086378\n","Epoch  9 Batch  6 / 228  Training Loss  0.0020347791723906994\n","Epoch  9 Batch  7 / 228  Training Loss  0.0011179468128830194\n","Epoch  9 Batch  8 / 228  Training Loss  0.0010420458856970072\n","Epoch  9 Batch  9 / 228  Training Loss  0.0011165125761181116\n","Epoch  9 Batch  10 / 228  Training Loss  0.0009049100335687399\n","Epoch  9 Batch  11 / 228  Training Loss  0.0013950741849839687\n","Epoch  9 Batch  12 / 228  Training Loss  0.002950292779132724\n","Epoch  9 Batch  13 / 228  Training Loss  0.0008308899705298245\n","Epoch  9 Batch  14 / 228  Training Loss  0.002580939093604684\n","Epoch  9 Batch  15 / 228  Training Loss  0.010825968347489834\n","Epoch  9 Batch  16 / 228  Training Loss  0.0019760499708354473\n","Epoch  9 Batch  17 / 228  Training Loss  0.003055339911952615\n","Epoch  9 Batch  18 / 228  Training Loss  0.0011071024928241968\n","Epoch  9 Batch  19 / 228  Training Loss  0.00297150993719697\n","Epoch  9 Batch  20 / 228  Training Loss  0.0012750618625432253\n","Epoch  9 Batch  21 / 228  Training Loss  0.0022300295531749725\n","Epoch  9 Batch  22 / 228  Training Loss  0.0017450656741857529\n","Epoch  9 Batch  23 / 228  Training Loss  0.0026156273670494556\n","Epoch  9 Batch  24 / 228  Training Loss  0.0019629457965493202\n","Epoch  9 Batch  25 / 228  Training Loss  0.0022122201044112444\n","Epoch  9 Batch  26 / 228  Training Loss  0.001130469492636621\n","Epoch  9 Batch  27 / 228  Training Loss  0.001030829269438982\n","Epoch  9 Batch  28 / 228  Training Loss  0.0008929579635150731\n","Epoch  9 Batch  29 / 228  Training Loss  0.0007450292469002306\n","Epoch  9 Batch  30 / 228  Training Loss  0.00044313055695965886\n","Epoch  9 Batch  31 / 228  Training Loss  0.0005698659806512296\n","Epoch  9 Batch  32 / 228  Training Loss  0.00039990086224861443\n","Epoch  9 Batch  33 / 228  Training Loss  0.0007691728533245623\n","Epoch  9 Batch  34 / 228  Training Loss  0.0004584038397297263\n","Epoch  9 Batch  35 / 228  Training Loss  0.00040894380072131753\n","Epoch  9 Batch  36 / 228  Training Loss  0.00040489755338057876\n","Epoch  9 Batch  37 / 228  Training Loss  0.0028821376617997885\n","Epoch  9 Batch  38 / 228  Training Loss  0.003271642839536071\n","Epoch  9 Batch  39 / 228  Training Loss  0.01416535209864378\n","Epoch  9 Batch  40 / 228  Training Loss  0.0011429592268541455\n","Epoch  9 Batch  41 / 228  Training Loss  0.0007212183554656804\n","Epoch  9 Batch  42 / 228  Training Loss  0.000723054283298552\n","Epoch  9 Batch  43 / 228  Training Loss  0.0020694902632385492\n","Epoch  9 Batch  44 / 228  Training Loss  0.00732717989012599\n","Epoch  9 Batch  45 / 228  Training Loss  0.005576075986027718\n","Epoch  9 Batch  46 / 228  Training Loss  0.0007910943240858614\n","Epoch  9 Batch  47 / 228  Training Loss  0.0011547744506970048\n","Epoch  9 Batch  48 / 228  Training Loss  0.0011741542257368565\n","Epoch  9 Batch  49 / 228  Training Loss  0.0006955054705031216\n","Epoch  9 Batch  50 / 228  Training Loss  0.0013525527901947498\n","Epoch  9 Batch  51 / 228  Training Loss  0.000862793589476496\n","Epoch  9 Batch  52 / 228  Training Loss  0.001111741061322391\n","Epoch  9 Batch  53 / 228  Training Loss  0.0020069919992238283\n","Epoch  9 Batch  54 / 228  Training Loss  0.0004034751618746668\n","Epoch  9 Batch  55 / 228  Training Loss  0.0008164283935911953\n","Epoch  9 Batch  56 / 228  Training Loss  0.00044695500400848687\n","Epoch  9 Batch  57 / 228  Training Loss  0.0005459339590743184\n","Epoch  9 Batch  58 / 228  Training Loss  0.0006860051653347909\n","Epoch  9 Batch  59 / 228  Training Loss  0.0008311616256833076\n","Epoch  9 Batch  60 / 228  Training Loss  0.0006759121897630394\n","Epoch  9 Batch  61 / 228  Training Loss  0.0018117112340405583\n","Epoch  9 Batch  62 / 228  Training Loss  0.0007635306101292372\n","Epoch  9 Batch  63 / 228  Training Loss  0.0011628387728706002\n","Epoch  9 Batch  64 / 228  Training Loss  0.0006416295072995126\n","Epoch  9 Batch  65 / 228  Training Loss  0.0008418136858381331\n","Epoch  9 Batch  66 / 228  Training Loss  0.0005962409195490181\n","Epoch  9 Batch  67 / 228  Training Loss  0.0028155785985291004\n","Epoch  9 Batch  68 / 228  Training Loss  0.0005573073285631835\n","Epoch  9 Batch  69 / 228  Training Loss  0.0005005138809792697\n","Epoch  9 Batch  70 / 228  Training Loss  0.0005860161618329585\n","Epoch  9 Batch  71 / 228  Training Loss  0.0005171718657948077\n","Epoch  9 Batch  72 / 228  Training Loss  0.01108827255666256\n","Epoch  9 Batch  73 / 228  Training Loss  0.0005039452807977796\n","Epoch  9 Batch  74 / 228  Training Loss  0.0007744155591353774\n","Epoch  9 Batch  75 / 228  Training Loss  0.0029635662212967873\n","Epoch  9 Batch  76 / 228  Training Loss  0.0009154292056336999\n","Epoch  9 Batch  77 / 228  Training Loss  0.0003859215066768229\n","Epoch  9 Batch  78 / 228  Training Loss  0.0009595512528903782\n","Epoch  9 Batch  79 / 228  Training Loss  0.00038056913763284683\n","Epoch  9 Batch  80 / 228  Training Loss  0.001000193995423615\n","Epoch  9 Batch  81 / 228  Training Loss  0.0014839785872027278\n","Epoch  9 Batch  82 / 228  Training Loss  0.0013820690801367164\n","Epoch  9 Batch  83 / 228  Training Loss  0.0007802589097991586\n","Epoch  9 Batch  84 / 228  Training Loss  0.0008754432201385498\n","Epoch  9 Batch  85 / 228  Training Loss  0.002632942283526063\n","Epoch  9 Batch  86 / 228  Training Loss  0.0006414882373064756\n","Epoch  9 Batch  87 / 228  Training Loss  0.001095288316719234\n","Epoch  9 Batch  88 / 228  Training Loss  0.0005860034725628793\n","Epoch  9 Batch  89 / 228  Training Loss  0.0029281126335263252\n","Epoch  9 Batch  90 / 228  Training Loss  0.0005941173294559121\n","Epoch  9 Batch  91 / 228  Training Loss  0.0004899839987047017\n","Epoch  9 Batch  92 / 228  Training Loss  0.0030103179160505533\n","Epoch  9 Batch  93 / 228  Training Loss  0.000580819440074265\n","Epoch  9 Batch  94 / 228  Training Loss  0.0024729680735617876\n","Epoch  9 Batch  95 / 228  Training Loss  0.0007955791079439223\n","Epoch  9 Batch  96 / 228  Training Loss  0.0011069910833612084\n","Epoch  9 Batch  97 / 228  Training Loss  0.0007869980181567371\n","Epoch  9 Batch  98 / 228  Training Loss  0.0005328480619937181\n","Epoch  9 Batch  99 / 228  Training Loss  0.0006108614034019411\n","Epoch  9 Batch  100 / 228  Training Loss  0.0005871485336683691\n","Epoch  9 Batch  101 / 228  Training Loss  0.0006565695512108505\n","Epoch  9 Batch  102 / 228  Training Loss  0.0017521120607852936\n","Epoch  9 Batch  103 / 228  Training Loss  0.002435482805594802\n","Epoch  9 Batch  104 / 228  Training Loss  0.0007879336480982602\n","Epoch  9 Batch  105 / 228  Training Loss  0.0005170752410776913\n","Epoch  9 Batch  106 / 228  Training Loss  0.0020245222840458155\n","Epoch  9 Batch  107 / 228  Training Loss  0.000656594755128026\n","Epoch  9 Batch  108 / 228  Training Loss  0.0007805416244082153\n","Epoch  9 Batch  109 / 228  Training Loss  0.0013960726791992784\n","Epoch  9 Batch  110 / 228  Training Loss  0.006482631899416447\n","Epoch  9 Batch  111 / 228  Training Loss  0.0005502020358107984\n","Epoch  9 Batch  112 / 228  Training Loss  0.000791433674748987\n","Epoch  9 Batch  113 / 228  Training Loss  0.005501191131770611\n","Epoch  9 Batch  114 / 228  Training Loss  0.00134929153136909\n","Epoch  9 Batch  115 / 228  Training Loss  0.0015343319391831756\n","Epoch  9 Batch  116 / 228  Training Loss  0.0004954796750098467\n","Epoch  9 Batch  117 / 228  Training Loss  0.0008990110945887864\n","Epoch  9 Batch  118 / 228  Training Loss  0.000746057543437928\n","Epoch  9 Batch  119 / 228  Training Loss  0.0026663620956242085\n","Epoch  9 Batch  120 / 228  Training Loss  0.0006994315190240741\n","Epoch  9 Batch  121 / 228  Training Loss  0.0008432008326053619\n","Epoch  9 Batch  122 / 228  Training Loss  0.0006085484637878835\n","Epoch  9 Batch  123 / 228  Training Loss  0.002459836658090353\n","Epoch  9 Batch  124 / 228  Training Loss  0.0031520079355686903\n","Epoch  9 Batch  125 / 228  Training Loss  0.0015006734756752849\n","Epoch  9 Batch  126 / 228  Training Loss  0.0009481136803515255\n","Epoch  9 Batch  127 / 228  Training Loss  0.0037658200599253178\n","Epoch  9 Batch  128 / 228  Training Loss  0.0014028658624738455\n","Epoch  9 Batch  129 / 228  Training Loss  0.005438785068690777\n","Epoch  9 Batch  130 / 228  Training Loss  0.0021868629846721888\n","Epoch  9 Batch  131 / 228  Training Loss  0.0006902199820615351\n","Epoch  9 Batch  132 / 228  Training Loss  0.0008479248499497771\n","Epoch  9 Batch  133 / 228  Training Loss  0.001827725674957037\n","Epoch  9 Batch  134 / 228  Training Loss  0.0006162453209981322\n","Epoch  9 Batch  135 / 228  Training Loss  0.0033950081560760736\n","Epoch  9 Batch  136 / 228  Training Loss  0.0014532061759382486\n","Epoch  9 Batch  137 / 228  Training Loss  0.0012632013531401753\n","Epoch  9 Batch  138 / 228  Training Loss  0.00486583961173892\n","Epoch  9 Batch  139 / 228  Training Loss  0.0014397457707673311\n","Epoch  9 Batch  140 / 228  Training Loss  0.00044128671288490295\n","Epoch  9 Batch  141 / 228  Training Loss  0.001124284928664565\n","Epoch  9 Batch  142 / 228  Training Loss  0.0003537793818395585\n","Epoch  9 Batch  143 / 228  Training Loss  0.000707747065462172\n","Epoch  9 Batch  144 / 228  Training Loss  0.0015076964627951384\n","Epoch  9 Batch  145 / 228  Training Loss  0.00179672718513757\n","Epoch  9 Batch  146 / 228  Training Loss  0.00047711608931422234\n","Epoch  9 Batch  147 / 228  Training Loss  0.0012701819650828838\n","Epoch  9 Batch  148 / 228  Training Loss  0.0005086800665594637\n","Epoch  9 Batch  149 / 228  Training Loss  0.0006447495543397963\n","Epoch  9 Batch  150 / 228  Training Loss  0.0007318768184632063\n","Epoch  9 Batch  151 / 228  Training Loss  0.0013204844435676932\n","Epoch  9 Batch  152 / 228  Training Loss  0.0006212789448909461\n","Epoch  9 Batch  153 / 228  Training Loss  0.0006394283263944089\n","Epoch  9 Batch  154 / 228  Training Loss  0.0003539587778504938\n","Epoch  9 Batch  155 / 228  Training Loss  0.0010344992624595761\n","Epoch  9 Batch  156 / 228  Training Loss  0.0016506373649463058\n","Epoch  9 Batch  157 / 228  Training Loss  0.002278192201629281\n","Epoch  9 Batch  158 / 228  Training Loss  0.0012029974022880197\n","Epoch  9 Batch  159 / 228  Training Loss  0.0023659809958189726\n","Epoch  9 Batch  160 / 228  Training Loss  0.0005308421095833182\n","Epoch  9 Batch  161 / 228  Training Loss  0.0018780817044898868\n","Epoch  9 Batch  162 / 228  Training Loss  0.0016050575068220496\n","Epoch  9 Batch  163 / 228  Training Loss  0.00040442164754495025\n","Epoch  9 Batch  164 / 228  Training Loss  0.0009493856923654675\n","Epoch  9 Batch  165 / 228  Training Loss  0.0021546364296227694\n","Epoch  9 Batch  166 / 228  Training Loss  0.0021495686378329992\n","Epoch  9 Batch  167 / 228  Training Loss  0.0009332933695986867\n","Epoch  9 Batch  168 / 228  Training Loss  0.0006351821357384324\n","Epoch  9 Batch  169 / 228  Training Loss  0.0005085468874312937\n","Epoch  9 Batch  170 / 228  Training Loss  0.0010172638576477766\n","Epoch  9 Batch  171 / 228  Training Loss  0.0006570752011612058\n","Epoch  9 Batch  172 / 228  Training Loss  0.0018269555876031518\n","Epoch  9 Batch  173 / 228  Training Loss  0.0007663551368750632\n","Epoch  9 Batch  174 / 228  Training Loss  0.0008825509576126933\n","Epoch  9 Batch  175 / 228  Training Loss  0.0005492052296176553\n","Epoch  9 Batch  176 / 228  Training Loss  0.00053780636517331\n","Epoch  9 Batch  177 / 228  Training Loss  0.005907283164560795\n","Epoch  9 Batch  178 / 228  Training Loss  0.0011674410197883844\n","Epoch  9 Batch  179 / 228  Training Loss  0.00305529055185616\n","Epoch  9 Batch  180 / 228  Training Loss  0.0007862461498007178\n","Epoch  9 Batch  181 / 228  Training Loss  0.002351474016904831\n","Epoch  9 Batch  182 / 228  Training Loss  0.0016113503370434046\n","Epoch  9 Batch  183 / 228  Training Loss  0.0013616144424304366\n","Epoch  9 Batch  184 / 228  Training Loss  0.0003438033745624125\n","Epoch  9 Batch  185 / 228  Training Loss  0.001352465245872736\n","Epoch  9 Batch  186 / 228  Training Loss  0.0005534466472454369\n","Epoch  9 Batch  187 / 228  Training Loss  0.0011272246483713388\n","Epoch  9 Batch  188 / 228  Training Loss  0.0006457985728047788\n","Epoch  9 Batch  189 / 228  Training Loss  0.0006189500563777983\n","Epoch  9 Batch  190 / 228  Training Loss  0.0010661195265129209\n","Epoch  9 Batch  191 / 228  Training Loss  0.0006782386917620897\n","Epoch  9 Batch  192 / 228  Training Loss  0.00057607312919572\n","Epoch  9 Batch  193 / 228  Training Loss  0.0015109782107174397\n","Epoch  9 Batch  194 / 228  Training Loss  0.0005312691209837794\n","Epoch  9 Batch  195 / 228  Training Loss  0.0006861226866021752\n","Epoch  9 Batch  196 / 228  Training Loss  0.0005297138122841716\n","Epoch  9 Batch  197 / 228  Training Loss  0.0006058322614990175\n","Epoch  9 Batch  198 / 228  Training Loss  0.0004416979500092566\n","Epoch  9 Batch  199 / 228  Training Loss  0.0012557877926155925\n","Epoch  9 Batch  200 / 228  Training Loss  0.0007309801876544952\n","Epoch  9 Batch  201 / 228  Training Loss  0.0005028436426073313\n","Epoch  9 Batch  202 / 228  Training Loss  0.0012808236060664058\n","Epoch  9 Batch  203 / 228  Training Loss  0.0003802792925853282\n","Epoch  9 Batch  204 / 228  Training Loss  0.0007032298017293215\n","Epoch  9 Batch  205 / 228  Training Loss  0.0008510074112564325\n","Epoch  9 Batch  206 / 228  Training Loss  0.00022712328063789755\n","Epoch  9 Batch  207 / 228  Training Loss  0.0006769312312826514\n","Epoch  9 Batch  208 / 228  Training Loss  0.0002206632198067382\n","Epoch  9 Batch  209 / 228  Training Loss  0.0007413327693939209\n","Epoch  9 Batch  210 / 228  Training Loss  0.00037213938776403666\n","Epoch  9 Batch  211 / 228  Training Loss  0.0005466296570375562\n","Epoch  9 Batch  212 / 228  Training Loss  0.0002366448170505464\n","Epoch  9 Batch  213 / 228  Training Loss  0.0009908865904435515\n","Epoch  9 Batch  214 / 228  Training Loss  0.0008810756844468415\n","Epoch  9 Batch  215 / 228  Training Loss  0.0010949609568342566\n","Epoch  9 Batch  216 / 228  Training Loss  0.0005468881572596729\n","Epoch  9 Batch  217 / 228  Training Loss  0.0006679006037302315\n","Epoch  9 Batch  218 / 228  Training Loss  0.0017545572482049465\n","Epoch  9 Batch  219 / 228  Training Loss  0.00046724043204449117\n","Epoch  9 Batch  220 / 228  Training Loss  0.00034861170570366085\n","Epoch  9 Batch  221 / 228  Training Loss  0.0003179668274242431\n","Epoch  9 Batch  222 / 228  Training Loss  0.0003070280363317579\n","Epoch  9 Batch  223 / 228  Training Loss  0.0006352154305204749\n","Epoch  9 Batch  224 / 228  Training Loss  0.0003644213720690459\n","Epoch  9 Batch  225 / 228  Training Loss  0.000841383880469948\n","Epoch  9 Batch  226 / 228  Training Loss  0.00045392135507427156\n","Epoch  9 Batch  227 / 228  Training Loss  0.0003048207436222583\n","  10    |    -    |   0.001544   | 90.434451\n","----------------------------------------------------------------------\n","Running epoch: 10\n","Epoch  10 Batch  0 / 228  Training Loss  0.00040055124554783106\n","Epoch  10 Batch  1 / 228  Training Loss  0.0006517895963042974\n","Epoch  10 Batch  2 / 228  Training Loss  0.00027847796445712447\n","Epoch  10 Batch  3 / 228  Training Loss  0.00033114664256572723\n","Epoch  10 Batch  4 / 228  Training Loss  0.0003136652521789074\n","Epoch  10 Batch  5 / 228  Training Loss  0.0003716895298566669\n","Epoch  10 Batch  6 / 228  Training Loss  0.00032182582071982324\n","Epoch  10 Batch  7 / 228  Training Loss  0.00297233322635293\n","Epoch  10 Batch  8 / 228  Training Loss  0.009590287692844868\n","Epoch  10 Batch  9 / 228  Training Loss  0.0003852964728139341\n","Epoch  10 Batch  10 / 228  Training Loss  0.0011311920825392008\n","Epoch  10 Batch  11 / 228  Training Loss  0.0008771611610427499\n","Epoch  10 Batch  12 / 228  Training Loss  0.0006688947323709726\n","Epoch  10 Batch  13 / 228  Training Loss  0.0008613746613264084\n","Epoch  10 Batch  14 / 228  Training Loss  0.00036025463487021625\n","Epoch  10 Batch  15 / 228  Training Loss  0.0002548339543864131\n","Epoch  10 Batch  16 / 228  Training Loss  0.0014578818809241056\n","Epoch  10 Batch  17 / 228  Training Loss  0.0006916469428688288\n","Epoch  10 Batch  18 / 228  Training Loss  0.0005065456498414278\n","Epoch  10 Batch  19 / 228  Training Loss  0.0007004124345257878\n","Epoch  10 Batch  20 / 228  Training Loss  0.000534962338861078\n","Epoch  10 Batch  21 / 228  Training Loss  0.0004313629469834268\n","Epoch  10 Batch  22 / 228  Training Loss  0.0006909893127158284\n","Epoch  10 Batch  23 / 228  Training Loss  0.00029678820283152163\n","Epoch  10 Batch  24 / 228  Training Loss  0.0007795707788318396\n","Epoch  10 Batch  25 / 228  Training Loss  0.0002665514766704291\n","Epoch  10 Batch  26 / 228  Training Loss  0.0005805230466648936\n","Epoch  10 Batch  27 / 228  Training Loss  0.0007965711411088705\n","Epoch  10 Batch  28 / 228  Training Loss  0.00032431798172183335\n","Epoch  10 Batch  29 / 228  Training Loss  0.0005112520884722471\n","Epoch  10 Batch  30 / 228  Training Loss  0.00035018485505133867\n","Epoch  10 Batch  31 / 228  Training Loss  0.0006126389489509165\n","Epoch  10 Batch  32 / 228  Training Loss  0.00020576274255290627\n","Epoch  10 Batch  33 / 228  Training Loss  0.0002661201870068908\n","Epoch  10 Batch  34 / 228  Training Loss  0.00031044406932778656\n","Epoch  10 Batch  35 / 228  Training Loss  0.00044482003431767225\n","Epoch  10 Batch  36 / 228  Training Loss  0.00022760215506423265\n","Epoch  10 Batch  37 / 228  Training Loss  0.00040877514402382076\n","Epoch  10 Batch  38 / 228  Training Loss  0.00040405261097475886\n","Epoch  10 Batch  39 / 228  Training Loss  0.00029230842483229935\n","Epoch  10 Batch  40 / 228  Training Loss  0.0003015746478922665\n","Epoch  10 Batch  41 / 228  Training Loss  0.00038004847010597587\n","Epoch  10 Batch  42 / 228  Training Loss  0.0004830194520764053\n","Epoch  10 Batch  43 / 228  Training Loss  0.0002506176824681461\n","Epoch  10 Batch  44 / 228  Training Loss  0.00037153789889998734\n","Epoch  10 Batch  45 / 228  Training Loss  0.0002780480426736176\n","Epoch  10 Batch  46 / 228  Training Loss  0.00019737219554372132\n","Epoch  10 Batch  47 / 228  Training Loss  0.000413401605328545\n","Epoch  10 Batch  48 / 228  Training Loss  0.0004561234381981194\n","Epoch  10 Batch  49 / 228  Training Loss  0.0001708862982923165\n","Epoch  10 Batch  50 / 228  Training Loss  0.00019488841644488275\n","Epoch  10 Batch  51 / 228  Training Loss  0.0005060979747213423\n","Epoch  10 Batch  52 / 228  Training Loss  0.0001796324795577675\n","Epoch  10 Batch  53 / 228  Training Loss  0.00028007518267259\n","Epoch  10 Batch  54 / 228  Training Loss  0.0004256511165294796\n","Epoch  10 Batch  55 / 228  Training Loss  0.00036721714423038065\n","Epoch  10 Batch  56 / 228  Training Loss  0.000327951624058187\n","Epoch  10 Batch  57 / 228  Training Loss  0.00022993143647909164\n","Epoch  10 Batch  58 / 228  Training Loss  0.0002905715082306415\n","Epoch  10 Batch  59 / 228  Training Loss  0.00042046484304592013\n","Epoch  10 Batch  60 / 228  Training Loss  0.00018203954095952213\n","Epoch  10 Batch  61 / 228  Training Loss  0.00036956448457203805\n","Epoch  10 Batch  62 / 228  Training Loss  0.0003216490149497986\n","Epoch  10 Batch  63 / 228  Training Loss  0.000311568466713652\n","Epoch  10 Batch  64 / 228  Training Loss  0.00028873325209133327\n","Epoch  10 Batch  65 / 228  Training Loss  0.0002888814778998494\n","Epoch  10 Batch  66 / 228  Training Loss  0.00029901062953285873\n","Epoch  10 Batch  67 / 228  Training Loss  0.000211954495171085\n","Epoch  10 Batch  68 / 228  Training Loss  0.0002462443080730736\n","Epoch  10 Batch  69 / 228  Training Loss  0.00022100232308730483\n","Epoch  10 Batch  70 / 228  Training Loss  0.0003064266638830304\n","Epoch  10 Batch  71 / 228  Training Loss  0.0003492670657578856\n","Epoch  10 Batch  72 / 228  Training Loss  0.00028922458295710385\n","Epoch  10 Batch  73 / 228  Training Loss  0.0002318546175956726\n","Epoch  10 Batch  74 / 228  Training Loss  0.00024092353123705834\n","Epoch  10 Batch  75 / 228  Training Loss  0.0003150290867779404\n","Epoch  10 Batch  76 / 228  Training Loss  0.0003631364379543811\n","Epoch  10 Batch  77 / 228  Training Loss  0.0009336018119938672\n","Epoch  10 Batch  78 / 228  Training Loss  0.0002566239854786545\n","Epoch  10 Batch  79 / 228  Training Loss  0.00025221859687007964\n","Epoch  10 Batch  80 / 228  Training Loss  0.00023714586859568954\n","Epoch  10 Batch  81 / 228  Training Loss  0.0002634997363202274\n","Epoch  10 Batch  82 / 228  Training Loss  0.00018760953389573842\n","Epoch  10 Batch  83 / 228  Training Loss  0.0002905099536292255\n","Epoch  10 Batch  84 / 228  Training Loss  0.0002700614568311721\n","Epoch  10 Batch  85 / 228  Training Loss  0.0003274734190199524\n","Epoch  10 Batch  86 / 228  Training Loss  0.0002505543234292418\n","Epoch  10 Batch  87 / 228  Training Loss  0.00038810077239759266\n","Epoch  10 Batch  88 / 228  Training Loss  0.00022535797324962914\n","Epoch  10 Batch  89 / 228  Training Loss  0.0004408723034430295\n","Epoch  10 Batch  90 / 228  Training Loss  0.00036067405017092824\n","Epoch  10 Batch  91 / 228  Training Loss  0.0008592377998866141\n","Epoch  10 Batch  92 / 228  Training Loss  0.0010016015730798244\n","Epoch  10 Batch  93 / 228  Training Loss  0.0005557768163271248\n","Epoch  10 Batch  94 / 228  Training Loss  0.0002355783071834594\n","Epoch  10 Batch  95 / 228  Training Loss  0.0003175448509864509\n","Epoch  10 Batch  96 / 228  Training Loss  0.004131629131734371\n","Epoch  10 Batch  97 / 228  Training Loss  0.00555512635037303\n","Epoch  10 Batch  98 / 228  Training Loss  0.0019193277694284916\n","Epoch  10 Batch  99 / 228  Training Loss  0.0012293760664761066\n","Epoch  10 Batch  100 / 228  Training Loss  0.0009137204615399241\n","Epoch  10 Batch  101 / 228  Training Loss  0.0016979642678052187\n","Epoch  10 Batch  102 / 228  Training Loss  0.0032248019706457853\n","Epoch  10 Batch  103 / 228  Training Loss  0.0034658678341656923\n","Epoch  10 Batch  104 / 228  Training Loss  0.0090096490457654\n","Epoch  10 Batch  105 / 228  Training Loss  0.0037215747870504856\n","Epoch  10 Batch  106 / 228  Training Loss  0.0012392658973112702\n","Epoch  10 Batch  107 / 228  Training Loss  0.0035207285545766354\n","Epoch  10 Batch  108 / 228  Training Loss  0.024614062160253525\n","Epoch  10 Batch  109 / 228  Training Loss  0.013316216878592968\n","Epoch  10 Batch  110 / 228  Training Loss  0.0019001506734639406\n","Epoch  10 Batch  111 / 228  Training Loss  0.020679881796240807\n","Epoch  10 Batch  112 / 228  Training Loss  0.0016543430974707007\n","Epoch  10 Batch  113 / 228  Training Loss  0.002608049660921097\n","Epoch  10 Batch  114 / 228  Training Loss  0.001604324672371149\n","Epoch  10 Batch  115 / 228  Training Loss  0.0008975790115073323\n","Epoch  10 Batch  116 / 228  Training Loss  0.0007082013762556016\n","Epoch  10 Batch  117 / 228  Training Loss  0.008868871256709099\n","Epoch  10 Batch  118 / 228  Training Loss  0.011041502468287945\n","Epoch  10 Batch  119 / 228  Training Loss  0.007915218360722065\n","Epoch  10 Batch  120 / 228  Training Loss  0.006520676892250776\n","Epoch  10 Batch  121 / 228  Training Loss  0.009690437465906143\n","Epoch  10 Batch  122 / 228  Training Loss  0.005445095710456371\n","Epoch  10 Batch  123 / 228  Training Loss  0.0024209197144955397\n","Epoch  10 Batch  124 / 228  Training Loss  0.006822110153734684\n","Epoch  10 Batch  125 / 228  Training Loss  0.004056741949170828\n","Epoch  10 Batch  126 / 228  Training Loss  0.002129015512764454\n","Epoch  10 Batch  127 / 228  Training Loss  0.0054856883361935616\n","Epoch  10 Batch  128 / 228  Training Loss  0.0022548565175384283\n","Epoch  10 Batch  129 / 228  Training Loss  0.0029888462740927935\n","Epoch  10 Batch  130 / 228  Training Loss  0.0017896038480103016\n","Epoch  10 Batch  131 / 228  Training Loss  0.0012293396284803748\n","Epoch  10 Batch  132 / 228  Training Loss  0.001571487169712782\n","Epoch  10 Batch  133 / 228  Training Loss  0.006658956408500671\n","Epoch  10 Batch  134 / 228  Training Loss  0.0028580306097865105\n","Epoch  10 Batch  135 / 228  Training Loss  0.0014281325275078416\n","Epoch  10 Batch  136 / 228  Training Loss  0.0008608854841440916\n","Epoch  10 Batch  137 / 228  Training Loss  0.003200850449502468\n","Epoch  10 Batch  138 / 228  Training Loss  0.02030809037387371\n","Epoch  10 Batch  139 / 228  Training Loss  0.018905507400631905\n","Epoch  10 Batch  140 / 228  Training Loss  0.010693807154893875\n","Epoch  10 Batch  141 / 228  Training Loss  0.002081497572362423\n","Epoch  10 Batch  142 / 228  Training Loss  0.0014118495164439082\n","Epoch  10 Batch  143 / 228  Training Loss  0.003164562862366438\n","Epoch  10 Batch  144 / 228  Training Loss  0.002180445706471801\n","Epoch  10 Batch  145 / 228  Training Loss  0.002357698976993561\n","Epoch  10 Batch  146 / 228  Training Loss  0.0007126575219444931\n","Epoch  10 Batch  147 / 228  Training Loss  0.0014193577226251364\n","Epoch  10 Batch  148 / 228  Training Loss  0.006951508112251759\n","Epoch  10 Batch  149 / 228  Training Loss  0.0034766250755637884\n","Epoch  10 Batch  150 / 228  Training Loss  0.0014354420127347112\n","Epoch  10 Batch  151 / 228  Training Loss  0.0016990465810522437\n","Epoch  10 Batch  152 / 228  Training Loss  0.005066712386906147\n","Epoch  10 Batch  153 / 228  Training Loss  0.004015273414552212\n","Epoch  10 Batch  154 / 228  Training Loss  0.0007029077387414873\n","Epoch  10 Batch  155 / 228  Training Loss  0.0013837063452228904\n","Epoch  10 Batch  156 / 228  Training Loss  0.0013366000493988395\n","Epoch  10 Batch  157 / 228  Training Loss  0.0013174260966479778\n","Epoch  10 Batch  158 / 228  Training Loss  0.0017456444911658764\n","Epoch  10 Batch  159 / 228  Training Loss  0.0005611871019937098\n","Epoch  10 Batch  160 / 228  Training Loss  0.002666242653504014\n","Epoch  10 Batch  161 / 228  Training Loss  0.000855764898005873\n","Epoch  10 Batch  162 / 228  Training Loss  0.004075582139194012\n","Epoch  10 Batch  163 / 228  Training Loss  0.0006375127122737467\n","Epoch  10 Batch  164 / 228  Training Loss  0.0011671677930280566\n","Epoch  10 Batch  165 / 228  Training Loss  0.0007816812139935791\n","Epoch  10 Batch  166 / 228  Training Loss  0.0011415891349315643\n","Epoch  10 Batch  167 / 228  Training Loss  0.0005190391675569117\n","Epoch  10 Batch  168 / 228  Training Loss  0.002061746781691909\n","Epoch  10 Batch  169 / 228  Training Loss  0.0004370177339296788\n","Epoch  10 Batch  170 / 228  Training Loss  0.0038848768454045057\n","Epoch  10 Batch  171 / 228  Training Loss  0.0015886354958638549\n","Epoch  10 Batch  172 / 228  Training Loss  0.000981327029876411\n","Epoch  10 Batch  173 / 228  Training Loss  0.0006419336423277855\n","Epoch  10 Batch  174 / 228  Training Loss  0.0074959262274205685\n","Epoch  10 Batch  175 / 228  Training Loss  0.0007058714400045574\n","Epoch  10 Batch  176 / 228  Training Loss  0.002018474042415619\n","Epoch  10 Batch  177 / 228  Training Loss  0.0006253294995985925\n","Epoch  10 Batch  178 / 228  Training Loss  0.0005864321137778461\n","Epoch  10 Batch  179 / 228  Training Loss  0.0009013821254484355\n","Epoch  10 Batch  180 / 228  Training Loss  0.0005838203360326588\n","Epoch  10 Batch  181 / 228  Training Loss  0.0006978014134801924\n","Epoch  10 Batch  182 / 228  Training Loss  0.0013067425461485982\n","Epoch  10 Batch  183 / 228  Training Loss  0.0004986366257071495\n","Epoch  10 Batch  184 / 228  Training Loss  0.0005206986679695547\n","Epoch  10 Batch  185 / 228  Training Loss  0.0020757161546498537\n","Epoch  10 Batch  186 / 228  Training Loss  0.002164858393371105\n","Epoch  10 Batch  187 / 228  Training Loss  0.0011283635394647717\n","Epoch  10 Batch  188 / 228  Training Loss  0.0011220995802432299\n","Epoch  10 Batch  189 / 228  Training Loss  0.0012991618132218719\n","Epoch  10 Batch  190 / 228  Training Loss  0.0036285670939832926\n","Epoch  10 Batch  191 / 228  Training Loss  0.00030844841967336833\n","Epoch  10 Batch  192 / 228  Training Loss  0.00053283671149984\n","Epoch  10 Batch  193 / 228  Training Loss  0.0003586145758163184\n","Epoch  10 Batch  194 / 228  Training Loss  0.0002899651590269059\n","Epoch  10 Batch  195 / 228  Training Loss  0.0005501518025994301\n","Epoch  10 Batch  196 / 228  Training Loss  0.0005248307134024799\n","Epoch  10 Batch  197 / 228  Training Loss  0.0008952795178629458\n","Epoch  10 Batch  198 / 228  Training Loss  0.00041251981747336686\n","Epoch  10 Batch  199 / 228  Training Loss  0.00027248141122981906\n","Epoch  10 Batch  200 / 228  Training Loss  0.00024157042207662016\n","Epoch  10 Batch  201 / 228  Training Loss  0.00031468880479224026\n","Epoch  10 Batch  202 / 228  Training Loss  0.0004198735987301916\n","Epoch  10 Batch  203 / 228  Training Loss  0.0003761308325920254\n","Epoch  10 Batch  204 / 228  Training Loss  0.00041352034895680845\n","Epoch  10 Batch  205 / 228  Training Loss  0.00038833863800391555\n","Epoch  10 Batch  206 / 228  Training Loss  0.00042765040416270494\n","Epoch  10 Batch  207 / 228  Training Loss  0.00036031994386576116\n","Epoch  10 Batch  208 / 228  Training Loss  0.0006311475881375372\n","Epoch  10 Batch  209 / 228  Training Loss  0.0003260764933656901\n","Epoch  10 Batch  210 / 228  Training Loss  0.0006115366122685373\n","Epoch  10 Batch  211 / 228  Training Loss  0.0006018960848450661\n","Epoch  10 Batch  212 / 228  Training Loss  0.0008205102058127522\n","Epoch  10 Batch  213 / 228  Training Loss  0.0026487447321414948\n","Epoch  10 Batch  214 / 228  Training Loss  0.0005756924510933459\n","Epoch  10 Batch  215 / 228  Training Loss  0.0008426295244134963\n","Epoch  10 Batch  216 / 228  Training Loss  0.0004239419649820775\n","Epoch  10 Batch  217 / 228  Training Loss  0.0003770614566747099\n","Epoch  10 Batch  218 / 228  Training Loss  0.0008212318061850965\n","Epoch  10 Batch  219 / 228  Training Loss  0.00028516232850961387\n","Epoch  10 Batch  220 / 228  Training Loss  0.0007463129586540163\n","Epoch  10 Batch  221 / 228  Training Loss  0.0005468263407237828\n","Epoch  10 Batch  222 / 228  Training Loss  0.001052386243827641\n","Epoch  10 Batch  223 / 228  Training Loss  0.0004507476114667952\n","Epoch  10 Batch  224 / 228  Training Loss  0.00022897013695910573\n","Epoch  10 Batch  225 / 228  Training Loss  0.002680972684174776\n","Epoch  10 Batch  226 / 228  Training Loss  0.0022494711447507143\n","Epoch  10 Batch  227 / 228  Training Loss  0.000576808990444988\n","  11    |    -    |   0.001842   | 89.977134\n","----------------------------------------------------------------------\n","Running epoch: 11\n","Epoch  11 Batch  0 / 228  Training Loss  0.00026311498368158937\n","Epoch  11 Batch  1 / 228  Training Loss  0.0004337254795245826\n","Epoch  11 Batch  2 / 228  Training Loss  0.00048235704889521003\n","Epoch  11 Batch  3 / 228  Training Loss  0.000750441977288574\n","Epoch  11 Batch  4 / 228  Training Loss  0.0002931347116827965\n","Epoch  11 Batch  5 / 228  Training Loss  0.0003940720052924007\n","Epoch  11 Batch  6 / 228  Training Loss  0.00034694405621849\n","Epoch  11 Batch  7 / 228  Training Loss  0.00022907514357939363\n","Epoch  11 Batch  8 / 228  Training Loss  0.000376404874259606\n","Epoch  11 Batch  9 / 228  Training Loss  0.0003095483989454806\n","Epoch  11 Batch  10 / 228  Training Loss  0.0003835783863905817\n","Epoch  11 Batch  11 / 228  Training Loss  0.00030414402135647833\n","Epoch  11 Batch  12 / 228  Training Loss  0.00034224253613501787\n","Epoch  11 Batch  13 / 228  Training Loss  0.0004554765473585576\n","Epoch  11 Batch  14 / 228  Training Loss  0.0005008701118640602\n","Epoch  11 Batch  15 / 228  Training Loss  0.0004251927020959556\n","Epoch  11 Batch  16 / 228  Training Loss  0.00024171519908122718\n","Epoch  11 Batch  17 / 228  Training Loss  0.00022915283625479788\n","Epoch  11 Batch  18 / 228  Training Loss  0.0003451164811849594\n","Epoch  11 Batch  19 / 228  Training Loss  0.0002502645074855536\n","Epoch  11 Batch  20 / 228  Training Loss  0.000340061989845708\n","Epoch  11 Batch  21 / 228  Training Loss  0.0003442884481046349\n","Epoch  11 Batch  22 / 228  Training Loss  0.0003077196015510708\n","Epoch  11 Batch  23 / 228  Training Loss  0.0005500402767211199\n","Epoch  11 Batch  24 / 228  Training Loss  0.00028605994884856045\n","Epoch  11 Batch  25 / 228  Training Loss  0.0003334562643431127\n","Epoch  11 Batch  26 / 228  Training Loss  0.0003942365583498031\n","Epoch  11 Batch  27 / 228  Training Loss  0.0004469685081858188\n","Epoch  11 Batch  28 / 228  Training Loss  0.0003428011841606349\n","Epoch  11 Batch  29 / 228  Training Loss  0.000489383062813431\n","Epoch  11 Batch  30 / 228  Training Loss  0.00025725841987878084\n","Epoch  11 Batch  31 / 228  Training Loss  0.00023364224762190133\n","Epoch  11 Batch  32 / 228  Training Loss  0.0002638063160702586\n","Epoch  11 Batch  33 / 228  Training Loss  0.0011153375962749124\n","Epoch  11 Batch  34 / 228  Training Loss  0.0002555382379796356\n","Epoch  11 Batch  35 / 228  Training Loss  0.00027320152730681\n","Epoch  11 Batch  36 / 228  Training Loss  0.00027725857216864824\n","Epoch  11 Batch  37 / 228  Training Loss  0.0004588790761772543\n","Epoch  11 Batch  38 / 228  Training Loss  0.0003925710916519165\n","Epoch  11 Batch  39 / 228  Training Loss  0.00036080542486160994\n","Epoch  11 Batch  40 / 228  Training Loss  0.00020475737983360887\n","Epoch  11 Batch  41 / 228  Training Loss  0.00033410362084396183\n","Epoch  11 Batch  42 / 228  Training Loss  0.00018983306654263288\n","Epoch  11 Batch  43 / 228  Training Loss  0.0005627315840683877\n","Epoch  11 Batch  44 / 228  Training Loss  0.00043603440281003714\n","Epoch  11 Batch  45 / 228  Training Loss  0.00023132350179366767\n","Epoch  11 Batch  46 / 228  Training Loss  0.00042714961455203593\n","Epoch  11 Batch  47 / 228  Training Loss  0.00034424805198796093\n","Epoch  11 Batch  48 / 228  Training Loss  0.00030883087310940027\n","Epoch  11 Batch  49 / 228  Training Loss  0.0001746537018334493\n","Epoch  11 Batch  50 / 228  Training Loss  0.0005963105941191316\n","Epoch  11 Batch  51 / 228  Training Loss  0.0003149106341879815\n","Epoch  11 Batch  52 / 228  Training Loss  0.0008746053208597004\n","Epoch  11 Batch  53 / 228  Training Loss  0.0005946802557446063\n","Epoch  11 Batch  54 / 228  Training Loss  0.0011679766466841102\n","Epoch  11 Batch  55 / 228  Training Loss  0.00017539381224196404\n","Epoch  11 Batch  56 / 228  Training Loss  0.0007036024471744895\n","Epoch  11 Batch  57 / 228  Training Loss  0.0003907238133251667\n","Epoch  11 Batch  58 / 228  Training Loss  0.0002059518883470446\n","Epoch  11 Batch  59 / 228  Training Loss  0.0005990049685351551\n","Epoch  11 Batch  60 / 228  Training Loss  0.00041487786802463233\n","Epoch  11 Batch  61 / 228  Training Loss  0.0010677349055185914\n","Epoch  11 Batch  62 / 228  Training Loss  0.0009930403903126717\n","Epoch  11 Batch  63 / 228  Training Loss  0.0002974152157548815\n","Epoch  11 Batch  64 / 228  Training Loss  0.0003655337786767632\n","Epoch  11 Batch  65 / 228  Training Loss  0.0005071679479442537\n","Epoch  11 Batch  66 / 228  Training Loss  0.0002960233832709491\n","Epoch  11 Batch  67 / 228  Training Loss  0.00030197080923244357\n","Epoch  11 Batch  68 / 228  Training Loss  0.0004644439904950559\n","Epoch  11 Batch  69 / 228  Training Loss  0.0005707608070224524\n","Epoch  11 Batch  70 / 228  Training Loss  0.0002443964476697147\n","Epoch  11 Batch  71 / 228  Training Loss  0.0005880335229448974\n","Epoch  11 Batch  72 / 228  Training Loss  0.00034241328830830753\n","Epoch  11 Batch  73 / 228  Training Loss  0.0020733291748911142\n","Epoch  11 Batch  74 / 228  Training Loss  0.002472841879352927\n","Epoch  11 Batch  75 / 228  Training Loss  0.000485888245748356\n","Epoch  11 Batch  76 / 228  Training Loss  0.00023069664894137532\n","Epoch  11 Batch  77 / 228  Training Loss  0.000337197125190869\n","Epoch  11 Batch  78 / 228  Training Loss  0.0005943659343756735\n","Epoch  11 Batch  79 / 228  Training Loss  0.00037227876600809395\n","Epoch  11 Batch  80 / 228  Training Loss  0.0004579193773679435\n","Epoch  11 Batch  81 / 228  Training Loss  0.00024556362768635154\n","Epoch  11 Batch  82 / 228  Training Loss  0.003446678165346384\n","Epoch  11 Batch  83 / 228  Training Loss  0.014969785697758198\n","Epoch  11 Batch  84 / 228  Training Loss  0.001303916797041893\n","Epoch  11 Batch  85 / 228  Training Loss  0.0006734031485393643\n","Epoch  11 Batch  86 / 228  Training Loss  0.0011874972842633724\n","Epoch  11 Batch  87 / 228  Training Loss  0.002391208428889513\n","Epoch  11 Batch  88 / 228  Training Loss  0.005489610135555267\n","Epoch  11 Batch  89 / 228  Training Loss  0.003062568372115493\n","Epoch  11 Batch  90 / 228  Training Loss  0.004345085006207228\n","Epoch  11 Batch  91 / 228  Training Loss  0.004762857221066952\n","Epoch  11 Batch  92 / 228  Training Loss  0.003774278098717332\n","Epoch  11 Batch  93 / 228  Training Loss  0.0016718892147764564\n","Epoch  11 Batch  94 / 228  Training Loss  0.005036292131990194\n","Epoch  11 Batch  95 / 228  Training Loss  0.007277352269738913\n","Epoch  11 Batch  96 / 228  Training Loss  0.0025359017308801413\n","Epoch  11 Batch  97 / 228  Training Loss  0.000810978701338172\n","Epoch  11 Batch  98 / 228  Training Loss  0.0011153951054438949\n","Epoch  11 Batch  99 / 228  Training Loss  0.0009762531262822449\n","Epoch  11 Batch  100 / 228  Training Loss  0.0024799855891615152\n","Epoch  11 Batch  101 / 228  Training Loss  0.0052351620979607105\n","Epoch  11 Batch  102 / 228  Training Loss  0.0038402783684432507\n","Epoch  11 Batch  103 / 228  Training Loss  0.001702289329841733\n","Epoch  11 Batch  104 / 228  Training Loss  0.004934967029839754\n","Epoch  11 Batch  105 / 228  Training Loss  0.003659636713564396\n","Epoch  11 Batch  106 / 228  Training Loss  0.002307161455973983\n","Epoch  11 Batch  107 / 228  Training Loss  0.0018442629370838404\n","Epoch  11 Batch  108 / 228  Training Loss  0.0038217457477003336\n","Epoch  11 Batch  109 / 228  Training Loss  0.007697294000536203\n","Epoch  11 Batch  110 / 228  Training Loss  0.004201818723231554\n","Epoch  11 Batch  111 / 228  Training Loss  0.0023060461971908808\n","Epoch  11 Batch  112 / 228  Training Loss  0.009082683362066746\n","Epoch  11 Batch  113 / 228  Training Loss  0.002762302290648222\n","Epoch  11 Batch  114 / 228  Training Loss  0.003500822465866804\n","Epoch  11 Batch  115 / 228  Training Loss  0.003293747315183282\n","Epoch  11 Batch  116 / 228  Training Loss  0.002981433877721429\n","Epoch  11 Batch  117 / 228  Training Loss  0.003036904614418745\n","Epoch  11 Batch  118 / 228  Training Loss  0.002087457338348031\n","Epoch  11 Batch  119 / 228  Training Loss  0.013627756386995316\n","Epoch  11 Batch  120 / 228  Training Loss  0.01122366450726986\n","Epoch  11 Batch  121 / 228  Training Loss  0.003152244258671999\n","Epoch  11 Batch  122 / 228  Training Loss  0.0015833440702408552\n","Epoch  11 Batch  123 / 228  Training Loss  0.004205420147627592\n","Epoch  11 Batch  124 / 228  Training Loss  0.0009879552526399493\n","Epoch  11 Batch  125 / 228  Training Loss  0.015570620074868202\n","Epoch  11 Batch  126 / 228  Training Loss  0.002150397514924407\n","Epoch  11 Batch  127 / 228  Training Loss  0.001127624069340527\n","Epoch  11 Batch  128 / 228  Training Loss  0.0009811260970309377\n","Epoch  11 Batch  129 / 228  Training Loss  0.002269860589876771\n","Epoch  11 Batch  130 / 228  Training Loss  0.0047487905248999596\n","Epoch  11 Batch  131 / 228  Training Loss  0.0026376284658908844\n","Epoch  11 Batch  132 / 228  Training Loss  0.0011447611032053828\n","Epoch  11 Batch  133 / 228  Training Loss  0.0007226340239867568\n","Epoch  11 Batch  134 / 228  Training Loss  0.0024045424070209265\n","Epoch  11 Batch  135 / 228  Training Loss  0.0005084678996354342\n","Epoch  11 Batch  136 / 228  Training Loss  0.0006679804646410048\n","Epoch  11 Batch  137 / 228  Training Loss  0.00046479832963086665\n","Epoch  11 Batch  138 / 228  Training Loss  0.0007261576247401536\n","Epoch  11 Batch  139 / 228  Training Loss  0.0023767852690070868\n","Epoch  11 Batch  140 / 228  Training Loss  0.0009672518935985863\n","Epoch  11 Batch  141 / 228  Training Loss  0.0019569022115319967\n","Epoch  11 Batch  142 / 228  Training Loss  0.0009201273205690086\n","Epoch  11 Batch  143 / 228  Training Loss  0.0010968087008222938\n","Epoch  11 Batch  144 / 228  Training Loss  0.0012909009819850326\n","Epoch  11 Batch  145 / 228  Training Loss  0.0007494321325793862\n","Epoch  11 Batch  146 / 228  Training Loss  0.003169877687469125\n","Epoch  11 Batch  147 / 228  Training Loss  0.0013285777531564236\n","Epoch  11 Batch  148 / 228  Training Loss  0.004178490024060011\n","Epoch  11 Batch  149 / 228  Training Loss  0.002285200171172619\n","Epoch  11 Batch  150 / 228  Training Loss  0.007231968455016613\n","Epoch  11 Batch  151 / 228  Training Loss  0.0009111397666856647\n","Epoch  11 Batch  152 / 228  Training Loss  0.0015533503610640764\n","Epoch  11 Batch  153 / 228  Training Loss  0.006128379143774509\n","Epoch  11 Batch  154 / 228  Training Loss  0.0006305425777100027\n","Epoch  11 Batch  155 / 228  Training Loss  0.0006964399362914264\n","Epoch  11 Batch  156 / 228  Training Loss  0.001241120626218617\n","Epoch  11 Batch  157 / 228  Training Loss  0.002888584276661277\n","Epoch  11 Batch  158 / 228  Training Loss  0.0003592631546780467\n","Epoch  11 Batch  159 / 228  Training Loss  0.0011112692300230265\n","Epoch  11 Batch  160 / 228  Training Loss  0.0021666267421096563\n","Epoch  11 Batch  161 / 228  Training Loss  0.0012486078776419163\n","Epoch  11 Batch  162 / 228  Training Loss  0.000447757396614179\n","Epoch  11 Batch  163 / 228  Training Loss  0.0004888258990831673\n","Epoch  11 Batch  164 / 228  Training Loss  0.0006138148601166904\n","Epoch  11 Batch  165 / 228  Training Loss  0.001513872528448701\n","Epoch  11 Batch  166 / 228  Training Loss  0.0014573170337826014\n","Epoch  11 Batch  167 / 228  Training Loss  0.0012257632333785295\n","Epoch  11 Batch  168 / 228  Training Loss  0.001165927154943347\n","Epoch  11 Batch  169 / 228  Training Loss  0.0005207195063121617\n","Epoch  11 Batch  170 / 228  Training Loss  0.000696286151651293\n","Epoch  11 Batch  171 / 228  Training Loss  0.0018876963295042515\n","Epoch  11 Batch  172 / 228  Training Loss  0.0006276768981479108\n","Epoch  11 Batch  173 / 228  Training Loss  0.0011587815824896097\n","Epoch  11 Batch  174 / 228  Training Loss  0.0005952042993158102\n","Epoch  11 Batch  175 / 228  Training Loss  0.0007246785098686814\n","Epoch  11 Batch  176 / 228  Training Loss  0.0008975880336947739\n","Epoch  11 Batch  177 / 228  Training Loss  0.00057730742264539\n","Epoch  11 Batch  178 / 228  Training Loss  0.000489274098072201\n","Epoch  11 Batch  179 / 228  Training Loss  0.00043799972627311945\n","Epoch  11 Batch  180 / 228  Training Loss  0.0003829581255558878\n","Epoch  11 Batch  181 / 228  Training Loss  0.0006779358955100179\n","Epoch  11 Batch  182 / 228  Training Loss  0.00038643134757876396\n","Epoch  11 Batch  183 / 228  Training Loss  0.0002480842231307179\n","Epoch  11 Batch  184 / 228  Training Loss  0.0007487634429708123\n","Epoch  11 Batch  185 / 228  Training Loss  0.0006424582679755986\n","Epoch  11 Batch  186 / 228  Training Loss  0.0002641599567141384\n","Epoch  11 Batch  187 / 228  Training Loss  0.00035076920175924897\n","Epoch  11 Batch  188 / 228  Training Loss  0.0004987283027730882\n","Epoch  11 Batch  189 / 228  Training Loss  0.0009961997857317328\n","Epoch  11 Batch  190 / 228  Training Loss  0.00047941404045559466\n","Epoch  11 Batch  191 / 228  Training Loss  0.0010735023533925414\n","Epoch  11 Batch  192 / 228  Training Loss  0.00090574542991817\n","Epoch  11 Batch  193 / 228  Training Loss  0.0003782005805987865\n","Epoch  11 Batch  194 / 228  Training Loss  0.00040909909876063466\n","Epoch  11 Batch  195 / 228  Training Loss  0.0006626715767197311\n","Epoch  11 Batch  196 / 228  Training Loss  0.00044224169687367976\n","Epoch  11 Batch  197 / 228  Training Loss  0.000499886111356318\n","Epoch  11 Batch  198 / 228  Training Loss  0.00031425943598151207\n","Epoch  11 Batch  199 / 228  Training Loss  0.0003767634916584939\n","Epoch  11 Batch  200 / 228  Training Loss  0.00021697458578273654\n","Epoch  11 Batch  201 / 228  Training Loss  0.00027704203967005014\n","Epoch  11 Batch  202 / 228  Training Loss  0.000394224509363994\n","Epoch  11 Batch  203 / 228  Training Loss  0.0010073239682242274\n","Epoch  11 Batch  204 / 228  Training Loss  0.0003515060234349221\n","Epoch  11 Batch  205 / 228  Training Loss  0.0005679155001416802\n","Epoch  11 Batch  206 / 228  Training Loss  0.00038133974885568023\n","Epoch  11 Batch  207 / 228  Training Loss  0.003222051076591015\n","Epoch  11 Batch  208 / 228  Training Loss  0.001325140125118196\n","Epoch  11 Batch  209 / 228  Training Loss  0.001268870895728469\n","Epoch  11 Batch  210 / 228  Training Loss  0.00032209407072514296\n","Epoch  11 Batch  211 / 228  Training Loss  0.00624594883993268\n","Epoch  11 Batch  212 / 228  Training Loss  0.0002721989876590669\n","Epoch  11 Batch  213 / 228  Training Loss  0.0003871037042699754\n","Epoch  11 Batch  214 / 228  Training Loss  0.0033497067634016275\n","Epoch  11 Batch  215 / 228  Training Loss  0.0041274395771324635\n","Epoch  11 Batch  216 / 228  Training Loss  0.0006623293738812208\n","Epoch  11 Batch  217 / 228  Training Loss  0.0006364614819176495\n","Epoch  11 Batch  218 / 228  Training Loss  0.00043896754505112767\n","Epoch  11 Batch  219 / 228  Training Loss  0.00018891204672399908\n","Epoch  11 Batch  220 / 228  Training Loss  0.00030858267564326525\n","Epoch  11 Batch  221 / 228  Training Loss  0.0046418700367212296\n","Epoch  11 Batch  222 / 228  Training Loss  0.01731722615659237\n","Epoch  11 Batch  223 / 228  Training Loss  0.001185000641271472\n","Epoch  11 Batch  224 / 228  Training Loss  0.002352264244109392\n","Epoch  11 Batch  225 / 228  Training Loss  0.0024501224979758263\n","Epoch  11 Batch  226 / 228  Training Loss  0.0028840615414083004\n","Epoch  11 Batch  227 / 228  Training Loss  0.012381530366837978\n","  12    |    -    |   0.001693   | 88.071646\n","----------------------------------------------------------------------\n","Running epoch: 12\n","Epoch  12 Batch  0 / 228  Training Loss  0.0005683598574250937\n","Epoch  12 Batch  1 / 228  Training Loss  0.0026227051857858896\n","Epoch  12 Batch  2 / 228  Training Loss  0.000953243114054203\n","Epoch  12 Batch  3 / 228  Training Loss  0.0016371195670217276\n","Epoch  12 Batch  4 / 228  Training Loss  0.0038167049642652273\n","Epoch  12 Batch  5 / 228  Training Loss  0.016591064631938934\n","Epoch  12 Batch  6 / 228  Training Loss  0.0043005673214793205\n","Epoch  12 Batch  7 / 228  Training Loss  0.0017835251055657864\n","Epoch  12 Batch  8 / 228  Training Loss  0.0034870172385126352\n","Epoch  12 Batch  9 / 228  Training Loss  0.007069380488246679\n","Epoch  12 Batch  10 / 228  Training Loss  0.00065380654996261\n","Epoch  12 Batch  11 / 228  Training Loss  0.001066006370820105\n","Epoch  12 Batch  12 / 228  Training Loss  0.0028940916527062654\n","Epoch  12 Batch  13 / 228  Training Loss  0.002274012891575694\n","Epoch  12 Batch  14 / 228  Training Loss  0.0003171283460687846\n","Epoch  12 Batch  15 / 228  Training Loss  0.0015896034892648458\n","Epoch  12 Batch  16 / 228  Training Loss  0.0012249890714883804\n","Epoch  12 Batch  17 / 228  Training Loss  0.00028169728466309607\n","Epoch  12 Batch  18 / 228  Training Loss  0.0007815504213795066\n","Epoch  12 Batch  19 / 228  Training Loss  0.0020236826967447996\n","Epoch  12 Batch  20 / 228  Training Loss  0.00034194925683550537\n","Epoch  12 Batch  21 / 228  Training Loss  0.0013265347806736827\n","Epoch  12 Batch  22 / 228  Training Loss  0.00036483383155427873\n","Epoch  12 Batch  23 / 228  Training Loss  0.0005031658220104873\n","Epoch  12 Batch  24 / 228  Training Loss  0.00046018415014259517\n","Epoch  12 Batch  25 / 228  Training Loss  0.002631125506013632\n","Epoch  12 Batch  26 / 228  Training Loss  0.00042496289825066924\n","Epoch  12 Batch  27 / 228  Training Loss  0.0005280664190649986\n","Epoch  12 Batch  28 / 228  Training Loss  0.0006351251504383981\n","Epoch  12 Batch  29 / 228  Training Loss  0.0010053779697045684\n","Epoch  12 Batch  30 / 228  Training Loss  0.0012670287396758795\n","Epoch  12 Batch  31 / 228  Training Loss  0.000425213307607919\n","Epoch  12 Batch  32 / 228  Training Loss  0.00035051992745138705\n","Epoch  12 Batch  33 / 228  Training Loss  0.0004906461690552533\n","Epoch  12 Batch  34 / 228  Training Loss  0.0005597589770331979\n","Epoch  12 Batch  35 / 228  Training Loss  0.0009851703653112054\n","Epoch  12 Batch  36 / 228  Training Loss  0.0003704308473970741\n","Epoch  12 Batch  37 / 228  Training Loss  0.0006965383654460311\n","Epoch  12 Batch  38 / 228  Training Loss  0.0003623700176831335\n","Epoch  12 Batch  39 / 228  Training Loss  0.0005117811379022896\n","Epoch  12 Batch  40 / 228  Training Loss  0.0004552096070256084\n","Epoch  12 Batch  41 / 228  Training Loss  0.0003476798301562667\n","Epoch  12 Batch  42 / 228  Training Loss  0.00037019155570305884\n","Epoch  12 Batch  43 / 228  Training Loss  0.00029893641476519406\n","Epoch  12 Batch  44 / 228  Training Loss  0.00043785080197267234\n","Epoch  12 Batch  45 / 228  Training Loss  0.0018963577458634973\n","Epoch  12 Batch  46 / 228  Training Loss  0.0011678654700517654\n","Epoch  12 Batch  47 / 228  Training Loss  0.0019745153840631247\n","Epoch  12 Batch  48 / 228  Training Loss  0.005501545965671539\n","Epoch  12 Batch  49 / 228  Training Loss  0.00041581832920201123\n","Epoch  12 Batch  50 / 228  Training Loss  0.0008971802308224142\n","Epoch  12 Batch  51 / 228  Training Loss  0.00038085842970758677\n","Epoch  12 Batch  52 / 228  Training Loss  0.001661086454987526\n","Epoch  12 Batch  53 / 228  Training Loss  0.0002974769740831107\n","Epoch  12 Batch  54 / 228  Training Loss  0.0005688515375368297\n","Epoch  12 Batch  55 / 228  Training Loss  0.0003026375197805464\n","Epoch  12 Batch  56 / 228  Training Loss  0.00046513977576978505\n","Epoch  12 Batch  57 / 228  Training Loss  0.00038492580642923713\n","Epoch  12 Batch  58 / 228  Training Loss  0.0005284635117277503\n","Epoch  12 Batch  59 / 228  Training Loss  0.0017454419285058975\n","Epoch  12 Batch  60 / 228  Training Loss  0.000634848780464381\n","Epoch  12 Batch  61 / 228  Training Loss  0.0003406822797842324\n","Epoch  12 Batch  62 / 228  Training Loss  0.002194357570260763\n","Epoch  12 Batch  63 / 228  Training Loss  0.0012697929050773382\n","Epoch  12 Batch  64 / 228  Training Loss  0.0013415080029517412\n","Epoch  12 Batch  65 / 228  Training Loss  0.0037222064565867186\n","Epoch  12 Batch  66 / 228  Training Loss  0.0005661737523041666\n","Epoch  12 Batch  67 / 228  Training Loss  0.00042641215259209275\n","Epoch  12 Batch  68 / 228  Training Loss  0.0012336972868070006\n","Epoch  12 Batch  69 / 228  Training Loss  0.00026632787194103\n","Epoch  12 Batch  70 / 228  Training Loss  0.0007075874600559473\n","Epoch  12 Batch  71 / 228  Training Loss  0.0005491377087309957\n","Epoch  12 Batch  72 / 228  Training Loss  0.00042636992293410003\n","Epoch  12 Batch  73 / 228  Training Loss  0.00022178489598445594\n","Epoch  12 Batch  74 / 228  Training Loss  0.0003457062703091651\n","Epoch  12 Batch  75 / 228  Training Loss  0.0002910019247792661\n","Epoch  12 Batch  76 / 228  Training Loss  0.0003593118453864008\n","Epoch  12 Batch  77 / 228  Training Loss  0.00043055973947048187\n","Epoch  12 Batch  78 / 228  Training Loss  0.00045714667066931725\n","Epoch  12 Batch  79 / 228  Training Loss  0.001370857353322208\n","Epoch  12 Batch  80 / 228  Training Loss  0.0002455268695484847\n","Epoch  12 Batch  81 / 228  Training Loss  0.0009475087281316519\n","Epoch  12 Batch  82 / 228  Training Loss  0.0011753687867894769\n","Epoch  12 Batch  83 / 228  Training Loss  0.0004469265113584697\n","Epoch  12 Batch  84 / 228  Training Loss  0.0006746145081706345\n","Epoch  12 Batch  85 / 228  Training Loss  0.00026773367426358163\n","Epoch  12 Batch  86 / 228  Training Loss  0.00027145427884534\n","Epoch  12 Batch  87 / 228  Training Loss  0.0006664932589046657\n","Epoch  12 Batch  88 / 228  Training Loss  0.0002762570802588016\n","Epoch  12 Batch  89 / 228  Training Loss  0.0002613017859403044\n","Epoch  12 Batch  90 / 228  Training Loss  0.00033339482615701854\n","Epoch  12 Batch  91 / 228  Training Loss  0.000779160181991756\n","Epoch  12 Batch  92 / 228  Training Loss  0.0002071086928481236\n","Epoch  12 Batch  93 / 228  Training Loss  0.0004381850885692984\n","Epoch  12 Batch  94 / 228  Training Loss  0.00046273364569060504\n","Epoch  12 Batch  95 / 228  Training Loss  0.00020523130660876632\n","Epoch  12 Batch  96 / 228  Training Loss  0.00023233077081386\n","Epoch  12 Batch  97 / 228  Training Loss  0.0005994272651150823\n","Epoch  12 Batch  98 / 228  Training Loss  0.00040304710273630917\n","Epoch  12 Batch  99 / 228  Training Loss  0.0004930005525238812\n","Epoch  12 Batch  100 / 228  Training Loss  0.0006109083187766373\n","Epoch  12 Batch  101 / 228  Training Loss  0.0005239163292571902\n","Epoch  12 Batch  102 / 228  Training Loss  0.0003250946174375713\n","Epoch  12 Batch  103 / 228  Training Loss  0.00032404164085164666\n","Epoch  12 Batch  104 / 228  Training Loss  0.00030041715945117176\n","Epoch  12 Batch  105 / 228  Training Loss  0.0005890832399018109\n","Epoch  12 Batch  106 / 228  Training Loss  0.00047084505786187947\n","Epoch  12 Batch  107 / 228  Training Loss  0.00047404292854480445\n","Epoch  12 Batch  108 / 228  Training Loss  0.00027311418671160936\n","Epoch  12 Batch  109 / 228  Training Loss  0.00017746214871294796\n","Epoch  12 Batch  110 / 228  Training Loss  0.00039503659354522824\n","Epoch  12 Batch  111 / 228  Training Loss  0.0002051167975878343\n","Epoch  12 Batch  112 / 228  Training Loss  0.0002760552742984146\n","Epoch  12 Batch  113 / 228  Training Loss  0.0010364946210756898\n","Epoch  12 Batch  114 / 228  Training Loss  0.010251292027533054\n","Epoch  12 Batch  115 / 228  Training Loss  0.0009076358401216567\n","Epoch  12 Batch  116 / 228  Training Loss  0.0010186798172071576\n","Epoch  12 Batch  117 / 228  Training Loss  0.0004729664360638708\n","Epoch  12 Batch  118 / 228  Training Loss  0.0018176882294937968\n","Epoch  12 Batch  119 / 228  Training Loss  0.00038083505933173\n","Epoch  12 Batch  120 / 228  Training Loss  0.0005077713285572827\n","Epoch  12 Batch  121 / 228  Training Loss  0.00022855277347844094\n","Epoch  12 Batch  122 / 228  Training Loss  0.0003264833358116448\n","Epoch  12 Batch  123 / 228  Training Loss  0.00038321345346048474\n","Epoch  12 Batch  124 / 228  Training Loss  0.00017160037532448769\n","Epoch  12 Batch  125 / 228  Training Loss  0.0003754638892132789\n","Epoch  12 Batch  126 / 228  Training Loss  0.0003517277946230024\n","Epoch  12 Batch  127 / 228  Training Loss  0.00026871098089031875\n","Epoch  12 Batch  128 / 228  Training Loss  0.00028411464882083237\n","Epoch  12 Batch  129 / 228  Training Loss  0.0003174144367221743\n","Epoch  12 Batch  130 / 228  Training Loss  0.00018062154413200915\n","Epoch  12 Batch  131 / 228  Training Loss  0.0005025686114095151\n","Epoch  12 Batch  132 / 228  Training Loss  0.000340836588293314\n","Epoch  12 Batch  133 / 228  Training Loss  0.00024195264268200845\n","Epoch  12 Batch  134 / 228  Training Loss  0.00023696314019616693\n","Epoch  12 Batch  135 / 228  Training Loss  0.0002489670878276229\n","Epoch  12 Batch  136 / 228  Training Loss  0.0005530827329494059\n","Epoch  12 Batch  137 / 228  Training Loss  0.00037251238245517015\n","Epoch  12 Batch  138 / 228  Training Loss  0.00037713273195549846\n","Epoch  12 Batch  139 / 228  Training Loss  0.0002984263701364398\n","Epoch  12 Batch  140 / 228  Training Loss  0.0001239696575794369\n","Epoch  12 Batch  141 / 228  Training Loss  0.0006142454803921282\n","Epoch  12 Batch  142 / 228  Training Loss  0.00024355360073968768\n","Epoch  12 Batch  143 / 228  Training Loss  0.0002180238807341084\n","Epoch  12 Batch  144 / 228  Training Loss  0.0002867642033379525\n","Epoch  12 Batch  145 / 228  Training Loss  0.0001975344930542633\n","Epoch  12 Batch  146 / 228  Training Loss  0.00018753111362457275\n","Epoch  12 Batch  147 / 228  Training Loss  0.00016174398479051888\n","Epoch  12 Batch  148 / 228  Training Loss  0.0001584113051649183\n","Epoch  12 Batch  149 / 228  Training Loss  0.0001394199498463422\n","Epoch  12 Batch  150 / 228  Training Loss  0.00020226222113706172\n","Epoch  12 Batch  151 / 228  Training Loss  0.00019112597510684282\n","Epoch  12 Batch  152 / 228  Training Loss  0.00020965015573892742\n","Epoch  12 Batch  153 / 228  Training Loss  0.00014771254791412503\n","Epoch  12 Batch  154 / 228  Training Loss  0.0005604581092484295\n","Epoch  12 Batch  155 / 228  Training Loss  0.0002378834324190393\n","Epoch  12 Batch  156 / 228  Training Loss  0.001430026488378644\n","Epoch  12 Batch  157 / 228  Training Loss  0.0010962217347696424\n","Epoch  12 Batch  158 / 228  Training Loss  0.0003580933262128383\n","Epoch  12 Batch  159 / 228  Training Loss  0.00038612374919466674\n","Epoch  12 Batch  160 / 228  Training Loss  0.00024072871019598097\n","Epoch  12 Batch  161 / 228  Training Loss  0.0004862448840867728\n","Epoch  12 Batch  162 / 228  Training Loss  0.00022035023721400648\n","Epoch  12 Batch  163 / 228  Training Loss  0.0002103983861161396\n","Epoch  12 Batch  164 / 228  Training Loss  0.0007096989429555833\n","Epoch  12 Batch  165 / 228  Training Loss  0.00032167654717341065\n","Epoch  12 Batch  166 / 228  Training Loss  0.0010750177316367626\n","Epoch  12 Batch  167 / 228  Training Loss  0.0016422668704763055\n","Epoch  12 Batch  168 / 228  Training Loss  0.0010901682544499636\n","Epoch  12 Batch  169 / 228  Training Loss  0.013404789380729198\n","Epoch  12 Batch  170 / 228  Training Loss  0.007280692458152771\n","Epoch  12 Batch  171 / 228  Training Loss  0.009640494361519814\n","Epoch  12 Batch  172 / 228  Training Loss  0.0034746292512863874\n","Epoch  12 Batch  173 / 228  Training Loss  0.0009500384912826121\n","Epoch  12 Batch  174 / 228  Training Loss  0.002098559867590666\n","Epoch  12 Batch  175 / 228  Training Loss  0.005041592288762331\n","Epoch  12 Batch  176 / 228  Training Loss  0.0037815202958881855\n","Epoch  12 Batch  177 / 228  Training Loss  0.000919888901989907\n","Epoch  12 Batch  178 / 228  Training Loss  0.00294848857447505\n","Epoch  12 Batch  179 / 228  Training Loss  0.003232441609725356\n","Epoch  12 Batch  180 / 228  Training Loss  0.0009608087711967528\n","Epoch  12 Batch  181 / 228  Training Loss  0.000490049016661942\n","Epoch  12 Batch  182 / 228  Training Loss  0.00045827726717107\n","Epoch  12 Batch  183 / 228  Training Loss  0.0017814678139984608\n","Epoch  12 Batch  184 / 228  Training Loss  0.006416873075067997\n","Epoch  12 Batch  185 / 228  Training Loss  0.0016212272457778454\n","Epoch  12 Batch  186 / 228  Training Loss  0.007033772300928831\n","Epoch  12 Batch  187 / 228  Training Loss  0.13722914457321167\n","Epoch  12 Batch  188 / 228  Training Loss  0.01101275347173214\n","Epoch  12 Batch  189 / 228  Training Loss  0.0052776457741856575\n","Epoch  12 Batch  190 / 228  Training Loss  0.0014407183043658733\n","Epoch  12 Batch  191 / 228  Training Loss  0.0009732068283483386\n","Epoch  12 Batch  192 / 228  Training Loss  0.0026329734828323126\n","Epoch  12 Batch  193 / 228  Training Loss  0.0012790752807632089\n","Epoch  12 Batch  194 / 228  Training Loss  0.0012470690999180079\n","Epoch  12 Batch  195 / 228  Training Loss  0.002230728743597865\n","Epoch  12 Batch  196 / 228  Training Loss  0.0007965663680806756\n","Epoch  12 Batch  197 / 228  Training Loss  0.0009386924793943763\n","Epoch  12 Batch  198 / 228  Training Loss  0.0007712571532465518\n","Epoch  12 Batch  199 / 228  Training Loss  0.0007661189883947372\n","Epoch  12 Batch  200 / 228  Training Loss  0.0013291400391608477\n","Epoch  12 Batch  201 / 228  Training Loss  0.0007109622820280492\n","Epoch  12 Batch  202 / 228  Training Loss  0.00044823033385910094\n","Epoch  12 Batch  203 / 228  Training Loss  0.0015055978437885642\n","Epoch  12 Batch  204 / 228  Training Loss  0.0013645754661411047\n","Epoch  12 Batch  205 / 228  Training Loss  0.0014722758205607533\n","Epoch  12 Batch  206 / 228  Training Loss  0.00041993334889411926\n","Epoch  12 Batch  207 / 228  Training Loss  0.001266756560653448\n","Epoch  12 Batch  208 / 228  Training Loss  0.0004958031931892037\n","Epoch  12 Batch  209 / 228  Training Loss  0.000615916564129293\n","Epoch  12 Batch  210 / 228  Training Loss  0.0005704784416593611\n","Epoch  12 Batch  211 / 228  Training Loss  0.00035815552109852433\n","Epoch  12 Batch  212 / 228  Training Loss  0.0005915314541198313\n","Epoch  12 Batch  213 / 228  Training Loss  0.003149532014504075\n","Epoch  12 Batch  214 / 228  Training Loss  0.0042448085732758045\n","Epoch  12 Batch  215 / 228  Training Loss  0.0006294936174526811\n","Epoch  12 Batch  216 / 228  Training Loss  0.0020747699309140444\n","Epoch  12 Batch  217 / 228  Training Loss  0.0008083541761152446\n","Epoch  12 Batch  218 / 228  Training Loss  0.006582426838576794\n","Epoch  12 Batch  219 / 228  Training Loss  0.0022049599792808294\n","Epoch  12 Batch  220 / 228  Training Loss  0.00518428860232234\n","Epoch  12 Batch  221 / 228  Training Loss  0.0012366881128400564\n","Epoch  12 Batch  222 / 228  Training Loss  0.002173957647755742\n","Epoch  12 Batch  223 / 228  Training Loss  0.002997675212100148\n","Epoch  12 Batch  224 / 228  Training Loss  0.007847442291676998\n","Epoch  12 Batch  225 / 228  Training Loss  0.0031596822664141655\n","Epoch  12 Batch  226 / 228  Training Loss  0.0017262313049286604\n","Epoch  12 Batch  227 / 228  Training Loss  0.0008473306079395115\n","  13    |    -    |   0.001980   | 89.291159\n","----------------------------------------------------------------------\n","Running epoch: 13\n","Epoch  13 Batch  0 / 228  Training Loss  0.004435326438397169\n","Epoch  13 Batch  1 / 228  Training Loss  0.0012751613976433873\n","Epoch  13 Batch  2 / 228  Training Loss  0.001984813716262579\n","Epoch  13 Batch  3 / 228  Training Loss  0.0010634195059537888\n","Epoch  13 Batch  4 / 228  Training Loss  0.0012385997688397765\n","Epoch  13 Batch  5 / 228  Training Loss  0.000598279875703156\n","Epoch  13 Batch  6 / 228  Training Loss  0.0008003661059774458\n","Epoch  13 Batch  7 / 228  Training Loss  0.0014738173922523856\n","Epoch  13 Batch  8 / 228  Training Loss  0.0006285145645961165\n","Epoch  13 Batch  9 / 228  Training Loss  0.00061949179507792\n","Epoch  13 Batch  10 / 228  Training Loss  0.0008669859962537885\n","Epoch  13 Batch  11 / 228  Training Loss  0.0008063414716161788\n","Epoch  13 Batch  12 / 228  Training Loss  0.002288864692673087\n","Epoch  13 Batch  13 / 228  Training Loss  0.003504067426547408\n","Epoch  13 Batch  14 / 228  Training Loss  0.0005355515168048441\n","Epoch  13 Batch  15 / 228  Training Loss  0.0014879932859912515\n","Epoch  13 Batch  16 / 228  Training Loss  0.002415439812466502\n","Epoch  13 Batch  17 / 228  Training Loss  0.0005915104993619025\n","Epoch  13 Batch  18 / 228  Training Loss  0.0002761585346888751\n","Epoch  13 Batch  19 / 228  Training Loss  0.0005311616114340723\n","Epoch  13 Batch  20 / 228  Training Loss  0.005961898248642683\n","Epoch  13 Batch  21 / 228  Training Loss  0.0013209163444116712\n","Epoch  13 Batch  22 / 228  Training Loss  0.0014610284706577659\n","Epoch  13 Batch  23 / 228  Training Loss  0.00029782752972096205\n","Epoch  13 Batch  24 / 228  Training Loss  0.0011667992221191525\n","Epoch  13 Batch  25 / 228  Training Loss  0.0008054380887188017\n","Epoch  13 Batch  26 / 228  Training Loss  0.0009128914098255336\n","Epoch  13 Batch  27 / 228  Training Loss  0.0013951243599876761\n","Epoch  13 Batch  28 / 228  Training Loss  0.00044593948405236006\n","Epoch  13 Batch  29 / 228  Training Loss  0.004297066945582628\n","Epoch  13 Batch  30 / 228  Training Loss  0.00028452760307118297\n","Epoch  13 Batch  31 / 228  Training Loss  0.0007100030779838562\n","Epoch  13 Batch  32 / 228  Training Loss  0.0021231777500361204\n","Epoch  13 Batch  33 / 228  Training Loss  0.0008333332370966673\n","Epoch  13 Batch  34 / 228  Training Loss  0.00046902886242605746\n","Epoch  13 Batch  35 / 228  Training Loss  0.000787844997830689\n","Epoch  13 Batch  36 / 228  Training Loss  0.001281260629184544\n","Epoch  13 Batch  37 / 228  Training Loss  0.00040580591303296387\n","Epoch  13 Batch  38 / 228  Training Loss  0.00032505314447917044\n","Epoch  13 Batch  39 / 228  Training Loss  0.0014919438399374485\n","Epoch  13 Batch  40 / 228  Training Loss  0.0008140384452417493\n","Epoch  13 Batch  41 / 228  Training Loss  0.0006057260907255113\n","Epoch  13 Batch  42 / 228  Training Loss  0.0005237993318587542\n","Epoch  13 Batch  43 / 228  Training Loss  0.0005021918914280832\n","Epoch  13 Batch  44 / 228  Training Loss  0.0007367099751718342\n","Epoch  13 Batch  45 / 228  Training Loss  0.0008958707912825048\n","Epoch  13 Batch  46 / 228  Training Loss  0.0007248243782669306\n","Epoch  13 Batch  47 / 228  Training Loss  0.0002838399668689817\n","Epoch  13 Batch  48 / 228  Training Loss  0.001558927004225552\n","Epoch  13 Batch  49 / 228  Training Loss  0.0005317271570675075\n","Epoch  13 Batch  50 / 228  Training Loss  0.0006539030582644045\n","Epoch  13 Batch  51 / 228  Training Loss  0.0010694819502532482\n","Epoch  13 Batch  52 / 228  Training Loss  0.00028893473790958524\n","Epoch  13 Batch  53 / 228  Training Loss  0.00045660772593691945\n","Epoch  13 Batch  54 / 228  Training Loss  0.00038608050090260804\n","Epoch  13 Batch  55 / 228  Training Loss  0.000476592656923458\n","Epoch  13 Batch  56 / 228  Training Loss  0.00038437082548625767\n","Epoch  13 Batch  57 / 228  Training Loss  0.0002659450692590326\n","Epoch  13 Batch  58 / 228  Training Loss  0.0005701417685486376\n","Epoch  13 Batch  59 / 228  Training Loss  0.0003667817800305784\n","Epoch  13 Batch  60 / 228  Training Loss  0.0003129949909634888\n","Epoch  13 Batch  61 / 228  Training Loss  0.000279313389910385\n","Epoch  13 Batch  62 / 228  Training Loss  0.0006404912564903498\n","Epoch  13 Batch  63 / 228  Training Loss  0.0006661543739028275\n","Epoch  13 Batch  64 / 228  Training Loss  0.00025352189550176263\n","Epoch  13 Batch  65 / 228  Training Loss  0.00030290588620118797\n","Epoch  13 Batch  66 / 228  Training Loss  0.0002771703293547034\n","Epoch  13 Batch  67 / 228  Training Loss  0.0008574991370551288\n","Epoch  13 Batch  68 / 228  Training Loss  0.00047608252498321235\n","Epoch  13 Batch  69 / 228  Training Loss  0.00033632724080234766\n","Epoch  13 Batch  70 / 228  Training Loss  0.000534643477294594\n","Epoch  13 Batch  71 / 228  Training Loss  0.00028121433570049703\n","Epoch  13 Batch  72 / 228  Training Loss  0.0009361676056869328\n","Epoch  13 Batch  73 / 228  Training Loss  0.0005729240947403014\n","Epoch  13 Batch  74 / 228  Training Loss  0.00023892529134172946\n","Epoch  13 Batch  75 / 228  Training Loss  0.0004117709759157151\n","Epoch  13 Batch  76 / 228  Training Loss  0.00030179141322150826\n","Epoch  13 Batch  77 / 228  Training Loss  0.00035605730954557657\n","Epoch  13 Batch  78 / 228  Training Loss  0.0005104277515783906\n","Epoch  13 Batch  79 / 228  Training Loss  0.00026851234724745154\n","Epoch  13 Batch  80 / 228  Training Loss  0.0003566198574844748\n","Epoch  13 Batch  81 / 228  Training Loss  0.0003563227073755115\n","Epoch  13 Batch  82 / 228  Training Loss  0.00017865911650005728\n","Epoch  13 Batch  83 / 228  Training Loss  0.0010605587158352137\n","Epoch  13 Batch  84 / 228  Training Loss  0.00024532520910725\n","Epoch  13 Batch  85 / 228  Training Loss  0.00036253297002986073\n","Epoch  13 Batch  86 / 228  Training Loss  0.0004373607225716114\n","Epoch  13 Batch  87 / 228  Training Loss  0.00020161816792096943\n","Epoch  13 Batch  88 / 228  Training Loss  0.008619707077741623\n","Epoch  13 Batch  89 / 228  Training Loss  0.021074801683425903\n","Epoch  13 Batch  90 / 228  Training Loss  0.0014905837597325444\n","Epoch  13 Batch  91 / 228  Training Loss  0.0015557091683149338\n","Epoch  13 Batch  92 / 228  Training Loss  0.00419261772185564\n","Epoch  13 Batch  93 / 228  Training Loss  0.030704936012625694\n","Epoch  13 Batch  94 / 228  Training Loss  0.017320580780506134\n","Epoch  13 Batch  95 / 228  Training Loss  0.016277115792036057\n","Epoch  13 Batch  96 / 228  Training Loss  0.005103005561977625\n","Epoch  13 Batch  97 / 228  Training Loss  0.0021485413890331984\n","Epoch  13 Batch  98 / 228  Training Loss  0.0010593754705041647\n","Epoch  13 Batch  99 / 228  Training Loss  0.003362073563039303\n","Epoch  13 Batch  100 / 228  Training Loss  0.0007947646081447601\n","Epoch  13 Batch  101 / 228  Training Loss  0.008679505437612534\n","Epoch  13 Batch  102 / 228  Training Loss  0.0010137382196262479\n","Epoch  13 Batch  103 / 228  Training Loss  0.004288411699235439\n","Epoch  13 Batch  104 / 228  Training Loss  0.018829690292477608\n","Epoch  13 Batch  105 / 228  Training Loss  0.0056964014656841755\n","Epoch  13 Batch  106 / 228  Training Loss  0.004370012786239386\n","Epoch  13 Batch  107 / 228  Training Loss  0.006499344948679209\n","Epoch  13 Batch  108 / 228  Training Loss  0.003100002883002162\n","Epoch  13 Batch  109 / 228  Training Loss  0.0004513921157922596\n","Epoch  13 Batch  110 / 228  Training Loss  0.0005326294922269881\n","Epoch  13 Batch  111 / 228  Training Loss  0.0007006173254922032\n","Epoch  13 Batch  112 / 228  Training Loss  0.0022881748154759407\n","Epoch  13 Batch  113 / 228  Training Loss  0.0023470460437238216\n","Epoch  13 Batch  114 / 228  Training Loss  0.001115851104259491\n","Epoch  13 Batch  115 / 228  Training Loss  0.002408910309895873\n","Epoch  13 Batch  116 / 228  Training Loss  0.0008219642913900316\n","Epoch  13 Batch  117 / 228  Training Loss  0.000331223156535998\n","Epoch  13 Batch  118 / 228  Training Loss  0.000518304412253201\n","Epoch  13 Batch  119 / 228  Training Loss  0.0002606794878374785\n","Epoch  13 Batch  120 / 228  Training Loss  0.002314896322786808\n","Epoch  13 Batch  121 / 228  Training Loss  0.003308150451630354\n","Epoch  13 Batch  122 / 228  Training Loss  0.0010839583119377494\n","Epoch  13 Batch  123 / 228  Training Loss  0.0005208207876421511\n","Epoch  13 Batch  124 / 228  Training Loss  0.0005282622878439724\n","Epoch  13 Batch  125 / 228  Training Loss  0.0009681361261755228\n","Epoch  13 Batch  126 / 228  Training Loss  0.008372807875275612\n","Epoch  13 Batch  127 / 228  Training Loss  0.00046192764420993626\n","Epoch  13 Batch  128 / 228  Training Loss  0.0005902125267311931\n","Epoch  13 Batch  129 / 228  Training Loss  0.005854849237948656\n","Epoch  13 Batch  130 / 228  Training Loss  0.0011824070243164897\n","Epoch  13 Batch  131 / 228  Training Loss  0.0018076495034620166\n","Epoch  13 Batch  132 / 228  Training Loss  0.0019492561696097255\n","Epoch  13 Batch  133 / 228  Training Loss  0.0005482185515575111\n","Epoch  13 Batch  134 / 228  Training Loss  0.001256901421584189\n","Epoch  13 Batch  135 / 228  Training Loss  0.00037900859024375677\n","Epoch  13 Batch  136 / 228  Training Loss  0.003393824677914381\n","Epoch  13 Batch  137 / 228  Training Loss  0.0003493653202895075\n","Epoch  13 Batch  138 / 228  Training Loss  0.002519381931051612\n","Epoch  13 Batch  139 / 228  Training Loss  0.0010497855255380273\n","Epoch  13 Batch  140 / 228  Training Loss  0.0022948652040213346\n","Epoch  13 Batch  141 / 228  Training Loss  0.0013755824184045196\n","Epoch  13 Batch  142 / 228  Training Loss  0.0008230877574533224\n","Epoch  13 Batch  143 / 228  Training Loss  0.001519747544080019\n","Epoch  13 Batch  144 / 228  Training Loss  0.0004928471171297133\n","Epoch  13 Batch  145 / 228  Training Loss  0.004273734986782074\n","Epoch  13 Batch  146 / 228  Training Loss  0.002263397676870227\n","Epoch  13 Batch  147 / 228  Training Loss  0.00025942089268937707\n","Epoch  13 Batch  148 / 228  Training Loss  0.0012108772061765194\n","Epoch  13 Batch  149 / 228  Training Loss  0.0002452706976328045\n","Epoch  13 Batch  150 / 228  Training Loss  0.0003925628843717277\n","Epoch  13 Batch  151 / 228  Training Loss  0.0007238517282530665\n","Epoch  13 Batch  152 / 228  Training Loss  0.0003977358865085989\n","Epoch  13 Batch  153 / 228  Training Loss  0.00043601574725471437\n","Epoch  13 Batch  154 / 228  Training Loss  0.0007099692011252046\n","Epoch  13 Batch  155 / 228  Training Loss  0.002196710091084242\n","Epoch  13 Batch  156 / 228  Training Loss  0.0006365914014168084\n","Epoch  13 Batch  157 / 228  Training Loss  0.0014433502219617367\n","Epoch  13 Batch  158 / 228  Training Loss  0.0015025173779577017\n","Epoch  13 Batch  159 / 228  Training Loss  0.000533278682269156\n","Epoch  13 Batch  160 / 228  Training Loss  0.00025348245981149375\n","Epoch  13 Batch  161 / 228  Training Loss  0.0008374112076126039\n","Epoch  13 Batch  162 / 228  Training Loss  0.00029373078723438084\n","Epoch  13 Batch  163 / 228  Training Loss  0.00027087601483799517\n","Epoch  13 Batch  164 / 228  Training Loss  0.0005269112880341709\n","Epoch  13 Batch  165 / 228  Training Loss  0.00034920262987725437\n","Epoch  13 Batch  166 / 228  Training Loss  0.00028825653134845197\n","Epoch  13 Batch  167 / 228  Training Loss  0.00028265363653190434\n","Epoch  13 Batch  168 / 228  Training Loss  0.00031258861417882144\n","Epoch  13 Batch  169 / 228  Training Loss  0.0006520174792967737\n","Epoch  13 Batch  170 / 228  Training Loss  0.0002599354775156826\n","Epoch  13 Batch  171 / 228  Training Loss  0.00014971618657000363\n","Epoch  13 Batch  172 / 228  Training Loss  0.0004273609956726432\n","Epoch  13 Batch  173 / 228  Training Loss  0.0005002084653824568\n","Epoch  13 Batch  174 / 228  Training Loss  0.00022104641539044678\n","Epoch  13 Batch  175 / 228  Training Loss  0.00016901674098335207\n","Epoch  13 Batch  176 / 228  Training Loss  0.0012134878197684884\n","Epoch  13 Batch  177 / 228  Training Loss  0.00038123491685837507\n","Epoch  13 Batch  178 / 228  Training Loss  0.0004310131480451673\n","Epoch  13 Batch  179 / 228  Training Loss  0.0003680683148559183\n","Epoch  13 Batch  180 / 228  Training Loss  0.001968534430488944\n","Epoch  13 Batch  181 / 228  Training Loss  0.0007360078161582351\n","Epoch  13 Batch  182 / 228  Training Loss  0.0008949020993895829\n","Epoch  13 Batch  183 / 228  Training Loss  0.00024104824115056545\n","Epoch  13 Batch  184 / 228  Training Loss  0.0009804714936763048\n","Epoch  13 Batch  185 / 228  Training Loss  0.00027854106156155467\n","Epoch  13 Batch  186 / 228  Training Loss  0.0003723326663020998\n","Epoch  13 Batch  187 / 228  Training Loss  0.0003583375655580312\n","Epoch  13 Batch  188 / 228  Training Loss  0.001303911441937089\n","Epoch  13 Batch  189 / 228  Training Loss  0.00026002919184975326\n","Epoch  13 Batch  190 / 228  Training Loss  0.00029282330069690943\n","Epoch  13 Batch  191 / 228  Training Loss  0.00047413614811375737\n","Epoch  13 Batch  192 / 228  Training Loss  0.0003100924368482083\n","Epoch  13 Batch  193 / 228  Training Loss  0.00036667671520262957\n","Epoch  13 Batch  194 / 228  Training Loss  0.0009628875995986164\n","Epoch  13 Batch  195 / 228  Training Loss  0.00035512723843567073\n","Epoch  13 Batch  196 / 228  Training Loss  0.00021010155614931136\n","Epoch  13 Batch  197 / 228  Training Loss  0.0002561375731602311\n","Epoch  13 Batch  198 / 228  Training Loss  0.0002660616592038423\n","Epoch  13 Batch  199 / 228  Training Loss  0.00023458441137336195\n","Epoch  13 Batch  200 / 228  Training Loss  0.0004239808476995677\n","Epoch  13 Batch  201 / 228  Training Loss  0.004273067694157362\n","Epoch  13 Batch  202 / 228  Training Loss  0.00029280796297825873\n","Epoch  13 Batch  203 / 228  Training Loss  0.0008914791396819055\n","Epoch  13 Batch  204 / 228  Training Loss  0.00035629505873657763\n","Epoch  13 Batch  205 / 228  Training Loss  0.00029372904100455344\n","Epoch  13 Batch  206 / 228  Training Loss  0.000518180662766099\n","Epoch  13 Batch  207 / 228  Training Loss  0.0004425212973728776\n","Epoch  13 Batch  208 / 228  Training Loss  0.0010514152236282825\n","Epoch  13 Batch  209 / 228  Training Loss  0.0002658673038240522\n","Epoch  13 Batch  210 / 228  Training Loss  0.0009609787375666201\n","Epoch  13 Batch  211 / 228  Training Loss  0.0031021402683109045\n","Epoch  13 Batch  212 / 228  Training Loss  0.00024850538466125727\n","Epoch  13 Batch  213 / 228  Training Loss  0.000966476509347558\n","Epoch  13 Batch  214 / 228  Training Loss  0.0009703405085019767\n","Epoch  13 Batch  215 / 228  Training Loss  0.0005055639194324613\n","Epoch  13 Batch  216 / 228  Training Loss  0.00036403213744051754\n","Epoch  13 Batch  217 / 228  Training Loss  0.0012467881897464395\n","Epoch  13 Batch  218 / 228  Training Loss  0.0002745207166299224\n","Epoch  13 Batch  219 / 228  Training Loss  0.00017219215806107968\n","Epoch  13 Batch  220 / 228  Training Loss  0.002119226846843958\n","Epoch  13 Batch  221 / 228  Training Loss  0.0022205219138413668\n","Epoch  13 Batch  222 / 228  Training Loss  0.0003273666079621762\n","Epoch  13 Batch  223 / 228  Training Loss  0.00036474267835728824\n","Epoch  13 Batch  224 / 228  Training Loss  0.001407615840435028\n","Epoch  13 Batch  225 / 228  Training Loss  0.0020736283622682095\n","Epoch  13 Batch  226 / 228  Training Loss  0.003276932518929243\n","Epoch  13 Batch  227 / 228  Training Loss  0.038917236030101776\n","  14    |    -    |   0.001781   | 72.789634\n","----------------------------------------------------------------------\n","Running epoch: 14\n","Epoch  14 Batch  0 / 228  Training Loss  0.032399579882621765\n","Epoch  14 Batch  1 / 228  Training Loss  0.0047386763617396355\n","Epoch  14 Batch  2 / 228  Training Loss  0.024185458198189735\n","Epoch  14 Batch  3 / 228  Training Loss  0.059272993355989456\n","Epoch  14 Batch  4 / 228  Training Loss  0.010087685659527779\n","Epoch  14 Batch  5 / 228  Training Loss  0.014708610251545906\n","Epoch  14 Batch  6 / 228  Training Loss  0.004926820285618305\n","Epoch  14 Batch  7 / 228  Training Loss  0.002979541663080454\n","Epoch  14 Batch  8 / 228  Training Loss  0.002180525567382574\n","Epoch  14 Batch  9 / 228  Training Loss  0.0061234962195158005\n","Epoch  14 Batch  10 / 228  Training Loss  0.003718229942023754\n","Epoch  14 Batch  11 / 228  Training Loss  0.0031235069036483765\n","Epoch  14 Batch  12 / 228  Training Loss  0.005754625890403986\n","Epoch  14 Batch  13 / 228  Training Loss  0.0010107563575729728\n","Epoch  14 Batch  14 / 228  Training Loss  0.004355765879154205\n","Epoch  14 Batch  15 / 228  Training Loss  0.009272689931094646\n","Epoch  14 Batch  16 / 228  Training Loss  0.0038100003730505705\n","Epoch  14 Batch  17 / 228  Training Loss  0.002803861629217863\n","Epoch  14 Batch  18 / 228  Training Loss  0.001137809013016522\n","Epoch  14 Batch  19 / 228  Training Loss  0.0009207100956700742\n","Epoch  14 Batch  20 / 228  Training Loss  0.001931976294144988\n","Epoch  14 Batch  21 / 228  Training Loss  0.000618680496700108\n","Epoch  14 Batch  22 / 228  Training Loss  0.0006501782336272299\n","Epoch  14 Batch  23 / 228  Training Loss  0.003075249958783388\n","Epoch  14 Batch  24 / 228  Training Loss  0.0013993463944643736\n","Epoch  14 Batch  25 / 228  Training Loss  0.0004109821456950158\n","Epoch  14 Batch  26 / 228  Training Loss  0.001614675740711391\n","Epoch  14 Batch  27 / 228  Training Loss  0.0005176682607270777\n","Epoch  14 Batch  28 / 228  Training Loss  0.00047324426122941077\n","Epoch  14 Batch  29 / 228  Training Loss  0.000613574346061796\n","Epoch  14 Batch  30 / 228  Training Loss  0.0009632781147956848\n","Epoch  14 Batch  31 / 228  Training Loss  0.001530010369606316\n","Epoch  14 Batch  32 / 228  Training Loss  0.0007308523054234684\n","Epoch  14 Batch  33 / 228  Training Loss  0.0003459684958215803\n","Epoch  14 Batch  34 / 228  Training Loss  0.0006894675316289067\n","Epoch  14 Batch  35 / 228  Training Loss  0.00028271539486013353\n","Epoch  14 Batch  36 / 228  Training Loss  0.0003874815593007952\n","Epoch  14 Batch  37 / 228  Training Loss  0.001831826870329678\n","Epoch  14 Batch  38 / 228  Training Loss  0.0003657642810139805\n","Epoch  14 Batch  39 / 228  Training Loss  0.0015491365920752287\n","Epoch  14 Batch  40 / 228  Training Loss  0.0010260657873004675\n","Epoch  14 Batch  41 / 228  Training Loss  0.0004428578249644488\n","Epoch  14 Batch  42 / 228  Training Loss  0.0002659732417669147\n","Epoch  14 Batch  43 / 228  Training Loss  0.00044175461516715586\n","Epoch  14 Batch  44 / 228  Training Loss  0.000769704463891685\n","Epoch  14 Batch  45 / 228  Training Loss  0.00034530225093476474\n","Epoch  14 Batch  46 / 228  Training Loss  0.00040491684922017157\n","Epoch  14 Batch  47 / 228  Training Loss  0.0002851046738214791\n","Epoch  14 Batch  48 / 228  Training Loss  0.0005932023050263524\n","Epoch  14 Batch  49 / 228  Training Loss  0.0006058972212485969\n","Epoch  14 Batch  50 / 228  Training Loss  0.00031765218591317534\n","Epoch  14 Batch  51 / 228  Training Loss  0.0004822250339202583\n","Epoch  14 Batch  52 / 228  Training Loss  0.00035486745764501393\n","Epoch  14 Batch  53 / 228  Training Loss  0.0002999613352585584\n","Epoch  14 Batch  54 / 228  Training Loss  0.00034473356208764017\n","Epoch  14 Batch  55 / 228  Training Loss  0.00038096262142062187\n","Epoch  14 Batch  56 / 228  Training Loss  0.005208519287407398\n","Epoch  14 Batch  57 / 228  Training Loss  0.0007989261066541076\n","Epoch  14 Batch  58 / 228  Training Loss  0.0004991331370547414\n","Epoch  14 Batch  59 / 228  Training Loss  0.000563964422326535\n","Epoch  14 Batch  60 / 228  Training Loss  0.0007199370884336531\n","Epoch  14 Batch  61 / 228  Training Loss  0.0004952330491505563\n","Epoch  14 Batch  62 / 228  Training Loss  0.0006000677240081131\n","Epoch  14 Batch  63 / 228  Training Loss  0.00048779023927636445\n","Epoch  14 Batch  64 / 228  Training Loss  0.0004954786854796112\n","Epoch  14 Batch  65 / 228  Training Loss  0.00018244834791403264\n","Epoch  14 Batch  66 / 228  Training Loss  0.0008431660826317966\n","Epoch  14 Batch  67 / 228  Training Loss  0.0001667876058490947\n","Epoch  14 Batch  68 / 228  Training Loss  0.0007852023118175566\n","Epoch  14 Batch  69 / 228  Training Loss  0.0017311865231022239\n","Epoch  14 Batch  70 / 228  Training Loss  0.0004392123664729297\n","Epoch  14 Batch  71 / 228  Training Loss  0.00040133472066372633\n","Epoch  14 Batch  72 / 228  Training Loss  0.0035232207737863064\n","Epoch  14 Batch  73 / 228  Training Loss  0.010171180590987206\n","Epoch  14 Batch  74 / 228  Training Loss  0.01103790570050478\n","Epoch  14 Batch  75 / 228  Training Loss  0.001065655960701406\n","Epoch  14 Batch  76 / 228  Training Loss  0.002689074259251356\n","Epoch  14 Batch  77 / 228  Training Loss  0.0038757508154958487\n","Epoch  14 Batch  78 / 228  Training Loss  0.00048128890921361744\n","Epoch  14 Batch  79 / 228  Training Loss  0.0021760938689112663\n","Epoch  14 Batch  80 / 228  Training Loss  0.008643620647490025\n","Epoch  14 Batch  81 / 228  Training Loss  0.002162123331800103\n","Epoch  14 Batch  82 / 228  Training Loss  0.002117906231433153\n","Epoch  14 Batch  83 / 228  Training Loss  0.004305346868932247\n","Epoch  14 Batch  84 / 228  Training Loss  0.00801522471010685\n","Epoch  14 Batch  85 / 228  Training Loss  0.0030522828456014395\n","Epoch  14 Batch  86 / 228  Training Loss  0.0013456991873681545\n","Epoch  14 Batch  87 / 228  Training Loss  0.001303850207477808\n","Epoch  14 Batch  88 / 228  Training Loss  0.0034157305490225554\n","Epoch  14 Batch  89 / 228  Training Loss  0.0006850428180769086\n","Epoch  14 Batch  90 / 228  Training Loss  0.001574650639668107\n","Epoch  14 Batch  91 / 228  Training Loss  0.00043744081631302834\n","Epoch  14 Batch  92 / 228  Training Loss  0.0023338599130511284\n","Epoch  14 Batch  93 / 228  Training Loss  0.0010838067391887307\n","Epoch  14 Batch  94 / 228  Training Loss  0.001598551170900464\n","Epoch  14 Batch  95 / 228  Training Loss  0.0022542395163327456\n","Epoch  14 Batch  96 / 228  Training Loss  0.0004972817841917276\n","Epoch  14 Batch  97 / 228  Training Loss  0.0003000324941240251\n","Epoch  14 Batch  98 / 228  Training Loss  0.00024313935136888176\n","Epoch  14 Batch  99 / 228  Training Loss  0.0004492810694500804\n","Epoch  14 Batch  100 / 228  Training Loss  0.0005829824949614704\n","Epoch  14 Batch  101 / 228  Training Loss  0.0035106355790048838\n","Epoch  14 Batch  102 / 228  Training Loss  0.005179031752049923\n","Epoch  14 Batch  103 / 228  Training Loss  0.012677456252276897\n","Epoch  14 Batch  104 / 228  Training Loss  0.0028536836616694927\n","Epoch  14 Batch  105 / 228  Training Loss  0.0016493145376443863\n","Epoch  14 Batch  106 / 228  Training Loss  0.0014432132011279464\n","Epoch  14 Batch  107 / 228  Training Loss  0.00038238882552832365\n","Epoch  14 Batch  108 / 228  Training Loss  0.000949588546063751\n","Epoch  14 Batch  109 / 228  Training Loss  0.0005246436921879649\n","Epoch  14 Batch  110 / 228  Training Loss  0.0033476122189313173\n","Epoch  14 Batch  111 / 228  Training Loss  0.0005229708622209728\n","Epoch  14 Batch  112 / 228  Training Loss  0.0016829435480758548\n","Epoch  14 Batch  113 / 228  Training Loss  0.000782893446739763\n","Epoch  14 Batch  114 / 228  Training Loss  0.004698128439486027\n","Epoch  14 Batch  115 / 228  Training Loss  0.0012559827882796526\n","Epoch  14 Batch  116 / 228  Training Loss  0.0005463691777549684\n","Epoch  14 Batch  117 / 228  Training Loss  0.0004618660605046898\n","Epoch  14 Batch  118 / 228  Training Loss  0.0004930596915073693\n","Epoch  14 Batch  119 / 228  Training Loss  0.0003049976075999439\n","Epoch  14 Batch  120 / 228  Training Loss  0.0005848820437677205\n","Epoch  14 Batch  121 / 228  Training Loss  0.0008077396778389812\n","Epoch  14 Batch  122 / 228  Training Loss  0.004669289104640484\n","Epoch  14 Batch  123 / 228  Training Loss  0.00040693432674743235\n","Epoch  14 Batch  124 / 228  Training Loss  0.0018160042818635702\n","Epoch  14 Batch  125 / 228  Training Loss  0.0007040971540845931\n","Epoch  14 Batch  126 / 228  Training Loss  0.00018008038750849664\n","Epoch  14 Batch  127 / 228  Training Loss  0.00045312801375985146\n","Epoch  14 Batch  128 / 228  Training Loss  0.00023560905538033694\n","Epoch  14 Batch  129 / 228  Training Loss  0.00023756601149216294\n","Epoch  14 Batch  130 / 228  Training Loss  0.00035377414315007627\n","Epoch  14 Batch  131 / 228  Training Loss  0.002450428670272231\n","Epoch  14 Batch  132 / 228  Training Loss  0.0049548945389688015\n","Epoch  14 Batch  133 / 228  Training Loss  0.00042525227763690054\n","Epoch  14 Batch  134 / 228  Training Loss  0.00022458142484538257\n","Epoch  14 Batch  135 / 228  Training Loss  0.00040588239789940417\n","Epoch  14 Batch  136 / 228  Training Loss  0.00033240800257772207\n","Epoch  14 Batch  137 / 228  Training Loss  0.00044499753857962787\n","Epoch  14 Batch  138 / 228  Training Loss  0.0005954883527010679\n","Epoch  14 Batch  139 / 228  Training Loss  0.000712746346835047\n","Epoch  14 Batch  140 / 228  Training Loss  0.0004483131633605808\n","Epoch  14 Batch  141 / 228  Training Loss  0.00027887176838703454\n","Epoch  14 Batch  142 / 228  Training Loss  0.0004169997409917414\n","Epoch  14 Batch  143 / 228  Training Loss  0.0007833337294869125\n","Epoch  14 Batch  144 / 228  Training Loss  0.00026400209753774107\n","Epoch  14 Batch  145 / 228  Training Loss  0.0002884729765355587\n","Epoch  14 Batch  146 / 228  Training Loss  0.00015509196964558214\n","Epoch  14 Batch  147 / 228  Training Loss  0.00030414111097343266\n","Epoch  14 Batch  148 / 228  Training Loss  0.00016587354184594005\n","Epoch  14 Batch  149 / 228  Training Loss  0.00037821897421963513\n","Epoch  14 Batch  150 / 228  Training Loss  0.00038273821701295674\n","Epoch  14 Batch  151 / 228  Training Loss  0.0004162259283475578\n","Epoch  14 Batch  152 / 228  Training Loss  0.00036292226286605\n","Epoch  14 Batch  153 / 228  Training Loss  0.00021550554083660245\n","Epoch  14 Batch  154 / 228  Training Loss  0.000275464728474617\n","Epoch  14 Batch  155 / 228  Training Loss  0.00029578423709608614\n","Epoch  14 Batch  156 / 228  Training Loss  0.002640490187332034\n","Epoch  14 Batch  157 / 228  Training Loss  0.00027049516211263835\n","Epoch  14 Batch  158 / 228  Training Loss  0.00038002419751137495\n","Epoch  14 Batch  159 / 228  Training Loss  0.0003650311555247754\n","Epoch  14 Batch  160 / 228  Training Loss  0.00014192763774190098\n","Epoch  14 Batch  161 / 228  Training Loss  0.0002988208143506199\n","Epoch  14 Batch  162 / 228  Training Loss  0.0002946594322565943\n","Epoch  14 Batch  163 / 228  Training Loss  0.0002995649410877377\n","Epoch  14 Batch  164 / 228  Training Loss  0.0002396264171693474\n","Epoch  14 Batch  165 / 228  Training Loss  0.0007558183278888464\n","Epoch  14 Batch  166 / 228  Training Loss  0.00020820088684558868\n","Epoch  14 Batch  167 / 228  Training Loss  0.00040366940083913505\n","Epoch  14 Batch  168 / 228  Training Loss  0.0002843157562892884\n","Epoch  14 Batch  169 / 228  Training Loss  0.0011861566454172134\n","Epoch  14 Batch  170 / 228  Training Loss  0.0008368801791220903\n","Epoch  14 Batch  171 / 228  Training Loss  0.00042524831951595843\n","Epoch  14 Batch  172 / 228  Training Loss  0.00035189930349588394\n","Epoch  14 Batch  173 / 228  Training Loss  0.0005009917658753693\n","Epoch  14 Batch  174 / 228  Training Loss  0.0002839265507645905\n","Epoch  14 Batch  175 / 228  Training Loss  0.0007606736035086215\n","Epoch  14 Batch  176 / 228  Training Loss  0.00048672372940927744\n","Epoch  14 Batch  177 / 228  Training Loss  0.0006474016117863357\n","Epoch  14 Batch  178 / 228  Training Loss  0.0003976291336584836\n","Epoch  14 Batch  179 / 228  Training Loss  0.0003281159442849457\n","Epoch  14 Batch  180 / 228  Training Loss  0.0001863885554485023\n","Epoch  14 Batch  181 / 228  Training Loss  0.00024314320762641728\n","Epoch  14 Batch  182 / 228  Training Loss  0.0005780419451184571\n","Epoch  14 Batch  183 / 228  Training Loss  0.00014924845891073346\n","Epoch  14 Batch  184 / 228  Training Loss  0.0001754900731611997\n","Epoch  14 Batch  185 / 228  Training Loss  0.0007049879059195518\n","Epoch  14 Batch  186 / 228  Training Loss  0.0005300227785483003\n","Epoch  14 Batch  187 / 228  Training Loss  0.00024747781571932137\n","Epoch  14 Batch  188 / 228  Training Loss  0.00029208444175310433\n","Epoch  14 Batch  189 / 228  Training Loss  0.000166573328897357\n","Epoch  14 Batch  190 / 228  Training Loss  0.00018462231673765928\n","Epoch  14 Batch  191 / 228  Training Loss  0.00020757972379215062\n","Epoch  14 Batch  192 / 228  Training Loss  0.0004977296921424568\n","Epoch  14 Batch  193 / 228  Training Loss  0.00028865697095170617\n","Epoch  14 Batch  194 / 228  Training Loss  0.0002566002367530018\n","Epoch  14 Batch  195 / 228  Training Loss  0.0002836865314748138\n","Epoch  14 Batch  196 / 228  Training Loss  0.00012724754924420267\n","Epoch  14 Batch  197 / 228  Training Loss  0.00024189332907553762\n","Epoch  14 Batch  198 / 228  Training Loss  0.00042449249303899705\n","Epoch  14 Batch  199 / 228  Training Loss  0.0001856349263107404\n","Epoch  14 Batch  200 / 228  Training Loss  0.00024563525221310556\n","Epoch  14 Batch  201 / 228  Training Loss  0.00019950800924561918\n","Epoch  14 Batch  202 / 228  Training Loss  0.00032742167240940034\n","Epoch  14 Batch  203 / 228  Training Loss  0.00018035781977232546\n","Epoch  14 Batch  204 / 228  Training Loss  0.000480593676911667\n","Epoch  14 Batch  205 / 228  Training Loss  0.00020353194850031286\n","Epoch  14 Batch  206 / 228  Training Loss  0.00022628541046287864\n","Epoch  14 Batch  207 / 228  Training Loss  0.0002960622950922698\n","Epoch  14 Batch  208 / 228  Training Loss  0.00019124904065392911\n","Epoch  14 Batch  209 / 228  Training Loss  0.00020946012227796018\n","Epoch  14 Batch  210 / 228  Training Loss  0.0006917171995155513\n","Epoch  14 Batch  211 / 228  Training Loss  0.00014768687833566219\n","Epoch  14 Batch  212 / 228  Training Loss  0.0012564717326313257\n","Epoch  14 Batch  213 / 228  Training Loss  0.00041663579759187996\n","Epoch  14 Batch  214 / 228  Training Loss  0.00023871270241215825\n","Epoch  14 Batch  215 / 228  Training Loss  0.0006713715847581625\n","Epoch  14 Batch  216 / 228  Training Loss  0.00024021351418923587\n","Epoch  14 Batch  217 / 228  Training Loss  0.0002959849371109158\n","Epoch  14 Batch  218 / 228  Training Loss  0.00013409709208644927\n","Epoch  14 Batch  219 / 228  Training Loss  0.00044650124618783593\n","Epoch  14 Batch  220 / 228  Training Loss  0.0002503306604921818\n","Epoch  14 Batch  221 / 228  Training Loss  0.0002197353751398623\n","Epoch  14 Batch  222 / 228  Training Loss  0.00019340289873071015\n","Epoch  14 Batch  223 / 228  Training Loss  0.00030301528749987483\n","Epoch  14 Batch  224 / 228  Training Loss  0.00036154905683360994\n","Epoch  14 Batch  225 / 228  Training Loss  0.00018967571668326855\n","Epoch  14 Batch  226 / 228  Training Loss  0.0003567697713151574\n","Epoch  14 Batch  227 / 228  Training Loss  0.0003202587249688804\n","  15    |    -    |   0.001835   | 90.586890\n","----------------------------------------------------------------------\n","Running epoch: 15\n","Epoch  15 Batch  0 / 228  Training Loss  0.00027247134130448103\n","Epoch  15 Batch  1 / 228  Training Loss  0.00017486521392129362\n","Epoch  15 Batch  2 / 228  Training Loss  0.00014415390614885837\n","Epoch  15 Batch  3 / 228  Training Loss  0.0001755345583660528\n","Epoch  15 Batch  4 / 228  Training Loss  0.0001276612892979756\n","Epoch  15 Batch  5 / 228  Training Loss  0.00012979439634364098\n","Epoch  15 Batch  6 / 228  Training Loss  0.00016548088751733303\n","Epoch  15 Batch  7 / 228  Training Loss  0.00022817229910288006\n","Epoch  15 Batch  8 / 228  Training Loss  0.00016859274182934314\n","Epoch  15 Batch  9 / 228  Training Loss  0.00018028795602731407\n","Epoch  15 Batch  10 / 228  Training Loss  0.00021051005751360208\n","Epoch  15 Batch  11 / 228  Training Loss  0.00015127747610677034\n","Epoch  15 Batch  12 / 228  Training Loss  0.0005773280863650143\n","Epoch  15 Batch  13 / 228  Training Loss  0.00015438096306752414\n","Epoch  15 Batch  14 / 228  Training Loss  0.00016401456377934664\n","Epoch  15 Batch  15 / 228  Training Loss  0.00019582119421102107\n","Epoch  15 Batch  16 / 228  Training Loss  0.00016921570932026953\n","Epoch  15 Batch  17 / 228  Training Loss  0.00012509235239122063\n","Epoch  15 Batch  18 / 228  Training Loss  0.00012949858501087874\n","Epoch  15 Batch  19 / 228  Training Loss  0.00019887911912519485\n","Epoch  15 Batch  20 / 228  Training Loss  0.0002950972702819854\n","Epoch  15 Batch  21 / 228  Training Loss  0.00020467425929382443\n","Epoch  15 Batch  22 / 228  Training Loss  0.00012642725778277963\n","Epoch  15 Batch  23 / 228  Training Loss  0.00023379124468192458\n","Epoch  15 Batch  24 / 228  Training Loss  0.00011077368253609166\n","Epoch  15 Batch  25 / 228  Training Loss  0.00017810599820222706\n","Epoch  15 Batch  26 / 228  Training Loss  0.00022656930377706885\n","Epoch  15 Batch  27 / 228  Training Loss  0.00010590090823825449\n","Epoch  15 Batch  28 / 228  Training Loss  8.039238309720531e-05\n","Epoch  15 Batch  29 / 228  Training Loss  0.00019266034360043705\n","Epoch  15 Batch  30 / 228  Training Loss  0.00014543354336638004\n","Epoch  15 Batch  31 / 228  Training Loss  0.0001565452548675239\n","Epoch  15 Batch  32 / 228  Training Loss  0.00012198342301417142\n","Epoch  15 Batch  33 / 228  Training Loss  0.00015565678768325597\n","Epoch  15 Batch  34 / 228  Training Loss  0.0001328133075730875\n","Epoch  15 Batch  35 / 228  Training Loss  0.0003281894896645099\n","Epoch  15 Batch  36 / 228  Training Loss  0.00011835077020805329\n","Epoch  15 Batch  37 / 228  Training Loss  0.00010183724225498736\n","Epoch  15 Batch  38 / 228  Training Loss  0.0001354202686343342\n","Epoch  15 Batch  39 / 228  Training Loss  0.00014438806101679802\n","Epoch  15 Batch  40 / 228  Training Loss  0.00017146646860055625\n","Epoch  15 Batch  41 / 228  Training Loss  0.0001576260692672804\n","Epoch  15 Batch  42 / 228  Training Loss  0.00019754463573917747\n","Epoch  15 Batch  43 / 228  Training Loss  0.0001423104404238984\n","Epoch  15 Batch  44 / 228  Training Loss  0.00011236588034080341\n","Epoch  15 Batch  45 / 228  Training Loss  0.00013410113751888275\n","Epoch  15 Batch  46 / 228  Training Loss  0.00014527566963806748\n","Epoch  15 Batch  47 / 228  Training Loss  0.00011694036948028952\n","Epoch  15 Batch  48 / 228  Training Loss  0.00014746052329428494\n","Epoch  15 Batch  49 / 228  Training Loss  0.00016390667587984353\n","Epoch  15 Batch  50 / 228  Training Loss  0.0001994965859921649\n","Epoch  15 Batch  51 / 228  Training Loss  0.0006048241048119962\n","Epoch  15 Batch  52 / 228  Training Loss  0.00021669495617970824\n","Epoch  15 Batch  53 / 228  Training Loss  0.00025949845439754426\n","Epoch  15 Batch  54 / 228  Training Loss  0.00020939593377988786\n","Epoch  15 Batch  55 / 228  Training Loss  0.00016520741337444633\n","Epoch  15 Batch  56 / 228  Training Loss  9.050774679053575e-05\n","Epoch  15 Batch  57 / 228  Training Loss  0.00014611250662710518\n","Epoch  15 Batch  58 / 228  Training Loss  0.00014475385250989348\n","Epoch  15 Batch  59 / 228  Training Loss  0.00010710694914450869\n","Epoch  15 Batch  60 / 228  Training Loss  0.00013055899762548506\n","Epoch  15 Batch  61 / 228  Training Loss  0.00014856331108603626\n","Epoch  15 Batch  62 / 228  Training Loss  0.00014776815078221262\n","Epoch  15 Batch  63 / 228  Training Loss  0.0001007218161248602\n","Epoch  15 Batch  64 / 228  Training Loss  0.00010951679723802954\n","Epoch  15 Batch  65 / 228  Training Loss  0.0001049024285748601\n","Epoch  15 Batch  66 / 228  Training Loss  0.00022317639377433807\n","Epoch  15 Batch  67 / 228  Training Loss  0.00010240254778182134\n","Epoch  15 Batch  68 / 228  Training Loss  0.0001288449566345662\n","Epoch  15 Batch  69 / 228  Training Loss  0.00010908101830864325\n","Epoch  15 Batch  70 / 228  Training Loss  8.93909964361228e-05\n","Epoch  15 Batch  71 / 228  Training Loss  0.00019539067579898983\n","Epoch  15 Batch  72 / 228  Training Loss  0.00014274528075475246\n","Epoch  15 Batch  73 / 228  Training Loss  0.00012772457557730377\n","Epoch  15 Batch  74 / 228  Training Loss  0.00017767443205229938\n","Epoch  15 Batch  75 / 228  Training Loss  0.00012838348629884422\n","Epoch  15 Batch  76 / 228  Training Loss  0.0001571597676957026\n","Epoch  15 Batch  77 / 228  Training Loss  0.00015250607975758612\n","Epoch  15 Batch  78 / 228  Training Loss  0.00011840055231004953\n","Epoch  15 Batch  79 / 228  Training Loss  0.0006821903516538441\n","Epoch  15 Batch  80 / 228  Training Loss  0.00020468182628974319\n","Epoch  15 Batch  81 / 228  Training Loss  0.0005333530134521425\n","Epoch  15 Batch  82 / 228  Training Loss  0.000261224020505324\n","Epoch  15 Batch  83 / 228  Training Loss  0.00016509903070982546\n","Epoch  15 Batch  84 / 228  Training Loss  0.00022336500114761293\n","Epoch  15 Batch  85 / 228  Training Loss  0.00010532584565225989\n","Epoch  15 Batch  86 / 228  Training Loss  0.0001315586268901825\n","Epoch  15 Batch  87 / 228  Training Loss  0.00013982169912196696\n","Epoch  15 Batch  88 / 228  Training Loss  0.0017430250300094485\n","Epoch  15 Batch  89 / 228  Training Loss  0.0001290733489440754\n","Epoch  15 Batch  90 / 228  Training Loss  0.00018646131502464414\n","Epoch  15 Batch  91 / 228  Training Loss  0.00017804726667236537\n","Epoch  15 Batch  92 / 228  Training Loss  0.00014252617256715894\n","Epoch  15 Batch  93 / 228  Training Loss  0.000140455988002941\n","Epoch  15 Batch  94 / 228  Training Loss  0.00011091904889326543\n","Epoch  15 Batch  95 / 228  Training Loss  0.00022221500694286078\n","Epoch  15 Batch  96 / 228  Training Loss  0.00010417393787065521\n","Epoch  15 Batch  97 / 228  Training Loss  0.0002650049573276192\n","Epoch  15 Batch  98 / 228  Training Loss  0.00032103873672895133\n","Epoch  15 Batch  99 / 228  Training Loss  0.00011411310697440058\n","Epoch  15 Batch  100 / 228  Training Loss  0.00018835785158444196\n","Epoch  15 Batch  101 / 228  Training Loss  0.00019824111950583756\n","Epoch  15 Batch  102 / 228  Training Loss  9.623655205359682e-05\n","Epoch  15 Batch  103 / 228  Training Loss  0.0004171804466750473\n","Epoch  15 Batch  104 / 228  Training Loss  0.0002144660975318402\n","Epoch  15 Batch  105 / 228  Training Loss  0.00011517271195771173\n","Epoch  15 Batch  106 / 228  Training Loss  0.00010658692190190777\n","Epoch  15 Batch  107 / 228  Training Loss  0.0001058333509718068\n","Epoch  15 Batch  108 / 228  Training Loss  0.002525747288018465\n","Epoch  15 Batch  109 / 228  Training Loss  0.00023282163601834327\n","Epoch  15 Batch  110 / 228  Training Loss  0.0001580140378791839\n","Epoch  15 Batch  111 / 228  Training Loss  0.00020429135474842042\n","Epoch  15 Batch  112 / 228  Training Loss  0.00012726220302283764\n","Epoch  15 Batch  113 / 228  Training Loss  0.00019894054275937378\n","Epoch  15 Batch  114 / 228  Training Loss  0.00010678247053874657\n","Epoch  15 Batch  115 / 228  Training Loss  0.00014632026432082057\n","Epoch  15 Batch  116 / 228  Training Loss  0.00022131793957669288\n","Epoch  15 Batch  117 / 228  Training Loss  8.04097144282423e-05\n","Epoch  15 Batch  118 / 228  Training Loss  0.00011042580445064232\n","Epoch  15 Batch  119 / 228  Training Loss  0.0001442780194338411\n","Epoch  15 Batch  120 / 228  Training Loss  0.00010822175681823865\n","Epoch  15 Batch  121 / 228  Training Loss  0.00013491822755895555\n","Epoch  15 Batch  122 / 228  Training Loss  0.00017693331756163388\n","Epoch  15 Batch  123 / 228  Training Loss  0.00033377017825841904\n","Epoch  15 Batch  124 / 228  Training Loss  8.6744170403108e-05\n","Epoch  15 Batch  125 / 228  Training Loss  9.025366307469085e-05\n","Epoch  15 Batch  126 / 228  Training Loss  0.00015475323016289622\n","Epoch  15 Batch  127 / 228  Training Loss  0.00015188210818450898\n","Epoch  15 Batch  128 / 228  Training Loss  0.0001179821920231916\n","Epoch  15 Batch  129 / 228  Training Loss  0.00014668257790617645\n","Epoch  15 Batch  130 / 228  Training Loss  0.00026155065279453993\n","Epoch  15 Batch  131 / 228  Training Loss  9.24521591514349e-05\n","Epoch  15 Batch  132 / 228  Training Loss  0.00019088381668552756\n","Epoch  15 Batch  133 / 228  Training Loss  0.00011992454528808594\n","Epoch  15 Batch  134 / 228  Training Loss  0.00016743427840992808\n","Epoch  15 Batch  135 / 228  Training Loss  0.00024028061307035387\n","Epoch  15 Batch  136 / 228  Training Loss  0.00021517739514820278\n","Epoch  15 Batch  137 / 228  Training Loss  0.0001307169732172042\n","Epoch  15 Batch  138 / 228  Training Loss  0.00011254823039053008\n","Epoch  15 Batch  139 / 228  Training Loss  0.00023489553132094443\n","Epoch  15 Batch  140 / 228  Training Loss  8.265335054602474e-05\n","Epoch  15 Batch  141 / 228  Training Loss  0.00017743500939104706\n","Epoch  15 Batch  142 / 228  Training Loss  0.0001989994925679639\n","Epoch  15 Batch  143 / 228  Training Loss  0.00013147699064575136\n","Epoch  15 Batch  144 / 228  Training Loss  0.00011961466952925548\n","Epoch  15 Batch  145 / 228  Training Loss  0.00013576983474195004\n","Epoch  15 Batch  146 / 228  Training Loss  0.00011829313007183373\n","Epoch  15 Batch  147 / 228  Training Loss  0.00016118599160108715\n","Epoch  15 Batch  148 / 228  Training Loss  0.00012937940482515842\n","Epoch  15 Batch  149 / 228  Training Loss  9.4991919468157e-05\n","Epoch  15 Batch  150 / 228  Training Loss  0.00011904662824235857\n","Epoch  15 Batch  151 / 228  Training Loss  0.000120612392493058\n","Epoch  15 Batch  152 / 228  Training Loss  0.00011882501712534577\n","Epoch  15 Batch  153 / 228  Training Loss  0.0002352382434764877\n","Epoch  15 Batch  154 / 228  Training Loss  0.00010388531518401578\n","Epoch  15 Batch  155 / 228  Training Loss  9.010772191686556e-05\n","Epoch  15 Batch  156 / 228  Training Loss  9.564654465066269e-05\n","Epoch  15 Batch  157 / 228  Training Loss  0.00017328187823295593\n","Epoch  15 Batch  158 / 228  Training Loss  0.00012327154399827123\n","Epoch  15 Batch  159 / 228  Training Loss  9.510181553196162e-05\n","Epoch  15 Batch  160 / 228  Training Loss  0.00011398203059798107\n","Epoch  15 Batch  161 / 228  Training Loss  0.00010870502592297271\n","Epoch  15 Batch  162 / 228  Training Loss  0.00011509763862704858\n","Epoch  15 Batch  163 / 228  Training Loss  0.00018507364438846707\n","Epoch  15 Batch  164 / 228  Training Loss  0.00013021759514231235\n","Epoch  15 Batch  165 / 228  Training Loss  9.959142334992066e-05\n","Epoch  15 Batch  166 / 228  Training Loss  0.00010119385115103796\n","Epoch  15 Batch  167 / 228  Training Loss  0.00013036916789133102\n","Epoch  15 Batch  168 / 228  Training Loss  8.834475738694891e-05\n","Epoch  15 Batch  169 / 228  Training Loss  0.0003559308242984116\n","Epoch  15 Batch  170 / 228  Training Loss  0.0004602990811690688\n","Epoch  15 Batch  171 / 228  Training Loss  8.797764894552529e-05\n","Epoch  15 Batch  172 / 228  Training Loss  0.00010858788300538436\n","Epoch  15 Batch  173 / 228  Training Loss  0.00015017857367638499\n","Epoch  15 Batch  174 / 228  Training Loss  0.00012666296970564872\n","Epoch  15 Batch  175 / 228  Training Loss  0.0001618437672732398\n","Epoch  15 Batch  176 / 228  Training Loss  0.00012127337686251849\n","Epoch  15 Batch  177 / 228  Training Loss  0.00020954305364284664\n","Epoch  15 Batch  178 / 228  Training Loss  0.00010787307837745175\n","Epoch  15 Batch  179 / 228  Training Loss  0.0001086071933968924\n","Epoch  15 Batch  180 / 228  Training Loss  0.00010140406084246933\n","Epoch  15 Batch  181 / 228  Training Loss  0.00014284657663665712\n","Epoch  15 Batch  182 / 228  Training Loss  0.00017503117851447314\n","Epoch  15 Batch  183 / 228  Training Loss  0.0001676481042522937\n","Epoch  15 Batch  184 / 228  Training Loss  0.00010171959002036601\n","Epoch  15 Batch  185 / 228  Training Loss  0.00011579260899452493\n","Epoch  15 Batch  186 / 228  Training Loss  0.0001261989527847618\n","Epoch  15 Batch  187 / 228  Training Loss  0.00011842405365314335\n","Epoch  15 Batch  188 / 228  Training Loss  0.00013694452354684472\n","Epoch  15 Batch  189 / 228  Training Loss  0.00010630423639668152\n","Epoch  15 Batch  190 / 228  Training Loss  0.00015326989523600787\n","Epoch  15 Batch  191 / 228  Training Loss  8.064249414019287e-05\n","Epoch  15 Batch  192 / 228  Training Loss  8.838087524054572e-05\n","Epoch  15 Batch  193 / 228  Training Loss  8.685600914759561e-05\n","Epoch  15 Batch  194 / 228  Training Loss  0.00011831965821329504\n","Epoch  15 Batch  195 / 228  Training Loss  0.00011905188875971362\n","Epoch  15 Batch  196 / 228  Training Loss  0.00018396320228930563\n","Epoch  15 Batch  197 / 228  Training Loss  0.00013954515452496707\n","Epoch  15 Batch  198 / 228  Training Loss  5.885563587071374e-05\n","Epoch  15 Batch  199 / 228  Training Loss  0.0022207750007510185\n","Epoch  15 Batch  200 / 228  Training Loss  0.00029129162430763245\n","Epoch  15 Batch  201 / 228  Training Loss  7.02254255884327e-05\n","Epoch  15 Batch  202 / 228  Training Loss  0.0009703222895041108\n","Epoch  15 Batch  203 / 228  Training Loss  0.00022395382984541357\n","Epoch  15 Batch  204 / 228  Training Loss  0.00017685441707726568\n","Epoch  15 Batch  205 / 228  Training Loss  0.00017253641271963716\n","Epoch  15 Batch  206 / 228  Training Loss  0.00018731346062850207\n","Epoch  15 Batch  207 / 228  Training Loss  0.0004188059247098863\n","Epoch  15 Batch  208 / 228  Training Loss  0.00019030507246498019\n","Epoch  15 Batch  209 / 228  Training Loss  0.0001417265011696145\n","Epoch  15 Batch  210 / 228  Training Loss  0.00017466214194428176\n","Epoch  15 Batch  211 / 228  Training Loss  0.00019078071636613458\n","Epoch  15 Batch  212 / 228  Training Loss  0.000134162517497316\n","Epoch  15 Batch  213 / 228  Training Loss  0.00010770957305794582\n","Epoch  15 Batch  214 / 228  Training Loss  0.0002076935925288126\n","Epoch  15 Batch  215 / 228  Training Loss  0.002951615722849965\n","Epoch  15 Batch  216 / 228  Training Loss  0.00011754401202779263\n","Epoch  15 Batch  217 / 228  Training Loss  0.0001285755861317739\n","Epoch  15 Batch  218 / 228  Training Loss  0.0019673032220453024\n","Epoch  15 Batch  219 / 228  Training Loss  0.00012802232231479138\n","Epoch  15 Batch  220 / 228  Training Loss  0.00027623670757748187\n","Epoch  15 Batch  221 / 228  Training Loss  0.0001932941231643781\n","Epoch  15 Batch  222 / 228  Training Loss  0.00021676026517525315\n","Epoch  15 Batch  223 / 228  Training Loss  0.0033967585768550634\n","Epoch  15 Batch  224 / 228  Training Loss  0.00018704532703850418\n","Epoch  15 Batch  225 / 228  Training Loss  0.00016897430759854615\n","Epoch  15 Batch  226 / 228  Training Loss  0.00015490452642552555\n","Epoch  15 Batch  227 / 228  Training Loss  9.229599527316168e-05\n","  16    |    -    |   0.000229   | 90.205793\n","----------------------------------------------------------------------\n","Running epoch: 16\n","Epoch  16 Batch  0 / 228  Training Loss  9.990407124860212e-05\n","Epoch  16 Batch  1 / 228  Training Loss  0.0001017233298625797\n","Epoch  16 Batch  2 / 228  Training Loss  0.0003329368191771209\n","Epoch  16 Batch  3 / 228  Training Loss  0.0001441384811187163\n","Epoch  16 Batch  4 / 228  Training Loss  0.0001299666619161144\n","Epoch  16 Batch  5 / 228  Training Loss  8.15817984403111e-05\n","Epoch  16 Batch  6 / 228  Training Loss  0.0002464746357873082\n","Epoch  16 Batch  7 / 228  Training Loss  9.939864685293287e-05\n","Epoch  16 Batch  8 / 228  Training Loss  0.00016515227616764605\n","Epoch  16 Batch  9 / 228  Training Loss  0.00010072215809486806\n","Epoch  16 Batch  10 / 228  Training Loss  9.49086679611355e-05\n","Epoch  16 Batch  11 / 228  Training Loss  0.00010738857235992327\n","Epoch  16 Batch  12 / 228  Training Loss  0.00017432350432500243\n","Epoch  16 Batch  13 / 228  Training Loss  0.00016856817819643766\n","Epoch  16 Batch  14 / 228  Training Loss  0.000157353570102714\n","Epoch  16 Batch  15 / 228  Training Loss  0.0009195544407702982\n","Epoch  16 Batch  16 / 228  Training Loss  0.0001212485003634356\n","Epoch  16 Batch  17 / 228  Training Loss  0.0005815649055875838\n","Epoch  16 Batch  18 / 228  Training Loss  0.00015232706209644675\n","Epoch  16 Batch  19 / 228  Training Loss  0.00012665410758927464\n","Epoch  16 Batch  20 / 228  Training Loss  0.0001994809281313792\n","Epoch  16 Batch  21 / 228  Training Loss  0.00018773572810459882\n","Epoch  16 Batch  22 / 228  Training Loss  8.177808922482654e-05\n","Epoch  16 Batch  23 / 228  Training Loss  0.00022557434567715973\n","Epoch  16 Batch  24 / 228  Training Loss  0.00013088570267427713\n","Epoch  16 Batch  25 / 228  Training Loss  0.00020583013247232884\n","Epoch  16 Batch  26 / 228  Training Loss  0.00011313259892631322\n","Epoch  16 Batch  27 / 228  Training Loss  0.00015245181566569954\n","Epoch  16 Batch  28 / 228  Training Loss  0.0011531633790582418\n","Epoch  16 Batch  29 / 228  Training Loss  0.0002547136100474745\n","Epoch  16 Batch  30 / 228  Training Loss  0.00014884043775964528\n","Epoch  16 Batch  31 / 228  Training Loss  9.623938967706636e-05\n","Epoch  16 Batch  32 / 228  Training Loss  0.00010834047134267166\n","Epoch  16 Batch  33 / 228  Training Loss  0.0001510934525867924\n","Epoch  16 Batch  34 / 228  Training Loss  0.00011111747153336182\n","Epoch  16 Batch  35 / 228  Training Loss  9.710396989248693e-05\n","Epoch  16 Batch  36 / 228  Training Loss  0.00015343296399805695\n","Epoch  16 Batch  37 / 228  Training Loss  7.430952246068045e-05\n","Epoch  16 Batch  38 / 228  Training Loss  0.0001429619442205876\n","Epoch  16 Batch  39 / 228  Training Loss  0.0001290783256990835\n","Epoch  16 Batch  40 / 228  Training Loss  0.00015551627438981086\n","Epoch  16 Batch  41 / 228  Training Loss  0.00014645645569544286\n","Epoch  16 Batch  42 / 228  Training Loss  0.00018238653137814254\n","Epoch  16 Batch  43 / 228  Training Loss  0.0001888006372610107\n","Epoch  16 Batch  44 / 228  Training Loss  0.00012701655214186758\n","Epoch  16 Batch  45 / 228  Training Loss  0.00011179529246874154\n","Epoch  16 Batch  46 / 228  Training Loss  0.0001225804298883304\n","Epoch  16 Batch  47 / 228  Training Loss  8.827441342873499e-05\n","Epoch  16 Batch  48 / 228  Training Loss  8.471417095279321e-05\n","Epoch  16 Batch  49 / 228  Training Loss  9.727910219226032e-05\n","Epoch  16 Batch  50 / 228  Training Loss  0.00011639005970209837\n","Epoch  16 Batch  51 / 228  Training Loss  0.00011927684681722894\n","Epoch  16 Batch  52 / 228  Training Loss  9.876441617961973e-05\n","Epoch  16 Batch  53 / 228  Training Loss  0.00011186414485564455\n","Epoch  16 Batch  54 / 228  Training Loss  0.00011043096310459077\n","Epoch  16 Batch  55 / 228  Training Loss  8.224228076869622e-05\n","Epoch  16 Batch  56 / 228  Training Loss  7.960959919728339e-05\n","Epoch  16 Batch  57 / 228  Training Loss  0.00016465494991280138\n","Epoch  16 Batch  58 / 228  Training Loss  0.0001148105293395929\n","Epoch  16 Batch  59 / 228  Training Loss  7.841729529900476e-05\n","Epoch  16 Batch  60 / 228  Training Loss  6.826237950008363e-05\n","Epoch  16 Batch  61 / 228  Training Loss  0.00010286851465934888\n","Epoch  16 Batch  62 / 228  Training Loss  0.00010582082177279517\n","Epoch  16 Batch  63 / 228  Training Loss  8.436921052634716e-05\n","Epoch  16 Batch  64 / 228  Training Loss  0.0001084788964362815\n","Epoch  16 Batch  65 / 228  Training Loss  9.662315278546885e-05\n","Epoch  16 Batch  66 / 228  Training Loss  8.205561607610434e-05\n","Epoch  16 Batch  67 / 228  Training Loss  6.415296957129613e-05\n","Epoch  16 Batch  68 / 228  Training Loss  0.0001274908281629905\n","Epoch  16 Batch  69 / 228  Training Loss  0.00017314101569354534\n","Epoch  16 Batch  70 / 228  Training Loss  0.0001552737521706149\n","Epoch  16 Batch  71 / 228  Training Loss  0.00012026264448650181\n","Epoch  16 Batch  72 / 228  Training Loss  0.00010627090523485094\n","Epoch  16 Batch  73 / 228  Training Loss  0.0001473988959332928\n","Epoch  16 Batch  74 / 228  Training Loss  0.00015259557403624058\n","Epoch  16 Batch  75 / 228  Training Loss  6.770114123355597e-05\n","Epoch  16 Batch  76 / 228  Training Loss  0.00016660783148836344\n","Epoch  16 Batch  77 / 228  Training Loss  0.00011491224722703919\n","Epoch  16 Batch  78 / 228  Training Loss  0.00010770702647278085\n","Epoch  16 Batch  79 / 228  Training Loss  9.124534699367359e-05\n","Epoch  16 Batch  80 / 228  Training Loss  0.00010834731801878661\n","Epoch  16 Batch  81 / 228  Training Loss  0.00010221795673714951\n","Epoch  16 Batch  82 / 228  Training Loss  0.00013753777602687478\n","Epoch  16 Batch  83 / 228  Training Loss  0.000139682088047266\n","Epoch  16 Batch  84 / 228  Training Loss  0.0001148066294263117\n","Epoch  16 Batch  85 / 228  Training Loss  0.00010060909698950127\n","Epoch  16 Batch  86 / 228  Training Loss  7.983515388332307e-05\n","Epoch  16 Batch  87 / 228  Training Loss  0.00019672205962706357\n","Epoch  16 Batch  88 / 228  Training Loss  9.232506999978796e-05\n","Epoch  16 Batch  89 / 228  Training Loss  8.77473212312907e-05\n","Epoch  16 Batch  90 / 228  Training Loss  0.0001163742781500332\n","Epoch  16 Batch  91 / 228  Training Loss  9.667465928941965e-05\n","Epoch  16 Batch  92 / 228  Training Loss  0.0001277224364457652\n","Epoch  16 Batch  93 / 228  Training Loss  9.374345245305449e-05\n","Epoch  16 Batch  94 / 228  Training Loss  0.00012135625729570165\n","Epoch  16 Batch  95 / 228  Training Loss  0.00011757000902434811\n","Epoch  16 Batch  96 / 228  Training Loss  7.513898162869737e-05\n","Epoch  16 Batch  97 / 228  Training Loss  0.0001256397517863661\n","Epoch  16 Batch  98 / 228  Training Loss  7.417987217195332e-05\n","Epoch  16 Batch  99 / 228  Training Loss  9.71404297160916e-05\n","Epoch  16 Batch  100 / 228  Training Loss  9.042909368872643e-05\n","Epoch  16 Batch  101 / 228  Training Loss  0.0001940027141245082\n","Epoch  16 Batch  102 / 228  Training Loss  0.00011146035831188783\n","Epoch  16 Batch  103 / 228  Training Loss  9.010555368149653e-05\n","Epoch  16 Batch  104 / 228  Training Loss  0.00013084447709843516\n","Epoch  16 Batch  105 / 228  Training Loss  8.880335371941328e-05\n","Epoch  16 Batch  106 / 228  Training Loss  0.00010080380889121443\n","Epoch  16 Batch  107 / 228  Training Loss  9.826423047343269e-05\n","Epoch  16 Batch  108 / 228  Training Loss  0.0001334610569756478\n","Epoch  16 Batch  109 / 228  Training Loss  9.081091411644593e-05\n","Epoch  16 Batch  110 / 228  Training Loss  0.0001299485011259094\n","Epoch  16 Batch  111 / 228  Training Loss  6.272199243539944e-05\n","Epoch  16 Batch  112 / 228  Training Loss  0.00011032419570256025\n","Epoch  16 Batch  113 / 228  Training Loss  7.95077285147272e-05\n","Epoch  16 Batch  114 / 228  Training Loss  4.6357152314158157e-05\n","Epoch  16 Batch  115 / 228  Training Loss  0.00012691035226453096\n","Epoch  16 Batch  116 / 228  Training Loss  7.791299867676571e-05\n","Epoch  16 Batch  117 / 228  Training Loss  6.755335198249668e-05\n","Epoch  16 Batch  118 / 228  Training Loss  8.053327474044636e-05\n","Epoch  16 Batch  119 / 228  Training Loss  0.00011561639257706702\n","Epoch  16 Batch  120 / 228  Training Loss  0.00010042388021247461\n","Epoch  16 Batch  121 / 228  Training Loss  0.0001439495536033064\n","Epoch  16 Batch  122 / 228  Training Loss  0.00013767022755928338\n","Epoch  16 Batch  123 / 228  Training Loss  9.989926184061915e-05\n","Epoch  16 Batch  124 / 228  Training Loss  0.00012992207484785467\n","Epoch  16 Batch  125 / 228  Training Loss  8.509335748385638e-05\n","Epoch  16 Batch  126 / 228  Training Loss  9.388876787852496e-05\n","Epoch  16 Batch  127 / 228  Training Loss  9.166255040327087e-05\n","Epoch  16 Batch  128 / 228  Training Loss  8.757225441513583e-05\n","Epoch  16 Batch  129 / 228  Training Loss  7.041059143375605e-05\n","Epoch  16 Batch  130 / 228  Training Loss  8.20102941361256e-05\n","Epoch  16 Batch  131 / 228  Training Loss  9.095571294892579e-05\n","Epoch  16 Batch  132 / 228  Training Loss  5.008893640479073e-05\n","Epoch  16 Batch  133 / 228  Training Loss  8.444928971584886e-05\n","Epoch  16 Batch  134 / 228  Training Loss  7.878556789364666e-05\n","Epoch  16 Batch  135 / 228  Training Loss  6.969590322114527e-05\n","Epoch  16 Batch  136 / 228  Training Loss  7.615506910951808e-05\n","Epoch  16 Batch  137 / 228  Training Loss  9.672777378000319e-05\n","Epoch  16 Batch  138 / 228  Training Loss  0.00013208974269218743\n","Epoch  16 Batch  139 / 228  Training Loss  8.338484622072428e-05\n","Epoch  16 Batch  140 / 228  Training Loss  9.56746080191806e-05\n","Epoch  16 Batch  141 / 228  Training Loss  7.410205580526963e-05\n","Epoch  16 Batch  142 / 228  Training Loss  6.690062582492828e-05\n","Epoch  16 Batch  143 / 228  Training Loss  9.19270096346736e-05\n","Epoch  16 Batch  144 / 228  Training Loss  7.689535414101556e-05\n","Epoch  16 Batch  145 / 228  Training Loss  7.963068492244929e-05\n","Epoch  16 Batch  146 / 228  Training Loss  8.18790213088505e-05\n","Epoch  16 Batch  147 / 228  Training Loss  0.00011004826956195757\n","Epoch  16 Batch  148 / 228  Training Loss  9.253113239537925e-05\n","Epoch  16 Batch  149 / 228  Training Loss  0.00010764885519165546\n","Epoch  16 Batch  150 / 228  Training Loss  0.00011424971307860687\n","Epoch  16 Batch  151 / 228  Training Loss  7.240880950121209e-05\n","Epoch  16 Batch  152 / 228  Training Loss  9.894270624499768e-05\n","Epoch  16 Batch  153 / 228  Training Loss  8.691722905496135e-05\n","Epoch  16 Batch  154 / 228  Training Loss  9.05303459148854e-05\n","Epoch  16 Batch  155 / 228  Training Loss  0.000100638710136991\n","Epoch  16 Batch  156 / 228  Training Loss  7.087198900990188e-05\n","Epoch  16 Batch  157 / 228  Training Loss  9.608893742552027e-05\n","Epoch  16 Batch  158 / 228  Training Loss  0.00010747250053100288\n","Epoch  16 Batch  159 / 228  Training Loss  6.624348316108808e-05\n","Epoch  16 Batch  160 / 228  Training Loss  7.032439316390082e-05\n","Epoch  16 Batch  161 / 228  Training Loss  7.680220733163878e-05\n","Epoch  16 Batch  162 / 228  Training Loss  0.00010560960799921304\n","Epoch  16 Batch  163 / 228  Training Loss  8.202575554605573e-05\n","Epoch  16 Batch  164 / 228  Training Loss  8.676513971295208e-05\n","Epoch  16 Batch  165 / 228  Training Loss  9.751568723004311e-05\n","Epoch  16 Batch  166 / 228  Training Loss  9.689246508060023e-05\n","Epoch  16 Batch  167 / 228  Training Loss  8.406619599554688e-05\n","Epoch  16 Batch  168 / 228  Training Loss  9.398534166393802e-05\n","Epoch  16 Batch  169 / 228  Training Loss  4.5947184844408184e-05\n","Epoch  16 Batch  170 / 228  Training Loss  7.864262443035841e-05\n","Epoch  16 Batch  171 / 228  Training Loss  6.72489550197497e-05\n","Epoch  16 Batch  172 / 228  Training Loss  0.00012804825382772833\n","Epoch  16 Batch  173 / 228  Training Loss  5.8580732002155855e-05\n","Epoch  16 Batch  174 / 228  Training Loss  8.915668877307326e-05\n","Epoch  16 Batch  175 / 228  Training Loss  6.890646182000637e-05\n","Epoch  16 Batch  176 / 228  Training Loss  0.00010828603990375996\n","Epoch  16 Batch  177 / 228  Training Loss  9.325695282313973e-05\n","Epoch  16 Batch  178 / 228  Training Loss  0.00012217897165101022\n","Epoch  16 Batch  179 / 228  Training Loss  0.00011751092824852094\n","Epoch  16 Batch  180 / 228  Training Loss  0.00011078572424594313\n","Epoch  16 Batch  181 / 228  Training Loss  0.00013420644972939044\n","Epoch  16 Batch  182 / 228  Training Loss  6.793089414713904e-05\n","Epoch  16 Batch  183 / 228  Training Loss  5.4683088819729164e-05\n","Epoch  16 Batch  184 / 228  Training Loss  0.0001277973351534456\n","Epoch  16 Batch  185 / 228  Training Loss  0.00011085422738688067\n","Epoch  16 Batch  186 / 228  Training Loss  6.523542833747342e-05\n","Epoch  16 Batch  187 / 228  Training Loss  0.00012279929069336504\n","Epoch  16 Batch  188 / 228  Training Loss  6.770667096134275e-05\n","Epoch  16 Batch  189 / 228  Training Loss  8.65490801515989e-05\n","Epoch  16 Batch  190 / 228  Training Loss  6.844090239610523e-05\n","Epoch  16 Batch  191 / 228  Training Loss  0.00011786352843046188\n","Epoch  16 Batch  192 / 228  Training Loss  8.858415822032839e-05\n","Epoch  16 Batch  193 / 228  Training Loss  7.355518755502999e-05\n","Epoch  16 Batch  194 / 228  Training Loss  8.045199501793832e-05\n","Epoch  16 Batch  195 / 228  Training Loss  7.457741594407707e-05\n","Epoch  16 Batch  196 / 228  Training Loss  0.00010982059029629454\n","Epoch  16 Batch  197 / 228  Training Loss  8.110854832921177e-05\n","Epoch  16 Batch  198 / 228  Training Loss  0.0001067739722202532\n","Epoch  16 Batch  199 / 228  Training Loss  6.834256055299193e-05\n","Epoch  16 Batch  200 / 228  Training Loss  6.551924889208749e-05\n","Epoch  16 Batch  201 / 228  Training Loss  5.653671905747615e-05\n","Epoch  16 Batch  202 / 228  Training Loss  6.461973680416122e-05\n","Epoch  16 Batch  203 / 228  Training Loss  8.81919841049239e-05\n","Epoch  16 Batch  204 / 228  Training Loss  8.300070476252586e-05\n","Epoch  16 Batch  205 / 228  Training Loss  7.922708027763292e-05\n","Epoch  16 Batch  206 / 228  Training Loss  5.068372411187738e-05\n","Epoch  16 Batch  207 / 228  Training Loss  6.886448682053015e-05\n","Epoch  16 Batch  208 / 228  Training Loss  7.879245094954967e-05\n","Epoch  16 Batch  209 / 228  Training Loss  8.026855357456952e-05\n","Epoch  16 Batch  210 / 228  Training Loss  9.862504521152005e-05\n","Epoch  16 Batch  211 / 228  Training Loss  0.00010590892634354532\n","Epoch  16 Batch  212 / 228  Training Loss  6.896322156535462e-05\n","Epoch  16 Batch  213 / 228  Training Loss  9.371703345095739e-05\n","Epoch  16 Batch  214 / 228  Training Loss  6.008598575135693e-05\n","Epoch  16 Batch  215 / 228  Training Loss  9.046845661941916e-05\n","Epoch  16 Batch  216 / 228  Training Loss  9.324811981059611e-05\n","Epoch  16 Batch  217 / 228  Training Loss  6.277271313592792e-05\n","Epoch  16 Batch  218 / 228  Training Loss  0.00011233703844482079\n","Epoch  16 Batch  219 / 228  Training Loss  0.00010321603622287512\n","Epoch  16 Batch  220 / 228  Training Loss  6.042622044333257e-05\n","Epoch  16 Batch  221 / 228  Training Loss  8.052836346905679e-05\n","Epoch  16 Batch  222 / 228  Training Loss  9.433998638996854e-05\n","Epoch  16 Batch  223 / 228  Training Loss  0.00010827892401721328\n","Epoch  16 Batch  224 / 228  Training Loss  4.560592060443014e-05\n","Epoch  16 Batch  225 / 228  Training Loss  6.62423117319122e-05\n","Epoch  16 Batch  226 / 228  Training Loss  6.002296868246049e-05\n","Epoch  16 Batch  227 / 228  Training Loss  7.622169505339116e-05\n","  17    |    -    |   0.000115   | 91.044207\n","----------------------------------------------------------------------\n","Running epoch: 17\n","Epoch  17 Batch  0 / 228  Training Loss  0.00014342422946356237\n","Epoch  17 Batch  1 / 228  Training Loss  7.462208304787055e-05\n","Epoch  17 Batch  2 / 228  Training Loss  5.672108818544075e-05\n","Epoch  17 Batch  3 / 228  Training Loss  7.939023635117337e-05\n","Epoch  17 Batch  4 / 228  Training Loss  0.00010114186443388462\n","Epoch  17 Batch  5 / 228  Training Loss  7.152942271204665e-05\n","Epoch  17 Batch  6 / 228  Training Loss  5.6989934819284827e-05\n","Epoch  17 Batch  7 / 228  Training Loss  7.444609946105629e-05\n","Epoch  17 Batch  8 / 228  Training Loss  6.97895884513855e-05\n","Epoch  17 Batch  9 / 228  Training Loss  0.00011481470573926345\n","Epoch  17 Batch  10 / 228  Training Loss  6.165100785437971e-05\n","Epoch  17 Batch  11 / 228  Training Loss  9.984675125451759e-05\n","Epoch  17 Batch  12 / 228  Training Loss  8.847167191561311e-05\n","Epoch  17 Batch  13 / 228  Training Loss  7.058171468088403e-05\n","Epoch  17 Batch  14 / 228  Training Loss  6.034680700395256e-05\n","Epoch  17 Batch  15 / 228  Training Loss  8.814393368083984e-05\n","Epoch  17 Batch  16 / 228  Training Loss  6.555698200827464e-05\n","Epoch  17 Batch  17 / 228  Training Loss  6.837293767603114e-05\n","Epoch  17 Batch  18 / 228  Training Loss  7.342040044022724e-05\n","Epoch  17 Batch  19 / 228  Training Loss  6.0773112636525184e-05\n","Epoch  17 Batch  20 / 228  Training Loss  7.134471525205299e-05\n","Epoch  17 Batch  21 / 228  Training Loss  8.604262984590605e-05\n","Epoch  17 Batch  22 / 228  Training Loss  7.248179463203996e-05\n","Epoch  17 Batch  23 / 228  Training Loss  8.713683200767264e-05\n","Epoch  17 Batch  24 / 228  Training Loss  5.95782657910604e-05\n","Epoch  17 Batch  25 / 228  Training Loss  7.013901631580666e-05\n","Epoch  17 Batch  26 / 228  Training Loss  5.818654244649224e-05\n","Epoch  17 Batch  27 / 228  Training Loss  8.337733743246645e-05\n","Epoch  17 Batch  28 / 228  Training Loss  8.412972965743393e-05\n","Epoch  17 Batch  29 / 228  Training Loss  9.408280311618e-05\n","Epoch  17 Batch  30 / 228  Training Loss  7.678940164623782e-05\n","Epoch  17 Batch  31 / 228  Training Loss  7.732981612207368e-05\n","Epoch  17 Batch  32 / 228  Training Loss  6.0388359997887164e-05\n","Epoch  17 Batch  33 / 228  Training Loss  9.634278831072152e-05\n","Epoch  17 Batch  34 / 228  Training Loss  5.831157977809198e-05\n","Epoch  17 Batch  35 / 228  Training Loss  6.962859333725646e-05\n","Epoch  17 Batch  36 / 228  Training Loss  7.507232658099383e-05\n","Epoch  17 Batch  37 / 228  Training Loss  5.372902887756936e-05\n","Epoch  17 Batch  38 / 228  Training Loss  6.195336754899472e-05\n","Epoch  17 Batch  39 / 228  Training Loss  5.6778197176754475e-05\n","Epoch  17 Batch  40 / 228  Training Loss  9.749776654643938e-05\n","Epoch  17 Batch  41 / 228  Training Loss  5.459642125060782e-05\n","Epoch  17 Batch  42 / 228  Training Loss  6.742074765497819e-05\n","Epoch  17 Batch  43 / 228  Training Loss  6.692826718790457e-05\n","Epoch  17 Batch  44 / 228  Training Loss  6.535532884299755e-05\n","Epoch  17 Batch  45 / 228  Training Loss  6.016524639562704e-05\n","Epoch  17 Batch  46 / 228  Training Loss  7.699348498135805e-05\n","Epoch  17 Batch  47 / 228  Training Loss  4.745418482343666e-05\n","Epoch  17 Batch  48 / 228  Training Loss  7.712505612289533e-05\n","Epoch  17 Batch  49 / 228  Training Loss  9.096406574826688e-05\n","Epoch  17 Batch  50 / 228  Training Loss  6.968567322473973e-05\n","Epoch  17 Batch  51 / 228  Training Loss  7.791205280227587e-05\n","Epoch  17 Batch  52 / 228  Training Loss  7.250152702908963e-05\n","Epoch  17 Batch  53 / 228  Training Loss  8.75273544806987e-05\n","Epoch  17 Batch  54 / 228  Training Loss  7.860402547521517e-05\n","Epoch  17 Batch  55 / 228  Training Loss  7.034368172753602e-05\n","Epoch  17 Batch  56 / 228  Training Loss  9.118520392803475e-05\n","Epoch  17 Batch  57 / 228  Training Loss  6.250545993680134e-05\n","Epoch  17 Batch  58 / 228  Training Loss  4.842869384447113e-05\n","Epoch  17 Batch  59 / 228  Training Loss  9.127377416007221e-05\n","Epoch  17 Batch  60 / 228  Training Loss  5.426948700915091e-05\n","Epoch  17 Batch  61 / 228  Training Loss  6.662093073828146e-05\n","Epoch  17 Batch  62 / 228  Training Loss  6.142914207885042e-05\n","Epoch  17 Batch  63 / 228  Training Loss  6.257832137634978e-05\n","Epoch  17 Batch  64 / 228  Training Loss  7.68742393120192e-05\n","Epoch  17 Batch  65 / 228  Training Loss  8.253757550846785e-05\n","Epoch  17 Batch  66 / 228  Training Loss  7.456530147464946e-05\n","Epoch  17 Batch  67 / 228  Training Loss  8.486244769301265e-05\n","Epoch  17 Batch  68 / 228  Training Loss  6.274330371525139e-05\n","Epoch  17 Batch  69 / 228  Training Loss  7.311291847145185e-05\n","Epoch  17 Batch  70 / 228  Training Loss  6.702301470795646e-05\n","Epoch  17 Batch  71 / 228  Training Loss  7.223740976769477e-05\n","Epoch  17 Batch  72 / 228  Training Loss  6.459579890361056e-05\n","Epoch  17 Batch  73 / 228  Training Loss  6.796731031499803e-05\n","Epoch  17 Batch  74 / 228  Training Loss  8.614394755568355e-05\n","Epoch  17 Batch  75 / 228  Training Loss  4.706829713541083e-05\n","Epoch  17 Batch  76 / 228  Training Loss  5.701235568267293e-05\n","Epoch  17 Batch  77 / 228  Training Loss  7.030626147752628e-05\n","Epoch  17 Batch  78 / 228  Training Loss  5.909374885959551e-05\n","Epoch  17 Batch  79 / 228  Training Loss  6.486474740086123e-05\n","Epoch  17 Batch  80 / 228  Training Loss  8.241633622674271e-05\n","Epoch  17 Batch  81 / 228  Training Loss  7.516619371017441e-05\n","Epoch  17 Batch  82 / 228  Training Loss  5.1083083235425875e-05\n","Epoch  17 Batch  83 / 228  Training Loss  6.869046046631411e-05\n","Epoch  17 Batch  84 / 228  Training Loss  6.65582629153505e-05\n","Epoch  17 Batch  85 / 228  Training Loss  5.849984518135898e-05\n","Epoch  17 Batch  86 / 228  Training Loss  7.452110730810091e-05\n","Epoch  17 Batch  87 / 228  Training Loss  7.738488056929782e-05\n","Epoch  17 Batch  88 / 228  Training Loss  5.4991796787362546e-05\n","Epoch  17 Batch  89 / 228  Training Loss  7.659450056962669e-05\n","Epoch  17 Batch  90 / 228  Training Loss  4.756770795211196e-05\n","Epoch  17 Batch  91 / 228  Training Loss  8.420378435403109e-05\n","Epoch  17 Batch  92 / 228  Training Loss  9.130774560617283e-05\n","Epoch  17 Batch  93 / 228  Training Loss  7.01404715073295e-05\n","Epoch  17 Batch  94 / 228  Training Loss  6.654432218056172e-05\n","Epoch  17 Batch  95 / 228  Training Loss  8.193666872102767e-05\n","Epoch  17 Batch  96 / 228  Training Loss  7.091366569511592e-05\n","Epoch  17 Batch  97 / 228  Training Loss  7.157193613238633e-05\n","Epoch  17 Batch  98 / 228  Training Loss  5.8056819398188964e-05\n","Epoch  17 Batch  99 / 228  Training Loss  6.945463246665895e-05\n","Epoch  17 Batch  100 / 228  Training Loss  4.3540312617551535e-05\n","Epoch  17 Batch  101 / 228  Training Loss  6.248345016501844e-05\n","Epoch  17 Batch  102 / 228  Training Loss  8.97409554454498e-05\n","Epoch  17 Batch  103 / 228  Training Loss  4.698281190940179e-05\n","Epoch  17 Batch  104 / 228  Training Loss  6.38679921394214e-05\n","Epoch  17 Batch  105 / 228  Training Loss  4.494120730669238e-05\n","Epoch  17 Batch  106 / 228  Training Loss  6.697466596961021e-05\n","Epoch  17 Batch  107 / 228  Training Loss  7.322628516703844e-05\n","Epoch  17 Batch  108 / 228  Training Loss  6.597071478608996e-05\n","Epoch  17 Batch  109 / 228  Training Loss  6.435484829125926e-05\n","Epoch  17 Batch  110 / 228  Training Loss  4.586587965604849e-05\n","Epoch  17 Batch  111 / 228  Training Loss  6.506336649181321e-05\n","Epoch  17 Batch  112 / 228  Training Loss  3.4738735848804936e-05\n","Epoch  17 Batch  113 / 228  Training Loss  7.018503674771637e-05\n","Epoch  17 Batch  114 / 228  Training Loss  5.570314169744961e-05\n","Epoch  17 Batch  115 / 228  Training Loss  5.797153426101431e-05\n","Epoch  17 Batch  116 / 228  Training Loss  5.493525168276392e-05\n","Epoch  17 Batch  117 / 228  Training Loss  0.00010122374806087464\n","Epoch  17 Batch  118 / 228  Training Loss  4.55888693977613e-05\n","Epoch  17 Batch  119 / 228  Training Loss  5.148053605807945e-05\n","Epoch  17 Batch  120 / 228  Training Loss  5.530123962671496e-05\n","Epoch  17 Batch  121 / 228  Training Loss  0.00010019064211519435\n","Epoch  17 Batch  122 / 228  Training Loss  5.512177449418232e-05\n","Epoch  17 Batch  123 / 228  Training Loss  6.113569543231279e-05\n","Epoch  17 Batch  124 / 228  Training Loss  4.717294723377563e-05\n","Epoch  17 Batch  125 / 228  Training Loss  6.742927507730201e-05\n","Epoch  17 Batch  126 / 228  Training Loss  4.615955549525097e-05\n","Epoch  17 Batch  127 / 228  Training Loss  5.937164314673282e-05\n","Epoch  17 Batch  128 / 228  Training Loss  7.551066664746031e-05\n","Epoch  17 Batch  129 / 228  Training Loss  3.542141348589212e-05\n","Epoch  17 Batch  130 / 228  Training Loss  6.713718175888062e-05\n","Epoch  17 Batch  131 / 228  Training Loss  4.8450925532961264e-05\n","Epoch  17 Batch  132 / 228  Training Loss  4.719270873465575e-05\n","Epoch  17 Batch  133 / 228  Training Loss  7.418082532240078e-05\n","Epoch  17 Batch  134 / 228  Training Loss  4.5585198677144945e-05\n","Epoch  17 Batch  135 / 228  Training Loss  0.000134900605189614\n","Epoch  17 Batch  136 / 228  Training Loss  6.512788240797818e-05\n","Epoch  17 Batch  137 / 228  Training Loss  5.3614618082065135e-05\n","Epoch  17 Batch  138 / 228  Training Loss  4.325447662267834e-05\n","Epoch  17 Batch  139 / 228  Training Loss  5.5867054470581934e-05\n","Epoch  17 Batch  140 / 228  Training Loss  9.752791083883494e-05\n","Epoch  17 Batch  141 / 228  Training Loss  7.636914233444259e-05\n","Epoch  17 Batch  142 / 228  Training Loss  7.943542004795745e-05\n","Epoch  17 Batch  143 / 228  Training Loss  7.085479592205957e-05\n","Epoch  17 Batch  144 / 228  Training Loss  5.166055416339077e-05\n","Epoch  17 Batch  145 / 228  Training Loss  4.4728505599778146e-05\n","Epoch  17 Batch  146 / 228  Training Loss  6.318606756394729e-05\n","Epoch  17 Batch  147 / 228  Training Loss  5.837506614625454e-05\n","Epoch  17 Batch  148 / 228  Training Loss  8.816471381578594e-05\n","Epoch  17 Batch  149 / 228  Training Loss  6.812078936491162e-05\n","Epoch  17 Batch  150 / 228  Training Loss  7.14631678420119e-05\n","Epoch  17 Batch  151 / 228  Training Loss  0.00021017542167101055\n","Epoch  17 Batch  152 / 228  Training Loss  5.999406857881695e-05\n","Epoch  17 Batch  153 / 228  Training Loss  4.3851887312484905e-05\n","Epoch  17 Batch  154 / 228  Training Loss  4.665053711505607e-05\n","Epoch  17 Batch  155 / 228  Training Loss  7.304642349481583e-05\n","Epoch  17 Batch  156 / 228  Training Loss  8.201447781175375e-05\n","Epoch  17 Batch  157 / 228  Training Loss  8.704247011337429e-05\n","Epoch  17 Batch  158 / 228  Training Loss  4.959267607773654e-05\n","Epoch  17 Batch  159 / 228  Training Loss  5.53277823200915e-05\n","Epoch  17 Batch  160 / 228  Training Loss  9.241585212294012e-05\n","Epoch  17 Batch  161 / 228  Training Loss  5.458156738313846e-05\n","Epoch  17 Batch  162 / 228  Training Loss  6.350343755912036e-05\n","Epoch  17 Batch  163 / 228  Training Loss  5.4416370403487235e-05\n","Epoch  17 Batch  164 / 228  Training Loss  6.2098893977236e-05\n","Epoch  17 Batch  165 / 228  Training Loss  7.011499837972224e-05\n","Epoch  17 Batch  166 / 228  Training Loss  8.105677989078686e-05\n","Epoch  17 Batch  167 / 228  Training Loss  5.5682507081655785e-05\n","Epoch  17 Batch  168 / 228  Training Loss  0.00010096377809531987\n","Epoch  17 Batch  169 / 228  Training Loss  6.0110745835117996e-05\n","Epoch  17 Batch  170 / 228  Training Loss  9.08420406631194e-05\n","Epoch  17 Batch  171 / 228  Training Loss  7.305525650735945e-05\n","Epoch  17 Batch  172 / 228  Training Loss  7.412129343720153e-05\n","Epoch  17 Batch  173 / 228  Training Loss  4.72648607683368e-05\n","Epoch  17 Batch  174 / 228  Training Loss  7.883525540819392e-05\n","Epoch  17 Batch  175 / 228  Training Loss  6.820724956924096e-05\n","Epoch  17 Batch  176 / 228  Training Loss  5.894842615816742e-05\n","Epoch  17 Batch  177 / 228  Training Loss  8.281037298729643e-05\n","Epoch  17 Batch  178 / 228  Training Loss  6.655474862782285e-05\n","Epoch  17 Batch  179 / 228  Training Loss  3.908756480086595e-05\n","Epoch  17 Batch  180 / 228  Training Loss  8.6313666542992e-05\n","Epoch  17 Batch  181 / 228  Training Loss  5.8412486396264285e-05\n","Epoch  17 Batch  182 / 228  Training Loss  7.599496166221797e-05\n","Epoch  17 Batch  183 / 228  Training Loss  4.6378769184229895e-05\n","Epoch  17 Batch  184 / 228  Training Loss  6.810738705098629e-05\n","Epoch  17 Batch  185 / 228  Training Loss  7.314104004763067e-05\n","Epoch  17 Batch  186 / 228  Training Loss  6.202518125064671e-05\n","Epoch  17 Batch  187 / 228  Training Loss  5.469203824759461e-05\n","Epoch  17 Batch  188 / 228  Training Loss  4.555795749183744e-05\n","Epoch  17 Batch  189 / 228  Training Loss  7.985595584614202e-05\n","Epoch  17 Batch  190 / 228  Training Loss  6.103360647102818e-05\n","Epoch  17 Batch  191 / 228  Training Loss  0.000227686672587879\n","Epoch  17 Batch  192 / 228  Training Loss  4.8306883400073275e-05\n","Epoch  17 Batch  193 / 228  Training Loss  3.6675755836768076e-05\n","Epoch  17 Batch  194 / 228  Training Loss  7.33239357941784e-05\n","Epoch  17 Batch  195 / 228  Training Loss  3.9496339013567194e-05\n","Epoch  17 Batch  196 / 228  Training Loss  6.933413533261046e-05\n","Epoch  17 Batch  197 / 228  Training Loss  6.404104351531714e-05\n","Epoch  17 Batch  198 / 228  Training Loss  6.0372622101567686e-05\n","Epoch  17 Batch  199 / 228  Training Loss  6.223525997484103e-05\n","Epoch  17 Batch  200 / 228  Training Loss  7.568807632196695e-05\n","Epoch  17 Batch  201 / 228  Training Loss  3.971916885348037e-05\n","Epoch  17 Batch  202 / 228  Training Loss  5.43040587217547e-05\n","Epoch  17 Batch  203 / 228  Training Loss  6.412150105461478e-05\n","Epoch  17 Batch  204 / 228  Training Loss  5.737628453061916e-05\n","Epoch  17 Batch  205 / 228  Training Loss  5.139000859344378e-05\n","Epoch  17 Batch  206 / 228  Training Loss  4.605844151228666e-05\n","Epoch  17 Batch  207 / 228  Training Loss  6.682473031105474e-05\n","Epoch  17 Batch  208 / 228  Training Loss  7.101554365362972e-05\n","Epoch  17 Batch  209 / 228  Training Loss  5.181664164410904e-05\n","Epoch  17 Batch  210 / 228  Training Loss  8.767916006036103e-05\n","Epoch  17 Batch  211 / 228  Training Loss  7.074526365613565e-05\n","Epoch  17 Batch  212 / 228  Training Loss  5.9162299294257537e-05\n","Epoch  17 Batch  213 / 228  Training Loss  6.0510883486131206e-05\n","Epoch  17 Batch  214 / 228  Training Loss  6.561490590684116e-05\n","Epoch  17 Batch  215 / 228  Training Loss  5.250210961094126e-05\n","Epoch  17 Batch  216 / 228  Training Loss  5.454070560517721e-05\n","Epoch  17 Batch  217 / 228  Training Loss  4.182580960332416e-05\n","Epoch  17 Batch  218 / 228  Training Loss  6.253594619920477e-05\n","Epoch  17 Batch  219 / 228  Training Loss  6.649916031165048e-05\n","Epoch  17 Batch  220 / 228  Training Loss  6.434899842133746e-05\n","Epoch  17 Batch  221 / 228  Training Loss  4.410098335938528e-05\n","Epoch  17 Batch  222 / 228  Training Loss  4.8319478082703426e-05\n","Epoch  17 Batch  223 / 228  Training Loss  4.434342190506868e-05\n","Epoch  17 Batch  224 / 228  Training Loss  3.5230139474151656e-05\n","Epoch  17 Batch  225 / 228  Training Loss  7.013313006609678e-05\n","Epoch  17 Batch  226 / 228  Training Loss  4.445611557457596e-05\n","Epoch  17 Batch  227 / 228  Training Loss  5.969539415673353e-05\n","  18    |    -    |   0.000068   | 90.853659\n","----------------------------------------------------------------------\n","Running epoch: 18\n","Epoch  18 Batch  0 / 228  Training Loss  4.701941361417994e-05\n","Epoch  18 Batch  1 / 228  Training Loss  6.041933374945074e-05\n","Epoch  18 Batch  2 / 228  Training Loss  4.190270192339085e-05\n","Epoch  18 Batch  3 / 228  Training Loss  6.755052891094238e-05\n","Epoch  18 Batch  4 / 228  Training Loss  3.953213672502898e-05\n","Epoch  18 Batch  5 / 228  Training Loss  4.2812353058252484e-05\n","Epoch  18 Batch  6 / 228  Training Loss  4.32244960393291e-05\n","Epoch  18 Batch  7 / 228  Training Loss  3.731086690095253e-05\n","Epoch  18 Batch  8 / 228  Training Loss  9.216582839144394e-05\n","Epoch  18 Batch  9 / 228  Training Loss  4.9043275794247165e-05\n","Epoch  18 Batch  10 / 228  Training Loss  8.635733684059232e-05\n","Epoch  18 Batch  11 / 228  Training Loss  5.415400300989859e-05\n","Epoch  18 Batch  12 / 228  Training Loss  5.2915325795765966e-05\n","Epoch  18 Batch  13 / 228  Training Loss  4.8647554649505764e-05\n","Epoch  18 Batch  14 / 228  Training Loss  5.1363822421990335e-05\n","Epoch  18 Batch  15 / 228  Training Loss  3.240611476940103e-05\n","Epoch  18 Batch  16 / 228  Training Loss  4.77492249046918e-05\n","Epoch  18 Batch  17 / 228  Training Loss  6.477017450379208e-05\n","Epoch  18 Batch  18 / 228  Training Loss  6.0909846069989726e-05\n","Epoch  18 Batch  19 / 228  Training Loss  6.581312482012436e-05\n","Epoch  18 Batch  20 / 228  Training Loss  6.98000003467314e-05\n","Epoch  18 Batch  21 / 228  Training Loss  7.361914322245866e-05\n","Epoch  18 Batch  22 / 228  Training Loss  5.276699812384322e-05\n","Epoch  18 Batch  23 / 228  Training Loss  4.502333831624128e-05\n","Epoch  18 Batch  24 / 228  Training Loss  8.893841732060537e-05\n","Epoch  18 Batch  25 / 228  Training Loss  6.094418131397106e-05\n","Epoch  18 Batch  26 / 228  Training Loss  6.930379458935931e-05\n","Epoch  18 Batch  27 / 228  Training Loss  4.014843580080196e-05\n","Epoch  18 Batch  28 / 228  Training Loss  3.823282168013975e-05\n","Epoch  18 Batch  29 / 228  Training Loss  6.561397458426654e-05\n","Epoch  18 Batch  30 / 228  Training Loss  8.0595571489539e-05\n","Epoch  18 Batch  31 / 228  Training Loss  6.711371679557487e-05\n","Epoch  18 Batch  32 / 228  Training Loss  3.9428516174666584e-05\n","Epoch  18 Batch  33 / 228  Training Loss  5.826188498758711e-05\n","Epoch  18 Batch  34 / 228  Training Loss  4.87590441480279e-05\n","Epoch  18 Batch  35 / 228  Training Loss  3.9668248064117506e-05\n","Epoch  18 Batch  36 / 228  Training Loss  3.7721521948697045e-05\n","Epoch  18 Batch  37 / 228  Training Loss  4.560480374493636e-05\n","Epoch  18 Batch  38 / 228  Training Loss  5.616060661850497e-05\n","Epoch  18 Batch  39 / 228  Training Loss  3.9481641579186544e-05\n","Epoch  18 Batch  40 / 228  Training Loss  6.180445780046284e-05\n","Epoch  18 Batch  41 / 228  Training Loss  5.760400017607026e-05\n","Epoch  18 Batch  42 / 228  Training Loss  7.86023010732606e-05\n","Epoch  18 Batch  43 / 228  Training Loss  4.883302972302772e-05\n","Epoch  18 Batch  44 / 228  Training Loss  5.137378320796415e-05\n","Epoch  18 Batch  45 / 228  Training Loss  3.556658703018911e-05\n","Epoch  18 Batch  46 / 228  Training Loss  5.8454756072023883e-05\n","Epoch  18 Batch  47 / 228  Training Loss  6.035804108250886e-05\n","Epoch  18 Batch  48 / 228  Training Loss  4.7131772589636967e-05\n","Epoch  18 Batch  49 / 228  Training Loss  5.417138891061768e-05\n","Epoch  18 Batch  50 / 228  Training Loss  5.6089425925165415e-05\n","Epoch  18 Batch  51 / 228  Training Loss  4.586323848343454e-05\n","Epoch  18 Batch  52 / 228  Training Loss  4.525358963292092e-05\n","Epoch  18 Batch  53 / 228  Training Loss  5.585964026977308e-05\n","Epoch  18 Batch  54 / 228  Training Loss  3.7013825931353495e-05\n","Epoch  18 Batch  55 / 228  Training Loss  5.807107299915515e-05\n","Epoch  18 Batch  56 / 228  Training Loss  6.31039947620593e-05\n","Epoch  18 Batch  57 / 228  Training Loss  6.906372436787933e-05\n","Epoch  18 Batch  58 / 228  Training Loss  7.380496390396729e-05\n","Epoch  18 Batch  59 / 228  Training Loss  5.022309778723866e-05\n","Epoch  18 Batch  60 / 228  Training Loss  7.197490776889026e-05\n","Epoch  18 Batch  61 / 228  Training Loss  3.947227742173709e-05\n","Epoch  18 Batch  62 / 228  Training Loss  7.154876220738515e-05\n","Epoch  18 Batch  63 / 228  Training Loss  5.46801820746623e-05\n","Epoch  18 Batch  64 / 228  Training Loss  6.209682032931596e-05\n","Epoch  18 Batch  65 / 228  Training Loss  5.449716991279274e-05\n","Epoch  18 Batch  66 / 228  Training Loss  5.03461342304945e-05\n","Epoch  18 Batch  67 / 228  Training Loss  4.6715962525922805e-05\n","Epoch  18 Batch  68 / 228  Training Loss  5.547023465624079e-05\n","Epoch  18 Batch  69 / 228  Training Loss  6.46301414235495e-05\n","Epoch  18 Batch  70 / 228  Training Loss  5.546383181354031e-05\n","Epoch  18 Batch  71 / 228  Training Loss  3.382088107173331e-05\n","Epoch  18 Batch  72 / 228  Training Loss  3.971578917116858e-05\n","Epoch  18 Batch  73 / 228  Training Loss  5.801361839985475e-05\n","Epoch  18 Batch  74 / 228  Training Loss  8.40014690766111e-05\n","Epoch  18 Batch  75 / 228  Training Loss  7.123192335711792e-05\n","Epoch  18 Batch  76 / 228  Training Loss  3.515410207910463e-05\n","Epoch  18 Batch  77 / 228  Training Loss  3.6030654882779345e-05\n","Epoch  18 Batch  78 / 228  Training Loss  6.0679194575641304e-05\n","Epoch  18 Batch  79 / 228  Training Loss  3.148468385916203e-05\n","Epoch  18 Batch  80 / 228  Training Loss  6.024194590281695e-05\n","Epoch  18 Batch  81 / 228  Training Loss  4.740034637507051e-05\n","Epoch  18 Batch  82 / 228  Training Loss  6.269656296353787e-05\n","Epoch  18 Batch  83 / 228  Training Loss  5.346448233467527e-05\n","Epoch  18 Batch  84 / 228  Training Loss  5.687311931978911e-05\n","Epoch  18 Batch  85 / 228  Training Loss  5.486250665853731e-05\n","Epoch  18 Batch  86 / 228  Training Loss  5.359277201932855e-05\n","Epoch  18 Batch  87 / 228  Training Loss  4.311689917813055e-05\n","Epoch  18 Batch  88 / 228  Training Loss  5.4427964641945437e-05\n","Epoch  18 Batch  89 / 228  Training Loss  3.870554064633325e-05\n","Epoch  18 Batch  90 / 228  Training Loss  5.748667899752036e-05\n","Epoch  18 Batch  91 / 228  Training Loss  4.574249032884836e-05\n","Epoch  18 Batch  92 / 228  Training Loss  5.484832581714727e-05\n","Epoch  18 Batch  93 / 228  Training Loss  5.494420474860817e-05\n","Epoch  18 Batch  94 / 228  Training Loss  5.400816007750109e-05\n","Epoch  18 Batch  95 / 228  Training Loss  4.584629277815111e-05\n","Epoch  18 Batch  96 / 228  Training Loss  4.4434080336941406e-05\n","Epoch  18 Batch  97 / 228  Training Loss  4.957092824042775e-05\n","Epoch  18 Batch  98 / 228  Training Loss  5.489832619787194e-05\n","Epoch  18 Batch  99 / 228  Training Loss  5.535976015380584e-05\n","Epoch  18 Batch  100 / 228  Training Loss  6.291269528446719e-05\n","Epoch  18 Batch  101 / 228  Training Loss  4.442868157639168e-05\n","Epoch  18 Batch  102 / 228  Training Loss  5.185862028156407e-05\n","Epoch  18 Batch  103 / 228  Training Loss  7.670924242120236e-05\n","Epoch  18 Batch  104 / 228  Training Loss  5.365451579564251e-05\n","Epoch  18 Batch  105 / 228  Training Loss  6.113060226198286e-05\n","Epoch  18 Batch  106 / 228  Training Loss  3.7916986912023276e-05\n","Epoch  18 Batch  107 / 228  Training Loss  4.4781645556213334e-05\n","Epoch  18 Batch  108 / 228  Training Loss  4.7464040108025074e-05\n","Epoch  18 Batch  109 / 228  Training Loss  8.096452074823901e-05\n","Epoch  18 Batch  110 / 228  Training Loss  5.2978411986259744e-05\n","Epoch  18 Batch  111 / 228  Training Loss  4.841265399591066e-05\n","Epoch  18 Batch  112 / 228  Training Loss  4.328316936152987e-05\n","Epoch  18 Batch  113 / 228  Training Loss  3.722069595823996e-05\n","Epoch  18 Batch  114 / 228  Training Loss  3.261033270973712e-05\n","Epoch  18 Batch  115 / 228  Training Loss  7.28241793694906e-05\n","Epoch  18 Batch  116 / 228  Training Loss  6.071509051253088e-05\n","Epoch  18 Batch  117 / 228  Training Loss  7.002770871622488e-05\n","Epoch  18 Batch  118 / 228  Training Loss  5.870069799129851e-05\n","Epoch  18 Batch  119 / 228  Training Loss  5.182495078770444e-05\n","Epoch  18 Batch  120 / 228  Training Loss  4.2015275539597496e-05\n","Epoch  18 Batch  121 / 228  Training Loss  4.216674278723076e-05\n","Epoch  18 Batch  122 / 228  Training Loss  6.880369619466364e-05\n","Epoch  18 Batch  123 / 228  Training Loss  4.733290188596584e-05\n","Epoch  18 Batch  124 / 228  Training Loss  4.1261948354076594e-05\n","Epoch  18 Batch  125 / 228  Training Loss  6.285718700382859e-05\n","Epoch  18 Batch  126 / 228  Training Loss  6.09536436968483e-05\n","Epoch  18 Batch  127 / 228  Training Loss  4.394030838739127e-05\n","Epoch  18 Batch  128 / 228  Training Loss  4.0558232285548e-05\n","Epoch  18 Batch  129 / 228  Training Loss  4.348384391050786e-05\n","Epoch  18 Batch  130 / 228  Training Loss  7.41214316803962e-05\n","Epoch  18 Batch  131 / 228  Training Loss  4.5753393351333216e-05\n","Epoch  18 Batch  132 / 228  Training Loss  7.005122461123392e-05\n","Epoch  18 Batch  133 / 228  Training Loss  3.8115831557661295e-05\n","Epoch  18 Batch  134 / 228  Training Loss  4.299833017284982e-05\n","Epoch  18 Batch  135 / 228  Training Loss  5.602489545708522e-05\n","Epoch  18 Batch  136 / 228  Training Loss  4.6399673010455444e-05\n","Epoch  18 Batch  137 / 228  Training Loss  4.64753175037913e-05\n","Epoch  18 Batch  138 / 228  Training Loss  4.9718288209987804e-05\n","Epoch  18 Batch  139 / 228  Training Loss  6.295595812844113e-05\n","Epoch  18 Batch  140 / 228  Training Loss  4.307811468606815e-05\n","Epoch  18 Batch  141 / 228  Training Loss  5.9906375099672005e-05\n","Epoch  18 Batch  142 / 228  Training Loss  5.5508982768515125e-05\n","Epoch  18 Batch  143 / 228  Training Loss  2.4711231162655167e-05\n","Epoch  18 Batch  144 / 228  Training Loss  4.113338218303397e-05\n","Epoch  18 Batch  145 / 228  Training Loss  7.378283044090495e-05\n","Epoch  18 Batch  146 / 228  Training Loss  3.4375523682683706e-05\n","Epoch  18 Batch  147 / 228  Training Loss  5.090679769637063e-05\n","Epoch  18 Batch  148 / 228  Training Loss  3.9006310544209555e-05\n","Epoch  18 Batch  149 / 228  Training Loss  5.7814570027403533e-05\n","Epoch  18 Batch  150 / 228  Training Loss  4.1040300857275724e-05\n","Epoch  18 Batch  151 / 228  Training Loss  4.49582003057003e-05\n","Epoch  18 Batch  152 / 228  Training Loss  4.732298475573771e-05\n","Epoch  18 Batch  153 / 228  Training Loss  5.3136747737880796e-05\n","Epoch  18 Batch  154 / 228  Training Loss  3.616063986555673e-05\n","Epoch  18 Batch  155 / 228  Training Loss  4.126784551772289e-05\n","Epoch  18 Batch  156 / 228  Training Loss  3.806245149462484e-05\n","Epoch  18 Batch  157 / 228  Training Loss  8.316295134136453e-05\n","Epoch  18 Batch  158 / 228  Training Loss  5.666571814799681e-05\n","Epoch  18 Batch  159 / 228  Training Loss  3.720325184985995e-05\n","Epoch  18 Batch  160 / 228  Training Loss  4.5325734390644357e-05\n","Epoch  18 Batch  161 / 228  Training Loss  4.888635521638207e-05\n","Epoch  18 Batch  162 / 228  Training Loss  4.895847450825386e-05\n","Epoch  18 Batch  163 / 228  Training Loss  6.0951526393182576e-05\n","Epoch  18 Batch  164 / 228  Training Loss  5.479487663251348e-05\n","Epoch  18 Batch  165 / 228  Training Loss  4.70425802632235e-05\n","Epoch  18 Batch  166 / 228  Training Loss  5.700985639123246e-05\n","Epoch  18 Batch  167 / 228  Training Loss  5.0780428864527494e-05\n","Epoch  18 Batch  168 / 228  Training Loss  5.764438901678659e-05\n","Epoch  18 Batch  169 / 228  Training Loss  4.8823254473973066e-05\n","Epoch  18 Batch  170 / 228  Training Loss  4.38621973444242e-05\n","Epoch  18 Batch  171 / 228  Training Loss  5.0039918278343976e-05\n","Epoch  18 Batch  172 / 228  Training Loss  4.421425182954408e-05\n","Epoch  18 Batch  173 / 228  Training Loss  6.152095011202618e-05\n","Epoch  18 Batch  174 / 228  Training Loss  4.084244937985204e-05\n","Epoch  18 Batch  175 / 228  Training Loss  3.519888559821993e-05\n","Epoch  18 Batch  176 / 228  Training Loss  4.225883458275348e-05\n","Epoch  18 Batch  177 / 228  Training Loss  5.270554174785502e-05\n","Epoch  18 Batch  178 / 228  Training Loss  5.64818546990864e-05\n","Epoch  18 Batch  179 / 228  Training Loss  4.270357749192044e-05\n","Epoch  18 Batch  180 / 228  Training Loss  3.613050648709759e-05\n","Epoch  18 Batch  181 / 228  Training Loss  3.0535433324985206e-05\n","Epoch  18 Batch  182 / 228  Training Loss  4.764476398122497e-05\n","Epoch  18 Batch  183 / 228  Training Loss  6.30384820397012e-05\n","Epoch  18 Batch  184 / 228  Training Loss  6.170228880364448e-05\n","Epoch  18 Batch  185 / 228  Training Loss  4.374308991828002e-05\n","Epoch  18 Batch  186 / 228  Training Loss  4.993342008674517e-05\n","Epoch  18 Batch  187 / 228  Training Loss  6.361144914990291e-05\n","Epoch  18 Batch  188 / 228  Training Loss  4.2255902371834964e-05\n","Epoch  18 Batch  189 / 228  Training Loss  5.613041503238492e-05\n","Epoch  18 Batch  190 / 228  Training Loss  6.52186936349608e-05\n","Epoch  18 Batch  191 / 228  Training Loss  6.338326056720689e-05\n","Epoch  18 Batch  192 / 228  Training Loss  2.7251882784185e-05\n","Epoch  18 Batch  193 / 228  Training Loss  3.21125153277535e-05\n","Epoch  18 Batch  194 / 228  Training Loss  7.264865416800603e-05\n","Epoch  18 Batch  195 / 228  Training Loss  5.325221354723908e-05\n","Epoch  18 Batch  196 / 228  Training Loss  4.205434743198566e-05\n","Epoch  18 Batch  197 / 228  Training Loss  5.0143156840931624e-05\n","Epoch  18 Batch  198 / 228  Training Loss  4.4276017433730885e-05\n","Epoch  18 Batch  199 / 228  Training Loss  3.9346155972452834e-05\n","Epoch  18 Batch  200 / 228  Training Loss  3.544546052580699e-05\n","Epoch  18 Batch  201 / 228  Training Loss  5.0468679546611384e-05\n","Epoch  18 Batch  202 / 228  Training Loss  5.4270178225124255e-05\n","Epoch  18 Batch  203 / 228  Training Loss  5.947918907622807e-05\n","Epoch  18 Batch  204 / 228  Training Loss  3.75308190996293e-05\n","Epoch  18 Batch  205 / 228  Training Loss  6.101001054048538e-05\n","Epoch  18 Batch  206 / 228  Training Loss  3.325035868328996e-05\n","Epoch  18 Batch  207 / 228  Training Loss  4.247623655828647e-05\n","Epoch  18 Batch  208 / 228  Training Loss  6.605078669963405e-05\n","Epoch  18 Batch  209 / 228  Training Loss  7.355007983278483e-05\n","Epoch  18 Batch  210 / 228  Training Loss  3.283802288933657e-05\n","Epoch  18 Batch  211 / 228  Training Loss  4.1058650822378695e-05\n","Epoch  18 Batch  212 / 228  Training Loss  4.5944045268697664e-05\n","Epoch  18 Batch  213 / 228  Training Loss  4.641452323994599e-05\n","Epoch  18 Batch  214 / 228  Training Loss  3.608832776080817e-05\n","Epoch  18 Batch  215 / 228  Training Loss  3.6305915273260325e-05\n","Epoch  18 Batch  216 / 228  Training Loss  6.501327879959717e-05\n","Epoch  18 Batch  217 / 228  Training Loss  4.0346614696318284e-05\n","Epoch  18 Batch  218 / 228  Training Loss  5.088094258098863e-05\n","Epoch  18 Batch  219 / 228  Training Loss  3.1457460863748565e-05\n","Epoch  18 Batch  220 / 228  Training Loss  7.663901487831026e-05\n","Epoch  18 Batch  221 / 228  Training Loss  4.7744153562234715e-05\n","Epoch  18 Batch  222 / 228  Training Loss  4.6156597818480805e-05\n","Epoch  18 Batch  223 / 228  Training Loss  4.748708306578919e-05\n","Epoch  18 Batch  224 / 228  Training Loss  4.077677658642642e-05\n","Epoch  18 Batch  225 / 228  Training Loss  2.9433134841383435e-05\n","Epoch  18 Batch  226 / 228  Training Loss  5.8236135373590514e-05\n","Epoch  18 Batch  227 / 228  Training Loss  6.071396637707949e-05\n","  19    |    -    |   0.000052   | 90.777439\n","----------------------------------------------------------------------\n","Running epoch: 19\n","Epoch  19 Batch  0 / 228  Training Loss  5.1846320275217295e-05\n","Epoch  19 Batch  1 / 228  Training Loss  3.627367550507188e-05\n","Epoch  19 Batch  2 / 228  Training Loss  4.1463379602646455e-05\n","Epoch  19 Batch  3 / 228  Training Loss  4.180604446446523e-05\n","Epoch  19 Batch  4 / 228  Training Loss  4.214110958855599e-05\n","Epoch  19 Batch  5 / 228  Training Loss  3.541842306731269e-05\n","Epoch  19 Batch  6 / 228  Training Loss  4.3085809011245146e-05\n","Epoch  19 Batch  7 / 228  Training Loss  5.376449189498089e-05\n","Epoch  19 Batch  8 / 228  Training Loss  5.768025584984571e-05\n","Epoch  19 Batch  9 / 228  Training Loss  3.97786388930399e-05\n","Epoch  19 Batch  10 / 228  Training Loss  3.4047938243020326e-05\n","Epoch  19 Batch  11 / 228  Training Loss  3.2969339372357354e-05\n","Epoch  19 Batch  12 / 228  Training Loss  3.5245189792476594e-05\n","Epoch  19 Batch  13 / 228  Training Loss  4.362296749604866e-05\n","Epoch  19 Batch  14 / 228  Training Loss  9.024821338243783e-05\n","Epoch  19 Batch  15 / 228  Training Loss  4.1414317820454016e-05\n","Epoch  19 Batch  16 / 228  Training Loss  4.416329466039315e-05\n","Epoch  19 Batch  17 / 228  Training Loss  4.140237797400914e-05\n","Epoch  19 Batch  18 / 228  Training Loss  6.34746829746291e-05\n","Epoch  19 Batch  19 / 228  Training Loss  5.801605948363431e-05\n","Epoch  19 Batch  20 / 228  Training Loss  3.225795808248222e-05\n","Epoch  19 Batch  21 / 228  Training Loss  5.598893403657712e-05\n","Epoch  19 Batch  22 / 228  Training Loss  5.3274510719347745e-05\n","Epoch  19 Batch  23 / 228  Training Loss  3.0036902899155393e-05\n","Epoch  19 Batch  24 / 228  Training Loss  5.5512271501356736e-05\n","Epoch  19 Batch  25 / 228  Training Loss  3.490492235869169e-05\n","Epoch  19 Batch  26 / 228  Training Loss  3.2981031836243346e-05\n","Epoch  19 Batch  27 / 228  Training Loss  4.355513010523282e-05\n","Epoch  19 Batch  28 / 228  Training Loss  5.001795580028556e-05\n","Epoch  19 Batch  29 / 228  Training Loss  4.440213160705753e-05\n","Epoch  19 Batch  30 / 228  Training Loss  5.156513361725956e-05\n","Epoch  19 Batch  31 / 228  Training Loss  4.80504022561945e-05\n","Epoch  19 Batch  32 / 228  Training Loss  2.9917166102677584e-05\n","Epoch  19 Batch  33 / 228  Training Loss  5.1442391850287095e-05\n","Epoch  19 Batch  34 / 228  Training Loss  4.2041894630528986e-05\n","Epoch  19 Batch  35 / 228  Training Loss  5.031765249441378e-05\n","Epoch  19 Batch  36 / 228  Training Loss  3.574599031708203e-05\n","Epoch  19 Batch  37 / 228  Training Loss  4.7513512981822714e-05\n","Epoch  19 Batch  38 / 228  Training Loss  6.639255298068747e-05\n","Epoch  19 Batch  39 / 228  Training Loss  3.009614192706067e-05\n","Epoch  19 Batch  40 / 228  Training Loss  4.4373238779371604e-05\n","Epoch  19 Batch  41 / 228  Training Loss  3.6724381061503664e-05\n","Epoch  19 Batch  42 / 228  Training Loss  4.451472341315821e-05\n","Epoch  19 Batch  43 / 228  Training Loss  3.644916068878956e-05\n","Epoch  19 Batch  44 / 228  Training Loss  6.83361358824186e-05\n","Epoch  19 Batch  45 / 228  Training Loss  4.989011358702555e-05\n","Epoch  19 Batch  46 / 228  Training Loss  3.314631248940714e-05\n","Epoch  19 Batch  47 / 228  Training Loss  4.8006793804233894e-05\n","Epoch  19 Batch  48 / 228  Training Loss  4.830731631955132e-05\n","Epoch  19 Batch  49 / 228  Training Loss  4.48762220912613e-05\n","Epoch  19 Batch  50 / 228  Training Loss  4.007849202025682e-05\n","Epoch  19 Batch  51 / 228  Training Loss  4.855485531152226e-05\n","Epoch  19 Batch  52 / 228  Training Loss  5.7367382396478206e-05\n","Epoch  19 Batch  53 / 228  Training Loss  4.3361618736526e-05\n","Epoch  19 Batch  54 / 228  Training Loss  2.2781392544857226e-05\n","Epoch  19 Batch  55 / 228  Training Loss  4.5051438064547256e-05\n","Epoch  19 Batch  56 / 228  Training Loss  3.659991853055544e-05\n","Epoch  19 Batch  57 / 228  Training Loss  4.448543768376112e-05\n","Epoch  19 Batch  58 / 228  Training Loss  5.439799133455381e-05\n","Epoch  19 Batch  59 / 228  Training Loss  5.075652006780729e-05\n","Epoch  19 Batch  60 / 228  Training Loss  5.604678517556749e-05\n","Epoch  19 Batch  61 / 228  Training Loss  5.390085789258592e-05\n","Epoch  19 Batch  62 / 228  Training Loss  6.270524318097159e-05\n","Epoch  19 Batch  63 / 228  Training Loss  4.4454634917201474e-05\n","Epoch  19 Batch  64 / 228  Training Loss  3.4840872103814036e-05\n","Epoch  19 Batch  65 / 228  Training Loss  3.8163580029504374e-05\n","Epoch  19 Batch  66 / 228  Training Loss  2.832971586030908e-05\n","Epoch  19 Batch  67 / 228  Training Loss  4.579647065838799e-05\n","Epoch  19 Batch  68 / 228  Training Loss  5.6812772527337074e-05\n","Epoch  19 Batch  69 / 228  Training Loss  3.216910408809781e-05\n","Epoch  19 Batch  70 / 228  Training Loss  5.2622403018176556e-05\n","Epoch  19 Batch  71 / 228  Training Loss  3.839599230559543e-05\n","Epoch  19 Batch  72 / 228  Training Loss  4.143627666053362e-05\n","Epoch  19 Batch  73 / 228  Training Loss  7.4382645834703e-05\n","Epoch  19 Batch  74 / 228  Training Loss  3.062434552703053e-05\n","Epoch  19 Batch  75 / 228  Training Loss  5.275159128359519e-05\n","Epoch  19 Batch  76 / 228  Training Loss  2.2362519302987494e-05\n","Epoch  19 Batch  77 / 228  Training Loss  3.923625990864821e-05\n","Epoch  19 Batch  78 / 228  Training Loss  5.191668969928287e-05\n","Epoch  19 Batch  79 / 228  Training Loss  3.712921534315683e-05\n","Epoch  19 Batch  80 / 228  Training Loss  4.2659728933358565e-05\n","Epoch  19 Batch  81 / 228  Training Loss  4.1569230234017596e-05\n","Epoch  19 Batch  82 / 228  Training Loss  3.9331756852334365e-05\n","Epoch  19 Batch  83 / 228  Training Loss  3.1210322049446404e-05\n","Epoch  19 Batch  84 / 228  Training Loss  4.321004234952852e-05\n","Epoch  19 Batch  85 / 228  Training Loss  3.0327984859468415e-05\n","Epoch  19 Batch  86 / 228  Training Loss  4.413743590703234e-05\n","Epoch  19 Batch  87 / 228  Training Loss  4.4157142838230357e-05\n","Epoch  19 Batch  88 / 228  Training Loss  5.5584740039194e-05\n","Epoch  19 Batch  89 / 228  Training Loss  3.976071093347855e-05\n","Epoch  19 Batch  90 / 228  Training Loss  6.140468030935153e-05\n","Epoch  19 Batch  91 / 228  Training Loss  3.471283343969844e-05\n","Epoch  19 Batch  92 / 228  Training Loss  3.197022306267172e-05\n","Epoch  19 Batch  93 / 228  Training Loss  3.1850082450546324e-05\n","Epoch  19 Batch  94 / 228  Training Loss  4.9813406803878024e-05\n","Epoch  19 Batch  95 / 228  Training Loss  4.394158531795256e-05\n","Epoch  19 Batch  96 / 228  Training Loss  4.622741471393965e-05\n","Epoch  19 Batch  97 / 228  Training Loss  3.9617010770598426e-05\n","Epoch  19 Batch  98 / 228  Training Loss  4.809411984751932e-05\n","Epoch  19 Batch  99 / 228  Training Loss  3.5214816307416186e-05\n","Epoch  19 Batch  100 / 228  Training Loss  4.4528023863676935e-05\n","Epoch  19 Batch  101 / 228  Training Loss  3.8897571357665583e-05\n","Epoch  19 Batch  102 / 228  Training Loss  3.485651177470572e-05\n","Epoch  19 Batch  103 / 228  Training Loss  3.865345934173092e-05\n","Epoch  19 Batch  104 / 228  Training Loss  5.395593325374648e-05\n","Epoch  19 Batch  105 / 228  Training Loss  4.381608960102312e-05\n","Epoch  19 Batch  106 / 228  Training Loss  5.0565959099913016e-05\n","Epoch  19 Batch  107 / 228  Training Loss  4.063882806804031e-05\n","Epoch  19 Batch  108 / 228  Training Loss  3.083766205236316e-05\n","Epoch  19 Batch  109 / 228  Training Loss  4.8956302634906024e-05\n","Epoch  19 Batch  110 / 228  Training Loss  4.986194835510105e-05\n","Epoch  19 Batch  111 / 228  Training Loss  3.348181417095475e-05\n","Epoch  19 Batch  112 / 228  Training Loss  3.629876664490439e-05\n","Epoch  19 Batch  113 / 228  Training Loss  4.3806780013255775e-05\n","Epoch  19 Batch  114 / 228  Training Loss  3.4072294511133805e-05\n","Epoch  19 Batch  115 / 228  Training Loss  4.4116728531662375e-05\n","Epoch  19 Batch  116 / 228  Training Loss  3.832159927696921e-05\n","Epoch  19 Batch  117 / 228  Training Loss  4.6781686251051724e-05\n","Epoch  19 Batch  118 / 228  Training Loss  6.578895408893004e-05\n","Epoch  19 Batch  119 / 228  Training Loss  3.428291893214919e-05\n","Epoch  19 Batch  120 / 228  Training Loss  3.445744005148299e-05\n","Epoch  19 Batch  121 / 228  Training Loss  4.012677163700573e-05\n","Epoch  19 Batch  122 / 228  Training Loss  5.316779424902052e-05\n","Epoch  19 Batch  123 / 228  Training Loss  3.8736052374588326e-05\n","Epoch  19 Batch  124 / 228  Training Loss  3.462663516984321e-05\n","Epoch  19 Batch  125 / 228  Training Loss  4.544232797343284e-05\n","Epoch  19 Batch  126 / 228  Training Loss  3.937296423828229e-05\n","Epoch  19 Batch  127 / 228  Training Loss  5.7080378610407934e-05\n","Epoch  19 Batch  128 / 228  Training Loss  3.768789247260429e-05\n","Epoch  19 Batch  129 / 228  Training Loss  3.473769538686611e-05\n","Epoch  19 Batch  130 / 228  Training Loss  7.400230242637917e-05\n","Epoch  19 Batch  131 / 228  Training Loss  5.959617192274891e-05\n","Epoch  19 Batch  132 / 228  Training Loss  3.4364540624665096e-05\n","Epoch  19 Batch  133 / 228  Training Loss  3.50315822288394e-05\n","Epoch  19 Batch  134 / 228  Training Loss  3.974315040977672e-05\n","Epoch  19 Batch  135 / 228  Training Loss  3.7257876101648435e-05\n","Epoch  19 Batch  136 / 228  Training Loss  2.9358603569562547e-05\n","Epoch  19 Batch  137 / 228  Training Loss  4.419297692948021e-05\n","Epoch  19 Batch  138 / 228  Training Loss  3.114729406661354e-05\n","Epoch  19 Batch  139 / 228  Training Loss  3.659817957668565e-05\n","Epoch  19 Batch  140 / 228  Training Loss  6.222829688340425e-05\n","Epoch  19 Batch  141 / 228  Training Loss  3.5389279219089076e-05\n","Epoch  19 Batch  142 / 228  Training Loss  3.421910514589399e-05\n","Epoch  19 Batch  143 / 228  Training Loss  5.3082349040778354e-05\n","Epoch  19 Batch  144 / 228  Training Loss  4.987753709428944e-05\n","Epoch  19 Batch  145 / 228  Training Loss  3.131341145490296e-05\n","Epoch  19 Batch  146 / 228  Training Loss  3.362724601174705e-05\n","Epoch  19 Batch  147 / 228  Training Loss  4.443637590156868e-05\n","Epoch  19 Batch  148 / 228  Training Loss  5.303299622028135e-05\n","Epoch  19 Batch  149 / 228  Training Loss  3.0443827199633233e-05\n","Epoch  19 Batch  150 / 228  Training Loss  3.517389632179402e-05\n","Epoch  19 Batch  151 / 228  Training Loss  3.490732342470437e-05\n","Epoch  19 Batch  152 / 228  Training Loss  4.680154597735964e-05\n","Epoch  19 Batch  153 / 228  Training Loss  5.8312991313869134e-05\n","Epoch  19 Batch  154 / 228  Training Loss  4.8633432015776634e-05\n","Epoch  19 Batch  155 / 228  Training Loss  4.432841524248943e-05\n","Epoch  19 Batch  156 / 228  Training Loss  3.172962533426471e-05\n","Epoch  19 Batch  157 / 228  Training Loss  2.6793661163537763e-05\n","Epoch  19 Batch  158 / 228  Training Loss  4.971345333615318e-05\n","Epoch  19 Batch  159 / 228  Training Loss  4.925872417516075e-05\n","Epoch  19 Batch  160 / 228  Training Loss  4.4044787500752136e-05\n","Epoch  19 Batch  161 / 228  Training Loss  3.917470894521102e-05\n","Epoch  19 Batch  162 / 228  Training Loss  3.6673893191618845e-05\n","Epoch  19 Batch  163 / 228  Training Loss  5.200551458983682e-05\n","Epoch  19 Batch  164 / 228  Training Loss  2.7901347493752837e-05\n","Epoch  19 Batch  165 / 228  Training Loss  5.036417132942006e-05\n","Epoch  19 Batch  166 / 228  Training Loss  3.487540016067214e-05\n","Epoch  19 Batch  167 / 228  Training Loss  4.937123958370648e-05\n","Epoch  19 Batch  168 / 228  Training Loss  4.2193580156890675e-05\n","Epoch  19 Batch  169 / 228  Training Loss  4.328722570789978e-05\n","Epoch  19 Batch  170 / 228  Training Loss  3.521815233398229e-05\n","Epoch  19 Batch  171 / 228  Training Loss  6.043497705832124e-05\n","Epoch  19 Batch  172 / 228  Training Loss  4.0143928345059976e-05\n","Epoch  19 Batch  173 / 228  Training Loss  3.485436536720954e-05\n","Epoch  19 Batch  174 / 228  Training Loss  5.456387953017838e-05\n","Epoch  19 Batch  175 / 228  Training Loss  3.335766450618394e-05\n","Epoch  19 Batch  176 / 228  Training Loss  6.0008271248079836e-05\n","Epoch  19 Batch  177 / 228  Training Loss  4.7128232836257666e-05\n","Epoch  19 Batch  178 / 228  Training Loss  4.360574530437589e-05\n","Epoch  19 Batch  179 / 228  Training Loss  3.916834975825623e-05\n","Epoch  19 Batch  180 / 228  Training Loss  4.470641215448268e-05\n","Epoch  19 Batch  181 / 228  Training Loss  5.5896074627526104e-05\n","Epoch  19 Batch  182 / 228  Training Loss  4.96573411510326e-05\n","Epoch  19 Batch  183 / 228  Training Loss  5.068476457381621e-05\n","Epoch  19 Batch  184 / 228  Training Loss  4.161089600529522e-05\n","Epoch  19 Batch  185 / 228  Training Loss  2.9117041776771657e-05\n","Epoch  19 Batch  186 / 228  Training Loss  2.5919907784555107e-05\n","Epoch  19 Batch  187 / 228  Training Loss  4.0902825276134536e-05\n","Epoch  19 Batch  188 / 228  Training Loss  3.6086545151192695e-05\n","Epoch  19 Batch  189 / 228  Training Loss  3.363842188264243e-05\n","Epoch  19 Batch  190 / 228  Training Loss  3.209795613656752e-05\n","Epoch  19 Batch  191 / 228  Training Loss  3.5694512916961685e-05\n","Epoch  19 Batch  192 / 228  Training Loss  3.7936839362373576e-05\n","Epoch  19 Batch  193 / 228  Training Loss  4.652994539355859e-05\n","Epoch  19 Batch  194 / 228  Training Loss  4.689101115218364e-05\n","Epoch  19 Batch  195 / 228  Training Loss  2.5512428692309186e-05\n","Epoch  19 Batch  196 / 228  Training Loss  3.667464989121072e-05\n","Epoch  19 Batch  197 / 228  Training Loss  4.34028661402408e-05\n","Epoch  19 Batch  198 / 228  Training Loss  5.967923061689362e-05\n","Epoch  19 Batch  199 / 228  Training Loss  4.82481591461692e-05\n","Epoch  19 Batch  200 / 228  Training Loss  3.878151983371936e-05\n","Epoch  19 Batch  201 / 228  Training Loss  3.261596793890931e-05\n","Epoch  19 Batch  202 / 228  Training Loss  4.060118590132333e-05\n","Epoch  19 Batch  203 / 228  Training Loss  4.021079439553432e-05\n","Epoch  19 Batch  204 / 228  Training Loss  4.492774678510614e-05\n","Epoch  19 Batch  205 / 228  Training Loss  3.438491694396362e-05\n","Epoch  19 Batch  206 / 228  Training Loss  4.650316259358078e-05\n","Epoch  19 Batch  207 / 228  Training Loss  4.346523928688839e-05\n","Epoch  19 Batch  208 / 228  Training Loss  4.083958265255205e-05\n","Epoch  19 Batch  209 / 228  Training Loss  2.7858552130055614e-05\n","Epoch  19 Batch  210 / 228  Training Loss  3.654043030110188e-05\n","Epoch  19 Batch  211 / 228  Training Loss  2.8890546673210338e-05\n","Epoch  19 Batch  212 / 228  Training Loss  4.3095180444652215e-05\n","Epoch  19 Batch  213 / 228  Training Loss  3.8870726712048054e-05\n","Epoch  19 Batch  214 / 228  Training Loss  3.353600550326519e-05\n","Epoch  19 Batch  215 / 228  Training Loss  3.6511100915959105e-05\n","Epoch  19 Batch  216 / 228  Training Loss  3.7213168980088085e-05\n","Epoch  19 Batch  217 / 228  Training Loss  4.079206337337382e-05\n","Epoch  19 Batch  218 / 228  Training Loss  6.327041774056852e-05\n","Epoch  19 Batch  219 / 228  Training Loss  3.765290603041649e-05\n","Epoch  19 Batch  220 / 228  Training Loss  4.4246335164643824e-05\n","Epoch  19 Batch  221 / 228  Training Loss  3.161900895065628e-05\n","Epoch  19 Batch  222 / 228  Training Loss  3.531450784066692e-05\n","Epoch  19 Batch  223 / 228  Training Loss  3.902913158526644e-05\n","Epoch  19 Batch  224 / 228  Training Loss  4.5108816266292706e-05\n","Epoch  19 Batch  225 / 228  Training Loss  4.525748227024451e-05\n","Epoch  19 Batch  226 / 228  Training Loss  3.3947391784749925e-05\n","Epoch  19 Batch  227 / 228  Training Loss  5.212084579397924e-05\n","  20    |    -    |   0.000043   | 91.006098\n","----------------------------------------------------------------------\n","Running epoch: 20\n","Epoch  20 Batch  0 / 228  Training Loss  4.120652374695055e-05\n","Epoch  20 Batch  1 / 228  Training Loss  3.9777904021320865e-05\n","Epoch  20 Batch  2 / 228  Training Loss  2.545849929447286e-05\n","Epoch  20 Batch  3 / 228  Training Loss  2.631578900036402e-05\n","Epoch  20 Batch  4 / 228  Training Loss  4.617007652996108e-05\n","Epoch  20 Batch  5 / 228  Training Loss  3.2734544220147654e-05\n","Epoch  20 Batch  6 / 228  Training Loss  6.099870370235294e-05\n","Epoch  20 Batch  7 / 228  Training Loss  2.0212701201671734e-05\n","Epoch  20 Batch  8 / 228  Training Loss  2.7466778192319907e-05\n","Epoch  20 Batch  9 / 228  Training Loss  4.061701110913418e-05\n","Epoch  20 Batch  10 / 228  Training Loss  4.5436052459990606e-05\n","Epoch  20 Batch  11 / 228  Training Loss  2.8755654057022184e-05\n","Epoch  20 Batch  12 / 228  Training Loss  3.792264760704711e-05\n","Epoch  20 Batch  13 / 228  Training Loss  3.596362512325868e-05\n","Epoch  20 Batch  14 / 228  Training Loss  3.755149373319e-05\n","Epoch  20 Batch  15 / 228  Training Loss  3.900622687069699e-05\n","Epoch  20 Batch  16 / 228  Training Loss  3.94313974538818e-05\n","Epoch  20 Batch  17 / 228  Training Loss  3.755514990189113e-05\n","Epoch  20 Batch  18 / 228  Training Loss  3.531200491124764e-05\n","Epoch  20 Batch  19 / 228  Training Loss  3.86311839974951e-05\n","Epoch  20 Batch  20 / 228  Training Loss  5.369359496398829e-05\n","Epoch  20 Batch  21 / 228  Training Loss  4.0210590668721125e-05\n","Epoch  20 Batch  22 / 228  Training Loss  3.7501649785554036e-05\n","Epoch  20 Batch  23 / 228  Training Loss  3.120296969427727e-05\n","Epoch  20 Batch  24 / 228  Training Loss  3.756289152079262e-05\n","Epoch  20 Batch  25 / 228  Training Loss  4.0268369048135355e-05\n","Epoch  20 Batch  26 / 228  Training Loss  3.188394839526154e-05\n","Epoch  20 Batch  27 / 228  Training Loss  2.0804003725061193e-05\n","Epoch  20 Batch  28 / 228  Training Loss  4.5069926272844896e-05\n","Epoch  20 Batch  29 / 228  Training Loss  3.281798854004592e-05\n","Epoch  20 Batch  30 / 228  Training Loss  2.829839286278002e-05\n","Epoch  20 Batch  31 / 228  Training Loss  3.2164371077669784e-05\n","Epoch  20 Batch  32 / 228  Training Loss  4.0997631003847346e-05\n","Epoch  20 Batch  33 / 228  Training Loss  4.257237014826387e-05\n","Epoch  20 Batch  34 / 228  Training Loss  3.606074824347161e-05\n","Epoch  20 Batch  35 / 228  Training Loss  4.714452370535582e-05\n","Epoch  20 Batch  36 / 228  Training Loss  2.7838572350447066e-05\n","Epoch  20 Batch  37 / 228  Training Loss  2.7100742954644375e-05\n","Epoch  20 Batch  38 / 228  Training Loss  5.7485420256853104e-05\n","Epoch  20 Batch  39 / 228  Training Loss  4.8780322686070576e-05\n","Epoch  20 Batch  40 / 228  Training Loss  2.50419216172304e-05\n","Epoch  20 Batch  41 / 228  Training Loss  3.4192467865068465e-05\n","Epoch  20 Batch  42 / 228  Training Loss  3.7440433516167104e-05\n","Epoch  20 Batch  43 / 228  Training Loss  4.077137418789789e-05\n","Epoch  20 Batch  44 / 228  Training Loss  3.7242050893837586e-05\n","Epoch  20 Batch  45 / 228  Training Loss  3.3914417144842446e-05\n","Epoch  20 Batch  46 / 228  Training Loss  2.705781298573129e-05\n","Epoch  20 Batch  47 / 228  Training Loss  3.360871414770372e-05\n","Epoch  20 Batch  48 / 228  Training Loss  2.6812725991476327e-05\n","Epoch  20 Batch  49 / 228  Training Loss  2.2693533537676558e-05\n","Epoch  20 Batch  50 / 228  Training Loss  3.725222995853983e-05\n","Epoch  20 Batch  51 / 228  Training Loss  4.797787914867513e-05\n","Epoch  20 Batch  52 / 228  Training Loss  4.25621765316464e-05\n","Epoch  20 Batch  53 / 228  Training Loss  4.0820155845722184e-05\n","Epoch  20 Batch  54 / 228  Training Loss  3.3997352147707716e-05\n","Epoch  20 Batch  55 / 228  Training Loss  7.099266076693311e-05\n","Epoch  20 Batch  56 / 228  Training Loss  4.656006785808131e-05\n","Epoch  20 Batch  57 / 228  Training Loss  3.0221894121496007e-05\n","Epoch  20 Batch  58 / 228  Training Loss  3.4044223866658285e-05\n","Epoch  20 Batch  59 / 228  Training Loss  4.510625876719132e-05\n","Epoch  20 Batch  60 / 228  Training Loss  3.7512119888560846e-05\n","Epoch  20 Batch  61 / 228  Training Loss  4.295458347769454e-05\n","Epoch  20 Batch  62 / 228  Training Loss  3.284709237050265e-05\n","Epoch  20 Batch  63 / 228  Training Loss  6.208780541783199e-05\n","Epoch  20 Batch  64 / 228  Training Loss  5.68596224184148e-05\n","Epoch  20 Batch  65 / 228  Training Loss  3.286920764367096e-05\n","Epoch  20 Batch  66 / 228  Training Loss  5.558624616242014e-05\n","Epoch  20 Batch  67 / 228  Training Loss  4.119373988942243e-05\n","Epoch  20 Batch  68 / 228  Training Loss  5.4535004892386496e-05\n","Epoch  20 Batch  69 / 228  Training Loss  3.639492206275463e-05\n","Epoch  20 Batch  70 / 228  Training Loss  3.275586277595721e-05\n","Epoch  20 Batch  71 / 228  Training Loss  3.2722909963922575e-05\n","Epoch  20 Batch  72 / 228  Training Loss  5.388130739447661e-05\n","Epoch  20 Batch  73 / 228  Training Loss  4.9281708925263956e-05\n","Epoch  20 Batch  74 / 228  Training Loss  3.1828254577703774e-05\n","Epoch  20 Batch  75 / 228  Training Loss  3.842445948976092e-05\n","Epoch  20 Batch  76 / 228  Training Loss  4.469283521757461e-05\n","Epoch  20 Batch  77 / 228  Training Loss  3.4203214454464614e-05\n","Epoch  20 Batch  78 / 228  Training Loss  4.097876080777496e-05\n","Epoch  20 Batch  79 / 228  Training Loss  4.444899605005048e-05\n","Epoch  20 Batch  80 / 228  Training Loss  3.201818253728561e-05\n","Epoch  20 Batch  81 / 228  Training Loss  3.238976205466315e-05\n","Epoch  20 Batch  82 / 228  Training Loss  3.8128266169223934e-05\n","Epoch  20 Batch  83 / 228  Training Loss  3.408110569580458e-05\n","Epoch  20 Batch  84 / 228  Training Loss  3.473347896942869e-05\n","Epoch  20 Batch  85 / 228  Training Loss  4.6870347432559356e-05\n","Epoch  20 Batch  86 / 228  Training Loss  2.7359845262253657e-05\n","Epoch  20 Batch  87 / 228  Training Loss  3.8929654692765325e-05\n","Epoch  20 Batch  88 / 228  Training Loss  4.2558880522847176e-05\n","Epoch  20 Batch  89 / 228  Training Loss  3.219132122467272e-05\n","Epoch  20 Batch  90 / 228  Training Loss  2.570682408986613e-05\n","Epoch  20 Batch  91 / 228  Training Loss  2.7719430363504216e-05\n","Epoch  20 Batch  92 / 228  Training Loss  5.062920899945311e-05\n","Epoch  20 Batch  93 / 228  Training Loss  5.1169521611882374e-05\n","Epoch  20 Batch  94 / 228  Training Loss  3.374290463398211e-05\n","Epoch  20 Batch  95 / 228  Training Loss  2.8917316740262322e-05\n","Epoch  20 Batch  96 / 228  Training Loss  4.0493745473213494e-05\n","Epoch  20 Batch  97 / 228  Training Loss  2.9983213607920334e-05\n","Epoch  20 Batch  98 / 228  Training Loss  2.7584548661252484e-05\n","Epoch  20 Batch  99 / 228  Training Loss  3.906212805304676e-05\n","Epoch  20 Batch  100 / 228  Training Loss  2.1101415768498555e-05\n","Epoch  20 Batch  101 / 228  Training Loss  3.346915400470607e-05\n","Epoch  20 Batch  102 / 228  Training Loss  4.089764115633443e-05\n","Epoch  20 Batch  103 / 228  Training Loss  4.5955552195664495e-05\n","Epoch  20 Batch  104 / 228  Training Loss  3.162348730256781e-05\n","Epoch  20 Batch  105 / 228  Training Loss  3.7147172406548634e-05\n","Epoch  20 Batch  106 / 228  Training Loss  4.177743903710507e-05\n","Epoch  20 Batch  107 / 228  Training Loss  3.676715641631745e-05\n","Epoch  20 Batch  108 / 228  Training Loss  4.118736615055241e-05\n","Epoch  20 Batch  109 / 228  Training Loss  2.8224942070664838e-05\n","Epoch  20 Batch  110 / 228  Training Loss  4.9764181312639266e-05\n","Epoch  20 Batch  111 / 228  Training Loss  3.759458923013881e-05\n","Epoch  20 Batch  112 / 228  Training Loss  3.301850301795639e-05\n","Epoch  20 Batch  113 / 228  Training Loss  3.5449196730041876e-05\n","Epoch  20 Batch  114 / 228  Training Loss  3.58883808075916e-05\n","Epoch  20 Batch  115 / 228  Training Loss  3.239222496631555e-05\n","Epoch  20 Batch  116 / 228  Training Loss  3.5160403058398515e-05\n","Epoch  20 Batch  117 / 228  Training Loss  4.9303525884170085e-05\n","Epoch  20 Batch  118 / 228  Training Loss  3.801671482506208e-05\n","Epoch  20 Batch  119 / 228  Training Loss  3.687613207148388e-05\n","Epoch  20 Batch  120 / 228  Training Loss  4.621761399903335e-05\n","Epoch  20 Batch  121 / 228  Training Loss  5.842600512551144e-05\n","Epoch  20 Batch  122 / 228  Training Loss  3.9417445805156603e-05\n","Epoch  20 Batch  123 / 228  Training Loss  2.3217593479785137e-05\n","Epoch  20 Batch  124 / 228  Training Loss  4.639959297492169e-05\n","Epoch  20 Batch  125 / 228  Training Loss  2.7147742002853192e-05\n","Epoch  20 Batch  126 / 228  Training Loss  3.513212504913099e-05\n","Epoch  20 Batch  127 / 228  Training Loss  2.4334505724254996e-05\n","Epoch  20 Batch  128 / 228  Training Loss  3.072306208196096e-05\n","Epoch  20 Batch  129 / 228  Training Loss  3.609941268223338e-05\n","Epoch  20 Batch  130 / 228  Training Loss  4.157093644607812e-05\n","Epoch  20 Batch  131 / 228  Training Loss  3.195421595592052e-05\n","Epoch  20 Batch  132 / 228  Training Loss  2.9600487323477864e-05\n","Epoch  20 Batch  133 / 228  Training Loss  2.6287403670721687e-05\n","Epoch  20 Batch  134 / 228  Training Loss  3.7075267755426466e-05\n","Epoch  20 Batch  135 / 228  Training Loss  3.126072260783985e-05\n","Epoch  20 Batch  136 / 228  Training Loss  3.9295046008192e-05\n","Epoch  20 Batch  137 / 228  Training Loss  3.105033465544693e-05\n","Epoch  20 Batch  138 / 228  Training Loss  4.103016181034036e-05\n","Epoch  20 Batch  139 / 228  Training Loss  3.190423012711108e-05\n","Epoch  20 Batch  140 / 228  Training Loss  4.669625559472479e-05\n","Epoch  20 Batch  141 / 228  Training Loss  2.5153180104098283e-05\n","Epoch  20 Batch  142 / 228  Training Loss  3.4015854907920584e-05\n","Epoch  20 Batch  143 / 228  Training Loss  3.1410276278620586e-05\n","Epoch  20 Batch  144 / 228  Training Loss  2.553873127908446e-05\n","Epoch  20 Batch  145 / 228  Training Loss  3.540219768183306e-05\n","Epoch  20 Batch  146 / 228  Training Loss  4.389416062622331e-05\n","Epoch  20 Batch  147 / 228  Training Loss  3.222909799660556e-05\n","Epoch  20 Batch  148 / 228  Training Loss  3.0418936148635112e-05\n","Epoch  20 Batch  149 / 228  Training Loss  4.6264085540315136e-05\n","Epoch  20 Batch  150 / 228  Training Loss  3.807041503023356e-05\n","Epoch  20 Batch  151 / 228  Training Loss  4.900012572761625e-05\n","Epoch  20 Batch  152 / 228  Training Loss  3.20287945214659e-05\n","Epoch  20 Batch  153 / 228  Training Loss  3.1434108677785844e-05\n","Epoch  20 Batch  154 / 228  Training Loss  3.8478628994198516e-05\n","Epoch  20 Batch  155 / 228  Training Loss  5.427766518550925e-05\n","Epoch  20 Batch  156 / 228  Training Loss  3.2175066735362634e-05\n","Epoch  20 Batch  157 / 228  Training Loss  3.964611096307635e-05\n","Epoch  20 Batch  158 / 228  Training Loss  4.652377901948057e-05\n","Epoch  20 Batch  159 / 228  Training Loss  3.1508297979598865e-05\n","Epoch  20 Batch  160 / 228  Training Loss  4.8323607188649476e-05\n","Epoch  20 Batch  161 / 228  Training Loss  4.3937710870523006e-05\n","Epoch  20 Batch  162 / 228  Training Loss  2.7456404495751485e-05\n","Epoch  20 Batch  163 / 228  Training Loss  4.9954080168390647e-05\n","Epoch  20 Batch  164 / 228  Training Loss  2.442914410494268e-05\n","Epoch  20 Batch  165 / 228  Training Loss  2.946541462733876e-05\n","Epoch  20 Batch  166 / 228  Training Loss  2.809933175740298e-05\n","Epoch  20 Batch  167 / 228  Training Loss  3.974418723373674e-05\n","Epoch  20 Batch  168 / 228  Training Loss  3.2390482374466956e-05\n","Epoch  20 Batch  169 / 228  Training Loss  3.090758400503546e-05\n","Epoch  20 Batch  170 / 228  Training Loss  3.511395334498957e-05\n","Epoch  20 Batch  171 / 228  Training Loss  6.010759898344986e-05\n","Epoch  20 Batch  172 / 228  Training Loss  4.120585072087124e-05\n","Epoch  20 Batch  173 / 228  Training Loss  3.40074511768762e-05\n","Epoch  20 Batch  174 / 228  Training Loss  3.6077126424061134e-05\n","Epoch  20 Batch  175 / 228  Training Loss  3.849558925139718e-05\n","Epoch  20 Batch  176 / 228  Training Loss  2.594365105323959e-05\n","Epoch  20 Batch  177 / 228  Training Loss  3.20245380862616e-05\n","Epoch  20 Batch  178 / 228  Training Loss  3.29844442603644e-05\n","Epoch  20 Batch  179 / 228  Training Loss  4.965463813277893e-05\n","Epoch  20 Batch  180 / 228  Training Loss  3.425468094064854e-05\n","Epoch  20 Batch  181 / 228  Training Loss  2.9952116165077314e-05\n","Epoch  20 Batch  182 / 228  Training Loss  3.088350786129013e-05\n","Epoch  20 Batch  183 / 228  Training Loss  3.716087667271495e-05\n","Epoch  20 Batch  184 / 228  Training Loss  2.3257196517079137e-05\n","Epoch  20 Batch  185 / 228  Training Loss  2.5614514015614986e-05\n","Epoch  20 Batch  186 / 228  Training Loss  3.955597276217304e-05\n","Epoch  20 Batch  187 / 228  Training Loss  4.157853254582733e-05\n","Epoch  20 Batch  188 / 228  Training Loss  4.252664803061634e-05\n","Epoch  20 Batch  189 / 228  Training Loss  4.283053567633033e-05\n","Epoch  20 Batch  190 / 228  Training Loss  3.073951666010544e-05\n","Epoch  20 Batch  191 / 228  Training Loss  3.190246206941083e-05\n","Epoch  20 Batch  192 / 228  Training Loss  2.9429318601614796e-05\n","Epoch  20 Batch  193 / 228  Training Loss  1.8904254829976708e-05\n","Epoch  20 Batch  194 / 228  Training Loss  3.516850483720191e-05\n","Epoch  20 Batch  195 / 228  Training Loss  4.8848236474441364e-05\n","Epoch  20 Batch  196 / 228  Training Loss  4.6042041503824294e-05\n","Epoch  20 Batch  197 / 228  Training Loss  4.0787967009237036e-05\n","Epoch  20 Batch  198 / 228  Training Loss  2.8619206204894e-05\n","Epoch  20 Batch  199 / 228  Training Loss  3.877245762851089e-05\n","Epoch  20 Batch  200 / 228  Training Loss  3.9435934013454244e-05\n","Epoch  20 Batch  201 / 228  Training Loss  2.996444709424395e-05\n","Epoch  20 Batch  202 / 228  Training Loss  4.156239083386026e-05\n","Epoch  20 Batch  203 / 228  Training Loss  3.823897714028135e-05\n","Epoch  20 Batch  204 / 228  Training Loss  3.1775827665114775e-05\n","Epoch  20 Batch  205 / 228  Training Loss  3.7868492654524744e-05\n","Epoch  20 Batch  206 / 228  Training Loss  2.3142294594435953e-05\n","Epoch  20 Batch  207 / 228  Training Loss  3.660272341221571e-05\n","Epoch  20 Batch  208 / 228  Training Loss  3.1138031772570685e-05\n","Epoch  20 Batch  209 / 228  Training Loss  2.83658919215668e-05\n","Epoch  20 Batch  210 / 228  Training Loss  3.6524659662973136e-05\n","Epoch  20 Batch  211 / 228  Training Loss  2.662204315129202e-05\n","Epoch  20 Batch  212 / 228  Training Loss  2.3147282263380475e-05\n","Epoch  20 Batch  213 / 228  Training Loss  3.2195242965826765e-05\n","Epoch  20 Batch  214 / 228  Training Loss  2.7412190320319496e-05\n","Epoch  20 Batch  215 / 228  Training Loss  3.063123222091235e-05\n","Epoch  20 Batch  216 / 228  Training Loss  3.915057095582597e-05\n","Epoch  20 Batch  217 / 228  Training Loss  3.4522581700002775e-05\n","Epoch  20 Batch  218 / 228  Training Loss  3.440344153204933e-05\n","Epoch  20 Batch  219 / 228  Training Loss  3.1913838029140607e-05\n","Epoch  20 Batch  220 / 228  Training Loss  2.5184797777910717e-05\n","Epoch  20 Batch  221 / 228  Training Loss  4.8028108722064644e-05\n","Epoch  20 Batch  222 / 228  Training Loss  2.6317698939237744e-05\n","Epoch  20 Batch  223 / 228  Training Loss  2.9366250601015054e-05\n","Epoch  20 Batch  224 / 228  Training Loss  5.131525904289447e-05\n","Epoch  20 Batch  225 / 228  Training Loss  2.9772758352919482e-05\n","Epoch  20 Batch  226 / 228  Training Loss  3.746125003090128e-05\n","Epoch  20 Batch  227 / 228  Training Loss  3.950376049033366e-05\n","  21    |    -    |   0.000037   | 91.044207\n","----------------------------------------------------------------------\n","Running epoch: 21\n","Epoch  21 Batch  0 / 228  Training Loss  2.9445442123687826e-05\n","Epoch  21 Batch  1 / 228  Training Loss  2.7832689738716e-05\n","Epoch  21 Batch  2 / 228  Training Loss  4.3182219087611884e-05\n","Epoch  21 Batch  3 / 228  Training Loss  3.3381696994183585e-05\n","Epoch  21 Batch  4 / 228  Training Loss  3.5914221371058375e-05\n","Epoch  21 Batch  5 / 228  Training Loss  3.236632255720906e-05\n","Epoch  21 Batch  6 / 228  Training Loss  3.7781264836667106e-05\n","Epoch  21 Batch  7 / 228  Training Loss  2.7765938284574077e-05\n","Epoch  21 Batch  8 / 228  Training Loss  4.293956226320006e-05\n","Epoch  21 Batch  9 / 228  Training Loss  3.7540117773460224e-05\n","Epoch  21 Batch  10 / 228  Training Loss  2.5559153073118068e-05\n","Epoch  21 Batch  11 / 228  Training Loss  3.382763679837808e-05\n","Epoch  21 Batch  12 / 228  Training Loss  3.233555253245868e-05\n","Epoch  21 Batch  13 / 228  Training Loss  3.47261484421324e-05\n","Epoch  21 Batch  14 / 228  Training Loss  3.246383857913315e-05\n","Epoch  21 Batch  15 / 228  Training Loss  3.322136035421863e-05\n","Epoch  21 Batch  16 / 228  Training Loss  3.438467319938354e-05\n","Epoch  21 Batch  17 / 228  Training Loss  3.667001874418929e-05\n","Epoch  21 Batch  18 / 228  Training Loss  3.4665845305426046e-05\n","Epoch  21 Batch  19 / 228  Training Loss  3.0377590519492514e-05\n","Epoch  21 Batch  20 / 228  Training Loss  2.482748095644638e-05\n","Epoch  21 Batch  21 / 228  Training Loss  2.2343867385643534e-05\n","Epoch  21 Batch  22 / 228  Training Loss  3.4852906537707895e-05\n","Epoch  21 Batch  23 / 228  Training Loss  3.0801475077169016e-05\n","Epoch  21 Batch  24 / 228  Training Loss  3.482328247628175e-05\n","Epoch  21 Batch  25 / 228  Training Loss  3.2216034014709294e-05\n","Epoch  21 Batch  26 / 228  Training Loss  5.418679938884452e-05\n","Epoch  21 Batch  27 / 228  Training Loss  3.4854732803069055e-05\n","Epoch  21 Batch  28 / 228  Training Loss  3.6643170460592955e-05\n","Epoch  21 Batch  29 / 228  Training Loss  2.5279998226324096e-05\n","Epoch  21 Batch  30 / 228  Training Loss  2.7110896553494968e-05\n","Epoch  21 Batch  31 / 228  Training Loss  5.027306542615406e-05\n","Epoch  21 Batch  32 / 228  Training Loss  3.3485957828816026e-05\n","Epoch  21 Batch  33 / 228  Training Loss  3.066194040002301e-05\n","Epoch  21 Batch  34 / 228  Training Loss  2.7257216061116196e-05\n","Epoch  21 Batch  35 / 228  Training Loss  3.622855365392752e-05\n","Epoch  21 Batch  36 / 228  Training Loss  2.6279010853613727e-05\n","Epoch  21 Batch  37 / 228  Training Loss  4.2150662920903414e-05\n","Epoch  21 Batch  38 / 228  Training Loss  3.9410562749253586e-05\n","Epoch  21 Batch  39 / 228  Training Loss  2.7317573767504655e-05\n","Epoch  21 Batch  40 / 228  Training Loss  3.150738848489709e-05\n","Epoch  21 Batch  41 / 228  Training Loss  2.488023710611742e-05\n","Epoch  21 Batch  42 / 228  Training Loss  3.220500366296619e-05\n","Epoch  21 Batch  43 / 228  Training Loss  2.6052572138723917e-05\n","Epoch  21 Batch  44 / 228  Training Loss  2.7637806851998903e-05\n","Epoch  21 Batch  45 / 228  Training Loss  4.919386628898792e-05\n","Epoch  21 Batch  46 / 228  Training Loss  3.131595076411031e-05\n","Epoch  21 Batch  47 / 228  Training Loss  1.6975935068330728e-05\n","Epoch  21 Batch  48 / 228  Training Loss  4.0654398617334664e-05\n","Epoch  21 Batch  49 / 228  Training Loss  3.898507929989137e-05\n","Epoch  21 Batch  50 / 228  Training Loss  2.9133450880181044e-05\n","Epoch  21 Batch  51 / 228  Training Loss  3.36057273671031e-05\n","Epoch  21 Batch  52 / 228  Training Loss  2.9116354198777117e-05\n","Epoch  21 Batch  53 / 228  Training Loss  2.7237603717367165e-05\n","Epoch  21 Batch  54 / 228  Training Loss  2.9331264158827253e-05\n","Epoch  21 Batch  55 / 228  Training Loss  2.7908321499126032e-05\n","Epoch  21 Batch  56 / 228  Training Loss  2.787736229947768e-05\n","Epoch  21 Batch  57 / 228  Training Loss  2.034355748037342e-05\n","Epoch  21 Batch  58 / 228  Training Loss  4.3736443330999464e-05\n","Epoch  21 Batch  59 / 228  Training Loss  4.228528632665984e-05\n","Epoch  21 Batch  60 / 228  Training Loss  4.162177356192842e-05\n","Epoch  21 Batch  61 / 228  Training Loss  4.3186922994209453e-05\n","Epoch  21 Batch  62 / 228  Training Loss  2.7195927032153122e-05\n","Epoch  21 Batch  63 / 228  Training Loss  2.8172955353511497e-05\n","Epoch  21 Batch  64 / 228  Training Loss  2.873330595321022e-05\n","Epoch  21 Batch  65 / 228  Training Loss  4.781788447871804e-05\n","Epoch  21 Batch  66 / 228  Training Loss  3.983628994319588e-05\n","Epoch  21 Batch  67 / 228  Training Loss  3.6888446629745886e-05\n","Epoch  21 Batch  68 / 228  Training Loss  2.6528163289185613e-05\n","Epoch  21 Batch  69 / 228  Training Loss  3.116634616162628e-05\n","Epoch  21 Batch  70 / 228  Training Loss  2.8296984964981675e-05\n","Epoch  21 Batch  71 / 228  Training Loss  3.966555232182145e-05\n","Epoch  21 Batch  72 / 228  Training Loss  3.0915882234694436e-05\n","Epoch  21 Batch  73 / 228  Training Loss  2.421445242362097e-05\n","Epoch  21 Batch  74 / 228  Training Loss  2.9852561056031846e-05\n","Epoch  21 Batch  75 / 228  Training Loss  5.2393799705896527e-05\n","Epoch  21 Batch  76 / 228  Training Loss  3.5299428418511525e-05\n","Epoch  21 Batch  77 / 228  Training Loss  2.6798721592058428e-05\n","Epoch  21 Batch  78 / 228  Training Loss  4.2664258216973394e-05\n","Epoch  21 Batch  79 / 228  Training Loss  2.6588233595248312e-05\n","Epoch  21 Batch  80 / 228  Training Loss  1.8181548512075096e-05\n","Epoch  21 Batch  81 / 228  Training Loss  2.980966200993862e-05\n","Epoch  21 Batch  82 / 228  Training Loss  3.383144940016791e-05\n","Epoch  21 Batch  83 / 228  Training Loss  2.309456431248691e-05\n","Epoch  21 Batch  84 / 228  Training Loss  2.947904431493953e-05\n","Epoch  21 Batch  85 / 228  Training Loss  3.682549504446797e-05\n","Epoch  21 Batch  86 / 228  Training Loss  3.2020852813730016e-05\n","Epoch  21 Batch  87 / 228  Training Loss  2.7787442377302796e-05\n","Epoch  21 Batch  88 / 228  Training Loss  5.1092338253511116e-05\n","Epoch  21 Batch  89 / 228  Training Loss  3.0707447876920924e-05\n","Epoch  21 Batch  90 / 228  Training Loss  2.3541017071693204e-05\n","Epoch  21 Batch  91 / 228  Training Loss  4.114965122425929e-05\n","Epoch  21 Batch  92 / 228  Training Loss  2.8974638553336263e-05\n","Epoch  21 Batch  93 / 228  Training Loss  3.080207898165099e-05\n","Epoch  21 Batch  94 / 228  Training Loss  2.9623593945871107e-05\n","Epoch  21 Batch  95 / 228  Training Loss  3.243120954721235e-05\n","Epoch  21 Batch  96 / 228  Training Loss  4.420777622726746e-05\n","Epoch  21 Batch  97 / 228  Training Loss  2.6620060452842154e-05\n","Epoch  21 Batch  98 / 228  Training Loss  4.5590262743644416e-05\n","Epoch  21 Batch  99 / 228  Training Loss  3.630394348874688e-05\n","Epoch  21 Batch  100 / 228  Training Loss  3.807355460594408e-05\n","Epoch  21 Batch  101 / 228  Training Loss  3.103189010289498e-05\n","Epoch  21 Batch  102 / 228  Training Loss  2.3728089217911474e-05\n","Epoch  21 Batch  103 / 228  Training Loss  3.965124051319435e-05\n","Epoch  21 Batch  104 / 228  Training Loss  4.211429404676892e-05\n","Epoch  21 Batch  105 / 228  Training Loss  1.8955366613226943e-05\n","Epoch  21 Batch  106 / 228  Training Loss  3.0801100365351886e-05\n","Epoch  21 Batch  107 / 228  Training Loss  3.541296609910205e-05\n","Epoch  21 Batch  108 / 228  Training Loss  1.5218900443869643e-05\n","Epoch  21 Batch  109 / 228  Training Loss  3.2994332286762074e-05\n","Epoch  21 Batch  110 / 228  Training Loss  2.5586427000234835e-05\n","Epoch  21 Batch  111 / 228  Training Loss  3.162410212098621e-05\n","Epoch  21 Batch  112 / 228  Training Loss  2.5609751901356503e-05\n","Epoch  21 Batch  113 / 228  Training Loss  4.329699731897563e-05\n","Epoch  21 Batch  114 / 228  Training Loss  2.5874736820696853e-05\n","Epoch  21 Batch  115 / 228  Training Loss  4.186575461062603e-05\n","Epoch  21 Batch  116 / 228  Training Loss  2.794365536828991e-05\n","Epoch  21 Batch  117 / 228  Training Loss  3.561596531653777e-05\n","Epoch  21 Batch  118 / 228  Training Loss  2.38024858845165e-05\n","Epoch  21 Batch  119 / 228  Training Loss  3.1571067665936425e-05\n","Epoch  21 Batch  120 / 228  Training Loss  4.077648191014305e-05\n","Epoch  21 Batch  121 / 228  Training Loss  3.2571126212133095e-05\n","Epoch  21 Batch  122 / 228  Training Loss  3.3550717489561066e-05\n","Epoch  21 Batch  123 / 228  Training Loss  4.357834404800087e-05\n","Epoch  21 Batch  124 / 228  Training Loss  3.103434937656857e-05\n","Epoch  21 Batch  125 / 228  Training Loss  2.5037687009898946e-05\n","Epoch  21 Batch  126 / 228  Training Loss  3.321305484860204e-05\n","Epoch  21 Batch  127 / 228  Training Loss  3.466167618171312e-05\n","Epoch  21 Batch  128 / 228  Training Loss  2.6265010092174634e-05\n","Epoch  21 Batch  129 / 228  Training Loss  4.3250267481198534e-05\n","Epoch  21 Batch  130 / 228  Training Loss  2.470299841661472e-05\n","Epoch  21 Batch  131 / 228  Training Loss  2.9162516511860304e-05\n","Epoch  21 Batch  132 / 228  Training Loss  3.3868498576339334e-05\n","Epoch  21 Batch  133 / 228  Training Loss  3.4838758438127115e-05\n","Epoch  21 Batch  134 / 228  Training Loss  4.7042383812367916e-05\n","Epoch  21 Batch  135 / 228  Training Loss  5.066000812803395e-05\n","Epoch  21 Batch  136 / 228  Training Loss  3.8272708479780704e-05\n","Epoch  21 Batch  137 / 228  Training Loss  3.8686343032168224e-05\n","Epoch  21 Batch  138 / 228  Training Loss  2.9991442715981975e-05\n","Epoch  21 Batch  139 / 228  Training Loss  2.416938696114812e-05\n","Epoch  21 Batch  140 / 228  Training Loss  3.709036172949709e-05\n","Epoch  21 Batch  141 / 228  Training Loss  4.1095601773122326e-05\n","Epoch  21 Batch  142 / 228  Training Loss  3.843315425910987e-05\n","Epoch  21 Batch  143 / 228  Training Loss  2.728700746956747e-05\n","Epoch  21 Batch  144 / 228  Training Loss  3.971751357312314e-05\n","Epoch  21 Batch  145 / 228  Training Loss  3.419148924876936e-05\n","Epoch  21 Batch  146 / 228  Training Loss  4.5199281885288656e-05\n","Epoch  21 Batch  147 / 228  Training Loss  2.1279931388562545e-05\n","Epoch  21 Batch  148 / 228  Training Loss  3.071593528147787e-05\n","Epoch  21 Batch  149 / 228  Training Loss  2.7713012968888506e-05\n","Epoch  21 Batch  150 / 228  Training Loss  3.124471550108865e-05\n","Epoch  21 Batch  151 / 228  Training Loss  3.184608794981614e-05\n","Epoch  21 Batch  152 / 228  Training Loss  2.5964936867239885e-05\n","Epoch  21 Batch  153 / 228  Training Loss  2.9008357159909792e-05\n","Epoch  21 Batch  154 / 228  Training Loss  3.2161355193238705e-05\n","Epoch  21 Batch  155 / 228  Training Loss  3.9207279769470915e-05\n","Epoch  21 Batch  156 / 228  Training Loss  2.330459392396733e-05\n","Epoch  21 Batch  157 / 228  Training Loss  3.5897097404813394e-05\n","Epoch  21 Batch  158 / 228  Training Loss  2.3956494260346517e-05\n","Epoch  21 Batch  159 / 228  Training Loss  3.18225211231038e-05\n","Epoch  21 Batch  160 / 228  Training Loss  3.5562090488383546e-05\n","Epoch  21 Batch  161 / 228  Training Loss  2.634705015225336e-05\n","Epoch  21 Batch  162 / 228  Training Loss  2.6289630113751628e-05\n","Epoch  21 Batch  163 / 228  Training Loss  2.891895746870432e-05\n","Epoch  21 Batch  164 / 228  Training Loss  3.203952655894682e-05\n","Epoch  21 Batch  165 / 228  Training Loss  3.7519352190429345e-05\n","Epoch  21 Batch  166 / 228  Training Loss  3.1591283914167434e-05\n","Epoch  21 Batch  167 / 228  Training Loss  3.7651047023246065e-05\n","Epoch  21 Batch  168 / 228  Training Loss  3.1768748158356175e-05\n","Epoch  21 Batch  169 / 228  Training Loss  2.4647497411933728e-05\n","Epoch  21 Batch  170 / 228  Training Loss  2.4938177375588566e-05\n","Epoch  21 Batch  171 / 228  Training Loss  3.449809082667343e-05\n","Epoch  21 Batch  172 / 228  Training Loss  2.5002767870319076e-05\n","Epoch  21 Batch  173 / 228  Training Loss  2.401072924840264e-05\n","Epoch  21 Batch  174 / 228  Training Loss  2.9035758416284807e-05\n","Epoch  21 Batch  175 / 228  Training Loss  2.9297418222995475e-05\n","Epoch  21 Batch  176 / 228  Training Loss  2.420493729005102e-05\n","Epoch  21 Batch  177 / 228  Training Loss  3.437690975260921e-05\n","Epoch  21 Batch  178 / 228  Training Loss  2.612855314509943e-05\n","Epoch  21 Batch  179 / 228  Training Loss  4.0656872442923486e-05\n","Epoch  21 Batch  180 / 228  Training Loss  4.409217945067212e-05\n","Epoch  21 Batch  181 / 228  Training Loss  2.674779898370616e-05\n","Epoch  21 Batch  182 / 228  Training Loss  3.306448707007803e-05\n","Epoch  21 Batch  183 / 228  Training Loss  3.747527807718143e-05\n","Epoch  21 Batch  184 / 228  Training Loss  2.2041163902031258e-05\n","Epoch  21 Batch  185 / 228  Training Loss  3.5831588320434093e-05\n","Epoch  21 Batch  186 / 228  Training Loss  2.9219658244983293e-05\n","Epoch  21 Batch  187 / 228  Training Loss  2.191639578086324e-05\n","Epoch  21 Batch  188 / 228  Training Loss  3.0568495276384056e-05\n","Epoch  21 Batch  189 / 228  Training Loss  3.390882920939475e-05\n","Epoch  21 Batch  190 / 228  Training Loss  4.0042596083367243e-05\n","Epoch  21 Batch  191 / 228  Training Loss  3.392528378753923e-05\n","Epoch  21 Batch  192 / 228  Training Loss  1.806266482162755e-05\n","Epoch  21 Batch  193 / 228  Training Loss  3.175208257744089e-05\n","Epoch  21 Batch  194 / 228  Training Loss  2.350094473513309e-05\n","Epoch  21 Batch  195 / 228  Training Loss  2.4919099814724177e-05\n","Epoch  21 Batch  196 / 228  Training Loss  2.2614598492509685e-05\n","Epoch  21 Batch  197 / 228  Training Loss  3.7999809137545526e-05\n","Epoch  21 Batch  198 / 228  Training Loss  4.879674452240579e-05\n","Epoch  21 Batch  199 / 228  Training Loss  2.4750714146648534e-05\n","Epoch  21 Batch  200 / 228  Training Loss  2.351059993088711e-05\n","Epoch  21 Batch  201 / 228  Training Loss  3.066321005462669e-05\n","Epoch  21 Batch  202 / 228  Training Loss  3.2370091503253207e-05\n","Epoch  21 Batch  203 / 228  Training Loss  3.68865039490629e-05\n","Epoch  21 Batch  204 / 228  Training Loss  2.7512885935720988e-05\n","Epoch  21 Batch  205 / 228  Training Loss  3.873595051118173e-05\n","Epoch  21 Batch  206 / 228  Training Loss  3.0297409466584213e-05\n","Epoch  21 Batch  207 / 228  Training Loss  1.7598958947928622e-05\n","Epoch  21 Batch  208 / 228  Training Loss  5.234943455434404e-05\n","Epoch  21 Batch  209 / 228  Training Loss  2.8408328944351524e-05\n","Epoch  21 Batch  210 / 228  Training Loss  3.478174039628357e-05\n","Epoch  21 Batch  211 / 228  Training Loss  2.5100505808950402e-05\n","Epoch  21 Batch  212 / 228  Training Loss  2.628942092997022e-05\n","Epoch  21 Batch  213 / 228  Training Loss  3.178458064212464e-05\n","Epoch  21 Batch  214 / 228  Training Loss  3.7476820580195636e-05\n","Epoch  21 Batch  215 / 228  Training Loss  2.4919980205595493e-05\n","Epoch  21 Batch  216 / 228  Training Loss  3.928578007617034e-05\n","Epoch  21 Batch  217 / 228  Training Loss  3.3633616112638265e-05\n","Epoch  21 Batch  218 / 228  Training Loss  3.086479409830645e-05\n","Epoch  21 Batch  219 / 228  Training Loss  2.632136420288589e-05\n","Epoch  21 Batch  220 / 228  Training Loss  3.428795753279701e-05\n","Epoch  21 Batch  221 / 228  Training Loss  3.712882607942447e-05\n","Epoch  21 Batch  222 / 228  Training Loss  3.770586772589013e-05\n","Epoch  21 Batch  223 / 228  Training Loss  2.5875900973915122e-05\n","Epoch  21 Batch  224 / 228  Training Loss  1.9518098270054907e-05\n","Epoch  21 Batch  225 / 228  Training Loss  3.1879189918981865e-05\n","Epoch  21 Batch  226 / 228  Training Loss  3.284218109911308e-05\n","Epoch  21 Batch  227 / 228  Training Loss  2.4507024136255495e-05\n","  22    |    -    |   0.000032   | 90.929878\n","----------------------------------------------------------------------\n","Running epoch: 22\n","Epoch  22 Batch  0 / 228  Training Loss  4.1996965592261404e-05\n","Epoch  22 Batch  1 / 228  Training Loss  3.381697024451569e-05\n","Epoch  22 Batch  2 / 228  Training Loss  2.9340515538933687e-05\n","Epoch  22 Batch  3 / 228  Training Loss  2.3498747395933606e-05\n","Epoch  22 Batch  4 / 228  Training Loss  3.137468593195081e-05\n","Epoch  22 Batch  5 / 228  Training Loss  3.3905573218362406e-05\n","Epoch  22 Batch  6 / 228  Training Loss  2.9663282475667074e-05\n","Epoch  22 Batch  7 / 228  Training Loss  3.0146165954647586e-05\n","Epoch  22 Batch  8 / 228  Training Loss  2.923415922850836e-05\n","Epoch  22 Batch  9 / 228  Training Loss  3.495311102597043e-05\n","Epoch  22 Batch  10 / 228  Training Loss  2.7504060199134983e-05\n","Epoch  22 Batch  11 / 228  Training Loss  3.338400711072609e-05\n","Epoch  22 Batch  12 / 228  Training Loss  2.4320817828993313e-05\n","Epoch  22 Batch  13 / 228  Training Loss  2.3524109565187246e-05\n","Epoch  22 Batch  14 / 228  Training Loss  3.0072254958213307e-05\n","Epoch  22 Batch  15 / 228  Training Loss  2.8652350010816008e-05\n","Epoch  22 Batch  16 / 228  Training Loss  2.3321230401052162e-05\n","Epoch  22 Batch  17 / 228  Training Loss  3.1615101761417463e-05\n","Epoch  22 Batch  18 / 228  Training Loss  2.8331869543762878e-05\n","Epoch  22 Batch  19 / 228  Training Loss  3.881608790834434e-05\n","Epoch  22 Batch  20 / 228  Training Loss  3.119480243185535e-05\n","Epoch  22 Batch  21 / 228  Training Loss  4.4863540097139776e-05\n","Epoch  22 Batch  22 / 228  Training Loss  2.5293875296483748e-05\n","Epoch  22 Batch  23 / 228  Training Loss  3.705740164150484e-05\n","Epoch  22 Batch  24 / 228  Training Loss  4.4599339162232354e-05\n","Epoch  22 Batch  25 / 228  Training Loss  3.403627488296479e-05\n","Epoch  22 Batch  26 / 228  Training Loss  2.599397521407809e-05\n","Epoch  22 Batch  27 / 228  Training Loss  3.923311669495888e-05\n","Epoch  22 Batch  28 / 228  Training Loss  3.849362110486254e-05\n","Epoch  22 Batch  29 / 228  Training Loss  1.6496765965712257e-05\n","Epoch  22 Batch  30 / 228  Training Loss  1.956623418664094e-05\n","Epoch  22 Batch  31 / 228  Training Loss  2.3137206881074235e-05\n","Epoch  22 Batch  32 / 228  Training Loss  2.5419540179427713e-05\n","Epoch  22 Batch  33 / 228  Training Loss  2.9934932172182016e-05\n","Epoch  22 Batch  34 / 228  Training Loss  2.6405750759295188e-05\n","Epoch  22 Batch  35 / 228  Training Loss  2.73555069725262e-05\n","Epoch  22 Batch  36 / 228  Training Loss  3.745550930034369e-05\n","Epoch  22 Batch  37 / 228  Training Loss  3.337517409818247e-05\n","Epoch  22 Batch  38 / 228  Training Loss  3.731494871317409e-05\n","Epoch  22 Batch  39 / 228  Training Loss  5.39111097168643e-05\n","Epoch  22 Batch  40 / 228  Training Loss  3.140266926493496e-05\n","Epoch  22 Batch  41 / 228  Training Loss  2.207086254202295e-05\n","Epoch  22 Batch  42 / 228  Training Loss  2.0809600755455904e-05\n","Epoch  22 Batch  43 / 228  Training Loss  3.588579784263857e-05\n","Epoch  22 Batch  44 / 228  Training Loss  3.35859258484561e-05\n","Epoch  22 Batch  45 / 228  Training Loss  2.7101490559289232e-05\n","Epoch  22 Batch  46 / 228  Training Loss  3.350869883433916e-05\n","Epoch  22 Batch  47 / 228  Training Loss  4.2084913729922846e-05\n","Epoch  22 Batch  48 / 228  Training Loss  2.8725720767397434e-05\n","Epoch  22 Batch  49 / 228  Training Loss  2.6345473088440485e-05\n","Epoch  22 Batch  50 / 228  Training Loss  4.311187149141915e-05\n","Epoch  22 Batch  51 / 228  Training Loss  2.1738307623309083e-05\n","Epoch  22 Batch  52 / 228  Training Loss  3.773981734411791e-05\n","Epoch  22 Batch  53 / 228  Training Loss  2.1159510652069002e-05\n","Epoch  22 Batch  54 / 228  Training Loss  2.13101229746826e-05\n","Epoch  22 Batch  55 / 228  Training Loss  4.378408266347833e-05\n","Epoch  22 Batch  56 / 228  Training Loss  3.609001942095347e-05\n","Epoch  22 Batch  57 / 228  Training Loss  2.9810404157615267e-05\n","Epoch  22 Batch  58 / 228  Training Loss  3.1062627385836095e-05\n","Epoch  22 Batch  59 / 228  Training Loss  3.0333279937622137e-05\n","Epoch  22 Batch  60 / 228  Training Loss  1.9193124899175018e-05\n","Epoch  22 Batch  61 / 228  Training Loss  3.057302819797769e-05\n","Epoch  22 Batch  62 / 228  Training Loss  2.9602184440591373e-05\n","Epoch  22 Batch  63 / 228  Training Loss  2.4194237994379364e-05\n","Epoch  22 Batch  64 / 228  Training Loss  2.365148793614935e-05\n","Epoch  22 Batch  65 / 228  Training Loss  1.841911034716759e-05\n","Epoch  22 Batch  66 / 228  Training Loss  1.718513340165373e-05\n","Epoch  22 Batch  67 / 228  Training Loss  3.669735087896697e-05\n","Epoch  22 Batch  68 / 228  Training Loss  2.3656715711695142e-05\n","Epoch  22 Batch  69 / 228  Training Loss  2.0957964807166718e-05\n","Epoch  22 Batch  70 / 228  Training Loss  3.528500747052021e-05\n","Epoch  22 Batch  71 / 228  Training Loss  2.445506652293261e-05\n","Epoch  22 Batch  72 / 228  Training Loss  2.2295052986009978e-05\n","Epoch  22 Batch  73 / 228  Training Loss  2.822480018949136e-05\n","Epoch  22 Batch  74 / 228  Training Loss  4.15930880990345e-05\n","Epoch  22 Batch  75 / 228  Training Loss  2.4269642381113954e-05\n","Epoch  22 Batch  76 / 228  Training Loss  2.553488411649596e-05\n","Epoch  22 Batch  77 / 228  Training Loss  2.8041029509040527e-05\n","Epoch  22 Batch  78 / 228  Training Loss  2.9745155188720673e-05\n","Epoch  22 Batch  79 / 228  Training Loss  2.433566260151565e-05\n","Epoch  22 Batch  80 / 228  Training Loss  2.9306062060641125e-05\n","Epoch  22 Batch  81 / 228  Training Loss  3.177356120431796e-05\n","Epoch  22 Batch  82 / 228  Training Loss  2.0149573174421676e-05\n","Epoch  22 Batch  83 / 228  Training Loss  2.094717274303548e-05\n","Epoch  22 Batch  84 / 228  Training Loss  2.7365225832909346e-05\n","Epoch  22 Batch  85 / 228  Training Loss  3.125053262920119e-05\n","Epoch  22 Batch  86 / 228  Training Loss  2.8109052436775528e-05\n","Epoch  22 Batch  87 / 228  Training Loss  4.761107265949249e-05\n","Epoch  22 Batch  88 / 228  Training Loss  2.5784795070649125e-05\n","Epoch  22 Batch  89 / 228  Training Loss  2.575884536781814e-05\n","Epoch  22 Batch  90 / 228  Training Loss  2.465601937728934e-05\n","Epoch  22 Batch  91 / 228  Training Loss  2.825274168571923e-05\n","Epoch  22 Batch  92 / 228  Training Loss  4.2641939216991886e-05\n","Epoch  22 Batch  93 / 228  Training Loss  2.897787999245338e-05\n","Epoch  22 Batch  94 / 228  Training Loss  3.415781247895211e-05\n","Epoch  22 Batch  95 / 228  Training Loss  4.381435428513214e-05\n","Epoch  22 Batch  96 / 228  Training Loss  2.4419952751486562e-05\n","Epoch  22 Batch  97 / 228  Training Loss  3.429786374908872e-05\n","Epoch  22 Batch  98 / 228  Training Loss  2.0422559828148223e-05\n","Epoch  22 Batch  99 / 228  Training Loss  2.5022318368428387e-05\n","Epoch  22 Batch  100 / 228  Training Loss  2.795344335027039e-05\n","Epoch  22 Batch  101 / 228  Training Loss  2.7575193598750047e-05\n","Epoch  22 Batch  102 / 228  Training Loss  2.166882586607244e-05\n","Epoch  22 Batch  103 / 228  Training Loss  3.609259510994889e-05\n","Epoch  22 Batch  104 / 228  Training Loss  2.8893957278341986e-05\n","Epoch  22 Batch  105 / 228  Training Loss  2.6462377718416974e-05\n","Epoch  22 Batch  106 / 228  Training Loss  3.640386785264127e-05\n","Epoch  22 Batch  107 / 228  Training Loss  1.9200380847905762e-05\n","Epoch  22 Batch  108 / 228  Training Loss  2.655800744832959e-05\n","Epoch  22 Batch  109 / 228  Training Loss  3.5751214454649016e-05\n","Epoch  22 Batch  110 / 228  Training Loss  2.5242525225621648e-05\n","Epoch  22 Batch  111 / 228  Training Loss  2.570734613982495e-05\n","Epoch  22 Batch  112 / 228  Training Loss  3.170763375237584e-05\n","Epoch  22 Batch  113 / 228  Training Loss  1.7993363144341856e-05\n","Epoch  22 Batch  114 / 228  Training Loss  2.0724934074678458e-05\n","Epoch  22 Batch  115 / 228  Training Loss  2.0488598238443956e-05\n","Epoch  22 Batch  116 / 228  Training Loss  3.0143894036882557e-05\n","Epoch  22 Batch  117 / 228  Training Loss  3.167293107253499e-05\n","Epoch  22 Batch  118 / 228  Training Loss  3.102070695604198e-05\n","Epoch  22 Batch  119 / 228  Training Loss  3.0017201424925588e-05\n","Epoch  22 Batch  120 / 228  Training Loss  2.0266654246370308e-05\n","Epoch  22 Batch  121 / 228  Training Loss  2.1775556888314895e-05\n","Epoch  22 Batch  122 / 228  Training Loss  3.992741767433472e-05\n","Epoch  22 Batch  123 / 228  Training Loss  2.130947723344434e-05\n","Epoch  22 Batch  124 / 228  Training Loss  2.0302013581385836e-05\n","Epoch  22 Batch  125 / 228  Training Loss  3.7590540159726515e-05\n","Epoch  22 Batch  126 / 228  Training Loss  3.663583629531786e-05\n","Epoch  22 Batch  127 / 228  Training Loss  2.521077658457216e-05\n","Epoch  22 Batch  128 / 228  Training Loss  2.771213257801719e-05\n","Epoch  22 Batch  129 / 228  Training Loss  3.098813976976089e-05\n","Epoch  22 Batch  130 / 228  Training Loss  3.116967127425596e-05\n","Epoch  22 Batch  131 / 228  Training Loss  2.6210796931991354e-05\n","Epoch  22 Batch  132 / 228  Training Loss  2.3953547497512773e-05\n","Epoch  22 Batch  133 / 228  Training Loss  2.963853330584243e-05\n","Epoch  22 Batch  134 / 228  Training Loss  2.7541746021597646e-05\n","Epoch  22 Batch  135 / 228  Training Loss  2.0800665879505686e-05\n","Epoch  22 Batch  136 / 228  Training Loss  2.6450183213455603e-05\n","Epoch  22 Batch  137 / 228  Training Loss  2.594762008811813e-05\n","Epoch  22 Batch  138 / 228  Training Loss  3.096092041232623e-05\n","Epoch  22 Batch  139 / 228  Training Loss  2.9500261007342488e-05\n","Epoch  22 Batch  140 / 228  Training Loss  5.430215605883859e-05\n","Epoch  22 Batch  141 / 228  Training Loss  2.2203057596925646e-05\n","Epoch  22 Batch  142 / 228  Training Loss  2.5317305698990822e-05\n","Epoch  22 Batch  143 / 228  Training Loss  3.1543961085844785e-05\n","Epoch  22 Batch  144 / 228  Training Loss  1.4862415810057428e-05\n","Epoch  22 Batch  145 / 228  Training Loss  2.8781185392290354e-05\n","Epoch  22 Batch  146 / 228  Training Loss  2.2324298697640188e-05\n","Epoch  22 Batch  147 / 228  Training Loss  2.4057233531493694e-05\n","Epoch  22 Batch  148 / 228  Training Loss  1.993242221942637e-05\n","Epoch  22 Batch  149 / 228  Training Loss  3.984426803071983e-05\n","Epoch  22 Batch  150 / 228  Training Loss  2.9326736694201827e-05\n","Epoch  22 Batch  151 / 228  Training Loss  3.0347031497512944e-05\n","Epoch  22 Batch  152 / 228  Training Loss  2.1139052478247322e-05\n","Epoch  22 Batch  153 / 228  Training Loss  2.9538081435021013e-05\n","Epoch  22 Batch  154 / 228  Training Loss  2.6468236683285795e-05\n","Epoch  22 Batch  155 / 228  Training Loss  3.32414529111702e-05\n","Epoch  22 Batch  156 / 228  Training Loss  3.331214975332841e-05\n","Epoch  22 Batch  157 / 228  Training Loss  3.177165490342304e-05\n","Epoch  22 Batch  158 / 228  Training Loss  2.3889033400337212e-05\n","Epoch  22 Batch  159 / 228  Training Loss  2.9831882784492336e-05\n","Epoch  22 Batch  160 / 228  Training Loss  1.8389420802122913e-05\n","Epoch  22 Batch  161 / 228  Training Loss  2.8098958864575252e-05\n","Epoch  22 Batch  162 / 228  Training Loss  4.001155684818514e-05\n","Epoch  22 Batch  163 / 228  Training Loss  3.566686427802779e-05\n","Epoch  22 Batch  164 / 228  Training Loss  1.579968011355959e-05\n","Epoch  22 Batch  165 / 228  Training Loss  2.5693643692648038e-05\n","Epoch  22 Batch  166 / 228  Training Loss  2.385492916801013e-05\n","Epoch  22 Batch  167 / 228  Training Loss  2.648950248840265e-05\n","Epoch  22 Batch  168 / 228  Training Loss  4.0383776649832726e-05\n","Epoch  22 Batch  169 / 228  Training Loss  3.404124436201528e-05\n","Epoch  22 Batch  170 / 228  Training Loss  2.4853965442162007e-05\n","Epoch  22 Batch  171 / 228  Training Loss  1.9500816051731817e-05\n","Epoch  22 Batch  172 / 228  Training Loss  1.8312786778551526e-05\n","Epoch  22 Batch  173 / 228  Training Loss  2.6069190425914712e-05\n","Epoch  22 Batch  174 / 228  Training Loss  2.8743670554831624e-05\n","Epoch  22 Batch  175 / 228  Training Loss  1.930394137161784e-05\n","Epoch  22 Batch  176 / 228  Training Loss  1.848429747042246e-05\n","Epoch  22 Batch  177 / 228  Training Loss  3.209183705621399e-05\n","Epoch  22 Batch  178 / 228  Training Loss  2.1553951228270307e-05\n","Epoch  22 Batch  179 / 228  Training Loss  2.9204511520219967e-05\n","Epoch  22 Batch  180 / 228  Training Loss  3.4977823816007e-05\n","Epoch  22 Batch  181 / 228  Training Loss  3.3137097489088774e-05\n","Epoch  22 Batch  182 / 228  Training Loss  4.055834870086983e-05\n","Epoch  22 Batch  183 / 228  Training Loss  1.905892167997081e-05\n","Epoch  22 Batch  184 / 228  Training Loss  2.4467773982905783e-05\n","Epoch  22 Batch  185 / 228  Training Loss  2.6828587579075247e-05\n","Epoch  22 Batch  186 / 228  Training Loss  2.2190626623341814e-05\n","Epoch  22 Batch  187 / 228  Training Loss  3.497885700198822e-05\n","Epoch  22 Batch  188 / 228  Training Loss  2.4846587621141225e-05\n","Epoch  22 Batch  189 / 228  Training Loss  3.153885336359963e-05\n","Epoch  22 Batch  190 / 228  Training Loss  2.2307651306618936e-05\n","Epoch  22 Batch  191 / 228  Training Loss  3.6313522286945954e-05\n","Epoch  22 Batch  192 / 228  Training Loss  2.8705913791782223e-05\n","Epoch  22 Batch  193 / 228  Training Loss  2.9214492315077223e-05\n","Epoch  22 Batch  194 / 228  Training Loss  2.72052002401324e-05\n","Epoch  22 Batch  195 / 228  Training Loss  1.4358704902406316e-05\n","Epoch  22 Batch  196 / 228  Training Loss  2.280474654980935e-05\n","Epoch  22 Batch  197 / 228  Training Loss  3.5599456168711185e-05\n","Epoch  22 Batch  198 / 228  Training Loss  2.3777220121701248e-05\n","Epoch  22 Batch  199 / 228  Training Loss  2.281229171785526e-05\n","Epoch  22 Batch  200 / 228  Training Loss  4.555956184049137e-05\n","Epoch  22 Batch  201 / 228  Training Loss  2.2175407139002346e-05\n","Epoch  22 Batch  202 / 228  Training Loss  2.869645322789438e-05\n","Epoch  22 Batch  203 / 228  Training Loss  2.8202579414937645e-05\n","Epoch  22 Batch  204 / 228  Training Loss  3.116445441264659e-05\n","Epoch  22 Batch  205 / 228  Training Loss  2.2656513465335593e-05\n","Epoch  22 Batch  206 / 228  Training Loss  2.54985843639588e-05\n","Epoch  22 Batch  207 / 228  Training Loss  4.0335755329579115e-05\n","Epoch  22 Batch  208 / 228  Training Loss  2.6804604203789495e-05\n","Epoch  22 Batch  209 / 228  Training Loss  4.123242979403585e-05\n","Epoch  22 Batch  210 / 228  Training Loss  2.7164529456058517e-05\n","Epoch  22 Batch  211 / 228  Training Loss  2.6520843675825745e-05\n","Epoch  22 Batch  212 / 228  Training Loss  2.3366728783003055e-05\n","Epoch  22 Batch  213 / 228  Training Loss  1.943572897289414e-05\n","Epoch  22 Batch  214 / 228  Training Loss  2.95092959277099e-05\n","Epoch  22 Batch  215 / 228  Training Loss  3.460555672063492e-05\n","Epoch  22 Batch  216 / 228  Training Loss  2.9723263651249e-05\n","Epoch  22 Batch  217 / 228  Training Loss  2.7949919967795722e-05\n","Epoch  22 Batch  218 / 228  Training Loss  2.3724704078631476e-05\n","Epoch  22 Batch  219 / 228  Training Loss  4.5709111873293296e-05\n","Epoch  22 Batch  220 / 228  Training Loss  2.6875834009842947e-05\n","Epoch  22 Batch  221 / 228  Training Loss  2.473947824910283e-05\n","Epoch  22 Batch  222 / 228  Training Loss  2.7998687073704787e-05\n","Epoch  22 Batch  223 / 228  Training Loss  3.133878271910362e-05\n","Epoch  22 Batch  224 / 228  Training Loss  2.5939321858459152e-05\n","Epoch  22 Batch  225 / 228  Training Loss  3.240489604650065e-05\n","Epoch  22 Batch  226 / 228  Training Loss  3.537035809131339e-05\n","Epoch  22 Batch  227 / 228  Training Loss  2.4654915250721388e-05\n","  23    |    -    |   0.000029   | 91.044207\n","----------------------------------------------------------------------\n","Running epoch: 23\n","Epoch  23 Batch  0 / 228  Training Loss  2.2811227609054185e-05\n","Epoch  23 Batch  1 / 228  Training Loss  5.17790685989894e-05\n","Epoch  23 Batch  2 / 228  Training Loss  2.9765122235403396e-05\n","Epoch  23 Batch  3 / 228  Training Loss  4.1860213968902826e-05\n","Epoch  23 Batch  4 / 228  Training Loss  3.3718049962772056e-05\n","Epoch  23 Batch  5 / 228  Training Loss  2.1020778149249963e-05\n","Epoch  23 Batch  6 / 228  Training Loss  2.600614243419841e-05\n","Epoch  23 Batch  7 / 228  Training Loss  2.3853188395150937e-05\n","Epoch  23 Batch  8 / 228  Training Loss  3.6695859307656065e-05\n","Epoch  23 Batch  9 / 228  Training Loss  2.0120558474445716e-05\n","Epoch  23 Batch  10 / 228  Training Loss  2.0160285203019157e-05\n","Epoch  23 Batch  11 / 228  Training Loss  3.2743726478656754e-05\n","Epoch  23 Batch  12 / 228  Training Loss  2.516630593163427e-05\n","Epoch  23 Batch  13 / 228  Training Loss  1.9253406208008528e-05\n","Epoch  23 Batch  14 / 228  Training Loss  2.109041997755412e-05\n","Epoch  23 Batch  15 / 228  Training Loss  3.3718166378093883e-05\n","Epoch  23 Batch  16 / 228  Training Loss  1.8992404875461943e-05\n","Epoch  23 Batch  17 / 228  Training Loss  3.206776091246866e-05\n","Epoch  23 Batch  18 / 228  Training Loss  3.4809600037988275e-05\n","Epoch  23 Batch  19 / 228  Training Loss  3.326258229208179e-05\n","Epoch  23 Batch  20 / 228  Training Loss  2.3658050849917345e-05\n","Epoch  23 Batch  21 / 228  Training Loss  2.317476537427865e-05\n","Epoch  23 Batch  22 / 228  Training Loss  2.6362997232354246e-05\n","Epoch  23 Batch  23 / 228  Training Loss  2.367060187680181e-05\n","Epoch  23 Batch  24 / 228  Training Loss  1.539930781291332e-05\n","Epoch  23 Batch  25 / 228  Training Loss  2.2972199076320976e-05\n","Epoch  23 Batch  26 / 228  Training Loss  2.9685363188036717e-05\n","Epoch  23 Batch  27 / 228  Training Loss  2.24257746594958e-05\n","Epoch  23 Batch  28 / 228  Training Loss  2.490641963959206e-05\n","Epoch  23 Batch  29 / 228  Training Loss  1.737460115691647e-05\n","Epoch  23 Batch  30 / 228  Training Loss  2.0161438442301005e-05\n","Epoch  23 Batch  31 / 228  Training Loss  3.4806074836524203e-05\n","Epoch  23 Batch  32 / 228  Training Loss  2.1775660570710897e-05\n","Epoch  23 Batch  33 / 228  Training Loss  4.3094543798360974e-05\n","Epoch  23 Batch  34 / 228  Training Loss  2.291983946633991e-05\n","Epoch  23 Batch  35 / 228  Training Loss  2.220110945927445e-05\n","Epoch  23 Batch  36 / 228  Training Loss  2.298873914696742e-05\n","Epoch  23 Batch  37 / 228  Training Loss  1.542209429317154e-05\n","Epoch  23 Batch  38 / 228  Training Loss  2.413646870991215e-05\n","Epoch  23 Batch  39 / 228  Training Loss  1.5392013665405102e-05\n","Epoch  23 Batch  40 / 228  Training Loss  1.6425306966993958e-05\n","Epoch  23 Batch  41 / 228  Training Loss  2.423929254291579e-05\n","Epoch  23 Batch  42 / 228  Training Loss  2.7648808099911548e-05\n","Epoch  23 Batch  43 / 228  Training Loss  3.084585114265792e-05\n","Epoch  23 Batch  44 / 228  Training Loss  2.66062852460891e-05\n","Epoch  23 Batch  45 / 228  Training Loss  2.7445979867479764e-05\n","Epoch  23 Batch  46 / 228  Training Loss  2.5152401576633565e-05\n","Epoch  23 Batch  47 / 228  Training Loss  3.375637606950477e-05\n","Epoch  23 Batch  48 / 228  Training Loss  2.5110388378379866e-05\n","Epoch  23 Batch  49 / 228  Training Loss  3.287387880845927e-05\n","Epoch  23 Batch  50 / 228  Training Loss  1.792727198335342e-05\n","Epoch  23 Batch  51 / 228  Training Loss  2.999283060489688e-05\n","Epoch  23 Batch  52 / 228  Training Loss  2.29686211241642e-05\n","Epoch  23 Batch  53 / 228  Training Loss  3.120133260381408e-05\n","Epoch  23 Batch  54 / 228  Training Loss  3.0288638299680315e-05\n","Epoch  23 Batch  55 / 228  Training Loss  2.3456435883417726e-05\n","Epoch  23 Batch  56 / 228  Training Loss  2.5143122911686078e-05\n","Epoch  23 Batch  57 / 228  Training Loss  2.4472798031638376e-05\n","Epoch  23 Batch  58 / 228  Training Loss  3.318062226753682e-05\n","Epoch  23 Batch  59 / 228  Training Loss  2.3805914679542184e-05\n","Epoch  23 Batch  60 / 228  Training Loss  2.0826935724471696e-05\n","Epoch  23 Batch  61 / 228  Training Loss  3.063355325139128e-05\n","Epoch  23 Batch  62 / 228  Training Loss  2.250959732919e-05\n","Epoch  23 Batch  63 / 228  Training Loss  2.6060915843117982e-05\n","Epoch  23 Batch  64 / 228  Training Loss  1.7581571228220128e-05\n","Epoch  23 Batch  65 / 228  Training Loss  2.6332079869462177e-05\n","Epoch  23 Batch  66 / 228  Training Loss  2.1317306163837202e-05\n","Epoch  23 Batch  67 / 228  Training Loss  2.818877146637533e-05\n","Epoch  23 Batch  68 / 228  Training Loss  2.5213023036485538e-05\n","Epoch  23 Batch  69 / 228  Training Loss  2.679275166883599e-05\n","Epoch  23 Batch  70 / 228  Training Loss  2.0795559976249933e-05\n","Epoch  23 Batch  71 / 228  Training Loss  2.5927154638338834e-05\n","Epoch  23 Batch  72 / 228  Training Loss  2.18961631617276e-05\n","Epoch  23 Batch  73 / 228  Training Loss  2.2989706849330105e-05\n","Epoch  23 Batch  74 / 228  Training Loss  3.674272375064902e-05\n","Epoch  23 Batch  75 / 228  Training Loss  2.087708344333805e-05\n","Epoch  23 Batch  76 / 228  Training Loss  3.092305632890202e-05\n","Epoch  23 Batch  77 / 228  Training Loss  1.8966593415825628e-05\n","Epoch  23 Batch  78 / 228  Training Loss  2.410814522590954e-05\n","Epoch  23 Batch  79 / 228  Training Loss  1.627715209906455e-05\n","Epoch  23 Batch  80 / 228  Training Loss  1.7875045159598812e-05\n","Epoch  23 Batch  81 / 228  Training Loss  1.511230493633775e-05\n","Epoch  23 Batch  82 / 228  Training Loss  2.642092113092076e-05\n","Epoch  23 Batch  83 / 228  Training Loss  1.3334502000361681e-05\n","Epoch  23 Batch  84 / 228  Training Loss  2.1385942091001198e-05\n","Epoch  23 Batch  85 / 228  Training Loss  2.5837049179244787e-05\n","Epoch  23 Batch  86 / 228  Training Loss  2.7812875487143174e-05\n","Epoch  23 Batch  87 / 228  Training Loss  2.3621232685400173e-05\n","Epoch  23 Batch  88 / 228  Training Loss  2.4179644242394716e-05\n","Epoch  23 Batch  89 / 228  Training Loss  2.6811150746652856e-05\n","Epoch  23 Batch  90 / 228  Training Loss  2.5409508452867158e-05\n","Epoch  23 Batch  91 / 228  Training Loss  2.416625284240581e-05\n","Epoch  23 Batch  92 / 228  Training Loss  3.171731805196032e-05\n","Epoch  23 Batch  93 / 228  Training Loss  2.329796552658081e-05\n","Epoch  23 Batch  94 / 228  Training Loss  3.993095378973521e-05\n","Epoch  23 Batch  95 / 228  Training Loss  2.3468270228477195e-05\n","Epoch  23 Batch  96 / 228  Training Loss  2.1083256797282957e-05\n","Epoch  23 Batch  97 / 228  Training Loss  2.805113763315603e-05\n","Epoch  23 Batch  98 / 228  Training Loss  2.0170600691926666e-05\n","Epoch  23 Batch  99 / 228  Training Loss  2.6054560294141993e-05\n","Epoch  23 Batch  100 / 228  Training Loss  2.810158366628457e-05\n","Epoch  23 Batch  101 / 228  Training Loss  2.5971548893721774e-05\n","Epoch  23 Batch  102 / 228  Training Loss  3.209848000551574e-05\n","Epoch  23 Batch  103 / 228  Training Loss  3.358948742970824e-05\n","Epoch  23 Batch  104 / 228  Training Loss  2.5205214114976116e-05\n","Epoch  23 Batch  105 / 228  Training Loss  2.6710988095146604e-05\n","Epoch  23 Batch  106 / 228  Training Loss  3.439070133026689e-05\n","Epoch  23 Batch  107 / 228  Training Loss  3.546632069628686e-05\n","Epoch  23 Batch  108 / 228  Training Loss  2.8322017897153273e-05\n","Epoch  23 Batch  109 / 228  Training Loss  1.656865242694039e-05\n","Epoch  23 Batch  110 / 228  Training Loss  1.9366509150131606e-05\n","Epoch  23 Batch  111 / 228  Training Loss  2.5431731046410277e-05\n","Epoch  23 Batch  112 / 228  Training Loss  2.794684041873552e-05\n","Epoch  23 Batch  113 / 228  Training Loss  2.554718594183214e-05\n","Epoch  23 Batch  114 / 228  Training Loss  1.859117764979601e-05\n","Epoch  23 Batch  115 / 228  Training Loss  2.226788637926802e-05\n","Epoch  23 Batch  116 / 228  Training Loss  4.578782682074234e-05\n","Epoch  23 Batch  117 / 228  Training Loss  3.057167850784026e-05\n","Epoch  23 Batch  118 / 228  Training Loss  1.7354781448375434e-05\n","Epoch  23 Batch  119 / 228  Training Loss  3.581961209420115e-05\n","Epoch  23 Batch  120 / 228  Training Loss  1.861837699834723e-05\n","Epoch  23 Batch  121 / 228  Training Loss  2.368868445046246e-05\n","Epoch  23 Batch  122 / 228  Training Loss  2.609933289932087e-05\n","Epoch  23 Batch  123 / 228  Training Loss  2.5736726456671022e-05\n","Epoch  23 Batch  124 / 228  Training Loss  1.9600227460614406e-05\n","Epoch  23 Batch  125 / 228  Training Loss  3.9399867091560736e-05\n","Epoch  23 Batch  126 / 228  Training Loss  3.862006269628182e-05\n","Epoch  23 Batch  127 / 228  Training Loss  2.4731220037210733e-05\n","Epoch  23 Batch  128 / 228  Training Loss  2.5670151444501244e-05\n","Epoch  23 Batch  129 / 228  Training Loss  3.7996607716195285e-05\n","Epoch  23 Batch  130 / 228  Training Loss  4.392020127852447e-05\n","Epoch  23 Batch  131 / 228  Training Loss  2.8843520340160467e-05\n","Epoch  23 Batch  132 / 228  Training Loss  2.5297040338045917e-05\n","Epoch  23 Batch  133 / 228  Training Loss  2.4166120056179352e-05\n","Epoch  23 Batch  134 / 228  Training Loss  1.6018879250623286e-05\n","Epoch  23 Batch  135 / 228  Training Loss  2.576700717327185e-05\n","Epoch  23 Batch  136 / 228  Training Loss  2.2540636564372107e-05\n","Epoch  23 Batch  137 / 228  Training Loss  1.910891296574846e-05\n","Epoch  23 Batch  138 / 228  Training Loss  2.0672991013270803e-05\n","Epoch  23 Batch  139 / 228  Training Loss  2.8909322281833738e-05\n","Epoch  23 Batch  140 / 228  Training Loss  2.7282911105430685e-05\n","Epoch  23 Batch  141 / 228  Training Loss  2.5444171114941128e-05\n","Epoch  23 Batch  142 / 228  Training Loss  2.0855959519394673e-05\n","Epoch  23 Batch  143 / 228  Training Loss  2.7843743737321347e-05\n","Epoch  23 Batch  144 / 228  Training Loss  2.322841464774683e-05\n","Epoch  23 Batch  145 / 228  Training Loss  2.844543632818386e-05\n","Epoch  23 Batch  146 / 228  Training Loss  2.243678864033427e-05\n","Epoch  23 Batch  147 / 228  Training Loss  1.745222289173398e-05\n","Epoch  23 Batch  148 / 228  Training Loss  2.2998903659754433e-05\n","Epoch  23 Batch  149 / 228  Training Loss  2.436659997329116e-05\n","Epoch  23 Batch  150 / 228  Training Loss  2.6486921342439018e-05\n","Epoch  23 Batch  151 / 228  Training Loss  2.6655421606847085e-05\n","Epoch  23 Batch  152 / 228  Training Loss  2.913329444709234e-05\n","Epoch  23 Batch  153 / 228  Training Loss  2.2471658667200245e-05\n","Epoch  23 Batch  154 / 228  Training Loss  2.031313670158852e-05\n","Epoch  23 Batch  155 / 228  Training Loss  2.9236220143502578e-05\n","Epoch  23 Batch  156 / 228  Training Loss  2.8172918973723426e-05\n","Epoch  23 Batch  157 / 228  Training Loss  4.135409471928142e-05\n","Epoch  23 Batch  158 / 228  Training Loss  2.3534148567705415e-05\n","Epoch  23 Batch  159 / 228  Training Loss  3.0306906410260126e-05\n","Epoch  23 Batch  160 / 228  Training Loss  1.4724762877449393e-05\n","Epoch  23 Batch  161 / 228  Training Loss  2.126574509020429e-05\n","Epoch  23 Batch  162 / 228  Training Loss  1.9481971321511082e-05\n","Epoch  23 Batch  163 / 228  Training Loss  2.628025868034456e-05\n","Epoch  23 Batch  164 / 228  Training Loss  2.058597056020517e-05\n","Epoch  23 Batch  165 / 228  Training Loss  3.739409658010118e-05\n","Epoch  23 Batch  166 / 228  Training Loss  2.795889668050222e-05\n","Epoch  23 Batch  167 / 228  Training Loss  4.046981848659925e-05\n","Epoch  23 Batch  168 / 228  Training Loss  2.017055157921277e-05\n","Epoch  23 Batch  169 / 228  Training Loss  2.650480564625468e-05\n","Epoch  23 Batch  170 / 228  Training Loss  2.6843616069527343e-05\n","Epoch  23 Batch  171 / 228  Training Loss  2.3359716578852385e-05\n","Epoch  23 Batch  172 / 228  Training Loss  2.329112248844467e-05\n","Epoch  23 Batch  173 / 228  Training Loss  3.090500831604004e-05\n","Epoch  23 Batch  174 / 228  Training Loss  3.718878360814415e-05\n","Epoch  23 Batch  175 / 228  Training Loss  1.9322407752042636e-05\n","Epoch  23 Batch  176 / 228  Training Loss  2.814891740854364e-05\n","Epoch  23 Batch  177 / 228  Training Loss  3.5255296097602695e-05\n","Epoch  23 Batch  178 / 228  Training Loss  2.623165892146062e-05\n","Epoch  23 Batch  179 / 228  Training Loss  2.314565790584311e-05\n","Epoch  23 Batch  180 / 228  Training Loss  3.475162520771846e-05\n","Epoch  23 Batch  181 / 228  Training Loss  1.549453372717835e-05\n","Epoch  23 Batch  182 / 228  Training Loss  3.0102382879704237e-05\n","Epoch  23 Batch  183 / 228  Training Loss  2.1115647541591898e-05\n","Epoch  23 Batch  184 / 228  Training Loss  2.5613082470954396e-05\n","Epoch  23 Batch  185 / 228  Training Loss  2.1667172404704615e-05\n","Epoch  23 Batch  186 / 228  Training Loss  2.961329482786823e-05\n","Epoch  23 Batch  187 / 228  Training Loss  2.481691808497999e-05\n","Epoch  23 Batch  188 / 228  Training Loss  2.6424782845424488e-05\n","Epoch  23 Batch  189 / 228  Training Loss  3.0265066015999764e-05\n","Epoch  23 Batch  190 / 228  Training Loss  2.567323281255085e-05\n","Epoch  23 Batch  191 / 228  Training Loss  2.405334817012772e-05\n","Epoch  23 Batch  192 / 228  Training Loss  2.2798083591624163e-05\n","Epoch  23 Batch  193 / 228  Training Loss  2.682256672414951e-05\n","Epoch  23 Batch  194 / 228  Training Loss  2.8433783882064745e-05\n","Epoch  23 Batch  195 / 228  Training Loss  3.041009586013388e-05\n","Epoch  23 Batch  196 / 228  Training Loss  2.892169686674606e-05\n","Epoch  23 Batch  197 / 228  Training Loss  1.6518461052328348e-05\n","Epoch  23 Batch  198 / 228  Training Loss  2.1971471142023802e-05\n","Epoch  23 Batch  199 / 228  Training Loss  4.32925189670641e-05\n","Epoch  23 Batch  200 / 228  Training Loss  3.908188227796927e-05\n","Epoch  23 Batch  201 / 228  Training Loss  3.152771751047112e-05\n","Epoch  23 Batch  202 / 228  Training Loss  2.309161027369555e-05\n","Epoch  23 Batch  203 / 228  Training Loss  2.288344330736436e-05\n","Epoch  23 Batch  204 / 228  Training Loss  2.741756361501757e-05\n","Epoch  23 Batch  205 / 228  Training Loss  2.695037801458966e-05\n","Epoch  23 Batch  206 / 228  Training Loss  1.9560880900826305e-05\n","Epoch  23 Batch  207 / 228  Training Loss  3.997001476818696e-05\n","Epoch  23 Batch  208 / 228  Training Loss  3.105566429439932e-05\n","Epoch  23 Batch  209 / 228  Training Loss  2.578425846877508e-05\n","Epoch  23 Batch  210 / 228  Training Loss  2.314060475328006e-05\n","Epoch  23 Batch  211 / 228  Training Loss  2.4521112209185958e-05\n","Epoch  23 Batch  212 / 228  Training Loss  2.247498014185112e-05\n","Epoch  23 Batch  213 / 228  Training Loss  2.4098098947433755e-05\n","Epoch  23 Batch  214 / 228  Training Loss  2.38466545852134e-05\n","Epoch  23 Batch  215 / 228  Training Loss  2.4905253667384386e-05\n","Epoch  23 Batch  216 / 228  Training Loss  2.5406485292478465e-05\n","Epoch  23 Batch  217 / 228  Training Loss  2.7703481464413926e-05\n","Epoch  23 Batch  218 / 228  Training Loss  2.8584521714947186e-05\n","Epoch  23 Batch  219 / 228  Training Loss  2.9744925996055827e-05\n","Epoch  23 Batch  220 / 228  Training Loss  3.1371586374007165e-05\n","Epoch  23 Batch  221 / 228  Training Loss  3.009655119967647e-05\n","Epoch  23 Batch  222 / 228  Training Loss  1.635672560951207e-05\n","Epoch  23 Batch  223 / 228  Training Loss  2.4137152649927884e-05\n","Epoch  23 Batch  224 / 228  Training Loss  2.2193506083567627e-05\n","Epoch  23 Batch  225 / 228  Training Loss  3.399571869522333e-05\n","Epoch  23 Batch  226 / 228  Training Loss  1.7670767192612402e-05\n","Epoch  23 Batch  227 / 228  Training Loss  1.9361568774911575e-05\n","  24    |    -    |   0.000026   | 90.929878\n","----------------------------------------------------------------------\n","Running epoch: 24\n","Epoch  24 Batch  0 / 228  Training Loss  2.3360915292869322e-05\n","Epoch  24 Batch  1 / 228  Training Loss  3.0242021239246242e-05\n","Epoch  24 Batch  2 / 228  Training Loss  3.255040792282671e-05\n","Epoch  24 Batch  3 / 228  Training Loss  2.8023232516716234e-05\n","Epoch  24 Batch  4 / 228  Training Loss  2.4053571905824356e-05\n","Epoch  24 Batch  5 / 228  Training Loss  2.4382776246056892e-05\n","Epoch  24 Batch  6 / 228  Training Loss  1.8967057258123532e-05\n","Epoch  24 Batch  7 / 228  Training Loss  2.948327710328158e-05\n","Epoch  24 Batch  8 / 228  Training Loss  2.386772212048527e-05\n","Epoch  24 Batch  9 / 228  Training Loss  1.7823105736169964e-05\n","Epoch  24 Batch  10 / 228  Training Loss  3.001232835231349e-05\n","Epoch  24 Batch  11 / 228  Training Loss  1.9833805708913133e-05\n","Epoch  24 Batch  12 / 228  Training Loss  2.8035596187692136e-05\n","Epoch  24 Batch  13 / 228  Training Loss  2.2949849153519608e-05\n","Epoch  24 Batch  14 / 228  Training Loss  1.7879967344924808e-05\n","Epoch  24 Batch  15 / 228  Training Loss  2.0251176465535536e-05\n","Epoch  24 Batch  16 / 228  Training Loss  1.7983209545491263e-05\n","Epoch  24 Batch  17 / 228  Training Loss  3.055986962863244e-05\n","Epoch  24 Batch  18 / 228  Training Loss  2.416577081021387e-05\n","Epoch  24 Batch  19 / 228  Training Loss  1.5496962078032084e-05\n","Epoch  24 Batch  20 / 228  Training Loss  2.6863603125093505e-05\n","Epoch  24 Batch  21 / 228  Training Loss  2.7688163754646666e-05\n","Epoch  24 Batch  22 / 228  Training Loss  4.245533637003973e-05\n","Epoch  24 Batch  23 / 228  Training Loss  1.6578605936956592e-05\n","Epoch  24 Batch  24 / 228  Training Loss  2.365180080232676e-05\n","Epoch  24 Batch  25 / 228  Training Loss  2.6384823286207393e-05\n","Epoch  24 Batch  26 / 228  Training Loss  2.151730950572528e-05\n","Epoch  24 Batch  27 / 228  Training Loss  2.8546892281156033e-05\n","Epoch  24 Batch  28 / 228  Training Loss  3.4262608096469194e-05\n","Epoch  24 Batch  29 / 228  Training Loss  2.116898940585088e-05\n","Epoch  24 Batch  30 / 228  Training Loss  2.3175107344286516e-05\n","Epoch  24 Batch  31 / 228  Training Loss  3.072047184105031e-05\n","Epoch  24 Batch  32 / 228  Training Loss  1.7261725588468835e-05\n","Epoch  24 Batch  33 / 228  Training Loss  2.9344992071855813e-05\n","Epoch  24 Batch  34 / 228  Training Loss  2.3307422452489845e-05\n","Epoch  24 Batch  35 / 228  Training Loss  3.055821071029641e-05\n","Epoch  24 Batch  36 / 228  Training Loss  2.8370070140226744e-05\n","Epoch  24 Batch  37 / 228  Training Loss  2.1761987227364443e-05\n","Epoch  24 Batch  38 / 228  Training Loss  2.8270098482607864e-05\n","Epoch  24 Batch  39 / 228  Training Loss  2.113239861500915e-05\n","Epoch  24 Batch  40 / 228  Training Loss  1.771973074937705e-05\n","Epoch  24 Batch  41 / 228  Training Loss  1.843785503297113e-05\n","Epoch  24 Batch  42 / 228  Training Loss  2.9927236028015614e-05\n","Epoch  24 Batch  43 / 228  Training Loss  2.6246118068229407e-05\n","Epoch  24 Batch  44 / 228  Training Loss  3.300954995211214e-05\n","Epoch  24 Batch  45 / 228  Training Loss  2.4357226720894687e-05\n","Epoch  24 Batch  46 / 228  Training Loss  1.7959468095796183e-05\n","Epoch  24 Batch  47 / 228  Training Loss  2.4023263904382475e-05\n","Epoch  24 Batch  48 / 228  Training Loss  2.4961476810858585e-05\n","Epoch  24 Batch  49 / 228  Training Loss  2.5080613340833224e-05\n","Epoch  24 Batch  50 / 228  Training Loss  2.179713010264095e-05\n","Epoch  24 Batch  51 / 228  Training Loss  2.509489240765106e-05\n","Epoch  24 Batch  52 / 228  Training Loss  2.0935607608407736e-05\n","Epoch  24 Batch  53 / 228  Training Loss  2.2804319087299518e-05\n","Epoch  24 Batch  54 / 228  Training Loss  2.2540154532180168e-05\n","Epoch  24 Batch  55 / 228  Training Loss  2.366483568039257e-05\n","Epoch  24 Batch  56 / 228  Training Loss  1.6244159269263037e-05\n","Epoch  24 Batch  57 / 228  Training Loss  2.6726176656666212e-05\n","Epoch  24 Batch  58 / 228  Training Loss  2.4624463549116626e-05\n","Epoch  24 Batch  59 / 228  Training Loss  2.4302951715071686e-05\n","Epoch  24 Batch  60 / 228  Training Loss  1.7153828594018705e-05\n","Epoch  24 Batch  61 / 228  Training Loss  2.207377656304743e-05\n","Epoch  24 Batch  62 / 228  Training Loss  2.158336246793624e-05\n","Epoch  24 Batch  63 / 228  Training Loss  2.254812716273591e-05\n","Epoch  24 Batch  64 / 228  Training Loss  1.889376108010765e-05\n","Epoch  24 Batch  65 / 228  Training Loss  3.4454005799489096e-05\n","Epoch  24 Batch  66 / 228  Training Loss  2.433337431284599e-05\n","Epoch  24 Batch  67 / 228  Training Loss  2.939884325314779e-05\n","Epoch  24 Batch  68 / 228  Training Loss  2.583762579888571e-05\n","Epoch  24 Batch  69 / 228  Training Loss  2.5284705770900473e-05\n","Epoch  24 Batch  70 / 228  Training Loss  1.8613874999573454e-05\n","Epoch  24 Batch  71 / 228  Training Loss  1.814557435864117e-05\n","Epoch  24 Batch  72 / 228  Training Loss  1.6611296814517118e-05\n","Epoch  24 Batch  73 / 228  Training Loss  2.1331892639864236e-05\n","Epoch  24 Batch  74 / 228  Training Loss  2.580864202172961e-05\n","Epoch  24 Batch  75 / 228  Training Loss  2.2009537133271806e-05\n","Epoch  24 Batch  76 / 228  Training Loss  2.5579394787200727e-05\n","Epoch  24 Batch  77 / 228  Training Loss  2.02882984012831e-05\n","Epoch  24 Batch  78 / 228  Training Loss  1.7715086869429797e-05\n","Epoch  24 Batch  79 / 228  Training Loss  3.473908509477042e-05\n","Epoch  24 Batch  80 / 228  Training Loss  1.9326977053424343e-05\n","Epoch  24 Batch  81 / 228  Training Loss  2.287065399286803e-05\n","Epoch  24 Batch  82 / 228  Training Loss  2.1727888452005573e-05\n","Epoch  24 Batch  83 / 228  Training Loss  2.1523888790397905e-05\n","Epoch  24 Batch  84 / 228  Training Loss  1.65732235473115e-05\n","Epoch  24 Batch  85 / 228  Training Loss  3.3771088055800647e-05\n","Epoch  24 Batch  86 / 228  Training Loss  2.5717639800859615e-05\n","Epoch  24 Batch  87 / 228  Training Loss  1.8966024072142318e-05\n","Epoch  24 Batch  88 / 228  Training Loss  2.141859476978425e-05\n","Epoch  24 Batch  89 / 228  Training Loss  2.1854993974557146e-05\n","Epoch  24 Batch  90 / 228  Training Loss  2.5554782041581348e-05\n","Epoch  24 Batch  91 / 228  Training Loss  2.8536069294204935e-05\n","Epoch  24 Batch  92 / 228  Training Loss  1.9566919945646077e-05\n","Epoch  24 Batch  93 / 228  Training Loss  1.6370595403714105e-05\n","Epoch  24 Batch  94 / 228  Training Loss  2.1714122340199538e-05\n","Epoch  24 Batch  95 / 228  Training Loss  2.072722782031633e-05\n","Epoch  24 Batch  96 / 228  Training Loss  2.5157069103443064e-05\n","Epoch  24 Batch  97 / 228  Training Loss  3.0650619009975344e-05\n","Epoch  24 Batch  98 / 228  Training Loss  1.904572309285868e-05\n","Epoch  24 Batch  99 / 228  Training Loss  1.6684236470609903e-05\n","Epoch  24 Batch  100 / 228  Training Loss  1.7815438695834018e-05\n","Epoch  24 Batch  101 / 228  Training Loss  2.0885338017251343e-05\n","Epoch  24 Batch  102 / 228  Training Loss  2.442062395857647e-05\n","Epoch  24 Batch  103 / 228  Training Loss  2.1405936422524974e-05\n","Epoch  24 Batch  104 / 228  Training Loss  2.1263684175210074e-05\n","Epoch  24 Batch  105 / 228  Training Loss  2.7231981221120805e-05\n","Epoch  24 Batch  106 / 228  Training Loss  1.841849552874919e-05\n","Epoch  24 Batch  107 / 228  Training Loss  2.3445894839824177e-05\n","Epoch  24 Batch  108 / 228  Training Loss  2.586054506537039e-05\n","Epoch  24 Batch  109 / 228  Training Loss  2.0804722225875594e-05\n","Epoch  24 Batch  110 / 228  Training Loss  2.0607412807294168e-05\n","Epoch  24 Batch  111 / 228  Training Loss  2.4644732548040338e-05\n","Epoch  24 Batch  112 / 228  Training Loss  2.3663556930841878e-05\n","Epoch  24 Batch  113 / 228  Training Loss  1.9644216081360355e-05\n","Epoch  24 Batch  114 / 228  Training Loss  1.686874747974798e-05\n","Epoch  24 Batch  115 / 228  Training Loss  2.470650724717416e-05\n","Epoch  24 Batch  116 / 228  Training Loss  2.178171052946709e-05\n","Epoch  24 Batch  117 / 228  Training Loss  2.675363612070214e-05\n","Epoch  24 Batch  118 / 228  Training Loss  2.7259718990535475e-05\n","Epoch  24 Batch  119 / 228  Training Loss  3.0119279472273774e-05\n","Epoch  24 Batch  120 / 228  Training Loss  3.5703182220458984e-05\n","Epoch  24 Batch  121 / 228  Training Loss  4.2269646655768156e-05\n","Epoch  24 Batch  122 / 228  Training Loss  2.2852824258734472e-05\n","Epoch  24 Batch  123 / 228  Training Loss  1.7500849935458973e-05\n","Epoch  24 Batch  124 / 228  Training Loss  2.1551944882958196e-05\n","Epoch  24 Batch  125 / 228  Training Loss  3.5350316466065124e-05\n","Epoch  24 Batch  126 / 228  Training Loss  3.178646511514671e-05\n","Epoch  24 Batch  127 / 228  Training Loss  2.9633345548063517e-05\n","Epoch  24 Batch  128 / 228  Training Loss  2.8633259717025794e-05\n","Epoch  24 Batch  129 / 228  Training Loss  1.97234439838212e-05\n","Epoch  24 Batch  130 / 228  Training Loss  2.0327443053247407e-05\n","Epoch  24 Batch  131 / 228  Training Loss  1.9900528059224598e-05\n","Epoch  24 Batch  132 / 228  Training Loss  2.5611881937948056e-05\n","Epoch  24 Batch  133 / 228  Training Loss  2.059638427454047e-05\n","Epoch  24 Batch  134 / 228  Training Loss  2.2246154912863858e-05\n","Epoch  24 Batch  135 / 228  Training Loss  2.613231845316477e-05\n","Epoch  24 Batch  136 / 228  Training Loss  2.399532968411222e-05\n","Epoch  24 Batch  137 / 228  Training Loss  1.760775012371596e-05\n","Epoch  24 Batch  138 / 228  Training Loss  1.962399619515054e-05\n","Epoch  24 Batch  139 / 228  Training Loss  3.3060965506592765e-05\n","Epoch  24 Batch  140 / 228  Training Loss  2.0624740500352345e-05\n","Epoch  24 Batch  141 / 228  Training Loss  2.3516338842455298e-05\n","Epoch  24 Batch  142 / 228  Training Loss  2.401872734481003e-05\n","Epoch  24 Batch  143 / 228  Training Loss  1.483300184190739e-05\n","Epoch  24 Batch  144 / 228  Training Loss  3.183976878062822e-05\n","Epoch  24 Batch  145 / 228  Training Loss  1.9394827177166007e-05\n","Epoch  24 Batch  146 / 228  Training Loss  2.244816641905345e-05\n","Epoch  24 Batch  147 / 228  Training Loss  2.5964252927224152e-05\n","Epoch  24 Batch  148 / 228  Training Loss  1.9497547327773646e-05\n","Epoch  24 Batch  149 / 228  Training Loss  1.91979888768401e-05\n","Epoch  24 Batch  150 / 228  Training Loss  1.5932291717035696e-05\n","Epoch  24 Batch  151 / 228  Training Loss  2.4532464522053488e-05\n","Epoch  24 Batch  152 / 228  Training Loss  2.06566255656071e-05\n","Epoch  24 Batch  153 / 228  Training Loss  1.781779428711161e-05\n","Epoch  24 Batch  154 / 228  Training Loss  3.6252138670533895e-05\n","Epoch  24 Batch  155 / 228  Training Loss  2.6961239200318232e-05\n","Epoch  24 Batch  156 / 228  Training Loss  2.4353132175747305e-05\n","Epoch  24 Batch  157 / 228  Training Loss  3.156236925860867e-05\n","Epoch  24 Batch  158 / 228  Training Loss  1.5760819223942235e-05\n","Epoch  24 Batch  159 / 228  Training Loss  2.3924258130136877e-05\n","Epoch  24 Batch  160 / 228  Training Loss  1.9237737433286384e-05\n","Epoch  24 Batch  161 / 228  Training Loss  2.0261713871150278e-05\n","Epoch  24 Batch  162 / 228  Training Loss  3.8769965613028035e-05\n","Epoch  24 Batch  163 / 228  Training Loss  1.6039830370573327e-05\n","Epoch  24 Batch  164 / 228  Training Loss  2.223515730293002e-05\n","Epoch  24 Batch  165 / 228  Training Loss  1.993769546970725e-05\n","Epoch  24 Batch  166 / 228  Training Loss  2.4439259505015798e-05\n","Epoch  24 Batch  167 / 228  Training Loss  2.0081241018488072e-05\n","Epoch  24 Batch  168 / 228  Training Loss  2.2637666916125454e-05\n","Epoch  24 Batch  169 / 228  Training Loss  2.8027690859744325e-05\n","Epoch  24 Batch  170 / 228  Training Loss  1.7677102732704952e-05\n","Epoch  24 Batch  171 / 228  Training Loss  3.116761945420876e-05\n","Epoch  24 Batch  172 / 228  Training Loss  2.3855596737121232e-05\n","Epoch  24 Batch  173 / 228  Training Loss  3.0037708711461164e-05\n","Epoch  24 Batch  174 / 228  Training Loss  2.7190624678041786e-05\n","Epoch  24 Batch  175 / 228  Training Loss  1.9307764887344092e-05\n","Epoch  24 Batch  176 / 228  Training Loss  2.181142735935282e-05\n","Epoch  24 Batch  177 / 228  Training Loss  2.342601328564342e-05\n","Epoch  24 Batch  178 / 228  Training Loss  2.7092955861007795e-05\n","Epoch  24 Batch  179 / 228  Training Loss  2.294781370437704e-05\n","Epoch  24 Batch  180 / 228  Training Loss  2.3127204258344136e-05\n","Epoch  24 Batch  181 / 228  Training Loss  2.3884049369371496e-05\n","Epoch  24 Batch  182 / 228  Training Loss  2.1090147129143588e-05\n","Epoch  24 Batch  183 / 228  Training Loss  2.490837505320087e-05\n","Epoch  24 Batch  184 / 228  Training Loss  2.279606633237563e-05\n","Epoch  24 Batch  185 / 228  Training Loss  2.533366387069691e-05\n","Epoch  24 Batch  186 / 228  Training Loss  2.2724259906681255e-05\n","Epoch  24 Batch  187 / 228  Training Loss  1.9575607439037412e-05\n","Epoch  24 Batch  188 / 228  Training Loss  1.949443503690418e-05\n","Epoch  24 Batch  189 / 228  Training Loss  2.5281791749875993e-05\n","Epoch  24 Batch  190 / 228  Training Loss  2.258488893858157e-05\n","Epoch  24 Batch  191 / 228  Training Loss  2.406622479611542e-05\n","Epoch  24 Batch  192 / 228  Training Loss  3.290263339295052e-05\n","Epoch  24 Batch  193 / 228  Training Loss  2.057550045719836e-05\n","Epoch  24 Batch  194 / 228  Training Loss  1.5959965821821243e-05\n","Epoch  24 Batch  195 / 228  Training Loss  2.6242516469210386e-05\n","Epoch  24 Batch  196 / 228  Training Loss  2.3416667318088003e-05\n","Epoch  24 Batch  197 / 228  Training Loss  1.5187412827799562e-05\n","Epoch  24 Batch  198 / 228  Training Loss  2.982505611726083e-05\n","Epoch  24 Batch  199 / 228  Training Loss  2.0254226910765283e-05\n","Epoch  24 Batch  200 / 228  Training Loss  1.7809048586059362e-05\n","Epoch  24 Batch  201 / 228  Training Loss  2.1224666852504015e-05\n","Epoch  24 Batch  202 / 228  Training Loss  2.6784573492477648e-05\n","Epoch  24 Batch  203 / 228  Training Loss  1.934535612235777e-05\n","Epoch  24 Batch  204 / 228  Training Loss  1.8286809790879488e-05\n","Epoch  24 Batch  205 / 228  Training Loss  2.6016496121883392e-05\n","Epoch  24 Batch  206 / 228  Training Loss  2.7975571356364526e-05\n","Epoch  24 Batch  207 / 228  Training Loss  1.7514572391519323e-05\n","Epoch  24 Batch  208 / 228  Training Loss  2.4530872906325385e-05\n","Epoch  24 Batch  209 / 228  Training Loss  2.7274627427686937e-05\n","Epoch  24 Batch  210 / 228  Training Loss  1.9702059944393113e-05\n","Epoch  24 Batch  211 / 228  Training Loss  2.6789522962644696e-05\n","Epoch  24 Batch  212 / 228  Training Loss  2.2625903511652723e-05\n","Epoch  24 Batch  213 / 228  Training Loss  1.8510605514165945e-05\n","Epoch  24 Batch  214 / 228  Training Loss  1.8247719708597288e-05\n","Epoch  24 Batch  215 / 228  Training Loss  2.1012716388213448e-05\n","Epoch  24 Batch  216 / 228  Training Loss  4.577135041472502e-05\n","Epoch  24 Batch  217 / 228  Training Loss  2.3594728190801106e-05\n","Epoch  24 Batch  218 / 228  Training Loss  2.1265592295094393e-05\n","Epoch  24 Batch  219 / 228  Training Loss  2.1991028916090727e-05\n","Epoch  24 Batch  220 / 228  Training Loss  3.135170481982641e-05\n","Epoch  24 Batch  221 / 228  Training Loss  3.8223872252274305e-05\n","Epoch  24 Batch  222 / 228  Training Loss  2.806662450893782e-05\n","Epoch  24 Batch  223 / 228  Training Loss  2.0700648747151718e-05\n","Epoch  24 Batch  224 / 228  Training Loss  2.5366216505062766e-05\n","Epoch  24 Batch  225 / 228  Training Loss  1.9762639567488804e-05\n","Epoch  24 Batch  226 / 228  Training Loss  2.9737226213910617e-05\n","Epoch  24 Batch  227 / 228  Training Loss  3.578513496904634e-05\n","  25    |    -    |   0.000024   | 90.929878\n","----------------------------------------------------------------------\n","Running epoch: 25\n","Epoch  25 Batch  0 / 228  Training Loss  1.9658353267004713e-05\n","Epoch  25 Batch  1 / 228  Training Loss  2.4723314709262922e-05\n","Epoch  25 Batch  2 / 228  Training Loss  2.1846821255167015e-05\n","Epoch  25 Batch  3 / 228  Training Loss  2.2141617591842078e-05\n","Epoch  25 Batch  4 / 228  Training Loss  2.7428688554209657e-05\n","Epoch  25 Batch  5 / 228  Training Loss  2.99126095342217e-05\n","Epoch  25 Batch  6 / 228  Training Loss  2.0911156752845272e-05\n","Epoch  25 Batch  7 / 228  Training Loss  3.421517612878233e-05\n","Epoch  25 Batch  8 / 228  Training Loss  2.480712100805249e-05\n","Epoch  25 Batch  9 / 228  Training Loss  1.8003669538302347e-05\n","Epoch  25 Batch  10 / 228  Training Loss  2.702005804167129e-05\n","Epoch  25 Batch  11 / 228  Training Loss  2.7125222914037295e-05\n","Epoch  25 Batch  12 / 228  Training Loss  2.597245475044474e-05\n","Epoch  25 Batch  13 / 228  Training Loss  1.6326092008966953e-05\n","Epoch  25 Batch  14 / 228  Training Loss  1.9689912733156234e-05\n","Epoch  25 Batch  15 / 228  Training Loss  3.2565698347752914e-05\n","Epoch  25 Batch  16 / 228  Training Loss  1.7010059309541248e-05\n","Epoch  25 Batch  17 / 228  Training Loss  2.0020408555865288e-05\n","Epoch  25 Batch  18 / 228  Training Loss  2.5642708351369947e-05\n","Epoch  25 Batch  19 / 228  Training Loss  1.8007474864134565e-05\n","Epoch  25 Batch  20 / 228  Training Loss  1.5898602214292623e-05\n","Epoch  25 Batch  21 / 228  Training Loss  2.2084857846493833e-05\n","Epoch  25 Batch  22 / 228  Training Loss  2.097738797601778e-05\n","Epoch  25 Batch  23 / 228  Training Loss  2.539811612223275e-05\n","Epoch  25 Batch  24 / 228  Training Loss  1.7316287994617596e-05\n","Epoch  25 Batch  25 / 228  Training Loss  2.3281114408746362e-05\n","Epoch  25 Batch  26 / 228  Training Loss  2.4094255422824062e-05\n","Epoch  25 Batch  27 / 228  Training Loss  1.6115018297568895e-05\n","Epoch  25 Batch  28 / 228  Training Loss  1.978466571017634e-05\n","Epoch  25 Batch  29 / 228  Training Loss  1.8791974071064033e-05\n","Epoch  25 Batch  30 / 228  Training Loss  2.4062963348114863e-05\n","Epoch  25 Batch  31 / 228  Training Loss  1.5647894542780705e-05\n","Epoch  25 Batch  32 / 228  Training Loss  2.6383850126876496e-05\n","Epoch  25 Batch  33 / 228  Training Loss  2.4596085495431907e-05\n","Epoch  25 Batch  34 / 228  Training Loss  1.973496182472445e-05\n","Epoch  25 Batch  35 / 228  Training Loss  1.776163117028773e-05\n","Epoch  25 Batch  36 / 228  Training Loss  2.1843565264134668e-05\n","Epoch  25 Batch  37 / 228  Training Loss  2.940505146398209e-05\n","Epoch  25 Batch  38 / 228  Training Loss  1.7693611880531535e-05\n","Epoch  25 Batch  39 / 228  Training Loss  2.28121079999255e-05\n","Epoch  25 Batch  40 / 228  Training Loss  2.1084990294184536e-05\n","Epoch  25 Batch  41 / 228  Training Loss  2.750871681200806e-05\n","Epoch  25 Batch  42 / 228  Training Loss  1.8466358596924692e-05\n","Epoch  25 Batch  43 / 228  Training Loss  2.3450196749763563e-05\n","Epoch  25 Batch  44 / 228  Training Loss  2.2895650545251556e-05\n","Epoch  25 Batch  45 / 228  Training Loss  2.1466106773004867e-05\n","Epoch  25 Batch  46 / 228  Training Loss  2.8111688152421266e-05\n","Epoch  25 Batch  47 / 228  Training Loss  2.6806197638507e-05\n","Epoch  25 Batch  48 / 228  Training Loss  2.2143494788906537e-05\n","Epoch  25 Batch  49 / 228  Training Loss  1.769679511198774e-05\n","Epoch  25 Batch  50 / 228  Training Loss  1.804464409360662e-05\n","Epoch  25 Batch  51 / 228  Training Loss  2.4283257516799495e-05\n","Epoch  25 Batch  52 / 228  Training Loss  3.1380455766338855e-05\n","Epoch  25 Batch  53 / 228  Training Loss  2.0410596334841102e-05\n","Epoch  25 Batch  54 / 228  Training Loss  2.742641299846582e-05\n","Epoch  25 Batch  55 / 228  Training Loss  2.146409678971395e-05\n","Epoch  25 Batch  56 / 228  Training Loss  2.6756881197798066e-05\n","Epoch  25 Batch  57 / 228  Training Loss  1.647202043386642e-05\n","Epoch  25 Batch  58 / 228  Training Loss  1.9853612684528343e-05\n","Epoch  25 Batch  59 / 228  Training Loss  2.6261455786880106e-05\n","Epoch  25 Batch  60 / 228  Training Loss  1.8531940440880135e-05\n","Epoch  25 Batch  61 / 228  Training Loss  1.7000207662931643e-05\n","Epoch  25 Batch  62 / 228  Training Loss  2.2413632905227132e-05\n","Epoch  25 Batch  63 / 228  Training Loss  2.1468413251568563e-05\n","Epoch  25 Batch  64 / 228  Training Loss  2.8558461053762585e-05\n","Epoch  25 Batch  65 / 228  Training Loss  2.0589775886037387e-05\n","Epoch  25 Batch  66 / 228  Training Loss  2.4196097001549788e-05\n","Epoch  25 Batch  67 / 228  Training Loss  1.723281093291007e-05\n","Epoch  25 Batch  68 / 228  Training Loss  3.889395156875253e-05\n","Epoch  25 Batch  69 / 228  Training Loss  1.7358679542667232e-05\n","Epoch  25 Batch  70 / 228  Training Loss  2.136693183274474e-05\n","Epoch  25 Batch  71 / 228  Training Loss  1.7654603652772494e-05\n","Epoch  25 Batch  72 / 228  Training Loss  2.9112310585333034e-05\n","Epoch  25 Batch  73 / 228  Training Loss  2.609233524708543e-05\n","Epoch  25 Batch  74 / 228  Training Loss  2.406760904705152e-05\n","Epoch  25 Batch  75 / 228  Training Loss  2.1941061277175322e-05\n","Epoch  25 Batch  76 / 228  Training Loss  2.337351361347828e-05\n","Epoch  25 Batch  77 / 228  Training Loss  2.120236422342714e-05\n","Epoch  25 Batch  78 / 228  Training Loss  1.8042654119199142e-05\n","Epoch  25 Batch  79 / 228  Training Loss  1.9898401660611853e-05\n","Epoch  25 Batch  80 / 228  Training Loss  3.078582449234091e-05\n","Epoch  25 Batch  81 / 228  Training Loss  2.77579983958276e-05\n","Epoch  25 Batch  82 / 228  Training Loss  2.4385184588027187e-05\n","Epoch  25 Batch  83 / 228  Training Loss  2.1181507690926082e-05\n","Epoch  25 Batch  84 / 228  Training Loss  1.954847357410472e-05\n","Epoch  25 Batch  85 / 228  Training Loss  2.870919524866622e-05\n","Epoch  25 Batch  86 / 228  Training Loss  2.5795357942115515e-05\n","Epoch  25 Batch  87 / 228  Training Loss  2.5805842597037554e-05\n","Epoch  25 Batch  88 / 228  Training Loss  2.4349781597265974e-05\n","Epoch  25 Batch  89 / 228  Training Loss  1.30860189528903e-05\n","Epoch  25 Batch  90 / 228  Training Loss  2.9335391445783898e-05\n","Epoch  25 Batch  91 / 228  Training Loss  2.0080677131772973e-05\n","Epoch  25 Batch  92 / 228  Training Loss  3.0543527827830985e-05\n","Epoch  25 Batch  93 / 228  Training Loss  1.850690387072973e-05\n","Epoch  25 Batch  94 / 228  Training Loss  2.51706824201392e-05\n","Epoch  25 Batch  95 / 228  Training Loss  2.5177394491038285e-05\n","Epoch  25 Batch  96 / 228  Training Loss  1.8045417164103128e-05\n","Epoch  25 Batch  97 / 228  Training Loss  2.054257129202597e-05\n","Epoch  25 Batch  98 / 228  Training Loss  2.689344728423748e-05\n","Epoch  25 Batch  99 / 228  Training Loss  2.279947693750728e-05\n","Epoch  25 Batch  100 / 228  Training Loss  1.8238442862639204e-05\n","Epoch  25 Batch  101 / 228  Training Loss  2.5269495381508023e-05\n","Epoch  25 Batch  102 / 228  Training Loss  1.689884447841905e-05\n","Epoch  25 Batch  103 / 228  Training Loss  2.1775404093204997e-05\n","Epoch  25 Batch  104 / 228  Training Loss  2.1904485038248822e-05\n","Epoch  25 Batch  105 / 228  Training Loss  2.3223492462420836e-05\n","Epoch  25 Batch  106 / 228  Training Loss  2.1117662981851026e-05\n","Epoch  25 Batch  107 / 228  Training Loss  1.5952175090205856e-05\n","Epoch  25 Batch  108 / 228  Training Loss  2.2212121621123515e-05\n","Epoch  25 Batch  109 / 228  Training Loss  1.5418783732457086e-05\n","Epoch  25 Batch  110 / 228  Training Loss  2.0599232811946422e-05\n","Epoch  25 Batch  111 / 228  Training Loss  2.5306471798103303e-05\n","Epoch  25 Batch  112 / 228  Training Loss  2.180826777475886e-05\n","Epoch  25 Batch  113 / 228  Training Loss  1.5605335647705942e-05\n","Epoch  25 Batch  114 / 228  Training Loss  2.0838584532612003e-05\n","Epoch  25 Batch  115 / 228  Training Loss  1.9047454770770855e-05\n","Epoch  25 Batch  116 / 228  Training Loss  2.2848007574793883e-05\n","Epoch  25 Batch  117 / 228  Training Loss  2.144932841474656e-05\n","Epoch  25 Batch  118 / 228  Training Loss  1.8286129488842562e-05\n","Epoch  25 Batch  119 / 228  Training Loss  1.9799565052380785e-05\n","Epoch  25 Batch  120 / 228  Training Loss  1.5128005543374456e-05\n","Epoch  25 Batch  121 / 228  Training Loss  2.632274481584318e-05\n","Epoch  25 Batch  122 / 228  Training Loss  1.3174890227674041e-05\n","Epoch  25 Batch  123 / 228  Training Loss  2.7975600460194983e-05\n","Epoch  25 Batch  124 / 228  Training Loss  3.054369881283492e-05\n","Epoch  25 Batch  125 / 228  Training Loss  1.581994365551509e-05\n","Epoch  25 Batch  126 / 228  Training Loss  3.0117067581159063e-05\n","Epoch  25 Batch  127 / 228  Training Loss  2.2243910279939882e-05\n","Epoch  25 Batch  128 / 228  Training Loss  1.5940335288178176e-05\n","Epoch  25 Batch  129 / 228  Training Loss  1.7160178686026484e-05\n","Epoch  25 Batch  130 / 228  Training Loss  2.2593932953896e-05\n","Epoch  25 Batch  131 / 228  Training Loss  2.0005150872748345e-05\n","Epoch  25 Batch  132 / 228  Training Loss  1.4448186448134948e-05\n","Epoch  25 Batch  133 / 228  Training Loss  2.270373261126224e-05\n","Epoch  25 Batch  134 / 228  Training Loss  2.6885370971285738e-05\n","Epoch  25 Batch  135 / 228  Training Loss  3.020403346454259e-05\n","Epoch  25 Batch  136 / 228  Training Loss  1.8999562598764896e-05\n","Epoch  25 Batch  137 / 228  Training Loss  2.5265366275561973e-05\n","Epoch  25 Batch  138 / 228  Training Loss  2.592381315480452e-05\n","Epoch  25 Batch  139 / 228  Training Loss  1.878761759144254e-05\n","Epoch  25 Batch  140 / 228  Training Loss  2.667263470357284e-05\n","Epoch  25 Batch  141 / 228  Training Loss  1.8326389181311242e-05\n","Epoch  25 Batch  142 / 228  Training Loss  1.3174953892303165e-05\n","Epoch  25 Batch  143 / 228  Training Loss  2.4151171601261012e-05\n","Epoch  25 Batch  144 / 228  Training Loss  1.9575702026486397e-05\n","Epoch  25 Batch  145 / 228  Training Loss  2.3644624889129773e-05\n","Epoch  25 Batch  146 / 228  Training Loss  1.5351643014582805e-05\n","Epoch  25 Batch  147 / 228  Training Loss  1.8028173144557513e-05\n","Epoch  25 Batch  148 / 228  Training Loss  1.8601946067065e-05\n","Epoch  25 Batch  149 / 228  Training Loss  1.5098620679054875e-05\n","Epoch  25 Batch  150 / 228  Training Loss  2.2188030925462954e-05\n","Epoch  25 Batch  151 / 228  Training Loss  2.8302813007030636e-05\n","Epoch  25 Batch  152 / 228  Training Loss  2.7520281946635805e-05\n","Epoch  25 Batch  153 / 228  Training Loss  2.6380053895991296e-05\n","Epoch  25 Batch  154 / 228  Training Loss  1.9936936951125972e-05\n","Epoch  25 Batch  155 / 228  Training Loss  1.8522388927522115e-05\n","Epoch  25 Batch  156 / 228  Training Loss  2.0983883587177843e-05\n","Epoch  25 Batch  157 / 228  Training Loss  2.3583545043948106e-05\n","Epoch  25 Batch  158 / 228  Training Loss  1.7667474821791984e-05\n","Epoch  25 Batch  159 / 228  Training Loss  2.3415102987200953e-05\n","Epoch  25 Batch  160 / 228  Training Loss  2.2486890884465538e-05\n","Epoch  25 Batch  161 / 228  Training Loss  1.4787236978008877e-05\n","Epoch  25 Batch  162 / 228  Training Loss  1.3658765965374187e-05\n","Epoch  25 Batch  163 / 228  Training Loss  2.2457039449363947e-05\n","Epoch  25 Batch  164 / 228  Training Loss  4.387658555060625e-05\n","Epoch  25 Batch  165 / 228  Training Loss  2.286060589540284e-05\n","Epoch  25 Batch  166 / 228  Training Loss  1.7754324289853685e-05\n","Epoch  25 Batch  167 / 228  Training Loss  1.8305261619389057e-05\n","Epoch  25 Batch  168 / 228  Training Loss  2.176399175368715e-05\n","Epoch  25 Batch  169 / 228  Training Loss  1.5728011931059882e-05\n","Epoch  25 Batch  170 / 228  Training Loss  2.178952308895532e-05\n","Epoch  25 Batch  171 / 228  Training Loss  1.4934556020307355e-05\n","Epoch  25 Batch  172 / 228  Training Loss  3.058640504605137e-05\n","Epoch  25 Batch  173 / 228  Training Loss  1.8251426809001714e-05\n","Epoch  25 Batch  174 / 228  Training Loss  1.7742735508363694e-05\n","Epoch  25 Batch  175 / 228  Training Loss  1.919775240821764e-05\n","Epoch  25 Batch  176 / 228  Training Loss  2.164092984457966e-05\n","Epoch  25 Batch  177 / 228  Training Loss  2.3082387997419573e-05\n","Epoch  25 Batch  178 / 228  Training Loss  2.7821848561870866e-05\n","Epoch  25 Batch  179 / 228  Training Loss  1.8571810869616456e-05\n","Epoch  25 Batch  180 / 228  Training Loss  1.2196310308354441e-05\n","Epoch  25 Batch  181 / 228  Training Loss  1.699373751762323e-05\n","Epoch  25 Batch  182 / 228  Training Loss  2.5935691155609675e-05\n","Epoch  25 Batch  183 / 228  Training Loss  1.8985081624123268e-05\n","Epoch  25 Batch  184 / 228  Training Loss  2.20729907596251e-05\n","Epoch  25 Batch  185 / 228  Training Loss  2.182569369324483e-05\n","Epoch  25 Batch  186 / 228  Training Loss  1.4957359780964907e-05\n","Epoch  25 Batch  187 / 228  Training Loss  2.3737327865092084e-05\n","Epoch  25 Batch  188 / 228  Training Loss  2.0238869183231145e-05\n","Epoch  25 Batch  189 / 228  Training Loss  1.89928960026009e-05\n","Epoch  25 Batch  190 / 228  Training Loss  1.837751733546611e-05\n","Epoch  25 Batch  191 / 228  Training Loss  1.7388076230417937e-05\n","Epoch  25 Batch  192 / 228  Training Loss  1.8205621017841622e-05\n","Epoch  25 Batch  193 / 228  Training Loss  1.2675664947892074e-05\n","Epoch  25 Batch  194 / 228  Training Loss  1.8051223378279246e-05\n","Epoch  25 Batch  195 / 228  Training Loss  2.057691199297551e-05\n","Epoch  25 Batch  196 / 228  Training Loss  3.0633931601187214e-05\n","Epoch  25 Batch  197 / 228  Training Loss  2.8952323191333562e-05\n","Epoch  25 Batch  198 / 228  Training Loss  1.7112399291363545e-05\n","Epoch  25 Batch  199 / 228  Training Loss  3.0115515983197838e-05\n","Epoch  25 Batch  200 / 228  Training Loss  2.2945005184737965e-05\n","Epoch  25 Batch  201 / 228  Training Loss  2.1777379515697248e-05\n","Epoch  25 Batch  202 / 228  Training Loss  2.754266643023584e-05\n","Epoch  25 Batch  203 / 228  Training Loss  2.2049143808544613e-05\n","Epoch  25 Batch  204 / 228  Training Loss  1.8021035430138e-05\n","Epoch  25 Batch  205 / 228  Training Loss  2.0703426343970932e-05\n","Epoch  25 Batch  206 / 228  Training Loss  2.980365570692811e-05\n","Epoch  25 Batch  207 / 228  Training Loss  2.1208132238825783e-05\n","Epoch  25 Batch  208 / 228  Training Loss  2.1621350242639892e-05\n","Epoch  25 Batch  209 / 228  Training Loss  1.7114323782152496e-05\n","Epoch  25 Batch  210 / 228  Training Loss  1.9456410882412456e-05\n","Epoch  25 Batch  211 / 228  Training Loss  2.024891546170693e-05\n","Epoch  25 Batch  212 / 228  Training Loss  2.7417754608904943e-05\n","Epoch  25 Batch  213 / 228  Training Loss  2.0369614503579214e-05\n","Epoch  25 Batch  214 / 228  Training Loss  1.3251107702672016e-05\n","Epoch  25 Batch  215 / 228  Training Loss  1.490471822762629e-05\n","Epoch  25 Batch  216 / 228  Training Loss  2.3347036403720267e-05\n","Epoch  25 Batch  217 / 228  Training Loss  2.215102176705841e-05\n","Epoch  25 Batch  218 / 228  Training Loss  2.8875756470370106e-05\n","Epoch  25 Batch  219 / 228  Training Loss  2.2887823433848098e-05\n","Epoch  25 Batch  220 / 228  Training Loss  2.3629932911717333e-05\n","Epoch  25 Batch  221 / 228  Training Loss  1.780445563781541e-05\n","Epoch  25 Batch  222 / 228  Training Loss  2.6460396838956513e-05\n","Epoch  25 Batch  223 / 228  Training Loss  1.833584246924147e-05\n","Epoch  25 Batch  224 / 228  Training Loss  3.199312413926236e-05\n","Epoch  25 Batch  225 / 228  Training Loss  2.412015965091996e-05\n","Epoch  25 Batch  226 / 228  Training Loss  1.7818385458667763e-05\n"]}]},{"cell_type":"code","metadata":{"id":"iTiNbM081LlD"},"source":["# Changing the directory to store the model there.\n","# print(os.getcwd())\n","# os.chdir('/content/drive/My Drive/Colab Notebooks/new/')\n","\n","# print(os.getcwd())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c3bq66KD1eg7"},"source":["#### Saving the Model (creating checkpoint)"]},{"cell_type":"code","metadata":{"id":"-fJcvYcG1nd3"},"source":["# PATH = \"fine_tune_10e_20eph.pt\"\n","# torch.save({\n","#             'epoch': num_of_epochs,\n","#             'model_state_dict': model.state_dict(),\n","#             'optimizer_state_dict': optimizer.state_dict(),\n","#             'loss': running_loss,\n","#             }, PATH)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AuyVxTpoGouA","executionInfo":{"status":"ok","timestamp":1638663738880,"user_tz":420,"elapsed":18709,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["model_load = T5ForConditionalGeneration.from_pretrained('FineTune_10e_Model_32eph_v2_10perc_bias.bin', return_dict=True, config='t5-base-config.json')"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8ud531fFI122"},"source":[""]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6bzZYxgxLhMd","executionInfo":{"status":"ok","timestamp":1638663832628,"user_tz":420,"elapsed":12779,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"4a267803-3190-46de-f87a-087c3e59fb23"},"source":["model_load.to(device)"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"elq45fvZvxKR","executionInfo":{"status":"ok","timestamp":1638664115923,"user_tz":420,"elapsed":161965,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"ceecbd62-16cc-4a11-fa90-26ff0a7df57a"},"source":["evaluate(model_load, test_dataloader)"],"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["90.33801020408163"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"lWLx5DywGtHI","executionInfo":{"status":"ok","timestamp":1638666493076,"user_tz":420,"elapsed":493,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["# Function to generate sentences from symptoms on the test dataset\n","def generateText(text):\n","  model_load.eval()\n","  input_ids = tokenizer.encode(text, return_tensors=\"pt\")  # Batch size 1\n","  # input_ids.to(dev)\n","  s = time.time()\n","  outputs = model_load.generate(input_ids)\n","  gen_text=tokenizer.decode(outputs[0]).replace('<pad>','').replace('</s>','')\n","  elapsed = time.time() - s\n","  print('Generated in {} seconds'.format(str(elapsed)[:4]))\n","\n","  return gen_text"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZCgk9LuLFOou","executionInfo":{"status":"ok","timestamp":1638666493349,"user_tz":420,"elapsed":3,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["data_ood = data_ood1.sample(n = 6000, random_state = 42).reset_index(drop=True)"],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"id":"0eDyu9_iFdmZ","executionInfo":{"status":"ok","timestamp":1638666495866,"user_tz":420,"elapsed":237,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}}},"source":["data_ood['type'] = \"OOD\""],"execution_count":65,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ShHp_36yH3do","executionInfo":{"status":"ok","timestamp":1638666497239,"user_tz":420,"elapsed":7,"user":{"displayName":"sai pavan kalyan reddy konda","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"04398557441060290164"}},"outputId":"84e3c23d-c9ea-4df2-9697-68550fc01cc0"},"source":["model_load.to('cpu')"],"execution_count":66,"outputs":[{"output_type":"execute_result","data":{"text/plain":["T5ForConditionalGeneration(\n","  (shared): Embedding(32128, 768)\n","  (encoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (decoder): T5Stack(\n","    (embed_tokens): Embedding(32128, 768)\n","    (block): ModuleList(\n","      (0): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","              (relative_attention_bias): Embedding(32, 12)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (1): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (2): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (3): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (4): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (5): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (6): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (7): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (8): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (9): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (10): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","      (11): T5Block(\n","        (layer): ModuleList(\n","          (0): T5LayerSelfAttention(\n","            (SelfAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (1): T5LayerCrossAttention(\n","            (EncDecAttention): T5Attention(\n","              (q): Linear(in_features=768, out_features=768, bias=False)\n","              (k): Linear(in_features=768, out_features=768, bias=False)\n","              (v): Linear(in_features=768, out_features=768, bias=False)\n","              (o): Linear(in_features=768, out_features=768, bias=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","          (2): T5LayerFF(\n","            (DenseReluDense): T5DenseReluDense(\n","              (wi): Linear(in_features=768, out_features=3072, bias=False)\n","              (wo): Linear(in_features=3072, out_features=768, bias=False)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (layer_norm): T5LayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (final_layer_norm): T5LayerNorm()\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",")"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c5sm0auVFnV1","outputId":"a1dfb66b-cd52-4242-963f-e9de646a8a6c"},"source":["data_test['type'] = 'iid'\n","data_ood['target_str'] = data_ood['target'].astype(str)\n","bigdata = data_test.append(data_ood, ignore_index=True)\n","bigdata['predictions'] = bigdata.apply(lambda x: generateText(x['inputs']), axis=1)\n","bigdata.to_csv('10e_results_10perc_bias.csv', index = False, header=None)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.42 seconds\n","Generated in 1.38 seconds\n","Generated in 1.08 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.37 seconds\n","Generated in 1.51 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.59 seconds\n","Generated in 1.40 seconds\n","Generated in 1.07 seconds\n","Generated in 1.40 seconds\n","Generated in 1.49 seconds\n","Generated in 1.42 seconds\n","Generated in 1.47 seconds\n","Generated in 1.46 seconds\n","Generated in 1.46 seconds\n","Generated in 1.40 seconds\n","Generated in 1.50 seconds\n","Generated in 1.36 seconds\n","Generated in 1.08 seconds\n","Generated in 1.09 seconds\n","Generated in 1.09 seconds\n","Generated in 1.40 seconds\n","Generated in 1.61 seconds\n","Generated in 1.42 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.59 seconds\n","Generated in 1.39 seconds\n","Generated in 1.08 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.61 seconds\n","Generated in 1.08 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.42 seconds\n","Generated in 1.40 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.58 seconds\n","Generated in 1.47 seconds\n","Generated in 1.59 seconds\n","Generated in 1.51 seconds\n","Generated in 1.59 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.58 seconds\n","Generated in 1.37 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.07 seconds\n","Generated in 1.53 seconds\n","Generated in 1.06 seconds\n","Generated in 1.59 seconds\n","Generated in 1.07 seconds\n","Generated in 1.41 seconds\n","Generated in 1.43 seconds\n","Generated in 1.47 seconds\n","Generated in 1.43 seconds\n","Generated in 1.41 seconds\n","Generated in 1.10 seconds\n","Generated in 1.43 seconds\n","Generated in 1.46 seconds\n","Generated in 1.56 seconds\n","Generated in 1.48 seconds\n","Generated in 1.08 seconds\n","Generated in 1.45 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.37 seconds\n","Generated in 1.43 seconds\n","Generated in 1.42 seconds\n","Generated in 1.42 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.47 seconds\n","Generated in 1.55 seconds\n","Generated in 1.10 seconds\n","Generated in 1.57 seconds\n","Generated in 1.45 seconds\n","Generated in 1.40 seconds\n","Generated in 1.20 seconds\n","Generated in 1.47 seconds\n","Generated in 1.56 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.16 seconds\n","Generated in 1.53 seconds\n","Generated in 1.40 seconds\n","Generated in 1.59 seconds\n","Generated in 1.50 seconds\n","Generated in 1.39 seconds\n","Generated in 1.09 seconds\n","Generated in 1.60 seconds\n","Generated in 1.39 seconds\n","Generated in 1.60 seconds\n","Generated in 1.49 seconds\n","Generated in 1.56 seconds\n","Generated in 1.41 seconds\n","Generated in 1.44 seconds\n","Generated in 1.45 seconds\n","Generated in 1.43 seconds\n","Generated in 1.11 seconds\n","Generated in 1.49 seconds\n","Generated in 1.07 seconds\n","Generated in 1.54 seconds\n","Generated in 1.36 seconds\n","Generated in 1.45 seconds\n","Generated in 1.45 seconds\n","Generated in 1.47 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.43 seconds\n","Generated in 1.47 seconds\n","Generated in 1.41 seconds\n","Generated in 1.58 seconds\n","Generated in 1.17 seconds\n","Generated in 1.40 seconds\n","Generated in 1.45 seconds\n","Generated in 1.48 seconds\n","Generated in 1.44 seconds\n","Generated in 1.51 seconds\n","Generated in 1.52 seconds\n","Generated in 1.41 seconds\n","Generated in 1.50 seconds\n","Generated in 1.08 seconds\n","Generated in 1.41 seconds\n","Generated in 1.42 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.41 seconds\n","Generated in 1.43 seconds\n","Generated in 1.51 seconds\n","Generated in 1.59 seconds\n","Generated in 1.42 seconds\n","Generated in 1.11 seconds\n","Generated in 1.11 seconds\n","Generated in 1.61 seconds\n","Generated in 1.46 seconds\n","Generated in 1.43 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.58 seconds\n","Generated in 1.43 seconds\n","Generated in 1.48 seconds\n","Generated in 1.41 seconds\n","Generated in 1.44 seconds\n","Generated in 1.40 seconds\n","Generated in 1.06 seconds\n","Generated in 1.23 seconds\n","Generated in 1.42 seconds\n","Generated in 1.49 seconds\n","Generated in 1.08 seconds\n","Generated in 1.42 seconds\n","Generated in 1.42 seconds\n","Generated in 1.10 seconds\n","Generated in 1.16 seconds\n","Generated in 1.61 seconds\n","Generated in 1.40 seconds\n","Generated in 1.57 seconds\n","Generated in 1.47 seconds\n","Generated in 1.43 seconds\n","Generated in 1.11 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.36 seconds\n","Generated in 1.07 seconds\n","Generated in 1.36 seconds\n","Generated in 1.43 seconds\n","Generated in 1.39 seconds\n","Generated in 1.43 seconds\n","Generated in 1.50 seconds\n","Generated in 1.40 seconds\n","Generated in 1.48 seconds\n","Generated in 1.11 seconds\n","Generated in 1.44 seconds\n","Generated in 1.42 seconds\n","Generated in 1.11 seconds\n","Generated in 1.43 seconds\n","Generated in 1.49 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.61 seconds\n","Generated in 1.42 seconds\n","Generated in 1.41 seconds\n","Generated in 1.43 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.48 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.49 seconds\n","Generated in 1.62 seconds\n","Generated in 1.58 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.08 seconds\n","Generated in 1.42 seconds\n","Generated in 1.43 seconds\n","Generated in 1.40 seconds\n","Generated in 1.42 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.49 seconds\n","Generated in 1.39 seconds\n","Generated in 1.49 seconds\n","Generated in 1.37 seconds\n","Generated in 1.58 seconds\n","Generated in 1.41 seconds\n","Generated in 1.45 seconds\n","Generated in 1.45 seconds\n","Generated in 1.58 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.45 seconds\n","Generated in 1.58 seconds\n","Generated in 1.37 seconds\n","Generated in 1.42 seconds\n","Generated in 1.43 seconds\n","Generated in 1.39 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.40 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.08 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.58 seconds\n","Generated in 1.39 seconds\n","Generated in 1.61 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.54 seconds\n","Generated in 1.46 seconds\n","Generated in 1.47 seconds\n","Generated in 1.40 seconds\n","Generated in 1.09 seconds\n","Generated in 1.38 seconds\n","Generated in 1.10 seconds\n","Generated in 1.50 seconds\n","Generated in 1.49 seconds\n","Generated in 1.60 seconds\n","Generated in 1.43 seconds\n","Generated in 1.48 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.11 seconds\n","Generated in 1.42 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.58 seconds\n","Generated in 1.50 seconds\n","Generated in 1.51 seconds\n","Generated in 1.39 seconds\n","Generated in 1.47 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.36 seconds\n","Generated in 1.58 seconds\n","Generated in 1.49 seconds\n","Generated in 1.40 seconds\n","Generated in 1.15 seconds\n","Generated in 1.18 seconds\n","Generated in 1.55 seconds\n","Generated in 1.60 seconds\n","Generated in 1.16 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.51 seconds\n","Generated in 1.61 seconds\n","Generated in 1.07 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.58 seconds\n","Generated in 1.40 seconds\n","Generated in 1.47 seconds\n","Generated in 1.40 seconds\n","Generated in 1.44 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.08 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.43 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.11 seconds\n","Generated in 1.42 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.57 seconds\n","Generated in 1.07 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.08 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.48 seconds\n","Generated in 1.40 seconds\n","Generated in 1.58 seconds\n","Generated in 1.41 seconds\n","Generated in 1.41 seconds\n","Generated in 1.60 seconds\n","Generated in 1.09 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.57 seconds\n","Generated in 1.36 seconds\n","Generated in 1.49 seconds\n","Generated in 1.55 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.41 seconds\n","Generated in 1.47 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.41 seconds\n","Generated in 1.47 seconds\n","Generated in 1.40 seconds\n","Generated in 1.09 seconds\n","Generated in 1.46 seconds\n","Generated in 1.42 seconds\n","Generated in 1.38 seconds\n","Generated in 1.52 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.49 seconds\n","Generated in 1.40 seconds\n","Generated in 1.43 seconds\n","Generated in 1.50 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.47 seconds\n","Generated in 1.40 seconds\n","Generated in 1.48 seconds\n","Generated in 1.49 seconds\n","Generated in 1.60 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.41 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.46 seconds\n","Generated in 1.41 seconds\n","Generated in 1.36 seconds\n","Generated in 1.53 seconds\n","Generated in 1.44 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.17 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.44 seconds\n","Generated in 1.38 seconds\n","Generated in 1.47 seconds\n","Generated in 1.09 seconds\n","Generated in 1.38 seconds\n","Generated in 1.41 seconds\n","Generated in 1.09 seconds\n","Generated in 1.41 seconds\n","Generated in 1.43 seconds\n","Generated in 1.40 seconds\n","Generated in 1.63 seconds\n","Generated in 1.59 seconds\n","Generated in 1.48 seconds\n","Generated in 1.34 seconds\n","Generated in 1.57 seconds\n","Generated in 1.48 seconds\n","Generated in 1.40 seconds\n","Generated in 1.10 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.59 seconds\n","Generated in 1.39 seconds\n","Generated in 1.47 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.45 seconds\n","Generated in 1.35 seconds\n","Generated in 1.14 seconds\n","Generated in 1.10 seconds\n","Generated in 1.41 seconds\n","Generated in 1.58 seconds\n","Generated in 1.52 seconds\n","Generated in 1.21 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.40 seconds\n","Generated in 1.57 seconds\n","Generated in 1.39 seconds\n","Generated in 1.46 seconds\n","Generated in 1.47 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.09 seconds\n","Generated in 1.48 seconds\n","Generated in 1.08 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.45 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.43 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.49 seconds\n","Generated in 1.09 seconds\n","Generated in 1.41 seconds\n","Generated in 1.49 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.42 seconds\n","Generated in 1.43 seconds\n","Generated in 1.38 seconds\n","Generated in 1.34 seconds\n","Generated in 1.39 seconds\n","Generated in 1.07 seconds\n","Generated in 1.40 seconds\n","Generated in 1.09 seconds\n","Generated in 1.46 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.08 seconds\n","Generated in 1.39 seconds\n","Generated in 1.44 seconds\n","Generated in 1.46 seconds\n","Generated in 1.41 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.52 seconds\n","Generated in 1.38 seconds\n","Generated in 1.60 seconds\n","Generated in 1.55 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.06 seconds\n","Generated in 1.49 seconds\n","Generated in 1.37 seconds\n","Generated in 1.08 seconds\n","Generated in 1.46 seconds\n","Generated in 1.39 seconds\n","Generated in 1.55 seconds\n","Generated in 1.48 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.54 seconds\n","Generated in 1.09 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.35 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.46 seconds\n","Generated in 1.08 seconds\n","Generated in 1.07 seconds\n","Generated in 1.46 seconds\n","Generated in 1.46 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.36 seconds\n","Generated in 1.48 seconds\n","Generated in 1.38 seconds\n","Generated in 1.44 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.47 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.47 seconds\n","Generated in 1.43 seconds\n","Generated in 1.38 seconds\n","Generated in 1.45 seconds\n","Generated in 1.41 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.16 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.44 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.07 seconds\n","Generated in 1.41 seconds\n","Generated in 1.43 seconds\n","Generated in 1.40 seconds\n","Generated in 1.57 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.53 seconds\n","Generated in 1.54 seconds\n","Generated in 1.42 seconds\n","Generated in 1.08 seconds\n","Generated in 1.39 seconds\n","Generated in 1.07 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.60 seconds\n","Generated in 1.55 seconds\n","Generated in 1.48 seconds\n","Generated in 1.34 seconds\n","Generated in 1.13 seconds\n","Generated in 1.47 seconds\n","Generated in 1.08 seconds\n","Generated in 1.42 seconds\n","Generated in 1.44 seconds\n","Generated in 1.08 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.44 seconds\n","Generated in 1.07 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.55 seconds\n","Generated in 1.44 seconds\n","Generated in 1.39 seconds\n","Generated in 1.60 seconds\n","Generated in 1.37 seconds\n","Generated in 1.36 seconds\n","Generated in 1.36 seconds\n","Generated in 1.57 seconds\n","Generated in 1.09 seconds\n","Generated in 1.08 seconds\n","Generated in 1.46 seconds\n","Generated in 1.58 seconds\n","Generated in 1.41 seconds\n","Generated in 1.10 seconds\n","Generated in 1.38 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.45 seconds\n","Generated in 1.36 seconds\n","Generated in 1.37 seconds\n","Generated in 1.16 seconds\n","Generated in 1.44 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.43 seconds\n","Generated in 1.44 seconds\n","Generated in 1.60 seconds\n","Generated in 1.43 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.47 seconds\n","Generated in 1.41 seconds\n","Generated in 1.55 seconds\n","Generated in 1.08 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.47 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.48 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.49 seconds\n","Generated in 1.47 seconds\n","Generated in 1.07 seconds\n","Generated in 1.35 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.43 seconds\n","Generated in 1.46 seconds\n","Generated in 1.37 seconds\n","Generated in 1.08 seconds\n","Generated in 1.39 seconds\n","Generated in 1.50 seconds\n","Generated in 1.39 seconds\n","Generated in 1.57 seconds\n","Generated in 1.60 seconds\n","Generated in 1.36 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.35 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.45 seconds\n","Generated in 1.37 seconds\n","Generated in 1.36 seconds\n","Generated in 1.57 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.47 seconds\n","Generated in 1.36 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.57 seconds\n","Generated in 1.48 seconds\n","Generated in 1.56 seconds\n","Generated in 1.13 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.60 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.52 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.61 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.36 seconds\n","Generated in 1.43 seconds\n","Generated in 1.47 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.09 seconds\n","Generated in 1.38 seconds\n","Generated in 1.50 seconds\n","Generated in 1.46 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.54 seconds\n","Generated in 1.39 seconds\n","Generated in 1.08 seconds\n","Generated in 1.56 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.55 seconds\n","Generated in 1.44 seconds\n","Generated in 1.37 seconds\n","Generated in 1.57 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.17 seconds\n","Generated in 1.41 seconds\n","Generated in 1.33 seconds\n","Generated in 1.56 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.47 seconds\n","Generated in 1.57 seconds\n","Generated in 1.45 seconds\n","Generated in 1.45 seconds\n","Generated in 1.17 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.41 seconds\n","Generated in 1.44 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.36 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.44 seconds\n","Generated in 1.44 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.55 seconds\n","Generated in 1.47 seconds\n","Generated in 1.56 seconds\n","Generated in 1.48 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.57 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.10 seconds\n","Generated in 1.37 seconds\n","Generated in 1.41 seconds\n","Generated in 1.16 seconds\n","Generated in 1.39 seconds\n","Generated in 1.45 seconds\n","Generated in 1.36 seconds\n","Generated in 1.48 seconds\n","Generated in 1.38 seconds\n","Generated in 1.08 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.58 seconds\n","Generated in 1.38 seconds\n","Generated in 1.56 seconds\n","Generated in 1.47 seconds\n","Generated in 1.60 seconds\n","Generated in 1.47 seconds\n","Generated in 1.07 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.57 seconds\n","Generated in 1.35 seconds\n","Generated in 1.38 seconds\n","Generated in 1.57 seconds\n","Generated in 1.47 seconds\n","Generated in 1.40 seconds\n","Generated in 1.48 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.38 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.46 seconds\n","Generated in 1.10 seconds\n","Generated in 1.39 seconds\n","Generated in 1.05 seconds\n","Generated in 1.38 seconds\n","Generated in 1.06 seconds\n","Generated in 1.07 seconds\n","Generated in 1.57 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.48 seconds\n","Generated in 1.06 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.35 seconds\n","Generated in 1.08 seconds\n","Generated in 1.57 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.41 seconds\n","Generated in 1.15 seconds\n","Generated in 1.38 seconds\n","Generated in 1.05 seconds\n","Generated in 1.45 seconds\n","Generated in 1.37 seconds\n","Generated in 1.57 seconds\n","Generated in 1.43 seconds\n","Generated in 1.43 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.10 seconds\n","Generated in 1.38 seconds\n","Generated in 1.44 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.47 seconds\n","Generated in 1.39 seconds\n","Generated in 1.59 seconds\n","Generated in 1.46 seconds\n","Generated in 1.42 seconds\n","Generated in 1.46 seconds\n","Generated in 1.43 seconds\n","Generated in 1.14 seconds\n","Generated in 1.43 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.42 seconds\n","Generated in 1.09 seconds\n","Generated in 1.09 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.57 seconds\n","Generated in 1.38 seconds\n","Generated in 1.47 seconds\n","Generated in 1.50 seconds\n","Generated in 1.58 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.43 seconds\n","Generated in 1.35 seconds\n","Generated in 1.34 seconds\n","Generated in 1.35 seconds\n","Generated in 1.55 seconds\n","Generated in 1.39 seconds\n","Generated in 1.44 seconds\n","Generated in 1.33 seconds\n","Generated in 1.57 seconds\n","Generated in 1.14 seconds\n","Generated in 1.35 seconds\n","Generated in 1.37 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.45 seconds\n","Generated in 1.06 seconds\n","Generated in 1.36 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.40 seconds\n","Generated in 1.45 seconds\n","Generated in 1.37 seconds\n","Generated in 1.36 seconds\n","Generated in 1.06 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.59 seconds\n","Generated in 1.42 seconds\n","Generated in 1.56 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.44 seconds\n","Generated in 1.40 seconds\n","Generated in 1.15 seconds\n","Generated in 1.38 seconds\n","Generated in 1.06 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.08 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.43 seconds\n","Generated in 1.12 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.55 seconds\n","Generated in 1.44 seconds\n","Generated in 1.46 seconds\n","Generated in 1.40 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.08 seconds\n","Generated in 1.39 seconds\n","Generated in 1.16 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.48 seconds\n","Generated in 1.38 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.05 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.45 seconds\n","Generated in 1.36 seconds\n","Generated in 1.07 seconds\n","Generated in 1.37 seconds\n","Generated in 1.44 seconds\n","Generated in 1.51 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.09 seconds\n","Generated in 1.53 seconds\n","Generated in 1.46 seconds\n","Generated in 1.34 seconds\n","Generated in 1.08 seconds\n","Generated in 1.41 seconds\n","Generated in 1.53 seconds\n","Generated in 1.52 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.08 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.43 seconds\n","Generated in 1.37 seconds\n","Generated in 1.53 seconds\n","Generated in 1.44 seconds\n","Generated in 1.39 seconds\n","Generated in 1.07 seconds\n","Generated in 1.47 seconds\n","Generated in 1.40 seconds\n","Generated in 0.77 seconds\n","Generated in 1.61 seconds\n","Generated in 1.47 seconds\n","Generated in 1.57 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.59 seconds\n","Generated in 1.38 seconds\n","Generated in 1.06 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.17 seconds\n","Generated in 1.39 seconds\n","Generated in 1.53 seconds\n","Generated in 1.45 seconds\n","Generated in 1.07 seconds\n","Generated in 1.09 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.45 seconds\n","Generated in 1.47 seconds\n","Generated in 1.13 seconds\n","Generated in 1.40 seconds\n","Generated in 1.54 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.44 seconds\n","Generated in 1.40 seconds\n","Generated in 1.51 seconds\n","Generated in 1.41 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.40 seconds\n","Generated in 1.35 seconds\n","Generated in 1.39 seconds\n","Generated in 1.10 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.45 seconds\n","Generated in 1.50 seconds\n","Generated in 1.05 seconds\n","Generated in 1.39 seconds\n","Generated in 1.57 seconds\n","Generated in 1.17 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.46 seconds\n","Generated in 1.47 seconds\n","Generated in 1.08 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.10 seconds\n","Generated in 1.42 seconds\n","Generated in 1.56 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.44 seconds\n","Generated in 1.35 seconds\n","Generated in 1.39 seconds\n","Generated in 1.09 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.51 seconds\n","Generated in 1.44 seconds\n","Generated in 1.07 seconds\n","Generated in 1.41 seconds\n","Generated in 1.44 seconds\n","Generated in 1.52 seconds\n","Generated in 1.07 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.45 seconds\n","Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.45 seconds\n","Generated in 1.41 seconds\n","Generated in 1.48 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.42 seconds\n","Generated in 1.06 seconds\n","Generated in 1.59 seconds\n","Generated in 1.47 seconds\n","Generated in 1.41 seconds\n","Generated in 1.49 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.42 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.47 seconds\n","Generated in 1.40 seconds\n","Generated in 1.06 seconds\n","Generated in 1.39 seconds\n","Generated in 1.44 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.48 seconds\n","Generated in 1.46 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.39 seconds\n","Generated in 1.60 seconds\n","Generated in 1.39 seconds\n","Generated in 1.55 seconds\n","Generated in 1.59 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.47 seconds\n","Generated in 1.58 seconds\n","Generated in 1.37 seconds\n","Generated in 1.44 seconds\n","Generated in 1.41 seconds\n","Generated in 1.47 seconds\n","Generated in 1.41 seconds\n","Generated in 1.42 seconds\n","Generated in 1.10 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.08 seconds\n","Generated in 1.39 seconds\n","Generated in 1.58 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.08 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.44 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.08 seconds\n","Generated in 1.46 seconds\n","Generated in 1.18 seconds\n","Generated in 1.47 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.08 seconds\n","Generated in 1.08 seconds\n","Generated in 1.47 seconds\n","Generated in 1.47 seconds\n","Generated in 1.38 seconds\n","Generated in 1.43 seconds\n","Generated in 1.46 seconds\n","Generated in 1.39 seconds\n","Generated in 1.57 seconds\n","Generated in 1.39 seconds\n","Generated in 1.49 seconds\n","Generated in 1.37 seconds\n","Generated in 1.09 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.41 seconds\n","Generated in 1.49 seconds\n","Generated in 1.43 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.47 seconds\n","Generated in 1.41 seconds\n","Generated in 1.46 seconds\n","Generated in 1.06 seconds\n","Generated in 1.44 seconds\n","Generated in 1.43 seconds\n","Generated in 1.07 seconds\n","Generated in 1.40 seconds\n","Generated in 1.45 seconds\n","Generated in 1.59 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.58 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.56 seconds\n","Generated in 1.17 seconds\n","Generated in 1.37 seconds\n","Generated in 1.44 seconds\n","Generated in 1.44 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.37 seconds\n","Generated in 1.48 seconds\n","Generated in 1.42 seconds\n","Generated in 1.52 seconds\n","Generated in 1.50 seconds\n","Generated in 1.41 seconds\n","Generated in 1.52 seconds\n","Generated in 1.41 seconds\n","Generated in 1.09 seconds\n","Generated in 1.32 seconds\n","Generated in 1.56 seconds\n","Generated in 1.47 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.46 seconds\n","Generated in 1.42 seconds\n","Generated in 1.50 seconds\n","Generated in 1.42 seconds\n","Generated in 1.48 seconds\n","Generated in 1.46 seconds\n","Generated in 1.37 seconds\n","Generated in 1.48 seconds\n","Generated in 1.46 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.09 seconds\n","Generated in 1.37 seconds\n","Generated in 1.49 seconds\n","Generated in 1.39 seconds\n","Generated in 1.47 seconds\n","Generated in 1.40 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.45 seconds\n","Generated in 1.61 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.59 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.44 seconds\n","Generated in 1.60 seconds\n","Generated in 1.41 seconds\n","Generated in 1.10 seconds\n","Generated in 1.11 seconds\n","Generated in 1.53 seconds\n","Generated in 1.40 seconds\n","Generated in 1.60 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.48 seconds\n","Generated in 1.37 seconds\n","Generated in 1.49 seconds\n","Generated in 1.49 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.58 seconds\n","Generated in 1.38 seconds\n","Generated in 1.45 seconds\n","Generated in 1.51 seconds\n","Generated in 1.53 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.57 seconds\n","Generated in 1.41 seconds\n","Generated in 1.48 seconds\n","Generated in 1.43 seconds\n","Generated in 1.41 seconds\n","Generated in 1.61 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.47 seconds\n","Generated in 1.56 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.06 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.43 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.42 seconds\n","Generated in 1.41 seconds\n","Generated in 1.41 seconds\n","Generated in 1.63 seconds\n","Generated in 1.61 seconds\n","Generated in 1.37 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.08 seconds\n","Generated in 1.38 seconds\n","Generated in 1.07 seconds\n","Generated in 1.39 seconds\n","Generated in 1.45 seconds\n","Generated in 1.43 seconds\n","Generated in 1.41 seconds\n","Generated in 1.46 seconds\n","Generated in 1.41 seconds\n","Generated in 1.08 seconds\n","Generated in 1.40 seconds\n","Generated in 1.08 seconds\n","Generated in 1.38 seconds\n","Generated in 1.08 seconds\n","Generated in 1.38 seconds\n","Generated in 1.41 seconds\n","Generated in 1.48 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.47 seconds\n","Generated in 1.45 seconds\n","Generated in 1.42 seconds\n","Generated in 1.58 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.59 seconds\n","Generated in 1.46 seconds\n","Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.43 seconds\n","Generated in 1.14 seconds\n","Generated in 1.36 seconds\n","Generated in 1.47 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.41 seconds\n","Generated in 1.09 seconds\n","Generated in 1.40 seconds\n","Generated in 1.36 seconds\n","Generated in 1.05 seconds\n","Generated in 1.08 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.44 seconds\n","Generated in 1.36 seconds\n","Generated in 1.58 seconds\n","Generated in 1.50 seconds\n","Generated in 1.47 seconds\n","Generated in 1.58 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.58 seconds\n","Generated in 1.61 seconds\n","Generated in 1.60 seconds\n","Generated in 1.61 seconds\n","Generated in 1.39 seconds\n","Generated in 1.60 seconds\n","Generated in 1.48 seconds\n","Generated in 1.42 seconds\n","Generated in 1.48 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.23 seconds\n","Generated in 1.46 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.48 seconds\n","Generated in 1.48 seconds\n","Generated in 1.57 seconds\n","Generated in 1.36 seconds\n","Generated in 1.48 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.35 seconds\n","Generated in 1.08 seconds\n","Generated in 1.45 seconds\n","Generated in 1.53 seconds\n","Generated in 1.58 seconds\n","Generated in 1.46 seconds\n","Generated in 1.47 seconds\n","Generated in 1.42 seconds\n","Generated in 1.17 seconds\n","Generated in 1.44 seconds\n","Generated in 1.47 seconds\n","Generated in 1.36 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.36 seconds\n","Generated in 1.41 seconds\n","Generated in 1.56 seconds\n","Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.37 seconds\n","Generated in 1.42 seconds\n","Generated in 1.09 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.35 seconds\n","Generated in 1.45 seconds\n","Generated in 1.41 seconds\n","Generated in 1.42 seconds\n","Generated in 1.55 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.08 seconds\n","Generated in 1.08 seconds\n","Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.06 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.47 seconds\n","Generated in 1.09 seconds\n","Generated in 1.46 seconds\n","Generated in 1.08 seconds\n","Generated in 1.08 seconds\n","Generated in 1.38 seconds\n","Generated in 1.35 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.39 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.57 seconds\n","Generated in 1.37 seconds\n","Generated in 1.59 seconds\n","Generated in 1.42 seconds\n","Generated in 1.46 seconds\n","Generated in 1.39 seconds\n","Generated in 1.56 seconds\n","Generated in 1.50 seconds\n","Generated in 1.59 seconds\n","Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.59 seconds\n","Generated in 1.15 seconds\n","Generated in 1.39 seconds\n","Generated in 1.07 seconds\n","Generated in 1.37 seconds\n","Generated in 1.41 seconds\n","Generated in 1.46 seconds\n","Generated in 1.55 seconds\n","Generated in 1.07 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.56 seconds\n","Generated in 1.36 seconds\n","Generated in 1.45 seconds\n","Generated in 1.49 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.43 seconds\n","Generated in 1.42 seconds\n","Generated in 1.44 seconds\n","Generated in 1.42 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.44 seconds\n","Generated in 1.37 seconds\n","Generated in 1.15 seconds\n","Generated in 1.06 seconds\n","Generated in 1.41 seconds\n","Generated in 1.36 seconds\n","Generated in 1.44 seconds\n","Generated in 1.48 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.44 seconds\n","Generated in 1.40 seconds\n","Generated in 1.49 seconds\n","Generated in 1.40 seconds\n","Generated in 1.59 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.42 seconds\n","Generated in 1.47 seconds\n","Generated in 1.40 seconds\n","Generated in 1.08 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.09 seconds\n","Generated in 1.37 seconds\n","Generated in 1.42 seconds\n","Generated in 1.10 seconds\n","Generated in 1.44 seconds\n","Generated in 1.47 seconds\n","Generated in 1.40 seconds\n","Generated in 1.54 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.47 seconds\n","Generated in 1.44 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.16 seconds\n","Generated in 1.42 seconds\n","Generated in 1.48 seconds\n","Generated in 1.09 seconds\n","Generated in 1.45 seconds\n","Generated in 1.06 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.57 seconds\n","Generated in 1.39 seconds\n","Generated in 1.58 seconds\n","Generated in 1.38 seconds\n","Generated in 1.10 seconds\n","Generated in 1.39 seconds\n","Generated in 1.57 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.57 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.08 seconds\n","Generated in 1.40 seconds\n","Generated in 1.10 seconds\n","Generated in 1.47 seconds\n","Generated in 1.45 seconds\n","Generated in 1.07 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.40 seconds\n","Generated in 1.56 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.59 seconds\n","Generated in 1.46 seconds\n","Generated in 1.40 seconds\n","Generated in 1.08 seconds\n","Generated in 1.08 seconds\n","Generated in 1.37 seconds\n","Generated in 1.36 seconds\n","Generated in 1.56 seconds\n","Generated in 1.37 seconds\n","Generated in 1.49 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.36 seconds\n","Generated in 1.35 seconds\n","Generated in 1.42 seconds\n","Generated in 1.08 seconds\n","Generated in 1.53 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.45 seconds\n","Generated in 1.36 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.11 seconds\n","Generated in 1.59 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.56 seconds\n","Generated in 1.39 seconds\n","Generated in 1.60 seconds\n","Generated in 1.43 seconds\n","Generated in 1.38 seconds\n","Generated in 1.58 seconds\n","Generated in 1.45 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.61 seconds\n","Generated in 1.47 seconds\n","Generated in 1.48 seconds\n","Generated in 1.62 seconds\n","Generated in 1.42 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.48 seconds\n","Generated in 1.39 seconds\n","Generated in 1.46 seconds\n","Generated in 1.55 seconds\n","Generated in 1.45 seconds\n","Generated in 1.41 seconds\n","Generated in 1.57 seconds\n","Generated in 1.38 seconds\n","Generated in 1.44 seconds\n","Generated in 1.56 seconds\n","Generated in 1.60 seconds\n","Generated in 1.39 seconds\n","Generated in 1.46 seconds\n","Generated in 1.57 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.06 seconds\n","Generated in 1.53 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.40 seconds\n","Generated in 1.42 seconds\n","Generated in 1.48 seconds\n","Generated in 1.09 seconds\n","Generated in 1.40 seconds\n","Generated in 1.16 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.08 seconds\n","Generated in 1.58 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.35 seconds\n","Generated in 1.10 seconds\n","Generated in 1.57 seconds\n","Generated in 1.41 seconds\n","Generated in 1.45 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.44 seconds\n","Generated in 1.38 seconds\n","Generated in 1.59 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.49 seconds\n","Generated in 1.43 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.37 seconds\n","Generated in 1.53 seconds\n","Generated in 1.09 seconds\n","Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.11 seconds\n","Generated in 1.38 seconds\n","Generated in 1.08 seconds\n","Generated in 1.45 seconds\n","Generated in 1.58 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.42 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.09 seconds\n","Generated in 1.38 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.47 seconds\n","Generated in 1.38 seconds\n","Generated in 1.47 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.41 seconds\n","Generated in 1.36 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.07 seconds\n","Generated in 1.45 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.49 seconds\n","Generated in 1.48 seconds\n","Generated in 1.10 seconds\n","Generated in 1.35 seconds\n","Generated in 1.58 seconds\n","Generated in 1.43 seconds\n","Generated in 1.44 seconds\n","Generated in 1.07 seconds\n","Generated in 1.49 seconds\n","Generated in 1.38 seconds\n","Generated in 1.09 seconds\n","Generated in 1.57 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.56 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.45 seconds\n","Generated in 1.48 seconds\n","Generated in 1.40 seconds\n","Generated in 1.58 seconds\n","Generated in 1.67 seconds\n","Generated in 1.48 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.10 seconds\n","Generated in 1.57 seconds\n","Generated in 1.46 seconds\n","Generated in 1.40 seconds\n","Generated in 1.50 seconds\n","Generated in 1.34 seconds\n","Generated in 1.35 seconds\n","Generated in 1.58 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.06 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.46 seconds\n","Generated in 1.47 seconds\n","Generated in 1.09 seconds\n","Generated in 1.38 seconds\n","Generated in 1.17 seconds\n","Generated in 1.38 seconds\n","Generated in 1.42 seconds\n","Generated in 1.58 seconds\n","Generated in 1.37 seconds\n","Generated in 1.06 seconds\n","Generated in 1.08 seconds\n","Generated in 1.39 seconds\n","Generated in 1.48 seconds\n","Generated in 1.46 seconds\n","Generated in 1.09 seconds\n","Generated in 1.36 seconds\n","Generated in 1.37 seconds\n","Generated in 1.44 seconds\n","Generated in 1.08 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.18 seconds\n","Generated in 1.39 seconds\n","Generated in 1.46 seconds\n","Generated in 1.52 seconds\n","Generated in 1.45 seconds\n","Generated in 1.40 seconds\n","Generated in 1.57 seconds\n","Generated in 1.07 seconds\n","Generated in 1.38 seconds\n","Generated in 1.09 seconds\n","Generated in 1.60 seconds\n","Generated in 1.60 seconds\n","Generated in 1.09 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.43 seconds\n","Generated in 1.08 seconds\n","Generated in 1.41 seconds\n","Generated in 1.58 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.55 seconds\n","Generated in 1.43 seconds\n","Generated in 1.37 seconds\n","Generated in 1.36 seconds\n","Generated in 1.35 seconds\n","Generated in 1.36 seconds\n","Generated in 1.36 seconds\n","Generated in 1.43 seconds\n","Generated in 1.05 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.42 seconds\n","Generated in 1.06 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.06 seconds\n","Generated in 1.07 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.43 seconds\n","Generated in 1.57 seconds\n","Generated in 1.48 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.49 seconds\n","Generated in 1.47 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.14 seconds\n","Generated in 1.43 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.56 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.09 seconds\n","Generated in 1.07 seconds\n","Generated in 1.40 seconds\n","Generated in 1.45 seconds\n","Generated in 1.09 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.58 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.57 seconds\n","Generated in 1.36 seconds\n","Generated in 1.36 seconds\n","Generated in 1.44 seconds\n","Generated in 1.35 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.43 seconds\n","Generated in 1.58 seconds\n","Generated in 1.41 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.43 seconds\n","Generated in 1.09 seconds\n","Generated in 1.10 seconds\n","Generated in 1.40 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.54 seconds\n","Generated in 1.43 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.07 seconds\n","Generated in 1.46 seconds\n","Generated in 1.39 seconds\n","Generated in 1.17 seconds\n","Generated in 1.38 seconds\n","Generated in 1.59 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.47 seconds\n","Generated in 1.37 seconds\n","Generated in 1.57 seconds\n","Generated in 1.38 seconds\n","Generated in 1.56 seconds\n","Generated in 1.36 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.07 seconds\n","Generated in 1.45 seconds\n","Generated in 1.43 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.34 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.09 seconds\n","Generated in 1.35 seconds\n","Generated in 1.45 seconds\n","Generated in 1.37 seconds\n","Generated in 1.56 seconds\n","Generated in 1.40 seconds\n","Generated in 1.45 seconds\n","Generated in 1.35 seconds\n","Generated in 1.07 seconds\n","Generated in 1.06 seconds\n","Generated in 1.37 seconds\n","Generated in 1.44 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.45 seconds\n","Generated in 1.40 seconds\n","Generated in 1.56 seconds\n","Generated in 1.50 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.55 seconds\n","Generated in 1.47 seconds\n","Generated in 1.35 seconds\n","Generated in 1.38 seconds\n","Generated in 1.09 seconds\n","Generated in 1.44 seconds\n","Generated in 1.42 seconds\n","Generated in 1.07 seconds\n","Generated in 1.36 seconds\n","Generated in 1.56 seconds\n","Generated in 1.38 seconds\n","Generated in 1.41 seconds\n","Generated in 1.59 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.44 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.34 seconds\n","Generated in 1.35 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.49 seconds\n","Generated in 1.52 seconds\n","Generated in 1.37 seconds\n","Generated in 1.41 seconds\n","Generated in 1.58 seconds\n","Generated in 1.48 seconds\n","Generated in 1.45 seconds\n","Generated in 1.47 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.56 seconds\n","Generated in 1.37 seconds\n","Generated in 1.07 seconds\n","Generated in 1.38 seconds\n","Generated in 1.42 seconds\n","Generated in 1.13 seconds\n","Generated in 1.37 seconds\n","Generated in 1.47 seconds\n","Generated in 1.42 seconds\n","Generated in 1.48 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.47 seconds\n","Generated in 1.37 seconds\n","Generated in 1.42 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.60 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.50 seconds\n","Generated in 1.42 seconds\n","Generated in 1.59 seconds\n","Generated in 1.38 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.42 seconds\n","Generated in 1.38 seconds\n","Generated in 1.59 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.60 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.41 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.43 seconds\n","Generated in 1.39 seconds\n","Generated in 1.45 seconds\n","Generated in 1.46 seconds\n","Generated in 1.53 seconds\n","Generated in 1.45 seconds\n","Generated in 1.16 seconds\n","Generated in 1.50 seconds\n","Generated in 1.40 seconds\n","Generated in 1.54 seconds\n","Generated in 1.42 seconds\n","Generated in 1.42 seconds\n","Generated in 1.35 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.42 seconds\n","Generated in 1.45 seconds\n","Generated in 1.48 seconds\n","Generated in 1.16 seconds\n","Generated in 1.43 seconds\n","Generated in 1.56 seconds\n","Generated in 1.09 seconds\n","Generated in 1.08 seconds\n","Generated in 1.06 seconds\n","Generated in 1.38 seconds\n","Generated in 1.47 seconds\n","Generated in 1.60 seconds\n","Generated in 1.44 seconds\n","Generated in 1.46 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.50 seconds\n","Generated in 1.39 seconds\n","Generated in 1.32 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.57 seconds\n","Generated in 1.04 seconds\n","Generated in 1.50 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.56 seconds\n","Generated in 1.38 seconds\n","Generated in 1.43 seconds\n","Generated in 1.49 seconds\n","Generated in 1.60 seconds\n","Generated in 1.40 seconds\n","Generated in 1.08 seconds\n","Generated in 1.43 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.09 seconds\n","Generated in 1.42 seconds\n","Generated in 1.35 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.41 seconds\n","Generated in 1.43 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.52 seconds\n","Generated in 1.38 seconds\n","Generated in 1.56 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.56 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.56 seconds\n","Generated in 1.09 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.10 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.50 seconds\n","Generated in 1.40 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.47 seconds\n","Generated in 1.35 seconds\n","Generated in 1.43 seconds\n","Generated in 1.55 seconds\n","Generated in 1.36 seconds\n","Generated in 1.58 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.45 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.36 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.57 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.06 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.07 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.45 seconds\n","Generated in 1.08 seconds\n","Generated in 1.40 seconds\n","Generated in 1.59 seconds\n","Generated in 1.44 seconds\n","Generated in 1.36 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.07 seconds\n","Generated in 1.41 seconds\n","Generated in 1.58 seconds\n","Generated in 1.43 seconds\n","Generated in 1.08 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.35 seconds\n","Generated in 1.37 seconds\n","Generated in 1.06 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.34 seconds\n","Generated in 1.45 seconds\n","Generated in 1.10 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.06 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.07 seconds\n","Generated in 1.47 seconds\n","Generated in 1.39 seconds\n","Generated in 1.09 seconds\n","Generated in 1.42 seconds\n","Generated in 1.36 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.57 seconds\n","Generated in 1.43 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.08 seconds\n","Generated in 1.58 seconds\n","Generated in 1.36 seconds\n","Generated in 1.42 seconds\n","Generated in 1.40 seconds\n","Generated in 1.36 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.57 seconds\n","Generated in 1.37 seconds\n","Generated in 1.08 seconds\n","Generated in 1.15 seconds\n","Generated in 1.46 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.47 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.09 seconds\n","Generated in 1.58 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.48 seconds\n","Generated in 1.59 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.47 seconds\n","Generated in 1.37 seconds\n","Generated in 1.51 seconds\n","Generated in 1.40 seconds\n","Generated in 1.44 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.35 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.42 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.14 seconds\n","Generated in 1.57 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.21 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.54 seconds\n","Generated in 1.39 seconds\n","Generated in 1.55 seconds\n","Generated in 1.40 seconds\n","Generated in 1.36 seconds\n","Generated in 1.56 seconds\n","Generated in 1.46 seconds\n","Generated in 1.08 seconds\n","Generated in 1.44 seconds\n","Generated in 1.35 seconds\n","Generated in 1.44 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.07 seconds\n","Generated in 1.39 seconds\n","Generated in 1.15 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.35 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.45 seconds\n","Generated in 1.57 seconds\n","Generated in 1.47 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.53 seconds\n","Generated in 1.44 seconds\n","Generated in 1.10 seconds\n","Generated in 1.57 seconds\n","Generated in 1.47 seconds\n","Generated in 1.47 seconds\n","Generated in 1.55 seconds\n","Generated in 1.46 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.35 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.45 seconds\n","Generated in 1.56 seconds\n","Generated in 1.45 seconds\n","Generated in 1.57 seconds\n","Generated in 1.47 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.36 seconds\n","Generated in 1.07 seconds\n","Generated in 1.06 seconds\n","Generated in 1.08 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.08 seconds\n","Generated in 1.40 seconds\n","Generated in 1.06 seconds\n","Generated in 1.47 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.14 seconds\n","Generated in 1.46 seconds\n","Generated in 1.41 seconds\n","Generated in 1.41 seconds\n","Generated in 1.57 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.07 seconds\n","Generated in 1.36 seconds\n","Generated in 1.46 seconds\n","Generated in 1.08 seconds\n","Generated in 1.07 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.44 seconds\n","Generated in 1.39 seconds\n","Generated in 1.56 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.45 seconds\n","Generated in 1.47 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.41 seconds\n","Generated in 1.10 seconds\n","Generated in 1.37 seconds\n","Generated in 1.53 seconds\n","Generated in 1.51 seconds\n","Generated in 1.46 seconds\n","Generated in 1.14 seconds\n","Generated in 1.44 seconds\n","Generated in 1.45 seconds\n","Generated in 1.56 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.57 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.55 seconds\n","Generated in 1.38 seconds\n","Generated in 1.48 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.46 seconds\n","Generated in 1.59 seconds\n","Generated in 1.51 seconds\n","Generated in 1.41 seconds\n","Generated in 1.38 seconds\n","Generated in 1.59 seconds\n","Generated in 1.35 seconds\n","Generated in 1.06 seconds\n","Generated in 1.37 seconds\n","Generated in 1.58 seconds\n","Generated in 1.36 seconds\n","Generated in 1.44 seconds\n","Generated in 1.54 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.10 seconds\n","Generated in 1.16 seconds\n","Generated in 1.44 seconds\n","Generated in 1.37 seconds\n","Generated in 1.08 seconds\n","Generated in 1.38 seconds\n","Generated in 1.44 seconds\n","Generated in 1.36 seconds\n","Generated in 1.51 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.14 seconds\n","Generated in 1.57 seconds\n","Generated in 1.06 seconds\n","Generated in 1.44 seconds\n","Generated in 1.39 seconds\n","Generated in 1.08 seconds\n","Generated in 1.70 seconds\n","Generated in 1.36 seconds\n","Generated in 1.37 seconds\n","Generated in 1.08 seconds\n","Generated in 1.41 seconds\n","Generated in 1.36 seconds\n","Generated in 1.07 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.47 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.43 seconds\n","Generated in 1.41 seconds\n","Generated in 1.36 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.56 seconds\n","Generated in 1.36 seconds\n","Generated in 1.36 seconds\n","Generated in 1.57 seconds\n","Generated in 1.47 seconds\n","Generated in 1.45 seconds\n","Generated in 1.36 seconds\n","Generated in 1.44 seconds\n","Generated in 1.38 seconds\n","Generated in 1.45 seconds\n","Generated in 1.06 seconds\n","Generated in 1.44 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.46 seconds\n","Generated in 1.36 seconds\n","Generated in 1.42 seconds\n","Generated in 1.40 seconds\n","Generated in 1.47 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.11 seconds\n","Generated in 1.37 seconds\n","Generated in 1.47 seconds\n","Generated in 1.40 seconds\n","Generated in 1.54 seconds\n","Generated in 1.41 seconds\n","Generated in 1.60 seconds\n","Generated in 1.56 seconds\n","Generated in 1.46 seconds\n","Generated in 1.56 seconds\n","Generated in 1.39 seconds\n","Generated in 1.59 seconds\n","Generated in 1.39 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.07 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.43 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.45 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.14 seconds\n","Generated in 1.46 seconds\n","Generated in 1.58 seconds\n","Generated in 1.45 seconds\n","Generated in 1.56 seconds\n","Generated in 1.07 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.45 seconds\n","Generated in 1.57 seconds\n","Generated in 1.48 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.47 seconds\n","Generated in 1.60 seconds\n","Generated in 1.57 seconds\n","Generated in 1.36 seconds\n","Generated in 1.42 seconds\n","Generated in 1.40 seconds\n","Generated in 1.39 seconds\n","Generated in 1.59 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.41 seconds\n","Generated in 1.57 seconds\n","Generated in 1.35 seconds\n","Generated in 1.23 seconds\n","Generated in 1.38 seconds\n","Generated in 1.56 seconds\n","Generated in 1.38 seconds\n","Generated in 1.47 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.33 seconds\n","Generated in 1.52 seconds\n","Generated in 1.41 seconds\n","Generated in 1.33 seconds\n","Generated in 1.33 seconds\n","Generated in 1.39 seconds\n","Generated in 1.50 seconds\n","Generated in 1.51 seconds\n","Generated in 1.43 seconds\n","Generated in 1.40 seconds\n","Generated in 1.08 seconds\n","Generated in 1.42 seconds\n","Generated in 1.38 seconds\n","Generated in 1.35 seconds\n","Generated in 1.57 seconds\n","Generated in 1.36 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.34 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.07 seconds\n","Generated in 1.08 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.46 seconds\n","Generated in 1.59 seconds\n","Generated in 1.33 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.51 seconds\n","Generated in 1.44 seconds\n","Generated in 1.58 seconds\n","Generated in 1.44 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.11 seconds\n","Generated in 1.57 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.46 seconds\n","Generated in 1.42 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.45 seconds\n","Generated in 1.45 seconds\n","Generated in 1.58 seconds\n","Generated in 1.36 seconds\n","Generated in 1.35 seconds\n","Generated in 1.38 seconds\n","Generated in 1.44 seconds\n","Generated in 1.44 seconds\n","Generated in 1.33 seconds\n","Generated in 1.37 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.56 seconds\n","Generated in 1.41 seconds\n","Generated in 1.57 seconds\n","Generated in 1.06 seconds\n","Generated in 1.47 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.35 seconds\n","Generated in 1.38 seconds\n","Generated in 1.45 seconds\n","Generated in 1.52 seconds\n","Generated in 1.35 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.34 seconds\n","Generated in 1.38 seconds\n","Generated in 1.07 seconds\n","Generated in 1.06 seconds\n","Generated in 1.08 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.08 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.56 seconds\n","Generated in 1.37 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.08 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.58 seconds\n","Generated in 1.44 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.42 seconds\n","Generated in 1.09 seconds\n","Generated in 1.38 seconds\n","Generated in 1.44 seconds\n","Generated in 1.44 seconds\n","Generated in 1.09 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.43 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.07 seconds\n","Generated in 1.08 seconds\n","Generated in 1.14 seconds\n","Generated in 1.41 seconds\n","Generated in 1.57 seconds\n","Generated in 1.46 seconds\n","Generated in 1.46 seconds\n","Generated in 1.57 seconds\n","Generated in 1.46 seconds\n","Generated in 1.09 seconds\n","Generated in 1.47 seconds\n","Generated in 1.15 seconds\n","Generated in 1.36 seconds\n","Generated in 1.58 seconds\n","Generated in 1.34 seconds\n","Generated in 1.06 seconds\n","Generated in 1.37 seconds\n","Generated in 1.52 seconds\n","Generated in 1.16 seconds\n","Generated in 1.59 seconds\n","Generated in 1.07 seconds\n","Generated in 1.43 seconds\n","Generated in 1.41 seconds\n","Generated in 1.36 seconds\n","Generated in 1.45 seconds\n","Generated in 1.45 seconds\n","Generated in 1.40 seconds\n","Generated in 1.57 seconds\n","Generated in 1.36 seconds\n","Generated in 1.36 seconds\n","Generated in 1.46 seconds\n","Generated in 1.35 seconds\n","Generated in 1.40 seconds\n","Generated in 1.44 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.55 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.44 seconds\n","Generated in 1.38 seconds\n","Generated in 1.57 seconds\n","Generated in 1.36 seconds\n","Generated in 1.47 seconds\n","Generated in 1.57 seconds\n","Generated in 1.08 seconds\n","Generated in 1.57 seconds\n","Generated in 1.45 seconds\n","Generated in 1.06 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.58 seconds\n","Generated in 1.37 seconds\n","Generated in 1.53 seconds\n","Generated in 1.40 seconds\n","Generated in 1.36 seconds\n","Generated in 1.48 seconds\n","Generated in 1.35 seconds\n","Generated in 1.46 seconds\n","Generated in 1.57 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.07 seconds\n","Generated in 1.06 seconds\n","Generated in 1.34 seconds\n","Generated in 1.15 seconds\n","Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.57 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.44 seconds\n","Generated in 1.43 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.47 seconds\n","Generated in 1.57 seconds\n","Generated in 1.38 seconds\n","Generated in 1.07 seconds\n","Generated in 1.37 seconds\n","Generated in 1.43 seconds\n","Generated in 1.07 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.57 seconds\n","Generated in 1.47 seconds\n","Generated in 1.14 seconds\n","Generated in 1.37 seconds\n","Generated in 1.07 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.42 seconds\n","Generated in 1.37 seconds\n","Generated in 1.60 seconds\n","Generated in 1.40 seconds\n","Generated in 1.07 seconds\n","Generated in 1.37 seconds\n","Generated in 1.34 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.58 seconds\n","Generated in 1.06 seconds\n","Generated in 1.38 seconds\n","Generated in 1.35 seconds\n","Generated in 1.10 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.08 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.07 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.43 seconds\n","Generated in 1.43 seconds\n","Generated in 1.46 seconds\n","Generated in 1.51 seconds\n","Generated in 1.36 seconds\n","Generated in 1.17 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.47 seconds\n","Generated in 1.57 seconds\n","Generated in 1.35 seconds\n","Generated in 1.36 seconds\n","Generated in 1.47 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.41 seconds\n","Generated in 1.46 seconds\n","Generated in 1.47 seconds\n","Generated in 1.55 seconds\n","Generated in 1.36 seconds\n","Generated in 1.46 seconds\n","Generated in 1.09 seconds\n","Generated in 1.35 seconds\n","Generated in 1.49 seconds\n","Generated in 1.40 seconds\n","Generated in 1.36 seconds\n","Generated in 1.44 seconds\n","Generated in 1.37 seconds\n","Generated in 1.46 seconds\n","Generated in 1.58 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.46 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.44 seconds\n","Generated in 1.38 seconds\n","Generated in 1.49 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.37 seconds\n","Generated in 1.49 seconds\n","Generated in 1.40 seconds\n","Generated in 1.35 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.60 seconds\n","Generated in 1.41 seconds\n","Generated in 1.10 seconds\n","Generated in 1.42 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.34 seconds\n","Generated in 1.45 seconds\n","Generated in 1.36 seconds\n","Generated in 1.57 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.07 seconds\n","Generated in 1.57 seconds\n","Generated in 1.36 seconds\n","Generated in 1.09 seconds\n","Generated in 1.37 seconds\n","Generated in 1.41 seconds\n","Generated in 1.48 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.58 seconds\n","Generated in 1.47 seconds\n","Generated in 1.37 seconds\n","Generated in 1.43 seconds\n","Generated in 1.44 seconds\n","Generated in 1.09 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.41 seconds\n","Generated in 1.07 seconds\n","Generated in 1.36 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.52 seconds\n","Generated in 1.42 seconds\n","Generated in 1.08 seconds\n","Generated in 1.45 seconds\n","Generated in 1.37 seconds\n","Generated in 1.08 seconds\n","Generated in 1.36 seconds\n","Generated in 1.55 seconds\n","Generated in 1.39 seconds\n","Generated in 1.08 seconds\n","Generated in 1.44 seconds\n","Generated in 1.04 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.36 seconds\n","Generated in 1.45 seconds\n","Generated in 1.52 seconds\n","Generated in 1.07 seconds\n","Generated in 1.41 seconds\n","Generated in 1.06 seconds\n","Generated in 1.41 seconds\n","Generated in 1.35 seconds\n","Generated in 1.39 seconds\n","Generated in 1.35 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.38 seconds\n","Generated in 1.08 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.42 seconds\n","Generated in 1.40 seconds\n","Generated in 1.44 seconds\n","Generated in 1.46 seconds\n","Generated in 1.46 seconds\n","Generated in 1.58 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.44 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.44 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.54 seconds\n","Generated in 1.08 seconds\n","Generated in 1.45 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.08 seconds\n","Generated in 1.58 seconds\n","Generated in 1.59 seconds\n","Generated in 1.40 seconds\n","Generated in 1.44 seconds\n","Generated in 1.45 seconds\n","Generated in 1.46 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.46 seconds\n","Generated in 1.35 seconds\n","Generated in 1.37 seconds\n","Generated in 1.49 seconds\n","Generated in 1.42 seconds\n","Generated in 1.36 seconds\n","Generated in 1.09 seconds\n","Generated in 1.58 seconds\n","Generated in 1.37 seconds\n","Generated in 1.37 seconds\n","Generated in 1.44 seconds\n","Generated in 1.47 seconds\n","Generated in 1.38 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.47 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.08 seconds\n","Generated in 1.47 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.54 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.13 seconds\n","Generated in 1.46 seconds\n","Generated in 1.08 seconds\n","Generated in 1.37 seconds\n","Generated in 1.35 seconds\n","Generated in 1.38 seconds\n","Generated in 1.07 seconds\n","Generated in 1.36 seconds\n","Generated in 1.44 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.47 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.44 seconds\n","Generated in 1.09 seconds\n","Generated in 1.39 seconds\n","Generated in 1.41 seconds\n","Generated in 1.57 seconds\n","Generated in 1.49 seconds\n","Generated in 1.40 seconds\n","Generated in 1.51 seconds\n","Generated in 1.59 seconds\n","Generated in 1.36 seconds\n","Generated in 1.08 seconds\n","Generated in 1.55 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.08 seconds\n","Generated in 1.38 seconds\n","Generated in 1.57 seconds\n","Generated in 1.40 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.42 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.09 seconds\n","Generated in 1.47 seconds\n","Generated in 1.58 seconds\n","Generated in 1.36 seconds\n","Generated in 1.09 seconds\n","Generated in 1.45 seconds\n","Generated in 1.36 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.38 seconds\n","Generated in 1.36 seconds\n","Generated in 1.40 seconds\n","Generated in 1.37 seconds\n","Generated in 1.09 seconds\n","Generated in 1.38 seconds\n","Generated in 1.45 seconds\n","Generated in 1.16 seconds\n","Generated in 1.37 seconds\n","Generated in 1.36 seconds\n","Generated in 1.57 seconds\n","Generated in 1.50 seconds\n","Generated in 1.44 seconds\n","Generated in 1.58 seconds\n","Generated in 1.49 seconds\n","Generated in 1.09 seconds\n","Generated in 1.45 seconds\n","Generated in 1.44 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.55 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.45 seconds\n","Generated in 1.41 seconds\n","Generated in 1.17 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.37 seconds\n","Generated in 1.45 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.43 seconds\n","Generated in 1.56 seconds\n","Generated in 1.36 seconds\n","Generated in 1.39 seconds\n","Generated in 1.40 seconds\n","Generated in 1.40 seconds\n","Generated in 1.41 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.40 seconds\n","Generated in 1.57 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.45 seconds\n","Generated in 1.08 seconds\n","Generated in 1.46 seconds\n","Generated in 1.57 seconds\n","Generated in 1.07 seconds\n","Generated in 1.45 seconds\n","Generated in 1.57 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.44 seconds\n","Generated in 1.38 seconds\n","Generated in 1.45 seconds\n","Generated in 1.39 seconds\n","Generated in 1.39 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.16 seconds\n","Generated in 1.37 seconds\n","Generated in 1.38 seconds\n","Generated in 1.39 seconds\n","Generated in 1.38 seconds\n","Generated in 1.43 seconds\n","Generated in 1.09 seconds\n","Generated in 1.37 seconds\n","Generated in 1.42 seconds\n","Generated in 1.41 seconds\n","Generated in 1.40 seconds\n","Generated in 1.44 seconds\n","Generated in 1.56 seconds\n","Generated in 1.38 seconds\n","Generated in 1.46 seconds\n","Generated in 1.59 seconds\n","Generated in 1.35 seconds\n","Generated in 1.38 seconds\n","Generated in 1.41 seconds\n","Generated in 1.35 seconds\n","Generated in 1.57 seconds\n","Generated in 1.50 seconds\n","Generated in 1.46 seconds\n","Generated in 1.40 seconds\n","Generated in 1.38 seconds\n","Generated in 1.37 seconds\n","Generated in 1.41 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.60 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.60 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.66 seconds\n","Generated in 1.59 seconds\n","Generated in 1.60 seconds\n","Generated in 1.58 seconds\n","Generated in 1.60 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.60 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.59 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.60 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.61 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.60 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.60 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.53 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.60 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.53 seconds\n","Generated in 1.58 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.53 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.61 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.60 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.60 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 2.20 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 2.28 seconds\n","Generated in 1.59 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.54 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.59 seconds\n","Generated in 1.59 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.60 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.52 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.53 seconds\n","Generated in 1.48 seconds\n","Generated in 1.55 seconds\n","Generated in 1.52 seconds\n","Generated in 1.53 seconds\n","Generated in 1.53 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.51 seconds\n","Generated in 1.54 seconds\n","Generated in 1.54 seconds\n","Generated in 1.55 seconds\n","Generated in 1.50 seconds\n","Generated in 1.51 seconds\n","Generated in 1.52 seconds\n","Generated in 1.52 seconds\n","Generated in 1.60 seconds\n","Generated in 1.53 seconds\n","Generated in 1.57 seconds\n","Generated in 1.53 seconds\n","Generated in 1.58 seconds\n","Generated in 1.54 seconds\n","Generated in 1.47 seconds\n","Generated in 1.47 seconds\n","Generated in 1.46 seconds\n","Generated in 1.47 seconds\n","Generated in 1.50 seconds\n","Generated in 1.50 seconds\n","Generated in 1.48 seconds\n","Generated in 1.52 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.54 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.54 seconds\n","Generated in 1.52 seconds\n","Generated in 1.52 seconds\n","Generated in 1.50 seconds\n","Generated in 1.47 seconds\n","Generated in 1.51 seconds\n","Generated in 1.51 seconds\n","Generated in 1.53 seconds\n","Generated in 1.53 seconds\n","Generated in 1.52 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.53 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.51 seconds\n","Generated in 1.59 seconds\n","Generated in 1.54 seconds\n","Generated in 1.59 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.60 seconds\n","Generated in 1.58 seconds\n","Generated in 1.54 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.61 seconds\n","Generated in 1.61 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.60 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.60 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.60 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.60 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.54 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.54 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.54 seconds\n","Generated in 1.54 seconds\n","Generated in 1.55 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.53 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.61 seconds\n","Generated in 1.59 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.53 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.59 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.52 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.54 seconds\n","Generated in 1.58 seconds\n","Generated in 1.59 seconds\n","Generated in 1.54 seconds\n","Generated in 1.55 seconds\n","Generated in 1.60 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.52 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.59 seconds\n","Generated in 1.54 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.53 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.55 seconds\n","Generated in 1.54 seconds\n","Generated in 1.55 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.53 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.53 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.53 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.51 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.53 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.60 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.54 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.54 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.54 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.54 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.53 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.87 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.54 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.53 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.52 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.60 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.60 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.54 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.55 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.56 seconds\n","Generated in 1.55 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.53 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.58 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.56 seconds\n","Generated in 1.61 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.60 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.57 seconds\n","Generated in 1.58 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.57 seconds\n","Generated in 1.59 seconds\n","Generated in 1.58 seconds\n","Generated in 1.55 seconds\n","Generated in 1.56 seconds\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"KllLHDqnJEUV","outputId":"0bf86bb8-df3f-4712-c294-ee27a06c7a96"},"source":["data_valid\n","# testing on this for now"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>inputs</th>\n","      <th>target</th>\n","      <th>target_str</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>The sum of &lt;extra_id_0&gt;58 and 806 is 1564</td>\n","      <td>7</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The sum of 755 and 2&lt;extra_id_0&gt;1 is 1016</td>\n","      <td>6</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>The sum of 864 and 95 is &lt;extra_id_0&gt;9</td>\n","      <td>95</td>\n","      <td>95</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>The sum of 263 and 915 is &lt;extra_id_0&gt;178</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>The sum of 569 and 460 is 1&lt;extra_id_0&gt;29</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>8995</th>\n","      <td>The sum of 745 and 755 is 1&lt;extra_id_0&gt;0</td>\n","      <td>50</td>\n","      <td>50</td>\n","    </tr>\n","    <tr>\n","      <th>8996</th>\n","      <td>The sum of 131 and &lt;extra_id_0&gt;91 is 922</td>\n","      <td>7</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>8997</th>\n","      <td>The sum of 732 and 735 is &lt;extra_id_0&gt;467</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>8998</th>\n","      <td>The sum of 4&lt;extra_id_0&gt;5 and 175 is 640</td>\n","      <td>6</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>8999</th>\n","      <td>The sum of 1&lt;extra_id_0&gt;1 and 318 is 419</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>9000 rows × 3 columns</p>\n","</div>"],"text/plain":["                                         inputs  target target_str\n","0     The sum of <extra_id_0>58 and 806 is 1564       7          7\n","1     The sum of 755 and 2<extra_id_0>1 is 1016       6          6\n","2        The sum of 864 and 95 is <extra_id_0>9      95         95\n","3     The sum of 263 and 915 is <extra_id_0>178       1          1\n","4     The sum of 569 and 460 is 1<extra_id_0>29       0          0\n","...                                         ...     ...        ...\n","8995   The sum of 745 and 755 is 1<extra_id_0>0      50         50\n","8996   The sum of 131 and <extra_id_0>91 is 922       7          7\n","8997  The sum of 732 and 735 is <extra_id_0>467       1          1\n","8998   The sum of 4<extra_id_0>5 and 175 is 640       6          6\n","8999   The sum of 1<extra_id_0>1 and 318 is 419       0          0\n","\n","[9000 rows x 3 columns]"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"mMFAn0nSGy2y","outputId":"452f7da0-ce5d-4318-8469-50cfa5a640ec"},"source":["generateText(\"The sum of 732 and 73<extra_id_0> is 1467\") # example"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Generated in 0.24 seconds\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' 7'"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","metadata":{"id":"qj7E2DPhfwpf"},"source":["data_test['predictions'] = data_valid.apply(lambda x: generateText(x))\n","# should try this."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m-qnPQqYmxBz"},"source":["\n","def convert_to_10ebased(number: str, split_type: str, invert_number: bool) -> str:\n","    signal = None\n","    if number[0] == '-':\n","        signal = '-'\n","        number = number[1:]\n","\n","    output = []\n","    for i, digit in enumerate(number[::-1]):\n","        if split_type is None:\n","            output.append('10e' + str(i))\n","        elif split_type == 'underscore':\n","            output.append('10e' + '_'.join(str(i)))\n","        elif split_type == 'character':\n","            output.append(' '.join('D' + str(i) + 'E'))\n","        else:\n","            raise Exception(f'Wrong split_type: {split_type}')\n","        output.append(digit)\n","\n","    if signal:\n","        output.append(signal)\n","\n","    # The output is already inverted. If we want it to _not_ be inverted, then we invert it.\n","    if not invert_number:\n","        output = output[::-1]\n","\n","    return ' '.join(output)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"LSKVJpXRrqj3","outputId":"415fbab8-533d-447a-d54f-c711453e8f3f"},"source":["convert_to_10ebased(\n","                \"932\", split_type=None, invert_number=False)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'9 10e2 3 10e1 2 10e0'"]},"metadata":{},"execution_count":61}]},{"cell_type":"code","metadata":{"id":"f5HnhuzcrudM"},"source":[""],"execution_count":null,"outputs":[]}]}