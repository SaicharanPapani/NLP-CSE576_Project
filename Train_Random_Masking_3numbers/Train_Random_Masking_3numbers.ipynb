{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Train_Random_Masking_3numbers.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "101d00afb57b490b9dcd2142608404f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58ede14985fb429ba2f836d86f4bf156",
              "IPY_MODEL_e1acf2a518944f33b63c71a51db757d6",
              "IPY_MODEL_c06be352fc5148eb9aa2e2c4ab91ac52"
            ],
            "layout": "IPY_MODEL_b4a67e742d7f47d5a101a8036d1d63c9"
          }
        },
        "b4a67e742d7f47d5a101a8036d1d63c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58ede14985fb429ba2f836d86f4bf156": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_89b6948b717b41c4a1a193630f107bf6",
            "placeholder": "​",
            "style": "IPY_MODEL_e7655fdeb9874f4cba982b8c8192dd19",
            "value": "Downloading: 100%"
          }
        },
        "e1acf2a518944f33b63c71a51db757d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62b803292100467e9b0ca5bc225bd6e2",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb8bff5edd86494abad2f5581daf17f8",
            "value": 791656
          }
        },
        "c06be352fc5148eb9aa2e2c4ab91ac52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21dc00481fce4801aa63827baef03729",
            "placeholder": "​",
            "style": "IPY_MODEL_d22e361274e54448b9582461026ce5e4",
            "value": " 773k/773k [00:00&lt;00:00, 654kB/s]"
          }
        },
        "e7655fdeb9874f4cba982b8c8192dd19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "89b6948b717b41c4a1a193630f107bf6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb8bff5edd86494abad2f5581daf17f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "62b803292100467e9b0ca5bc225bd6e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d22e361274e54448b9582461026ce5e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21dc00481fce4801aa63827baef03729": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03ebd844c5df41d48859323806c80d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fe51b9a26fc40f1b2cdfb0dc7d94762",
              "IPY_MODEL_40ff8d6f1b8a4f0db47c8b7007bd2c13",
              "IPY_MODEL_d925a6082003450db0086b83a9453e19"
            ],
            "layout": "IPY_MODEL_cfc416c7e0ec4fd5ba1737e24cf6be02"
          }
        },
        "cfc416c7e0ec4fd5ba1737e24cf6be02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fe51b9a26fc40f1b2cdfb0dc7d94762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1b7229e4e6149c9a01df97a35128496",
            "placeholder": "​",
            "style": "IPY_MODEL_82e49600911d460796f1cdd2865efd8a",
            "value": "Downloading: 100%"
          }
        },
        "40ff8d6f1b8a4f0db47c8b7007bd2c13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb990acf9c6b4556b203383ca7b9874d",
            "max": 1389353,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a40f2ab3d094f87a6f6ae77c364905c",
            "value": 1389353
          }
        },
        "d925a6082003450db0086b83a9453e19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a8a615392e44a76b630385d3f91552c",
            "placeholder": "​",
            "style": "IPY_MODEL_bd205025646b46dd82c1fc8fc1536fbf",
            "value": " 1.32M/1.32M [00:00&lt;00:00, 7.59MB/s]"
          }
        },
        "82e49600911d460796f1cdd2865efd8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1b7229e4e6149c9a01df97a35128496": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a40f2ab3d094f87a6f6ae77c364905c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb990acf9c6b4556b203383ca7b9874d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd205025646b46dd82c1fc8fc1536fbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a8a615392e44a76b630385d3f91552c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dca725301db4a77a55b76d69d5d8c12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1978588971e3432fa5088b743536786c",
              "IPY_MODEL_d5a4a3bd14334bca981cc42f61d01227",
              "IPY_MODEL_df4aac9bc5d44178b642de10317c830d"
            ],
            "layout": "IPY_MODEL_4541de796b354e128e8990c43b86c2bb"
          }
        },
        "4541de796b354e128e8990c43b86c2bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1978588971e3432fa5088b743536786c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ab388abbbe8429cbc16a81029d7842c",
            "placeholder": "​",
            "style": "IPY_MODEL_b7ec3b0bf17944469314d4514bc40cb9",
            "value": "Downloading: 100%"
          }
        },
        "d5a4a3bd14334bca981cc42f61d01227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f6bc0bc97f54a90acedac65bc16a31b",
            "max": 1199,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_23b0f511b0804c6b9202e02f9528742a",
            "value": 1199
          }
        },
        "df4aac9bc5d44178b642de10317c830d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e711f7f486e746f1a29eda6b53413fb8",
            "placeholder": "​",
            "style": "IPY_MODEL_d065e9ba68e14447a20d874395381ed8",
            "value": " 1.17k/1.17k [00:00&lt;00:00, 31.5kB/s]"
          }
        },
        "b7ec3b0bf17944469314d4514bc40cb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ab388abbbe8429cbc16a81029d7842c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23b0f511b0804c6b9202e02f9528742a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f6bc0bc97f54a90acedac65bc16a31b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d065e9ba68e14447a20d874395381ed8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e711f7f486e746f1a29eda6b53413fb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f4d4a278e774883934eb2872f653f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f787992d5ff4c3b95ec461793a0ea89",
              "IPY_MODEL_0604306dcffb401d8d7368eba412c4b3",
              "IPY_MODEL_1bfca8ef5fd74a29a7503364d5a8079c"
            ],
            "layout": "IPY_MODEL_d0a26fb82b784810a0126198241798d4"
          }
        },
        "d0a26fb82b784810a0126198241798d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f787992d5ff4c3b95ec461793a0ea89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fec687ab824d44948efa609b3679609b",
            "placeholder": "​",
            "style": "IPY_MODEL_2904cb398a6d489e87184c1141094076",
            "value": "Downloading: 100%"
          }
        },
        "0604306dcffb401d8d7368eba412c4b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5766ef81cf2c482babb99d079c8e7726",
            "max": 891691430,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a566c2744bfa4cb6b28c2bbb64e1731c",
            "value": 891691430
          }
        },
        "1bfca8ef5fd74a29a7503364d5a8079c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cda71bb134d54609aac07d1592868af0",
            "placeholder": "​",
            "style": "IPY_MODEL_ea1f173418b547138e1a502d756bc7b3",
            "value": " 850M/850M [00:30&lt;00:00, 31.9MB/s]"
          }
        },
        "2904cb398a6d489e87184c1141094076": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fec687ab824d44948efa609b3679609b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a566c2744bfa4cb6b28c2bbb64e1731c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5766ef81cf2c482babb99d079c8e7726": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea1f173418b547138e1a502d756bc7b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cda71bb134d54609aac07d1592868af0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iosbJtxoNiJ",
        "outputId": "35e951db-086e-493c-aaf3-0a37995d7d5d"
      },
      "source": [
        "!pip install transformers\n",
        "# https://huggingface.co/transformers/installation.html\n",
        "!pip install sentencepiece\n",
        "# https://pypi.org/project/sentencepiece/\n",
        "# Python wrapper for SentencePiece. This API will offer the encoding, decoding and training of Sentencepiece.\n",
        "!pip install Cython\n",
        "# https://pypi.org/project/Cython/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in d:\\software\\anaconda\\lib\\site-packages (4.12.5)\n",
            "Requirement already satisfied: packaging>=20.0 in d:\\software\\anaconda\\lib\\site-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: filelock in d:\\software\\anaconda\\lib\\site-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyyaml>=5.1 in d:\\software\\anaconda\\lib\\site-packages (from transformers) (5.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in d:\\software\\anaconda\\lib\\site-packages (from transformers) (4.59.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in d:\\software\\anaconda\\lib\\site-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in d:\\software\\anaconda\\lib\\site-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in d:\\software\\anaconda\\lib\\site-packages (from transformers) (2021.4.4)\n",
            "Requirement already satisfied: numpy>=1.17 in d:\\software\\anaconda\\lib\\site-packages (from transformers) (1.20.1)\n",
            "Requirement already satisfied: sacremoses in d:\\software\\anaconda\\lib\\site-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: requests in d:\\software\\anaconda\\lib\\site-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\software\\anaconda\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in d:\\software\\anaconda\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in d:\\software\\anaconda\\lib\\site-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\software\\anaconda\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\software\\anaconda\\lib\\site-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in d:\\software\\anaconda\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: click in d:\\software\\anaconda\\lib\\site-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in d:\\software\\anaconda\\lib\\site-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in d:\\software\\anaconda\\lib\\site-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: sentencepiece in d:\\software\\anaconda\\lib\\site-packages (0.1.96)\n",
            "Requirement already satisfied: Cython in d:\\software\\anaconda\\lib\\site-packages (0.29.23)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0eHb96rox2o",
        "outputId": "6f878c0c-b4c3-4714-a43b-7e8a09d311d8"
      },
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\") \n",
        "    print(\"GPU\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CPU\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrjDwHOwo1pO"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU-UZe2cBPpq"
      },
      "source": [
        "## Importing the required packages:\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTTN8t8doyaK"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WQHhVE9oyvl"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import transformers\n",
        "from transformers.optimization import Adafactor \n",
        "import time\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import (\n",
        "    AdamW,\n",
        "    T5ForConditionalGeneration,\n",
        "    T5Tokenizer,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "import torch\n",
        "import random\n",
        "import re\n",
        "\n",
        "#os.chdir('/content/drive/MyDrive/NLP-Project/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbxcQklaozdw",
        "outputId": "5dd59c6d-48eb-45ee-8bee-70687b7b4cf3"
      },
      "source": [
        "import pandas as pd\n",
        "# Reading csv\n",
        "data = pd.read_csv('dataset.csv', header=None, names=['inputs', 'target'])\n",
        "print(data.head(5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                      inputs  target\n",
            "0  The sum of 875 and <extra_id_0>21 is 1096       2\n",
            "1  The sum of 875 and 221 is <extra_id_0>096       1\n",
            "2    The sum of <extra_id_0>33 and 27 is 360       3\n",
            "3    The sum of 333 and <extra_id_0>7 is 360       2\n",
            "4  The sum of 855 and 7<extra_id_0>8 is 1583       2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgCqjq9YpPfh"
      },
      "source": [
        "data = data.sample(n = 40000, random_state = 42).reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Pf_NT1EpPu-"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Test and validation split\n",
        "train, validation = train_test_split(data, test_size=0.3, random_state=42)\n",
        "train, test = train_test_split(train, test_size=0.4, random_state=42)\n",
        "\n",
        "data_train = train.reset_index(drop=True)\n",
        "data_valid = validation.reset_index(drop=True)\n",
        "data_test = test.reset_index(drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo4XPCuNoV2S",
        "outputId": "e223d1b3-c985-430f-d51c-6bb28d72b8e6"
      },
      "source": [
        "data_train_inter.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16800, 2)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuC0EfQ8oZzB",
        "outputId": "1b96ff98-59bf-4fcb-c347-e82f0f03be1f"
      },
      "source": [
        "data_valid_inter.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(12000, 2)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZlhXVLdLpXB6"
      },
      "source": [
        "# Initializing Parameters \n",
        "batch_size, num_of_epochs = 32, 50\n",
        "num_of_batches = int(len(data_train)/batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "TzBzd-OFpXOl",
        "outputId": "f9f5c5f4-29e8-4ac9-910a-871f120a69a3"
      },
      "source": [
        "# get length of all the messages in the train set\n",
        "seq_len = [len(i.split()) for i in data_train['inputs']]\n",
        "\n",
        "pd.Series(seq_len).hist(bins = 30)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "101d00afb57b490b9dcd2142608404f0",
            "b4a67e742d7f47d5a101a8036d1d63c9",
            "58ede14985fb429ba2f836d86f4bf156",
            "e1acf2a518944f33b63c71a51db757d6",
            "c06be352fc5148eb9aa2e2c4ab91ac52",
            "e7655fdeb9874f4cba982b8c8192dd19",
            "89b6948b717b41c4a1a193630f107bf6",
            "eb8bff5edd86494abad2f5581daf17f8",
            "62b803292100467e9b0ca5bc225bd6e2",
            "d22e361274e54448b9582461026ce5e4",
            "21dc00481fce4801aa63827baef03729",
            "03ebd844c5df41d48859323806c80d96",
            "cfc416c7e0ec4fd5ba1737e24cf6be02",
            "6fe51b9a26fc40f1b2cdfb0dc7d94762",
            "40ff8d6f1b8a4f0db47c8b7007bd2c13",
            "d925a6082003450db0086b83a9453e19",
            "82e49600911d460796f1cdd2865efd8a",
            "b1b7229e4e6149c9a01df97a35128496",
            "0a40f2ab3d094f87a6f6ae77c364905c",
            "cb990acf9c6b4556b203383ca7b9874d",
            "bd205025646b46dd82c1fc8fc1536fbf",
            "4a8a615392e44a76b630385d3f91552c",
            "7dca725301db4a77a55b76d69d5d8c12",
            "4541de796b354e128e8990c43b86c2bb",
            "1978588971e3432fa5088b743536786c",
            "d5a4a3bd14334bca981cc42f61d01227",
            "df4aac9bc5d44178b642de10317c830d",
            "b7ec3b0bf17944469314d4514bc40cb9",
            "0ab388abbbe8429cbc16a81029d7842c",
            "23b0f511b0804c6b9202e02f9528742a",
            "1f6bc0bc97f54a90acedac65bc16a31b",
            "d065e9ba68e14447a20d874395381ed8",
            "e711f7f486e746f1a29eda6b53413fb8",
            "6f4d4a278e774883934eb2872f653f14",
            "d0a26fb82b784810a0126198241798d4",
            "1f787992d5ff4c3b95ec461793a0ea89",
            "0604306dcffb401d8d7368eba412c4b3",
            "1bfca8ef5fd74a29a7503364d5a8079c",
            "2904cb398a6d489e87184c1141094076",
            "fec687ab824d44948efa609b3679609b",
            "a566c2744bfa4cb6b28c2bbb64e1731c",
            "5766ef81cf2c482babb99d079c8e7726",
            "ea1f173418b547138e1a502d756bc7b3",
            "cda71bb134d54609aac07d1592868af0"
          ]
        },
        "id": "GlbyzdgF8u8V",
        "outputId": "4d0ad7b7-50f5-497c-cfe0-2f5774482f7c"
      },
      "source": [
        "# T5-base\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base', return_dict=True)\n",
        "# moving the model to device(GPU/CPU)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBpbOX64pjgh"
      },
      "source": [
        "token_lens = []\n",
        "\n",
        "for txt in data_train.inputs:\n",
        "    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n",
        "    token_lens.append(len(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "HyUeN0_lpl2N",
        "outputId": "05dfb49c-c9e0-473a-81a4-308da53971ff"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(1,len(token_lens)+1), token_lens)\n",
        "plt.ylabel('length of tokens')\n",
        "plt.show()\n",
        "\n",
        "MAX_LEN = max(token_lens)\n",
        "print(\"Maximum length is: \", MAX_LEN)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Maximum length is:  14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gYjoqJd-pndC"
      },
      "source": [
        "MAX_LEN = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L82cQ0C0pqIW"
      },
      "source": [
        "def get_word_embeddings(data):\n",
        "    input_ids=[]\n",
        "    attention_masks = []\n",
        "    for sent in data:\n",
        "        encoded_sent = tokenizer.encode_plus(\n",
        "            text=sent,  \n",
        "            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n",
        "            max_length=MAX_LEN,                  # Max length to truncate/pad\n",
        "            pad_to_max_length=True,         # Pad sentence to max length\n",
        "            #return_tensors='pt',           # Return PyTorch tensor\n",
        "            return_attention_mask=True      # Return attention mask\n",
        "            )\n",
        "        \n",
        "        # Add the outputs to the lists\n",
        "        input_ids.append(encoded_sent.get('input_ids'))\n",
        "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
        "\n",
        "  # Convert lists to tensors\n",
        "    input_ids = torch.tensor(input_ids)\n",
        "    attention_masks = torch.tensor(attention_masks)\n",
        "\n",
        "    return input_ids, attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLkjYbYGpr5b",
        "outputId": "8edad19c-5917-4c91-8706-5a99e37c437b"
      },
      "source": [
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "train_inputs, train_masks = get_word_embeddings(data_train['inputs'])\n",
        "val_inputs, val_masks = get_word_embeddings(data_valid['inputs'])\n",
        "test_inputs, test_masks = get_word_embeddings(data_test['inputs'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "D:\\Software\\Anaconda\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2212: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A4_l3-AptkG"
      },
      "source": [
        "data_train['target_str'] = data_train['target'].astype(str)\n",
        "data_valid['target_str'] = data_valid['target'].astype(str)\n",
        "data_test['target_str'] = data_test['target'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s40XgUpIpw-l"
      },
      "source": [
        "#convert lists to tensors\n",
        "train_labels = get_word_embeddings(data_train['target_str'])[0]\n",
        "val_labels = get_word_embeddings(data_valid['target_str'])[0]\n",
        "test_labels = get_word_embeddings(data_test['target_str'])[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qPQ0mIdpyrD",
        "outputId": "91a33a4a-32a6-48ed-f04a-4bbceb327fbd"
      },
      "source": [
        "train_labels.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([32179, 20])"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq7TrUePp0-V",
        "outputId": "fadc60aa-4489-42b0-9b95-795a0dc44a9e"
      },
      "source": [
        "data_train['target']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0         8\n",
              "1         6\n",
              "2        47\n",
              "3         0\n",
              "4         2\n",
              "         ..\n",
              "32174     1\n",
              "32175     8\n",
              "32176     2\n",
              "32177    56\n",
              "32178    17\n",
              "Name: target, Length: 32179, dtype: int64"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "It9MTLIgohSm",
        "outputId": "47e87a5d-ec1b-44eb-b8d3-57a7812f892e"
      },
      "source": [
        "train_labels_inter.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([16800, 20])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htTCQcCEp4oo"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_dataloader = DataLoader(train_data, shuffle = True, batch_size = batch_size)\n",
        "\n",
        "val_data = TensorDataset(val_inputs, val_masks, val_labels)\n",
        "val_dataloader = DataLoader(val_data, shuffle = True, batch_size = batch_size)\n",
        "\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_dataloader = DataLoader(test_data, shuffle = True, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjJHFuY0oNeL"
      },
      "source": [
        "#  Optimizer\n",
        "# https://huggingface.co/transformers/model_doc/t5.html#overview\n",
        "optimizer = Adafactor(\n",
        "    model.parameters(),\n",
        "    lr=5e-4, # Initializing the learning Rate as suggested in the T5 official documentation\n",
        "    eps=(1e-8, 1e-3),\n",
        "    clip_threshold=1.0,\n",
        "    decay_rate=-0.3,\n",
        "    beta1=None,\n",
        "    weight_decay=0.0,\n",
        "    relative_step=False,\n",
        "    scale_parameter=False,\n",
        "    warmup_init=False\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uugaGB5KoNeL"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "# Setting the progress, with html as UI.\n",
        "def progress(loss, value, max=100):\n",
        "    return HTML(\"\"\" Batch loss :{loss}\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 100%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(loss=loss,value=value, max=max))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX4A0tpXoNeL"
      },
      "source": [
        "def evaluate(model, val_dataloader):\n",
        "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
        "    on our validation set.\n",
        "    \"\"\"\n",
        "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
        "    # the test time.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables\n",
        "    v_accuracy = []\n",
        "    v_loss = []\n",
        "\n",
        "    # For each batch in our validation set...\n",
        "    for batch in val_dataloader:\n",
        "        # Load batch to GPU\n",
        "        \n",
        "        v_input_ids, v_attn_mask, v_labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # print(v_input_ids.shape, v_labels.shape)\n",
        "\n",
        "        val_outputs = model.generate(input_ids=v_input_ids, attention_mask=v_attn_mask)\n",
        "\n",
        "        val_preds = [ tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            for output in val_outputs]\n",
        "\n",
        "        val_labels = [ tokenizer.decode(output, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "            for output in v_labels]\n",
        "        \n",
        "        # v_loss.append(val_outputs.loss)\n",
        "\n",
        "        # Get the predictions\n",
        "        # print(val_outputs.logits.shape)\n",
        "        # val_preds = torch.argmax(val_outputs.logits, dim=1).flatten()\n",
        "        # print(val_preds, val_labels)\n",
        "        # Calculate the accuracy rate\n",
        "\n",
        "        val_preds = np.array(val_preds)\n",
        "        val_labels = np.array(val_labels)\n",
        "        accuracy = ((val_preds == val_labels).sum() / len(val_labels)) * 100\n",
        "        v_accuracy.append(accuracy)\n",
        "\n",
        "    # Compute the average accuracy and loss over the validation set.\n",
        "    # v_loss = np.mean(v_loss)\n",
        "    v_accuracy = np.mean(v_accuracy)\n",
        "\n",
        "    return v_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-qnPQqYmxBz"
      },
      "source": [
        "# Function to generate sentences from symptoms on the test dataset\n",
        "def predictSum(text, target):\n",
        "    model.eval()\n",
        "    model.to(\"cpu\")\n",
        "    numbers = [num for num in text.split() if num.isdigit()]\n",
        "    op1, op2 = numbers[0], numbers[1]\n",
        "    maxlen = max(len(str(op1)), len(str(op2)))\n",
        "    minlen = min(len(str(op1)), len(str(op2)))\n",
        "    input_ids = tokenizer.encode(text, return_tensors=\"pt\")  # Batch size 1\n",
        "    # s = time.time()\n",
        "    input_ids.to(\"cpu\")\n",
        "    outputs = model.generate(input_ids)\n",
        "    prediction=tokenizer.decode(outputs[0]).replace('<pad>','').replace('</s>','')\n",
        "    # elapsed = time.time() - s\n",
        "    # print('Generated in {} seconds'.format(str(elapsed)[:4]))\n",
        "    predictlen = len(str(prediction))\n",
        "    tmp = [text, minlen, maxlen, predictlen, target, prediction]\n",
        "    return tmp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC8i7ot9oNeM"
      },
      "source": [
        "## Pre-train, 3operands, 5e-4 and -0.03"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAg-Ddjm2I2s",
        "scrolled": true,
        "outputId": "a8f44df8-20e5-40db-e285-42b16ce86536"
      },
      "source": [
        "import gc\n",
        "\n",
        "val_acc = 0\n",
        "train_accuracy = 0\n",
        "\n",
        "# Sets the module in training mode\n",
        "model.train()\n",
        "\n",
        "for epoch in range(1,num_of_epochs+1):\n",
        "    print('Running epoch: {}'.format(epoch))\n",
        "    running_loss=0\n",
        "    # out = display(progress(1, num_of_batches+1), display_id=True)\n",
        "    i =0 \n",
        "    for batch in train_dataloader:\n",
        "        input_ids, attn_mask, labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # clear out the gradients of all Variables \n",
        "        optimizer.zero_grad()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Forward propogation\n",
        "        # print(model(input_ids=input_ids, attention_mask=attn_mask, labels=labels))\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss_num=loss.item()\n",
        "        logits = outputs.logits\n",
        "        running_loss+=loss_num\n",
        "        # out.update(progress(loss_num,i, num_of_batches+1))\n",
        "\n",
        "        # calculating the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # updating the params\n",
        "        optimizer.step()\n",
        "\n",
        "        print(\"Epoch \", epoch, \"Batch \", i, \"/\", len(train_dataloader), \" Training Loss \", loss_num)\n",
        "        i += 1\n",
        "\n",
        "    running_loss = running_loss/len(train_dataloader)\n",
        "    # v_input_ids, v_attn_mask, v_labels = tuple(t.to(device) for t in data_valid)\n",
        "\n",
        "    curr_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "    # print('Epoch: {} , Running loss: {}'.format(epoch,running_loss))\n",
        "    print(f\"{epoch + 1:^7} | {'-':^7} | {running_loss:^12.6f} | {curr_accuracy:^9.6f}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    if curr_accuracy > val_acc:\n",
        "        val_acc = curr_accuracy\n",
        "        # Saving the best model\n",
        "        torch.save(model.state_dict(),'Pretrain_NPM_3op.bin')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running epoch: 1\n",
            "Epoch  1 Batch  0 / 525  Training Loss  18.863079071044922\n",
            "Epoch  1 Batch  1 / 525  Training Loss  8.888226509094238\n",
            "Epoch  1 Batch  2 / 525  Training Loss  2.5964248180389404\n",
            "Epoch  1 Batch  3 / 525  Training Loss  1.634514570236206\n",
            "Epoch  1 Batch  4 / 525  Training Loss  1.0193078517913818\n",
            "Epoch  1 Batch  5 / 525  Training Loss  0.6540965437889099\n",
            "Epoch  1 Batch  6 / 525  Training Loss  0.5210062265396118\n",
            "Epoch  1 Batch  7 / 525  Training Loss  0.34585070610046387\n",
            "Epoch  1 Batch  8 / 525  Training Loss  0.2842172384262085\n",
            "Epoch  1 Batch  9 / 525  Training Loss  0.27595651149749756\n",
            "Epoch  1 Batch  10 / 525  Training Loss  0.2695380449295044\n",
            "Epoch  1 Batch  11 / 525  Training Loss  0.23714599013328552\n",
            "Epoch  1 Batch  12 / 525  Training Loss  0.2548668682575226\n",
            "Epoch  1 Batch  13 / 525  Training Loss  0.2305338829755783\n",
            "Epoch  1 Batch  14 / 525  Training Loss  0.22013136744499207\n",
            "Epoch  1 Batch  15 / 525  Training Loss  0.20326487720012665\n",
            "Epoch  1 Batch  16 / 525  Training Loss  0.2291821986436844\n",
            "Epoch  1 Batch  17 / 525  Training Loss  0.2270323485136032\n",
            "Epoch  1 Batch  18 / 525  Training Loss  0.21230605244636536\n",
            "Epoch  1 Batch  19 / 525  Training Loss  0.22423061728477478\n",
            "Epoch  1 Batch  20 / 525  Training Loss  0.21088993549346924\n",
            "Epoch  1 Batch  21 / 525  Training Loss  0.19556638598442078\n",
            "Epoch  1 Batch  22 / 525  Training Loss  0.1901521384716034\n",
            "Epoch  1 Batch  23 / 525  Training Loss  0.2390507459640503\n",
            "Epoch  1 Batch  24 / 525  Training Loss  0.19898447394371033\n",
            "Epoch  1 Batch  25 / 525  Training Loss  0.22219622135162354\n",
            "Epoch  1 Batch  26 / 525  Training Loss  0.21339324116706848\n",
            "Epoch  1 Batch  27 / 525  Training Loss  0.21924197673797607\n",
            "Epoch  1 Batch  28 / 525  Training Loss  0.23307593166828156\n",
            "Epoch  1 Batch  29 / 525  Training Loss  0.2191646844148636\n",
            "Epoch  1 Batch  30 / 525  Training Loss  0.20821669697761536\n",
            "Epoch  1 Batch  31 / 525  Training Loss  0.20677343010902405\n",
            "Epoch  1 Batch  32 / 525  Training Loss  0.2183404415845871\n",
            "Epoch  1 Batch  33 / 525  Training Loss  0.19980940222740173\n",
            "Epoch  1 Batch  34 / 525  Training Loss  0.21875569224357605\n",
            "Epoch  1 Batch  35 / 525  Training Loss  0.21521899104118347\n",
            "Epoch  1 Batch  36 / 525  Training Loss  0.20794805884361267\n",
            "Epoch  1 Batch  37 / 525  Training Loss  0.2304268777370453\n",
            "Epoch  1 Batch  38 / 525  Training Loss  0.20356819033622742\n",
            "Epoch  1 Batch  39 / 525  Training Loss  0.2320471704006195\n",
            "Epoch  1 Batch  40 / 525  Training Loss  0.19911512732505798\n",
            "Epoch  1 Batch  41 / 525  Training Loss  0.195783331990242\n",
            "Epoch  1 Batch  42 / 525  Training Loss  0.21709290146827698\n",
            "Epoch  1 Batch  43 / 525  Training Loss  0.19197051227092743\n",
            "Epoch  1 Batch  44 / 525  Training Loss  0.21721073985099792\n",
            "Epoch  1 Batch  45 / 525  Training Loss  0.21555940806865692\n",
            "Epoch  1 Batch  46 / 525  Training Loss  0.2179872989654541\n",
            "Epoch  1 Batch  47 / 525  Training Loss  0.20504562556743622\n",
            "Epoch  1 Batch  48 / 525  Training Loss  0.20395860075950623\n",
            "Epoch  1 Batch  49 / 525  Training Loss  0.209385946393013\n",
            "Epoch  1 Batch  50 / 525  Training Loss  0.21215489506721497\n",
            "Epoch  1 Batch  51 / 525  Training Loss  0.20526139438152313\n",
            "Epoch  1 Batch  52 / 525  Training Loss  0.21318268775939941\n",
            "Epoch  1 Batch  53 / 525  Training Loss  0.2121480405330658\n",
            "Epoch  1 Batch  54 / 525  Training Loss  0.21331655979156494\n",
            "Epoch  1 Batch  55 / 525  Training Loss  0.21342340111732483\n",
            "Epoch  1 Batch  56 / 525  Training Loss  0.20092017948627472\n",
            "Epoch  1 Batch  57 / 525  Training Loss  0.20410463213920593\n",
            "Epoch  1 Batch  58 / 525  Training Loss  0.2026427686214447\n",
            "Epoch  1 Batch  59 / 525  Training Loss  0.20712682604789734\n",
            "Epoch  1 Batch  60 / 525  Training Loss  0.2258206605911255\n",
            "Epoch  1 Batch  61 / 525  Training Loss  0.2213507443666458\n",
            "Epoch  1 Batch  62 / 525  Training Loss  0.20154571533203125\n",
            "Epoch  1 Batch  63 / 525  Training Loss  0.20369109511375427\n",
            "Epoch  1 Batch  64 / 525  Training Loss  0.21623718738555908\n",
            "Epoch  1 Batch  65 / 525  Training Loss  0.18950557708740234\n",
            "Epoch  1 Batch  66 / 525  Training Loss  0.2013998031616211\n",
            "Epoch  1 Batch  67 / 525  Training Loss  0.22323182225227356\n",
            "Epoch  1 Batch  68 / 525  Training Loss  0.20024947822093964\n",
            "Epoch  1 Batch  69 / 525  Training Loss  0.18483854830265045\n",
            "Epoch  1 Batch  70 / 525  Training Loss  0.21212239563465118\n",
            "Epoch  1 Batch  71 / 525  Training Loss  0.19474700093269348\n",
            "Epoch  1 Batch  72 / 525  Training Loss  0.20728616416454315\n",
            "Epoch  1 Batch  73 / 525  Training Loss  0.20174503326416016\n",
            "Epoch  1 Batch  74 / 525  Training Loss  0.19923077523708344\n",
            "Epoch  1 Batch  75 / 525  Training Loss  0.19909527897834778\n",
            "Epoch  1 Batch  76 / 525  Training Loss  0.18163363635540009\n",
            "Epoch  1 Batch  77 / 525  Training Loss  0.20103207230567932\n",
            "Epoch  1 Batch  78 / 525  Training Loss  0.19289706647396088\n",
            "Epoch  1 Batch  79 / 525  Training Loss  0.19843068718910217\n",
            "Epoch  1 Batch  80 / 525  Training Loss  0.18733808398246765\n",
            "Epoch  1 Batch  81 / 525  Training Loss  0.19908109307289124\n",
            "Epoch  1 Batch  82 / 525  Training Loss  0.2091660499572754\n",
            "Epoch  1 Batch  83 / 525  Training Loss  0.19185279309749603\n",
            "Epoch  1 Batch  84 / 525  Training Loss  0.19990122318267822\n",
            "Epoch  1 Batch  85 / 525  Training Loss  0.18697485327720642\n",
            "Epoch  1 Batch  86 / 525  Training Loss  0.21673648059368134\n",
            "Epoch  1 Batch  87 / 525  Training Loss  0.2004776895046234\n",
            "Epoch  1 Batch  88 / 525  Training Loss  0.18234416842460632\n",
            "Epoch  1 Batch  89 / 525  Training Loss  0.19648636877536774\n",
            "Epoch  1 Batch  90 / 525  Training Loss  0.21549519896507263\n",
            "Epoch  1 Batch  91 / 525  Training Loss  0.180843323469162\n",
            "Epoch  1 Batch  92 / 525  Training Loss  0.20836134254932404\n",
            "Epoch  1 Batch  93 / 525  Training Loss  0.20237013697624207\n",
            "Epoch  1 Batch  94 / 525  Training Loss  0.1815674751996994\n",
            "Epoch  1 Batch  95 / 525  Training Loss  0.17955870926380157\n",
            "Epoch  1 Batch  96 / 525  Training Loss  0.19495396316051483\n",
            "Epoch  1 Batch  97 / 525  Training Loss  0.21330949664115906\n",
            "Epoch  1 Batch  98 / 525  Training Loss  0.19314472377300262\n",
            "Epoch  1 Batch  99 / 525  Training Loss  0.2194535732269287\n",
            "Epoch  1 Batch  100 / 525  Training Loss  0.201151043176651\n",
            "Epoch  1 Batch  101 / 525  Training Loss  0.19723699986934662\n",
            "Epoch  1 Batch  102 / 525  Training Loss  0.16936799883842468\n",
            "Epoch  1 Batch  103 / 525  Training Loss  0.19554421305656433\n",
            "Epoch  1 Batch  104 / 525  Training Loss  0.20899291336536407\n",
            "Epoch  1 Batch  105 / 525  Training Loss  0.18812184035778046\n",
            "Epoch  1 Batch  106 / 525  Training Loss  0.1991223394870758\n",
            "Epoch  1 Batch  107 / 525  Training Loss  0.20165026187896729\n",
            "Epoch  1 Batch  108 / 525  Training Loss  0.20481351017951965\n",
            "Epoch  1 Batch  109 / 525  Training Loss  0.16734451055526733\n",
            "Epoch  1 Batch  110 / 525  Training Loss  0.18370068073272705\n",
            "Epoch  1 Batch  111 / 525  Training Loss  0.14858898520469666\n",
            "Epoch  1 Batch  112 / 525  Training Loss  0.18007604777812958\n",
            "Epoch  1 Batch  113 / 525  Training Loss  0.2033858597278595\n",
            "Epoch  1 Batch  114 / 525  Training Loss  0.17973163723945618\n",
            "Epoch  1 Batch  115 / 525  Training Loss  0.21478864550590515\n",
            "Epoch  1 Batch  116 / 525  Training Loss  0.20408916473388672\n",
            "Epoch  1 Batch  117 / 525  Training Loss  0.18312472105026245\n",
            "Epoch  1 Batch  118 / 525  Training Loss  0.18900908529758453\n",
            "Epoch  1 Batch  119 / 525  Training Loss  0.20066514611244202\n",
            "Epoch  1 Batch  120 / 525  Training Loss  0.1796742081642151\n",
            "Epoch  1 Batch  121 / 525  Training Loss  0.1831386834383011\n",
            "Epoch  1 Batch  122 / 525  Training Loss  0.19127270579338074\n",
            "Epoch  1 Batch  123 / 525  Training Loss  0.21231353282928467\n",
            "Epoch  1 Batch  124 / 525  Training Loss  0.21477553248405457\n",
            "Epoch  1 Batch  125 / 525  Training Loss  0.1821073591709137\n",
            "Epoch  1 Batch  126 / 525  Training Loss  0.1783694326877594\n",
            "Epoch  1 Batch  127 / 525  Training Loss  0.19495715200901031\n",
            "Epoch  1 Batch  128 / 525  Training Loss  0.20753617584705353\n",
            "Epoch  1 Batch  129 / 525  Training Loss  0.20178210735321045\n",
            "Epoch  1 Batch  130 / 525  Training Loss  0.20701272785663605\n",
            "Epoch  1 Batch  131 / 525  Training Loss  0.17158880829811096\n",
            "Epoch  1 Batch  132 / 525  Training Loss  0.17527766525745392\n",
            "Epoch  1 Batch  133 / 525  Training Loss  0.1677643060684204\n",
            "Epoch  1 Batch  134 / 525  Training Loss  0.1906752586364746\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1 Batch  135 / 525  Training Loss  0.19765783846378326\n",
            "Epoch  1 Batch  136 / 525  Training Loss  0.1805277168750763\n",
            "Epoch  1 Batch  137 / 525  Training Loss  0.17998455464839935\n",
            "Epoch  1 Batch  138 / 525  Training Loss  0.1900021731853485\n",
            "Epoch  1 Batch  139 / 525  Training Loss  0.20393633842468262\n",
            "Epoch  1 Batch  140 / 525  Training Loss  0.18625888228416443\n",
            "Epoch  1 Batch  141 / 525  Training Loss  0.17933855950832367\n",
            "Epoch  1 Batch  142 / 525  Training Loss  0.16979604959487915\n",
            "Epoch  1 Batch  143 / 525  Training Loss  0.18455755710601807\n",
            "Epoch  1 Batch  144 / 525  Training Loss  0.18925541639328003\n",
            "Epoch  1 Batch  145 / 525  Training Loss  0.1825886219739914\n",
            "Epoch  1 Batch  146 / 525  Training Loss  0.20542769134044647\n",
            "Epoch  1 Batch  147 / 525  Training Loss  0.166683167219162\n",
            "Epoch  1 Batch  148 / 525  Training Loss  0.18641377985477448\n",
            "Epoch  1 Batch  149 / 525  Training Loss  0.19464340806007385\n",
            "Epoch  1 Batch  150 / 525  Training Loss  0.20397870242595673\n",
            "Epoch  1 Batch  151 / 525  Training Loss  0.1723513901233673\n",
            "Epoch  1 Batch  152 / 525  Training Loss  0.1987675130367279\n",
            "Epoch  1 Batch  153 / 525  Training Loss  0.18656712770462036\n",
            "Epoch  1 Batch  154 / 525  Training Loss  0.20194657146930695\n",
            "Epoch  1 Batch  155 / 525  Training Loss  0.17674490809440613\n",
            "Epoch  1 Batch  156 / 525  Training Loss  0.17781738936901093\n",
            "Epoch  1 Batch  157 / 525  Training Loss  0.1860474944114685\n",
            "Epoch  1 Batch  158 / 525  Training Loss  0.1823984980583191\n",
            "Epoch  1 Batch  159 / 525  Training Loss  0.18152296543121338\n",
            "Epoch  1 Batch  160 / 525  Training Loss  0.19289401173591614\n",
            "Epoch  1 Batch  161 / 525  Training Loss  0.1837662160396576\n",
            "Epoch  1 Batch  162 / 525  Training Loss  0.16924919188022614\n",
            "Epoch  1 Batch  163 / 525  Training Loss  0.18244650959968567\n",
            "Epoch  1 Batch  164 / 525  Training Loss  0.17577426135540009\n",
            "Epoch  1 Batch  165 / 525  Training Loss  0.18810999393463135\n",
            "Epoch  1 Batch  166 / 525  Training Loss  0.17828865349292755\n",
            "Epoch  1 Batch  167 / 525  Training Loss  0.20120453834533691\n",
            "Epoch  1 Batch  168 / 525  Training Loss  0.18950682878494263\n",
            "Epoch  1 Batch  169 / 525  Training Loss  0.16271352767944336\n",
            "Epoch  1 Batch  170 / 525  Training Loss  0.18104812502861023\n",
            "Epoch  1 Batch  171 / 525  Training Loss  0.17346307635307312\n",
            "Epoch  1 Batch  172 / 525  Training Loss  0.18765859305858612\n",
            "Epoch  1 Batch  173 / 525  Training Loss  0.19053764641284943\n",
            "Epoch  1 Batch  174 / 525  Training Loss  0.17476508021354675\n",
            "Epoch  1 Batch  175 / 525  Training Loss  0.17856958508491516\n",
            "Epoch  1 Batch  176 / 525  Training Loss  0.1858440339565277\n",
            "Epoch  1 Batch  177 / 525  Training Loss  0.18436317145824432\n",
            "Epoch  1 Batch  178 / 525  Training Loss  0.17554035782814026\n",
            "Epoch  1 Batch  179 / 525  Training Loss  0.18779511749744415\n",
            "Epoch  1 Batch  180 / 525  Training Loss  0.17006729543209076\n",
            "Epoch  1 Batch  181 / 525  Training Loss  0.16523317992687225\n",
            "Epoch  1 Batch  182 / 525  Training Loss  0.15462976694107056\n",
            "Epoch  1 Batch  183 / 525  Training Loss  0.1737237423658371\n",
            "Epoch  1 Batch  184 / 525  Training Loss  0.18420979380607605\n",
            "Epoch  1 Batch  185 / 525  Training Loss  0.17917165160179138\n",
            "Epoch  1 Batch  186 / 525  Training Loss  0.18530690670013428\n",
            "Epoch  1 Batch  187 / 525  Training Loss  0.16846999526023865\n",
            "Epoch  1 Batch  188 / 525  Training Loss  0.19071495532989502\n",
            "Epoch  1 Batch  189 / 525  Training Loss  0.19313350319862366\n",
            "Epoch  1 Batch  190 / 525  Training Loss  0.18134424090385437\n",
            "Epoch  1 Batch  191 / 525  Training Loss  0.18928202986717224\n",
            "Epoch  1 Batch  192 / 525  Training Loss  0.1788681298494339\n",
            "Epoch  1 Batch  193 / 525  Training Loss  0.17818683385849\n",
            "Epoch  1 Batch  194 / 525  Training Loss  0.17228202521800995\n",
            "Epoch  1 Batch  195 / 525  Training Loss  0.19031177461147308\n",
            "Epoch  1 Batch  196 / 525  Training Loss  0.15361151099205017\n",
            "Epoch  1 Batch  197 / 525  Training Loss  0.18129149079322815\n",
            "Epoch  1 Batch  198 / 525  Training Loss  0.16840627789497375\n",
            "Epoch  1 Batch  199 / 525  Training Loss  0.1693296879529953\n",
            "Epoch  1 Batch  200 / 525  Training Loss  0.17827188968658447\n",
            "Epoch  1 Batch  201 / 525  Training Loss  0.17965708673000336\n",
            "Epoch  1 Batch  202 / 525  Training Loss  0.16929645836353302\n",
            "Epoch  1 Batch  203 / 525  Training Loss  0.1977059692144394\n",
            "Epoch  1 Batch  204 / 525  Training Loss  0.18638525903224945\n",
            "Epoch  1 Batch  205 / 525  Training Loss  0.17071375250816345\n",
            "Epoch  1 Batch  206 / 525  Training Loss  0.16588810086250305\n",
            "Epoch  1 Batch  207 / 525  Training Loss  0.18987266719341278\n",
            "Epoch  1 Batch  208 / 525  Training Loss  0.17438629269599915\n",
            "Epoch  1 Batch  209 / 525  Training Loss  0.20289325714111328\n",
            "Epoch  1 Batch  210 / 525  Training Loss  0.15868797898292542\n",
            "Epoch  1 Batch  211 / 525  Training Loss  0.18564821779727936\n",
            "Epoch  1 Batch  212 / 525  Training Loss  0.17018361389636993\n",
            "Epoch  1 Batch  213 / 525  Training Loss  0.18173861503601074\n",
            "Epoch  1 Batch  214 / 525  Training Loss  0.16855715215206146\n",
            "Epoch  1 Batch  215 / 525  Training Loss  0.17519061267375946\n",
            "Epoch  1 Batch  216 / 525  Training Loss  0.17662706971168518\n",
            "Epoch  1 Batch  217 / 525  Training Loss  0.18494614958763123\n",
            "Epoch  1 Batch  218 / 525  Training Loss  0.18098288774490356\n",
            "Epoch  1 Batch  219 / 525  Training Loss  0.16948281228542328\n",
            "Epoch  1 Batch  220 / 525  Training Loss  0.1653256118297577\n",
            "Epoch  1 Batch  221 / 525  Training Loss  0.16815198957920074\n",
            "Epoch  1 Batch  222 / 525  Training Loss  0.16796037554740906\n",
            "Epoch  1 Batch  223 / 525  Training Loss  0.16640989482402802\n",
            "Epoch  1 Batch  224 / 525  Training Loss  0.17795968055725098\n",
            "Epoch  1 Batch  225 / 525  Training Loss  0.1855558305978775\n",
            "Epoch  1 Batch  226 / 525  Training Loss  0.1974027454853058\n",
            "Epoch  1 Batch  227 / 525  Training Loss  0.18416911363601685\n",
            "Epoch  1 Batch  228 / 525  Training Loss  0.1737070083618164\n",
            "Epoch  1 Batch  229 / 525  Training Loss  0.1961876004934311\n",
            "Epoch  1 Batch  230 / 525  Training Loss  0.15876014530658722\n",
            "Epoch  1 Batch  231 / 525  Training Loss  0.18558908998966217\n",
            "Epoch  1 Batch  232 / 525  Training Loss  0.18420380353927612\n",
            "Epoch  1 Batch  233 / 525  Training Loss  0.1619812399148941\n",
            "Epoch  1 Batch  234 / 525  Training Loss  0.13840022683143616\n",
            "Epoch  1 Batch  235 / 525  Training Loss  0.1846507489681244\n",
            "Epoch  1 Batch  236 / 525  Training Loss  0.17137417197227478\n",
            "Epoch  1 Batch  237 / 525  Training Loss  0.17284612357616425\n",
            "Epoch  1 Batch  238 / 525  Training Loss  0.17991168797016144\n",
            "Epoch  1 Batch  239 / 525  Training Loss  0.1807529479265213\n",
            "Epoch  1 Batch  240 / 525  Training Loss  0.16263283789157867\n",
            "Epoch  1 Batch  241 / 525  Training Loss  0.1722872108221054\n",
            "Epoch  1 Batch  242 / 525  Training Loss  0.1539735645055771\n",
            "Epoch  1 Batch  243 / 525  Training Loss  0.16979631781578064\n",
            "Epoch  1 Batch  244 / 525  Training Loss  0.1790430247783661\n",
            "Epoch  1 Batch  245 / 525  Training Loss  0.18673890829086304\n",
            "Epoch  1 Batch  246 / 525  Training Loss  0.1904500275850296\n",
            "Epoch  1 Batch  247 / 525  Training Loss  0.18439748883247375\n",
            "Epoch  1 Batch  248 / 525  Training Loss  0.1753290891647339\n",
            "Epoch  1 Batch  249 / 525  Training Loss  0.19740189611911774\n",
            "Epoch  1 Batch  250 / 525  Training Loss  0.1898515522480011\n",
            "Epoch  1 Batch  251 / 525  Training Loss  0.16448727250099182\n",
            "Epoch  1 Batch  252 / 525  Training Loss  0.17319290339946747\n",
            "Epoch  1 Batch  253 / 525  Training Loss  0.15451975166797638\n",
            "Epoch  1 Batch  254 / 525  Training Loss  0.16256381571292877\n",
            "Epoch  1 Batch  255 / 525  Training Loss  0.17060764133930206\n",
            "Epoch  1 Batch  256 / 525  Training Loss  0.17532816529273987\n",
            "Epoch  1 Batch  257 / 525  Training Loss  0.17344005405902863\n",
            "Epoch  1 Batch  258 / 525  Training Loss  0.14092177152633667\n",
            "Epoch  1 Batch  259 / 525  Training Loss  0.18002614378929138\n",
            "Epoch  1 Batch  260 / 525  Training Loss  0.16902002692222595\n",
            "Epoch  1 Batch  261 / 525  Training Loss  0.16393494606018066\n",
            "Epoch  1 Batch  262 / 525  Training Loss  0.17100447416305542\n",
            "Epoch  1 Batch  263 / 525  Training Loss  0.18090437352657318\n",
            "Epoch  1 Batch  264 / 525  Training Loss  0.1831401139497757\n",
            "Epoch  1 Batch  265 / 525  Training Loss  0.1538795828819275\n",
            "Epoch  1 Batch  266 / 525  Training Loss  0.1857215166091919\n",
            "Epoch  1 Batch  267 / 525  Training Loss  0.17396442592144012\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1 Batch  268 / 525  Training Loss  0.18446044623851776\n",
            "Epoch  1 Batch  269 / 525  Training Loss  0.1854640543460846\n",
            "Epoch  1 Batch  270 / 525  Training Loss  0.14638999104499817\n",
            "Epoch  1 Batch  271 / 525  Training Loss  0.19027680158615112\n",
            "Epoch  1 Batch  272 / 525  Training Loss  0.16862188279628754\n",
            "Epoch  1 Batch  273 / 525  Training Loss  0.17885342240333557\n",
            "Epoch  1 Batch  274 / 525  Training Loss  0.17859186232089996\n",
            "Epoch  1 Batch  275 / 525  Training Loss  0.17599622905254364\n",
            "Epoch  1 Batch  276 / 525  Training Loss  0.16142170131206512\n",
            "Epoch  1 Batch  277 / 525  Training Loss  0.18808239698410034\n",
            "Epoch  1 Batch  278 / 525  Training Loss  0.18210414052009583\n",
            "Epoch  1 Batch  279 / 525  Training Loss  0.1848982870578766\n",
            "Epoch  1 Batch  280 / 525  Training Loss  0.19165536761283875\n",
            "Epoch  1 Batch  281 / 525  Training Loss  0.16894268989562988\n",
            "Epoch  1 Batch  282 / 525  Training Loss  0.16990146040916443\n",
            "Epoch  1 Batch  283 / 525  Training Loss  0.16319647431373596\n",
            "Epoch  1 Batch  284 / 525  Training Loss  0.15598949790000916\n",
            "Epoch  1 Batch  285 / 525  Training Loss  0.16005097329616547\n",
            "Epoch  1 Batch  286 / 525  Training Loss  0.18163910508155823\n",
            "Epoch  1 Batch  287 / 525  Training Loss  0.15522462129592896\n",
            "Epoch  1 Batch  288 / 525  Training Loss  0.19231900572776794\n",
            "Epoch  1 Batch  289 / 525  Training Loss  0.16725344955921173\n",
            "Epoch  1 Batch  290 / 525  Training Loss  0.17969964444637299\n",
            "Epoch  1 Batch  291 / 525  Training Loss  0.1597064584493637\n",
            "Epoch  1 Batch  292 / 525  Training Loss  0.214223712682724\n",
            "Epoch  1 Batch  293 / 525  Training Loss  0.18019187450408936\n",
            "Epoch  1 Batch  294 / 525  Training Loss  0.18312612175941467\n",
            "Epoch  1 Batch  295 / 525  Training Loss  0.17349790036678314\n",
            "Epoch  1 Batch  296 / 525  Training Loss  0.18013007938861847\n",
            "Epoch  1 Batch  297 / 525  Training Loss  0.16280683875083923\n",
            "Epoch  1 Batch  298 / 525  Training Loss  0.1697562038898468\n",
            "Epoch  1 Batch  299 / 525  Training Loss  0.1768447756767273\n",
            "Epoch  1 Batch  300 / 525  Training Loss  0.1446966528892517\n",
            "Epoch  1 Batch  301 / 525  Training Loss  0.17687806487083435\n",
            "Epoch  1 Batch  302 / 525  Training Loss  0.1849379986524582\n",
            "Epoch  1 Batch  303 / 525  Training Loss  0.17579448223114014\n",
            "Epoch  1 Batch  304 / 525  Training Loss  0.17148275673389435\n",
            "Epoch  1 Batch  305 / 525  Training Loss  0.168840229511261\n",
            "Epoch  1 Batch  306 / 525  Training Loss  0.18145737051963806\n",
            "Epoch  1 Batch  307 / 525  Training Loss  0.17930728197097778\n",
            "Epoch  1 Batch  308 / 525  Training Loss  0.14869365096092224\n",
            "Epoch  1 Batch  309 / 525  Training Loss  0.1767815500497818\n",
            "Epoch  1 Batch  310 / 525  Training Loss  0.1686277985572815\n",
            "Epoch  1 Batch  311 / 525  Training Loss  0.17029781639575958\n",
            "Epoch  1 Batch  312 / 525  Training Loss  0.1736672818660736\n",
            "Epoch  1 Batch  313 / 525  Training Loss  0.17644187808036804\n",
            "Epoch  1 Batch  314 / 525  Training Loss  0.16743384301662445\n",
            "Epoch  1 Batch  315 / 525  Training Loss  0.16156628727912903\n",
            "Epoch  1 Batch  316 / 525  Training Loss  0.17755405604839325\n",
            "Epoch  1 Batch  317 / 525  Training Loss  0.1472136378288269\n",
            "Epoch  1 Batch  318 / 525  Training Loss  0.15157447755336761\n",
            "Epoch  1 Batch  319 / 525  Training Loss  0.1566247045993805\n",
            "Epoch  1 Batch  320 / 525  Training Loss  0.17774292826652527\n",
            "Epoch  1 Batch  321 / 525  Training Loss  0.1882190853357315\n",
            "Epoch  1 Batch  322 / 525  Training Loss  0.1708231121301651\n",
            "Epoch  1 Batch  323 / 525  Training Loss  0.18274852633476257\n",
            "Epoch  1 Batch  324 / 525  Training Loss  0.1646287441253662\n",
            "Epoch  1 Batch  325 / 525  Training Loss  0.16467444598674774\n",
            "Epoch  1 Batch  326 / 525  Training Loss  0.17027252912521362\n",
            "Epoch  1 Batch  327 / 525  Training Loss  0.14442992210388184\n",
            "Epoch  1 Batch  328 / 525  Training Loss  0.1845601350069046\n",
            "Epoch  1 Batch  329 / 525  Training Loss  0.16640545427799225\n",
            "Epoch  1 Batch  330 / 525  Training Loss  0.18065093457698822\n",
            "Epoch  1 Batch  331 / 525  Training Loss  0.17540839314460754\n",
            "Epoch  1 Batch  332 / 525  Training Loss  0.18300370872020721\n",
            "Epoch  1 Batch  333 / 525  Training Loss  0.15217655897140503\n",
            "Epoch  1 Batch  334 / 525  Training Loss  0.170588418841362\n",
            "Epoch  1 Batch  335 / 525  Training Loss  0.18006631731987\n",
            "Epoch  1 Batch  336 / 525  Training Loss  0.16163457930088043\n",
            "Epoch  1 Batch  337 / 525  Training Loss  0.16306699812412262\n",
            "Epoch  1 Batch  338 / 525  Training Loss  0.17307277023792267\n",
            "Epoch  1 Batch  339 / 525  Training Loss  0.17979171872138977\n",
            "Epoch  1 Batch  340 / 525  Training Loss  0.1528920978307724\n",
            "Epoch  1 Batch  341 / 525  Training Loss  0.1807517111301422\n",
            "Epoch  1 Batch  342 / 525  Training Loss  0.16660627722740173\n",
            "Epoch  1 Batch  343 / 525  Training Loss  0.19391368329524994\n",
            "Epoch  1 Batch  344 / 525  Training Loss  0.1734403371810913\n",
            "Epoch  1 Batch  345 / 525  Training Loss  0.14069783687591553\n",
            "Epoch  1 Batch  346 / 525  Training Loss  0.18144050240516663\n",
            "Epoch  1 Batch  347 / 525  Training Loss  0.17075133323669434\n",
            "Epoch  1 Batch  348 / 525  Training Loss  0.18925490975379944\n",
            "Epoch  1 Batch  349 / 525  Training Loss  0.1603701412677765\n",
            "Epoch  1 Batch  350 / 525  Training Loss  0.17160074412822723\n",
            "Epoch  1 Batch  351 / 525  Training Loss  0.1911385953426361\n",
            "Epoch  1 Batch  352 / 525  Training Loss  0.17751306295394897\n",
            "Epoch  1 Batch  353 / 525  Training Loss  0.169221892952919\n",
            "Epoch  1 Batch  354 / 525  Training Loss  0.15135130286216736\n",
            "Epoch  1 Batch  355 / 525  Training Loss  0.16185227036476135\n",
            "Epoch  1 Batch  356 / 525  Training Loss  0.1806703358888626\n",
            "Epoch  1 Batch  357 / 525  Training Loss  0.16161778569221497\n",
            "Epoch  1 Batch  358 / 525  Training Loss  0.16864004731178284\n",
            "Epoch  1 Batch  359 / 525  Training Loss  0.19770489633083344\n",
            "Epoch  1 Batch  360 / 525  Training Loss  0.1612635850906372\n",
            "Epoch  1 Batch  361 / 525  Training Loss  0.17257502675056458\n",
            "Epoch  1 Batch  362 / 525  Training Loss  0.17780590057373047\n",
            "Epoch  1 Batch  363 / 525  Training Loss  0.17517098784446716\n",
            "Epoch  1 Batch  364 / 525  Training Loss  0.1621590405702591\n",
            "Epoch  1 Batch  365 / 525  Training Loss  0.1803806722164154\n",
            "Epoch  1 Batch  366 / 525  Training Loss  0.18889375030994415\n",
            "Epoch  1 Batch  367 / 525  Training Loss  0.16336235404014587\n",
            "Epoch  1 Batch  368 / 525  Training Loss  0.1446623057126999\n",
            "Epoch  1 Batch  369 / 525  Training Loss  0.17000648379325867\n",
            "Epoch  1 Batch  370 / 525  Training Loss  0.1550847589969635\n",
            "Epoch  1 Batch  371 / 525  Training Loss  0.20138725638389587\n",
            "Epoch  1 Batch  372 / 525  Training Loss  0.1845654398202896\n",
            "Epoch  1 Batch  373 / 525  Training Loss  0.16200360655784607\n",
            "Epoch  1 Batch  374 / 525  Training Loss  0.1627206653356552\n",
            "Epoch  1 Batch  375 / 525  Training Loss  0.16738036274909973\n",
            "Epoch  1 Batch  376 / 525  Training Loss  0.17621244490146637\n",
            "Epoch  1 Batch  377 / 525  Training Loss  0.16685527563095093\n",
            "Epoch  1 Batch  378 / 525  Training Loss  0.1546134352684021\n",
            "Epoch  1 Batch  379 / 525  Training Loss  0.16853930056095123\n",
            "Epoch  1 Batch  380 / 525  Training Loss  0.18863341212272644\n",
            "Epoch  1 Batch  381 / 525  Training Loss  0.173243910074234\n",
            "Epoch  1 Batch  382 / 525  Training Loss  0.16456958651542664\n",
            "Epoch  1 Batch  383 / 525  Training Loss  0.17346559464931488\n",
            "Epoch  1 Batch  384 / 525  Training Loss  0.1608893871307373\n",
            "Epoch  1 Batch  385 / 525  Training Loss  0.1627158671617508\n",
            "Epoch  1 Batch  386 / 525  Training Loss  0.18968650698661804\n",
            "Epoch  1 Batch  387 / 525  Training Loss  0.1552761346101761\n",
            "Epoch  1 Batch  388 / 525  Training Loss  0.1796630322933197\n",
            "Epoch  1 Batch  389 / 525  Training Loss  0.1808619350194931\n",
            "Epoch  1 Batch  390 / 525  Training Loss  0.19016031920909882\n",
            "Epoch  1 Batch  391 / 525  Training Loss  0.15777185559272766\n",
            "Epoch  1 Batch  392 / 525  Training Loss  0.16898928582668304\n",
            "Epoch  1 Batch  393 / 525  Training Loss  0.17040228843688965\n",
            "Epoch  1 Batch  394 / 525  Training Loss  0.1621619462966919\n",
            "Epoch  1 Batch  395 / 525  Training Loss  0.18104110658168793\n",
            "Epoch  1 Batch  396 / 525  Training Loss  0.13572363555431366\n",
            "Epoch  1 Batch  397 / 525  Training Loss  0.17909547686576843\n",
            "Epoch  1 Batch  398 / 525  Training Loss  0.1513780951499939\n",
            "Epoch  1 Batch  399 / 525  Training Loss  0.17518630623817444\n",
            "Epoch  1 Batch  400 / 525  Training Loss  0.15953542292118073\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1 Batch  401 / 525  Training Loss  0.16548438370227814\n",
            "Epoch  1 Batch  402 / 525  Training Loss  0.16531303524971008\n",
            "Epoch  1 Batch  403 / 525  Training Loss  0.19778437912464142\n",
            "Epoch  1 Batch  404 / 525  Training Loss  0.15942062437534332\n",
            "Epoch  1 Batch  405 / 525  Training Loss  0.18568561971187592\n",
            "Epoch  1 Batch  406 / 525  Training Loss  0.17022016644477844\n",
            "Epoch  1 Batch  407 / 525  Training Loss  0.1647629588842392\n",
            "Epoch  1 Batch  408 / 525  Training Loss  0.16120906174182892\n",
            "Epoch  1 Batch  409 / 525  Training Loss  0.15836399793624878\n",
            "Epoch  1 Batch  410 / 525  Training Loss  0.15245947241783142\n",
            "Epoch  1 Batch  411 / 525  Training Loss  0.17751018702983856\n",
            "Epoch  1 Batch  412 / 525  Training Loss  0.16383804380893707\n",
            "Epoch  1 Batch  413 / 525  Training Loss  0.15498004853725433\n",
            "Epoch  1 Batch  414 / 525  Training Loss  0.18529072403907776\n",
            "Epoch  1 Batch  415 / 525  Training Loss  0.18190370500087738\n",
            "Epoch  1 Batch  416 / 525  Training Loss  0.19035598635673523\n",
            "Epoch  1 Batch  417 / 525  Training Loss  0.19023558497428894\n",
            "Epoch  1 Batch  418 / 525  Training Loss  0.15401245653629303\n",
            "Epoch  1 Batch  419 / 525  Training Loss  0.1774589717388153\n",
            "Epoch  1 Batch  420 / 525  Training Loss  0.179866224527359\n",
            "Epoch  1 Batch  421 / 525  Training Loss  0.1758054792881012\n",
            "Epoch  1 Batch  422 / 525  Training Loss  0.1559210568666458\n",
            "Epoch  1 Batch  423 / 525  Training Loss  0.16651812195777893\n",
            "Epoch  1 Batch  424 / 525  Training Loss  0.19134403765201569\n",
            "Epoch  1 Batch  425 / 525  Training Loss  0.1648680567741394\n",
            "Epoch  1 Batch  426 / 525  Training Loss  0.19178485870361328\n",
            "Epoch  1 Batch  427 / 525  Training Loss  0.16872252523899078\n",
            "Epoch  1 Batch  428 / 525  Training Loss  0.1565726399421692\n",
            "Epoch  1 Batch  429 / 525  Training Loss  0.16576699912548065\n",
            "Epoch  1 Batch  430 / 525  Training Loss  0.15197166800498962\n",
            "Epoch  1 Batch  431 / 525  Training Loss  0.16286808252334595\n",
            "Epoch  1 Batch  432 / 525  Training Loss  0.18211977183818817\n",
            "Epoch  1 Batch  433 / 525  Training Loss  0.1837911605834961\n",
            "Epoch  1 Batch  434 / 525  Training Loss  0.16424475610256195\n",
            "Epoch  1 Batch  435 / 525  Training Loss  0.17002327740192413\n",
            "Epoch  1 Batch  436 / 525  Training Loss  0.16131919622421265\n",
            "Epoch  1 Batch  437 / 525  Training Loss  0.16180676221847534\n",
            "Epoch  1 Batch  438 / 525  Training Loss  0.18245509266853333\n",
            "Epoch  1 Batch  439 / 525  Training Loss  0.16160231828689575\n",
            "Epoch  1 Batch  440 / 525  Training Loss  0.14393213391304016\n",
            "Epoch  1 Batch  441 / 525  Training Loss  0.17122960090637207\n",
            "Epoch  1 Batch  442 / 525  Training Loss  0.16908031702041626\n",
            "Epoch  1 Batch  443 / 525  Training Loss  0.1643729954957962\n",
            "Epoch  1 Batch  444 / 525  Training Loss  0.16159187257289886\n",
            "Epoch  1 Batch  445 / 525  Training Loss  0.18491105735301971\n",
            "Epoch  1 Batch  446 / 525  Training Loss  0.15993742644786835\n",
            "Epoch  1 Batch  447 / 525  Training Loss  0.17564183473587036\n",
            "Epoch  1 Batch  448 / 525  Training Loss  0.16577085852622986\n",
            "Epoch  1 Batch  449 / 525  Training Loss  0.18441064655780792\n",
            "Epoch  1 Batch  450 / 525  Training Loss  0.1677650362253189\n",
            "Epoch  1 Batch  451 / 525  Training Loss  0.17422227561473846\n",
            "Epoch  1 Batch  452 / 525  Training Loss  0.1593114286661148\n",
            "Epoch  1 Batch  453 / 525  Training Loss  0.15953025221824646\n",
            "Epoch  1 Batch  454 / 525  Training Loss  0.16495844721794128\n",
            "Epoch  1 Batch  455 / 525  Training Loss  0.16807344555854797\n",
            "Epoch  1 Batch  456 / 525  Training Loss  0.1465476155281067\n",
            "Epoch  1 Batch  457 / 525  Training Loss  0.16929949820041656\n",
            "Epoch  1 Batch  458 / 525  Training Loss  0.18225353956222534\n",
            "Epoch  1 Batch  459 / 525  Training Loss  0.17197278141975403\n",
            "Epoch  1 Batch  460 / 525  Training Loss  0.176695317029953\n",
            "Epoch  1 Batch  461 / 525  Training Loss  0.1374211609363556\n",
            "Epoch  1 Batch  462 / 525  Training Loss  0.15799643099308014\n",
            "Epoch  1 Batch  463 / 525  Training Loss  0.16706326603889465\n",
            "Epoch  1 Batch  464 / 525  Training Loss  0.18142370879650116\n",
            "Epoch  1 Batch  465 / 525  Training Loss  0.17606766521930695\n",
            "Epoch  1 Batch  466 / 525  Training Loss  0.16966578364372253\n",
            "Epoch  1 Batch  467 / 525  Training Loss  0.16012558341026306\n",
            "Epoch  1 Batch  468 / 525  Training Loss  0.1577806919813156\n",
            "Epoch  1 Batch  469 / 525  Training Loss  0.1633707731962204\n",
            "Epoch  1 Batch  470 / 525  Training Loss  0.14977151155471802\n",
            "Epoch  1 Batch  471 / 525  Training Loss  0.14838507771492004\n",
            "Epoch  1 Batch  472 / 525  Training Loss  0.17204038798809052\n",
            "Epoch  1 Batch  473 / 525  Training Loss  0.16983559727668762\n",
            "Epoch  1 Batch  474 / 525  Training Loss  0.15776845812797546\n",
            "Epoch  1 Batch  475 / 525  Training Loss  0.16412188112735748\n",
            "Epoch  1 Batch  476 / 525  Training Loss  0.1760287582874298\n",
            "Epoch  1 Batch  477 / 525  Training Loss  0.18109047412872314\n",
            "Epoch  1 Batch  478 / 525  Training Loss  0.14344365894794464\n",
            "Epoch  1 Batch  479 / 525  Training Loss  0.1698807030916214\n",
            "Epoch  1 Batch  480 / 525  Training Loss  0.16473889350891113\n",
            "Epoch  1 Batch  481 / 525  Training Loss  0.14967957139015198\n",
            "Epoch  1 Batch  482 / 525  Training Loss  0.1609121412038803\n",
            "Epoch  1 Batch  483 / 525  Training Loss  0.16579636931419373\n",
            "Epoch  1 Batch  484 / 525  Training Loss  0.1612536609172821\n",
            "Epoch  1 Batch  485 / 525  Training Loss  0.16198626160621643\n",
            "Epoch  1 Batch  486 / 525  Training Loss  0.153351292014122\n",
            "Epoch  1 Batch  487 / 525  Training Loss  0.17389069497585297\n",
            "Epoch  1 Batch  488 / 525  Training Loss  0.16662627458572388\n",
            "Epoch  1 Batch  489 / 525  Training Loss  0.17122769355773926\n",
            "Epoch  1 Batch  490 / 525  Training Loss  0.14650920033454895\n",
            "Epoch  1 Batch  491 / 525  Training Loss  0.16533467173576355\n",
            "Epoch  1 Batch  492 / 525  Training Loss  0.17587199807167053\n",
            "Epoch  1 Batch  493 / 525  Training Loss  0.1813688576221466\n",
            "Epoch  1 Batch  494 / 525  Training Loss  0.18651872873306274\n",
            "Epoch  1 Batch  495 / 525  Training Loss  0.14831982553005219\n",
            "Epoch  1 Batch  496 / 525  Training Loss  0.154756098985672\n",
            "Epoch  1 Batch  497 / 525  Training Loss  0.16509810090065002\n",
            "Epoch  1 Batch  498 / 525  Training Loss  0.16725899279117584\n",
            "Epoch  1 Batch  499 / 525  Training Loss  0.14930227398872375\n",
            "Epoch  1 Batch  500 / 525  Training Loss  0.14634768664836884\n",
            "Epoch  1 Batch  501 / 525  Training Loss  0.16455230116844177\n",
            "Epoch  1 Batch  502 / 525  Training Loss  0.15770548582077026\n",
            "Epoch  1 Batch  503 / 525  Training Loss  0.17359505593776703\n",
            "Epoch  1 Batch  504 / 525  Training Loss  0.16553419828414917\n",
            "Epoch  1 Batch  505 / 525  Training Loss  0.15782853960990906\n",
            "Epoch  1 Batch  506 / 525  Training Loss  0.1665305644273758\n",
            "Epoch  1 Batch  507 / 525  Training Loss  0.1516837775707245\n",
            "Epoch  1 Batch  508 / 525  Training Loss  0.16497300565242767\n",
            "Epoch  1 Batch  509 / 525  Training Loss  0.15322181582450867\n",
            "Epoch  1 Batch  510 / 525  Training Loss  0.16609026491641998\n",
            "Epoch  1 Batch  511 / 525  Training Loss  0.16339978575706482\n",
            "Epoch  1 Batch  512 / 525  Training Loss  0.15617796778678894\n",
            "Epoch  1 Batch  513 / 525  Training Loss  0.1751193404197693\n",
            "Epoch  1 Batch  514 / 525  Training Loss  0.15429174900054932\n",
            "Epoch  1 Batch  515 / 525  Training Loss  0.17391103506088257\n",
            "Epoch  1 Batch  516 / 525  Training Loss  0.1705033779144287\n",
            "Epoch  1 Batch  517 / 525  Training Loss  0.16519173979759216\n",
            "Epoch  1 Batch  518 / 525  Training Loss  0.1543588936328888\n",
            "Epoch  1 Batch  519 / 525  Training Loss  0.15759685635566711\n",
            "Epoch  1 Batch  520 / 525  Training Loss  0.16814719140529633\n",
            "Epoch  1 Batch  521 / 525  Training Loss  0.1538250595331192\n",
            "Epoch  1 Batch  522 / 525  Training Loss  0.14079232513904572\n",
            "Epoch  1 Batch  523 / 525  Training Loss  0.17839781939983368\n",
            "Epoch  1 Batch  524 / 525  Training Loss  0.17232635617256165\n",
            "   2    |    -    |   0.243188   | 11.975000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 2\n",
            "Epoch  2 Batch  0 / 525  Training Loss  0.15857774019241333\n",
            "Epoch  2 Batch  1 / 525  Training Loss  0.14846116304397583\n",
            "Epoch  2 Batch  2 / 525  Training Loss  0.16200882196426392\n",
            "Epoch  2 Batch  3 / 525  Training Loss  0.13362984359264374\n",
            "Epoch  2 Batch  4 / 525  Training Loss  0.1400369107723236\n",
            "Epoch  2 Batch  5 / 525  Training Loss  0.14486268162727356\n",
            "Epoch  2 Batch  6 / 525  Training Loss  0.14278215169906616\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  2 Batch  7 / 525  Training Loss  0.15031519532203674\n",
            "Epoch  2 Batch  8 / 525  Training Loss  0.14478909969329834\n",
            "Epoch  2 Batch  9 / 525  Training Loss  0.14865612983703613\n",
            "Epoch  2 Batch  10 / 525  Training Loss  0.16161565482616425\n",
            "Epoch  2 Batch  11 / 525  Training Loss  0.18431456387043\n",
            "Epoch  2 Batch  12 / 525  Training Loss  0.17860335111618042\n",
            "Epoch  2 Batch  13 / 525  Training Loss  0.15535768866539001\n",
            "Epoch  2 Batch  14 / 525  Training Loss  0.15582327544689178\n",
            "Epoch  2 Batch  15 / 525  Training Loss  0.14559680223464966\n",
            "Epoch  2 Batch  16 / 525  Training Loss  0.13121266663074493\n",
            "Epoch  2 Batch  17 / 525  Training Loss  0.14644984900951385\n",
            "Epoch  2 Batch  18 / 525  Training Loss  0.1596994549036026\n",
            "Epoch  2 Batch  19 / 525  Training Loss  0.1345430314540863\n",
            "Epoch  2 Batch  20 / 525  Training Loss  0.14903604984283447\n",
            "Epoch  2 Batch  21 / 525  Training Loss  0.14447443187236786\n",
            "Epoch  2 Batch  22 / 525  Training Loss  0.14734092354774475\n",
            "Epoch  2 Batch  23 / 525  Training Loss  0.17062485218048096\n",
            "Epoch  2 Batch  24 / 525  Training Loss  0.17071077227592468\n",
            "Epoch  2 Batch  25 / 525  Training Loss  0.14403551816940308\n",
            "Epoch  2 Batch  26 / 525  Training Loss  0.15835651755332947\n",
            "Epoch  2 Batch  27 / 525  Training Loss  0.14873892068862915\n",
            "Epoch  2 Batch  28 / 525  Training Loss  0.15401773154735565\n",
            "Epoch  2 Batch  29 / 525  Training Loss  0.14819195866584778\n",
            "Epoch  2 Batch  30 / 525  Training Loss  0.148732990026474\n",
            "Epoch  2 Batch  31 / 525  Training Loss  0.15243445336818695\n",
            "Epoch  2 Batch  32 / 525  Training Loss  0.15088225901126862\n",
            "Epoch  2 Batch  33 / 525  Training Loss  0.15969952940940857\n",
            "Epoch  2 Batch  34 / 525  Training Loss  0.1624251753091812\n",
            "Epoch  2 Batch  35 / 525  Training Loss  0.1342620700597763\n",
            "Epoch  2 Batch  36 / 525  Training Loss  0.1493445336818695\n",
            "Epoch  2 Batch  37 / 525  Training Loss  0.14503568410873413\n",
            "Epoch  2 Batch  38 / 525  Training Loss  0.15716227889060974\n",
            "Epoch  2 Batch  39 / 525  Training Loss  0.16626021265983582\n",
            "Epoch  2 Batch  40 / 525  Training Loss  0.16656175255775452\n",
            "Epoch  2 Batch  41 / 525  Training Loss  0.14925920963287354\n",
            "Epoch  2 Batch  42 / 525  Training Loss  0.14530059695243835\n",
            "Epoch  2 Batch  43 / 525  Training Loss  0.1486091911792755\n",
            "Epoch  2 Batch  44 / 525  Training Loss  0.14091116189956665\n",
            "Epoch  2 Batch  45 / 525  Training Loss  0.16610780358314514\n",
            "Epoch  2 Batch  46 / 525  Training Loss  0.14408008754253387\n",
            "Epoch  2 Batch  47 / 525  Training Loss  0.1263677328824997\n",
            "Epoch  2 Batch  48 / 525  Training Loss  0.14701376855373383\n",
            "Epoch  2 Batch  49 / 525  Training Loss  0.14110393822193146\n",
            "Epoch  2 Batch  50 / 525  Training Loss  0.13982588052749634\n",
            "Epoch  2 Batch  51 / 525  Training Loss  0.1527138352394104\n",
            "Epoch  2 Batch  52 / 525  Training Loss  0.15793487429618835\n",
            "Epoch  2 Batch  53 / 525  Training Loss  0.15485596656799316\n",
            "Epoch  2 Batch  54 / 525  Training Loss  0.14538393914699554\n",
            "Epoch  2 Batch  55 / 525  Training Loss  0.14692534506320953\n",
            "Epoch  2 Batch  56 / 525  Training Loss  0.15030120313167572\n",
            "Epoch  2 Batch  57 / 525  Training Loss  0.14930443465709686\n",
            "Epoch  2 Batch  58 / 525  Training Loss  0.13914154469966888\n",
            "Epoch  2 Batch  59 / 525  Training Loss  0.13997839391231537\n",
            "Epoch  2 Batch  60 / 525  Training Loss  0.1536273956298828\n",
            "Epoch  2 Batch  61 / 525  Training Loss  0.14474257826805115\n",
            "Epoch  2 Batch  62 / 525  Training Loss  0.14743587374687195\n",
            "Epoch  2 Batch  63 / 525  Training Loss  0.1558433622121811\n",
            "Epoch  2 Batch  64 / 525  Training Loss  0.17115959525108337\n",
            "Epoch  2 Batch  65 / 525  Training Loss  0.1656627207994461\n",
            "Epoch  2 Batch  66 / 525  Training Loss  0.153915673494339\n",
            "Epoch  2 Batch  67 / 525  Training Loss  0.16169357299804688\n",
            "Epoch  2 Batch  68 / 525  Training Loss  0.15035897493362427\n",
            "Epoch  2 Batch  69 / 525  Training Loss  0.15678958594799042\n",
            "Epoch  2 Batch  70 / 525  Training Loss  0.13883019983768463\n",
            "Epoch  2 Batch  71 / 525  Training Loss  0.1181749701499939\n",
            "Epoch  2 Batch  72 / 525  Training Loss  0.1495780348777771\n",
            "Epoch  2 Batch  73 / 525  Training Loss  0.11749464273452759\n",
            "Epoch  2 Batch  74 / 525  Training Loss  0.15590263903141022\n",
            "Epoch  2 Batch  75 / 525  Training Loss  0.1509263515472412\n",
            "Epoch  2 Batch  76 / 525  Training Loss  0.13894622027873993\n",
            "Epoch  2 Batch  77 / 525  Training Loss  0.14563366770744324\n",
            "Epoch  2 Batch  78 / 525  Training Loss  0.13347221910953522\n",
            "Epoch  2 Batch  79 / 525  Training Loss  0.13739563524723053\n",
            "Epoch  2 Batch  80 / 525  Training Loss  0.15365739166736603\n",
            "Epoch  2 Batch  81 / 525  Training Loss  0.16962632536888123\n",
            "Epoch  2 Batch  82 / 525  Training Loss  0.1535080373287201\n",
            "Epoch  2 Batch  83 / 525  Training Loss  0.16190889477729797\n",
            "Epoch  2 Batch  84 / 525  Training Loss  0.15576878190040588\n",
            "Epoch  2 Batch  85 / 525  Training Loss  0.14980575442314148\n",
            "Epoch  2 Batch  86 / 525  Training Loss  0.15666744112968445\n",
            "Epoch  2 Batch  87 / 525  Training Loss  0.1448715180158615\n",
            "Epoch  2 Batch  88 / 525  Training Loss  0.1439809650182724\n",
            "Epoch  2 Batch  89 / 525  Training Loss  0.15118630230426788\n",
            "Epoch  2 Batch  90 / 525  Training Loss  0.13245424628257751\n",
            "Epoch  2 Batch  91 / 525  Training Loss  0.14491531252861023\n",
            "Epoch  2 Batch  92 / 525  Training Loss  0.16911852359771729\n",
            "Epoch  2 Batch  93 / 525  Training Loss  0.15006670355796814\n",
            "Epoch  2 Batch  94 / 525  Training Loss  0.15018144249916077\n",
            "Epoch  2 Batch  95 / 525  Training Loss  0.1420026421546936\n",
            "Epoch  2 Batch  96 / 525  Training Loss  0.1301228404045105\n",
            "Epoch  2 Batch  97 / 525  Training Loss  0.161266028881073\n",
            "Epoch  2 Batch  98 / 525  Training Loss  0.1467575579881668\n",
            "Epoch  2 Batch  99 / 525  Training Loss  0.15177033841609955\n",
            "Epoch  2 Batch  100 / 525  Training Loss  0.16223837435245514\n",
            "Epoch  2 Batch  101 / 525  Training Loss  0.13024242222309113\n",
            "Epoch  2 Batch  102 / 525  Training Loss  0.1418682336807251\n",
            "Epoch  2 Batch  103 / 525  Training Loss  0.14761003851890564\n",
            "Epoch  2 Batch  104 / 525  Training Loss  0.1637064516544342\n",
            "Epoch  2 Batch  105 / 525  Training Loss  0.1629100739955902\n",
            "Epoch  2 Batch  106 / 525  Training Loss  0.14940710365772247\n",
            "Epoch  2 Batch  107 / 525  Training Loss  0.12312372028827667\n",
            "Epoch  2 Batch  108 / 525  Training Loss  0.1458549201488495\n",
            "Epoch  2 Batch  109 / 525  Training Loss  0.13972178101539612\n",
            "Epoch  2 Batch  110 / 525  Training Loss  0.14411817491054535\n",
            "Epoch  2 Batch  111 / 525  Training Loss  0.14906997978687286\n",
            "Epoch  2 Batch  112 / 525  Training Loss  0.15511783957481384\n",
            "Epoch  2 Batch  113 / 525  Training Loss  0.1576172262430191\n",
            "Epoch  2 Batch  114 / 525  Training Loss  0.15654918551445007\n",
            "Epoch  2 Batch  115 / 525  Training Loss  0.1487867683172226\n",
            "Epoch  2 Batch  116 / 525  Training Loss  0.1333286613225937\n",
            "Epoch  2 Batch  117 / 525  Training Loss  0.13084545731544495\n",
            "Epoch  2 Batch  118 / 525  Training Loss  0.12681619822978973\n",
            "Epoch  2 Batch  119 / 525  Training Loss  0.14455099403858185\n",
            "Epoch  2 Batch  120 / 525  Training Loss  0.12746864557266235\n",
            "Epoch  2 Batch  121 / 525  Training Loss  0.15163172781467438\n",
            "Epoch  2 Batch  122 / 525  Training Loss  0.12671376764774323\n",
            "Epoch  2 Batch  123 / 525  Training Loss  0.11918701231479645\n",
            "Epoch  2 Batch  124 / 525  Training Loss  0.14948223531246185\n",
            "Epoch  2 Batch  125 / 525  Training Loss  0.13118398189544678\n",
            "Epoch  2 Batch  126 / 525  Training Loss  0.1449415385723114\n",
            "Epoch  2 Batch  127 / 525  Training Loss  0.12907683849334717\n",
            "Epoch  2 Batch  128 / 525  Training Loss  0.11458870023488998\n",
            "Epoch  2 Batch  129 / 525  Training Loss  0.1259978711605072\n",
            "Epoch  2 Batch  130 / 525  Training Loss  0.13398383557796478\n",
            "Epoch  2 Batch  131 / 525  Training Loss  0.12921404838562012\n",
            "Epoch  2 Batch  132 / 525  Training Loss  0.12984956800937653\n",
            "Epoch  2 Batch  133 / 525  Training Loss  0.1403166949748993\n",
            "Epoch  2 Batch  134 / 525  Training Loss  0.12217257916927338\n",
            "Epoch  2 Batch  135 / 525  Training Loss  0.1292446404695511\n",
            "Epoch  2 Batch  136 / 525  Training Loss  0.13769447803497314\n",
            "Epoch  2 Batch  137 / 525  Training Loss  0.13363787531852722\n",
            "Epoch  2 Batch  138 / 525  Training Loss  0.13473764061927795\n",
            "Epoch  2 Batch  139 / 525  Training Loss  0.13224557042121887\n",
            "Epoch  2 Batch  140 / 525  Training Loss  0.13374784588813782\n",
            "Epoch  2 Batch  141 / 525  Training Loss  0.14972037076950073\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  2 Batch  142 / 525  Training Loss  0.1593354046344757\n",
            "Epoch  2 Batch  143 / 525  Training Loss  0.16138797998428345\n",
            "Epoch  2 Batch  144 / 525  Training Loss  0.14771459996700287\n",
            "Epoch  2 Batch  145 / 525  Training Loss  0.16297854483127594\n",
            "Epoch  2 Batch  146 / 525  Training Loss  0.1405731737613678\n",
            "Epoch  2 Batch  147 / 525  Training Loss  0.1513659656047821\n",
            "Epoch  2 Batch  148 / 525  Training Loss  0.13798415660858154\n",
            "Epoch  2 Batch  149 / 525  Training Loss  0.13134905695915222\n",
            "Epoch  2 Batch  150 / 525  Training Loss  0.15069417655467987\n",
            "Epoch  2 Batch  151 / 525  Training Loss  0.138605535030365\n",
            "Epoch  2 Batch  152 / 525  Training Loss  0.13601483404636383\n",
            "Epoch  2 Batch  153 / 525  Training Loss  0.1356518566608429\n",
            "Epoch  2 Batch  154 / 525  Training Loss  0.1445169746875763\n",
            "Epoch  2 Batch  155 / 525  Training Loss  0.12384052574634552\n",
            "Epoch  2 Batch  156 / 525  Training Loss  0.13975587487220764\n",
            "Epoch  2 Batch  157 / 525  Training Loss  0.1347489058971405\n",
            "Epoch  2 Batch  158 / 525  Training Loss  0.11753026396036148\n",
            "Epoch  2 Batch  159 / 525  Training Loss  0.13910043239593506\n",
            "Epoch  2 Batch  160 / 525  Training Loss  0.12851423025131226\n",
            "Epoch  2 Batch  161 / 525  Training Loss  0.14863501489162445\n",
            "Epoch  2 Batch  162 / 525  Training Loss  0.13798698782920837\n",
            "Epoch  2 Batch  163 / 525  Training Loss  0.13180798292160034\n",
            "Epoch  2 Batch  164 / 525  Training Loss  0.13786432147026062\n",
            "Epoch  2 Batch  165 / 525  Training Loss  0.12755702435970306\n",
            "Epoch  2 Batch  166 / 525  Training Loss  0.13209444284439087\n",
            "Epoch  2 Batch  167 / 525  Training Loss  0.13686373829841614\n",
            "Epoch  2 Batch  168 / 525  Training Loss  0.12378690391778946\n",
            "Epoch  2 Batch  169 / 525  Training Loss  0.1456584632396698\n",
            "Epoch  2 Batch  170 / 525  Training Loss  0.12417210638523102\n",
            "Epoch  2 Batch  171 / 525  Training Loss  0.12820596992969513\n",
            "Epoch  2 Batch  172 / 525  Training Loss  0.11806397140026093\n",
            "Epoch  2 Batch  173 / 525  Training Loss  0.12633034586906433\n",
            "Epoch  2 Batch  174 / 525  Training Loss  0.13954724371433258\n",
            "Epoch  2 Batch  175 / 525  Training Loss  0.1388513147830963\n",
            "Epoch  2 Batch  176 / 525  Training Loss  0.1395343691110611\n",
            "Epoch  2 Batch  177 / 525  Training Loss  0.13536295294761658\n",
            "Epoch  2 Batch  178 / 525  Training Loss  0.13819095492362976\n",
            "Epoch  2 Batch  179 / 525  Training Loss  0.13965964317321777\n",
            "Epoch  2 Batch  180 / 525  Training Loss  0.13691924512386322\n",
            "Epoch  2 Batch  181 / 525  Training Loss  0.12732908129692078\n",
            "Epoch  2 Batch  182 / 525  Training Loss  0.13646721839904785\n",
            "Epoch  2 Batch  183 / 525  Training Loss  0.12003549188375473\n",
            "Epoch  2 Batch  184 / 525  Training Loss  0.15247401595115662\n",
            "Epoch  2 Batch  185 / 525  Training Loss  0.12122635543346405\n",
            "Epoch  2 Batch  186 / 525  Training Loss  0.13851262629032135\n",
            "Epoch  2 Batch  187 / 525  Training Loss  0.14049942791461945\n",
            "Epoch  2 Batch  188 / 525  Training Loss  0.1401129961013794\n",
            "Epoch  2 Batch  189 / 525  Training Loss  0.12404765188694\n",
            "Epoch  2 Batch  190 / 525  Training Loss  0.13837416470050812\n",
            "Epoch  2 Batch  191 / 525  Training Loss  0.1358121633529663\n",
            "Epoch  2 Batch  192 / 525  Training Loss  0.13732615113258362\n",
            "Epoch  2 Batch  193 / 525  Training Loss  0.12378604710102081\n",
            "Epoch  2 Batch  194 / 525  Training Loss  0.117704376578331\n",
            "Epoch  2 Batch  195 / 525  Training Loss  0.14875474572181702\n",
            "Epoch  2 Batch  196 / 525  Training Loss  0.1361365169286728\n",
            "Epoch  2 Batch  197 / 525  Training Loss  0.10989385843276978\n",
            "Epoch  2 Batch  198 / 525  Training Loss  0.12910546362400055\n",
            "Epoch  2 Batch  199 / 525  Training Loss  0.11825867742300034\n",
            "Epoch  2 Batch  200 / 525  Training Loss  0.13521486520767212\n",
            "Epoch  2 Batch  201 / 525  Training Loss  0.14578694105148315\n",
            "Epoch  2 Batch  202 / 525  Training Loss  0.126412495970726\n",
            "Epoch  2 Batch  203 / 525  Training Loss  0.141050323843956\n",
            "Epoch  2 Batch  204 / 525  Training Loss  0.1288483887910843\n",
            "Epoch  2 Batch  205 / 525  Training Loss  0.11258206516504288\n",
            "Epoch  2 Batch  206 / 525  Training Loss  0.1413581222295761\n",
            "Epoch  2 Batch  207 / 525  Training Loss  0.11630085855722427\n",
            "Epoch  2 Batch  208 / 525  Training Loss  0.11061497777700424\n",
            "Epoch  2 Batch  209 / 525  Training Loss  0.11113210022449493\n",
            "Epoch  2 Batch  210 / 525  Training Loss  0.12949712574481964\n",
            "Epoch  2 Batch  211 / 525  Training Loss  0.114452064037323\n",
            "Epoch  2 Batch  212 / 525  Training Loss  0.1441054344177246\n",
            "Epoch  2 Batch  213 / 525  Training Loss  0.12630945444107056\n",
            "Epoch  2 Batch  214 / 525  Training Loss  0.12070365250110626\n",
            "Epoch  2 Batch  215 / 525  Training Loss  0.1392577588558197\n",
            "Epoch  2 Batch  216 / 525  Training Loss  0.13122305274009705\n",
            "Epoch  2 Batch  217 / 525  Training Loss  0.1427575796842575\n",
            "Epoch  2 Batch  218 / 525  Training Loss  0.12853077054023743\n",
            "Epoch  2 Batch  219 / 525  Training Loss  0.139026939868927\n",
            "Epoch  2 Batch  220 / 525  Training Loss  0.14394763112068176\n",
            "Epoch  2 Batch  221 / 525  Training Loss  0.14240862429141998\n",
            "Epoch  2 Batch  222 / 525  Training Loss  0.133493572473526\n",
            "Epoch  2 Batch  223 / 525  Training Loss  0.1402747929096222\n",
            "Epoch  2 Batch  224 / 525  Training Loss  0.13271817564964294\n",
            "Epoch  2 Batch  225 / 525  Training Loss  0.11552677303552628\n",
            "Epoch  2 Batch  226 / 525  Training Loss  0.13185101747512817\n",
            "Epoch  2 Batch  227 / 525  Training Loss  0.12361719459295273\n",
            "Epoch  2 Batch  228 / 525  Training Loss  0.13979217410087585\n",
            "Epoch  2 Batch  229 / 525  Training Loss  0.1395874321460724\n",
            "Epoch  2 Batch  230 / 525  Training Loss  0.14773796498775482\n",
            "Epoch  2 Batch  231 / 525  Training Loss  0.12041433155536652\n",
            "Epoch  2 Batch  232 / 525  Training Loss  0.11703784763813019\n",
            "Epoch  2 Batch  233 / 525  Training Loss  0.13902553915977478\n",
            "Epoch  2 Batch  234 / 525  Training Loss  0.14312070608139038\n",
            "Epoch  2 Batch  235 / 525  Training Loss  0.13880738615989685\n",
            "Epoch  2 Batch  236 / 525  Training Loss  0.12276691198348999\n",
            "Epoch  2 Batch  237 / 525  Training Loss  0.12088839709758759\n",
            "Epoch  2 Batch  238 / 525  Training Loss  0.14385241270065308\n",
            "Epoch  2 Batch  239 / 525  Training Loss  0.12783606350421906\n",
            "Epoch  2 Batch  240 / 525  Training Loss  0.13881585001945496\n",
            "Epoch  2 Batch  241 / 525  Training Loss  0.11342743784189224\n",
            "Epoch  2 Batch  242 / 525  Training Loss  0.13229964673519135\n",
            "Epoch  2 Batch  243 / 525  Training Loss  0.1396627277135849\n",
            "Epoch  2 Batch  244 / 525  Training Loss  0.13167549669742584\n",
            "Epoch  2 Batch  245 / 525  Training Loss  0.12728948891162872\n",
            "Epoch  2 Batch  246 / 525  Training Loss  0.12424000352621078\n",
            "Epoch  2 Batch  247 / 525  Training Loss  0.12756936252117157\n",
            "Epoch  2 Batch  248 / 525  Training Loss  0.12592507898807526\n",
            "Epoch  2 Batch  249 / 525  Training Loss  0.12970609962940216\n",
            "Epoch  2 Batch  250 / 525  Training Loss  0.1150241494178772\n",
            "Epoch  2 Batch  251 / 525  Training Loss  0.11068694293498993\n",
            "Epoch  2 Batch  252 / 525  Training Loss  0.113658107817173\n",
            "Epoch  2 Batch  253 / 525  Training Loss  0.13019444048404694\n",
            "Epoch  2 Batch  254 / 525  Training Loss  0.14179937541484833\n",
            "Epoch  2 Batch  255 / 525  Training Loss  0.12345154583454132\n",
            "Epoch  2 Batch  256 / 525  Training Loss  0.13126179575920105\n",
            "Epoch  2 Batch  257 / 525  Training Loss  0.13887573778629303\n",
            "Epoch  2 Batch  258 / 525  Training Loss  0.12950626015663147\n",
            "Epoch  2 Batch  259 / 525  Training Loss  0.12868240475654602\n",
            "Epoch  2 Batch  260 / 525  Training Loss  0.11209283024072647\n",
            "Epoch  2 Batch  261 / 525  Training Loss  0.11709654331207275\n",
            "Epoch  2 Batch  262 / 525  Training Loss  0.11872971057891846\n",
            "Epoch  2 Batch  263 / 525  Training Loss  0.11685027927160263\n",
            "Epoch  2 Batch  264 / 525  Training Loss  0.11860080808401108\n",
            "Epoch  2 Batch  265 / 525  Training Loss  0.1383245885372162\n",
            "Epoch  2 Batch  266 / 525  Training Loss  0.12876394391059875\n",
            "Epoch  2 Batch  267 / 525  Training Loss  0.14300408959388733\n",
            "Epoch  2 Batch  268 / 525  Training Loss  0.13502736389636993\n",
            "Epoch  2 Batch  269 / 525  Training Loss  0.11731065809726715\n",
            "Epoch  2 Batch  270 / 525  Training Loss  0.10946309566497803\n",
            "Epoch  2 Batch  271 / 525  Training Loss  0.11872442066669464\n",
            "Epoch  2 Batch  272 / 525  Training Loss  0.12884293496608734\n",
            "Epoch  2 Batch  273 / 525  Training Loss  0.16637346148490906\n",
            "Epoch  2 Batch  274 / 525  Training Loss  0.14514091610908508\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  2 Batch  275 / 525  Training Loss  0.13355371356010437\n",
            "Epoch  2 Batch  276 / 525  Training Loss  0.12045492976903915\n",
            "Epoch  2 Batch  277 / 525  Training Loss  0.13112345337867737\n",
            "Epoch  2 Batch  278 / 525  Training Loss  0.1413102000951767\n",
            "Epoch  2 Batch  279 / 525  Training Loss  0.13686875998973846\n",
            "Epoch  2 Batch  280 / 525  Training Loss  0.1386284977197647\n",
            "Epoch  2 Batch  281 / 525  Training Loss  0.1329793632030487\n",
            "Epoch  2 Batch  282 / 525  Training Loss  0.12978611886501312\n",
            "Epoch  2 Batch  283 / 525  Training Loss  0.11962024867534637\n",
            "Epoch  2 Batch  284 / 525  Training Loss  0.12392505258321762\n",
            "Epoch  2 Batch  285 / 525  Training Loss  0.13293102383613586\n",
            "Epoch  2 Batch  286 / 525  Training Loss  0.13364620506763458\n",
            "Epoch  2 Batch  287 / 525  Training Loss  0.1204514354467392\n",
            "Epoch  2 Batch  288 / 525  Training Loss  0.114419125020504\n",
            "Epoch  2 Batch  289 / 525  Training Loss  0.12819638848304749\n",
            "Epoch  2 Batch  290 / 525  Training Loss  0.14104297757148743\n",
            "Epoch  2 Batch  291 / 525  Training Loss  0.12836423516273499\n",
            "Epoch  2 Batch  292 / 525  Training Loss  0.13742294907569885\n",
            "Epoch  2 Batch  293 / 525  Training Loss  0.11884864419698715\n",
            "Epoch  2 Batch  294 / 525  Training Loss  0.10719327628612518\n",
            "Epoch  2 Batch  295 / 525  Training Loss  0.12897923588752747\n",
            "Epoch  2 Batch  296 / 525  Training Loss  0.11521909385919571\n",
            "Epoch  2 Batch  297 / 525  Training Loss  0.10192908346652985\n",
            "Epoch  2 Batch  298 / 525  Training Loss  0.11450965702533722\n",
            "Epoch  2 Batch  299 / 525  Training Loss  0.11752539873123169\n",
            "Epoch  2 Batch  300 / 525  Training Loss  0.1261318027973175\n",
            "Epoch  2 Batch  301 / 525  Training Loss  0.11381562799215317\n",
            "Epoch  2 Batch  302 / 525  Training Loss  0.146576926112175\n",
            "Epoch  2 Batch  303 / 525  Training Loss  0.12496761232614517\n",
            "Epoch  2 Batch  304 / 525  Training Loss  0.1214064210653305\n",
            "Epoch  2 Batch  305 / 525  Training Loss  0.12571081519126892\n",
            "Epoch  2 Batch  306 / 525  Training Loss  0.1070118099451065\n",
            "Epoch  2 Batch  307 / 525  Training Loss  0.11770661175251007\n",
            "Epoch  2 Batch  308 / 525  Training Loss  0.11739976704120636\n",
            "Epoch  2 Batch  309 / 525  Training Loss  0.15016357600688934\n",
            "Epoch  2 Batch  310 / 525  Training Loss  0.10995586216449738\n",
            "Epoch  2 Batch  311 / 525  Training Loss  0.10514082759618759\n",
            "Epoch  2 Batch  312 / 525  Training Loss  0.11171158403158188\n",
            "Epoch  2 Batch  313 / 525  Training Loss  0.13784578442573547\n",
            "Epoch  2 Batch  314 / 525  Training Loss  0.12302780151367188\n",
            "Epoch  2 Batch  315 / 525  Training Loss  0.12188409268856049\n",
            "Epoch  2 Batch  316 / 525  Training Loss  0.11603166908025742\n",
            "Epoch  2 Batch  317 / 525  Training Loss  0.13253618776798248\n",
            "Epoch  2 Batch  318 / 525  Training Loss  0.1269141137599945\n",
            "Epoch  2 Batch  319 / 525  Training Loss  0.11846816539764404\n",
            "Epoch  2 Batch  320 / 525  Training Loss  0.12821969389915466\n",
            "Epoch  2 Batch  321 / 525  Training Loss  0.12575499713420868\n",
            "Epoch  2 Batch  322 / 525  Training Loss  0.0953502506017685\n",
            "Epoch  2 Batch  323 / 525  Training Loss  0.12948718667030334\n",
            "Epoch  2 Batch  324 / 525  Training Loss  0.1278831958770752\n",
            "Epoch  2 Batch  325 / 525  Training Loss  0.13319548964500427\n",
            "Epoch  2 Batch  326 / 525  Training Loss  0.11547690629959106\n",
            "Epoch  2 Batch  327 / 525  Training Loss  0.129104882478714\n",
            "Epoch  2 Batch  328 / 525  Training Loss  0.13161946833133698\n",
            "Epoch  2 Batch  329 / 525  Training Loss  0.11769679933786392\n",
            "Epoch  2 Batch  330 / 525  Training Loss  0.1361062228679657\n",
            "Epoch  2 Batch  331 / 525  Training Loss  0.13202305138111115\n",
            "Epoch  2 Batch  332 / 525  Training Loss  0.11768496036529541\n",
            "Epoch  2 Batch  333 / 525  Training Loss  0.13740944862365723\n",
            "Epoch  2 Batch  334 / 525  Training Loss  0.13528664410114288\n",
            "Epoch  2 Batch  335 / 525  Training Loss  0.13311420381069183\n",
            "Epoch  2 Batch  336 / 525  Training Loss  0.14925284683704376\n",
            "Epoch  2 Batch  337 / 525  Training Loss  0.11776152998209\n",
            "Epoch  2 Batch  338 / 525  Training Loss  0.13025079667568207\n",
            "Epoch  2 Batch  339 / 525  Training Loss  0.11666442453861237\n",
            "Epoch  2 Batch  340 / 525  Training Loss  0.10566920042037964\n",
            "Epoch  2 Batch  341 / 525  Training Loss  0.12008102983236313\n",
            "Epoch  2 Batch  342 / 525  Training Loss  0.1348058134317398\n",
            "Epoch  2 Batch  343 / 525  Training Loss  0.12041886150836945\n",
            "Epoch  2 Batch  344 / 525  Training Loss  0.1354779303073883\n",
            "Epoch  2 Batch  345 / 525  Training Loss  0.11299632489681244\n",
            "Epoch  2 Batch  346 / 525  Training Loss  0.1349591165781021\n",
            "Epoch  2 Batch  347 / 525  Training Loss  0.09561479091644287\n",
            "Epoch  2 Batch  348 / 525  Training Loss  0.12718868255615234\n",
            "Epoch  2 Batch  349 / 525  Training Loss  0.12504547834396362\n",
            "Epoch  2 Batch  350 / 525  Training Loss  0.12440906465053558\n",
            "Epoch  2 Batch  351 / 525  Training Loss  0.1153038740158081\n",
            "Epoch  2 Batch  352 / 525  Training Loss  0.1322547197341919\n",
            "Epoch  2 Batch  353 / 525  Training Loss  0.1241946816444397\n",
            "Epoch  2 Batch  354 / 525  Training Loss  0.1275160163640976\n",
            "Epoch  2 Batch  355 / 525  Training Loss  0.11444719135761261\n",
            "Epoch  2 Batch  356 / 525  Training Loss  0.11380839347839355\n",
            "Epoch  2 Batch  357 / 525  Training Loss  0.12239648401737213\n",
            "Epoch  2 Batch  358 / 525  Training Loss  0.12124278396368027\n",
            "Epoch  2 Batch  359 / 525  Training Loss  0.10657205432653427\n",
            "Epoch  2 Batch  360 / 525  Training Loss  0.1073196530342102\n",
            "Epoch  2 Batch  361 / 525  Training Loss  0.10380244255065918\n",
            "Epoch  2 Batch  362 / 525  Training Loss  0.12046658992767334\n",
            "Epoch  2 Batch  363 / 525  Training Loss  0.12400270998477936\n",
            "Epoch  2 Batch  364 / 525  Training Loss  0.12202750146389008\n",
            "Epoch  2 Batch  365 / 525  Training Loss  0.11234532296657562\n",
            "Epoch  2 Batch  366 / 525  Training Loss  0.1046062558889389\n",
            "Epoch  2 Batch  367 / 525  Training Loss  0.15274517238140106\n",
            "Epoch  2 Batch  368 / 525  Training Loss  0.16361768543720245\n",
            "Epoch  2 Batch  369 / 525  Training Loss  0.11968741565942764\n",
            "Epoch  2 Batch  370 / 525  Training Loss  0.13910380005836487\n",
            "Epoch  2 Batch  371 / 525  Training Loss  0.11482536792755127\n",
            "Epoch  2 Batch  372 / 525  Training Loss  0.12919585406780243\n",
            "Epoch  2 Batch  373 / 525  Training Loss  0.11287368834018707\n",
            "Epoch  2 Batch  374 / 525  Training Loss  0.10140721499919891\n",
            "Epoch  2 Batch  375 / 525  Training Loss  0.11934562027454376\n",
            "Epoch  2 Batch  376 / 525  Training Loss  0.10936164855957031\n",
            "Epoch  2 Batch  377 / 525  Training Loss  0.11898569762706757\n",
            "Epoch  2 Batch  378 / 525  Training Loss  0.14065274596214294\n",
            "Epoch  2 Batch  379 / 525  Training Loss  0.12186239659786224\n",
            "Epoch  2 Batch  380 / 525  Training Loss  0.12765586376190186\n",
            "Epoch  2 Batch  381 / 525  Training Loss  0.12659695744514465\n",
            "Epoch  2 Batch  382 / 525  Training Loss  0.128769651055336\n",
            "Epoch  2 Batch  383 / 525  Training Loss  0.13200803101062775\n",
            "Epoch  2 Batch  384 / 525  Training Loss  0.12681718170642853\n",
            "Epoch  2 Batch  385 / 525  Training Loss  0.12075173854827881\n",
            "Epoch  2 Batch  386 / 525  Training Loss  0.10309000313282013\n",
            "Epoch  2 Batch  387 / 525  Training Loss  0.13073846697807312\n",
            "Epoch  2 Batch  388 / 525  Training Loss  0.13148590922355652\n",
            "Epoch  2 Batch  389 / 525  Training Loss  0.1419871747493744\n",
            "Epoch  2 Batch  390 / 525  Training Loss  0.11867161840200424\n",
            "Epoch  2 Batch  391 / 525  Training Loss  0.12509417533874512\n",
            "Epoch  2 Batch  392 / 525  Training Loss  0.12150923907756805\n",
            "Epoch  2 Batch  393 / 525  Training Loss  0.1416238248348236\n",
            "Epoch  2 Batch  394 / 525  Training Loss  0.1279257982969284\n",
            "Epoch  2 Batch  395 / 525  Training Loss  0.10106663405895233\n",
            "Epoch  2 Batch  396 / 525  Training Loss  0.10044656693935394\n",
            "Epoch  2 Batch  397 / 525  Training Loss  0.10517323017120361\n",
            "Epoch  2 Batch  398 / 525  Training Loss  0.08247014880180359\n",
            "Epoch  2 Batch  399 / 525  Training Loss  0.11366339772939682\n",
            "Epoch  2 Batch  400 / 525  Training Loss  0.11374157667160034\n",
            "Epoch  2 Batch  401 / 525  Training Loss  0.11832556873559952\n",
            "Epoch  2 Batch  402 / 525  Training Loss  0.110781729221344\n",
            "Epoch  2 Batch  403 / 525  Training Loss  0.12018345296382904\n",
            "Epoch  2 Batch  404 / 525  Training Loss  0.11682961881160736\n",
            "Epoch  2 Batch  405 / 525  Training Loss  0.12138734757900238\n",
            "Epoch  2 Batch  406 / 525  Training Loss  0.13976052403450012\n",
            "Epoch  2 Batch  407 / 525  Training Loss  0.1059240847826004\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  2 Batch  408 / 525  Training Loss  0.12149244546890259\n",
            "Epoch  2 Batch  409 / 525  Training Loss  0.12670311331748962\n",
            "Epoch  2 Batch  410 / 525  Training Loss  0.12783771753311157\n",
            "Epoch  2 Batch  411 / 525  Training Loss  0.12332173436880112\n",
            "Epoch  2 Batch  412 / 525  Training Loss  0.10676354169845581\n",
            "Epoch  2 Batch  413 / 525  Training Loss  0.12465448677539825\n",
            "Epoch  2 Batch  414 / 525  Training Loss  0.12009800970554352\n",
            "Epoch  2 Batch  415 / 525  Training Loss  0.13649345934391022\n",
            "Epoch  2 Batch  416 / 525  Training Loss  0.13446956872940063\n",
            "Epoch  2 Batch  417 / 525  Training Loss  0.12240760028362274\n",
            "Epoch  2 Batch  418 / 525  Training Loss  0.11123225837945938\n",
            "Epoch  2 Batch  419 / 525  Training Loss  0.11413191258907318\n",
            "Epoch  2 Batch  420 / 525  Training Loss  0.11247130483388901\n",
            "Epoch  2 Batch  421 / 525  Training Loss  0.10708532482385635\n",
            "Epoch  2 Batch  422 / 525  Training Loss  0.12084220349788666\n",
            "Epoch  2 Batch  423 / 525  Training Loss  0.1217917650938034\n",
            "Epoch  2 Batch  424 / 525  Training Loss  0.129496231675148\n",
            "Epoch  2 Batch  425 / 525  Training Loss  0.11748657375574112\n",
            "Epoch  2 Batch  426 / 525  Training Loss  0.114191934466362\n",
            "Epoch  2 Batch  427 / 525  Training Loss  0.14941205084323883\n",
            "Epoch  2 Batch  428 / 525  Training Loss  0.1290576159954071\n",
            "Epoch  2 Batch  429 / 525  Training Loss  0.10059177875518799\n",
            "Epoch  2 Batch  430 / 525  Training Loss  0.10725729167461395\n",
            "Epoch  2 Batch  431 / 525  Training Loss  0.12789860367774963\n",
            "Epoch  2 Batch  432 / 525  Training Loss  0.1300908327102661\n",
            "Epoch  2 Batch  433 / 525  Training Loss  0.12514083087444305\n",
            "Epoch  2 Batch  434 / 525  Training Loss  0.12190202623605728\n",
            "Epoch  2 Batch  435 / 525  Training Loss  0.128867045044899\n",
            "Epoch  2 Batch  436 / 525  Training Loss  0.10631607472896576\n",
            "Epoch  2 Batch  437 / 525  Training Loss  0.12335790693759918\n",
            "Epoch  2 Batch  438 / 525  Training Loss  0.1209723949432373\n",
            "Epoch  2 Batch  439 / 525  Training Loss  0.11063410341739655\n",
            "Epoch  2 Batch  440 / 525  Training Loss  0.11231160163879395\n",
            "Epoch  2 Batch  441 / 525  Training Loss  0.1329231560230255\n",
            "Epoch  2 Batch  442 / 525  Training Loss  0.11451797187328339\n",
            "Epoch  2 Batch  443 / 525  Training Loss  0.12434564530849457\n",
            "Epoch  2 Batch  444 / 525  Training Loss  0.14127670228481293\n",
            "Epoch  2 Batch  445 / 525  Training Loss  0.14293381571769714\n",
            "Epoch  2 Batch  446 / 525  Training Loss  0.13509371876716614\n",
            "Epoch  2 Batch  447 / 525  Training Loss  0.12440580129623413\n",
            "Epoch  2 Batch  448 / 525  Training Loss  0.1274595558643341\n",
            "Epoch  2 Batch  449 / 525  Training Loss  0.1290714293718338\n",
            "Epoch  2 Batch  450 / 525  Training Loss  0.13426482677459717\n",
            "Epoch  2 Batch  451 / 525  Training Loss  0.13301624357700348\n",
            "Epoch  2 Batch  452 / 525  Training Loss  0.10376341640949249\n",
            "Epoch  2 Batch  453 / 525  Training Loss  0.10611075162887573\n",
            "Epoch  2 Batch  454 / 525  Training Loss  0.1025385856628418\n",
            "Epoch  2 Batch  455 / 525  Training Loss  0.11225797981023788\n",
            "Epoch  2 Batch  456 / 525  Training Loss  0.11751041561365128\n",
            "Epoch  2 Batch  457 / 525  Training Loss  0.10185199975967407\n",
            "Epoch  2 Batch  458 / 525  Training Loss  0.11408692598342896\n",
            "Epoch  2 Batch  459 / 525  Training Loss  0.09824948757886887\n",
            "Epoch  2 Batch  460 / 525  Training Loss  0.12254562228918076\n",
            "Epoch  2 Batch  461 / 525  Training Loss  0.11774501949548721\n",
            "Epoch  2 Batch  462 / 525  Training Loss  0.11207316815853119\n",
            "Epoch  2 Batch  463 / 525  Training Loss  0.1191953644156456\n",
            "Epoch  2 Batch  464 / 525  Training Loss  0.12993860244750977\n",
            "Epoch  2 Batch  465 / 525  Training Loss  0.14236430823802948\n",
            "Epoch  2 Batch  466 / 525  Training Loss  0.11551173031330109\n",
            "Epoch  2 Batch  467 / 525  Training Loss  0.1305961310863495\n",
            "Epoch  2 Batch  468 / 525  Training Loss  0.10202831029891968\n",
            "Epoch  2 Batch  469 / 525  Training Loss  0.10470335185527802\n",
            "Epoch  2 Batch  470 / 525  Training Loss  0.11874084174633026\n",
            "Epoch  2 Batch  471 / 525  Training Loss  0.12150897085666656\n",
            "Epoch  2 Batch  472 / 525  Training Loss  0.10959514230489731\n",
            "Epoch  2 Batch  473 / 525  Training Loss  0.11013076454401016\n",
            "Epoch  2 Batch  474 / 525  Training Loss  0.12817899882793427\n",
            "Epoch  2 Batch  475 / 525  Training Loss  0.12240234762430191\n",
            "Epoch  2 Batch  476 / 525  Training Loss  0.1163271814584732\n",
            "Epoch  2 Batch  477 / 525  Training Loss  0.1152501329779625\n",
            "Epoch  2 Batch  478 / 525  Training Loss  0.12117687612771988\n",
            "Epoch  2 Batch  479 / 525  Training Loss  0.12707571685314178\n",
            "Epoch  2 Batch  480 / 525  Training Loss  0.13285663723945618\n",
            "Epoch  2 Batch  481 / 525  Training Loss  0.10538984835147858\n",
            "Epoch  2 Batch  482 / 525  Training Loss  0.12739579379558563\n",
            "Epoch  2 Batch  483 / 525  Training Loss  0.1229156106710434\n",
            "Epoch  2 Batch  484 / 525  Training Loss  0.11665479093790054\n",
            "Epoch  2 Batch  485 / 525  Training Loss  0.12680764496326447\n",
            "Epoch  2 Batch  486 / 525  Training Loss  0.11665336042642593\n",
            "Epoch  2 Batch  487 / 525  Training Loss  0.12306366860866547\n",
            "Epoch  2 Batch  488 / 525  Training Loss  0.1044270247220993\n",
            "Epoch  2 Batch  489 / 525  Training Loss  0.11530493199825287\n",
            "Epoch  2 Batch  490 / 525  Training Loss  0.10899177938699722\n",
            "Epoch  2 Batch  491 / 525  Training Loss  0.12674444913864136\n",
            "Epoch  2 Batch  492 / 525  Training Loss  0.11780204623937607\n",
            "Epoch  2 Batch  493 / 525  Training Loss  0.1180947795510292\n",
            "Epoch  2 Batch  494 / 525  Training Loss  0.0915193110704422\n",
            "Epoch  2 Batch  495 / 525  Training Loss  0.09912814199924469\n",
            "Epoch  2 Batch  496 / 525  Training Loss  0.13638608157634735\n",
            "Epoch  2 Batch  497 / 525  Training Loss  0.1258501261472702\n",
            "Epoch  2 Batch  498 / 525  Training Loss  0.13041725754737854\n",
            "Epoch  2 Batch  499 / 525  Training Loss  0.10967685282230377\n",
            "Epoch  2 Batch  500 / 525  Training Loss  0.11632609367370605\n",
            "Epoch  2 Batch  501 / 525  Training Loss  0.11629464477300644\n",
            "Epoch  2 Batch  502 / 525  Training Loss  0.11207493394613266\n",
            "Epoch  2 Batch  503 / 525  Training Loss  0.12854406237602234\n",
            "Epoch  2 Batch  504 / 525  Training Loss  0.11790002882480621\n",
            "Epoch  2 Batch  505 / 525  Training Loss  0.09914746880531311\n",
            "Epoch  2 Batch  506 / 525  Training Loss  0.12161828577518463\n",
            "Epoch  2 Batch  507 / 525  Training Loss  0.10797126591205597\n",
            "Epoch  2 Batch  508 / 525  Training Loss  0.10746705532073975\n",
            "Epoch  2 Batch  509 / 525  Training Loss  0.1201227530837059\n",
            "Epoch  2 Batch  510 / 525  Training Loss  0.1297207772731781\n",
            "Epoch  2 Batch  511 / 525  Training Loss  0.12538060545921326\n",
            "Epoch  2 Batch  512 / 525  Training Loss  0.11637371778488159\n",
            "Epoch  2 Batch  513 / 525  Training Loss  0.1272169053554535\n",
            "Epoch  2 Batch  514 / 525  Training Loss  0.10446707904338837\n",
            "Epoch  2 Batch  515 / 525  Training Loss  0.11727658659219742\n",
            "Epoch  2 Batch  516 / 525  Training Loss  0.10898522287607193\n",
            "Epoch  2 Batch  517 / 525  Training Loss  0.09501107037067413\n",
            "Epoch  2 Batch  518 / 525  Training Loss  0.11269228160381317\n",
            "Epoch  2 Batch  519 / 525  Training Loss  0.09763012826442719\n",
            "Epoch  2 Batch  520 / 525  Training Loss  0.11029736697673798\n",
            "Epoch  2 Batch  521 / 525  Training Loss  0.09462223947048187\n",
            "Epoch  2 Batch  522 / 525  Training Loss  0.10517294704914093\n",
            "Epoch  2 Batch  523 / 525  Training Loss  0.13043458759784698\n",
            "Epoch  2 Batch  524 / 525  Training Loss  0.11618302762508392\n",
            "   3    |    -    |   0.130753   | 23.016667\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 3\n",
            "Epoch  3 Batch  0 / 525  Training Loss  0.10264813899993896\n",
            "Epoch  3 Batch  1 / 525  Training Loss  0.11370344460010529\n",
            "Epoch  3 Batch  2 / 525  Training Loss  0.11921532452106476\n",
            "Epoch  3 Batch  3 / 525  Training Loss  0.12250791490077972\n",
            "Epoch  3 Batch  4 / 525  Training Loss  0.11268898099660873\n",
            "Epoch  3 Batch  5 / 525  Training Loss  0.10528900474309921\n",
            "Epoch  3 Batch  6 / 525  Training Loss  0.08721044659614563\n",
            "Epoch  3 Batch  7 / 525  Training Loss  0.08708040416240692\n",
            "Epoch  3 Batch  8 / 525  Training Loss  0.11905020475387573\n",
            "Epoch  3 Batch  9 / 525  Training Loss  0.10485240072011948\n",
            "Epoch  3 Batch  10 / 525  Training Loss  0.10952216386795044\n",
            "Epoch  3 Batch  11 / 525  Training Loss  0.12781202793121338\n",
            "Epoch  3 Batch  12 / 525  Training Loss  0.11705575883388519\n",
            "Epoch  3 Batch  13 / 525  Training Loss  0.11196942627429962\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  3 Batch  14 / 525  Training Loss  0.10829305648803711\n",
            "Epoch  3 Batch  15 / 525  Training Loss  0.11267556250095367\n",
            "Epoch  3 Batch  16 / 525  Training Loss  0.13207462430000305\n",
            "Epoch  3 Batch  17 / 525  Training Loss  0.0962919145822525\n",
            "Epoch  3 Batch  18 / 525  Training Loss  0.10069688409566879\n",
            "Epoch  3 Batch  19 / 525  Training Loss  0.13677799701690674\n",
            "Epoch  3 Batch  20 / 525  Training Loss  0.11137393862009048\n",
            "Epoch  3 Batch  21 / 525  Training Loss  0.11121498048305511\n",
            "Epoch  3 Batch  22 / 525  Training Loss  0.1123281940817833\n",
            "Epoch  3 Batch  23 / 525  Training Loss  0.12576982378959656\n",
            "Epoch  3 Batch  24 / 525  Training Loss  0.10545192658901215\n",
            "Epoch  3 Batch  25 / 525  Training Loss  0.09721538424491882\n",
            "Epoch  3 Batch  26 / 525  Training Loss  0.09645761549472809\n",
            "Epoch  3 Batch  27 / 525  Training Loss  0.12126225233078003\n",
            "Epoch  3 Batch  28 / 525  Training Loss  0.10882551968097687\n",
            "Epoch  3 Batch  29 / 525  Training Loss  0.12015102058649063\n",
            "Epoch  3 Batch  30 / 525  Training Loss  0.09956775605678558\n",
            "Epoch  3 Batch  31 / 525  Training Loss  0.119676373898983\n",
            "Epoch  3 Batch  32 / 525  Training Loss  0.12161823362112045\n",
            "Epoch  3 Batch  33 / 525  Training Loss  0.1052975058555603\n",
            "Epoch  3 Batch  34 / 525  Training Loss  0.10181621462106705\n",
            "Epoch  3 Batch  35 / 525  Training Loss  0.13688234984874725\n",
            "Epoch  3 Batch  36 / 525  Training Loss  0.12389206886291504\n",
            "Epoch  3 Batch  37 / 525  Training Loss  0.13696642220020294\n",
            "Epoch  3 Batch  38 / 525  Training Loss  0.12603093683719635\n",
            "Epoch  3 Batch  39 / 525  Training Loss  0.10557615756988525\n",
            "Epoch  3 Batch  40 / 525  Training Loss  0.09997332841157913\n",
            "Epoch  3 Batch  41 / 525  Training Loss  0.1269986629486084\n",
            "Epoch  3 Batch  42 / 525  Training Loss  0.10931932926177979\n",
            "Epoch  3 Batch  43 / 525  Training Loss  0.11005532741546631\n",
            "Epoch  3 Batch  44 / 525  Training Loss  0.09310737997293472\n",
            "Epoch  3 Batch  45 / 525  Training Loss  0.12528428435325623\n",
            "Epoch  3 Batch  46 / 525  Training Loss  0.09492306411266327\n",
            "Epoch  3 Batch  47 / 525  Training Loss  0.10835901647806168\n",
            "Epoch  3 Batch  48 / 525  Training Loss  0.10742566734552383\n",
            "Epoch  3 Batch  49 / 525  Training Loss  0.09280141443014145\n",
            "Epoch  3 Batch  50 / 525  Training Loss  0.10761209577322006\n",
            "Epoch  3 Batch  51 / 525  Training Loss  0.1108817607164383\n",
            "Epoch  3 Batch  52 / 525  Training Loss  0.11378534883260727\n",
            "Epoch  3 Batch  53 / 525  Training Loss  0.118805430829525\n",
            "Epoch  3 Batch  54 / 525  Training Loss  0.1333007961511612\n",
            "Epoch  3 Batch  55 / 525  Training Loss  0.09724987298250198\n",
            "Epoch  3 Batch  56 / 525  Training Loss  0.10470036417245865\n",
            "Epoch  3 Batch  57 / 525  Training Loss  0.12512212991714478\n",
            "Epoch  3 Batch  58 / 525  Training Loss  0.11873376369476318\n",
            "Epoch  3 Batch  59 / 525  Training Loss  0.09270359575748444\n",
            "Epoch  3 Batch  60 / 525  Training Loss  0.10729177296161652\n",
            "Epoch  3 Batch  61 / 525  Training Loss  0.10016965866088867\n",
            "Epoch  3 Batch  62 / 525  Training Loss  0.11687131971120834\n",
            "Epoch  3 Batch  63 / 525  Training Loss  0.11041395366191864\n",
            "Epoch  3 Batch  64 / 525  Training Loss  0.10480831563472748\n",
            "Epoch  3 Batch  65 / 525  Training Loss  0.11358700692653656\n",
            "Epoch  3 Batch  66 / 525  Training Loss  0.11141513288021088\n",
            "Epoch  3 Batch  67 / 525  Training Loss  0.1234753280878067\n",
            "Epoch  3 Batch  68 / 525  Training Loss  0.11932836472988129\n",
            "Epoch  3 Batch  69 / 525  Training Loss  0.1102164015173912\n",
            "Epoch  3 Batch  70 / 525  Training Loss  0.1242779940366745\n",
            "Epoch  3 Batch  71 / 525  Training Loss  0.13182537257671356\n",
            "Epoch  3 Batch  72 / 525  Training Loss  0.1121203675866127\n",
            "Epoch  3 Batch  73 / 525  Training Loss  0.10785172134637833\n",
            "Epoch  3 Batch  74 / 525  Training Loss  0.10470534861087799\n",
            "Epoch  3 Batch  75 / 525  Training Loss  0.09500598907470703\n",
            "Epoch  3 Batch  76 / 525  Training Loss  0.09964089095592499\n",
            "Epoch  3 Batch  77 / 525  Training Loss  0.13245980441570282\n",
            "Epoch  3 Batch  78 / 525  Training Loss  0.10358236730098724\n",
            "Epoch  3 Batch  79 / 525  Training Loss  0.09389185160398483\n",
            "Epoch  3 Batch  80 / 525  Training Loss  0.1244335025548935\n",
            "Epoch  3 Batch  81 / 525  Training Loss  0.11761970818042755\n",
            "Epoch  3 Batch  82 / 525  Training Loss  0.1188756451010704\n",
            "Epoch  3 Batch  83 / 525  Training Loss  0.124616838991642\n",
            "Epoch  3 Batch  84 / 525  Training Loss  0.1188955157995224\n",
            "Epoch  3 Batch  85 / 525  Training Loss  0.09143191576004028\n",
            "Epoch  3 Batch  86 / 525  Training Loss  0.09928986430168152\n",
            "Epoch  3 Batch  87 / 525  Training Loss  0.11746637523174286\n",
            "Epoch  3 Batch  88 / 525  Training Loss  0.11083139479160309\n",
            "Epoch  3 Batch  89 / 525  Training Loss  0.10455340147018433\n",
            "Epoch  3 Batch  90 / 525  Training Loss  0.1237139105796814\n",
            "Epoch  3 Batch  91 / 525  Training Loss  0.13020500540733337\n",
            "Epoch  3 Batch  92 / 525  Training Loss  0.11805818229913712\n",
            "Epoch  3 Batch  93 / 525  Training Loss  0.11776862293481827\n",
            "Epoch  3 Batch  94 / 525  Training Loss  0.09273017197847366\n",
            "Epoch  3 Batch  95 / 525  Training Loss  0.11592531204223633\n",
            "Epoch  3 Batch  96 / 525  Training Loss  0.09220815449953079\n",
            "Epoch  3 Batch  97 / 525  Training Loss  0.11631952226161957\n",
            "Epoch  3 Batch  98 / 525  Training Loss  0.10254700481891632\n",
            "Epoch  3 Batch  99 / 525  Training Loss  0.12203265726566315\n",
            "Epoch  3 Batch  100 / 525  Training Loss  0.09317545592784882\n",
            "Epoch  3 Batch  101 / 525  Training Loss  0.1027757078409195\n",
            "Epoch  3 Batch  102 / 525  Training Loss  0.09848569333553314\n",
            "Epoch  3 Batch  103 / 525  Training Loss  0.09990732371807098\n",
            "Epoch  3 Batch  104 / 525  Training Loss  0.14799650013446808\n",
            "Epoch  3 Batch  105 / 525  Training Loss  0.10441996902227402\n",
            "Epoch  3 Batch  106 / 525  Training Loss  0.10721059888601303\n",
            "Epoch  3 Batch  107 / 525  Training Loss  0.1178535595536232\n",
            "Epoch  3 Batch  108 / 525  Training Loss  0.10013341903686523\n",
            "Epoch  3 Batch  109 / 525  Training Loss  0.11493663489818573\n",
            "Epoch  3 Batch  110 / 525  Training Loss  0.11130489408969879\n",
            "Epoch  3 Batch  111 / 525  Training Loss  0.096816286444664\n",
            "Epoch  3 Batch  112 / 525  Training Loss  0.11120814085006714\n",
            "Epoch  3 Batch  113 / 525  Training Loss  0.10536718368530273\n",
            "Epoch  3 Batch  114 / 525  Training Loss  0.08854884654283524\n",
            "Epoch  3 Batch  115 / 525  Training Loss  0.10576239973306656\n",
            "Epoch  3 Batch  116 / 525  Training Loss  0.10657745599746704\n",
            "Epoch  3 Batch  117 / 525  Training Loss  0.10059647262096405\n",
            "Epoch  3 Batch  118 / 525  Training Loss  0.10320110619068146\n",
            "Epoch  3 Batch  119 / 525  Training Loss  0.1245744377374649\n",
            "Epoch  3 Batch  120 / 525  Training Loss  0.1132459044456482\n",
            "Epoch  3 Batch  121 / 525  Training Loss  0.09942088276147842\n",
            "Epoch  3 Batch  122 / 525  Training Loss  0.11664174497127533\n",
            "Epoch  3 Batch  123 / 525  Training Loss  0.1033463105559349\n",
            "Epoch  3 Batch  124 / 525  Training Loss  0.1306036114692688\n",
            "Epoch  3 Batch  125 / 525  Training Loss  0.12626326084136963\n",
            "Epoch  3 Batch  126 / 525  Training Loss  0.11497917026281357\n",
            "Epoch  3 Batch  127 / 525  Training Loss  0.10548339039087296\n",
            "Epoch  3 Batch  128 / 525  Training Loss  0.1157713308930397\n",
            "Epoch  3 Batch  129 / 525  Training Loss  0.10921672731637955\n",
            "Epoch  3 Batch  130 / 525  Training Loss  0.08593489229679108\n",
            "Epoch  3 Batch  131 / 525  Training Loss  0.0903649777173996\n",
            "Epoch  3 Batch  132 / 525  Training Loss  0.11089255660772324\n",
            "Epoch  3 Batch  133 / 525  Training Loss  0.10380818694829941\n",
            "Epoch  3 Batch  134 / 525  Training Loss  0.10092660039663315\n",
            "Epoch  3 Batch  135 / 525  Training Loss  0.10296845436096191\n",
            "Epoch  3 Batch  136 / 525  Training Loss  0.10715384781360626\n",
            "Epoch  3 Batch  137 / 525  Training Loss  0.09826923906803131\n",
            "Epoch  3 Batch  138 / 525  Training Loss  0.09712692350149155\n",
            "Epoch  3 Batch  139 / 525  Training Loss  0.09387976676225662\n",
            "Epoch  3 Batch  140 / 525  Training Loss  0.09587479382753372\n",
            "Epoch  3 Batch  141 / 525  Training Loss  0.11153393983840942\n",
            "Epoch  3 Batch  142 / 525  Training Loss  0.10197187960147858\n",
            "Epoch  3 Batch  143 / 525  Training Loss  0.1030348539352417\n",
            "Epoch  3 Batch  144 / 525  Training Loss  0.12995006144046783\n",
            "Epoch  3 Batch  145 / 525  Training Loss  0.11542632430791855\n",
            "Epoch  3 Batch  146 / 525  Training Loss  0.11280156672000885\n",
            "Epoch  3 Batch  147 / 525  Training Loss  0.10515318810939789\n",
            "Epoch  3 Batch  148 / 525  Training Loss  0.09112288802862167\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  3 Batch  149 / 525  Training Loss  0.11249037832021713\n",
            "Epoch  3 Batch  150 / 525  Training Loss  0.11082905530929565\n",
            "Epoch  3 Batch  151 / 525  Training Loss  0.10855734348297119\n",
            "Epoch  3 Batch  152 / 525  Training Loss  0.10378563404083252\n",
            "Epoch  3 Batch  153 / 525  Training Loss  0.11405191570520401\n",
            "Epoch  3 Batch  154 / 525  Training Loss  0.11080865561962128\n",
            "Epoch  3 Batch  155 / 525  Training Loss  0.12272725254297256\n",
            "Epoch  3 Batch  156 / 525  Training Loss  0.1056206002831459\n",
            "Epoch  3 Batch  157 / 525  Training Loss  0.10050211101770401\n",
            "Epoch  3 Batch  158 / 525  Training Loss  0.10281185060739517\n",
            "Epoch  3 Batch  159 / 525  Training Loss  0.11202877759933472\n",
            "Epoch  3 Batch  160 / 525  Training Loss  0.1048494428396225\n",
            "Epoch  3 Batch  161 / 525  Training Loss  0.11989549547433853\n",
            "Epoch  3 Batch  162 / 525  Training Loss  0.12637017667293549\n",
            "Epoch  3 Batch  163 / 525  Training Loss  0.11507872492074966\n",
            "Epoch  3 Batch  164 / 525  Training Loss  0.10187296569347382\n",
            "Epoch  3 Batch  165 / 525  Training Loss  0.09455295652151108\n",
            "Epoch  3 Batch  166 / 525  Training Loss  0.10333703458309174\n",
            "Epoch  3 Batch  167 / 525  Training Loss  0.11153008043766022\n",
            "Epoch  3 Batch  168 / 525  Training Loss  0.12222293764352798\n",
            "Epoch  3 Batch  169 / 525  Training Loss  0.09912268072366714\n",
            "Epoch  3 Batch  170 / 525  Training Loss  0.13192284107208252\n",
            "Epoch  3 Batch  171 / 525  Training Loss  0.1254568099975586\n",
            "Epoch  3 Batch  172 / 525  Training Loss  0.10524250566959381\n",
            "Epoch  3 Batch  173 / 525  Training Loss  0.1097683310508728\n",
            "Epoch  3 Batch  174 / 525  Training Loss  0.10706628859043121\n",
            "Epoch  3 Batch  175 / 525  Training Loss  0.1059354692697525\n",
            "Epoch  3 Batch  176 / 525  Training Loss  0.10674679279327393\n",
            "Epoch  3 Batch  177 / 525  Training Loss  0.10994625091552734\n",
            "Epoch  3 Batch  178 / 525  Training Loss  0.11640654504299164\n",
            "Epoch  3 Batch  179 / 525  Training Loss  0.08877109736204147\n",
            "Epoch  3 Batch  180 / 525  Training Loss  0.10087265819311142\n",
            "Epoch  3 Batch  181 / 525  Training Loss  0.09780506789684296\n",
            "Epoch  3 Batch  182 / 525  Training Loss  0.11159203201532364\n",
            "Epoch  3 Batch  183 / 525  Training Loss  0.10501161962747574\n",
            "Epoch  3 Batch  184 / 525  Training Loss  0.09895599633455276\n",
            "Epoch  3 Batch  185 / 525  Training Loss  0.12314591556787491\n",
            "Epoch  3 Batch  186 / 525  Training Loss  0.13255129754543304\n",
            "Epoch  3 Batch  187 / 525  Training Loss  0.09360633790493011\n",
            "Epoch  3 Batch  188 / 525  Training Loss  0.10757161676883698\n",
            "Epoch  3 Batch  189 / 525  Training Loss  0.0902848169207573\n",
            "Epoch  3 Batch  190 / 525  Training Loss  0.08801735192537308\n",
            "Epoch  3 Batch  191 / 525  Training Loss  0.10979137569665909\n",
            "Epoch  3 Batch  192 / 525  Training Loss  0.11671682447195053\n",
            "Epoch  3 Batch  193 / 525  Training Loss  0.11166594922542572\n",
            "Epoch  3 Batch  194 / 525  Training Loss  0.09746827185153961\n",
            "Epoch  3 Batch  195 / 525  Training Loss  0.10789512097835541\n",
            "Epoch  3 Batch  196 / 525  Training Loss  0.10129870474338531\n",
            "Epoch  3 Batch  197 / 525  Training Loss  0.10725851356983185\n",
            "Epoch  3 Batch  198 / 525  Training Loss  0.12045301496982574\n",
            "Epoch  3 Batch  199 / 525  Training Loss  0.11865918338298798\n",
            "Epoch  3 Batch  200 / 525  Training Loss  0.1265927255153656\n",
            "Epoch  3 Batch  201 / 525  Training Loss  0.10454340279102325\n",
            "Epoch  3 Batch  202 / 525  Training Loss  0.08769125491380692\n",
            "Epoch  3 Batch  203 / 525  Training Loss  0.11597887426614761\n",
            "Epoch  3 Batch  204 / 525  Training Loss  0.10567520558834076\n",
            "Epoch  3 Batch  205 / 525  Training Loss  0.1118418425321579\n",
            "Epoch  3 Batch  206 / 525  Training Loss  0.10542750358581543\n",
            "Epoch  3 Batch  207 / 525  Training Loss  0.09241704642772675\n",
            "Epoch  3 Batch  208 / 525  Training Loss  0.11270755529403687\n",
            "Epoch  3 Batch  209 / 525  Training Loss  0.10572972148656845\n",
            "Epoch  3 Batch  210 / 525  Training Loss  0.11199726164340973\n",
            "Epoch  3 Batch  211 / 525  Training Loss  0.089194156229496\n",
            "Epoch  3 Batch  212 / 525  Training Loss  0.10503468662500381\n",
            "Epoch  3 Batch  213 / 525  Training Loss  0.1027410626411438\n",
            "Epoch  3 Batch  214 / 525  Training Loss  0.09543538838624954\n",
            "Epoch  3 Batch  215 / 525  Training Loss  0.09900464117527008\n",
            "Epoch  3 Batch  216 / 525  Training Loss  0.0897153988480568\n",
            "Epoch  3 Batch  217 / 525  Training Loss  0.10801714658737183\n",
            "Epoch  3 Batch  218 / 525  Training Loss  0.10877592861652374\n",
            "Epoch  3 Batch  219 / 525  Training Loss  0.10419255495071411\n",
            "Epoch  3 Batch  220 / 525  Training Loss  0.1276862919330597\n",
            "Epoch  3 Batch  221 / 525  Training Loss  0.14264196157455444\n",
            "Epoch  3 Batch  222 / 525  Training Loss  0.10597576200962067\n",
            "Epoch  3 Batch  223 / 525  Training Loss  0.0932377502322197\n",
            "Epoch  3 Batch  224 / 525  Training Loss  0.10387452691793442\n",
            "Epoch  3 Batch  225 / 525  Training Loss  0.10219724476337433\n",
            "Epoch  3 Batch  226 / 525  Training Loss  0.10535427182912827\n",
            "Epoch  3 Batch  227 / 525  Training Loss  0.11794298887252808\n",
            "Epoch  3 Batch  228 / 525  Training Loss  0.0905647724866867\n",
            "Epoch  3 Batch  229 / 525  Training Loss  0.11084461212158203\n",
            "Epoch  3 Batch  230 / 525  Training Loss  0.09930877387523651\n",
            "Epoch  3 Batch  231 / 525  Training Loss  0.10874591767787933\n",
            "Epoch  3 Batch  232 / 525  Training Loss  0.1119433268904686\n",
            "Epoch  3 Batch  233 / 525  Training Loss  0.11646340787410736\n",
            "Epoch  3 Batch  234 / 525  Training Loss  0.1268264800310135\n",
            "Epoch  3 Batch  235 / 525  Training Loss  0.09298954159021378\n",
            "Epoch  3 Batch  236 / 525  Training Loss  0.12707313895225525\n",
            "Epoch  3 Batch  237 / 525  Training Loss  0.10585472732782364\n",
            "Epoch  3 Batch  238 / 525  Training Loss  0.11117559671401978\n",
            "Epoch  3 Batch  239 / 525  Training Loss  0.08719794452190399\n",
            "Epoch  3 Batch  240 / 525  Training Loss  0.08699215948581696\n",
            "Epoch  3 Batch  241 / 525  Training Loss  0.10010115802288055\n",
            "Epoch  3 Batch  242 / 525  Training Loss  0.10272608697414398\n",
            "Epoch  3 Batch  243 / 525  Training Loss  0.1200353279709816\n",
            "Epoch  3 Batch  244 / 525  Training Loss  0.08722835779190063\n",
            "Epoch  3 Batch  245 / 525  Training Loss  0.10565761476755142\n",
            "Epoch  3 Batch  246 / 525  Training Loss  0.10397710651159286\n",
            "Epoch  3 Batch  247 / 525  Training Loss  0.11541694402694702\n",
            "Epoch  3 Batch  248 / 525  Training Loss  0.10406129062175751\n",
            "Epoch  3 Batch  249 / 525  Training Loss  0.10052721202373505\n",
            "Epoch  3 Batch  250 / 525  Training Loss  0.11447937786579132\n",
            "Epoch  3 Batch  251 / 525  Training Loss  0.11199267208576202\n",
            "Epoch  3 Batch  252 / 525  Training Loss  0.1039855107665062\n",
            "Epoch  3 Batch  253 / 525  Training Loss  0.11085263639688492\n",
            "Epoch  3 Batch  254 / 525  Training Loss  0.10294251143932343\n",
            "Epoch  3 Batch  255 / 525  Training Loss  0.10834316164255142\n",
            "Epoch  3 Batch  256 / 525  Training Loss  0.1177377924323082\n",
            "Epoch  3 Batch  257 / 525  Training Loss  0.11365608870983124\n",
            "Epoch  3 Batch  258 / 525  Training Loss  0.10994143784046173\n",
            "Epoch  3 Batch  259 / 525  Training Loss  0.09521618485450745\n",
            "Epoch  3 Batch  260 / 525  Training Loss  0.10153470188379288\n",
            "Epoch  3 Batch  261 / 525  Training Loss  0.09891152381896973\n",
            "Epoch  3 Batch  262 / 525  Training Loss  0.10448293387889862\n",
            "Epoch  3 Batch  263 / 525  Training Loss  0.09823150932788849\n",
            "Epoch  3 Batch  264 / 525  Training Loss  0.09459395706653595\n",
            "Epoch  3 Batch  265 / 525  Training Loss  0.11219578981399536\n",
            "Epoch  3 Batch  266 / 525  Training Loss  0.11548706144094467\n",
            "Epoch  3 Batch  267 / 525  Training Loss  0.11163987219333649\n",
            "Epoch  3 Batch  268 / 525  Training Loss  0.10848145186901093\n",
            "Epoch  3 Batch  269 / 525  Training Loss  0.11931683123111725\n",
            "Epoch  3 Batch  270 / 525  Training Loss  0.11573299020528793\n",
            "Epoch  3 Batch  271 / 525  Training Loss  0.1189684271812439\n",
            "Epoch  3 Batch  272 / 525  Training Loss  0.10465089231729507\n",
            "Epoch  3 Batch  273 / 525  Training Loss  0.10140879452228546\n",
            "Epoch  3 Batch  274 / 525  Training Loss  0.09205564111471176\n",
            "Epoch  3 Batch  275 / 525  Training Loss  0.08444362133741379\n",
            "Epoch  3 Batch  276 / 525  Training Loss  0.11218880116939545\n",
            "Epoch  3 Batch  277 / 525  Training Loss  0.14711321890354156\n",
            "Epoch  3 Batch  278 / 525  Training Loss  0.11995361000299454\n",
            "Epoch  3 Batch  279 / 525  Training Loss  0.10596083104610443\n",
            "Epoch  3 Batch  280 / 525  Training Loss  0.11699847131967545\n",
            "Epoch  3 Batch  281 / 525  Training Loss  0.13146157562732697\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  3 Batch  282 / 525  Training Loss  0.08771055936813354\n",
            "Epoch  3 Batch  283 / 525  Training Loss  0.1237582191824913\n",
            "Epoch  3 Batch  284 / 525  Training Loss  0.10531125217676163\n",
            "Epoch  3 Batch  285 / 525  Training Loss  0.10327378660440445\n",
            "Epoch  3 Batch  286 / 525  Training Loss  0.10124123096466064\n",
            "Epoch  3 Batch  287 / 525  Training Loss  0.09974433481693268\n",
            "Epoch  3 Batch  288 / 525  Training Loss  0.1022663563489914\n",
            "Epoch  3 Batch  289 / 525  Training Loss  0.10844864696264267\n",
            "Epoch  3 Batch  290 / 525  Training Loss  0.11781448125839233\n",
            "Epoch  3 Batch  291 / 525  Training Loss  0.11027703434228897\n",
            "Epoch  3 Batch  292 / 525  Training Loss  0.10229114443063736\n",
            "Epoch  3 Batch  293 / 525  Training Loss  0.10588494688272476\n",
            "Epoch  3 Batch  294 / 525  Training Loss  0.11901652812957764\n",
            "Epoch  3 Batch  295 / 525  Training Loss  0.10002310574054718\n",
            "Epoch  3 Batch  296 / 525  Training Loss  0.10660061985254288\n",
            "Epoch  3 Batch  297 / 525  Training Loss  0.10045097023248672\n",
            "Epoch  3 Batch  298 / 525  Training Loss  0.10302184522151947\n",
            "Epoch  3 Batch  299 / 525  Training Loss  0.10484334081411362\n",
            "Epoch  3 Batch  300 / 525  Training Loss  0.11920998990535736\n",
            "Epoch  3 Batch  301 / 525  Training Loss  0.11543048918247223\n",
            "Epoch  3 Batch  302 / 525  Training Loss  0.09552771598100662\n",
            "Epoch  3 Batch  303 / 525  Training Loss  0.10876180976629257\n",
            "Epoch  3 Batch  304 / 525  Training Loss  0.09107120335102081\n",
            "Epoch  3 Batch  305 / 525  Training Loss  0.11914785951375961\n",
            "Epoch  3 Batch  306 / 525  Training Loss  0.09515957534313202\n",
            "Epoch  3 Batch  307 / 525  Training Loss  0.11130154132843018\n",
            "Epoch  3 Batch  308 / 525  Training Loss  0.09761343896389008\n",
            "Epoch  3 Batch  309 / 525  Training Loss  0.12049337476491928\n",
            "Epoch  3 Batch  310 / 525  Training Loss  0.11951277405023575\n",
            "Epoch  3 Batch  311 / 525  Training Loss  0.09713143110275269\n",
            "Epoch  3 Batch  312 / 525  Training Loss  0.09467265754938126\n",
            "Epoch  3 Batch  313 / 525  Training Loss  0.11128576099872589\n",
            "Epoch  3 Batch  314 / 525  Training Loss  0.09727457165718079\n",
            "Epoch  3 Batch  315 / 525  Training Loss  0.10802265256643295\n",
            "Epoch  3 Batch  316 / 525  Training Loss  0.11214631795883179\n",
            "Epoch  3 Batch  317 / 525  Training Loss  0.08831937611103058\n",
            "Epoch  3 Batch  318 / 525  Training Loss  0.1053069457411766\n",
            "Epoch  3 Batch  319 / 525  Training Loss  0.11541438102722168\n",
            "Epoch  3 Batch  320 / 525  Training Loss  0.11463107913732529\n",
            "Epoch  3 Batch  321 / 525  Training Loss  0.1109166294336319\n",
            "Epoch  3 Batch  322 / 525  Training Loss  0.08832208812236786\n",
            "Epoch  3 Batch  323 / 525  Training Loss  0.0968959853053093\n",
            "Epoch  3 Batch  324 / 525  Training Loss  0.11440293490886688\n",
            "Epoch  3 Batch  325 / 525  Training Loss  0.09798728674650192\n",
            "Epoch  3 Batch  326 / 525  Training Loss  0.09889320284128189\n",
            "Epoch  3 Batch  327 / 525  Training Loss  0.12986193597316742\n",
            "Epoch  3 Batch  328 / 525  Training Loss  0.10122319310903549\n",
            "Epoch  3 Batch  329 / 525  Training Loss  0.10153184831142426\n",
            "Epoch  3 Batch  330 / 525  Training Loss  0.10618361085653305\n",
            "Epoch  3 Batch  331 / 525  Training Loss  0.0971732884645462\n",
            "Epoch  3 Batch  332 / 525  Training Loss  0.102547787129879\n",
            "Epoch  3 Batch  333 / 525  Training Loss  0.11183252185583115\n",
            "Epoch  3 Batch  334 / 525  Training Loss  0.10977302491664886\n",
            "Epoch  3 Batch  335 / 525  Training Loss  0.10091568529605865\n",
            "Epoch  3 Batch  336 / 525  Training Loss  0.09720989316701889\n",
            "Epoch  3 Batch  337 / 525  Training Loss  0.10001691430807114\n",
            "Epoch  3 Batch  338 / 525  Training Loss  0.0971132442355156\n",
            "Epoch  3 Batch  339 / 525  Training Loss  0.08672765642404556\n",
            "Epoch  3 Batch  340 / 525  Training Loss  0.10895965993404388\n",
            "Epoch  3 Batch  341 / 525  Training Loss  0.11275599896907806\n",
            "Epoch  3 Batch  342 / 525  Training Loss  0.0688713937997818\n",
            "Epoch  3 Batch  343 / 525  Training Loss  0.09076286852359772\n",
            "Epoch  3 Batch  344 / 525  Training Loss  0.09556187689304352\n",
            "Epoch  3 Batch  345 / 525  Training Loss  0.11472870409488678\n",
            "Epoch  3 Batch  346 / 525  Training Loss  0.11433128267526627\n",
            "Epoch  3 Batch  347 / 525  Training Loss  0.12554553151130676\n",
            "Epoch  3 Batch  348 / 525  Training Loss  0.10092637687921524\n",
            "Epoch  3 Batch  349 / 525  Training Loss  0.09721507877111435\n",
            "Epoch  3 Batch  350 / 525  Training Loss  0.09755498915910721\n",
            "Epoch  3 Batch  351 / 525  Training Loss  0.11358416080474854\n",
            "Epoch  3 Batch  352 / 525  Training Loss  0.11950591951608658\n",
            "Epoch  3 Batch  353 / 525  Training Loss  0.10372070968151093\n",
            "Epoch  3 Batch  354 / 525  Training Loss  0.1329592913389206\n",
            "Epoch  3 Batch  355 / 525  Training Loss  0.1259712129831314\n",
            "Epoch  3 Batch  356 / 525  Training Loss  0.10889794677495956\n",
            "Epoch  3 Batch  357 / 525  Training Loss  0.1196388229727745\n",
            "Epoch  3 Batch  358 / 525  Training Loss  0.11409678310155869\n",
            "Epoch  3 Batch  359 / 525  Training Loss  0.11761905252933502\n",
            "Epoch  3 Batch  360 / 525  Training Loss  0.13003109395503998\n",
            "Epoch  3 Batch  361 / 525  Training Loss  0.1033489927649498\n",
            "Epoch  3 Batch  362 / 525  Training Loss  0.10437451303005219\n",
            "Epoch  3 Batch  363 / 525  Training Loss  0.09664034843444824\n",
            "Epoch  3 Batch  364 / 525  Training Loss  0.10355981439352036\n",
            "Epoch  3 Batch  365 / 525  Training Loss  0.10492861270904541\n",
            "Epoch  3 Batch  366 / 525  Training Loss  0.11271579563617706\n",
            "Epoch  3 Batch  367 / 525  Training Loss  0.1122238039970398\n",
            "Epoch  3 Batch  368 / 525  Training Loss  0.09758028388023376\n",
            "Epoch  3 Batch  369 / 525  Training Loss  0.10604169219732285\n",
            "Epoch  3 Batch  370 / 525  Training Loss  0.08060538023710251\n",
            "Epoch  3 Batch  371 / 525  Training Loss  0.09937842190265656\n",
            "Epoch  3 Batch  372 / 525  Training Loss  0.09423717111349106\n",
            "Epoch  3 Batch  373 / 525  Training Loss  0.12083961069583893\n",
            "Epoch  3 Batch  374 / 525  Training Loss  0.10433618724346161\n",
            "Epoch  3 Batch  375 / 525  Training Loss  0.10754314810037613\n",
            "Epoch  3 Batch  376 / 525  Training Loss  0.10219348967075348\n",
            "Epoch  3 Batch  377 / 525  Training Loss  0.09199406206607819\n",
            "Epoch  3 Batch  378 / 525  Training Loss  0.09896692633628845\n",
            "Epoch  3 Batch  379 / 525  Training Loss  0.10867123305797577\n",
            "Epoch  3 Batch  380 / 525  Training Loss  0.11802341789007187\n",
            "Epoch  3 Batch  381 / 525  Training Loss  0.1043555736541748\n",
            "Epoch  3 Batch  382 / 525  Training Loss  0.10215864330530167\n",
            "Epoch  3 Batch  383 / 525  Training Loss  0.09939141571521759\n",
            "Epoch  3 Batch  384 / 525  Training Loss  0.09144017100334167\n",
            "Epoch  3 Batch  385 / 525  Training Loss  0.09736832231283188\n",
            "Epoch  3 Batch  386 / 525  Training Loss  0.10109599679708481\n",
            "Epoch  3 Batch  387 / 525  Training Loss  0.10563848167657852\n",
            "Epoch  3 Batch  388 / 525  Training Loss  0.10714785009622574\n",
            "Epoch  3 Batch  389 / 525  Training Loss  0.128196582198143\n",
            "Epoch  3 Batch  390 / 525  Training Loss  0.1079697236418724\n",
            "Epoch  3 Batch  391 / 525  Training Loss  0.11019577085971832\n",
            "Epoch  3 Batch  392 / 525  Training Loss  0.10736113786697388\n",
            "Epoch  3 Batch  393 / 525  Training Loss  0.09322398900985718\n",
            "Epoch  3 Batch  394 / 525  Training Loss  0.11133436858654022\n",
            "Epoch  3 Batch  395 / 525  Training Loss  0.10642874240875244\n",
            "Epoch  3 Batch  396 / 525  Training Loss  0.11343210935592651\n",
            "Epoch  3 Batch  397 / 525  Training Loss  0.10782154649496078\n",
            "Epoch  3 Batch  398 / 525  Training Loss  0.11448625475168228\n",
            "Epoch  3 Batch  399 / 525  Training Loss  0.10723330825567245\n",
            "Epoch  3 Batch  400 / 525  Training Loss  0.12214548885822296\n",
            "Epoch  3 Batch  401 / 525  Training Loss  0.10122646391391754\n",
            "Epoch  3 Batch  402 / 525  Training Loss  0.08335308730602264\n",
            "Epoch  3 Batch  403 / 525  Training Loss  0.11817939579486847\n",
            "Epoch  3 Batch  404 / 525  Training Loss  0.08305151760578156\n",
            "Epoch  3 Batch  405 / 525  Training Loss  0.10154000669717789\n",
            "Epoch  3 Batch  406 / 525  Training Loss  0.0978216826915741\n",
            "Epoch  3 Batch  407 / 525  Training Loss  0.10616530478000641\n",
            "Epoch  3 Batch  408 / 525  Training Loss  0.095474012196064\n",
            "Epoch  3 Batch  409 / 525  Training Loss  0.09145995229482651\n",
            "Epoch  3 Batch  410 / 525  Training Loss  0.0891328901052475\n",
            "Epoch  3 Batch  411 / 525  Training Loss  0.10462434589862823\n",
            "Epoch  3 Batch  412 / 525  Training Loss  0.09098853170871735\n",
            "Epoch  3 Batch  413 / 525  Training Loss  0.10788804292678833\n",
            "Epoch  3 Batch  414 / 525  Training Loss  0.08356083184480667\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  3 Batch  415 / 525  Training Loss  0.10346056520938873\n",
            "Epoch  3 Batch  416 / 525  Training Loss  0.12445144355297089\n",
            "Epoch  3 Batch  417 / 525  Training Loss  0.08613471686840057\n",
            "Epoch  3 Batch  418 / 525  Training Loss  0.09303347766399384\n",
            "Epoch  3 Batch  419 / 525  Training Loss  0.11237913370132446\n",
            "Epoch  3 Batch  420 / 525  Training Loss  0.10775125026702881\n",
            "Epoch  3 Batch  421 / 525  Training Loss  0.1015729159116745\n",
            "Epoch  3 Batch  422 / 525  Training Loss  0.11282064765691757\n",
            "Epoch  3 Batch  423 / 525  Training Loss  0.08734200149774551\n",
            "Epoch  3 Batch  424 / 525  Training Loss  0.09711568802595139\n",
            "Epoch  3 Batch  425 / 525  Training Loss  0.11332078278064728\n",
            "Epoch  3 Batch  426 / 525  Training Loss  0.1197936162352562\n",
            "Epoch  3 Batch  427 / 525  Training Loss  0.09358585625886917\n",
            "Epoch  3 Batch  428 / 525  Training Loss  0.10062894970178604\n",
            "Epoch  3 Batch  429 / 525  Training Loss  0.1261533796787262\n",
            "Epoch  3 Batch  430 / 525  Training Loss  0.1127379909157753\n",
            "Epoch  3 Batch  431 / 525  Training Loss  0.10172083228826523\n",
            "Epoch  3 Batch  432 / 525  Training Loss  0.08808667957782745\n",
            "Epoch  3 Batch  433 / 525  Training Loss  0.10619229078292847\n",
            "Epoch  3 Batch  434 / 525  Training Loss  0.09268203377723694\n",
            "Epoch  3 Batch  435 / 525  Training Loss  0.09622235596179962\n",
            "Epoch  3 Batch  436 / 525  Training Loss  0.10637509822845459\n",
            "Epoch  3 Batch  437 / 525  Training Loss  0.09644575417041779\n",
            "Epoch  3 Batch  438 / 525  Training Loss  0.09764037281274796\n",
            "Epoch  3 Batch  439 / 525  Training Loss  0.1159927248954773\n",
            "Epoch  3 Batch  440 / 525  Training Loss  0.10894665867090225\n",
            "Epoch  3 Batch  441 / 525  Training Loss  0.10773180425167084\n",
            "Epoch  3 Batch  442 / 525  Training Loss  0.10766557604074478\n",
            "Epoch  3 Batch  443 / 525  Training Loss  0.10377035290002823\n",
            "Epoch  3 Batch  444 / 525  Training Loss  0.08489921689033508\n",
            "Epoch  3 Batch  445 / 525  Training Loss  0.0934915691614151\n",
            "Epoch  3 Batch  446 / 525  Training Loss  0.10009866952896118\n",
            "Epoch  3 Batch  447 / 525  Training Loss  0.08156685531139374\n",
            "Epoch  3 Batch  448 / 525  Training Loss  0.11197880655527115\n",
            "Epoch  3 Batch  449 / 525  Training Loss  0.07512423396110535\n",
            "Epoch  3 Batch  450 / 525  Training Loss  0.11228132247924805\n",
            "Epoch  3 Batch  451 / 525  Training Loss  0.11716171354055405\n",
            "Epoch  3 Batch  452 / 525  Training Loss  0.11432863771915436\n",
            "Epoch  3 Batch  453 / 525  Training Loss  0.09505582600831985\n",
            "Epoch  3 Batch  454 / 525  Training Loss  0.10640790313482285\n",
            "Epoch  3 Batch  455 / 525  Training Loss  0.10255732387304306\n",
            "Epoch  3 Batch  456 / 525  Training Loss  0.12655046582221985\n",
            "Epoch  3 Batch  457 / 525  Training Loss  0.11543960869312286\n",
            "Epoch  3 Batch  458 / 525  Training Loss  0.10766097158193588\n",
            "Epoch  3 Batch  459 / 525  Training Loss  0.10420630127191544\n",
            "Epoch  3 Batch  460 / 525  Training Loss  0.09969460219144821\n",
            "Epoch  3 Batch  461 / 525  Training Loss  0.10588450729846954\n",
            "Epoch  3 Batch  462 / 525  Training Loss  0.10728360712528229\n",
            "Epoch  3 Batch  463 / 525  Training Loss  0.11213473975658417\n",
            "Epoch  3 Batch  464 / 525  Training Loss  0.09343160688877106\n",
            "Epoch  3 Batch  465 / 525  Training Loss  0.11367076635360718\n",
            "Epoch  3 Batch  466 / 525  Training Loss  0.1067776307463646\n",
            "Epoch  3 Batch  467 / 525  Training Loss  0.08686475455760956\n",
            "Epoch  3 Batch  468 / 525  Training Loss  0.09413247555494308\n",
            "Epoch  3 Batch  469 / 525  Training Loss  0.11669055372476578\n",
            "Epoch  3 Batch  470 / 525  Training Loss  0.10555368661880493\n",
            "Epoch  3 Batch  471 / 525  Training Loss  0.10687941312789917\n",
            "Epoch  3 Batch  472 / 525  Training Loss  0.10123765468597412\n",
            "Epoch  3 Batch  473 / 525  Training Loss  0.1143447607755661\n",
            "Epoch  3 Batch  474 / 525  Training Loss  0.1052204817533493\n",
            "Epoch  3 Batch  475 / 525  Training Loss  0.08850766718387604\n",
            "Epoch  3 Batch  476 / 525  Training Loss  0.09853693842887878\n",
            "Epoch  3 Batch  477 / 525  Training Loss  0.09279056638479233\n",
            "Epoch  3 Batch  478 / 525  Training Loss  0.10290658473968506\n",
            "Epoch  3 Batch  479 / 525  Training Loss  0.09277768433094025\n",
            "Epoch  3 Batch  480 / 525  Training Loss  0.11057545989751816\n",
            "Epoch  3 Batch  481 / 525  Training Loss  0.1083415150642395\n",
            "Epoch  3 Batch  482 / 525  Training Loss  0.11179770529270172\n",
            "Epoch  3 Batch  483 / 525  Training Loss  0.11045466363430023\n",
            "Epoch  3 Batch  484 / 525  Training Loss  0.09929686784744263\n",
            "Epoch  3 Batch  485 / 525  Training Loss  0.09612318128347397\n",
            "Epoch  3 Batch  486 / 525  Training Loss  0.10946892201900482\n",
            "Epoch  3 Batch  487 / 525  Training Loss  0.11418452113866806\n",
            "Epoch  3 Batch  488 / 525  Training Loss  0.11825889348983765\n",
            "Epoch  3 Batch  489 / 525  Training Loss  0.09584439545869827\n",
            "Epoch  3 Batch  490 / 525  Training Loss  0.11106033623218536\n",
            "Epoch  3 Batch  491 / 525  Training Loss  0.10634104907512665\n",
            "Epoch  3 Batch  492 / 525  Training Loss  0.1028752475976944\n",
            "Epoch  3 Batch  493 / 525  Training Loss  0.08856063336133957\n",
            "Epoch  3 Batch  494 / 525  Training Loss  0.11058914661407471\n",
            "Epoch  3 Batch  495 / 525  Training Loss  0.07845935970544815\n",
            "Epoch  3 Batch  496 / 525  Training Loss  0.10563371330499649\n",
            "Epoch  3 Batch  497 / 525  Training Loss  0.10347510874271393\n",
            "Epoch  3 Batch  498 / 525  Training Loss  0.09784754365682602\n",
            "Epoch  3 Batch  499 / 525  Training Loss  0.10973776876926422\n",
            "Epoch  3 Batch  500 / 525  Training Loss  0.11650444567203522\n",
            "Epoch  3 Batch  501 / 525  Training Loss  0.11908086389303207\n",
            "Epoch  3 Batch  502 / 525  Training Loss  0.10670015960931778\n",
            "Epoch  3 Batch  503 / 525  Training Loss  0.11436019837856293\n",
            "Epoch  3 Batch  504 / 525  Training Loss  0.0945114940404892\n",
            "Epoch  3 Batch  505 / 525  Training Loss  0.11432905495166779\n",
            "Epoch  3 Batch  506 / 525  Training Loss  0.09513094276189804\n",
            "Epoch  3 Batch  507 / 525  Training Loss  0.11023624241352081\n",
            "Epoch  3 Batch  508 / 525  Training Loss  0.10515372455120087\n",
            "Epoch  3 Batch  509 / 525  Training Loss  0.09197334200143814\n",
            "Epoch  3 Batch  510 / 525  Training Loss  0.10488621890544891\n",
            "Epoch  3 Batch  511 / 525  Training Loss  0.0954848900437355\n",
            "Epoch  3 Batch  512 / 525  Training Loss  0.11407580226659775\n",
            "Epoch  3 Batch  513 / 525  Training Loss  0.10105178505182266\n",
            "Epoch  3 Batch  514 / 525  Training Loss  0.09463222324848175\n",
            "Epoch  3 Batch  515 / 525  Training Loss  0.10750216245651245\n",
            "Epoch  3 Batch  516 / 525  Training Loss  0.09780249744653702\n",
            "Epoch  3 Batch  517 / 525  Training Loss  0.10376070439815521\n",
            "Epoch  3 Batch  518 / 525  Training Loss  0.10517646372318268\n",
            "Epoch  3 Batch  519 / 525  Training Loss  0.11307930946350098\n",
            "Epoch  3 Batch  520 / 525  Training Loss  0.09358571469783783\n",
            "Epoch  3 Batch  521 / 525  Training Loss  0.08871530741453171\n",
            "Epoch  3 Batch  522 / 525  Training Loss  0.09788308292627335\n",
            "Epoch  3 Batch  523 / 525  Training Loss  0.10857423394918442\n",
            "Epoch  3 Batch  524 / 525  Training Loss  0.10360445827245712\n",
            "   4    |    -    |   0.106936   | 28.458333\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 4\n",
            "Epoch  4 Batch  0 / 525  Training Loss  0.08408569544553757\n",
            "Epoch  4 Batch  1 / 525  Training Loss  0.08429546654224396\n",
            "Epoch  4 Batch  2 / 525  Training Loss  0.09317763149738312\n",
            "Epoch  4 Batch  3 / 525  Training Loss  0.09004157781600952\n",
            "Epoch  4 Batch  4 / 525  Training Loss  0.10811245441436768\n",
            "Epoch  4 Batch  5 / 525  Training Loss  0.0877843052148819\n",
            "Epoch  4 Batch  6 / 525  Training Loss  0.11069189012050629\n",
            "Epoch  4 Batch  7 / 525  Training Loss  0.11023753881454468\n",
            "Epoch  4 Batch  8 / 525  Training Loss  0.11471746861934662\n",
            "Epoch  4 Batch  9 / 525  Training Loss  0.11586284637451172\n",
            "Epoch  4 Batch  10 / 525  Training Loss  0.10575604438781738\n",
            "Epoch  4 Batch  11 / 525  Training Loss  0.07967346161603928\n",
            "Epoch  4 Batch  12 / 525  Training Loss  0.09106916189193726\n",
            "Epoch  4 Batch  13 / 525  Training Loss  0.09956012666225433\n",
            "Epoch  4 Batch  14 / 525  Training Loss  0.08997297286987305\n",
            "Epoch  4 Batch  15 / 525  Training Loss  0.08405859768390656\n",
            "Epoch  4 Batch  16 / 525  Training Loss  0.09144075959920883\n",
            "Epoch  4 Batch  17 / 525  Training Loss  0.09675338119268417\n",
            "Epoch  4 Batch  18 / 525  Training Loss  0.09169206768274307\n",
            "Epoch  4 Batch  19 / 525  Training Loss  0.09649026393890381\n",
            "Epoch  4 Batch  20 / 525  Training Loss  0.08640103042125702\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  4 Batch  21 / 525  Training Loss  0.10584483295679092\n",
            "Epoch  4 Batch  22 / 525  Training Loss  0.09897451102733612\n",
            "Epoch  4 Batch  23 / 525  Training Loss  0.09031198918819427\n",
            "Epoch  4 Batch  24 / 525  Training Loss  0.12298323959112167\n",
            "Epoch  4 Batch  25 / 525  Training Loss  0.09595506638288498\n",
            "Epoch  4 Batch  26 / 525  Training Loss  0.11271693557500839\n",
            "Epoch  4 Batch  27 / 525  Training Loss  0.09401512891054153\n",
            "Epoch  4 Batch  28 / 525  Training Loss  0.09142662584781647\n",
            "Epoch  4 Batch  29 / 525  Training Loss  0.0870433822274208\n",
            "Epoch  4 Batch  30 / 525  Training Loss  0.09353618323802948\n",
            "Epoch  4 Batch  31 / 525  Training Loss  0.12580755352973938\n",
            "Epoch  4 Batch  32 / 525  Training Loss  0.10233265161514282\n",
            "Epoch  4 Batch  33 / 525  Training Loss  0.08664525300264359\n",
            "Epoch  4 Batch  34 / 525  Training Loss  0.08898594230413437\n",
            "Epoch  4 Batch  35 / 525  Training Loss  0.09723378717899323\n",
            "Epoch  4 Batch  36 / 525  Training Loss  0.09715413302183151\n",
            "Epoch  4 Batch  37 / 525  Training Loss  0.10227362811565399\n",
            "Epoch  4 Batch  38 / 525  Training Loss  0.09074994176626205\n",
            "Epoch  4 Batch  39 / 525  Training Loss  0.08993630856275558\n",
            "Epoch  4 Batch  40 / 525  Training Loss  0.08272270858287811\n",
            "Epoch  4 Batch  41 / 525  Training Loss  0.09438935667276382\n",
            "Epoch  4 Batch  42 / 525  Training Loss  0.097483329474926\n",
            "Epoch  4 Batch  43 / 525  Training Loss  0.10183773189783096\n",
            "Epoch  4 Batch  44 / 525  Training Loss  0.09927871823310852\n",
            "Epoch  4 Batch  45 / 525  Training Loss  0.09006448090076447\n",
            "Epoch  4 Batch  46 / 525  Training Loss  0.10620471090078354\n",
            "Epoch  4 Batch  47 / 525  Training Loss  0.09569443762302399\n",
            "Epoch  4 Batch  48 / 525  Training Loss  0.08701541274785995\n",
            "Epoch  4 Batch  49 / 525  Training Loss  0.0888310894370079\n",
            "Epoch  4 Batch  50 / 525  Training Loss  0.09442605823278427\n",
            "Epoch  4 Batch  51 / 525  Training Loss  0.08870797604322433\n",
            "Epoch  4 Batch  52 / 525  Training Loss  0.09079615026712418\n",
            "Epoch  4 Batch  53 / 525  Training Loss  0.09727595746517181\n",
            "Epoch  4 Batch  54 / 525  Training Loss  0.08132960647344589\n",
            "Epoch  4 Batch  55 / 525  Training Loss  0.08765963464975357\n",
            "Epoch  4 Batch  56 / 525  Training Loss  0.08650092780590057\n",
            "Epoch  4 Batch  57 / 525  Training Loss  0.11703324317932129\n",
            "Epoch  4 Batch  58 / 525  Training Loss  0.10710076242685318\n",
            "Epoch  4 Batch  59 / 525  Training Loss  0.0959915742278099\n",
            "Epoch  4 Batch  60 / 525  Training Loss  0.09679737687110901\n",
            "Epoch  4 Batch  61 / 525  Training Loss  0.11608678102493286\n",
            "Epoch  4 Batch  62 / 525  Training Loss  0.10593730211257935\n",
            "Epoch  4 Batch  63 / 525  Training Loss  0.08509957790374756\n",
            "Epoch  4 Batch  64 / 525  Training Loss  0.11362914741039276\n",
            "Epoch  4 Batch  65 / 525  Training Loss  0.09779657423496246\n",
            "Epoch  4 Batch  66 / 525  Training Loss  0.10151199996471405\n",
            "Epoch  4 Batch  67 / 525  Training Loss  0.11422045528888702\n",
            "Epoch  4 Batch  68 / 525  Training Loss  0.10035192966461182\n",
            "Epoch  4 Batch  69 / 525  Training Loss  0.10903303325176239\n",
            "Epoch  4 Batch  70 / 525  Training Loss  0.09368535876274109\n",
            "Epoch  4 Batch  71 / 525  Training Loss  0.0953022912144661\n",
            "Epoch  4 Batch  72 / 525  Training Loss  0.10049355030059814\n",
            "Epoch  4 Batch  73 / 525  Training Loss  0.0943169891834259\n",
            "Epoch  4 Batch  74 / 525  Training Loss  0.09371785074472427\n",
            "Epoch  4 Batch  75 / 525  Training Loss  0.10685275495052338\n",
            "Epoch  4 Batch  76 / 525  Training Loss  0.09874431788921356\n",
            "Epoch  4 Batch  77 / 525  Training Loss  0.07864463329315186\n",
            "Epoch  4 Batch  78 / 525  Training Loss  0.09067007899284363\n",
            "Epoch  4 Batch  79 / 525  Training Loss  0.11414895951747894\n",
            "Epoch  4 Batch  80 / 525  Training Loss  0.10094407945871353\n",
            "Epoch  4 Batch  81 / 525  Training Loss  0.1185876727104187\n",
            "Epoch  4 Batch  82 / 525  Training Loss  0.08915881812572479\n",
            "Epoch  4 Batch  83 / 525  Training Loss  0.09434621781110764\n",
            "Epoch  4 Batch  84 / 525  Training Loss  0.09756016731262207\n",
            "Epoch  4 Batch  85 / 525  Training Loss  0.10351382195949554\n",
            "Epoch  4 Batch  86 / 525  Training Loss  0.10303719341754913\n",
            "Epoch  4 Batch  87 / 525  Training Loss  0.1025722473859787\n",
            "Epoch  4 Batch  88 / 525  Training Loss  0.090647391974926\n",
            "Epoch  4 Batch  89 / 525  Training Loss  0.11377179622650146\n",
            "Epoch  4 Batch  90 / 525  Training Loss  0.0691947489976883\n",
            "Epoch  4 Batch  91 / 525  Training Loss  0.11260237544775009\n",
            "Epoch  4 Batch  92 / 525  Training Loss  0.10566284507513046\n",
            "Epoch  4 Batch  93 / 525  Training Loss  0.09790117293596268\n",
            "Epoch  4 Batch  94 / 525  Training Loss  0.09812565892934799\n",
            "Epoch  4 Batch  95 / 525  Training Loss  0.07415153086185455\n",
            "Epoch  4 Batch  96 / 525  Training Loss  0.08807146549224854\n",
            "Epoch  4 Batch  97 / 525  Training Loss  0.10217617452144623\n",
            "Epoch  4 Batch  98 / 525  Training Loss  0.11143342405557632\n",
            "Epoch  4 Batch  99 / 525  Training Loss  0.10306636989116669\n",
            "Epoch  4 Batch  100 / 525  Training Loss  0.10165425390005112\n",
            "Epoch  4 Batch  101 / 525  Training Loss  0.09833504259586334\n",
            "Epoch  4 Batch  102 / 525  Training Loss  0.1064174547791481\n",
            "Epoch  4 Batch  103 / 525  Training Loss  0.09461583197116852\n",
            "Epoch  4 Batch  104 / 525  Training Loss  0.09737181663513184\n",
            "Epoch  4 Batch  105 / 525  Training Loss  0.10226534307003021\n",
            "Epoch  4 Batch  106 / 525  Training Loss  0.09563931077718735\n",
            "Epoch  4 Batch  107 / 525  Training Loss  0.0903007835149765\n",
            "Epoch  4 Batch  108 / 525  Training Loss  0.08995901048183441\n",
            "Epoch  4 Batch  109 / 525  Training Loss  0.08752110600471497\n",
            "Epoch  4 Batch  110 / 525  Training Loss  0.10008998960256577\n",
            "Epoch  4 Batch  111 / 525  Training Loss  0.09060128778219223\n",
            "Epoch  4 Batch  112 / 525  Training Loss  0.10165642201900482\n",
            "Epoch  4 Batch  113 / 525  Training Loss  0.10759986937046051\n",
            "Epoch  4 Batch  114 / 525  Training Loss  0.10153482109308243\n",
            "Epoch  4 Batch  115 / 525  Training Loss  0.09652721881866455\n",
            "Epoch  4 Batch  116 / 525  Training Loss  0.08721534162759781\n",
            "Epoch  4 Batch  117 / 525  Training Loss  0.10283608734607697\n",
            "Epoch  4 Batch  118 / 525  Training Loss  0.09292401373386383\n",
            "Epoch  4 Batch  119 / 525  Training Loss  0.10562572628259659\n",
            "Epoch  4 Batch  120 / 525  Training Loss  0.08762268722057343\n",
            "Epoch  4 Batch  121 / 525  Training Loss  0.10495700687170029\n",
            "Epoch  4 Batch  122 / 525  Training Loss  0.08981064707040787\n",
            "Epoch  4 Batch  123 / 525  Training Loss  0.07051023095846176\n",
            "Epoch  4 Batch  124 / 525  Training Loss  0.09800221771001816\n",
            "Epoch  4 Batch  125 / 525  Training Loss  0.08501487970352173\n",
            "Epoch  4 Batch  126 / 525  Training Loss  0.08347370475530624\n",
            "Epoch  4 Batch  127 / 525  Training Loss  0.1028192788362503\n",
            "Epoch  4 Batch  128 / 525  Training Loss  0.09753764420747757\n",
            "Epoch  4 Batch  129 / 525  Training Loss  0.08876661956310272\n",
            "Epoch  4 Batch  130 / 525  Training Loss  0.08571483939886093\n",
            "Epoch  4 Batch  131 / 525  Training Loss  0.11252222210168839\n",
            "Epoch  4 Batch  132 / 525  Training Loss  0.10430823266506195\n",
            "Epoch  4 Batch  133 / 525  Training Loss  0.09469462931156158\n",
            "Epoch  4 Batch  134 / 525  Training Loss  0.10505528748035431\n",
            "Epoch  4 Batch  135 / 525  Training Loss  0.10084662586450577\n",
            "Epoch  4 Batch  136 / 525  Training Loss  0.11259858310222626\n",
            "Epoch  4 Batch  137 / 525  Training Loss  0.09194370359182358\n",
            "Epoch  4 Batch  138 / 525  Training Loss  0.06901545822620392\n",
            "Epoch  4 Batch  139 / 525  Training Loss  0.10837024450302124\n",
            "Epoch  4 Batch  140 / 525  Training Loss  0.08130253851413727\n",
            "Epoch  4 Batch  141 / 525  Training Loss  0.10492075979709625\n",
            "Epoch  4 Batch  142 / 525  Training Loss  0.08832666277885437\n",
            "Epoch  4 Batch  143 / 525  Training Loss  0.09983013570308685\n",
            "Epoch  4 Batch  144 / 525  Training Loss  0.0676998645067215\n",
            "Epoch  4 Batch  145 / 525  Training Loss  0.10847028344869614\n",
            "Epoch  4 Batch  146 / 525  Training Loss  0.1018027514219284\n",
            "Epoch  4 Batch  147 / 525  Training Loss  0.08689375966787338\n",
            "Epoch  4 Batch  148 / 525  Training Loss  0.0657508373260498\n",
            "Epoch  4 Batch  149 / 525  Training Loss  0.11879046261310577\n",
            "Epoch  4 Batch  150 / 525  Training Loss  0.07702130824327469\n",
            "Epoch  4 Batch  151 / 525  Training Loss  0.0988118126988411\n",
            "Epoch  4 Batch  152 / 525  Training Loss  0.08884838223457336\n",
            "Epoch  4 Batch  153 / 525  Training Loss  0.09943084418773651\n",
            "Epoch  4 Batch  154 / 525  Training Loss  0.09886278212070465\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  4 Batch  155 / 525  Training Loss  0.08949302136898041\n",
            "Epoch  4 Batch  156 / 525  Training Loss  0.0980539470911026\n",
            "Epoch  4 Batch  157 / 525  Training Loss  0.10892494022846222\n",
            "Epoch  4 Batch  158 / 525  Training Loss  0.09664526581764221\n",
            "Epoch  4 Batch  159 / 525  Training Loss  0.09769157320261002\n",
            "Epoch  4 Batch  160 / 525  Training Loss  0.09810395538806915\n",
            "Epoch  4 Batch  161 / 525  Training Loss  0.10348387807607651\n",
            "Epoch  4 Batch  162 / 525  Training Loss  0.07782449573278427\n",
            "Epoch  4 Batch  163 / 525  Training Loss  0.10018181800842285\n",
            "Epoch  4 Batch  164 / 525  Training Loss  0.11638859659433365\n",
            "Epoch  4 Batch  165 / 525  Training Loss  0.09390340000391006\n",
            "Epoch  4 Batch  166 / 525  Training Loss  0.0896562933921814\n",
            "Epoch  4 Batch  167 / 525  Training Loss  0.08316059410572052\n",
            "Epoch  4 Batch  168 / 525  Training Loss  0.08097834140062332\n",
            "Epoch  4 Batch  169 / 525  Training Loss  0.08936825394630432\n",
            "Epoch  4 Batch  170 / 525  Training Loss  0.09718873351812363\n",
            "Epoch  4 Batch  171 / 525  Training Loss  0.09200803190469742\n",
            "Epoch  4 Batch  172 / 525  Training Loss  0.10201145708560944\n",
            "Epoch  4 Batch  173 / 525  Training Loss  0.10561474412679672\n",
            "Epoch  4 Batch  174 / 525  Training Loss  0.06831380724906921\n",
            "Epoch  4 Batch  175 / 525  Training Loss  0.09732834994792938\n",
            "Epoch  4 Batch  176 / 525  Training Loss  0.07221385091543198\n",
            "Epoch  4 Batch  177 / 525  Training Loss  0.0926869586110115\n",
            "Epoch  4 Batch  178 / 525  Training Loss  0.08670501410961151\n",
            "Epoch  4 Batch  179 / 525  Training Loss  0.11078040301799774\n",
            "Epoch  4 Batch  180 / 525  Training Loss  0.09481221437454224\n",
            "Epoch  4 Batch  181 / 525  Training Loss  0.10637176036834717\n",
            "Epoch  4 Batch  182 / 525  Training Loss  0.08070576190948486\n",
            "Epoch  4 Batch  183 / 525  Training Loss  0.09637893736362457\n",
            "Epoch  4 Batch  184 / 525  Training Loss  0.0810113400220871\n",
            "Epoch  4 Batch  185 / 525  Training Loss  0.07824350893497467\n",
            "Epoch  4 Batch  186 / 525  Training Loss  0.08867491781711578\n",
            "Epoch  4 Batch  187 / 525  Training Loss  0.10701823234558105\n",
            "Epoch  4 Batch  188 / 525  Training Loss  0.1175071969628334\n",
            "Epoch  4 Batch  189 / 525  Training Loss  0.09660245478153229\n",
            "Epoch  4 Batch  190 / 525  Training Loss  0.0924689918756485\n",
            "Epoch  4 Batch  191 / 525  Training Loss  0.10454370081424713\n",
            "Epoch  4 Batch  192 / 525  Training Loss  0.09018145501613617\n",
            "Epoch  4 Batch  193 / 525  Training Loss  0.0906558558344841\n",
            "Epoch  4 Batch  194 / 525  Training Loss  0.09646435081958771\n",
            "Epoch  4 Batch  195 / 525  Training Loss  0.09788832813501358\n",
            "Epoch  4 Batch  196 / 525  Training Loss  0.09101967513561249\n",
            "Epoch  4 Batch  197 / 525  Training Loss  0.10450770705938339\n",
            "Epoch  4 Batch  198 / 525  Training Loss  0.09435565769672394\n",
            "Epoch  4 Batch  199 / 525  Training Loss  0.1084190234541893\n",
            "Epoch  4 Batch  200 / 525  Training Loss  0.1046925038099289\n",
            "Epoch  4 Batch  201 / 525  Training Loss  0.09458474069833755\n",
            "Epoch  4 Batch  202 / 525  Training Loss  0.09862404316663742\n",
            "Epoch  4 Batch  203 / 525  Training Loss  0.09625329822301865\n",
            "Epoch  4 Batch  204 / 525  Training Loss  0.08608432859182358\n",
            "Epoch  4 Batch  205 / 525  Training Loss  0.09959165751934052\n",
            "Epoch  4 Batch  206 / 525  Training Loss  0.07894550263881683\n",
            "Epoch  4 Batch  207 / 525  Training Loss  0.0709015429019928\n",
            "Epoch  4 Batch  208 / 525  Training Loss  0.09537411481142044\n",
            "Epoch  4 Batch  209 / 525  Training Loss  0.0829421877861023\n",
            "Epoch  4 Batch  210 / 525  Training Loss  0.08722472935914993\n",
            "Epoch  4 Batch  211 / 525  Training Loss  0.08941095322370529\n",
            "Epoch  4 Batch  212 / 525  Training Loss  0.09529952704906464\n",
            "Epoch  4 Batch  213 / 525  Training Loss  0.11083287000656128\n",
            "Epoch  4 Batch  214 / 525  Training Loss  0.11440770328044891\n",
            "Epoch  4 Batch  215 / 525  Training Loss  0.10326123237609863\n",
            "Epoch  4 Batch  216 / 525  Training Loss  0.08907300978899002\n",
            "Epoch  4 Batch  217 / 525  Training Loss  0.0707080140709877\n",
            "Epoch  4 Batch  218 / 525  Training Loss  0.11075788736343384\n",
            "Epoch  4 Batch  219 / 525  Training Loss  0.09718171507120132\n",
            "Epoch  4 Batch  220 / 525  Training Loss  0.11005039513111115\n",
            "Epoch  4 Batch  221 / 525  Training Loss  0.11715511232614517\n",
            "Epoch  4 Batch  222 / 525  Training Loss  0.08880896866321564\n",
            "Epoch  4 Batch  223 / 525  Training Loss  0.09707694500684738\n",
            "Epoch  4 Batch  224 / 525  Training Loss  0.1132107749581337\n",
            "Epoch  4 Batch  225 / 525  Training Loss  0.10554611682891846\n",
            "Epoch  4 Batch  226 / 525  Training Loss  0.0900571420788765\n",
            "Epoch  4 Batch  227 / 525  Training Loss  0.09478151798248291\n",
            "Epoch  4 Batch  228 / 525  Training Loss  0.09749948978424072\n",
            "Epoch  4 Batch  229 / 525  Training Loss  0.08524121344089508\n",
            "Epoch  4 Batch  230 / 525  Training Loss  0.09744780510663986\n",
            "Epoch  4 Batch  231 / 525  Training Loss  0.08788669854402542\n",
            "Epoch  4 Batch  232 / 525  Training Loss  0.09391731768846512\n",
            "Epoch  4 Batch  233 / 525  Training Loss  0.09306641668081284\n",
            "Epoch  4 Batch  234 / 525  Training Loss  0.09596385061740875\n",
            "Epoch  4 Batch  235 / 525  Training Loss  0.09403613209724426\n",
            "Epoch  4 Batch  236 / 525  Training Loss  0.09657314419746399\n",
            "Epoch  4 Batch  237 / 525  Training Loss  0.10121031105518341\n",
            "Epoch  4 Batch  238 / 525  Training Loss  0.09849768131971359\n",
            "Epoch  4 Batch  239 / 525  Training Loss  0.0829419195652008\n",
            "Epoch  4 Batch  240 / 525  Training Loss  0.08281666785478592\n",
            "Epoch  4 Batch  241 / 525  Training Loss  0.1089482307434082\n",
            "Epoch  4 Batch  242 / 525  Training Loss  0.0941101610660553\n",
            "Epoch  4 Batch  243 / 525  Training Loss  0.09957435727119446\n",
            "Epoch  4 Batch  244 / 525  Training Loss  0.11457037925720215\n",
            "Epoch  4 Batch  245 / 525  Training Loss  0.09748702496290207\n",
            "Epoch  4 Batch  246 / 525  Training Loss  0.09737235307693481\n",
            "Epoch  4 Batch  247 / 525  Training Loss  0.09720800071954727\n",
            "Epoch  4 Batch  248 / 525  Training Loss  0.09120072424411774\n",
            "Epoch  4 Batch  249 / 525  Training Loss  0.07902474701404572\n",
            "Epoch  4 Batch  250 / 525  Training Loss  0.09550046175718307\n",
            "Epoch  4 Batch  251 / 525  Training Loss  0.1074012741446495\n",
            "Epoch  4 Batch  252 / 525  Training Loss  0.08740632981061935\n",
            "Epoch  4 Batch  253 / 525  Training Loss  0.08994399011135101\n",
            "Epoch  4 Batch  254 / 525  Training Loss  0.10023164749145508\n",
            "Epoch  4 Batch  255 / 525  Training Loss  0.11170870065689087\n",
            "Epoch  4 Batch  256 / 525  Training Loss  0.11415299028158188\n",
            "Epoch  4 Batch  257 / 525  Training Loss  0.1046367883682251\n",
            "Epoch  4 Batch  258 / 525  Training Loss  0.10647208988666534\n",
            "Epoch  4 Batch  259 / 525  Training Loss  0.10188053548336029\n",
            "Epoch  4 Batch  260 / 525  Training Loss  0.09636560082435608\n",
            "Epoch  4 Batch  261 / 525  Training Loss  0.11720891296863556\n",
            "Epoch  4 Batch  262 / 525  Training Loss  0.08975104242563248\n",
            "Epoch  4 Batch  263 / 525  Training Loss  0.08381587266921997\n",
            "Epoch  4 Batch  264 / 525  Training Loss  0.10357248783111572\n",
            "Epoch  4 Batch  265 / 525  Training Loss  0.10193896293640137\n",
            "Epoch  4 Batch  266 / 525  Training Loss  0.10267903655767441\n",
            "Epoch  4 Batch  267 / 525  Training Loss  0.10801571607589722\n",
            "Epoch  4 Batch  268 / 525  Training Loss  0.08654280006885529\n",
            "Epoch  4 Batch  269 / 525  Training Loss  0.09770570695400238\n",
            "Epoch  4 Batch  270 / 525  Training Loss  0.08821006119251251\n",
            "Epoch  4 Batch  271 / 525  Training Loss  0.08505641669034958\n",
            "Epoch  4 Batch  272 / 525  Training Loss  0.10909266769886017\n",
            "Epoch  4 Batch  273 / 525  Training Loss  0.08300097286701202\n",
            "Epoch  4 Batch  274 / 525  Training Loss  0.09659798443317413\n",
            "Epoch  4 Batch  275 / 525  Training Loss  0.08090361207723618\n",
            "Epoch  4 Batch  276 / 525  Training Loss  0.09723789989948273\n",
            "Epoch  4 Batch  277 / 525  Training Loss  0.08048655092716217\n",
            "Epoch  4 Batch  278 / 525  Training Loss  0.07800246775150299\n",
            "Epoch  4 Batch  279 / 525  Training Loss  0.09581946581602097\n",
            "Epoch  4 Batch  280 / 525  Training Loss  0.09328006207942963\n",
            "Epoch  4 Batch  281 / 525  Training Loss  0.09710841625928879\n",
            "Epoch  4 Batch  282 / 525  Training Loss  0.0936286672949791\n",
            "Epoch  4 Batch  283 / 525  Training Loss  0.08520637452602386\n",
            "Epoch  4 Batch  284 / 525  Training Loss  0.0848894864320755\n",
            "Epoch  4 Batch  285 / 525  Training Loss  0.10611595213413239\n",
            "Epoch  4 Batch  286 / 525  Training Loss  0.10288014262914658\n",
            "Epoch  4 Batch  287 / 525  Training Loss  0.1074531227350235\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  4 Batch  288 / 525  Training Loss  0.1070183515548706\n",
            "Epoch  4 Batch  289 / 525  Training Loss  0.08792316168546677\n",
            "Epoch  4 Batch  290 / 525  Training Loss  0.11121350526809692\n",
            "Epoch  4 Batch  291 / 525  Training Loss  0.11012472957372665\n",
            "Epoch  4 Batch  292 / 525  Training Loss  0.09719497710466385\n",
            "Epoch  4 Batch  293 / 525  Training Loss  0.10410793870687485\n",
            "Epoch  4 Batch  294 / 525  Training Loss  0.08072872459888458\n",
            "Epoch  4 Batch  295 / 525  Training Loss  0.09638528525829315\n",
            "Epoch  4 Batch  296 / 525  Training Loss  0.10018986463546753\n",
            "Epoch  4 Batch  297 / 525  Training Loss  0.10318905115127563\n",
            "Epoch  4 Batch  298 / 525  Training Loss  0.1047719269990921\n",
            "Epoch  4 Batch  299 / 525  Training Loss  0.11121369898319244\n",
            "Epoch  4 Batch  300 / 525  Training Loss  0.11158967018127441\n",
            "Epoch  4 Batch  301 / 525  Training Loss  0.07338827103376389\n",
            "Epoch  4 Batch  302 / 525  Training Loss  0.10016415268182755\n",
            "Epoch  4 Batch  303 / 525  Training Loss  0.09171339869499207\n",
            "Epoch  4 Batch  304 / 525  Training Loss  0.12328288704156876\n",
            "Epoch  4 Batch  305 / 525  Training Loss  0.10031044483184814\n",
            "Epoch  4 Batch  306 / 525  Training Loss  0.10610467195510864\n",
            "Epoch  4 Batch  307 / 525  Training Loss  0.09208963811397552\n",
            "Epoch  4 Batch  308 / 525  Training Loss  0.106941357254982\n",
            "Epoch  4 Batch  309 / 525  Training Loss  0.09390521794557571\n",
            "Epoch  4 Batch  310 / 525  Training Loss  0.07777562737464905\n",
            "Epoch  4 Batch  311 / 525  Training Loss  0.10389695316553116\n",
            "Epoch  4 Batch  312 / 525  Training Loss  0.09323310106992722\n",
            "Epoch  4 Batch  313 / 525  Training Loss  0.09870056807994843\n",
            "Epoch  4 Batch  314 / 525  Training Loss  0.0948399156332016\n",
            "Epoch  4 Batch  315 / 525  Training Loss  0.08071228116750717\n",
            "Epoch  4 Batch  316 / 525  Training Loss  0.09903039783239365\n",
            "Epoch  4 Batch  317 / 525  Training Loss  0.09341179579496384\n",
            "Epoch  4 Batch  318 / 525  Training Loss  0.09277454018592834\n",
            "Epoch  4 Batch  319 / 525  Training Loss  0.0722278356552124\n",
            "Epoch  4 Batch  320 / 525  Training Loss  0.10785318911075592\n",
            "Epoch  4 Batch  321 / 525  Training Loss  0.09956945478916168\n",
            "Epoch  4 Batch  322 / 525  Training Loss  0.10085269063711166\n",
            "Epoch  4 Batch  323 / 525  Training Loss  0.09912682324647903\n",
            "Epoch  4 Batch  324 / 525  Training Loss  0.11062155663967133\n",
            "Epoch  4 Batch  325 / 525  Training Loss  0.0899319127202034\n",
            "Epoch  4 Batch  326 / 525  Training Loss  0.10006258636713028\n",
            "Epoch  4 Batch  327 / 525  Training Loss  0.09096067398786545\n",
            "Epoch  4 Batch  328 / 525  Training Loss  0.07594314217567444\n",
            "Epoch  4 Batch  329 / 525  Training Loss  0.10648544132709503\n",
            "Epoch  4 Batch  330 / 525  Training Loss  0.11596890538930893\n",
            "Epoch  4 Batch  331 / 525  Training Loss  0.07673700153827667\n",
            "Epoch  4 Batch  332 / 525  Training Loss  0.09121139347553253\n",
            "Epoch  4 Batch  333 / 525  Training Loss  0.09442862868309021\n",
            "Epoch  4 Batch  334 / 525  Training Loss  0.10569320619106293\n",
            "Epoch  4 Batch  335 / 525  Training Loss  0.0972672551870346\n",
            "Epoch  4 Batch  336 / 525  Training Loss  0.08916577696800232\n",
            "Epoch  4 Batch  337 / 525  Training Loss  0.08798199892044067\n",
            "Epoch  4 Batch  338 / 525  Training Loss  0.08979277312755585\n",
            "Epoch  4 Batch  339 / 525  Training Loss  0.08930647373199463\n",
            "Epoch  4 Batch  340 / 525  Training Loss  0.10762699693441391\n",
            "Epoch  4 Batch  341 / 525  Training Loss  0.12324631214141846\n",
            "Epoch  4 Batch  342 / 525  Training Loss  0.0970911979675293\n",
            "Epoch  4 Batch  343 / 525  Training Loss  0.10252835601568222\n",
            "Epoch  4 Batch  344 / 525  Training Loss  0.09267369657754898\n",
            "Epoch  4 Batch  345 / 525  Training Loss  0.10227515548467636\n",
            "Epoch  4 Batch  346 / 525  Training Loss  0.1141500324010849\n",
            "Epoch  4 Batch  347 / 525  Training Loss  0.10308559983968735\n",
            "Epoch  4 Batch  348 / 525  Training Loss  0.0847049131989479\n",
            "Epoch  4 Batch  349 / 525  Training Loss  0.08813898265361786\n",
            "Epoch  4 Batch  350 / 525  Training Loss  0.08023789525032043\n",
            "Epoch  4 Batch  351 / 525  Training Loss  0.07374952733516693\n",
            "Epoch  4 Batch  352 / 525  Training Loss  0.08817743510007858\n",
            "Epoch  4 Batch  353 / 525  Training Loss  0.08314015716314316\n",
            "Epoch  4 Batch  354 / 525  Training Loss  0.10061930119991302\n",
            "Epoch  4 Batch  355 / 525  Training Loss  0.08654613792896271\n",
            "Epoch  4 Batch  356 / 525  Training Loss  0.0979190468788147\n",
            "Epoch  4 Batch  357 / 525  Training Loss  0.08635294437408447\n",
            "Epoch  4 Batch  358 / 525  Training Loss  0.1067332848906517\n",
            "Epoch  4 Batch  359 / 525  Training Loss  0.08646761626005173\n",
            "Epoch  4 Batch  360 / 525  Training Loss  0.0957612469792366\n",
            "Epoch  4 Batch  361 / 525  Training Loss  0.10819163173437119\n",
            "Epoch  4 Batch  362 / 525  Training Loss  0.11598547548055649\n",
            "Epoch  4 Batch  363 / 525  Training Loss  0.09824465960264206\n",
            "Epoch  4 Batch  364 / 525  Training Loss  0.10090428590774536\n",
            "Epoch  4 Batch  365 / 525  Training Loss  0.07519559562206268\n",
            "Epoch  4 Batch  366 / 525  Training Loss  0.09076179563999176\n",
            "Epoch  4 Batch  367 / 525  Training Loss  0.0956311821937561\n",
            "Epoch  4 Batch  368 / 525  Training Loss  0.0945042222738266\n",
            "Epoch  4 Batch  369 / 525  Training Loss  0.10165002197027206\n",
            "Epoch  4 Batch  370 / 525  Training Loss  0.08471070975065231\n",
            "Epoch  4 Batch  371 / 525  Training Loss  0.09073150157928467\n",
            "Epoch  4 Batch  372 / 525  Training Loss  0.09394682198762894\n",
            "Epoch  4 Batch  373 / 525  Training Loss  0.09463668614625931\n",
            "Epoch  4 Batch  374 / 525  Training Loss  0.09876970201730728\n",
            "Epoch  4 Batch  375 / 525  Training Loss  0.0867990106344223\n",
            "Epoch  4 Batch  376 / 525  Training Loss  0.09305404126644135\n",
            "Epoch  4 Batch  377 / 525  Training Loss  0.086290143430233\n",
            "Epoch  4 Batch  378 / 525  Training Loss  0.08361758291721344\n",
            "Epoch  4 Batch  379 / 525  Training Loss  0.1155981793999672\n",
            "Epoch  4 Batch  380 / 525  Training Loss  0.11734261363744736\n",
            "Epoch  4 Batch  381 / 525  Training Loss  0.09805929660797119\n",
            "Epoch  4 Batch  382 / 525  Training Loss  0.0854949802160263\n",
            "Epoch  4 Batch  383 / 525  Training Loss  0.09595662355422974\n",
            "Epoch  4 Batch  384 / 525  Training Loss  0.09534953534603119\n",
            "Epoch  4 Batch  385 / 525  Training Loss  0.08283113688230515\n",
            "Epoch  4 Batch  386 / 525  Training Loss  0.1068766862154007\n",
            "Epoch  4 Batch  387 / 525  Training Loss  0.08477134257555008\n",
            "Epoch  4 Batch  388 / 525  Training Loss  0.08610887825489044\n",
            "Epoch  4 Batch  389 / 525  Training Loss  0.09411751478910446\n",
            "Epoch  4 Batch  390 / 525  Training Loss  0.10685907304286957\n",
            "Epoch  4 Batch  391 / 525  Training Loss  0.11201298236846924\n",
            "Epoch  4 Batch  392 / 525  Training Loss  0.08966235816478729\n",
            "Epoch  4 Batch  393 / 525  Training Loss  0.10848812758922577\n",
            "Epoch  4 Batch  394 / 525  Training Loss  0.1004108414053917\n",
            "Epoch  4 Batch  395 / 525  Training Loss  0.10043613612651825\n",
            "Epoch  4 Batch  396 / 525  Training Loss  0.09193171560764313\n",
            "Epoch  4 Batch  397 / 525  Training Loss  0.09624879062175751\n",
            "Epoch  4 Batch  398 / 525  Training Loss  0.1075998991727829\n",
            "Epoch  4 Batch  399 / 525  Training Loss  0.09061520546674728\n",
            "Epoch  4 Batch  400 / 525  Training Loss  0.09485526382923126\n",
            "Epoch  4 Batch  401 / 525  Training Loss  0.1013011485338211\n",
            "Epoch  4 Batch  402 / 525  Training Loss  0.10648095607757568\n",
            "Epoch  4 Batch  403 / 525  Training Loss  0.09252333641052246\n",
            "Epoch  4 Batch  404 / 525  Training Loss  0.09319056570529938\n",
            "Epoch  4 Batch  405 / 525  Training Loss  0.11069560050964355\n",
            "Epoch  4 Batch  406 / 525  Training Loss  0.09467430412769318\n",
            "Epoch  4 Batch  407 / 525  Training Loss  0.0931493267416954\n",
            "Epoch  4 Batch  408 / 525  Training Loss  0.07485033571720123\n",
            "Epoch  4 Batch  409 / 525  Training Loss  0.08613981306552887\n",
            "Epoch  4 Batch  410 / 525  Training Loss  0.09404651075601578\n",
            "Epoch  4 Batch  411 / 525  Training Loss  0.0830317959189415\n",
            "Epoch  4 Batch  412 / 525  Training Loss  0.08723008632659912\n",
            "Epoch  4 Batch  413 / 525  Training Loss  0.08069513738155365\n",
            "Epoch  4 Batch  414 / 525  Training Loss  0.09973247349262238\n",
            "Epoch  4 Batch  415 / 525  Training Loss  0.09632659703493118\n",
            "Epoch  4 Batch  416 / 525  Training Loss  0.09318222105503082\n",
            "Epoch  4 Batch  417 / 525  Training Loss  0.11264859139919281\n",
            "Epoch  4 Batch  418 / 525  Training Loss  0.10842619091272354\n",
            "Epoch  4 Batch  419 / 525  Training Loss  0.0916072353720665\n",
            "Epoch  4 Batch  420 / 525  Training Loss  0.07743076235055923\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  4 Batch  421 / 525  Training Loss  0.08276708424091339\n",
            "Epoch  4 Batch  422 / 525  Training Loss  0.09220509231090546\n",
            "Epoch  4 Batch  423 / 525  Training Loss  0.09318137168884277\n",
            "Epoch  4 Batch  424 / 525  Training Loss  0.08801974356174469\n",
            "Epoch  4 Batch  425 / 525  Training Loss  0.08218467235565186\n",
            "Epoch  4 Batch  426 / 525  Training Loss  0.0938630998134613\n",
            "Epoch  4 Batch  427 / 525  Training Loss  0.1018986701965332\n",
            "Epoch  4 Batch  428 / 525  Training Loss  0.11323714256286621\n",
            "Epoch  4 Batch  429 / 525  Training Loss  0.08457614481449127\n",
            "Epoch  4 Batch  430 / 525  Training Loss  0.09282092750072479\n",
            "Epoch  4 Batch  431 / 525  Training Loss  0.0843370258808136\n",
            "Epoch  4 Batch  432 / 525  Training Loss  0.0794774740934372\n",
            "Epoch  4 Batch  433 / 525  Training Loss  0.08010825514793396\n",
            "Epoch  4 Batch  434 / 525  Training Loss  0.08540026098489761\n",
            "Epoch  4 Batch  435 / 525  Training Loss  0.09910035133361816\n",
            "Epoch  4 Batch  436 / 525  Training Loss  0.09215344488620758\n",
            "Epoch  4 Batch  437 / 525  Training Loss  0.056893240660429\n",
            "Epoch  4 Batch  438 / 525  Training Loss  0.09038721024990082\n",
            "Epoch  4 Batch  439 / 525  Training Loss  0.08519713580608368\n",
            "Epoch  4 Batch  440 / 525  Training Loss  0.0930132046341896\n",
            "Epoch  4 Batch  441 / 525  Training Loss  0.07939403504133224\n",
            "Epoch  4 Batch  442 / 525  Training Loss  0.09842536598443985\n",
            "Epoch  4 Batch  443 / 525  Training Loss  0.09592317044734955\n",
            "Epoch  4 Batch  444 / 525  Training Loss  0.09614325314760208\n",
            "Epoch  4 Batch  445 / 525  Training Loss  0.09744079411029816\n",
            "Epoch  4 Batch  446 / 525  Training Loss  0.10196004807949066\n",
            "Epoch  4 Batch  447 / 525  Training Loss  0.08048151433467865\n",
            "Epoch  4 Batch  448 / 525  Training Loss  0.09348385035991669\n",
            "Epoch  4 Batch  449 / 525  Training Loss  0.09348593652248383\n",
            "Epoch  4 Batch  450 / 525  Training Loss  0.07671535760164261\n",
            "Epoch  4 Batch  451 / 525  Training Loss  0.10907995700836182\n",
            "Epoch  4 Batch  452 / 525  Training Loss  0.0777411088347435\n",
            "Epoch  4 Batch  453 / 525  Training Loss  0.08082384616136551\n",
            "Epoch  4 Batch  454 / 525  Training Loss  0.10148368030786514\n",
            "Epoch  4 Batch  455 / 525  Training Loss  0.06917446851730347\n",
            "Epoch  4 Batch  456 / 525  Training Loss  0.08719182759523392\n",
            "Epoch  4 Batch  457 / 525  Training Loss  0.09796817600727081\n",
            "Epoch  4 Batch  458 / 525  Training Loss  0.08039555698633194\n",
            "Epoch  4 Batch  459 / 525  Training Loss  0.070327028632164\n",
            "Epoch  4 Batch  460 / 525  Training Loss  0.07257810235023499\n",
            "Epoch  4 Batch  461 / 525  Training Loss  0.11393763870000839\n",
            "Epoch  4 Batch  462 / 525  Training Loss  0.09094538539648056\n",
            "Epoch  4 Batch  463 / 525  Training Loss  0.09357907623052597\n",
            "Epoch  4 Batch  464 / 525  Training Loss  0.09780766069889069\n",
            "Epoch  4 Batch  465 / 525  Training Loss  0.09273694455623627\n",
            "Epoch  4 Batch  466 / 525  Training Loss  0.09062755852937698\n",
            "Epoch  4 Batch  467 / 525  Training Loss  0.09294017404317856\n",
            "Epoch  4 Batch  468 / 525  Training Loss  0.09512972086668015\n",
            "Epoch  4 Batch  469 / 525  Training Loss  0.09397751837968826\n",
            "Epoch  4 Batch  470 / 525  Training Loss  0.09071282297372818\n",
            "Epoch  4 Batch  471 / 525  Training Loss  0.09693503379821777\n",
            "Epoch  4 Batch  472 / 525  Training Loss  0.08017993718385696\n",
            "Epoch  4 Batch  473 / 525  Training Loss  0.09146042913198471\n",
            "Epoch  4 Batch  474 / 525  Training Loss  0.0961904227733612\n",
            "Epoch  4 Batch  475 / 525  Training Loss  0.10176829248666763\n",
            "Epoch  4 Batch  476 / 525  Training Loss  0.07494977861642838\n",
            "Epoch  4 Batch  477 / 525  Training Loss  0.09006567299365997\n",
            "Epoch  4 Batch  478 / 525  Training Loss  0.06385964155197144\n",
            "Epoch  4 Batch  479 / 525  Training Loss  0.10337777435779572\n",
            "Epoch  4 Batch  480 / 525  Training Loss  0.06386280804872513\n",
            "Epoch  4 Batch  481 / 525  Training Loss  0.09967382252216339\n",
            "Epoch  4 Batch  482 / 525  Training Loss  0.08090677112340927\n",
            "Epoch  4 Batch  483 / 525  Training Loss  0.10177265107631683\n",
            "Epoch  4 Batch  484 / 525  Training Loss  0.09919437766075134\n",
            "Epoch  4 Batch  485 / 525  Training Loss  0.10555752366781235\n",
            "Epoch  4 Batch  486 / 525  Training Loss  0.09016172587871552\n",
            "Epoch  4 Batch  487 / 525  Training Loss  0.10403995215892792\n",
            "Epoch  4 Batch  488 / 525  Training Loss  0.08553757518529892\n",
            "Epoch  4 Batch  489 / 525  Training Loss  0.09511137008666992\n",
            "Epoch  4 Batch  490 / 525  Training Loss  0.07639189064502716\n",
            "Epoch  4 Batch  491 / 525  Training Loss  0.09386461973190308\n",
            "Epoch  4 Batch  492 / 525  Training Loss  0.09117155522108078\n",
            "Epoch  4 Batch  493 / 525  Training Loss  0.09673841297626495\n",
            "Epoch  4 Batch  494 / 525  Training Loss  0.08821383863687515\n",
            "Epoch  4 Batch  495 / 525  Training Loss  0.0996391549706459\n",
            "Epoch  4 Batch  496 / 525  Training Loss  0.1097390428185463\n",
            "Epoch  4 Batch  497 / 525  Training Loss  0.10071249306201935\n",
            "Epoch  4 Batch  498 / 525  Training Loss  0.08485554158687592\n",
            "Epoch  4 Batch  499 / 525  Training Loss  0.07305868715047836\n",
            "Epoch  4 Batch  500 / 525  Training Loss  0.08440760523080826\n",
            "Epoch  4 Batch  501 / 525  Training Loss  0.09303645044565201\n",
            "Epoch  4 Batch  502 / 525  Training Loss  0.09710218757390976\n",
            "Epoch  4 Batch  503 / 525  Training Loss  0.11328189074993134\n",
            "Epoch  4 Batch  504 / 525  Training Loss  0.11636859178543091\n",
            "Epoch  4 Batch  505 / 525  Training Loss  0.1031179428100586\n",
            "Epoch  4 Batch  506 / 525  Training Loss  0.094281867146492\n",
            "Epoch  4 Batch  507 / 525  Training Loss  0.09987346827983856\n",
            "Epoch  4 Batch  508 / 525  Training Loss  0.11525758355855942\n",
            "Epoch  4 Batch  509 / 525  Training Loss  0.0919061154127121\n",
            "Epoch  4 Batch  510 / 525  Training Loss  0.09135638177394867\n",
            "Epoch  4 Batch  511 / 525  Training Loss  0.11473164707422256\n",
            "Epoch  4 Batch  512 / 525  Training Loss  0.07275881618261337\n",
            "Epoch  4 Batch  513 / 525  Training Loss  0.08938908576965332\n",
            "Epoch  4 Batch  514 / 525  Training Loss  0.07949624955654144\n",
            "Epoch  4 Batch  515 / 525  Training Loss  0.08746035397052765\n",
            "Epoch  4 Batch  516 / 525  Training Loss  0.07928064465522766\n",
            "Epoch  4 Batch  517 / 525  Training Loss  0.08658833801746368\n",
            "Epoch  4 Batch  518 / 525  Training Loss  0.10084044933319092\n",
            "Epoch  4 Batch  519 / 525  Training Loss  0.0821438580751419\n",
            "Epoch  4 Batch  520 / 525  Training Loss  0.09245681017637253\n",
            "Epoch  4 Batch  521 / 525  Training Loss  0.09178976714611053\n",
            "Epoch  4 Batch  522 / 525  Training Loss  0.08986711502075195\n",
            "Epoch  4 Batch  523 / 525  Training Loss  0.08748562633991241\n",
            "Epoch  4 Batch  524 / 525  Training Loss  0.09314848482608795\n",
            "   5    |    -    |   0.095146   | 35.158333\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 5\n",
            "Epoch  5 Batch  0 / 525  Training Loss  0.06224587559700012\n",
            "Epoch  5 Batch  1 / 525  Training Loss  0.07871010154485703\n",
            "Epoch  5 Batch  2 / 525  Training Loss  0.0885215625166893\n",
            "Epoch  5 Batch  3 / 525  Training Loss  0.07693687826395035\n",
            "Epoch  5 Batch  4 / 525  Training Loss  0.11656711995601654\n",
            "Epoch  5 Batch  5 / 525  Training Loss  0.09422153979539871\n",
            "Epoch  5 Batch  6 / 525  Training Loss  0.07010392844676971\n",
            "Epoch  5 Batch  7 / 525  Training Loss  0.08202008903026581\n",
            "Epoch  5 Batch  8 / 525  Training Loss  0.10780684649944305\n",
            "Epoch  5 Batch  9 / 525  Training Loss  0.0943264588713646\n",
            "Epoch  5 Batch  10 / 525  Training Loss  0.08362768590450287\n",
            "Epoch  5 Batch  11 / 525  Training Loss  0.09690498560667038\n",
            "Epoch  5 Batch  12 / 525  Training Loss  0.08766849339008331\n",
            "Epoch  5 Batch  13 / 525  Training Loss  0.0652497336268425\n",
            "Epoch  5 Batch  14 / 525  Training Loss  0.0958755612373352\n",
            "Epoch  5 Batch  15 / 525  Training Loss  0.09693572670221329\n",
            "Epoch  5 Batch  16 / 525  Training Loss  0.10378599166870117\n",
            "Epoch  5 Batch  17 / 525  Training Loss  0.07919099181890488\n",
            "Epoch  5 Batch  18 / 525  Training Loss  0.089784637093544\n",
            "Epoch  5 Batch  19 / 525  Training Loss  0.10395941883325577\n",
            "Epoch  5 Batch  20 / 525  Training Loss  0.0974331647157669\n",
            "Epoch  5 Batch  21 / 525  Training Loss  0.09374615550041199\n",
            "Epoch  5 Batch  22 / 525  Training Loss  0.07275322824716568\n",
            "Epoch  5 Batch  23 / 525  Training Loss  0.08555353432893753\n",
            "Epoch  5 Batch  24 / 525  Training Loss  0.09335866570472717\n",
            "Epoch  5 Batch  25 / 525  Training Loss  0.07730771601200104\n",
            "Epoch  5 Batch  26 / 525  Training Loss  0.09758545458316803\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  5 Batch  27 / 525  Training Loss  0.07284285128116608\n",
            "Epoch  5 Batch  28 / 525  Training Loss  0.06274215131998062\n",
            "Epoch  5 Batch  29 / 525  Training Loss  0.07969578355550766\n",
            "Epoch  5 Batch  30 / 525  Training Loss  0.09062634408473969\n",
            "Epoch  5 Batch  31 / 525  Training Loss  0.09302347898483276\n",
            "Epoch  5 Batch  32 / 525  Training Loss  0.10911080986261368\n",
            "Epoch  5 Batch  33 / 525  Training Loss  0.09366266429424286\n",
            "Epoch  5 Batch  34 / 525  Training Loss  0.0885808914899826\n",
            "Epoch  5 Batch  35 / 525  Training Loss  0.11087153106927872\n",
            "Epoch  5 Batch  36 / 525  Training Loss  0.07193704694509506\n",
            "Epoch  5 Batch  37 / 525  Training Loss  0.0972888246178627\n",
            "Epoch  5 Batch  38 / 525  Training Loss  0.09386542439460754\n",
            "Epoch  5 Batch  39 / 525  Training Loss  0.08188822120428085\n",
            "Epoch  5 Batch  40 / 525  Training Loss  0.09676030278205872\n",
            "Epoch  5 Batch  41 / 525  Training Loss  0.09051892906427383\n",
            "Epoch  5 Batch  42 / 525  Training Loss  0.08279334008693695\n",
            "Epoch  5 Batch  43 / 525  Training Loss  0.09368286281824112\n",
            "Epoch  5 Batch  44 / 525  Training Loss  0.08509307354688644\n",
            "Epoch  5 Batch  45 / 525  Training Loss  0.08638791739940643\n",
            "Epoch  5 Batch  46 / 525  Training Loss  0.07416807860136032\n",
            "Epoch  5 Batch  47 / 525  Training Loss  0.07387705147266388\n",
            "Epoch  5 Batch  48 / 525  Training Loss  0.07796244323253632\n",
            "Epoch  5 Batch  49 / 525  Training Loss  0.0897940844297409\n",
            "Epoch  5 Batch  50 / 525  Training Loss  0.07947449386119843\n",
            "Epoch  5 Batch  51 / 525  Training Loss  0.07871653139591217\n",
            "Epoch  5 Batch  52 / 525  Training Loss  0.0653923749923706\n",
            "Epoch  5 Batch  53 / 525  Training Loss  0.09295158088207245\n",
            "Epoch  5 Batch  54 / 525  Training Loss  0.08335865288972855\n",
            "Epoch  5 Batch  55 / 525  Training Loss  0.09515450149774551\n",
            "Epoch  5 Batch  56 / 525  Training Loss  0.09180940687656403\n",
            "Epoch  5 Batch  57 / 525  Training Loss  0.0801314041018486\n",
            "Epoch  5 Batch  58 / 525  Training Loss  0.08635952323675156\n",
            "Epoch  5 Batch  59 / 525  Training Loss  0.10339377820491791\n",
            "Epoch  5 Batch  60 / 525  Training Loss  0.09192240983247757\n",
            "Epoch  5 Batch  61 / 525  Training Loss  0.09077845513820648\n",
            "Epoch  5 Batch  62 / 525  Training Loss  0.08168655633926392\n",
            "Epoch  5 Batch  63 / 525  Training Loss  0.0847039446234703\n",
            "Epoch  5 Batch  64 / 525  Training Loss  0.07486838847398758\n",
            "Epoch  5 Batch  65 / 525  Training Loss  0.09438350796699524\n",
            "Epoch  5 Batch  66 / 525  Training Loss  0.08455858379602432\n",
            "Epoch  5 Batch  67 / 525  Training Loss  0.08647502958774567\n",
            "Epoch  5 Batch  68 / 525  Training Loss  0.09975931793451309\n",
            "Epoch  5 Batch  69 / 525  Training Loss  0.09555579721927643\n",
            "Epoch  5 Batch  70 / 525  Training Loss  0.07052822411060333\n",
            "Epoch  5 Batch  71 / 525  Training Loss  0.07635779678821564\n",
            "Epoch  5 Batch  72 / 525  Training Loss  0.07714155316352844\n",
            "Epoch  5 Batch  73 / 525  Training Loss  0.08628173172473907\n",
            "Epoch  5 Batch  74 / 525  Training Loss  0.0786801278591156\n",
            "Epoch  5 Batch  75 / 525  Training Loss  0.09602914750576019\n",
            "Epoch  5 Batch  76 / 525  Training Loss  0.10089763253927231\n",
            "Epoch  5 Batch  77 / 525  Training Loss  0.10332940518856049\n",
            "Epoch  5 Batch  78 / 525  Training Loss  0.08180360496044159\n",
            "Epoch  5 Batch  79 / 525  Training Loss  0.08735569566488266\n",
            "Epoch  5 Batch  80 / 525  Training Loss  0.09187738597393036\n",
            "Epoch  5 Batch  81 / 525  Training Loss  0.09146182239055634\n",
            "Epoch  5 Batch  82 / 525  Training Loss  0.0938897356390953\n",
            "Epoch  5 Batch  83 / 525  Training Loss  0.09890742599964142\n",
            "Epoch  5 Batch  84 / 525  Training Loss  0.09838452190160751\n",
            "Epoch  5 Batch  85 / 525  Training Loss  0.08096526563167572\n",
            "Epoch  5 Batch  86 / 525  Training Loss  0.08664878457784653\n",
            "Epoch  5 Batch  87 / 525  Training Loss  0.07783600687980652\n",
            "Epoch  5 Batch  88 / 525  Training Loss  0.1014537662267685\n",
            "Epoch  5 Batch  89 / 525  Training Loss  0.08025829493999481\n",
            "Epoch  5 Batch  90 / 525  Training Loss  0.08596275746822357\n",
            "Epoch  5 Batch  91 / 525  Training Loss  0.08989022672176361\n",
            "Epoch  5 Batch  92 / 525  Training Loss  0.09161457419395447\n",
            "Epoch  5 Batch  93 / 525  Training Loss  0.08264653384685516\n",
            "Epoch  5 Batch  94 / 525  Training Loss  0.10109499841928482\n",
            "Epoch  5 Batch  95 / 525  Training Loss  0.10227914154529572\n",
            "Epoch  5 Batch  96 / 525  Training Loss  0.08349400758743286\n",
            "Epoch  5 Batch  97 / 525  Training Loss  0.10335390269756317\n",
            "Epoch  5 Batch  98 / 525  Training Loss  0.08274505287408829\n",
            "Epoch  5 Batch  99 / 525  Training Loss  0.0814640149474144\n",
            "Epoch  5 Batch  100 / 525  Training Loss  0.07934585958719254\n",
            "Epoch  5 Batch  101 / 525  Training Loss  0.10338453948497772\n",
            "Epoch  5 Batch  102 / 525  Training Loss  0.10199303925037384\n",
            "Epoch  5 Batch  103 / 525  Training Loss  0.08164878189563751\n",
            "Epoch  5 Batch  104 / 525  Training Loss  0.07335066050291061\n",
            "Epoch  5 Batch  105 / 525  Training Loss  0.06986083090305328\n",
            "Epoch  5 Batch  106 / 525  Training Loss  0.093716099858284\n",
            "Epoch  5 Batch  107 / 525  Training Loss  0.09299227595329285\n",
            "Epoch  5 Batch  108 / 525  Training Loss  0.07467421144247055\n",
            "Epoch  5 Batch  109 / 525  Training Loss  0.07033385336399078\n",
            "Epoch  5 Batch  110 / 525  Training Loss  0.09193706512451172\n",
            "Epoch  5 Batch  111 / 525  Training Loss  0.08909578621387482\n",
            "Epoch  5 Batch  112 / 525  Training Loss  0.10181577503681183\n",
            "Epoch  5 Batch  113 / 525  Training Loss  0.09098149836063385\n",
            "Epoch  5 Batch  114 / 525  Training Loss  0.0977790355682373\n",
            "Epoch  5 Batch  115 / 525  Training Loss  0.09473074972629547\n",
            "Epoch  5 Batch  116 / 525  Training Loss  0.08443371206521988\n",
            "Epoch  5 Batch  117 / 525  Training Loss  0.0972476676106453\n",
            "Epoch  5 Batch  118 / 525  Training Loss  0.08249607682228088\n",
            "Epoch  5 Batch  119 / 525  Training Loss  0.0984143614768982\n",
            "Epoch  5 Batch  120 / 525  Training Loss  0.08627650141716003\n",
            "Epoch  5 Batch  121 / 525  Training Loss  0.08628419041633606\n",
            "Epoch  5 Batch  122 / 525  Training Loss  0.09118432551622391\n",
            "Epoch  5 Batch  123 / 525  Training Loss  0.08328279107809067\n",
            "Epoch  5 Batch  124 / 525  Training Loss  0.07037165015935898\n",
            "Epoch  5 Batch  125 / 525  Training Loss  0.08764086663722992\n",
            "Epoch  5 Batch  126 / 525  Training Loss  0.08757957816123962\n",
            "Epoch  5 Batch  127 / 525  Training Loss  0.09471580386161804\n",
            "Epoch  5 Batch  128 / 525  Training Loss  0.08383359014987946\n",
            "Epoch  5 Batch  129 / 525  Training Loss  0.10124757140874863\n",
            "Epoch  5 Batch  130 / 525  Training Loss  0.09289427846670151\n",
            "Epoch  5 Batch  131 / 525  Training Loss  0.0849136933684349\n",
            "Epoch  5 Batch  132 / 525  Training Loss  0.10205813497304916\n",
            "Epoch  5 Batch  133 / 525  Training Loss  0.08613242954015732\n",
            "Epoch  5 Batch  134 / 525  Training Loss  0.08183246850967407\n",
            "Epoch  5 Batch  135 / 525  Training Loss  0.10490088164806366\n",
            "Epoch  5 Batch  136 / 525  Training Loss  0.09486552327871323\n",
            "Epoch  5 Batch  137 / 525  Training Loss  0.09305785596370697\n",
            "Epoch  5 Batch  138 / 525  Training Loss  0.0964418426156044\n",
            "Epoch  5 Batch  139 / 525  Training Loss  0.08827924728393555\n",
            "Epoch  5 Batch  140 / 525  Training Loss  0.09122592955827713\n",
            "Epoch  5 Batch  141 / 525  Training Loss  0.10167314857244492\n",
            "Epoch  5 Batch  142 / 525  Training Loss  0.1042824536561966\n",
            "Epoch  5 Batch  143 / 525  Training Loss  0.0943608507514\n",
            "Epoch  5 Batch  144 / 525  Training Loss  0.0943923369050026\n",
            "Epoch  5 Batch  145 / 525  Training Loss  0.09483528137207031\n",
            "Epoch  5 Batch  146 / 525  Training Loss  0.10164390504360199\n",
            "Epoch  5 Batch  147 / 525  Training Loss  0.09459761530160904\n",
            "Epoch  5 Batch  148 / 525  Training Loss  0.06797871738672256\n",
            "Epoch  5 Batch  149 / 525  Training Loss  0.09639997035264969\n",
            "Epoch  5 Batch  150 / 525  Training Loss  0.08617111295461655\n",
            "Epoch  5 Batch  151 / 525  Training Loss  0.10117405652999878\n",
            "Epoch  5 Batch  152 / 525  Training Loss  0.06587035953998566\n",
            "Epoch  5 Batch  153 / 525  Training Loss  0.08738350123167038\n",
            "Epoch  5 Batch  154 / 525  Training Loss  0.09753192961215973\n",
            "Epoch  5 Batch  155 / 525  Training Loss  0.09085243195295334\n",
            "Epoch  5 Batch  156 / 525  Training Loss  0.06091093271970749\n",
            "Epoch  5 Batch  157 / 525  Training Loss  0.07624979317188263\n",
            "Epoch  5 Batch  158 / 525  Training Loss  0.08613135665655136\n",
            "Epoch  5 Batch  159 / 525  Training Loss  0.11273517459630966\n",
            "Epoch  5 Batch  160 / 525  Training Loss  0.08899601548910141\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  5 Batch  161 / 525  Training Loss  0.06963609158992767\n",
            "Epoch  5 Batch  162 / 525  Training Loss  0.06272320449352264\n",
            "Epoch  5 Batch  163 / 525  Training Loss  0.09378349781036377\n",
            "Epoch  5 Batch  164 / 525  Training Loss  0.08944177627563477\n",
            "Epoch  5 Batch  165 / 525  Training Loss  0.10312272608280182\n",
            "Epoch  5 Batch  166 / 525  Training Loss  0.06568829715251923\n",
            "Epoch  5 Batch  167 / 525  Training Loss  0.11190564930438995\n",
            "Epoch  5 Batch  168 / 525  Training Loss  0.0836770161986351\n",
            "Epoch  5 Batch  169 / 525  Training Loss  0.06460030376911163\n",
            "Epoch  5 Batch  170 / 525  Training Loss  0.09625106304883957\n",
            "Epoch  5 Batch  171 / 525  Training Loss  0.08552534878253937\n",
            "Epoch  5 Batch  172 / 525  Training Loss  0.0942496508359909\n",
            "Epoch  5 Batch  173 / 525  Training Loss  0.10292712599039078\n",
            "Epoch  5 Batch  174 / 525  Training Loss  0.07329490035772324\n",
            "Epoch  5 Batch  175 / 525  Training Loss  0.09045764058828354\n",
            "Epoch  5 Batch  176 / 525  Training Loss  0.0817100778222084\n",
            "Epoch  5 Batch  177 / 525  Training Loss  0.06778883188962936\n",
            "Epoch  5 Batch  178 / 525  Training Loss  0.06590095907449722\n",
            "Epoch  5 Batch  179 / 525  Training Loss  0.07882596552371979\n",
            "Epoch  5 Batch  180 / 525  Training Loss  0.0846477597951889\n",
            "Epoch  5 Batch  181 / 525  Training Loss  0.08816979825496674\n",
            "Epoch  5 Batch  182 / 525  Training Loss  0.10162577778100967\n",
            "Epoch  5 Batch  183 / 525  Training Loss  0.08986905962228775\n",
            "Epoch  5 Batch  184 / 525  Training Loss  0.08409496396780014\n",
            "Epoch  5 Batch  185 / 525  Training Loss  0.09382767975330353\n",
            "Epoch  5 Batch  186 / 525  Training Loss  0.10449911653995514\n",
            "Epoch  5 Batch  187 / 525  Training Loss  0.07777796685695648\n",
            "Epoch  5 Batch  188 / 525  Training Loss  0.08697617799043655\n",
            "Epoch  5 Batch  189 / 525  Training Loss  0.08557531237602234\n",
            "Epoch  5 Batch  190 / 525  Training Loss  0.09643558412790298\n",
            "Epoch  5 Batch  191 / 525  Training Loss  0.1046050414443016\n",
            "Epoch  5 Batch  192 / 525  Training Loss  0.09189344197511673\n",
            "Epoch  5 Batch  193 / 525  Training Loss  0.06754978746175766\n",
            "Epoch  5 Batch  194 / 525  Training Loss  0.08561258018016815\n",
            "Epoch  5 Batch  195 / 525  Training Loss  0.09164146333932877\n",
            "Epoch  5 Batch  196 / 525  Training Loss  0.08398240059614182\n",
            "Epoch  5 Batch  197 / 525  Training Loss  0.08629113435745239\n",
            "Epoch  5 Batch  198 / 525  Training Loss  0.061943668872117996\n",
            "Epoch  5 Batch  199 / 525  Training Loss  0.09564390033483505\n",
            "Epoch  5 Batch  200 / 525  Training Loss  0.09389622509479523\n",
            "Epoch  5 Batch  201 / 525  Training Loss  0.0750429779291153\n",
            "Epoch  5 Batch  202 / 525  Training Loss  0.09990398585796356\n",
            "Epoch  5 Batch  203 / 525  Training Loss  0.09765144437551498\n",
            "Epoch  5 Batch  204 / 525  Training Loss  0.08294631540775299\n",
            "Epoch  5 Batch  205 / 525  Training Loss  0.07676038891077042\n",
            "Epoch  5 Batch  206 / 525  Training Loss  0.08459170907735825\n",
            "Epoch  5 Batch  207 / 525  Training Loss  0.09047157317399979\n",
            "Epoch  5 Batch  208 / 525  Training Loss  0.08211580663919449\n",
            "Epoch  5 Batch  209 / 525  Training Loss  0.08909446746110916\n",
            "Epoch  5 Batch  210 / 525  Training Loss  0.07845939695835114\n",
            "Epoch  5 Batch  211 / 525  Training Loss  0.07323804497718811\n",
            "Epoch  5 Batch  212 / 525  Training Loss  0.0825233906507492\n",
            "Epoch  5 Batch  213 / 525  Training Loss  0.06456510722637177\n",
            "Epoch  5 Batch  214 / 525  Training Loss  0.10147496312856674\n",
            "Epoch  5 Batch  215 / 525  Training Loss  0.08174832165241241\n",
            "Epoch  5 Batch  216 / 525  Training Loss  0.08198396861553192\n",
            "Epoch  5 Batch  217 / 525  Training Loss  0.10874490439891815\n",
            "Epoch  5 Batch  218 / 525  Training Loss  0.08477453887462616\n",
            "Epoch  5 Batch  219 / 525  Training Loss  0.09743200242519379\n",
            "Epoch  5 Batch  220 / 525  Training Loss  0.07880142331123352\n",
            "Epoch  5 Batch  221 / 525  Training Loss  0.0957142561674118\n",
            "Epoch  5 Batch  222 / 525  Training Loss  0.0927819013595581\n",
            "Epoch  5 Batch  223 / 525  Training Loss  0.06584951281547546\n",
            "Epoch  5 Batch  224 / 525  Training Loss  0.08843723684549332\n",
            "Epoch  5 Batch  225 / 525  Training Loss  0.07861484587192535\n",
            "Epoch  5 Batch  226 / 525  Training Loss  0.07322777807712555\n",
            "Epoch  5 Batch  227 / 525  Training Loss  0.08239109814167023\n",
            "Epoch  5 Batch  228 / 525  Training Loss  0.0955503061413765\n",
            "Epoch  5 Batch  229 / 525  Training Loss  0.08940500766038895\n",
            "Epoch  5 Batch  230 / 525  Training Loss  0.10214831680059433\n",
            "Epoch  5 Batch  231 / 525  Training Loss  0.0806487500667572\n",
            "Epoch  5 Batch  232 / 525  Training Loss  0.0803307443857193\n",
            "Epoch  5 Batch  233 / 525  Training Loss  0.09945473074913025\n",
            "Epoch  5 Batch  234 / 525  Training Loss  0.08071090281009674\n",
            "Epoch  5 Batch  235 / 525  Training Loss  0.08449620753526688\n",
            "Epoch  5 Batch  236 / 525  Training Loss  0.07406951487064362\n",
            "Epoch  5 Batch  237 / 525  Training Loss  0.10604164749383926\n",
            "Epoch  5 Batch  238 / 525  Training Loss  0.08569999784231186\n",
            "Epoch  5 Batch  239 / 525  Training Loss  0.09069611877202988\n",
            "Epoch  5 Batch  240 / 525  Training Loss  0.08826542645692825\n",
            "Epoch  5 Batch  241 / 525  Training Loss  0.062419842928647995\n",
            "Epoch  5 Batch  242 / 525  Training Loss  0.08133146911859512\n",
            "Epoch  5 Batch  243 / 525  Training Loss  0.07701073586940765\n",
            "Epoch  5 Batch  244 / 525  Training Loss  0.07027266919612885\n",
            "Epoch  5 Batch  245 / 525  Training Loss  0.09210298955440521\n",
            "Epoch  5 Batch  246 / 525  Training Loss  0.08169469982385635\n",
            "Epoch  5 Batch  247 / 525  Training Loss  0.09064039587974548\n",
            "Epoch  5 Batch  248 / 525  Training Loss  0.08480747044086456\n",
            "Epoch  5 Batch  249 / 525  Training Loss  0.08946112543344498\n",
            "Epoch  5 Batch  250 / 525  Training Loss  0.06350623071193695\n",
            "Epoch  5 Batch  251 / 525  Training Loss  0.07636398822069168\n",
            "Epoch  5 Batch  252 / 525  Training Loss  0.08958269655704498\n",
            "Epoch  5 Batch  253 / 525  Training Loss  0.0658469870686531\n",
            "Epoch  5 Batch  254 / 525  Training Loss  0.09155656397342682\n",
            "Epoch  5 Batch  255 / 525  Training Loss  0.07846289128065109\n",
            "Epoch  5 Batch  256 / 525  Training Loss  0.05578091740608215\n",
            "Epoch  5 Batch  257 / 525  Training Loss  0.09008379280567169\n",
            "Epoch  5 Batch  258 / 525  Training Loss  0.08248502761125565\n",
            "Epoch  5 Batch  259 / 525  Training Loss  0.07973740249872208\n",
            "Epoch  5 Batch  260 / 525  Training Loss  0.08143197000026703\n",
            "Epoch  5 Batch  261 / 525  Training Loss  0.08573265373706818\n",
            "Epoch  5 Batch  262 / 525  Training Loss  0.09511864930391312\n",
            "Epoch  5 Batch  263 / 525  Training Loss  0.08848230540752411\n",
            "Epoch  5 Batch  264 / 525  Training Loss  0.09002711623907089\n",
            "Epoch  5 Batch  265 / 525  Training Loss  0.08111335337162018\n",
            "Epoch  5 Batch  266 / 525  Training Loss  0.0916367843747139\n",
            "Epoch  5 Batch  267 / 525  Training Loss  0.09480530023574829\n",
            "Epoch  5 Batch  268 / 525  Training Loss  0.07820333540439606\n",
            "Epoch  5 Batch  269 / 525  Training Loss  0.08779841661453247\n",
            "Epoch  5 Batch  270 / 525  Training Loss  0.08501344919204712\n",
            "Epoch  5 Batch  271 / 525  Training Loss  0.1018209308385849\n",
            "Epoch  5 Batch  272 / 525  Training Loss  0.08306888490915298\n",
            "Epoch  5 Batch  273 / 525  Training Loss  0.093134306371212\n",
            "Epoch  5 Batch  274 / 525  Training Loss  0.08461154997348785\n",
            "Epoch  5 Batch  275 / 525  Training Loss  0.08915987610816956\n",
            "Epoch  5 Batch  276 / 525  Training Loss  0.07603771984577179\n",
            "Epoch  5 Batch  277 / 525  Training Loss  0.09393328428268433\n",
            "Epoch  5 Batch  278 / 525  Training Loss  0.08668732643127441\n",
            "Epoch  5 Batch  279 / 525  Training Loss  0.09893235564231873\n",
            "Epoch  5 Batch  280 / 525  Training Loss  0.0723239928483963\n",
            "Epoch  5 Batch  281 / 525  Training Loss  0.07337526977062225\n",
            "Epoch  5 Batch  282 / 525  Training Loss  0.0835774689912796\n",
            "Epoch  5 Batch  283 / 525  Training Loss  0.10448752343654633\n",
            "Epoch  5 Batch  284 / 525  Training Loss  0.0797620639204979\n",
            "Epoch  5 Batch  285 / 525  Training Loss  0.0747731477022171\n",
            "Epoch  5 Batch  286 / 525  Training Loss  0.07382816076278687\n",
            "Epoch  5 Batch  287 / 525  Training Loss  0.07911507785320282\n",
            "Epoch  5 Batch  288 / 525  Training Loss  0.09033790975809097\n",
            "Epoch  5 Batch  289 / 525  Training Loss  0.09041941910982132\n",
            "Epoch  5 Batch  290 / 525  Training Loss  0.07587700337171555\n",
            "Epoch  5 Batch  291 / 525  Training Loss  0.08893062174320221\n",
            "Epoch  5 Batch  292 / 525  Training Loss  0.08290746062994003\n",
            "Epoch  5 Batch  293 / 525  Training Loss  0.07740991562604904\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  5 Batch  294 / 525  Training Loss  0.06804446876049042\n",
            "Epoch  5 Batch  295 / 525  Training Loss  0.0745733380317688\n",
            "Epoch  5 Batch  296 / 525  Training Loss  0.0875098928809166\n",
            "Epoch  5 Batch  297 / 525  Training Loss  0.08220306038856506\n",
            "Epoch  5 Batch  298 / 525  Training Loss  0.06916508078575134\n",
            "Epoch  5 Batch  299 / 525  Training Loss  0.07700464874505997\n",
            "Epoch  5 Batch  300 / 525  Training Loss  0.09248904883861542\n",
            "Epoch  5 Batch  301 / 525  Training Loss  0.08093556761741638\n",
            "Epoch  5 Batch  302 / 525  Training Loss  0.09611638635396957\n",
            "Epoch  5 Batch  303 / 525  Training Loss  0.08819814771413803\n",
            "Epoch  5 Batch  304 / 525  Training Loss  0.09980051219463348\n",
            "Epoch  5 Batch  305 / 525  Training Loss  0.07466167956590652\n",
            "Epoch  5 Batch  306 / 525  Training Loss  0.0747973769903183\n",
            "Epoch  5 Batch  307 / 525  Training Loss  0.09873078018426895\n",
            "Epoch  5 Batch  308 / 525  Training Loss  0.07704982161521912\n",
            "Epoch  5 Batch  309 / 525  Training Loss  0.08414680510759354\n",
            "Epoch  5 Batch  310 / 525  Training Loss  0.07618177682161331\n",
            "Epoch  5 Batch  311 / 525  Training Loss  0.08195864409208298\n",
            "Epoch  5 Batch  312 / 525  Training Loss  0.0716833621263504\n",
            "Epoch  5 Batch  313 / 525  Training Loss  0.07065986096858978\n",
            "Epoch  5 Batch  314 / 525  Training Loss  0.07512686401605606\n",
            "Epoch  5 Batch  315 / 525  Training Loss  0.09613317996263504\n",
            "Epoch  5 Batch  316 / 525  Training Loss  0.08903477340936661\n",
            "Epoch  5 Batch  317 / 525  Training Loss  0.10035158693790436\n",
            "Epoch  5 Batch  318 / 525  Training Loss  0.08614625036716461\n",
            "Epoch  5 Batch  319 / 525  Training Loss  0.07903282344341278\n",
            "Epoch  5 Batch  320 / 525  Training Loss  0.08573009073734283\n",
            "Epoch  5 Batch  321 / 525  Training Loss  0.07944311946630478\n",
            "Epoch  5 Batch  322 / 525  Training Loss  0.08224980533123016\n",
            "Epoch  5 Batch  323 / 525  Training Loss  0.0781979039311409\n",
            "Epoch  5 Batch  324 / 525  Training Loss  0.08758118748664856\n",
            "Epoch  5 Batch  325 / 525  Training Loss  0.09004581719636917\n",
            "Epoch  5 Batch  326 / 525  Training Loss  0.08142025023698807\n",
            "Epoch  5 Batch  327 / 525  Training Loss  0.09421388059854507\n",
            "Epoch  5 Batch  328 / 525  Training Loss  0.08975924551486969\n",
            "Epoch  5 Batch  329 / 525  Training Loss  0.10914969444274902\n",
            "Epoch  5 Batch  330 / 525  Training Loss  0.07473873347043991\n",
            "Epoch  5 Batch  331 / 525  Training Loss  0.0809827670454979\n",
            "Epoch  5 Batch  332 / 525  Training Loss  0.06663884222507477\n",
            "Epoch  5 Batch  333 / 525  Training Loss  0.08331697434186935\n",
            "Epoch  5 Batch  334 / 525  Training Loss  0.10007276386022568\n",
            "Epoch  5 Batch  335 / 525  Training Loss  0.08832891285419464\n",
            "Epoch  5 Batch  336 / 525  Training Loss  0.0878273993730545\n",
            "Epoch  5 Batch  337 / 525  Training Loss  0.08194661140441895\n",
            "Epoch  5 Batch  338 / 525  Training Loss  0.09647595137357712\n",
            "Epoch  5 Batch  339 / 525  Training Loss  0.09159425646066666\n",
            "Epoch  5 Batch  340 / 525  Training Loss  0.07941559702157974\n",
            "Epoch  5 Batch  341 / 525  Training Loss  0.08151639252901077\n",
            "Epoch  5 Batch  342 / 525  Training Loss  0.08029528707265854\n",
            "Epoch  5 Batch  343 / 525  Training Loss  0.0905027836561203\n",
            "Epoch  5 Batch  344 / 525  Training Loss  0.0850004106760025\n",
            "Epoch  5 Batch  345 / 525  Training Loss  0.07659608870744705\n",
            "Epoch  5 Batch  346 / 525  Training Loss  0.0993122011423111\n",
            "Epoch  5 Batch  347 / 525  Training Loss  0.07422924786806107\n",
            "Epoch  5 Batch  348 / 525  Training Loss  0.08792006224393845\n",
            "Epoch  5 Batch  349 / 525  Training Loss  0.08597393333911896\n",
            "Epoch  5 Batch  350 / 525  Training Loss  0.08995376527309418\n",
            "Epoch  5 Batch  351 / 525  Training Loss  0.08871491253376007\n",
            "Epoch  5 Batch  352 / 525  Training Loss  0.08525745570659637\n",
            "Epoch  5 Batch  353 / 525  Training Loss  0.09883274137973785\n",
            "Epoch  5 Batch  354 / 525  Training Loss  0.07213456183671951\n",
            "Epoch  5 Batch  355 / 525  Training Loss  0.08803436905145645\n",
            "Epoch  5 Batch  356 / 525  Training Loss  0.10119682550430298\n",
            "Epoch  5 Batch  357 / 525  Training Loss  0.07870908081531525\n",
            "Epoch  5 Batch  358 / 525  Training Loss  0.09449099004268646\n",
            "Epoch  5 Batch  359 / 525  Training Loss  0.08768768608570099\n",
            "Epoch  5 Batch  360 / 525  Training Loss  0.09002216160297394\n",
            "Epoch  5 Batch  361 / 525  Training Loss  0.07900450378656387\n",
            "Epoch  5 Batch  362 / 525  Training Loss  0.0775434598326683\n",
            "Epoch  5 Batch  363 / 525  Training Loss  0.07389432191848755\n",
            "Epoch  5 Batch  364 / 525  Training Loss  0.07661251723766327\n",
            "Epoch  5 Batch  365 / 525  Training Loss  0.10030211508274078\n",
            "Epoch  5 Batch  366 / 525  Training Loss  0.08365356177091599\n",
            "Epoch  5 Batch  367 / 525  Training Loss  0.09689023345708847\n",
            "Epoch  5 Batch  368 / 525  Training Loss  0.081959068775177\n",
            "Epoch  5 Batch  369 / 525  Training Loss  0.07624916732311249\n",
            "Epoch  5 Batch  370 / 525  Training Loss  0.08610711991786957\n",
            "Epoch  5 Batch  371 / 525  Training Loss  0.0993988886475563\n",
            "Epoch  5 Batch  372 / 525  Training Loss  0.07257428020238876\n",
            "Epoch  5 Batch  373 / 525  Training Loss  0.08992431312799454\n",
            "Epoch  5 Batch  374 / 525  Training Loss  0.07477938383817673\n",
            "Epoch  5 Batch  375 / 525  Training Loss  0.0925363227725029\n",
            "Epoch  5 Batch  376 / 525  Training Loss  0.09532570838928223\n",
            "Epoch  5 Batch  377 / 525  Training Loss  0.073531873524189\n",
            "Epoch  5 Batch  378 / 525  Training Loss  0.07160510122776031\n",
            "Epoch  5 Batch  379 / 525  Training Loss  0.07521119713783264\n",
            "Epoch  5 Batch  380 / 525  Training Loss  0.0805702656507492\n",
            "Epoch  5 Batch  381 / 525  Training Loss  0.0753108486533165\n",
            "Epoch  5 Batch  382 / 525  Training Loss  0.07577017694711685\n",
            "Epoch  5 Batch  383 / 525  Training Loss  0.07994687557220459\n",
            "Epoch  5 Batch  384 / 525  Training Loss  0.12003161013126373\n",
            "Epoch  5 Batch  385 / 525  Training Loss  0.08768878877162933\n",
            "Epoch  5 Batch  386 / 525  Training Loss  0.07971159368753433\n",
            "Epoch  5 Batch  387 / 525  Training Loss  0.07572366297245026\n",
            "Epoch  5 Batch  388 / 525  Training Loss  0.07516106218099594\n",
            "Epoch  5 Batch  389 / 525  Training Loss  0.10045834630727768\n",
            "Epoch  5 Batch  390 / 525  Training Loss  0.09144147485494614\n",
            "Epoch  5 Batch  391 / 525  Training Loss  0.10443432629108429\n",
            "Epoch  5 Batch  392 / 525  Training Loss  0.10830143839120865\n",
            "Epoch  5 Batch  393 / 525  Training Loss  0.07926259934902191\n",
            "Epoch  5 Batch  394 / 525  Training Loss  0.09595385938882828\n",
            "Epoch  5 Batch  395 / 525  Training Loss  0.09521166980266571\n",
            "Epoch  5 Batch  396 / 525  Training Loss  0.10095679759979248\n",
            "Epoch  5 Batch  397 / 525  Training Loss  0.07021056115627289\n",
            "Epoch  5 Batch  398 / 525  Training Loss  0.09756996482610703\n",
            "Epoch  5 Batch  399 / 525  Training Loss  0.09306810796260834\n",
            "Epoch  5 Batch  400 / 525  Training Loss  0.09151490032672882\n",
            "Epoch  5 Batch  401 / 525  Training Loss  0.08811167627573013\n",
            "Epoch  5 Batch  402 / 525  Training Loss  0.06907491385936737\n",
            "Epoch  5 Batch  403 / 525  Training Loss  0.09767400473356247\n",
            "Epoch  5 Batch  404 / 525  Training Loss  0.09083409607410431\n",
            "Epoch  5 Batch  405 / 525  Training Loss  0.08055762946605682\n",
            "Epoch  5 Batch  406 / 525  Training Loss  0.0794927105307579\n",
            "Epoch  5 Batch  407 / 525  Training Loss  0.06624356657266617\n",
            "Epoch  5 Batch  408 / 525  Training Loss  0.0795220360159874\n",
            "Epoch  5 Batch  409 / 525  Training Loss  0.09379831701517105\n",
            "Epoch  5 Batch  410 / 525  Training Loss  0.07606350630521774\n",
            "Epoch  5 Batch  411 / 525  Training Loss  0.0878068059682846\n",
            "Epoch  5 Batch  412 / 525  Training Loss  0.08423404395580292\n",
            "Epoch  5 Batch  413 / 525  Training Loss  0.078830286860466\n",
            "Epoch  5 Batch  414 / 525  Training Loss  0.09062542021274567\n",
            "Epoch  5 Batch  415 / 525  Training Loss  0.08347787708044052\n",
            "Epoch  5 Batch  416 / 525  Training Loss  0.09528309106826782\n",
            "Epoch  5 Batch  417 / 525  Training Loss  0.08671604841947556\n",
            "Epoch  5 Batch  418 / 525  Training Loss  0.08738312870264053\n",
            "Epoch  5 Batch  419 / 525  Training Loss  0.08062038570642471\n",
            "Epoch  5 Batch  420 / 525  Training Loss  0.08977946639060974\n",
            "Epoch  5 Batch  421 / 525  Training Loss  0.09416379034519196\n",
            "Epoch  5 Batch  422 / 525  Training Loss  0.09540321677923203\n",
            "Epoch  5 Batch  423 / 525  Training Loss  0.0785541832447052\n",
            "Epoch  5 Batch  424 / 525  Training Loss  0.07457081228494644\n",
            "Epoch  5 Batch  425 / 525  Training Loss  0.07637966424226761\n",
            "Epoch  5 Batch  426 / 525  Training Loss  0.07461632788181305\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  5 Batch  427 / 525  Training Loss  0.09125674515962601\n",
            "Epoch  5 Batch  428 / 525  Training Loss  0.09127135574817657\n",
            "Epoch  5 Batch  429 / 525  Training Loss  0.07501455396413803\n",
            "Epoch  5 Batch  430 / 525  Training Loss  0.07968363165855408\n",
            "Epoch  5 Batch  431 / 525  Training Loss  0.09313046932220459\n",
            "Epoch  5 Batch  432 / 525  Training Loss  0.0785214751958847\n",
            "Epoch  5 Batch  433 / 525  Training Loss  0.08344296365976334\n",
            "Epoch  5 Batch  434 / 525  Training Loss  0.08882211148738861\n",
            "Epoch  5 Batch  435 / 525  Training Loss  0.06802819669246674\n",
            "Epoch  5 Batch  436 / 525  Training Loss  0.08307483047246933\n",
            "Epoch  5 Batch  437 / 525  Training Loss  0.0966460257768631\n",
            "Epoch  5 Batch  438 / 525  Training Loss  0.079526886343956\n",
            "Epoch  5 Batch  439 / 525  Training Loss  0.07614775747060776\n",
            "Epoch  5 Batch  440 / 525  Training Loss  0.10040459781885147\n",
            "Epoch  5 Batch  441 / 525  Training Loss  0.07574279606342316\n",
            "Epoch  5 Batch  442 / 525  Training Loss  0.09349299967288971\n",
            "Epoch  5 Batch  443 / 525  Training Loss  0.0924680307507515\n",
            "Epoch  5 Batch  444 / 525  Training Loss  0.06845167279243469\n",
            "Epoch  5 Batch  445 / 525  Training Loss  0.0913824737071991\n",
            "Epoch  5 Batch  446 / 525  Training Loss  0.08589289337396622\n",
            "Epoch  5 Batch  447 / 525  Training Loss  0.11249908059835434\n",
            "Epoch  5 Batch  448 / 525  Training Loss  0.08270647376775742\n",
            "Epoch  5 Batch  449 / 525  Training Loss  0.0967288538813591\n",
            "Epoch  5 Batch  450 / 525  Training Loss  0.08054251223802567\n",
            "Epoch  5 Batch  451 / 525  Training Loss  0.081588514149189\n",
            "Epoch  5 Batch  452 / 525  Training Loss  0.11348414421081543\n",
            "Epoch  5 Batch  453 / 525  Training Loss  0.08161468058824539\n",
            "Epoch  5 Batch  454 / 525  Training Loss  0.08087822794914246\n",
            "Epoch  5 Batch  455 / 525  Training Loss  0.09888864308595657\n",
            "Epoch  5 Batch  456 / 525  Training Loss  0.09649870544672012\n",
            "Epoch  5 Batch  457 / 525  Training Loss  0.07575909048318863\n",
            "Epoch  5 Batch  458 / 525  Training Loss  0.08150871843099594\n",
            "Epoch  5 Batch  459 / 525  Training Loss  0.06660754233598709\n",
            "Epoch  5 Batch  460 / 525  Training Loss  0.09111746400594711\n",
            "Epoch  5 Batch  461 / 525  Training Loss  0.08569534122943878\n",
            "Epoch  5 Batch  462 / 525  Training Loss  0.09869535267353058\n",
            "Epoch  5 Batch  463 / 525  Training Loss  0.09699413180351257\n",
            "Epoch  5 Batch  464 / 525  Training Loss  0.0763891413807869\n",
            "Epoch  5 Batch  465 / 525  Training Loss  0.08394875377416611\n",
            "Epoch  5 Batch  466 / 525  Training Loss  0.09385252743959427\n",
            "Epoch  5 Batch  467 / 525  Training Loss  0.08610508590936661\n",
            "Epoch  5 Batch  468 / 525  Training Loss  0.0745551660656929\n",
            "Epoch  5 Batch  469 / 525  Training Loss  0.1025269404053688\n",
            "Epoch  5 Batch  470 / 525  Training Loss  0.08724403381347656\n",
            "Epoch  5 Batch  471 / 525  Training Loss  0.08555121719837189\n",
            "Epoch  5 Batch  472 / 525  Training Loss  0.07564856112003326\n",
            "Epoch  5 Batch  473 / 525  Training Loss  0.08558901399374008\n",
            "Epoch  5 Batch  474 / 525  Training Loss  0.08141794800758362\n",
            "Epoch  5 Batch  475 / 525  Training Loss  0.06639482080936432\n",
            "Epoch  5 Batch  476 / 525  Training Loss  0.07587681710720062\n",
            "Epoch  5 Batch  477 / 525  Training Loss  0.08572512120008469\n",
            "Epoch  5 Batch  478 / 525  Training Loss  0.09835074841976166\n",
            "Epoch  5 Batch  479 / 525  Training Loss  0.10959162563085556\n",
            "Epoch  5 Batch  480 / 525  Training Loss  0.08818911015987396\n",
            "Epoch  5 Batch  481 / 525  Training Loss  0.08953967690467834\n",
            "Epoch  5 Batch  482 / 525  Training Loss  0.09234411269426346\n",
            "Epoch  5 Batch  483 / 525  Training Loss  0.08264987915754318\n",
            "Epoch  5 Batch  484 / 525  Training Loss  0.0693177655339241\n",
            "Epoch  5 Batch  485 / 525  Training Loss  0.08635968714952469\n",
            "Epoch  5 Batch  486 / 525  Training Loss  0.08494577556848526\n",
            "Epoch  5 Batch  487 / 525  Training Loss  0.09870897233486176\n",
            "Epoch  5 Batch  488 / 525  Training Loss  0.09842897206544876\n",
            "Epoch  5 Batch  489 / 525  Training Loss  0.06464909017086029\n",
            "Epoch  5 Batch  490 / 525  Training Loss  0.07985562831163406\n",
            "Epoch  5 Batch  491 / 525  Training Loss  0.10382529348134995\n",
            "Epoch  5 Batch  492 / 525  Training Loss  0.089842788875103\n",
            "Epoch  5 Batch  493 / 525  Training Loss  0.10300527513027191\n",
            "Epoch  5 Batch  494 / 525  Training Loss  0.1000952497124672\n",
            "Epoch  5 Batch  495 / 525  Training Loss  0.09552903473377228\n",
            "Epoch  5 Batch  496 / 525  Training Loss  0.10295248031616211\n",
            "Epoch  5 Batch  497 / 525  Training Loss  0.09997335821390152\n",
            "Epoch  5 Batch  498 / 525  Training Loss  0.09909941256046295\n",
            "Epoch  5 Batch  499 / 525  Training Loss  0.09925272315740585\n",
            "Epoch  5 Batch  500 / 525  Training Loss  0.08417126536369324\n",
            "Epoch  5 Batch  501 / 525  Training Loss  0.06099379062652588\n",
            "Epoch  5 Batch  502 / 525  Training Loss  0.08980096131563187\n",
            "Epoch  5 Batch  503 / 525  Training Loss  0.07545987516641617\n",
            "Epoch  5 Batch  504 / 525  Training Loss  0.08637677133083344\n",
            "Epoch  5 Batch  505 / 525  Training Loss  0.08864586800336838\n",
            "Epoch  5 Batch  506 / 525  Training Loss  0.08752022683620453\n",
            "Epoch  5 Batch  507 / 525  Training Loss  0.0913989469408989\n",
            "Epoch  5 Batch  508 / 525  Training Loss  0.08693854510784149\n",
            "Epoch  5 Batch  509 / 525  Training Loss  0.07651908695697784\n",
            "Epoch  5 Batch  510 / 525  Training Loss  0.09260836243629456\n",
            "Epoch  5 Batch  511 / 525  Training Loss  0.0868353620171547\n",
            "Epoch  5 Batch  512 / 525  Training Loss  0.07688796520233154\n",
            "Epoch  5 Batch  513 / 525  Training Loss  0.08236726373434067\n",
            "Epoch  5 Batch  514 / 525  Training Loss  0.0955289974808693\n",
            "Epoch  5 Batch  515 / 525  Training Loss  0.09395488351583481\n",
            "Epoch  5 Batch  516 / 525  Training Loss  0.09209100157022476\n",
            "Epoch  5 Batch  517 / 525  Training Loss  0.10666541010141373\n",
            "Epoch  5 Batch  518 / 525  Training Loss  0.07644690573215485\n",
            "Epoch  5 Batch  519 / 525  Training Loss  0.07804056257009506\n",
            "Epoch  5 Batch  520 / 525  Training Loss  0.07490743696689606\n",
            "Epoch  5 Batch  521 / 525  Training Loss  0.08734001219272614\n",
            "Epoch  5 Batch  522 / 525  Training Loss  0.09203843772411346\n",
            "Epoch  5 Batch  523 / 525  Training Loss  0.08524219691753387\n",
            "Epoch  5 Batch  524 / 525  Training Loss  0.0732310563325882\n",
            "   6    |    -    |   0.086527   | 34.841667\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 6\n",
            "Epoch  6 Batch  0 / 525  Training Loss  0.0941387265920639\n",
            "Epoch  6 Batch  1 / 525  Training Loss  0.07119660079479218\n",
            "Epoch  6 Batch  2 / 525  Training Loss  0.07372910529375076\n",
            "Epoch  6 Batch  3 / 525  Training Loss  0.08981537818908691\n",
            "Epoch  6 Batch  4 / 525  Training Loss  0.06398642808198929\n",
            "Epoch  6 Batch  5 / 525  Training Loss  0.0605526864528656\n",
            "Epoch  6 Batch  6 / 525  Training Loss  0.09183492511510849\n",
            "Epoch  6 Batch  7 / 525  Training Loss  0.06954694539308548\n",
            "Epoch  6 Batch  8 / 525  Training Loss  0.08622995018959045\n",
            "Epoch  6 Batch  9 / 525  Training Loss  0.08613122254610062\n",
            "Epoch  6 Batch  10 / 525  Training Loss  0.07103374600410461\n",
            "Epoch  6 Batch  11 / 525  Training Loss  0.06563039869070053\n",
            "Epoch  6 Batch  12 / 525  Training Loss  0.0861315205693245\n",
            "Epoch  6 Batch  13 / 525  Training Loss  0.06202751398086548\n",
            "Epoch  6 Batch  14 / 525  Training Loss  0.0745914950966835\n",
            "Epoch  6 Batch  15 / 525  Training Loss  0.0844184011220932\n",
            "Epoch  6 Batch  16 / 525  Training Loss  0.08813323825597763\n",
            "Epoch  6 Batch  17 / 525  Training Loss  0.072626493871212\n",
            "Epoch  6 Batch  18 / 525  Training Loss  0.08112646639347076\n",
            "Epoch  6 Batch  19 / 525  Training Loss  0.07807905226945877\n",
            "Epoch  6 Batch  20 / 525  Training Loss  0.09515418857336044\n",
            "Epoch  6 Batch  21 / 525  Training Loss  0.0803597941994667\n",
            "Epoch  6 Batch  22 / 525  Training Loss  0.1084241047501564\n",
            "Epoch  6 Batch  23 / 525  Training Loss  0.0771072655916214\n",
            "Epoch  6 Batch  24 / 525  Training Loss  0.06813288480043411\n",
            "Epoch  6 Batch  25 / 525  Training Loss  0.05791492015123367\n",
            "Epoch  6 Batch  26 / 525  Training Loss  0.07425060868263245\n",
            "Epoch  6 Batch  27 / 525  Training Loss  0.09169991314411163\n",
            "Epoch  6 Batch  28 / 525  Training Loss  0.08922829478979111\n",
            "Epoch  6 Batch  29 / 525  Training Loss  0.0870516449213028\n",
            "Epoch  6 Batch  30 / 525  Training Loss  0.07989328354597092\n",
            "Epoch  6 Batch  31 / 525  Training Loss  0.08881264179944992\n",
            "Epoch  6 Batch  32 / 525  Training Loss  0.07154053449630737\n",
            "Epoch  6 Batch  33 / 525  Training Loss  0.07810734957456589\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  6 Batch  34 / 525  Training Loss  0.08723834902048111\n",
            "Epoch  6 Batch  35 / 525  Training Loss  0.07042385637760162\n",
            "Epoch  6 Batch  36 / 525  Training Loss  0.0891687422990799\n",
            "Epoch  6 Batch  37 / 525  Training Loss  0.08193902671337128\n",
            "Epoch  6 Batch  38 / 525  Training Loss  0.07865426689386368\n",
            "Epoch  6 Batch  39 / 525  Training Loss  0.07835289090871811\n",
            "Epoch  6 Batch  40 / 525  Training Loss  0.09454021602869034\n",
            "Epoch  6 Batch  41 / 525  Training Loss  0.07586096227169037\n",
            "Epoch  6 Batch  42 / 525  Training Loss  0.0790485069155693\n",
            "Epoch  6 Batch  43 / 525  Training Loss  0.0758054107427597\n",
            "Epoch  6 Batch  44 / 525  Training Loss  0.08074916154146194\n",
            "Epoch  6 Batch  45 / 525  Training Loss  0.07832324504852295\n",
            "Epoch  6 Batch  46 / 525  Training Loss  0.07837440073490143\n",
            "Epoch  6 Batch  47 / 525  Training Loss  0.08179821074008942\n",
            "Epoch  6 Batch  48 / 525  Training Loss  0.09397686272859573\n",
            "Epoch  6 Batch  49 / 525  Training Loss  0.08188137412071228\n",
            "Epoch  6 Batch  50 / 525  Training Loss  0.07836095243692398\n",
            "Epoch  6 Batch  51 / 525  Training Loss  0.07607702910900116\n",
            "Epoch  6 Batch  52 / 525  Training Loss  0.07929928600788116\n",
            "Epoch  6 Batch  53 / 525  Training Loss  0.07592791318893433\n",
            "Epoch  6 Batch  54 / 525  Training Loss  0.06693895906209946\n",
            "Epoch  6 Batch  55 / 525  Training Loss  0.08363606035709381\n",
            "Epoch  6 Batch  56 / 525  Training Loss  0.0831460952758789\n",
            "Epoch  6 Batch  57 / 525  Training Loss  0.07236361503601074\n",
            "Epoch  6 Batch  58 / 525  Training Loss  0.09125963598489761\n",
            "Epoch  6 Batch  59 / 525  Training Loss  0.0727858915925026\n",
            "Epoch  6 Batch  60 / 525  Training Loss  0.07403190433979034\n",
            "Epoch  6 Batch  61 / 525  Training Loss  0.08736898750066757\n",
            "Epoch  6 Batch  62 / 525  Training Loss  0.07553566992282867\n",
            "Epoch  6 Batch  63 / 525  Training Loss  0.08659200370311737\n",
            "Epoch  6 Batch  64 / 525  Training Loss  0.08164671063423157\n",
            "Epoch  6 Batch  65 / 525  Training Loss  0.0738198533654213\n",
            "Epoch  6 Batch  66 / 525  Training Loss  0.07948468625545502\n",
            "Epoch  6 Batch  67 / 525  Training Loss  0.08351447433233261\n",
            "Epoch  6 Batch  68 / 525  Training Loss  0.07056406140327454\n",
            "Epoch  6 Batch  69 / 525  Training Loss  0.07714603841304779\n",
            "Epoch  6 Batch  70 / 525  Training Loss  0.058350205421447754\n",
            "Epoch  6 Batch  71 / 525  Training Loss  0.06558717787265778\n",
            "Epoch  6 Batch  72 / 525  Training Loss  0.0749509260058403\n",
            "Epoch  6 Batch  73 / 525  Training Loss  0.08200021833181381\n",
            "Epoch  6 Batch  74 / 525  Training Loss  0.0956135243177414\n",
            "Epoch  6 Batch  75 / 525  Training Loss  0.09488607197999954\n",
            "Epoch  6 Batch  76 / 525  Training Loss  0.08034221827983856\n",
            "Epoch  6 Batch  77 / 525  Training Loss  0.07204623520374298\n",
            "Epoch  6 Batch  78 / 525  Training Loss  0.0821496993303299\n",
            "Epoch  6 Batch  79 / 525  Training Loss  0.06057017296552658\n",
            "Epoch  6 Batch  80 / 525  Training Loss  0.08375617861747742\n",
            "Epoch  6 Batch  81 / 525  Training Loss  0.07687214016914368\n",
            "Epoch  6 Batch  82 / 525  Training Loss  0.08551599085330963\n",
            "Epoch  6 Batch  83 / 525  Training Loss  0.08983170241117477\n",
            "Epoch  6 Batch  84 / 525  Training Loss  0.08483900874853134\n",
            "Epoch  6 Batch  85 / 525  Training Loss  0.07688358426094055\n",
            "Epoch  6 Batch  86 / 525  Training Loss  0.07454739511013031\n",
            "Epoch  6 Batch  87 / 525  Training Loss  0.058845262974500656\n",
            "Epoch  6 Batch  88 / 525  Training Loss  0.09631417691707611\n",
            "Epoch  6 Batch  89 / 525  Training Loss  0.07039856165647507\n",
            "Epoch  6 Batch  90 / 525  Training Loss  0.09532375633716583\n",
            "Epoch  6 Batch  91 / 525  Training Loss  0.09668774902820587\n",
            "Epoch  6 Batch  92 / 525  Training Loss  0.07296666502952576\n",
            "Epoch  6 Batch  93 / 525  Training Loss  0.0899762436747551\n",
            "Epoch  6 Batch  94 / 525  Training Loss  0.07079479098320007\n",
            "Epoch  6 Batch  95 / 525  Training Loss  0.08444882184267044\n",
            "Epoch  6 Batch  96 / 525  Training Loss  0.0771666020154953\n",
            "Epoch  6 Batch  97 / 525  Training Loss  0.08200554549694061\n",
            "Epoch  6 Batch  98 / 525  Training Loss  0.08241645246744156\n",
            "Epoch  6 Batch  99 / 525  Training Loss  0.08504639565944672\n",
            "Epoch  6 Batch  100 / 525  Training Loss  0.08519363403320312\n",
            "Epoch  6 Batch  101 / 525  Training Loss  0.08276273310184479\n",
            "Epoch  6 Batch  102 / 525  Training Loss  0.08518777787685394\n",
            "Epoch  6 Batch  103 / 525  Training Loss  0.08911295980215073\n",
            "Epoch  6 Batch  104 / 525  Training Loss  0.06715042889118195\n",
            "Epoch  6 Batch  105 / 525  Training Loss  0.06719440966844559\n",
            "Epoch  6 Batch  106 / 525  Training Loss  0.06608874350786209\n",
            "Epoch  6 Batch  107 / 525  Training Loss  0.0808272659778595\n",
            "Epoch  6 Batch  108 / 525  Training Loss  0.06703322380781174\n",
            "Epoch  6 Batch  109 / 525  Training Loss  0.0820300281047821\n",
            "Epoch  6 Batch  110 / 525  Training Loss  0.06406620889902115\n",
            "Epoch  6 Batch  111 / 525  Training Loss  0.0920012891292572\n",
            "Epoch  6 Batch  112 / 525  Training Loss  0.0837675929069519\n",
            "Epoch  6 Batch  113 / 525  Training Loss  0.09183889627456665\n",
            "Epoch  6 Batch  114 / 525  Training Loss  0.08067874610424042\n",
            "Epoch  6 Batch  115 / 525  Training Loss  0.0718945786356926\n",
            "Epoch  6 Batch  116 / 525  Training Loss  0.07548759132623672\n",
            "Epoch  6 Batch  117 / 525  Training Loss  0.0915331020951271\n",
            "Epoch  6 Batch  118 / 525  Training Loss  0.0681278258562088\n",
            "Epoch  6 Batch  119 / 525  Training Loss  0.05859356373548508\n",
            "Epoch  6 Batch  120 / 525  Training Loss  0.07374625653028488\n",
            "Epoch  6 Batch  121 / 525  Training Loss  0.0534740686416626\n",
            "Epoch  6 Batch  122 / 525  Training Loss  0.07587490975856781\n",
            "Epoch  6 Batch  123 / 525  Training Loss  0.07653312385082245\n",
            "Epoch  6 Batch  124 / 525  Training Loss  0.07229937613010406\n",
            "Epoch  6 Batch  125 / 525  Training Loss  0.07505539804697037\n",
            "Epoch  6 Batch  126 / 525  Training Loss  0.06653718650341034\n",
            "Epoch  6 Batch  127 / 525  Training Loss  0.06263980269432068\n",
            "Epoch  6 Batch  128 / 525  Training Loss  0.07275708764791489\n",
            "Epoch  6 Batch  129 / 525  Training Loss  0.05602557584643364\n",
            "Epoch  6 Batch  130 / 525  Training Loss  0.08786234259605408\n",
            "Epoch  6 Batch  131 / 525  Training Loss  0.08432258665561676\n",
            "Epoch  6 Batch  132 / 525  Training Loss  0.0845431312918663\n",
            "Epoch  6 Batch  133 / 525  Training Loss  0.07682064920663834\n",
            "Epoch  6 Batch  134 / 525  Training Loss  0.07357735931873322\n",
            "Epoch  6 Batch  135 / 525  Training Loss  0.0845007449388504\n",
            "Epoch  6 Batch  136 / 525  Training Loss  0.06465286016464233\n",
            "Epoch  6 Batch  137 / 525  Training Loss  0.07294237613677979\n",
            "Epoch  6 Batch  138 / 525  Training Loss  0.08198313415050507\n",
            "Epoch  6 Batch  139 / 525  Training Loss  0.0596565306186676\n",
            "Epoch  6 Batch  140 / 525  Training Loss  0.07418781518936157\n",
            "Epoch  6 Batch  141 / 525  Training Loss  0.08006324619054794\n",
            "Epoch  6 Batch  142 / 525  Training Loss  0.08254596590995789\n",
            "Epoch  6 Batch  143 / 525  Training Loss  0.07910064607858658\n",
            "Epoch  6 Batch  144 / 525  Training Loss  0.10193884372711182\n",
            "Epoch  6 Batch  145 / 525  Training Loss  0.08414815366268158\n",
            "Epoch  6 Batch  146 / 525  Training Loss  0.07321713864803314\n",
            "Epoch  6 Batch  147 / 525  Training Loss  0.0806024968624115\n",
            "Epoch  6 Batch  148 / 525  Training Loss  0.07950332015752792\n",
            "Epoch  6 Batch  149 / 525  Training Loss  0.07388977706432343\n",
            "Epoch  6 Batch  150 / 525  Training Loss  0.06712310016155243\n",
            "Epoch  6 Batch  151 / 525  Training Loss  0.06640331447124481\n",
            "Epoch  6 Batch  152 / 525  Training Loss  0.06404490768909454\n",
            "Epoch  6 Batch  153 / 525  Training Loss  0.06887469440698624\n",
            "Epoch  6 Batch  154 / 525  Training Loss  0.07971227169036865\n",
            "Epoch  6 Batch  155 / 525  Training Loss  0.09163512289524078\n",
            "Epoch  6 Batch  156 / 525  Training Loss  0.08419685810804367\n",
            "Epoch  6 Batch  157 / 525  Training Loss  0.06795261055231094\n",
            "Epoch  6 Batch  158 / 525  Training Loss  0.06682851165533066\n",
            "Epoch  6 Batch  159 / 525  Training Loss  0.07843960076570511\n",
            "Epoch  6 Batch  160 / 525  Training Loss  0.07387343049049377\n",
            "Epoch  6 Batch  161 / 525  Training Loss  0.08032919466495514\n",
            "Epoch  6 Batch  162 / 525  Training Loss  0.09040620923042297\n",
            "Epoch  6 Batch  163 / 525  Training Loss  0.07411850988864899\n",
            "Epoch  6 Batch  164 / 525  Training Loss  0.06555162370204926\n",
            "Epoch  6 Batch  165 / 525  Training Loss  0.08145616948604584\n",
            "Epoch  6 Batch  166 / 525  Training Loss  0.07519896328449249\n",
            "Epoch  6 Batch  167 / 525  Training Loss  0.07817962020635605\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  6 Batch  168 / 525  Training Loss  0.07463165372610092\n",
            "Epoch  6 Batch  169 / 525  Training Loss  0.08722423017024994\n",
            "Epoch  6 Batch  170 / 525  Training Loss  0.09020474553108215\n",
            "Epoch  6 Batch  171 / 525  Training Loss  0.07657979428768158\n",
            "Epoch  6 Batch  172 / 525  Training Loss  0.09867566078901291\n",
            "Epoch  6 Batch  173 / 525  Training Loss  0.08366694301366806\n",
            "Epoch  6 Batch  174 / 525  Training Loss  0.1031743735074997\n",
            "Epoch  6 Batch  175 / 525  Training Loss  0.10225822776556015\n",
            "Epoch  6 Batch  176 / 525  Training Loss  0.07333119958639145\n",
            "Epoch  6 Batch  177 / 525  Training Loss  0.08090829104185104\n",
            "Epoch  6 Batch  178 / 525  Training Loss  0.0810294896364212\n",
            "Epoch  6 Batch  179 / 525  Training Loss  0.08447923511266708\n",
            "Epoch  6 Batch  180 / 525  Training Loss  0.08723604679107666\n",
            "Epoch  6 Batch  181 / 525  Training Loss  0.08862017095088959\n",
            "Epoch  6 Batch  182 / 525  Training Loss  0.0967402309179306\n",
            "Epoch  6 Batch  183 / 525  Training Loss  0.07195217907428741\n",
            "Epoch  6 Batch  184 / 525  Training Loss  0.08151507377624512\n",
            "Epoch  6 Batch  185 / 525  Training Loss  0.08342224359512329\n",
            "Epoch  6 Batch  186 / 525  Training Loss  0.08677560091018677\n",
            "Epoch  6 Batch  187 / 525  Training Loss  0.0925808846950531\n",
            "Epoch  6 Batch  188 / 525  Training Loss  0.08588342368602753\n",
            "Epoch  6 Batch  189 / 525  Training Loss  0.07635685801506042\n",
            "Epoch  6 Batch  190 / 525  Training Loss  0.09518979489803314\n",
            "Epoch  6 Batch  191 / 525  Training Loss  0.09020063281059265\n",
            "Epoch  6 Batch  192 / 525  Training Loss  0.08821430802345276\n",
            "Epoch  6 Batch  193 / 525  Training Loss  0.07182030379772186\n",
            "Epoch  6 Batch  194 / 525  Training Loss  0.0851704403758049\n",
            "Epoch  6 Batch  195 / 525  Training Loss  0.0747082531452179\n",
            "Epoch  6 Batch  196 / 525  Training Loss  0.07299520075321198\n",
            "Epoch  6 Batch  197 / 525  Training Loss  0.09510323405265808\n",
            "Epoch  6 Batch  198 / 525  Training Loss  0.07259763032197952\n",
            "Epoch  6 Batch  199 / 525  Training Loss  0.061871688812971115\n",
            "Epoch  6 Batch  200 / 525  Training Loss  0.07719717919826508\n",
            "Epoch  6 Batch  201 / 525  Training Loss  0.07461094856262207\n",
            "Epoch  6 Batch  202 / 525  Training Loss  0.09863585978746414\n",
            "Epoch  6 Batch  203 / 525  Training Loss  0.07740407437086105\n",
            "Epoch  6 Batch  204 / 525  Training Loss  0.0946989506483078\n",
            "Epoch  6 Batch  205 / 525  Training Loss  0.06278898566961288\n",
            "Epoch  6 Batch  206 / 525  Training Loss  0.08160331845283508\n",
            "Epoch  6 Batch  207 / 525  Training Loss  0.06279931962490082\n",
            "Epoch  6 Batch  208 / 525  Training Loss  0.0573042556643486\n",
            "Epoch  6 Batch  209 / 525  Training Loss  0.06803029775619507\n",
            "Epoch  6 Batch  210 / 525  Training Loss  0.0876910611987114\n",
            "Epoch  6 Batch  211 / 525  Training Loss  0.0801537036895752\n",
            "Epoch  6 Batch  212 / 525  Training Loss  0.08358369022607803\n",
            "Epoch  6 Batch  213 / 525  Training Loss  0.07650940120220184\n",
            "Epoch  6 Batch  214 / 525  Training Loss  0.11020730435848236\n",
            "Epoch  6 Batch  215 / 525  Training Loss  0.09464234113693237\n",
            "Epoch  6 Batch  216 / 525  Training Loss  0.08312315493822098\n",
            "Epoch  6 Batch  217 / 525  Training Loss  0.08146775513887405\n",
            "Epoch  6 Batch  218 / 525  Training Loss  0.07603509724140167\n",
            "Epoch  6 Batch  219 / 525  Training Loss  0.0733543261885643\n",
            "Epoch  6 Batch  220 / 525  Training Loss  0.06881940364837646\n",
            "Epoch  6 Batch  221 / 525  Training Loss  0.07319684326648712\n",
            "Epoch  6 Batch  222 / 525  Training Loss  0.0781785175204277\n",
            "Epoch  6 Batch  223 / 525  Training Loss  0.08405978232622147\n",
            "Epoch  6 Batch  224 / 525  Training Loss  0.09032393991947174\n",
            "Epoch  6 Batch  225 / 525  Training Loss  0.07492431253194809\n",
            "Epoch  6 Batch  226 / 525  Training Loss  0.0852065309882164\n",
            "Epoch  6 Batch  227 / 525  Training Loss  0.06421739608049393\n",
            "Epoch  6 Batch  228 / 525  Training Loss  0.07346731424331665\n",
            "Epoch  6 Batch  229 / 525  Training Loss  0.07950439304113388\n",
            "Epoch  6 Batch  230 / 525  Training Loss  0.09249524772167206\n",
            "Epoch  6 Batch  231 / 525  Training Loss  0.0866752341389656\n",
            "Epoch  6 Batch  232 / 525  Training Loss  0.0765175074338913\n",
            "Epoch  6 Batch  233 / 525  Training Loss  0.06938648223876953\n",
            "Epoch  6 Batch  234 / 525  Training Loss  0.06982997059822083\n",
            "Epoch  6 Batch  235 / 525  Training Loss  0.07134464383125305\n",
            "Epoch  6 Batch  236 / 525  Training Loss  0.08879649639129639\n",
            "Epoch  6 Batch  237 / 525  Training Loss  0.06300611048936844\n",
            "Epoch  6 Batch  238 / 525  Training Loss  0.05829261988401413\n",
            "Epoch  6 Batch  239 / 525  Training Loss  0.103205606341362\n",
            "Epoch  6 Batch  240 / 525  Training Loss  0.09295566380023956\n",
            "Epoch  6 Batch  241 / 525  Training Loss  0.08379311859607697\n",
            "Epoch  6 Batch  242 / 525  Training Loss  0.0814373642206192\n",
            "Epoch  6 Batch  243 / 525  Training Loss  0.06723250448703766\n",
            "Epoch  6 Batch  244 / 525  Training Loss  0.0839032307267189\n",
            "Epoch  6 Batch  245 / 525  Training Loss  0.07893408089876175\n",
            "Epoch  6 Batch  246 / 525  Training Loss  0.08762870728969574\n",
            "Epoch  6 Batch  247 / 525  Training Loss  0.07547036558389664\n",
            "Epoch  6 Batch  248 / 525  Training Loss  0.093899205327034\n",
            "Epoch  6 Batch  249 / 525  Training Loss  0.1039348617196083\n",
            "Epoch  6 Batch  250 / 525  Training Loss  0.07163777202367783\n",
            "Epoch  6 Batch  251 / 525  Training Loss  0.07267890870571136\n",
            "Epoch  6 Batch  252 / 525  Training Loss  0.05434756726026535\n",
            "Epoch  6 Batch  253 / 525  Training Loss  0.07302386313676834\n",
            "Epoch  6 Batch  254 / 525  Training Loss  0.08474870771169662\n",
            "Epoch  6 Batch  255 / 525  Training Loss  0.09066526591777802\n",
            "Epoch  6 Batch  256 / 525  Training Loss  0.06747298687696457\n",
            "Epoch  6 Batch  257 / 525  Training Loss  0.06606946140527725\n",
            "Epoch  6 Batch  258 / 525  Training Loss  0.08924762159585953\n",
            "Epoch  6 Batch  259 / 525  Training Loss  0.08617046475410461\n",
            "Epoch  6 Batch  260 / 525  Training Loss  0.08395956456661224\n",
            "Epoch  6 Batch  261 / 525  Training Loss  0.08766986429691315\n",
            "Epoch  6 Batch  262 / 525  Training Loss  0.07052356749773026\n",
            "Epoch  6 Batch  263 / 525  Training Loss  0.070779949426651\n",
            "Epoch  6 Batch  264 / 525  Training Loss  0.07505171000957489\n",
            "Epoch  6 Batch  265 / 525  Training Loss  0.08616840094327927\n",
            "Epoch  6 Batch  266 / 525  Training Loss  0.08103550970554352\n",
            "Epoch  6 Batch  267 / 525  Training Loss  0.08432181179523468\n",
            "Epoch  6 Batch  268 / 525  Training Loss  0.08402140438556671\n",
            "Epoch  6 Batch  269 / 525  Training Loss  0.08192788809537888\n",
            "Epoch  6 Batch  270 / 525  Training Loss  0.07419457286596298\n",
            "Epoch  6 Batch  271 / 525  Training Loss  0.05906612426042557\n",
            "Epoch  6 Batch  272 / 525  Training Loss  0.06894548237323761\n",
            "Epoch  6 Batch  273 / 525  Training Loss  0.10301797091960907\n",
            "Epoch  6 Batch  274 / 525  Training Loss  0.07758452743291855\n",
            "Epoch  6 Batch  275 / 525  Training Loss  0.05063766986131668\n",
            "Epoch  6 Batch  276 / 525  Training Loss  0.07380750775337219\n",
            "Epoch  6 Batch  277 / 525  Training Loss  0.08491455018520355\n",
            "Epoch  6 Batch  278 / 525  Training Loss  0.09542369842529297\n",
            "Epoch  6 Batch  279 / 525  Training Loss  0.08162470161914825\n",
            "Epoch  6 Batch  280 / 525  Training Loss  0.07616650313138962\n",
            "Epoch  6 Batch  281 / 525  Training Loss  0.06476324051618576\n",
            "Epoch  6 Batch  282 / 525  Training Loss  0.08306465297937393\n",
            "Epoch  6 Batch  283 / 525  Training Loss  0.06414035707712173\n",
            "Epoch  6 Batch  284 / 525  Training Loss  0.09537530690431595\n",
            "Epoch  6 Batch  285 / 525  Training Loss  0.08812447637319565\n",
            "Epoch  6 Batch  286 / 525  Training Loss  0.094967782497406\n",
            "Epoch  6 Batch  287 / 525  Training Loss  0.10153843462467194\n",
            "Epoch  6 Batch  288 / 525  Training Loss  0.08198275417089462\n",
            "Epoch  6 Batch  289 / 525  Training Loss  0.08111279457807541\n",
            "Epoch  6 Batch  290 / 525  Training Loss  0.060272328555583954\n",
            "Epoch  6 Batch  291 / 525  Training Loss  0.08233024924993515\n",
            "Epoch  6 Batch  292 / 525  Training Loss  0.07483188062906265\n",
            "Epoch  6 Batch  293 / 525  Training Loss  0.07776986807584763\n",
            "Epoch  6 Batch  294 / 525  Training Loss  0.07044275104999542\n",
            "Epoch  6 Batch  295 / 525  Training Loss  0.08934368193149567\n",
            "Epoch  6 Batch  296 / 525  Training Loss  0.08831629157066345\n",
            "Epoch  6 Batch  297 / 525  Training Loss  0.06678562611341476\n",
            "Epoch  6 Batch  298 / 525  Training Loss  0.0821085125207901\n",
            "Epoch  6 Batch  299 / 525  Training Loss  0.08585674315690994\n",
            "Epoch  6 Batch  300 / 525  Training Loss  0.08287981897592545\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  6 Batch  301 / 525  Training Loss  0.05426063388586044\n",
            "Epoch  6 Batch  302 / 525  Training Loss  0.07681073248386383\n",
            "Epoch  6 Batch  303 / 525  Training Loss  0.07987360656261444\n",
            "Epoch  6 Batch  304 / 525  Training Loss  0.07211790233850479\n",
            "Epoch  6 Batch  305 / 525  Training Loss  0.08306415379047394\n",
            "Epoch  6 Batch  306 / 525  Training Loss  0.08155861496925354\n",
            "Epoch  6 Batch  307 / 525  Training Loss  0.08278001844882965\n",
            "Epoch  6 Batch  308 / 525  Training Loss  0.08894212543964386\n",
            "Epoch  6 Batch  309 / 525  Training Loss  0.07793913036584854\n",
            "Epoch  6 Batch  310 / 525  Training Loss  0.10065998136997223\n",
            "Epoch  6 Batch  311 / 525  Training Loss  0.07434836030006409\n",
            "Epoch  6 Batch  312 / 525  Training Loss  0.07416795939207077\n",
            "Epoch  6 Batch  313 / 525  Training Loss  0.06321118772029877\n",
            "Epoch  6 Batch  314 / 525  Training Loss  0.08661507070064545\n",
            "Epoch  6 Batch  315 / 525  Training Loss  0.08323097974061966\n",
            "Epoch  6 Batch  316 / 525  Training Loss  0.08153656870126724\n",
            "Epoch  6 Batch  317 / 525  Training Loss  0.08047202229499817\n",
            "Epoch  6 Batch  318 / 525  Training Loss  0.08282342553138733\n",
            "Epoch  6 Batch  319 / 525  Training Loss  0.08768477290868759\n",
            "Epoch  6 Batch  320 / 525  Training Loss  0.0763697475194931\n",
            "Epoch  6 Batch  321 / 525  Training Loss  0.07129830121994019\n",
            "Epoch  6 Batch  322 / 525  Training Loss  0.0794130191206932\n",
            "Epoch  6 Batch  323 / 525  Training Loss  0.08416391164064407\n",
            "Epoch  6 Batch  324 / 525  Training Loss  0.062414150685071945\n",
            "Epoch  6 Batch  325 / 525  Training Loss  0.08008267730474472\n",
            "Epoch  6 Batch  326 / 525  Training Loss  0.06581954658031464\n",
            "Epoch  6 Batch  327 / 525  Training Loss  0.06712298095226288\n",
            "Epoch  6 Batch  328 / 525  Training Loss  0.07824768871068954\n",
            "Epoch  6 Batch  329 / 525  Training Loss  0.09599639475345612\n",
            "Epoch  6 Batch  330 / 525  Training Loss  0.08845069259405136\n",
            "Epoch  6 Batch  331 / 525  Training Loss  0.10251788794994354\n",
            "Epoch  6 Batch  332 / 525  Training Loss  0.06814192235469818\n",
            "Epoch  6 Batch  333 / 525  Training Loss  0.0734805017709732\n",
            "Epoch  6 Batch  334 / 525  Training Loss  0.0810776799917221\n",
            "Epoch  6 Batch  335 / 525  Training Loss  0.08932949602603912\n",
            "Epoch  6 Batch  336 / 525  Training Loss  0.07272253930568695\n",
            "Epoch  6 Batch  337 / 525  Training Loss  0.10592076927423477\n",
            "Epoch  6 Batch  338 / 525  Training Loss  0.07757292687892914\n",
            "Epoch  6 Batch  339 / 525  Training Loss  0.08463376015424728\n",
            "Epoch  6 Batch  340 / 525  Training Loss  0.08741980791091919\n",
            "Epoch  6 Batch  341 / 525  Training Loss  0.05910160392522812\n",
            "Epoch  6 Batch  342 / 525  Training Loss  0.08238466084003448\n",
            "Epoch  6 Batch  343 / 525  Training Loss  0.0863988846540451\n",
            "Epoch  6 Batch  344 / 525  Training Loss  0.08538800477981567\n",
            "Epoch  6 Batch  345 / 525  Training Loss  0.06487631797790527\n",
            "Epoch  6 Batch  346 / 525  Training Loss  0.08018360286951065\n",
            "Epoch  6 Batch  347 / 525  Training Loss  0.09427273273468018\n",
            "Epoch  6 Batch  348 / 525  Training Loss  0.08926771581172943\n",
            "Epoch  6 Batch  349 / 525  Training Loss  0.07830511033535004\n",
            "Epoch  6 Batch  350 / 525  Training Loss  0.07378900796175003\n",
            "Epoch  6 Batch  351 / 525  Training Loss  0.09524645656347275\n",
            "Epoch  6 Batch  352 / 525  Training Loss  0.06494102627038956\n",
            "Epoch  6 Batch  353 / 525  Training Loss  0.09193643927574158\n",
            "Epoch  6 Batch  354 / 525  Training Loss  0.07946504652500153\n",
            "Epoch  6 Batch  355 / 525  Training Loss  0.08927491307258606\n",
            "Epoch  6 Batch  356 / 525  Training Loss  0.08590026199817657\n",
            "Epoch  6 Batch  357 / 525  Training Loss  0.07503367960453033\n",
            "Epoch  6 Batch  358 / 525  Training Loss  0.08761833608150482\n",
            "Epoch  6 Batch  359 / 525  Training Loss  0.0711447149515152\n",
            "Epoch  6 Batch  360 / 525  Training Loss  0.10243834555149078\n",
            "Epoch  6 Batch  361 / 525  Training Loss  0.08089634776115417\n",
            "Epoch  6 Batch  362 / 525  Training Loss  0.07597360759973526\n",
            "Epoch  6 Batch  363 / 525  Training Loss  0.0637999027967453\n",
            "Epoch  6 Batch  364 / 525  Training Loss  0.07172529399394989\n",
            "Epoch  6 Batch  365 / 525  Training Loss  0.08945293724536896\n",
            "Epoch  6 Batch  366 / 525  Training Loss  0.06522026658058167\n",
            "Epoch  6 Batch  367 / 525  Training Loss  0.09643935412168503\n",
            "Epoch  6 Batch  368 / 525  Training Loss  0.07173039019107819\n",
            "Epoch  6 Batch  369 / 525  Training Loss  0.060299795120954514\n",
            "Epoch  6 Batch  370 / 525  Training Loss  0.1035638079047203\n",
            "Epoch  6 Batch  371 / 525  Training Loss  0.09499642252922058\n",
            "Epoch  6 Batch  372 / 525  Training Loss  0.09214706718921661\n",
            "Epoch  6 Batch  373 / 525  Training Loss  0.08978221565485\n",
            "Epoch  6 Batch  374 / 525  Training Loss  0.07966920733451843\n",
            "Epoch  6 Batch  375 / 525  Training Loss  0.0919274240732193\n",
            "Epoch  6 Batch  376 / 525  Training Loss  0.08696169406175613\n",
            "Epoch  6 Batch  377 / 525  Training Loss  0.06372234225273132\n",
            "Epoch  6 Batch  378 / 525  Training Loss  0.09618930518627167\n",
            "Epoch  6 Batch  379 / 525  Training Loss  0.08993642032146454\n",
            "Epoch  6 Batch  380 / 525  Training Loss  0.08968694508075714\n",
            "Epoch  6 Batch  381 / 525  Training Loss  0.09195254743099213\n",
            "Epoch  6 Batch  382 / 525  Training Loss  0.08582283556461334\n",
            "Epoch  6 Batch  383 / 525  Training Loss  0.08277758955955505\n",
            "Epoch  6 Batch  384 / 525  Training Loss  0.09270834922790527\n",
            "Epoch  6 Batch  385 / 525  Training Loss  0.09032690525054932\n",
            "Epoch  6 Batch  386 / 525  Training Loss  0.07745683938264847\n",
            "Epoch  6 Batch  387 / 525  Training Loss  0.06531775742769241\n",
            "Epoch  6 Batch  388 / 525  Training Loss  0.09190256148576736\n",
            "Epoch  6 Batch  389 / 525  Training Loss  0.07085580378770828\n",
            "Epoch  6 Batch  390 / 525  Training Loss  0.09769067168235779\n",
            "Epoch  6 Batch  391 / 525  Training Loss  0.08143186569213867\n",
            "Epoch  6 Batch  392 / 525  Training Loss  0.09436190128326416\n",
            "Epoch  6 Batch  393 / 525  Training Loss  0.0796811431646347\n",
            "Epoch  6 Batch  394 / 525  Training Loss  0.08276691287755966\n",
            "Epoch  6 Batch  395 / 525  Training Loss  0.08255116641521454\n",
            "Epoch  6 Batch  396 / 525  Training Loss  0.06418975442647934\n",
            "Epoch  6 Batch  397 / 525  Training Loss  0.0861624926328659\n",
            "Epoch  6 Batch  398 / 525  Training Loss  0.08266092091798782\n",
            "Epoch  6 Batch  399 / 525  Training Loss  0.09920214116573334\n",
            "Epoch  6 Batch  400 / 525  Training Loss  0.0676107332110405\n",
            "Epoch  6 Batch  401 / 525  Training Loss  0.08830578625202179\n",
            "Epoch  6 Batch  402 / 525  Training Loss  0.08499293029308319\n",
            "Epoch  6 Batch  403 / 525  Training Loss  0.09369435161352158\n",
            "Epoch  6 Batch  404 / 525  Training Loss  0.0759492814540863\n",
            "Epoch  6 Batch  405 / 525  Training Loss  0.09077464789152145\n",
            "Epoch  6 Batch  406 / 525  Training Loss  0.08069019019603729\n",
            "Epoch  6 Batch  407 / 525  Training Loss  0.07744668424129486\n",
            "Epoch  6 Batch  408 / 525  Training Loss  0.09311562776565552\n",
            "Epoch  6 Batch  409 / 525  Training Loss  0.0582045316696167\n",
            "Epoch  6 Batch  410 / 525  Training Loss  0.08759657293558121\n",
            "Epoch  6 Batch  411 / 525  Training Loss  0.06931896507740021\n",
            "Epoch  6 Batch  412 / 525  Training Loss  0.08429080247879028\n",
            "Epoch  6 Batch  413 / 525  Training Loss  0.08972451835870743\n",
            "Epoch  6 Batch  414 / 525  Training Loss  0.08292943984270096\n",
            "Epoch  6 Batch  415 / 525  Training Loss  0.08344000577926636\n",
            "Epoch  6 Batch  416 / 525  Training Loss  0.0689021423459053\n",
            "Epoch  6 Batch  417 / 525  Training Loss  0.09289240092039108\n",
            "Epoch  6 Batch  418 / 525  Training Loss  0.07704560458660126\n",
            "Epoch  6 Batch  419 / 525  Training Loss  0.0703209638595581\n",
            "Epoch  6 Batch  420 / 525  Training Loss  0.0822361558675766\n",
            "Epoch  6 Batch  421 / 525  Training Loss  0.07795893400907516\n",
            "Epoch  6 Batch  422 / 525  Training Loss  0.07693197578191757\n",
            "Epoch  6 Batch  423 / 525  Training Loss  0.0715276449918747\n",
            "Epoch  6 Batch  424 / 525  Training Loss  0.09711595624685287\n",
            "Epoch  6 Batch  425 / 525  Training Loss  0.10807733237743378\n",
            "Epoch  6 Batch  426 / 525  Training Loss  0.08737275749444962\n",
            "Epoch  6 Batch  427 / 525  Training Loss  0.07692541182041168\n",
            "Epoch  6 Batch  428 / 525  Training Loss  0.08179741352796555\n",
            "Epoch  6 Batch  429 / 525  Training Loss  0.07971110939979553\n",
            "Epoch  6 Batch  430 / 525  Training Loss  0.08425136655569077\n",
            "Epoch  6 Batch  431 / 525  Training Loss  0.06542541086673737\n",
            "Epoch  6 Batch  432 / 525  Training Loss  0.08848699182271957\n",
            "Epoch  6 Batch  433 / 525  Training Loss  0.07320308685302734\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  6 Batch  434 / 525  Training Loss  0.07350658625364304\n",
            "Epoch  6 Batch  435 / 525  Training Loss  0.07791543751955032\n",
            "Epoch  6 Batch  436 / 525  Training Loss  0.06609322130680084\n",
            "Epoch  6 Batch  437 / 525  Training Loss  0.06836874037981033\n",
            "Epoch  6 Batch  438 / 525  Training Loss  0.08218641579151154\n",
            "Epoch  6 Batch  439 / 525  Training Loss  0.08512962609529495\n",
            "Epoch  6 Batch  440 / 525  Training Loss  0.07184968888759613\n",
            "Epoch  6 Batch  441 / 525  Training Loss  0.091039277613163\n",
            "Epoch  6 Batch  442 / 525  Training Loss  0.08196493238210678\n",
            "Epoch  6 Batch  443 / 525  Training Loss  0.08421148359775543\n",
            "Epoch  6 Batch  444 / 525  Training Loss  0.0760464072227478\n",
            "Epoch  6 Batch  445 / 525  Training Loss  0.08076551556587219\n",
            "Epoch  6 Batch  446 / 525  Training Loss  0.07909513264894485\n",
            "Epoch  6 Batch  447 / 525  Training Loss  0.08525067567825317\n",
            "Epoch  6 Batch  448 / 525  Training Loss  0.0734655112028122\n",
            "Epoch  6 Batch  449 / 525  Training Loss  0.0958544909954071\n",
            "Epoch  6 Batch  450 / 525  Training Loss  0.07534842193126678\n",
            "Epoch  6 Batch  451 / 525  Training Loss  0.06831543892621994\n",
            "Epoch  6 Batch  452 / 525  Training Loss  0.06502898037433624\n",
            "Epoch  6 Batch  453 / 525  Training Loss  0.07408948987722397\n",
            "Epoch  6 Batch  454 / 525  Training Loss  0.08454866707324982\n",
            "Epoch  6 Batch  455 / 525  Training Loss  0.06620817631483078\n",
            "Epoch  6 Batch  456 / 525  Training Loss  0.08129235357046127\n",
            "Epoch  6 Batch  457 / 525  Training Loss  0.09118415415287018\n",
            "Epoch  6 Batch  458 / 525  Training Loss  0.08377860486507416\n",
            "Epoch  6 Batch  459 / 525  Training Loss  0.07425259053707123\n",
            "Epoch  6 Batch  460 / 525  Training Loss  0.06517995893955231\n",
            "Epoch  6 Batch  461 / 525  Training Loss  0.07923264056444168\n",
            "Epoch  6 Batch  462 / 525  Training Loss  0.07289189845323563\n",
            "Epoch  6 Batch  463 / 525  Training Loss  0.0827607735991478\n",
            "Epoch  6 Batch  464 / 525  Training Loss  0.08365155756473541\n",
            "Epoch  6 Batch  465 / 525  Training Loss  0.06395862251520157\n",
            "Epoch  6 Batch  466 / 525  Training Loss  0.10125595331192017\n",
            "Epoch  6 Batch  467 / 525  Training Loss  0.09051279723644257\n",
            "Epoch  6 Batch  468 / 525  Training Loss  0.08135659992694855\n",
            "Epoch  6 Batch  469 / 525  Training Loss  0.07288181781768799\n",
            "Epoch  6 Batch  470 / 525  Training Loss  0.07775953412055969\n",
            "Epoch  6 Batch  471 / 525  Training Loss  0.09408942610025406\n",
            "Epoch  6 Batch  472 / 525  Training Loss  0.09212290495634079\n",
            "Epoch  6 Batch  473 / 525  Training Loss  0.06826786696910858\n",
            "Epoch  6 Batch  474 / 525  Training Loss  0.08357985317707062\n",
            "Epoch  6 Batch  475 / 525  Training Loss  0.07997561991214752\n",
            "Epoch  6 Batch  476 / 525  Training Loss  0.08248507976531982\n",
            "Epoch  6 Batch  477 / 525  Training Loss  0.07515496015548706\n",
            "Epoch  6 Batch  478 / 525  Training Loss  0.06868089735507965\n",
            "Epoch  6 Batch  479 / 525  Training Loss  0.08139419555664062\n",
            "Epoch  6 Batch  480 / 525  Training Loss  0.08834908902645111\n",
            "Epoch  6 Batch  481 / 525  Training Loss  0.0737646073102951\n",
            "Epoch  6 Batch  482 / 525  Training Loss  0.1026197075843811\n",
            "Epoch  6 Batch  483 / 525  Training Loss  0.07864874601364136\n",
            "Epoch  6 Batch  484 / 525  Training Loss  0.09285891056060791\n",
            "Epoch  6 Batch  485 / 525  Training Loss  0.07529078423976898\n",
            "Epoch  6 Batch  486 / 525  Training Loss  0.092217355966568\n",
            "Epoch  6 Batch  487 / 525  Training Loss  0.06827037781476974\n",
            "Epoch  6 Batch  488 / 525  Training Loss  0.08139202743768692\n",
            "Epoch  6 Batch  489 / 525  Training Loss  0.0793246179819107\n",
            "Epoch  6 Batch  490 / 525  Training Loss  0.07975464314222336\n",
            "Epoch  6 Batch  491 / 525  Training Loss  0.0724860355257988\n",
            "Epoch  6 Batch  492 / 525  Training Loss  0.07352989166975021\n",
            "Epoch  6 Batch  493 / 525  Training Loss  0.07882466167211533\n",
            "Epoch  6 Batch  494 / 525  Training Loss  0.07414991408586502\n",
            "Epoch  6 Batch  495 / 525  Training Loss  0.09603793919086456\n",
            "Epoch  6 Batch  496 / 525  Training Loss  0.08978892117738724\n",
            "Epoch  6 Batch  497 / 525  Training Loss  0.07497341930866241\n",
            "Epoch  6 Batch  498 / 525  Training Loss  0.08916716277599335\n",
            "Epoch  6 Batch  499 / 525  Training Loss  0.10226353257894516\n",
            "Epoch  6 Batch  500 / 525  Training Loss  0.06857467442750931\n",
            "Epoch  6 Batch  501 / 525  Training Loss  0.08180668205022812\n",
            "Epoch  6 Batch  502 / 525  Training Loss  0.07055731862783432\n",
            "Epoch  6 Batch  503 / 525  Training Loss  0.07240084558725357\n",
            "Epoch  6 Batch  504 / 525  Training Loss  0.08974029868841171\n",
            "Epoch  6 Batch  505 / 525  Training Loss  0.077347032725811\n",
            "Epoch  6 Batch  506 / 525  Training Loss  0.06883658468723297\n",
            "Epoch  6 Batch  507 / 525  Training Loss  0.0858420580625534\n",
            "Epoch  6 Batch  508 / 525  Training Loss  0.06990834325551987\n",
            "Epoch  6 Batch  509 / 525  Training Loss  0.0806380957365036\n",
            "Epoch  6 Batch  510 / 525  Training Loss  0.08473386615514755\n",
            "Epoch  6 Batch  511 / 525  Training Loss  0.10320840030908585\n",
            "Epoch  6 Batch  512 / 525  Training Loss  0.0697418600320816\n",
            "Epoch  6 Batch  513 / 525  Training Loss  0.06515137851238251\n",
            "Epoch  6 Batch  514 / 525  Training Loss  0.07919520139694214\n",
            "Epoch  6 Batch  515 / 525  Training Loss  0.06599728763103485\n",
            "Epoch  6 Batch  516 / 525  Training Loss  0.08750534057617188\n",
            "Epoch  6 Batch  517 / 525  Training Loss  0.09645392000675201\n",
            "Epoch  6 Batch  518 / 525  Training Loss  0.07183296978473663\n",
            "Epoch  6 Batch  519 / 525  Training Loss  0.07707715779542923\n",
            "Epoch  6 Batch  520 / 525  Training Loss  0.09521202743053436\n",
            "Epoch  6 Batch  521 / 525  Training Loss  0.09343527257442474\n",
            "Epoch  6 Batch  522 / 525  Training Loss  0.06649184972047806\n",
            "Epoch  6 Batch  523 / 525  Training Loss  0.10402387380599976\n",
            "Epoch  6 Batch  524 / 525  Training Loss  0.08208118379116058\n",
            "   7    |    -    |   0.080066   | 37.108333\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 7\n",
            "Epoch  7 Batch  0 / 525  Training Loss  0.07589209824800491\n",
            "Epoch  7 Batch  1 / 525  Training Loss  0.06761075556278229\n",
            "Epoch  7 Batch  2 / 525  Training Loss  0.08593430370092392\n",
            "Epoch  7 Batch  3 / 525  Training Loss  0.052533648908138275\n",
            "Epoch  7 Batch  4 / 525  Training Loss  0.0720452070236206\n",
            "Epoch  7 Batch  5 / 525  Training Loss  0.056808631867170334\n",
            "Epoch  7 Batch  6 / 525  Training Loss  0.07212530076503754\n",
            "Epoch  7 Batch  7 / 525  Training Loss  0.07054181396961212\n",
            "Epoch  7 Batch  8 / 525  Training Loss  0.07353395223617554\n",
            "Epoch  7 Batch  9 / 525  Training Loss  0.08365052938461304\n",
            "Epoch  7 Batch  10 / 525  Training Loss  0.06719350069761276\n",
            "Epoch  7 Batch  11 / 525  Training Loss  0.07728727161884308\n",
            "Epoch  7 Batch  12 / 525  Training Loss  0.07507376372814178\n",
            "Epoch  7 Batch  13 / 525  Training Loss  0.08319322764873505\n",
            "Epoch  7 Batch  14 / 525  Training Loss  0.08491820842027664\n",
            "Epoch  7 Batch  15 / 525  Training Loss  0.08137167245149612\n",
            "Epoch  7 Batch  16 / 525  Training Loss  0.07740163058042526\n",
            "Epoch  7 Batch  17 / 525  Training Loss  0.07078897207975388\n",
            "Epoch  7 Batch  18 / 525  Training Loss  0.06374461948871613\n",
            "Epoch  7 Batch  19 / 525  Training Loss  0.06721266359090805\n",
            "Epoch  7 Batch  20 / 525  Training Loss  0.08702296763658524\n",
            "Epoch  7 Batch  21 / 525  Training Loss  0.07494594901800156\n",
            "Epoch  7 Batch  22 / 525  Training Loss  0.07932831346988678\n",
            "Epoch  7 Batch  23 / 525  Training Loss  0.06477399170398712\n",
            "Epoch  7 Batch  24 / 525  Training Loss  0.0786174014210701\n",
            "Epoch  7 Batch  25 / 525  Training Loss  0.07548719644546509\n",
            "Epoch  7 Batch  26 / 525  Training Loss  0.06787100434303284\n",
            "Epoch  7 Batch  27 / 525  Training Loss  0.07622785866260529\n",
            "Epoch  7 Batch  28 / 525  Training Loss  0.0736982598900795\n",
            "Epoch  7 Batch  29 / 525  Training Loss  0.08164653927087784\n",
            "Epoch  7 Batch  30 / 525  Training Loss  0.07517577707767487\n",
            "Epoch  7 Batch  31 / 525  Training Loss  0.09587785601615906\n",
            "Epoch  7 Batch  32 / 525  Training Loss  0.07532256841659546\n",
            "Epoch  7 Batch  33 / 525  Training Loss  0.0686493068933487\n",
            "Epoch  7 Batch  34 / 525  Training Loss  0.08340071886777878\n",
            "Epoch  7 Batch  35 / 525  Training Loss  0.08866696804761887\n",
            "Epoch  7 Batch  36 / 525  Training Loss  0.08750580251216888\n",
            "Epoch  7 Batch  37 / 525  Training Loss  0.0924803763628006\n",
            "Epoch  7 Batch  38 / 525  Training Loss  0.07803560793399811\n",
            "Epoch  7 Batch  39 / 525  Training Loss  0.07494523376226425\n",
            "Epoch  7 Batch  40 / 525  Training Loss  0.07006179541349411\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  7 Batch  41 / 525  Training Loss  0.08600397408008575\n",
            "Epoch  7 Batch  42 / 525  Training Loss  0.06231178715825081\n",
            "Epoch  7 Batch  43 / 525  Training Loss  0.0670294538140297\n",
            "Epoch  7 Batch  44 / 525  Training Loss  0.07259714603424072\n",
            "Epoch  7 Batch  45 / 525  Training Loss  0.08846403658390045\n",
            "Epoch  7 Batch  46 / 525  Training Loss  0.08013045787811279\n",
            "Epoch  7 Batch  47 / 525  Training Loss  0.0728839859366417\n",
            "Epoch  7 Batch  48 / 525  Training Loss  0.08275644481182098\n",
            "Epoch  7 Batch  49 / 525  Training Loss  0.08251579850912094\n",
            "Epoch  7 Batch  50 / 525  Training Loss  0.0721769779920578\n",
            "Epoch  7 Batch  51 / 525  Training Loss  0.08764241635799408\n",
            "Epoch  7 Batch  52 / 525  Training Loss  0.061626922339200974\n",
            "Epoch  7 Batch  53 / 525  Training Loss  0.07953734695911407\n",
            "Epoch  7 Batch  54 / 525  Training Loss  0.062005817890167236\n",
            "Epoch  7 Batch  55 / 525  Training Loss  0.09230755269527435\n",
            "Epoch  7 Batch  56 / 525  Training Loss  0.0845845565199852\n",
            "Epoch  7 Batch  57 / 525  Training Loss  0.06284834444522858\n",
            "Epoch  7 Batch  58 / 525  Training Loss  0.0863320380449295\n",
            "Epoch  7 Batch  59 / 525  Training Loss  0.06879166513681412\n",
            "Epoch  7 Batch  60 / 525  Training Loss  0.07182607799768448\n",
            "Epoch  7 Batch  61 / 525  Training Loss  0.08308804780244827\n",
            "Epoch  7 Batch  62 / 525  Training Loss  0.0758613869547844\n",
            "Epoch  7 Batch  63 / 525  Training Loss  0.08323115855455399\n",
            "Epoch  7 Batch  64 / 525  Training Loss  0.07624794542789459\n",
            "Epoch  7 Batch  65 / 525  Training Loss  0.07062198221683502\n",
            "Epoch  7 Batch  66 / 525  Training Loss  0.07595080882310867\n",
            "Epoch  7 Batch  67 / 525  Training Loss  0.08491846174001694\n",
            "Epoch  7 Batch  68 / 525  Training Loss  0.08015500009059906\n",
            "Epoch  7 Batch  69 / 525  Training Loss  0.06874484568834305\n",
            "Epoch  7 Batch  70 / 525  Training Loss  0.10069183260202408\n",
            "Epoch  7 Batch  71 / 525  Training Loss  0.09224779158830643\n",
            "Epoch  7 Batch  72 / 525  Training Loss  0.12076760828495026\n",
            "Epoch  7 Batch  73 / 525  Training Loss  0.09750351309776306\n",
            "Epoch  7 Batch  74 / 525  Training Loss  0.0655851811170578\n",
            "Epoch  7 Batch  75 / 525  Training Loss  0.08361183106899261\n",
            "Epoch  7 Batch  76 / 525  Training Loss  0.07679460942745209\n",
            "Epoch  7 Batch  77 / 525  Training Loss  0.07220571488142014\n",
            "Epoch  7 Batch  78 / 525  Training Loss  0.0695740133523941\n",
            "Epoch  7 Batch  79 / 525  Training Loss  0.06310849636793137\n",
            "Epoch  7 Batch  80 / 525  Training Loss  0.07917214184999466\n",
            "Epoch  7 Batch  81 / 525  Training Loss  0.08194722980260849\n",
            "Epoch  7 Batch  82 / 525  Training Loss  0.07781925052404404\n",
            "Epoch  7 Batch  83 / 525  Training Loss  0.06569359451532364\n",
            "Epoch  7 Batch  84 / 525  Training Loss  0.06084812432527542\n",
            "Epoch  7 Batch  85 / 525  Training Loss  0.08960557729005814\n",
            "Epoch  7 Batch  86 / 525  Training Loss  0.07674611359834671\n",
            "Epoch  7 Batch  87 / 525  Training Loss  0.06680794060230255\n",
            "Epoch  7 Batch  88 / 525  Training Loss  0.06940527260303497\n",
            "Epoch  7 Batch  89 / 525  Training Loss  0.06318598240613937\n",
            "Epoch  7 Batch  90 / 525  Training Loss  0.04802674800157547\n",
            "Epoch  7 Batch  91 / 525  Training Loss  0.05311362072825432\n",
            "Epoch  7 Batch  92 / 525  Training Loss  0.0710635557770729\n",
            "Epoch  7 Batch  93 / 525  Training Loss  0.0720474049448967\n",
            "Epoch  7 Batch  94 / 525  Training Loss  0.08247487246990204\n",
            "Epoch  7 Batch  95 / 525  Training Loss  0.07124971598386765\n",
            "Epoch  7 Batch  96 / 525  Training Loss  0.07119853794574738\n",
            "Epoch  7 Batch  97 / 525  Training Loss  0.08695252239704132\n",
            "Epoch  7 Batch  98 / 525  Training Loss  0.07785224169492722\n",
            "Epoch  7 Batch  99 / 525  Training Loss  0.08536148816347122\n",
            "Epoch  7 Batch  100 / 525  Training Loss  0.04430777207016945\n",
            "Epoch  7 Batch  101 / 525  Training Loss  0.0678083598613739\n",
            "Epoch  7 Batch  102 / 525  Training Loss  0.07088717818260193\n",
            "Epoch  7 Batch  103 / 525  Training Loss  0.07752789556980133\n",
            "Epoch  7 Batch  104 / 525  Training Loss  0.06730231642723083\n",
            "Epoch  7 Batch  105 / 525  Training Loss  0.06761784851551056\n",
            "Epoch  7 Batch  106 / 525  Training Loss  0.05625084787607193\n",
            "Epoch  7 Batch  107 / 525  Training Loss  0.08295373618602753\n",
            "Epoch  7 Batch  108 / 525  Training Loss  0.0713295042514801\n",
            "Epoch  7 Batch  109 / 525  Training Loss  0.06286365538835526\n",
            "Epoch  7 Batch  110 / 525  Training Loss  0.06934202462434769\n",
            "Epoch  7 Batch  111 / 525  Training Loss  0.08483263850212097\n",
            "Epoch  7 Batch  112 / 525  Training Loss  0.05698457360267639\n",
            "Epoch  7 Batch  113 / 525  Training Loss  0.08128780126571655\n",
            "Epoch  7 Batch  114 / 525  Training Loss  0.0627434030175209\n",
            "Epoch  7 Batch  115 / 525  Training Loss  0.08488942682743073\n",
            "Epoch  7 Batch  116 / 525  Training Loss  0.08069305866956711\n",
            "Epoch  7 Batch  117 / 525  Training Loss  0.06042720004916191\n",
            "Epoch  7 Batch  118 / 525  Training Loss  0.07890023291110992\n",
            "Epoch  7 Batch  119 / 525  Training Loss  0.06905809044837952\n",
            "Epoch  7 Batch  120 / 525  Training Loss  0.08471652120351791\n",
            "Epoch  7 Batch  121 / 525  Training Loss  0.07931660115718842\n",
            "Epoch  7 Batch  122 / 525  Training Loss  0.05543408915400505\n",
            "Epoch  7 Batch  123 / 525  Training Loss  0.0848294198513031\n",
            "Epoch  7 Batch  124 / 525  Training Loss  0.07510112226009369\n",
            "Epoch  7 Batch  125 / 525  Training Loss  0.07628490775823593\n",
            "Epoch  7 Batch  126 / 525  Training Loss  0.07061415165662766\n",
            "Epoch  7 Batch  127 / 525  Training Loss  0.08864950388669968\n",
            "Epoch  7 Batch  128 / 525  Training Loss  0.07416873425245285\n",
            "Epoch  7 Batch  129 / 525  Training Loss  0.07098095118999481\n",
            "Epoch  7 Batch  130 / 525  Training Loss  0.08509503304958344\n",
            "Epoch  7 Batch  131 / 525  Training Loss  0.10224370658397675\n",
            "Epoch  7 Batch  132 / 525  Training Loss  0.08835397660732269\n",
            "Epoch  7 Batch  133 / 525  Training Loss  0.07464025914669037\n",
            "Epoch  7 Batch  134 / 525  Training Loss  0.07041078060865402\n",
            "Epoch  7 Batch  135 / 525  Training Loss  0.08064554631710052\n",
            "Epoch  7 Batch  136 / 525  Training Loss  0.08438711613416672\n",
            "Epoch  7 Batch  137 / 525  Training Loss  0.06487762182950974\n",
            "Epoch  7 Batch  138 / 525  Training Loss  0.06989508867263794\n",
            "Epoch  7 Batch  139 / 525  Training Loss  0.056754112243652344\n",
            "Epoch  7 Batch  140 / 525  Training Loss  0.07629275321960449\n",
            "Epoch  7 Batch  141 / 525  Training Loss  0.08471600711345673\n",
            "Epoch  7 Batch  142 / 525  Training Loss  0.07066390663385391\n",
            "Epoch  7 Batch  143 / 525  Training Loss  0.07694528996944427\n",
            "Epoch  7 Batch  144 / 525  Training Loss  0.06987304985523224\n",
            "Epoch  7 Batch  145 / 525  Training Loss  0.07436064630746841\n",
            "Epoch  7 Batch  146 / 525  Training Loss  0.0703948363661766\n",
            "Epoch  7 Batch  147 / 525  Training Loss  0.0678529143333435\n",
            "Epoch  7 Batch  148 / 525  Training Loss  0.07185955345630646\n",
            "Epoch  7 Batch  149 / 525  Training Loss  0.07152773439884186\n",
            "Epoch  7 Batch  150 / 525  Training Loss  0.08135495334863663\n",
            "Epoch  7 Batch  151 / 525  Training Loss  0.0785263404250145\n",
            "Epoch  7 Batch  152 / 525  Training Loss  0.06529437005519867\n",
            "Epoch  7 Batch  153 / 525  Training Loss  0.06987008452415466\n",
            "Epoch  7 Batch  154 / 525  Training Loss  0.06516070663928986\n",
            "Epoch  7 Batch  155 / 525  Training Loss  0.057663239538669586\n",
            "Epoch  7 Batch  156 / 525  Training Loss  0.08324693143367767\n",
            "Epoch  7 Batch  157 / 525  Training Loss  0.08715352416038513\n",
            "Epoch  7 Batch  158 / 525  Training Loss  0.06840790808200836\n",
            "Epoch  7 Batch  159 / 525  Training Loss  0.07359566539525986\n",
            "Epoch  7 Batch  160 / 525  Training Loss  0.08842034637928009\n",
            "Epoch  7 Batch  161 / 525  Training Loss  0.0938958078622818\n",
            "Epoch  7 Batch  162 / 525  Training Loss  0.08769065141677856\n",
            "Epoch  7 Batch  163 / 525  Training Loss  0.08131855726242065\n",
            "Epoch  7 Batch  164 / 525  Training Loss  0.08979243040084839\n",
            "Epoch  7 Batch  165 / 525  Training Loss  0.057751916348934174\n",
            "Epoch  7 Batch  166 / 525  Training Loss  0.06068313866853714\n",
            "Epoch  7 Batch  167 / 525  Training Loss  0.0708843395113945\n",
            "Epoch  7 Batch  168 / 525  Training Loss  0.08094551414251328\n",
            "Epoch  7 Batch  169 / 525  Training Loss  0.0748135894536972\n",
            "Epoch  7 Batch  170 / 525  Training Loss  0.08422242850065231\n",
            "Epoch  7 Batch  171 / 525  Training Loss  0.07736668735742569\n",
            "Epoch  7 Batch  172 / 525  Training Loss  0.08727359771728516\n",
            "Epoch  7 Batch  173 / 525  Training Loss  0.08136705309152603\n",
            "Epoch  7 Batch  174 / 525  Training Loss  0.08205950260162354\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  7 Batch  175 / 525  Training Loss  0.07545461505651474\n",
            "Epoch  7 Batch  176 / 525  Training Loss  0.07195675373077393\n",
            "Epoch  7 Batch  177 / 525  Training Loss  0.08830513060092926\n",
            "Epoch  7 Batch  178 / 525  Training Loss  0.07175548374652863\n",
            "Epoch  7 Batch  179 / 525  Training Loss  0.060651153326034546\n",
            "Epoch  7 Batch  180 / 525  Training Loss  0.060452379286289215\n",
            "Epoch  7 Batch  181 / 525  Training Loss  0.07388889789581299\n",
            "Epoch  7 Batch  182 / 525  Training Loss  0.0785280168056488\n",
            "Epoch  7 Batch  183 / 525  Training Loss  0.07990272343158722\n",
            "Epoch  7 Batch  184 / 525  Training Loss  0.07821394503116608\n",
            "Epoch  7 Batch  185 / 525  Training Loss  0.07444272935390472\n",
            "Epoch  7 Batch  186 / 525  Training Loss  0.08026845008134842\n",
            "Epoch  7 Batch  187 / 525  Training Loss  0.05670148879289627\n",
            "Epoch  7 Batch  188 / 525  Training Loss  0.06340673565864563\n",
            "Epoch  7 Batch  189 / 525  Training Loss  0.06535959988832474\n",
            "Epoch  7 Batch  190 / 525  Training Loss  0.06651850044727325\n",
            "Epoch  7 Batch  191 / 525  Training Loss  0.0805468037724495\n",
            "Epoch  7 Batch  192 / 525  Training Loss  0.06743356585502625\n",
            "Epoch  7 Batch  193 / 525  Training Loss  0.07974439859390259\n",
            "Epoch  7 Batch  194 / 525  Training Loss  0.07059963047504425\n",
            "Epoch  7 Batch  195 / 525  Training Loss  0.05891520902514458\n",
            "Epoch  7 Batch  196 / 525  Training Loss  0.06864914298057556\n",
            "Epoch  7 Batch  197 / 525  Training Loss  0.07135547697544098\n",
            "Epoch  7 Batch  198 / 525  Training Loss  0.09555184096097946\n",
            "Epoch  7 Batch  199 / 525  Training Loss  0.06577091664075851\n",
            "Epoch  7 Batch  200 / 525  Training Loss  0.04940284416079521\n",
            "Epoch  7 Batch  201 / 525  Training Loss  0.0755079835653305\n",
            "Epoch  7 Batch  202 / 525  Training Loss  0.07171304523944855\n",
            "Epoch  7 Batch  203 / 525  Training Loss  0.06547732651233673\n",
            "Epoch  7 Batch  204 / 525  Training Loss  0.07862058281898499\n",
            "Epoch  7 Batch  205 / 525  Training Loss  0.08087709546089172\n",
            "Epoch  7 Batch  206 / 525  Training Loss  0.06404796987771988\n",
            "Epoch  7 Batch  207 / 525  Training Loss  0.06801290810108185\n",
            "Epoch  7 Batch  208 / 525  Training Loss  0.07038566470146179\n",
            "Epoch  7 Batch  209 / 525  Training Loss  0.07804670929908752\n",
            "Epoch  7 Batch  210 / 525  Training Loss  0.06845615804195404\n",
            "Epoch  7 Batch  211 / 525  Training Loss  0.07012443244457245\n",
            "Epoch  7 Batch  212 / 525  Training Loss  0.07885666936635971\n",
            "Epoch  7 Batch  213 / 525  Training Loss  0.07460852712392807\n",
            "Epoch  7 Batch  214 / 525  Training Loss  0.08210963755846024\n",
            "Epoch  7 Batch  215 / 525  Training Loss  0.07003221660852432\n",
            "Epoch  7 Batch  216 / 525  Training Loss  0.0695650577545166\n",
            "Epoch  7 Batch  217 / 525  Training Loss  0.06419137865304947\n",
            "Epoch  7 Batch  218 / 525  Training Loss  0.07403232157230377\n",
            "Epoch  7 Batch  219 / 525  Training Loss  0.06155775859951973\n",
            "Epoch  7 Batch  220 / 525  Training Loss  0.06568852066993713\n",
            "Epoch  7 Batch  221 / 525  Training Loss  0.07378911226987839\n",
            "Epoch  7 Batch  222 / 525  Training Loss  0.0730419009923935\n",
            "Epoch  7 Batch  223 / 525  Training Loss  0.07783938944339752\n",
            "Epoch  7 Batch  224 / 525  Training Loss  0.061704475432634354\n",
            "Epoch  7 Batch  225 / 525  Training Loss  0.07412045449018478\n",
            "Epoch  7 Batch  226 / 525  Training Loss  0.05760353058576584\n",
            "Epoch  7 Batch  227 / 525  Training Loss  0.06606068462133408\n",
            "Epoch  7 Batch  228 / 525  Training Loss  0.07637596875429153\n",
            "Epoch  7 Batch  229 / 525  Training Loss  0.08356626331806183\n",
            "Epoch  7 Batch  230 / 525  Training Loss  0.07523676007986069\n",
            "Epoch  7 Batch  231 / 525  Training Loss  0.06928717344999313\n",
            "Epoch  7 Batch  232 / 525  Training Loss  0.07050509750843048\n",
            "Epoch  7 Batch  233 / 525  Training Loss  0.07778052985668182\n",
            "Epoch  7 Batch  234 / 525  Training Loss  0.0700647383928299\n",
            "Epoch  7 Batch  235 / 525  Training Loss  0.07067157328128815\n",
            "Epoch  7 Batch  236 / 525  Training Loss  0.09041068702936172\n",
            "Epoch  7 Batch  237 / 525  Training Loss  0.07305730879306793\n",
            "Epoch  7 Batch  238 / 525  Training Loss  0.08780673146247864\n",
            "Epoch  7 Batch  239 / 525  Training Loss  0.06870272010564804\n",
            "Epoch  7 Batch  240 / 525  Training Loss  0.0710044652223587\n",
            "Epoch  7 Batch  241 / 525  Training Loss  0.06976459175348282\n",
            "Epoch  7 Batch  242 / 525  Training Loss  0.08305557817220688\n",
            "Epoch  7 Batch  243 / 525  Training Loss  0.07215746492147446\n",
            "Epoch  7 Batch  244 / 525  Training Loss  0.08491164445877075\n",
            "Epoch  7 Batch  245 / 525  Training Loss  0.07378143072128296\n",
            "Epoch  7 Batch  246 / 525  Training Loss  0.07131551206111908\n",
            "Epoch  7 Batch  247 / 525  Training Loss  0.10429579019546509\n",
            "Epoch  7 Batch  248 / 525  Training Loss  0.07697141170501709\n",
            "Epoch  7 Batch  249 / 525  Training Loss  0.0759049654006958\n",
            "Epoch  7 Batch  250 / 525  Training Loss  0.09803710877895355\n",
            "Epoch  7 Batch  251 / 525  Training Loss  0.07885050028562546\n",
            "Epoch  7 Batch  252 / 525  Training Loss  0.09124502539634705\n",
            "Epoch  7 Batch  253 / 525  Training Loss  0.07852329313755035\n",
            "Epoch  7 Batch  254 / 525  Training Loss  0.0889800488948822\n",
            "Epoch  7 Batch  255 / 525  Training Loss  0.06683824956417084\n",
            "Epoch  7 Batch  256 / 525  Training Loss  0.0741143673658371\n",
            "Epoch  7 Batch  257 / 525  Training Loss  0.06304483115673065\n",
            "Epoch  7 Batch  258 / 525  Training Loss  0.08477380126714706\n",
            "Epoch  7 Batch  259 / 525  Training Loss  0.0857318788766861\n",
            "Epoch  7 Batch  260 / 525  Training Loss  0.06482096016407013\n",
            "Epoch  7 Batch  261 / 525  Training Loss  0.07250164449214935\n",
            "Epoch  7 Batch  262 / 525  Training Loss  0.07592132687568665\n",
            "Epoch  7 Batch  263 / 525  Training Loss  0.061075855046510696\n",
            "Epoch  7 Batch  264 / 525  Training Loss  0.06404325366020203\n",
            "Epoch  7 Batch  265 / 525  Training Loss  0.05765223503112793\n",
            "Epoch  7 Batch  266 / 525  Training Loss  0.07189644873142242\n",
            "Epoch  7 Batch  267 / 525  Training Loss  0.0633881688117981\n",
            "Epoch  7 Batch  268 / 525  Training Loss  0.08153103291988373\n",
            "Epoch  7 Batch  269 / 525  Training Loss  0.0736607015132904\n",
            "Epoch  7 Batch  270 / 525  Training Loss  0.06547010689973831\n",
            "Epoch  7 Batch  271 / 525  Training Loss  0.08302327245473862\n",
            "Epoch  7 Batch  272 / 525  Training Loss  0.07480521500110626\n",
            "Epoch  7 Batch  273 / 525  Training Loss  0.06702921539545059\n",
            "Epoch  7 Batch  274 / 525  Training Loss  0.0826510339975357\n",
            "Epoch  7 Batch  275 / 525  Training Loss  0.08069995045661926\n",
            "Epoch  7 Batch  276 / 525  Training Loss  0.06092455983161926\n",
            "Epoch  7 Batch  277 / 525  Training Loss  0.08810258656740189\n",
            "Epoch  7 Batch  278 / 525  Training Loss  0.08084370940923691\n",
            "Epoch  7 Batch  279 / 525  Training Loss  0.07107135653495789\n",
            "Epoch  7 Batch  280 / 525  Training Loss  0.07294328510761261\n",
            "Epoch  7 Batch  281 / 525  Training Loss  0.08654418587684631\n",
            "Epoch  7 Batch  282 / 525  Training Loss  0.09012530744075775\n",
            "Epoch  7 Batch  283 / 525  Training Loss  0.07159095257520676\n",
            "Epoch  7 Batch  284 / 525  Training Loss  0.08340604603290558\n",
            "Epoch  7 Batch  285 / 525  Training Loss  0.07279981672763824\n",
            "Epoch  7 Batch  286 / 525  Training Loss  0.06468978524208069\n",
            "Epoch  7 Batch  287 / 525  Training Loss  0.07004104554653168\n",
            "Epoch  7 Batch  288 / 525  Training Loss  0.08427007496356964\n",
            "Epoch  7 Batch  289 / 525  Training Loss  0.05994541570544243\n",
            "Epoch  7 Batch  290 / 525  Training Loss  0.07650376856327057\n",
            "Epoch  7 Batch  291 / 525  Training Loss  0.06015455722808838\n",
            "Epoch  7 Batch  292 / 525  Training Loss  0.08419495075941086\n",
            "Epoch  7 Batch  293 / 525  Training Loss  0.06623639911413193\n",
            "Epoch  7 Batch  294 / 525  Training Loss  0.07649042457342148\n",
            "Epoch  7 Batch  295 / 525  Training Loss  0.09009481966495514\n",
            "Epoch  7 Batch  296 / 525  Training Loss  0.07120973616838455\n",
            "Epoch  7 Batch  297 / 525  Training Loss  0.06727799028158188\n",
            "Epoch  7 Batch  298 / 525  Training Loss  0.06655772030353546\n",
            "Epoch  7 Batch  299 / 525  Training Loss  0.0575266107916832\n",
            "Epoch  7 Batch  300 / 525  Training Loss  0.07485466450452805\n",
            "Epoch  7 Batch  301 / 525  Training Loss  0.08119533956050873\n",
            "Epoch  7 Batch  302 / 525  Training Loss  0.06553453952074051\n",
            "Epoch  7 Batch  303 / 525  Training Loss  0.05929871276021004\n",
            "Epoch  7 Batch  304 / 525  Training Loss  0.0681477040052414\n",
            "Epoch  7 Batch  305 / 525  Training Loss  0.07123328745365143\n",
            "Epoch  7 Batch  306 / 525  Training Loss  0.057536959648132324\n",
            "Epoch  7 Batch  307 / 525  Training Loss  0.09504388272762299\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  7 Batch  308 / 525  Training Loss  0.06651619076728821\n",
            "Epoch  7 Batch  309 / 525  Training Loss  0.05976552516222\n",
            "Epoch  7 Batch  310 / 525  Training Loss  0.05721230432391167\n",
            "Epoch  7 Batch  311 / 525  Training Loss  0.06967999786138535\n",
            "Epoch  7 Batch  312 / 525  Training Loss  0.07538486272096634\n",
            "Epoch  7 Batch  313 / 525  Training Loss  0.06431231647729874\n",
            "Epoch  7 Batch  314 / 525  Training Loss  0.07579123228788376\n",
            "Epoch  7 Batch  315 / 525  Training Loss  0.05969295650720596\n",
            "Epoch  7 Batch  316 / 525  Training Loss  0.06200409680604935\n",
            "Epoch  7 Batch  317 / 525  Training Loss  0.05561799928545952\n",
            "Epoch  7 Batch  318 / 525  Training Loss  0.07606834173202515\n",
            "Epoch  7 Batch  319 / 525  Training Loss  0.06959891319274902\n",
            "Epoch  7 Batch  320 / 525  Training Loss  0.08459020406007767\n",
            "Epoch  7 Batch  321 / 525  Training Loss  0.06955669820308685\n",
            "Epoch  7 Batch  322 / 525  Training Loss  0.09130051732063293\n",
            "Epoch  7 Batch  323 / 525  Training Loss  0.06888828426599503\n",
            "Epoch  7 Batch  324 / 525  Training Loss  0.06859210878610611\n",
            "Epoch  7 Batch  325 / 525  Training Loss  0.07128472626209259\n",
            "Epoch  7 Batch  326 / 525  Training Loss  0.05328790098428726\n",
            "Epoch  7 Batch  327 / 525  Training Loss  0.06766436249017715\n",
            "Epoch  7 Batch  328 / 525  Training Loss  0.061029382050037384\n",
            "Epoch  7 Batch  329 / 525  Training Loss  0.07888065278530121\n",
            "Epoch  7 Batch  330 / 525  Training Loss  0.08034585416316986\n",
            "Epoch  7 Batch  331 / 525  Training Loss  0.09068271517753601\n",
            "Epoch  7 Batch  332 / 525  Training Loss  0.08093968778848648\n",
            "Epoch  7 Batch  333 / 525  Training Loss  0.05549488589167595\n",
            "Epoch  7 Batch  334 / 525  Training Loss  0.0606725811958313\n",
            "Epoch  7 Batch  335 / 525  Training Loss  0.06858278810977936\n",
            "Epoch  7 Batch  336 / 525  Training Loss  0.07762932777404785\n",
            "Epoch  7 Batch  337 / 525  Training Loss  0.05612478777766228\n",
            "Epoch  7 Batch  338 / 525  Training Loss  0.07093899697065353\n",
            "Epoch  7 Batch  339 / 525  Training Loss  0.07404331862926483\n",
            "Epoch  7 Batch  340 / 525  Training Loss  0.08075636625289917\n",
            "Epoch  7 Batch  341 / 525  Training Loss  0.07465492188930511\n",
            "Epoch  7 Batch  342 / 525  Training Loss  0.09982354938983917\n",
            "Epoch  7 Batch  343 / 525  Training Loss  0.06773814558982849\n",
            "Epoch  7 Batch  344 / 525  Training Loss  0.07465998828411102\n",
            "Epoch  7 Batch  345 / 525  Training Loss  0.0685163363814354\n",
            "Epoch  7 Batch  346 / 525  Training Loss  0.06445276737213135\n",
            "Epoch  7 Batch  347 / 525  Training Loss  0.07411693036556244\n",
            "Epoch  7 Batch  348 / 525  Training Loss  0.06648246943950653\n",
            "Epoch  7 Batch  349 / 525  Training Loss  0.08956441283226013\n",
            "Epoch  7 Batch  350 / 525  Training Loss  0.0892285630106926\n",
            "Epoch  7 Batch  351 / 525  Training Loss  0.06467325985431671\n",
            "Epoch  7 Batch  352 / 525  Training Loss  0.07946929335594177\n",
            "Epoch  7 Batch  353 / 525  Training Loss  0.07815971970558167\n",
            "Epoch  7 Batch  354 / 525  Training Loss  0.058106280863285065\n",
            "Epoch  7 Batch  355 / 525  Training Loss  0.06687380373477936\n",
            "Epoch  7 Batch  356 / 525  Training Loss  0.0758611410856247\n",
            "Epoch  7 Batch  357 / 525  Training Loss  0.08641897141933441\n",
            "Epoch  7 Batch  358 / 525  Training Loss  0.051923006772994995\n",
            "Epoch  7 Batch  359 / 525  Training Loss  0.06333553045988083\n",
            "Epoch  7 Batch  360 / 525  Training Loss  0.07737371325492859\n",
            "Epoch  7 Batch  361 / 525  Training Loss  0.07205218821763992\n",
            "Epoch  7 Batch  362 / 525  Training Loss  0.07113851606845856\n",
            "Epoch  7 Batch  363 / 525  Training Loss  0.07727257907390594\n",
            "Epoch  7 Batch  364 / 525  Training Loss  0.06568451970815659\n",
            "Epoch  7 Batch  365 / 525  Training Loss  0.05624785274267197\n",
            "Epoch  7 Batch  366 / 525  Training Loss  0.0684322789311409\n",
            "Epoch  7 Batch  367 / 525  Training Loss  0.06935480237007141\n",
            "Epoch  7 Batch  368 / 525  Training Loss  0.07423076778650284\n",
            "Epoch  7 Batch  369 / 525  Training Loss  0.09889642894268036\n",
            "Epoch  7 Batch  370 / 525  Training Loss  0.08359956741333008\n",
            "Epoch  7 Batch  371 / 525  Training Loss  0.07824976742267609\n",
            "Epoch  7 Batch  372 / 525  Training Loss  0.05103132873773575\n",
            "Epoch  7 Batch  373 / 525  Training Loss  0.07635124027729034\n",
            "Epoch  7 Batch  374 / 525  Training Loss  0.0639384537935257\n",
            "Epoch  7 Batch  375 / 525  Training Loss  0.07971332222223282\n",
            "Epoch  7 Batch  376 / 525  Training Loss  0.08554691076278687\n",
            "Epoch  7 Batch  377 / 525  Training Loss  0.07855340093374252\n",
            "Epoch  7 Batch  378 / 525  Training Loss  0.05844464898109436\n",
            "Epoch  7 Batch  379 / 525  Training Loss  0.08396662771701813\n",
            "Epoch  7 Batch  380 / 525  Training Loss  0.07221241295337677\n",
            "Epoch  7 Batch  381 / 525  Training Loss  0.06366590410470963\n",
            "Epoch  7 Batch  382 / 525  Training Loss  0.06132293865084648\n",
            "Epoch  7 Batch  383 / 525  Training Loss  0.07384245097637177\n",
            "Epoch  7 Batch  384 / 525  Training Loss  0.09068159759044647\n",
            "Epoch  7 Batch  385 / 525  Training Loss  0.08200334757566452\n",
            "Epoch  7 Batch  386 / 525  Training Loss  0.0835079550743103\n",
            "Epoch  7 Batch  387 / 525  Training Loss  0.06899312883615494\n",
            "Epoch  7 Batch  388 / 525  Training Loss  0.07188578695058823\n",
            "Epoch  7 Batch  389 / 525  Training Loss  0.06090850755572319\n",
            "Epoch  7 Batch  390 / 525  Training Loss  0.07529523968696594\n",
            "Epoch  7 Batch  391 / 525  Training Loss  0.07512760162353516\n",
            "Epoch  7 Batch  392 / 525  Training Loss  0.07453130185604095\n",
            "Epoch  7 Batch  393 / 525  Training Loss  0.06058304384350777\n",
            "Epoch  7 Batch  394 / 525  Training Loss  0.07644306868314743\n",
            "Epoch  7 Batch  395 / 525  Training Loss  0.067890465259552\n",
            "Epoch  7 Batch  396 / 525  Training Loss  0.08624407649040222\n",
            "Epoch  7 Batch  397 / 525  Training Loss  0.07624311000108719\n",
            "Epoch  7 Batch  398 / 525  Training Loss  0.06395699828863144\n",
            "Epoch  7 Batch  399 / 525  Training Loss  0.0809946283698082\n",
            "Epoch  7 Batch  400 / 525  Training Loss  0.06807871162891388\n",
            "Epoch  7 Batch  401 / 525  Training Loss  0.07839321345090866\n",
            "Epoch  7 Batch  402 / 525  Training Loss  0.05805160477757454\n",
            "Epoch  7 Batch  403 / 525  Training Loss  0.07701703161001205\n",
            "Epoch  7 Batch  404 / 525  Training Loss  0.08749178051948547\n",
            "Epoch  7 Batch  405 / 525  Training Loss  0.06788216531276703\n",
            "Epoch  7 Batch  406 / 525  Training Loss  0.07727673649787903\n",
            "Epoch  7 Batch  407 / 525  Training Loss  0.06477922946214676\n",
            "Epoch  7 Batch  408 / 525  Training Loss  0.07090039551258087\n",
            "Epoch  7 Batch  409 / 525  Training Loss  0.08028866350650787\n",
            "Epoch  7 Batch  410 / 525  Training Loss  0.08594377338886261\n",
            "Epoch  7 Batch  411 / 525  Training Loss  0.09056547284126282\n",
            "Epoch  7 Batch  412 / 525  Training Loss  0.0841478556394577\n",
            "Epoch  7 Batch  413 / 525  Training Loss  0.07996676862239838\n",
            "Epoch  7 Batch  414 / 525  Training Loss  0.06773416697978973\n",
            "Epoch  7 Batch  415 / 525  Training Loss  0.07951721549034119\n",
            "Epoch  7 Batch  416 / 525  Training Loss  0.0771043598651886\n",
            "Epoch  7 Batch  417 / 525  Training Loss  0.07159130275249481\n",
            "Epoch  7 Batch  418 / 525  Training Loss  0.06616096943616867\n",
            "Epoch  7 Batch  419 / 525  Training Loss  0.0706278383731842\n",
            "Epoch  7 Batch  420 / 525  Training Loss  0.07325230538845062\n",
            "Epoch  7 Batch  421 / 525  Training Loss  0.07088567316532135\n",
            "Epoch  7 Batch  422 / 525  Training Loss  0.06488018482923508\n",
            "Epoch  7 Batch  423 / 525  Training Loss  0.07961463928222656\n",
            "Epoch  7 Batch  424 / 525  Training Loss  0.07121434807777405\n",
            "Epoch  7 Batch  425 / 525  Training Loss  0.07424883544445038\n",
            "Epoch  7 Batch  426 / 525  Training Loss  0.08618545532226562\n",
            "Epoch  7 Batch  427 / 525  Training Loss  0.0668206587433815\n",
            "Epoch  7 Batch  428 / 525  Training Loss  0.07353346049785614\n",
            "Epoch  7 Batch  429 / 525  Training Loss  0.07074974477291107\n",
            "Epoch  7 Batch  430 / 525  Training Loss  0.04951133579015732\n",
            "Epoch  7 Batch  431 / 525  Training Loss  0.062360286712646484\n",
            "Epoch  7 Batch  432 / 525  Training Loss  0.08944130688905716\n",
            "Epoch  7 Batch  433 / 525  Training Loss  0.053184933960437775\n",
            "Epoch  7 Batch  434 / 525  Training Loss  0.07225669920444489\n",
            "Epoch  7 Batch  435 / 525  Training Loss  0.06824254244565964\n",
            "Epoch  7 Batch  436 / 525  Training Loss  0.08976350724697113\n",
            "Epoch  7 Batch  437 / 525  Training Loss  0.06084244325757027\n",
            "Epoch  7 Batch  438 / 525  Training Loss  0.0762060284614563\n",
            "Epoch  7 Batch  439 / 525  Training Loss  0.0676988884806633\n",
            "Epoch  7 Batch  440 / 525  Training Loss  0.08278805017471313\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  7 Batch  441 / 525  Training Loss  0.07440891116857529\n",
            "Epoch  7 Batch  442 / 525  Training Loss  0.07465829700231552\n",
            "Epoch  7 Batch  443 / 525  Training Loss  0.07222862541675568\n",
            "Epoch  7 Batch  444 / 525  Training Loss  0.08190587162971497\n",
            "Epoch  7 Batch  445 / 525  Training Loss  0.06486054509878159\n",
            "Epoch  7 Batch  446 / 525  Training Loss  0.07450979202985764\n",
            "Epoch  7 Batch  447 / 525  Training Loss  0.08417003601789474\n",
            "Epoch  7 Batch  448 / 525  Training Loss  0.0713593065738678\n",
            "Epoch  7 Batch  449 / 525  Training Loss  0.07847193628549576\n",
            "Epoch  7 Batch  450 / 525  Training Loss  0.046718329191207886\n",
            "Epoch  7 Batch  451 / 525  Training Loss  0.06846116483211517\n",
            "Epoch  7 Batch  452 / 525  Training Loss  0.07651697099208832\n",
            "Epoch  7 Batch  453 / 525  Training Loss  0.08603398501873016\n",
            "Epoch  7 Batch  454 / 525  Training Loss  0.08585786074399948\n",
            "Epoch  7 Batch  455 / 525  Training Loss  0.05940381810069084\n",
            "Epoch  7 Batch  456 / 525  Training Loss  0.07441768795251846\n",
            "Epoch  7 Batch  457 / 525  Training Loss  0.06423114240169525\n",
            "Epoch  7 Batch  458 / 525  Training Loss  0.061000924557447433\n",
            "Epoch  7 Batch  459 / 525  Training Loss  0.08646728843450546\n",
            "Epoch  7 Batch  460 / 525  Training Loss  0.06824103742837906\n",
            "Epoch  7 Batch  461 / 525  Training Loss  0.08187348395586014\n",
            "Epoch  7 Batch  462 / 525  Training Loss  0.09008379280567169\n",
            "Epoch  7 Batch  463 / 525  Training Loss  0.07939304411411285\n",
            "Epoch  7 Batch  464 / 525  Training Loss  0.06258577853441238\n",
            "Epoch  7 Batch  465 / 525  Training Loss  0.07722430676221848\n",
            "Epoch  7 Batch  466 / 525  Training Loss  0.08986694365739822\n",
            "Epoch  7 Batch  467 / 525  Training Loss  0.06151679903268814\n",
            "Epoch  7 Batch  468 / 525  Training Loss  0.07438880205154419\n",
            "Epoch  7 Batch  469 / 525  Training Loss  0.06065654009580612\n",
            "Epoch  7 Batch  470 / 525  Training Loss  0.0800703838467598\n",
            "Epoch  7 Batch  471 / 525  Training Loss  0.0968729555606842\n",
            "Epoch  7 Batch  472 / 525  Training Loss  0.07957278937101364\n",
            "Epoch  7 Batch  473 / 525  Training Loss  0.07079660147428513\n",
            "Epoch  7 Batch  474 / 525  Training Loss  0.0753059834241867\n",
            "Epoch  7 Batch  475 / 525  Training Loss  0.08695585280656815\n",
            "Epoch  7 Batch  476 / 525  Training Loss  0.07235144078731537\n",
            "Epoch  7 Batch  477 / 525  Training Loss  0.0570409782230854\n",
            "Epoch  7 Batch  478 / 525  Training Loss  0.0826360434293747\n",
            "Epoch  7 Batch  479 / 525  Training Loss  0.0850856825709343\n",
            "Epoch  7 Batch  480 / 525  Training Loss  0.06917820870876312\n",
            "Epoch  7 Batch  481 / 525  Training Loss  0.07003519684076309\n",
            "Epoch  7 Batch  482 / 525  Training Loss  0.08011383563280106\n",
            "Epoch  7 Batch  483 / 525  Training Loss  0.06828345358371735\n",
            "Epoch  7 Batch  484 / 525  Training Loss  0.07289387285709381\n",
            "Epoch  7 Batch  485 / 525  Training Loss  0.09412677586078644\n",
            "Epoch  7 Batch  486 / 525  Training Loss  0.09067369252443314\n",
            "Epoch  7 Batch  487 / 525  Training Loss  0.05913905054330826\n",
            "Epoch  7 Batch  488 / 525  Training Loss  0.08936131000518799\n",
            "Epoch  7 Batch  489 / 525  Training Loss  0.0797150582075119\n",
            "Epoch  7 Batch  490 / 525  Training Loss  0.07169053703546524\n",
            "Epoch  7 Batch  491 / 525  Training Loss  0.0806637853384018\n",
            "Epoch  7 Batch  492 / 525  Training Loss  0.08812450617551804\n",
            "Epoch  7 Batch  493 / 525  Training Loss  0.08773898333311081\n",
            "Epoch  7 Batch  494 / 525  Training Loss  0.08026914298534393\n",
            "Epoch  7 Batch  495 / 525  Training Loss  0.08350634574890137\n",
            "Epoch  7 Batch  496 / 525  Training Loss  0.07453922927379608\n",
            "Epoch  7 Batch  497 / 525  Training Loss  0.0745721310377121\n",
            "Epoch  7 Batch  498 / 525  Training Loss  0.0615738220512867\n",
            "Epoch  7 Batch  499 / 525  Training Loss  0.0734969973564148\n",
            "Epoch  7 Batch  500 / 525  Training Loss  0.07780744135379791\n",
            "Epoch  7 Batch  501 / 525  Training Loss  0.07301441580057144\n",
            "Epoch  7 Batch  502 / 525  Training Loss  0.06962954998016357\n",
            "Epoch  7 Batch  503 / 525  Training Loss  0.07356126606464386\n",
            "Epoch  7 Batch  504 / 525  Training Loss  0.08797063678503036\n",
            "Epoch  7 Batch  505 / 525  Training Loss  0.0909227579832077\n",
            "Epoch  7 Batch  506 / 525  Training Loss  0.09764904528856277\n",
            "Epoch  7 Batch  507 / 525  Training Loss  0.09313332289457321\n",
            "Epoch  7 Batch  508 / 525  Training Loss  0.06770171225070953\n",
            "Epoch  7 Batch  509 / 525  Training Loss  0.06252545863389969\n",
            "Epoch  7 Batch  510 / 525  Training Loss  0.09018737077713013\n",
            "Epoch  7 Batch  511 / 525  Training Loss  0.06475791335105896\n",
            "Epoch  7 Batch  512 / 525  Training Loss  0.08540502935647964\n",
            "Epoch  7 Batch  513 / 525  Training Loss  0.07103971391916275\n",
            "Epoch  7 Batch  514 / 525  Training Loss  0.07164411246776581\n",
            "Epoch  7 Batch  515 / 525  Training Loss  0.06808996200561523\n",
            "Epoch  7 Batch  516 / 525  Training Loss  0.07526075094938278\n",
            "Epoch  7 Batch  517 / 525  Training Loss  0.0677887499332428\n",
            "Epoch  7 Batch  518 / 525  Training Loss  0.07607599347829819\n",
            "Epoch  7 Batch  519 / 525  Training Loss  0.0795266181230545\n",
            "Epoch  7 Batch  520 / 525  Training Loss  0.06433294713497162\n",
            "Epoch  7 Batch  521 / 525  Training Loss  0.06768307089805603\n",
            "Epoch  7 Batch  522 / 525  Training Loss  0.07009853422641754\n",
            "Epoch  7 Batch  523 / 525  Training Loss  0.08296443521976471\n",
            "Epoch  7 Batch  524 / 525  Training Loss  0.08887262642383575\n",
            "   8    |    -    |   0.074309   | 39.058333\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 8\n",
            "Epoch  8 Batch  0 / 525  Training Loss  0.06731933355331421\n",
            "Epoch  8 Batch  1 / 525  Training Loss  0.05463537573814392\n",
            "Epoch  8 Batch  2 / 525  Training Loss  0.054051052778959274\n",
            "Epoch  8 Batch  3 / 525  Training Loss  0.06853477656841278\n",
            "Epoch  8 Batch  4 / 525  Training Loss  0.05749371647834778\n",
            "Epoch  8 Batch  5 / 525  Training Loss  0.05563816428184509\n",
            "Epoch  8 Batch  6 / 525  Training Loss  0.0763794332742691\n",
            "Epoch  8 Batch  7 / 525  Training Loss  0.0685199424624443\n",
            "Epoch  8 Batch  8 / 525  Training Loss  0.06794897466897964\n",
            "Epoch  8 Batch  9 / 525  Training Loss  0.07599659264087677\n",
            "Epoch  8 Batch  10 / 525  Training Loss  0.05695268511772156\n",
            "Epoch  8 Batch  11 / 525  Training Loss  0.06755083054304123\n",
            "Epoch  8 Batch  12 / 525  Training Loss  0.04791413992643356\n",
            "Epoch  8 Batch  13 / 525  Training Loss  0.0709039494395256\n",
            "Epoch  8 Batch  14 / 525  Training Loss  0.06388235837221146\n",
            "Epoch  8 Batch  15 / 525  Training Loss  0.06187216565012932\n",
            "Epoch  8 Batch  16 / 525  Training Loss  0.05979989841580391\n",
            "Epoch  8 Batch  17 / 525  Training Loss  0.07320218533277512\n",
            "Epoch  8 Batch  18 / 525  Training Loss  0.07237937301397324\n",
            "Epoch  8 Batch  19 / 525  Training Loss  0.07336170226335526\n",
            "Epoch  8 Batch  20 / 525  Training Loss  0.07076002657413483\n",
            "Epoch  8 Batch  21 / 525  Training Loss  0.056880444288253784\n",
            "Epoch  8 Batch  22 / 525  Training Loss  0.07306933403015137\n",
            "Epoch  8 Batch  23 / 525  Training Loss  0.07262440025806427\n",
            "Epoch  8 Batch  24 / 525  Training Loss  0.058309782296419144\n",
            "Epoch  8 Batch  25 / 525  Training Loss  0.07536161690950394\n",
            "Epoch  8 Batch  26 / 525  Training Loss  0.06813783943653107\n",
            "Epoch  8 Batch  27 / 525  Training Loss  0.059656184166669846\n",
            "Epoch  8 Batch  28 / 525  Training Loss  0.052423059940338135\n",
            "Epoch  8 Batch  29 / 525  Training Loss  0.06353307515382767\n",
            "Epoch  8 Batch  30 / 525  Training Loss  0.07428250461816788\n",
            "Epoch  8 Batch  31 / 525  Training Loss  0.06184198334813118\n",
            "Epoch  8 Batch  32 / 525  Training Loss  0.05209432169795036\n",
            "Epoch  8 Batch  33 / 525  Training Loss  0.0774960145354271\n",
            "Epoch  8 Batch  34 / 525  Training Loss  0.06301744282245636\n",
            "Epoch  8 Batch  35 / 525  Training Loss  0.05763111263513565\n",
            "Epoch  8 Batch  36 / 525  Training Loss  0.06020023673772812\n",
            "Epoch  8 Batch  37 / 525  Training Loss  0.05088216811418533\n",
            "Epoch  8 Batch  38 / 525  Training Loss  0.07135020196437836\n",
            "Epoch  8 Batch  39 / 525  Training Loss  0.05616573244333267\n",
            "Epoch  8 Batch  40 / 525  Training Loss  0.0797060877084732\n",
            "Epoch  8 Batch  41 / 525  Training Loss  0.08063243329524994\n",
            "Epoch  8 Batch  42 / 525  Training Loss  0.07549287378787994\n",
            "Epoch  8 Batch  43 / 525  Training Loss  0.06198132783174515\n",
            "Epoch  8 Batch  44 / 525  Training Loss  0.0741674154996872\n",
            "Epoch  8 Batch  45 / 525  Training Loss  0.05158185958862305\n",
            "Epoch  8 Batch  46 / 525  Training Loss  0.05647771432995796\n",
            "Epoch  8 Batch  47 / 525  Training Loss  0.07969348132610321\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  8 Batch  48 / 525  Training Loss  0.060969769954681396\n",
            "Epoch  8 Batch  49 / 525  Training Loss  0.06551666557788849\n",
            "Epoch  8 Batch  50 / 525  Training Loss  0.08064302057027817\n",
            "Epoch  8 Batch  51 / 525  Training Loss  0.06137041375041008\n",
            "Epoch  8 Batch  52 / 525  Training Loss  0.06067986041307449\n",
            "Epoch  8 Batch  53 / 525  Training Loss  0.07531597465276718\n",
            "Epoch  8 Batch  54 / 525  Training Loss  0.06706804782152176\n",
            "Epoch  8 Batch  55 / 525  Training Loss  0.07190513610839844\n",
            "Epoch  8 Batch  56 / 525  Training Loss  0.08254110813140869\n",
            "Epoch  8 Batch  57 / 525  Training Loss  0.071584053337574\n",
            "Epoch  8 Batch  58 / 525  Training Loss  0.06714095175266266\n",
            "Epoch  8 Batch  59 / 525  Training Loss  0.07410527765750885\n",
            "Epoch  8 Batch  60 / 525  Training Loss  0.056947432458400726\n",
            "Epoch  8 Batch  61 / 525  Training Loss  0.05313698202371597\n",
            "Epoch  8 Batch  62 / 525  Training Loss  0.061411261558532715\n",
            "Epoch  8 Batch  63 / 525  Training Loss  0.060682594776153564\n",
            "Epoch  8 Batch  64 / 525  Training Loss  0.07344995439052582\n",
            "Epoch  8 Batch  65 / 525  Training Loss  0.07041600346565247\n",
            "Epoch  8 Batch  66 / 525  Training Loss  0.0471804104745388\n",
            "Epoch  8 Batch  67 / 525  Training Loss  0.042424276471138\n",
            "Epoch  8 Batch  68 / 525  Training Loss  0.08045397698879242\n",
            "Epoch  8 Batch  69 / 525  Training Loss  0.06836188584566116\n",
            "Epoch  8 Batch  70 / 525  Training Loss  0.06841370463371277\n",
            "Epoch  8 Batch  71 / 525  Training Loss  0.06499208509922028\n",
            "Epoch  8 Batch  72 / 525  Training Loss  0.058454640209674835\n",
            "Epoch  8 Batch  73 / 525  Training Loss  0.062375180423259735\n",
            "Epoch  8 Batch  74 / 525  Training Loss  0.055541496723890305\n",
            "Epoch  8 Batch  75 / 525  Training Loss  0.05577385425567627\n",
            "Epoch  8 Batch  76 / 525  Training Loss  0.07180877029895782\n",
            "Epoch  8 Batch  77 / 525  Training Loss  0.055846601724624634\n",
            "Epoch  8 Batch  78 / 525  Training Loss  0.060931771993637085\n",
            "Epoch  8 Batch  79 / 525  Training Loss  0.07680751383304596\n",
            "Epoch  8 Batch  80 / 525  Training Loss  0.067951999604702\n",
            "Epoch  8 Batch  81 / 525  Training Loss  0.07239054888486862\n",
            "Epoch  8 Batch  82 / 525  Training Loss  0.06738948822021484\n",
            "Epoch  8 Batch  83 / 525  Training Loss  0.06368818134069443\n",
            "Epoch  8 Batch  84 / 525  Training Loss  0.07233081012964249\n",
            "Epoch  8 Batch  85 / 525  Training Loss  0.07233729213476181\n",
            "Epoch  8 Batch  86 / 525  Training Loss  0.05567372590303421\n",
            "Epoch  8 Batch  87 / 525  Training Loss  0.06423678249120712\n",
            "Epoch  8 Batch  88 / 525  Training Loss  0.06871454417705536\n",
            "Epoch  8 Batch  89 / 525  Training Loss  0.06711307168006897\n",
            "Epoch  8 Batch  90 / 525  Training Loss  0.06494981795549393\n",
            "Epoch  8 Batch  91 / 525  Training Loss  0.0708300992846489\n",
            "Epoch  8 Batch  92 / 525  Training Loss  0.0710771232843399\n",
            "Epoch  8 Batch  93 / 525  Training Loss  0.0857807844877243\n",
            "Epoch  8 Batch  94 / 525  Training Loss  0.08214199542999268\n",
            "Epoch  8 Batch  95 / 525  Training Loss  0.07349878549575806\n",
            "Epoch  8 Batch  96 / 525  Training Loss  0.08969245851039886\n",
            "Epoch  8 Batch  97 / 525  Training Loss  0.08933895826339722\n",
            "Epoch  8 Batch  98 / 525  Training Loss  0.0671783983707428\n",
            "Epoch  8 Batch  99 / 525  Training Loss  0.07806486636400223\n",
            "Epoch  8 Batch  100 / 525  Training Loss  0.06428435444831848\n",
            "Epoch  8 Batch  101 / 525  Training Loss  0.05472787469625473\n",
            "Epoch  8 Batch  102 / 525  Training Loss  0.056945670396089554\n",
            "Epoch  8 Batch  103 / 525  Training Loss  0.08412463963031769\n",
            "Epoch  8 Batch  104 / 525  Training Loss  0.05338302254676819\n",
            "Epoch  8 Batch  105 / 525  Training Loss  0.05937381461262703\n",
            "Epoch  8 Batch  106 / 525  Training Loss  0.051450230181217194\n",
            "Epoch  8 Batch  107 / 525  Training Loss  0.0636616125702858\n",
            "Epoch  8 Batch  108 / 525  Training Loss  0.058982811868190765\n",
            "Epoch  8 Batch  109 / 525  Training Loss  0.051698941737413406\n",
            "Epoch  8 Batch  110 / 525  Training Loss  0.05847107619047165\n",
            "Epoch  8 Batch  111 / 525  Training Loss  0.06874265521764755\n",
            "Epoch  8 Batch  112 / 525  Training Loss  0.04738324135541916\n",
            "Epoch  8 Batch  113 / 525  Training Loss  0.04966464266180992\n",
            "Epoch  8 Batch  114 / 525  Training Loss  0.08301558345556259\n",
            "Epoch  8 Batch  115 / 525  Training Loss  0.07037407159805298\n",
            "Epoch  8 Batch  116 / 525  Training Loss  0.0826578363776207\n",
            "Epoch  8 Batch  117 / 525  Training Loss  0.06471313536167145\n",
            "Epoch  8 Batch  118 / 525  Training Loss  0.054797928780317307\n",
            "Epoch  8 Batch  119 / 525  Training Loss  0.06856556236743927\n",
            "Epoch  8 Batch  120 / 525  Training Loss  0.06977351009845734\n",
            "Epoch  8 Batch  121 / 525  Training Loss  0.06813720613718033\n",
            "Epoch  8 Batch  122 / 525  Training Loss  0.08086250722408295\n",
            "Epoch  8 Batch  123 / 525  Training Loss  0.06320659071207047\n",
            "Epoch  8 Batch  124 / 525  Training Loss  0.051553208380937576\n",
            "Epoch  8 Batch  125 / 525  Training Loss  0.06933407485485077\n",
            "Epoch  8 Batch  126 / 525  Training Loss  0.07380889356136322\n",
            "Epoch  8 Batch  127 / 525  Training Loss  0.06317947804927826\n",
            "Epoch  8 Batch  128 / 525  Training Loss  0.06934914737939835\n",
            "Epoch  8 Batch  129 / 525  Training Loss  0.04567284509539604\n",
            "Epoch  8 Batch  130 / 525  Training Loss  0.07568050175905228\n",
            "Epoch  8 Batch  131 / 525  Training Loss  0.06491262465715408\n",
            "Epoch  8 Batch  132 / 525  Training Loss  0.07454396784305573\n",
            "Epoch  8 Batch  133 / 525  Training Loss  0.05828511714935303\n",
            "Epoch  8 Batch  134 / 525  Training Loss  0.0603480264544487\n",
            "Epoch  8 Batch  135 / 525  Training Loss  0.06024032086133957\n",
            "Epoch  8 Batch  136 / 525  Training Loss  0.044175885617733\n",
            "Epoch  8 Batch  137 / 525  Training Loss  0.05533856153488159\n",
            "Epoch  8 Batch  138 / 525  Training Loss  0.05587980896234512\n",
            "Epoch  8 Batch  139 / 525  Training Loss  0.07378555834293365\n",
            "Epoch  8 Batch  140 / 525  Training Loss  0.088298000395298\n",
            "Epoch  8 Batch  141 / 525  Training Loss  0.07249630242586136\n",
            "Epoch  8 Batch  142 / 525  Training Loss  0.08207406848669052\n",
            "Epoch  8 Batch  143 / 525  Training Loss  0.07011959701776505\n",
            "Epoch  8 Batch  144 / 525  Training Loss  0.08843336999416351\n",
            "Epoch  8 Batch  145 / 525  Training Loss  0.06531719863414764\n",
            "Epoch  8 Batch  146 / 525  Training Loss  0.06984100490808487\n",
            "Epoch  8 Batch  147 / 525  Training Loss  0.06955288350582123\n",
            "Epoch  8 Batch  148 / 525  Training Loss  0.08204653859138489\n",
            "Epoch  8 Batch  149 / 525  Training Loss  0.06738010048866272\n",
            "Epoch  8 Batch  150 / 525  Training Loss  0.05743243172764778\n",
            "Epoch  8 Batch  151 / 525  Training Loss  0.08621492981910706\n",
            "Epoch  8 Batch  152 / 525  Training Loss  0.05599549412727356\n",
            "Epoch  8 Batch  153 / 525  Training Loss  0.05428620055317879\n",
            "Epoch  8 Batch  154 / 525  Training Loss  0.05674121901392937\n",
            "Epoch  8 Batch  155 / 525  Training Loss  0.07239753752946854\n",
            "Epoch  8 Batch  156 / 525  Training Loss  0.08789670467376709\n",
            "Epoch  8 Batch  157 / 525  Training Loss  0.08887018263339996\n",
            "Epoch  8 Batch  158 / 525  Training Loss  0.08412741869688034\n",
            "Epoch  8 Batch  159 / 525  Training Loss  0.0567488856613636\n",
            "Epoch  8 Batch  160 / 525  Training Loss  0.07023178040981293\n",
            "Epoch  8 Batch  161 / 525  Training Loss  0.07118407636880875\n",
            "Epoch  8 Batch  162 / 525  Training Loss  0.07497034966945648\n",
            "Epoch  8 Batch  163 / 525  Training Loss  0.07064434140920639\n",
            "Epoch  8 Batch  164 / 525  Training Loss  0.0656643658876419\n",
            "Epoch  8 Batch  165 / 525  Training Loss  0.08160657435655594\n",
            "Epoch  8 Batch  166 / 525  Training Loss  0.05215359851717949\n",
            "Epoch  8 Batch  167 / 525  Training Loss  0.06861508637666702\n",
            "Epoch  8 Batch  168 / 525  Training Loss  0.07205431163311005\n",
            "Epoch  8 Batch  169 / 525  Training Loss  0.07588649541139603\n",
            "Epoch  8 Batch  170 / 525  Training Loss  0.08104623854160309\n",
            "Epoch  8 Batch  171 / 525  Training Loss  0.06940682232379913\n",
            "Epoch  8 Batch  172 / 525  Training Loss  0.07457022368907928\n",
            "Epoch  8 Batch  173 / 525  Training Loss  0.05871203541755676\n",
            "Epoch  8 Batch  174 / 525  Training Loss  0.05570952966809273\n",
            "Epoch  8 Batch  175 / 525  Training Loss  0.05099915713071823\n",
            "Epoch  8 Batch  176 / 525  Training Loss  0.07147769629955292\n",
            "Epoch  8 Batch  177 / 525  Training Loss  0.058073170483112335\n",
            "Epoch  8 Batch  178 / 525  Training Loss  0.07374979555606842\n",
            "Epoch  8 Batch  179 / 525  Training Loss  0.06171144172549248\n",
            "Epoch  8 Batch  180 / 525  Training Loss  0.06902085244655609\n",
            "Epoch  8 Batch  181 / 525  Training Loss  0.06900777667760849\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  8 Batch  182 / 525  Training Loss  0.07440358400344849\n",
            "Epoch  8 Batch  183 / 525  Training Loss  0.062118709087371826\n",
            "Epoch  8 Batch  184 / 525  Training Loss  0.07351292669773102\n",
            "Epoch  8 Batch  185 / 525  Training Loss  0.10918857902288437\n",
            "Epoch  8 Batch  186 / 525  Training Loss  0.07217639684677124\n",
            "Epoch  8 Batch  187 / 525  Training Loss  0.09093506634235382\n",
            "Epoch  8 Batch  188 / 525  Training Loss  0.0726289302110672\n",
            "Epoch  8 Batch  189 / 525  Training Loss  0.08414231240749359\n",
            "Epoch  8 Batch  190 / 525  Training Loss  0.06578566133975983\n",
            "Epoch  8 Batch  191 / 525  Training Loss  0.06730000674724579\n",
            "Epoch  8 Batch  192 / 525  Training Loss  0.06940458714962006\n",
            "Epoch  8 Batch  193 / 525  Training Loss  0.06583239883184433\n",
            "Epoch  8 Batch  194 / 525  Training Loss  0.0580889992415905\n",
            "Epoch  8 Batch  195 / 525  Training Loss  0.051776330918073654\n",
            "Epoch  8 Batch  196 / 525  Training Loss  0.06983396410942078\n",
            "Epoch  8 Batch  197 / 525  Training Loss  0.05728776380419731\n",
            "Epoch  8 Batch  198 / 525  Training Loss  0.07347629964351654\n",
            "Epoch  8 Batch  199 / 525  Training Loss  0.03667377308011055\n",
            "Epoch  8 Batch  200 / 525  Training Loss  0.07922773063182831\n",
            "Epoch  8 Batch  201 / 525  Training Loss  0.07163946330547333\n",
            "Epoch  8 Batch  202 / 525  Training Loss  0.09473393857479095\n",
            "Epoch  8 Batch  203 / 525  Training Loss  0.0653497725725174\n",
            "Epoch  8 Batch  204 / 525  Training Loss  0.06859772652387619\n",
            "Epoch  8 Batch  205 / 525  Training Loss  0.07718850672245026\n",
            "Epoch  8 Batch  206 / 525  Training Loss  0.07017236202955246\n",
            "Epoch  8 Batch  207 / 525  Training Loss  0.05250563099980354\n",
            "Epoch  8 Batch  208 / 525  Training Loss  0.06764628738164902\n",
            "Epoch  8 Batch  209 / 525  Training Loss  0.06609046459197998\n",
            "Epoch  8 Batch  210 / 525  Training Loss  0.06051130220293999\n",
            "Epoch  8 Batch  211 / 525  Training Loss  0.06173316761851311\n",
            "Epoch  8 Batch  212 / 525  Training Loss  0.0628214180469513\n",
            "Epoch  8 Batch  213 / 525  Training Loss  0.0810859352350235\n",
            "Epoch  8 Batch  214 / 525  Training Loss  0.07468034327030182\n",
            "Epoch  8 Batch  215 / 525  Training Loss  0.07266293466091156\n",
            "Epoch  8 Batch  216 / 525  Training Loss  0.05822508782148361\n",
            "Epoch  8 Batch  217 / 525  Training Loss  0.07502030581235886\n",
            "Epoch  8 Batch  218 / 525  Training Loss  0.06750616431236267\n",
            "Epoch  8 Batch  219 / 525  Training Loss  0.05945441126823425\n",
            "Epoch  8 Batch  220 / 525  Training Loss  0.0714053064584732\n",
            "Epoch  8 Batch  221 / 525  Training Loss  0.08483695983886719\n",
            "Epoch  8 Batch  222 / 525  Training Loss  0.09186744689941406\n",
            "Epoch  8 Batch  223 / 525  Training Loss  0.07395201176404953\n",
            "Epoch  8 Batch  224 / 525  Training Loss  0.06298045814037323\n",
            "Epoch  8 Batch  225 / 525  Training Loss  0.07942250370979309\n",
            "Epoch  8 Batch  226 / 525  Training Loss  0.08747212588787079\n",
            "Epoch  8 Batch  227 / 525  Training Loss  0.08037737756967545\n",
            "Epoch  8 Batch  228 / 525  Training Loss  0.0811004489660263\n",
            "Epoch  8 Batch  229 / 525  Training Loss  0.05831195041537285\n",
            "Epoch  8 Batch  230 / 525  Training Loss  0.06578026711940765\n",
            "Epoch  8 Batch  231 / 525  Training Loss  0.06867300719022751\n",
            "Epoch  8 Batch  232 / 525  Training Loss  0.07434184104204178\n",
            "Epoch  8 Batch  233 / 525  Training Loss  0.07664920389652252\n",
            "Epoch  8 Batch  234 / 525  Training Loss  0.06662546098232269\n",
            "Epoch  8 Batch  235 / 525  Training Loss  0.08323460072278976\n",
            "Epoch  8 Batch  236 / 525  Training Loss  0.08617059886455536\n",
            "Epoch  8 Batch  237 / 525  Training Loss  0.05157870799303055\n",
            "Epoch  8 Batch  238 / 525  Training Loss  0.0765659362077713\n",
            "Epoch  8 Batch  239 / 525  Training Loss  0.0726952776312828\n",
            "Epoch  8 Batch  240 / 525  Training Loss  0.07866846024990082\n",
            "Epoch  8 Batch  241 / 525  Training Loss  0.07901287823915482\n",
            "Epoch  8 Batch  242 / 525  Training Loss  0.07343897223472595\n",
            "Epoch  8 Batch  243 / 525  Training Loss  0.0713372677564621\n",
            "Epoch  8 Batch  244 / 525  Training Loss  0.057997506111860275\n",
            "Epoch  8 Batch  245 / 525  Training Loss  0.08679074794054031\n",
            "Epoch  8 Batch  246 / 525  Training Loss  0.06882119923830032\n",
            "Epoch  8 Batch  247 / 525  Training Loss  0.05442280322313309\n",
            "Epoch  8 Batch  248 / 525  Training Loss  0.06108473986387253\n",
            "Epoch  8 Batch  249 / 525  Training Loss  0.04428824782371521\n",
            "Epoch  8 Batch  250 / 525  Training Loss  0.0681738555431366\n",
            "Epoch  8 Batch  251 / 525  Training Loss  0.05079822614789009\n",
            "Epoch  8 Batch  252 / 525  Training Loss  0.05699489638209343\n",
            "Epoch  8 Batch  253 / 525  Training Loss  0.05727884918451309\n",
            "Epoch  8 Batch  254 / 525  Training Loss  0.06429748237133026\n",
            "Epoch  8 Batch  255 / 525  Training Loss  0.06284590065479279\n",
            "Epoch  8 Batch  256 / 525  Training Loss  0.08877831697463989\n",
            "Epoch  8 Batch  257 / 525  Training Loss  0.0652109831571579\n",
            "Epoch  8 Batch  258 / 525  Training Loss  0.07548799365758896\n",
            "Epoch  8 Batch  259 / 525  Training Loss  0.06369338929653168\n",
            "Epoch  8 Batch  260 / 525  Training Loss  0.0847087949514389\n",
            "Epoch  8 Batch  261 / 525  Training Loss  0.06665848195552826\n",
            "Epoch  8 Batch  262 / 525  Training Loss  0.06970608979463577\n",
            "Epoch  8 Batch  263 / 525  Training Loss  0.05898643657565117\n",
            "Epoch  8 Batch  264 / 525  Training Loss  0.0690971165895462\n",
            "Epoch  8 Batch  265 / 525  Training Loss  0.09153299033641815\n",
            "Epoch  8 Batch  266 / 525  Training Loss  0.08123921602964401\n",
            "Epoch  8 Batch  267 / 525  Training Loss  0.08033932745456696\n",
            "Epoch  8 Batch  268 / 525  Training Loss  0.060410480946302414\n",
            "Epoch  8 Batch  269 / 525  Training Loss  0.06583072990179062\n",
            "Epoch  8 Batch  270 / 525  Training Loss  0.07551424950361252\n",
            "Epoch  8 Batch  271 / 525  Training Loss  0.069149911403656\n",
            "Epoch  8 Batch  272 / 525  Training Loss  0.06106923148036003\n",
            "Epoch  8 Batch  273 / 525  Training Loss  0.059657204896211624\n",
            "Epoch  8 Batch  274 / 525  Training Loss  0.06229778379201889\n",
            "Epoch  8 Batch  275 / 525  Training Loss  0.08612066507339478\n",
            "Epoch  8 Batch  276 / 525  Training Loss  0.07150960713624954\n",
            "Epoch  8 Batch  277 / 525  Training Loss  0.06044738367199898\n",
            "Epoch  8 Batch  278 / 525  Training Loss  0.06182757019996643\n",
            "Epoch  8 Batch  279 / 525  Training Loss  0.0709802657365799\n",
            "Epoch  8 Batch  280 / 525  Training Loss  0.049460239708423615\n",
            "Epoch  8 Batch  281 / 525  Training Loss  0.06479434669017792\n",
            "Epoch  8 Batch  282 / 525  Training Loss  0.0518672950565815\n",
            "Epoch  8 Batch  283 / 525  Training Loss  0.049821220338344574\n",
            "Epoch  8 Batch  284 / 525  Training Loss  0.081426240503788\n",
            "Epoch  8 Batch  285 / 525  Training Loss  0.07274580001831055\n",
            "Epoch  8 Batch  286 / 525  Training Loss  0.07762210071086884\n",
            "Epoch  8 Batch  287 / 525  Training Loss  0.04743954911828041\n",
            "Epoch  8 Batch  288 / 525  Training Loss  0.05974150449037552\n",
            "Epoch  8 Batch  289 / 525  Training Loss  0.07114638388156891\n",
            "Epoch  8 Batch  290 / 525  Training Loss  0.07449300587177277\n",
            "Epoch  8 Batch  291 / 525  Training Loss  0.07424551993608475\n",
            "Epoch  8 Batch  292 / 525  Training Loss  0.0720529705286026\n",
            "Epoch  8 Batch  293 / 525  Training Loss  0.0706675797700882\n",
            "Epoch  8 Batch  294 / 525  Training Loss  0.05560985952615738\n",
            "Epoch  8 Batch  295 / 525  Training Loss  0.08295010030269623\n",
            "Epoch  8 Batch  296 / 525  Training Loss  0.07615616172552109\n",
            "Epoch  8 Batch  297 / 525  Training Loss  0.05550447851419449\n",
            "Epoch  8 Batch  298 / 525  Training Loss  0.06515198200941086\n",
            "Epoch  8 Batch  299 / 525  Training Loss  0.06038530915975571\n",
            "Epoch  8 Batch  300 / 525  Training Loss  0.058012574911117554\n",
            "Epoch  8 Batch  301 / 525  Training Loss  0.0747426301240921\n",
            "Epoch  8 Batch  302 / 525  Training Loss  0.08472301065921783\n",
            "Epoch  8 Batch  303 / 525  Training Loss  0.0658586174249649\n",
            "Epoch  8 Batch  304 / 525  Training Loss  0.054107047617435455\n",
            "Epoch  8 Batch  305 / 525  Training Loss  0.07374095916748047\n",
            "Epoch  8 Batch  306 / 525  Training Loss  0.07210824638605118\n",
            "Epoch  8 Batch  307 / 525  Training Loss  0.0575551874935627\n",
            "Epoch  8 Batch  308 / 525  Training Loss  0.0658254325389862\n",
            "Epoch  8 Batch  309 / 525  Training Loss  0.08855029940605164\n",
            "Epoch  8 Batch  310 / 525  Training Loss  0.05324207618832588\n",
            "Epoch  8 Batch  311 / 525  Training Loss  0.07074596732854843\n",
            "Epoch  8 Batch  312 / 525  Training Loss  0.05250634625554085\n",
            "Epoch  8 Batch  313 / 525  Training Loss  0.07725964486598969\n",
            "Epoch  8 Batch  314 / 525  Training Loss  0.06410231441259384\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  8 Batch  315 / 525  Training Loss  0.07037927210330963\n",
            "Epoch  8 Batch  316 / 525  Training Loss  0.05803926661610603\n",
            "Epoch  8 Batch  317 / 525  Training Loss  0.06870423257350922\n",
            "Epoch  8 Batch  318 / 525  Training Loss  0.06779283285140991\n",
            "Epoch  8 Batch  319 / 525  Training Loss  0.07077954709529877\n",
            "Epoch  8 Batch  320 / 525  Training Loss  0.06012270972132683\n",
            "Epoch  8 Batch  321 / 525  Training Loss  0.05706123262643814\n",
            "Epoch  8 Batch  322 / 525  Training Loss  0.06329631060361862\n",
            "Epoch  8 Batch  323 / 525  Training Loss  0.05121312662959099\n",
            "Epoch  8 Batch  324 / 525  Training Loss  0.06498457491397858\n",
            "Epoch  8 Batch  325 / 525  Training Loss  0.0601981058716774\n",
            "Epoch  8 Batch  326 / 525  Training Loss  0.06971053779125214\n",
            "Epoch  8 Batch  327 / 525  Training Loss  0.0583086721599102\n",
            "Epoch  8 Batch  328 / 525  Training Loss  0.06953836232423782\n",
            "Epoch  8 Batch  329 / 525  Training Loss  0.06506030261516571\n",
            "Epoch  8 Batch  330 / 525  Training Loss  0.07984448224306107\n",
            "Epoch  8 Batch  331 / 525  Training Loss  0.08936191350221634\n",
            "Epoch  8 Batch  332 / 525  Training Loss  0.06630248576402664\n",
            "Epoch  8 Batch  333 / 525  Training Loss  0.0824626162648201\n",
            "Epoch  8 Batch  334 / 525  Training Loss  0.07311229407787323\n",
            "Epoch  8 Batch  335 / 525  Training Loss  0.07797171175479889\n",
            "Epoch  8 Batch  336 / 525  Training Loss  0.057691313326358795\n",
            "Epoch  8 Batch  337 / 525  Training Loss  0.05890681594610214\n",
            "Epoch  8 Batch  338 / 525  Training Loss  0.05325092002749443\n",
            "Epoch  8 Batch  339 / 525  Training Loss  0.06385456025600433\n",
            "Epoch  8 Batch  340 / 525  Training Loss  0.06479949504137039\n",
            "Epoch  8 Batch  341 / 525  Training Loss  0.0845765769481659\n",
            "Epoch  8 Batch  342 / 525  Training Loss  0.08643578737974167\n",
            "Epoch  8 Batch  343 / 525  Training Loss  0.07425350695848465\n",
            "Epoch  8 Batch  344 / 525  Training Loss  0.054766543209552765\n",
            "Epoch  8 Batch  345 / 525  Training Loss  0.07396799325942993\n",
            "Epoch  8 Batch  346 / 525  Training Loss  0.07423599809408188\n",
            "Epoch  8 Batch  347 / 525  Training Loss  0.08425598591566086\n",
            "Epoch  8 Batch  348 / 525  Training Loss  0.06206366419792175\n",
            "Epoch  8 Batch  349 / 525  Training Loss  0.05939695984125137\n",
            "Epoch  8 Batch  350 / 525  Training Loss  0.06676744669675827\n",
            "Epoch  8 Batch  351 / 525  Training Loss  0.06068137288093567\n",
            "Epoch  8 Batch  352 / 525  Training Loss  0.08270377665758133\n",
            "Epoch  8 Batch  353 / 525  Training Loss  0.07351528853178024\n",
            "Epoch  8 Batch  354 / 525  Training Loss  0.07343341410160065\n",
            "Epoch  8 Batch  355 / 525  Training Loss  0.07112885266542435\n",
            "Epoch  8 Batch  356 / 525  Training Loss  0.05878268554806709\n",
            "Epoch  8 Batch  357 / 525  Training Loss  0.04515429213643074\n",
            "Epoch  8 Batch  358 / 525  Training Loss  0.056738246232271194\n",
            "Epoch  8 Batch  359 / 525  Training Loss  0.0726318284869194\n",
            "Epoch  8 Batch  360 / 525  Training Loss  0.05834287405014038\n",
            "Epoch  8 Batch  361 / 525  Training Loss  0.0689404085278511\n",
            "Epoch  8 Batch  362 / 525  Training Loss  0.07555986940860748\n",
            "Epoch  8 Batch  363 / 525  Training Loss  0.05208846926689148\n",
            "Epoch  8 Batch  364 / 525  Training Loss  0.0681210532784462\n",
            "Epoch  8 Batch  365 / 525  Training Loss  0.06899027526378632\n",
            "Epoch  8 Batch  366 / 525  Training Loss  0.05846187472343445\n",
            "Epoch  8 Batch  367 / 525  Training Loss  0.0781109482049942\n",
            "Epoch  8 Batch  368 / 525  Training Loss  0.06733664125204086\n",
            "Epoch  8 Batch  369 / 525  Training Loss  0.07528011500835419\n",
            "Epoch  8 Batch  370 / 525  Training Loss  0.06539465487003326\n",
            "Epoch  8 Batch  371 / 525  Training Loss  0.06680432707071304\n",
            "Epoch  8 Batch  372 / 525  Training Loss  0.06832698732614517\n",
            "Epoch  8 Batch  373 / 525  Training Loss  0.05348151922225952\n",
            "Epoch  8 Batch  374 / 525  Training Loss  0.06888817250728607\n",
            "Epoch  8 Batch  375 / 525  Training Loss  0.05163269490003586\n",
            "Epoch  8 Batch  376 / 525  Training Loss  0.06610503047704697\n",
            "Epoch  8 Batch  377 / 525  Training Loss  0.07396560907363892\n",
            "Epoch  8 Batch  378 / 525  Training Loss  0.06809680163860321\n",
            "Epoch  8 Batch  379 / 525  Training Loss  0.076298788189888\n",
            "Epoch  8 Batch  380 / 525  Training Loss  0.07103309035301208\n",
            "Epoch  8 Batch  381 / 525  Training Loss  0.0648684948682785\n",
            "Epoch  8 Batch  382 / 525  Training Loss  0.06416130810976028\n",
            "Epoch  8 Batch  383 / 525  Training Loss  0.06639611721038818\n",
            "Epoch  8 Batch  384 / 525  Training Loss  0.07570143043994904\n",
            "Epoch  8 Batch  385 / 525  Training Loss  0.0669310986995697\n",
            "Epoch  8 Batch  386 / 525  Training Loss  0.06195777654647827\n",
            "Epoch  8 Batch  387 / 525  Training Loss  0.06077011674642563\n",
            "Epoch  8 Batch  388 / 525  Training Loss  0.07125534117221832\n",
            "Epoch  8 Batch  389 / 525  Training Loss  0.06820649653673172\n",
            "Epoch  8 Batch  390 / 525  Training Loss  0.060227375477552414\n",
            "Epoch  8 Batch  391 / 525  Training Loss  0.062349725514650345\n",
            "Epoch  8 Batch  392 / 525  Training Loss  0.06577199697494507\n",
            "Epoch  8 Batch  393 / 525  Training Loss  0.061444155871868134\n",
            "Epoch  8 Batch  394 / 525  Training Loss  0.06393866240978241\n",
            "Epoch  8 Batch  395 / 525  Training Loss  0.08305011689662933\n",
            "Epoch  8 Batch  396 / 525  Training Loss  0.06262346357107162\n",
            "Epoch  8 Batch  397 / 525  Training Loss  0.06710301339626312\n",
            "Epoch  8 Batch  398 / 525  Training Loss  0.054979968816041946\n",
            "Epoch  8 Batch  399 / 525  Training Loss  0.05997733026742935\n",
            "Epoch  8 Batch  400 / 525  Training Loss  0.05117737129330635\n",
            "Epoch  8 Batch  401 / 525  Training Loss  0.06565361469984055\n",
            "Epoch  8 Batch  402 / 525  Training Loss  0.06556031107902527\n",
            "Epoch  8 Batch  403 / 525  Training Loss  0.0682893767952919\n",
            "Epoch  8 Batch  404 / 525  Training Loss  0.05244918912649155\n",
            "Epoch  8 Batch  405 / 525  Training Loss  0.08304409682750702\n",
            "Epoch  8 Batch  406 / 525  Training Loss  0.0692463144659996\n",
            "Epoch  8 Batch  407 / 525  Training Loss  0.08511711657047272\n",
            "Epoch  8 Batch  408 / 525  Training Loss  0.06150985509157181\n",
            "Epoch  8 Batch  409 / 525  Training Loss  0.055994708091020584\n",
            "Epoch  8 Batch  410 / 525  Training Loss  0.07923159003257751\n",
            "Epoch  8 Batch  411 / 525  Training Loss  0.05403826758265495\n",
            "Epoch  8 Batch  412 / 525  Training Loss  0.05530263110995293\n",
            "Epoch  8 Batch  413 / 525  Training Loss  0.07567300647497177\n",
            "Epoch  8 Batch  414 / 525  Training Loss  0.061311475932598114\n",
            "Epoch  8 Batch  415 / 525  Training Loss  0.0692020058631897\n",
            "Epoch  8 Batch  416 / 525  Training Loss  0.05765294283628464\n",
            "Epoch  8 Batch  417 / 525  Training Loss  0.06794346868991852\n",
            "Epoch  8 Batch  418 / 525  Training Loss  0.05923197790980339\n",
            "Epoch  8 Batch  419 / 525  Training Loss  0.06343340873718262\n",
            "Epoch  8 Batch  420 / 525  Training Loss  0.07448181509971619\n",
            "Epoch  8 Batch  421 / 525  Training Loss  0.05550079420208931\n",
            "Epoch  8 Batch  422 / 525  Training Loss  0.06321963667869568\n",
            "Epoch  8 Batch  423 / 525  Training Loss  0.07241007685661316\n",
            "Epoch  8 Batch  424 / 525  Training Loss  0.0872284322977066\n",
            "Epoch  8 Batch  425 / 525  Training Loss  0.0601346418261528\n",
            "Epoch  8 Batch  426 / 525  Training Loss  0.05989032983779907\n",
            "Epoch  8 Batch  427 / 525  Training Loss  0.0672193244099617\n",
            "Epoch  8 Batch  428 / 525  Training Loss  0.04988802224397659\n",
            "Epoch  8 Batch  429 / 525  Training Loss  0.05301595479249954\n",
            "Epoch  8 Batch  430 / 525  Training Loss  0.048228271305561066\n",
            "Epoch  8 Batch  431 / 525  Training Loss  0.06564490497112274\n",
            "Epoch  8 Batch  432 / 525  Training Loss  0.05227385833859444\n",
            "Epoch  8 Batch  433 / 525  Training Loss  0.07598768174648285\n",
            "Epoch  8 Batch  434 / 525  Training Loss  0.055934179574251175\n",
            "Epoch  8 Batch  435 / 525  Training Loss  0.054891668260097504\n",
            "Epoch  8 Batch  436 / 525  Training Loss  0.06534319370985031\n",
            "Epoch  8 Batch  437 / 525  Training Loss  0.06456632912158966\n",
            "Epoch  8 Batch  438 / 525  Training Loss  0.06512535363435745\n",
            "Epoch  8 Batch  439 / 525  Training Loss  0.07411416620016098\n",
            "Epoch  8 Batch  440 / 525  Training Loss  0.06805066019296646\n",
            "Epoch  8 Batch  441 / 525  Training Loss  0.0842621847987175\n",
            "Epoch  8 Batch  442 / 525  Training Loss  0.06113797426223755\n",
            "Epoch  8 Batch  443 / 525  Training Loss  0.07798384875059128\n",
            "Epoch  8 Batch  444 / 525  Training Loss  0.050932325422763824\n",
            "Epoch  8 Batch  445 / 525  Training Loss  0.07580909878015518\n",
            "Epoch  8 Batch  446 / 525  Training Loss  0.07782282680273056\n",
            "Epoch  8 Batch  447 / 525  Training Loss  0.06856005638837814\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  8 Batch  448 / 525  Training Loss  0.07503369450569153\n",
            "Epoch  8 Batch  449 / 525  Training Loss  0.060327716171741486\n",
            "Epoch  8 Batch  450 / 525  Training Loss  0.06143425777554512\n",
            "Epoch  8 Batch  451 / 525  Training Loss  0.0714392215013504\n",
            "Epoch  8 Batch  452 / 525  Training Loss  0.048034463077783585\n",
            "Epoch  8 Batch  453 / 525  Training Loss  0.06431134045124054\n",
            "Epoch  8 Batch  454 / 525  Training Loss  0.0664377361536026\n",
            "Epoch  8 Batch  455 / 525  Training Loss  0.06774690747261047\n",
            "Epoch  8 Batch  456 / 525  Training Loss  0.059151239693164825\n",
            "Epoch  8 Batch  457 / 525  Training Loss  0.042984191328287125\n",
            "Epoch  8 Batch  458 / 525  Training Loss  0.07003730535507202\n",
            "Epoch  8 Batch  459 / 525  Training Loss  0.05331883952021599\n",
            "Epoch  8 Batch  460 / 525  Training Loss  0.06061785668134689\n",
            "Epoch  8 Batch  461 / 525  Training Loss  0.08126527070999146\n",
            "Epoch  8 Batch  462 / 525  Training Loss  0.05253567546606064\n",
            "Epoch  8 Batch  463 / 525  Training Loss  0.08650264143943787\n",
            "Epoch  8 Batch  464 / 525  Training Loss  0.06911992281675339\n",
            "Epoch  8 Batch  465 / 525  Training Loss  0.05279485136270523\n",
            "Epoch  8 Batch  466 / 525  Training Loss  0.0362967774271965\n",
            "Epoch  8 Batch  467 / 525  Training Loss  0.07527673989534378\n",
            "Epoch  8 Batch  468 / 525  Training Loss  0.056937091052532196\n",
            "Epoch  8 Batch  469 / 525  Training Loss  0.06727392226457596\n",
            "Epoch  8 Batch  470 / 525  Training Loss  0.050977062433958054\n",
            "Epoch  8 Batch  471 / 525  Training Loss  0.07682255655527115\n",
            "Epoch  8 Batch  472 / 525  Training Loss  0.07383867353200912\n",
            "Epoch  8 Batch  473 / 525  Training Loss  0.06054769083857536\n",
            "Epoch  8 Batch  474 / 525  Training Loss  0.06570646911859512\n",
            "Epoch  8 Batch  475 / 525  Training Loss  0.058523423969745636\n",
            "Epoch  8 Batch  476 / 525  Training Loss  0.07362008839845657\n",
            "Epoch  8 Batch  477 / 525  Training Loss  0.07295311987400055\n",
            "Epoch  8 Batch  478 / 525  Training Loss  0.08128181844949722\n",
            "Epoch  8 Batch  479 / 525  Training Loss  0.07306601107120514\n",
            "Epoch  8 Batch  480 / 525  Training Loss  0.06605635583400726\n",
            "Epoch  8 Batch  481 / 525  Training Loss  0.0733642503619194\n",
            "Epoch  8 Batch  482 / 525  Training Loss  0.05434640124440193\n",
            "Epoch  8 Batch  483 / 525  Training Loss  0.05489029735326767\n",
            "Epoch  8 Batch  484 / 525  Training Loss  0.06822308897972107\n",
            "Epoch  8 Batch  485 / 525  Training Loss  0.08161471784114838\n",
            "Epoch  8 Batch  486 / 525  Training Loss  0.06909887492656708\n",
            "Epoch  8 Batch  487 / 525  Training Loss  0.07001037150621414\n",
            "Epoch  8 Batch  488 / 525  Training Loss  0.06798355281352997\n",
            "Epoch  8 Batch  489 / 525  Training Loss  0.06251133978366852\n",
            "Epoch  8 Batch  490 / 525  Training Loss  0.06703309714794159\n",
            "Epoch  8 Batch  491 / 525  Training Loss  0.06962728500366211\n",
            "Epoch  8 Batch  492 / 525  Training Loss  0.08053001016378403\n",
            "Epoch  8 Batch  493 / 525  Training Loss  0.06109634041786194\n",
            "Epoch  8 Batch  494 / 525  Training Loss  0.04797450080513954\n",
            "Epoch  8 Batch  495 / 525  Training Loss  0.05601147934794426\n",
            "Epoch  8 Batch  496 / 525  Training Loss  0.08528845012187958\n",
            "Epoch  8 Batch  497 / 525  Training Loss  0.06400422751903534\n",
            "Epoch  8 Batch  498 / 525  Training Loss  0.07595894485712051\n",
            "Epoch  8 Batch  499 / 525  Training Loss  0.06269637495279312\n",
            "Epoch  8 Batch  500 / 525  Training Loss  0.06975816190242767\n",
            "Epoch  8 Batch  501 / 525  Training Loss  0.06326770782470703\n",
            "Epoch  8 Batch  502 / 525  Training Loss  0.05960264801979065\n",
            "Epoch  8 Batch  503 / 525  Training Loss  0.06153424456715584\n",
            "Epoch  8 Batch  504 / 525  Training Loss  0.06089570000767708\n",
            "Epoch  8 Batch  505 / 525  Training Loss  0.072117879986763\n",
            "Epoch  8 Batch  506 / 525  Training Loss  0.06633023917675018\n",
            "Epoch  8 Batch  507 / 525  Training Loss  0.08383237570524216\n",
            "Epoch  8 Batch  508 / 525  Training Loss  0.06545347720384598\n",
            "Epoch  8 Batch  509 / 525  Training Loss  0.07256712019443512\n",
            "Epoch  8 Batch  510 / 525  Training Loss  0.058459531515836716\n",
            "Epoch  8 Batch  511 / 525  Training Loss  0.048329636454582214\n",
            "Epoch  8 Batch  512 / 525  Training Loss  0.07770036160945892\n",
            "Epoch  8 Batch  513 / 525  Training Loss  0.05802193284034729\n",
            "Epoch  8 Batch  514 / 525  Training Loss  0.076970674097538\n",
            "Epoch  8 Batch  515 / 525  Training Loss  0.08233122527599335\n",
            "Epoch  8 Batch  516 / 525  Training Loss  0.05442814156413078\n",
            "Epoch  8 Batch  517 / 525  Training Loss  0.07517780363559723\n",
            "Epoch  8 Batch  518 / 525  Training Loss  0.0663009062409401\n",
            "Epoch  8 Batch  519 / 525  Training Loss  0.06278195232152939\n",
            "Epoch  8 Batch  520 / 525  Training Loss  0.07888279855251312\n",
            "Epoch  8 Batch  521 / 525  Training Loss  0.075283944606781\n",
            "Epoch  8 Batch  522 / 525  Training Loss  0.048932984471321106\n",
            "Epoch  8 Batch  523 / 525  Training Loss  0.07230351120233536\n",
            "Epoch  8 Batch  524 / 525  Training Loss  0.07316963374614716\n",
            "   9    |    -    |   0.067115   | 40.666667\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 9\n",
            "Epoch  9 Batch  0 / 525  Training Loss  0.06047778204083443\n",
            "Epoch  9 Batch  1 / 525  Training Loss  0.07971267402172089\n",
            "Epoch  9 Batch  2 / 525  Training Loss  0.048233453184366226\n",
            "Epoch  9 Batch  3 / 525  Training Loss  0.05733298137784004\n",
            "Epoch  9 Batch  4 / 525  Training Loss  0.05226749926805496\n",
            "Epoch  9 Batch  5 / 525  Training Loss  0.052650995552539825\n",
            "Epoch  9 Batch  6 / 525  Training Loss  0.04368862509727478\n",
            "Epoch  9 Batch  7 / 525  Training Loss  0.06491480767726898\n",
            "Epoch  9 Batch  8 / 525  Training Loss  0.0654241070151329\n",
            "Epoch  9 Batch  9 / 525  Training Loss  0.048763856291770935\n",
            "Epoch  9 Batch  10 / 525  Training Loss  0.08110350370407104\n",
            "Epoch  9 Batch  11 / 525  Training Loss  0.07487093657255173\n",
            "Epoch  9 Batch  12 / 525  Training Loss  0.05060497671365738\n",
            "Epoch  9 Batch  13 / 525  Training Loss  0.056853752583265305\n",
            "Epoch  9 Batch  14 / 525  Training Loss  0.059484999626874924\n",
            "Epoch  9 Batch  15 / 525  Training Loss  0.038424864411354065\n",
            "Epoch  9 Batch  16 / 525  Training Loss  0.058093808591365814\n",
            "Epoch  9 Batch  17 / 525  Training Loss  0.06821586191654205\n",
            "Epoch  9 Batch  18 / 525  Training Loss  0.06365223973989487\n",
            "Epoch  9 Batch  19 / 525  Training Loss  0.0582963302731514\n",
            "Epoch  9 Batch  20 / 525  Training Loss  0.04357225075364113\n",
            "Epoch  9 Batch  21 / 525  Training Loss  0.06166255474090576\n",
            "Epoch  9 Batch  22 / 525  Training Loss  0.06075214594602585\n",
            "Epoch  9 Batch  23 / 525  Training Loss  0.06570807844400406\n",
            "Epoch  9 Batch  24 / 525  Training Loss  0.04193907976150513\n",
            "Epoch  9 Batch  25 / 525  Training Loss  0.048437610268592834\n",
            "Epoch  9 Batch  26 / 525  Training Loss  0.0555720329284668\n",
            "Epoch  9 Batch  27 / 525  Training Loss  0.06261933594942093\n",
            "Epoch  9 Batch  28 / 525  Training Loss  0.0500815212726593\n",
            "Epoch  9 Batch  29 / 525  Training Loss  0.07658182084560394\n",
            "Epoch  9 Batch  30 / 525  Training Loss  0.06262977421283722\n",
            "Epoch  9 Batch  31 / 525  Training Loss  0.06976001709699631\n",
            "Epoch  9 Batch  32 / 525  Training Loss  0.06142842769622803\n",
            "Epoch  9 Batch  33 / 525  Training Loss  0.0613795630633831\n",
            "Epoch  9 Batch  34 / 525  Training Loss  0.05918104574084282\n",
            "Epoch  9 Batch  35 / 525  Training Loss  0.06958158314228058\n",
            "Epoch  9 Batch  36 / 525  Training Loss  0.07621471583843231\n",
            "Epoch  9 Batch  37 / 525  Training Loss  0.05265083163976669\n",
            "Epoch  9 Batch  38 / 525  Training Loss  0.04930026829242706\n",
            "Epoch  9 Batch  39 / 525  Training Loss  0.060700349509716034\n",
            "Epoch  9 Batch  40 / 525  Training Loss  0.049822043627500534\n",
            "Epoch  9 Batch  41 / 525  Training Loss  0.05589047074317932\n",
            "Epoch  9 Batch  42 / 525  Training Loss  0.05552101135253906\n",
            "Epoch  9 Batch  43 / 525  Training Loss  0.0723116472363472\n",
            "Epoch  9 Batch  44 / 525  Training Loss  0.07818165421485901\n",
            "Epoch  9 Batch  45 / 525  Training Loss  0.07112365961074829\n",
            "Epoch  9 Batch  46 / 525  Training Loss  0.04863051325082779\n",
            "Epoch  9 Batch  47 / 525  Training Loss  0.03933149576187134\n",
            "Epoch  9 Batch  48 / 525  Training Loss  0.04826173931360245\n",
            "Epoch  9 Batch  49 / 525  Training Loss  0.07700255513191223\n",
            "Epoch  9 Batch  50 / 525  Training Loss  0.06409544497728348\n",
            "Epoch  9 Batch  51 / 525  Training Loss  0.049415744841098785\n",
            "Epoch  9 Batch  52 / 525  Training Loss  0.056130826473236084\n",
            "Epoch  9 Batch  53 / 525  Training Loss  0.05321790650486946\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  9 Batch  54 / 525  Training Loss  0.057857491075992584\n",
            "Epoch  9 Batch  55 / 525  Training Loss  0.06116136163473129\n",
            "Epoch  9 Batch  56 / 525  Training Loss  0.06839773058891296\n",
            "Epoch  9 Batch  57 / 525  Training Loss  0.05517556145787239\n",
            "Epoch  9 Batch  58 / 525  Training Loss  0.06385530531406403\n",
            "Epoch  9 Batch  59 / 525  Training Loss  0.05717325955629349\n",
            "Epoch  9 Batch  60 / 525  Training Loss  0.069893017411232\n",
            "Epoch  9 Batch  61 / 525  Training Loss  0.06545919924974442\n",
            "Epoch  9 Batch  62 / 525  Training Loss  0.06586458534002304\n",
            "Epoch  9 Batch  63 / 525  Training Loss  0.06834156811237335\n",
            "Epoch  9 Batch  64 / 525  Training Loss  0.04707622900605202\n",
            "Epoch  9 Batch  65 / 525  Training Loss  0.04795681685209274\n",
            "Epoch  9 Batch  66 / 525  Training Loss  0.06083027273416519\n",
            "Epoch  9 Batch  67 / 525  Training Loss  0.05257203057408333\n",
            "Epoch  9 Batch  68 / 525  Training Loss  0.06469734013080597\n",
            "Epoch  9 Batch  69 / 525  Training Loss  0.05222820118069649\n",
            "Epoch  9 Batch  70 / 525  Training Loss  0.06469712406396866\n",
            "Epoch  9 Batch  71 / 525  Training Loss  0.07074742019176483\n",
            "Epoch  9 Batch  72 / 525  Training Loss  0.05156312137842178\n",
            "Epoch  9 Batch  73 / 525  Training Loss  0.062322694808244705\n",
            "Epoch  9 Batch  74 / 525  Training Loss  0.0644754022359848\n",
            "Epoch  9 Batch  75 / 525  Training Loss  0.07297691702842712\n",
            "Epoch  9 Batch  76 / 525  Training Loss  0.06801498681306839\n",
            "Epoch  9 Batch  77 / 525  Training Loss  0.06760929524898529\n",
            "Epoch  9 Batch  78 / 525  Training Loss  0.07044126093387604\n",
            "Epoch  9 Batch  79 / 525  Training Loss  0.047738924622535706\n",
            "Epoch  9 Batch  80 / 525  Training Loss  0.055349964648485184\n",
            "Epoch  9 Batch  81 / 525  Training Loss  0.08588626980781555\n",
            "Epoch  9 Batch  82 / 525  Training Loss  0.049745239317417145\n",
            "Epoch  9 Batch  83 / 525  Training Loss  0.045938022434711456\n",
            "Epoch  9 Batch  84 / 525  Training Loss  0.06109672039747238\n",
            "Epoch  9 Batch  85 / 525  Training Loss  0.054247837513685226\n",
            "Epoch  9 Batch  86 / 525  Training Loss  0.04868125915527344\n",
            "Epoch  9 Batch  87 / 525  Training Loss  0.0438285768032074\n",
            "Epoch  9 Batch  88 / 525  Training Loss  0.050463877618312836\n",
            "Epoch  9 Batch  89 / 525  Training Loss  0.07179318368434906\n",
            "Epoch  9 Batch  90 / 525  Training Loss  0.06620877981185913\n",
            "Epoch  9 Batch  91 / 525  Training Loss  0.04638601839542389\n",
            "Epoch  9 Batch  92 / 525  Training Loss  0.05998315289616585\n",
            "Epoch  9 Batch  93 / 525  Training Loss  0.04479993134737015\n",
            "Epoch  9 Batch  94 / 525  Training Loss  0.04516027122735977\n",
            "Epoch  9 Batch  95 / 525  Training Loss  0.06307666003704071\n",
            "Epoch  9 Batch  96 / 525  Training Loss  0.06390748172998428\n",
            "Epoch  9 Batch  97 / 525  Training Loss  0.059211283922195435\n",
            "Epoch  9 Batch  98 / 525  Training Loss  0.06482140719890594\n",
            "Epoch  9 Batch  99 / 525  Training Loss  0.05462947487831116\n",
            "Epoch  9 Batch  100 / 525  Training Loss  0.05886401608586311\n",
            "Epoch  9 Batch  101 / 525  Training Loss  0.05366601422429085\n",
            "Epoch  9 Batch  102 / 525  Training Loss  0.04756871238350868\n",
            "Epoch  9 Batch  103 / 525  Training Loss  0.04665946215391159\n",
            "Epoch  9 Batch  104 / 525  Training Loss  0.05263566970825195\n",
            "Epoch  9 Batch  105 / 525  Training Loss  0.046613361686468124\n",
            "Epoch  9 Batch  106 / 525  Training Loss  0.06785736232995987\n",
            "Epoch  9 Batch  107 / 525  Training Loss  0.06491817533969879\n",
            "Epoch  9 Batch  108 / 525  Training Loss  0.04959673434495926\n",
            "Epoch  9 Batch  109 / 525  Training Loss  0.05869372561573982\n",
            "Epoch  9 Batch  110 / 525  Training Loss  0.06686695665121078\n",
            "Epoch  9 Batch  111 / 525  Training Loss  0.04636971652507782\n",
            "Epoch  9 Batch  112 / 525  Training Loss  0.0669403225183487\n",
            "Epoch  9 Batch  113 / 525  Training Loss  0.04756174236536026\n",
            "Epoch  9 Batch  114 / 525  Training Loss  0.056610286235809326\n",
            "Epoch  9 Batch  115 / 525  Training Loss  0.070123091340065\n",
            "Epoch  9 Batch  116 / 525  Training Loss  0.06155075877904892\n",
            "Epoch  9 Batch  117 / 525  Training Loss  0.08265091478824615\n",
            "Epoch  9 Batch  118 / 525  Training Loss  0.04944964498281479\n",
            "Epoch  9 Batch  119 / 525  Training Loss  0.04691106826066971\n",
            "Epoch  9 Batch  120 / 525  Training Loss  0.06298013776540756\n",
            "Epoch  9 Batch  121 / 525  Training Loss  0.05791764706373215\n",
            "Epoch  9 Batch  122 / 525  Training Loss  0.05340774729847908\n",
            "Epoch  9 Batch  123 / 525  Training Loss  0.048360154032707214\n",
            "Epoch  9 Batch  124 / 525  Training Loss  0.05925431847572327\n",
            "Epoch  9 Batch  125 / 525  Training Loss  0.051547981798648834\n",
            "Epoch  9 Batch  126 / 525  Training Loss  0.04940543323755264\n",
            "Epoch  9 Batch  127 / 525  Training Loss  0.057674963027238846\n",
            "Epoch  9 Batch  128 / 525  Training Loss  0.0813036784529686\n",
            "Epoch  9 Batch  129 / 525  Training Loss  0.06272147595882416\n",
            "Epoch  9 Batch  130 / 525  Training Loss  0.06821857392787933\n",
            "Epoch  9 Batch  131 / 525  Training Loss  0.05055645853281021\n",
            "Epoch  9 Batch  132 / 525  Training Loss  0.06392481178045273\n",
            "Epoch  9 Batch  133 / 525  Training Loss  0.053979940712451935\n",
            "Epoch  9 Batch  134 / 525  Training Loss  0.058965444564819336\n",
            "Epoch  9 Batch  135 / 525  Training Loss  0.05281178280711174\n",
            "Epoch  9 Batch  136 / 525  Training Loss  0.05219576880335808\n",
            "Epoch  9 Batch  137 / 525  Training Loss  0.07568792998790741\n",
            "Epoch  9 Batch  138 / 525  Training Loss  0.06952065229415894\n",
            "Epoch  9 Batch  139 / 525  Training Loss  0.03839746117591858\n",
            "Epoch  9 Batch  140 / 525  Training Loss  0.05600839853286743\n",
            "Epoch  9 Batch  141 / 525  Training Loss  0.06380902230739594\n",
            "Epoch  9 Batch  142 / 525  Training Loss  0.05333332344889641\n",
            "Epoch  9 Batch  143 / 525  Training Loss  0.07486975938081741\n",
            "Epoch  9 Batch  144 / 525  Training Loss  0.046544820070266724\n",
            "Epoch  9 Batch  145 / 525  Training Loss  0.05340753123164177\n",
            "Epoch  9 Batch  146 / 525  Training Loss  0.05315682291984558\n",
            "Epoch  9 Batch  147 / 525  Training Loss  0.0780491977930069\n",
            "Epoch  9 Batch  148 / 525  Training Loss  0.05616557598114014\n",
            "Epoch  9 Batch  149 / 525  Training Loss  0.04011658951640129\n",
            "Epoch  9 Batch  150 / 525  Training Loss  0.05339425057172775\n",
            "Epoch  9 Batch  151 / 525  Training Loss  0.07144532352685928\n",
            "Epoch  9 Batch  152 / 525  Training Loss  0.06941324472427368\n",
            "Epoch  9 Batch  153 / 525  Training Loss  0.061518289148807526\n",
            "Epoch  9 Batch  154 / 525  Training Loss  0.05546675994992256\n",
            "Epoch  9 Batch  155 / 525  Training Loss  0.05632494017481804\n",
            "Epoch  9 Batch  156 / 525  Training Loss  0.045323655009269714\n",
            "Epoch  9 Batch  157 / 525  Training Loss  0.05026392266154289\n",
            "Epoch  9 Batch  158 / 525  Training Loss  0.07426737993955612\n",
            "Epoch  9 Batch  159 / 525  Training Loss  0.05903036519885063\n",
            "Epoch  9 Batch  160 / 525  Training Loss  0.052764344960451126\n",
            "Epoch  9 Batch  161 / 525  Training Loss  0.052560221403837204\n",
            "Epoch  9 Batch  162 / 525  Training Loss  0.06479042023420334\n",
            "Epoch  9 Batch  163 / 525  Training Loss  0.06410449743270874\n",
            "Epoch  9 Batch  164 / 525  Training Loss  0.05673053115606308\n",
            "Epoch  9 Batch  165 / 525  Training Loss  0.06776825338602066\n",
            "Epoch  9 Batch  166 / 525  Training Loss  0.06239499896764755\n",
            "Epoch  9 Batch  167 / 525  Training Loss  0.060664139688014984\n",
            "Epoch  9 Batch  168 / 525  Training Loss  0.05498534440994263\n",
            "Epoch  9 Batch  169 / 525  Training Loss  0.03404314070940018\n",
            "Epoch  9 Batch  170 / 525  Training Loss  0.04977835342288017\n",
            "Epoch  9 Batch  171 / 525  Training Loss  0.06543812900781631\n",
            "Epoch  9 Batch  172 / 525  Training Loss  0.06387173384428024\n",
            "Epoch  9 Batch  173 / 525  Training Loss  0.049022916704416275\n",
            "Epoch  9 Batch  174 / 525  Training Loss  0.06881874054670334\n",
            "Epoch  9 Batch  175 / 525  Training Loss  0.042771752923727036\n",
            "Epoch  9 Batch  176 / 525  Training Loss  0.04637598991394043\n",
            "Epoch  9 Batch  177 / 525  Training Loss  0.06668423861265182\n",
            "Epoch  9 Batch  178 / 525  Training Loss  0.06878945976495743\n",
            "Epoch  9 Batch  179 / 525  Training Loss  0.05990927293896675\n",
            "Epoch  9 Batch  180 / 525  Training Loss  0.07032226026058197\n",
            "Epoch  9 Batch  181 / 525  Training Loss  0.055203717201948166\n",
            "Epoch  9 Batch  182 / 525  Training Loss  0.062298644334077835\n",
            "Epoch  9 Batch  183 / 525  Training Loss  0.06492672115564346\n",
            "Epoch  9 Batch  184 / 525  Training Loss  0.059585511684417725\n",
            "Epoch  9 Batch  185 / 525  Training Loss  0.03950513154268265\n",
            "Epoch  9 Batch  186 / 525  Training Loss  0.0647759884595871\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  9 Batch  187 / 525  Training Loss  0.0770936831831932\n",
            "Epoch  9 Batch  188 / 525  Training Loss  0.07688718289136887\n",
            "Epoch  9 Batch  189 / 525  Training Loss  0.06706864386796951\n",
            "Epoch  9 Batch  190 / 525  Training Loss  0.05127856135368347\n",
            "Epoch  9 Batch  191 / 525  Training Loss  0.040914230048656464\n",
            "Epoch  9 Batch  192 / 525  Training Loss  0.05887977406382561\n",
            "Epoch  9 Batch  193 / 525  Training Loss  0.04141298681497574\n",
            "Epoch  9 Batch  194 / 525  Training Loss  0.06117025017738342\n",
            "Epoch  9 Batch  195 / 525  Training Loss  0.07764275372028351\n",
            "Epoch  9 Batch  196 / 525  Training Loss  0.05240634083747864\n",
            "Epoch  9 Batch  197 / 525  Training Loss  0.06280118227005005\n",
            "Epoch  9 Batch  198 / 525  Training Loss  0.051883261650800705\n",
            "Epoch  9 Batch  199 / 525  Training Loss  0.059570759534835815\n",
            "Epoch  9 Batch  200 / 525  Training Loss  0.05468184873461723\n",
            "Epoch  9 Batch  201 / 525  Training Loss  0.058351971209049225\n",
            "Epoch  9 Batch  202 / 525  Training Loss  0.058325719088315964\n",
            "Epoch  9 Batch  203 / 525  Training Loss  0.0553952232003212\n",
            "Epoch  9 Batch  204 / 525  Training Loss  0.051906831562519073\n",
            "Epoch  9 Batch  205 / 525  Training Loss  0.06096091866493225\n",
            "Epoch  9 Batch  206 / 525  Training Loss  0.061243463307619095\n",
            "Epoch  9 Batch  207 / 525  Training Loss  0.06318698823451996\n",
            "Epoch  9 Batch  208 / 525  Training Loss  0.057309724390506744\n",
            "Epoch  9 Batch  209 / 525  Training Loss  0.05501677468419075\n",
            "Epoch  9 Batch  210 / 525  Training Loss  0.060049187391996384\n",
            "Epoch  9 Batch  211 / 525  Training Loss  0.06798321008682251\n",
            "Epoch  9 Batch  212 / 525  Training Loss  0.05735187605023384\n",
            "Epoch  9 Batch  213 / 525  Training Loss  0.06259052455425262\n",
            "Epoch  9 Batch  214 / 525  Training Loss  0.06590895354747772\n",
            "Epoch  9 Batch  215 / 525  Training Loss  0.058991603553295135\n",
            "Epoch  9 Batch  216 / 525  Training Loss  0.046413399279117584\n",
            "Epoch  9 Batch  217 / 525  Training Loss  0.04754134267568588\n",
            "Epoch  9 Batch  218 / 525  Training Loss  0.05589594691991806\n",
            "Epoch  9 Batch  219 / 525  Training Loss  0.050268352031707764\n",
            "Epoch  9 Batch  220 / 525  Training Loss  0.05351674556732178\n",
            "Epoch  9 Batch  221 / 525  Training Loss  0.06366076320409775\n",
            "Epoch  9 Batch  222 / 525  Training Loss  0.057961054146289825\n",
            "Epoch  9 Batch  223 / 525  Training Loss  0.043133482336997986\n",
            "Epoch  9 Batch  224 / 525  Training Loss  0.049838874489068985\n",
            "Epoch  9 Batch  225 / 525  Training Loss  0.05823634937405586\n",
            "Epoch  9 Batch  226 / 525  Training Loss  0.07659827172756195\n",
            "Epoch  9 Batch  227 / 525  Training Loss  0.056662481278181076\n",
            "Epoch  9 Batch  228 / 525  Training Loss  0.04641125351190567\n",
            "Epoch  9 Batch  229 / 525  Training Loss  0.045579031109809875\n",
            "Epoch  9 Batch  230 / 525  Training Loss  0.05386350303888321\n",
            "Epoch  9 Batch  231 / 525  Training Loss  0.06369882076978683\n",
            "Epoch  9 Batch  232 / 525  Training Loss  0.07207231223583221\n",
            "Epoch  9 Batch  233 / 525  Training Loss  0.05528220534324646\n",
            "Epoch  9 Batch  234 / 525  Training Loss  0.055105168372392654\n",
            "Epoch  9 Batch  235 / 525  Training Loss  0.05506352335214615\n",
            "Epoch  9 Batch  236 / 525  Training Loss  0.051116906106472015\n",
            "Epoch  9 Batch  237 / 525  Training Loss  0.04202485457062721\n",
            "Epoch  9 Batch  238 / 525  Training Loss  0.047745008021593094\n",
            "Epoch  9 Batch  239 / 525  Training Loss  0.056566815823316574\n",
            "Epoch  9 Batch  240 / 525  Training Loss  0.07753031700849533\n",
            "Epoch  9 Batch  241 / 525  Training Loss  0.044697146862745285\n",
            "Epoch  9 Batch  242 / 525  Training Loss  0.05525815486907959\n",
            "Epoch  9 Batch  243 / 525  Training Loss  0.06454403698444366\n",
            "Epoch  9 Batch  244 / 525  Training Loss  0.0597728006541729\n",
            "Epoch  9 Batch  245 / 525  Training Loss  0.0625791922211647\n",
            "Epoch  9 Batch  246 / 525  Training Loss  0.06748420000076294\n",
            "Epoch  9 Batch  247 / 525  Training Loss  0.05998852103948593\n",
            "Epoch  9 Batch  248 / 525  Training Loss  0.05187378451228142\n",
            "Epoch  9 Batch  249 / 525  Training Loss  0.07110118120908737\n",
            "Epoch  9 Batch  250 / 525  Training Loss  0.06360621750354767\n",
            "Epoch  9 Batch  251 / 525  Training Loss  0.06535954773426056\n",
            "Epoch  9 Batch  252 / 525  Training Loss  0.06586717069149017\n",
            "Epoch  9 Batch  253 / 525  Training Loss  0.06006522849202156\n",
            "Epoch  9 Batch  254 / 525  Training Loss  0.046293921768665314\n",
            "Epoch  9 Batch  255 / 525  Training Loss  0.07936850935220718\n",
            "Epoch  9 Batch  256 / 525  Training Loss  0.05251773074269295\n",
            "Epoch  9 Batch  257 / 525  Training Loss  0.06287820637226105\n",
            "Epoch  9 Batch  258 / 525  Training Loss  0.07012305408716202\n",
            "Epoch  9 Batch  259 / 525  Training Loss  0.06324341893196106\n",
            "Epoch  9 Batch  260 / 525  Training Loss  0.06016085296869278\n",
            "Epoch  9 Batch  261 / 525  Training Loss  0.06544603407382965\n",
            "Epoch  9 Batch  262 / 525  Training Loss  0.0616484060883522\n",
            "Epoch  9 Batch  263 / 525  Training Loss  0.05128280073404312\n",
            "Epoch  9 Batch  264 / 525  Training Loss  0.06330595910549164\n",
            "Epoch  9 Batch  265 / 525  Training Loss  0.06959410756826401\n",
            "Epoch  9 Batch  266 / 525  Training Loss  0.04923173040151596\n",
            "Epoch  9 Batch  267 / 525  Training Loss  0.055883169174194336\n",
            "Epoch  9 Batch  268 / 525  Training Loss  0.06037783622741699\n",
            "Epoch  9 Batch  269 / 525  Training Loss  0.05840710923075676\n",
            "Epoch  9 Batch  270 / 525  Training Loss  0.062140680849552155\n",
            "Epoch  9 Batch  271 / 525  Training Loss  0.07626952230930328\n",
            "Epoch  9 Batch  272 / 525  Training Loss  0.058673612773418427\n",
            "Epoch  9 Batch  273 / 525  Training Loss  0.08307056128978729\n",
            "Epoch  9 Batch  274 / 525  Training Loss  0.06206081062555313\n",
            "Epoch  9 Batch  275 / 525  Training Loss  0.05697443336248398\n",
            "Epoch  9 Batch  276 / 525  Training Loss  0.06729881465435028\n",
            "Epoch  9 Batch  277 / 525  Training Loss  0.05191982910037041\n",
            "Epoch  9 Batch  278 / 525  Training Loss  0.05016583949327469\n",
            "Epoch  9 Batch  279 / 525  Training Loss  0.06328873336315155\n",
            "Epoch  9 Batch  280 / 525  Training Loss  0.05430693179368973\n",
            "Epoch  9 Batch  281 / 525  Training Loss  0.0586172454059124\n",
            "Epoch  9 Batch  282 / 525  Training Loss  0.06535834074020386\n",
            "Epoch  9 Batch  283 / 525  Training Loss  0.05285757780075073\n",
            "Epoch  9 Batch  284 / 525  Training Loss  0.06307590752840042\n",
            "Epoch  9 Batch  285 / 525  Training Loss  0.06240922957658768\n",
            "Epoch  9 Batch  286 / 525  Training Loss  0.057353656738996506\n",
            "Epoch  9 Batch  287 / 525  Training Loss  0.05817004293203354\n",
            "Epoch  9 Batch  288 / 525  Training Loss  0.05675780773162842\n",
            "Epoch  9 Batch  289 / 525  Training Loss  0.05426212400197983\n",
            "Epoch  9 Batch  290 / 525  Training Loss  0.059899717569351196\n",
            "Epoch  9 Batch  291 / 525  Training Loss  0.060633670538663864\n",
            "Epoch  9 Batch  292 / 525  Training Loss  0.06446091830730438\n",
            "Epoch  9 Batch  293 / 525  Training Loss  0.0522674024105072\n",
            "Epoch  9 Batch  294 / 525  Training Loss  0.06049813702702522\n",
            "Epoch  9 Batch  295 / 525  Training Loss  0.06849543005228043\n",
            "Epoch  9 Batch  296 / 525  Training Loss  0.0504617802798748\n",
            "Epoch  9 Batch  297 / 525  Training Loss  0.06726564466953278\n",
            "Epoch  9 Batch  298 / 525  Training Loss  0.07242902368307114\n",
            "Epoch  9 Batch  299 / 525  Training Loss  0.07891155779361725\n",
            "Epoch  9 Batch  300 / 525  Training Loss  0.059949927031993866\n",
            "Epoch  9 Batch  301 / 525  Training Loss  0.0645417720079422\n",
            "Epoch  9 Batch  302 / 525  Training Loss  0.05532065033912659\n",
            "Epoch  9 Batch  303 / 525  Training Loss  0.07097304612398148\n",
            "Epoch  9 Batch  304 / 525  Training Loss  0.048131637275218964\n",
            "Epoch  9 Batch  305 / 525  Training Loss  0.06693895161151886\n",
            "Epoch  9 Batch  306 / 525  Training Loss  0.07119771093130112\n",
            "Epoch  9 Batch  307 / 525  Training Loss  0.04362381249666214\n",
            "Epoch  9 Batch  308 / 525  Training Loss  0.060033224523067474\n",
            "Epoch  9 Batch  309 / 525  Training Loss  0.07147104293107986\n",
            "Epoch  9 Batch  310 / 525  Training Loss  0.04463754966855049\n",
            "Epoch  9 Batch  311 / 525  Training Loss  0.06289440393447876\n",
            "Epoch  9 Batch  312 / 525  Training Loss  0.04930071160197258\n",
            "Epoch  9 Batch  313 / 525  Training Loss  0.05607038736343384\n",
            "Epoch  9 Batch  314 / 525  Training Loss  0.07914184778928757\n",
            "Epoch  9 Batch  315 / 525  Training Loss  0.06867793202400208\n",
            "Epoch  9 Batch  316 / 525  Training Loss  0.05856139212846756\n",
            "Epoch  9 Batch  317 / 525  Training Loss  0.08030462265014648\n",
            "Epoch  9 Batch  318 / 525  Training Loss  0.0681266039609909\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  9 Batch  319 / 525  Training Loss  0.0653819888830185\n",
            "Epoch  9 Batch  320 / 525  Training Loss  0.06224460527300835\n",
            "Epoch  9 Batch  321 / 525  Training Loss  0.07026791572570801\n",
            "Epoch  9 Batch  322 / 525  Training Loss  0.05079377815127373\n",
            "Epoch  9 Batch  323 / 525  Training Loss  0.06361176818609238\n",
            "Epoch  9 Batch  324 / 525  Training Loss  0.06568856537342072\n",
            "Epoch  9 Batch  325 / 525  Training Loss  0.05714768171310425\n",
            "Epoch  9 Batch  326 / 525  Training Loss  0.05394340679049492\n",
            "Epoch  9 Batch  327 / 525  Training Loss  0.06476832181215286\n",
            "Epoch  9 Batch  328 / 525  Training Loss  0.06053171306848526\n",
            "Epoch  9 Batch  329 / 525  Training Loss  0.04614692181348801\n",
            "Epoch  9 Batch  330 / 525  Training Loss  0.04864170402288437\n",
            "Epoch  9 Batch  331 / 525  Training Loss  0.06787417829036713\n",
            "Epoch  9 Batch  332 / 525  Training Loss  0.0547526553273201\n",
            "Epoch  9 Batch  333 / 525  Training Loss  0.06973376870155334\n",
            "Epoch  9 Batch  334 / 525  Training Loss  0.045426566153764725\n",
            "Epoch  9 Batch  335 / 525  Training Loss  0.06080939620733261\n",
            "Epoch  9 Batch  336 / 525  Training Loss  0.04443945735692978\n",
            "Epoch  9 Batch  337 / 525  Training Loss  0.05231543257832527\n",
            "Epoch  9 Batch  338 / 525  Training Loss  0.05685365945100784\n",
            "Epoch  9 Batch  339 / 525  Training Loss  0.04648114740848541\n",
            "Epoch  9 Batch  340 / 525  Training Loss  0.05724841356277466\n",
            "Epoch  9 Batch  341 / 525  Training Loss  0.06453587114810944\n",
            "Epoch  9 Batch  342 / 525  Training Loss  0.07709525525569916\n",
            "Epoch  9 Batch  343 / 525  Training Loss  0.05788185074925423\n",
            "Epoch  9 Batch  344 / 525  Training Loss  0.06719167530536652\n",
            "Epoch  9 Batch  345 / 525  Training Loss  0.05471581220626831\n",
            "Epoch  9 Batch  346 / 525  Training Loss  0.06437616050243378\n",
            "Epoch  9 Batch  347 / 525  Training Loss  0.0751819983124733\n",
            "Epoch  9 Batch  348 / 525  Training Loss  0.05868847295641899\n",
            "Epoch  9 Batch  349 / 525  Training Loss  0.06007562205195427\n",
            "Epoch  9 Batch  350 / 525  Training Loss  0.0723046138882637\n",
            "Epoch  9 Batch  351 / 525  Training Loss  0.053390394896268845\n",
            "Epoch  9 Batch  352 / 525  Training Loss  0.06735251843929291\n",
            "Epoch  9 Batch  353 / 525  Training Loss  0.056555189192295074\n",
            "Epoch  9 Batch  354 / 525  Training Loss  0.037932951003313065\n",
            "Epoch  9 Batch  355 / 525  Training Loss  0.06254211813211441\n",
            "Epoch  9 Batch  356 / 525  Training Loss  0.05269203707575798\n",
            "Epoch  9 Batch  357 / 525  Training Loss  0.06321726739406586\n",
            "Epoch  9 Batch  358 / 525  Training Loss  0.05133556202054024\n",
            "Epoch  9 Batch  359 / 525  Training Loss  0.05008162185549736\n",
            "Epoch  9 Batch  360 / 525  Training Loss  0.0658973902463913\n",
            "Epoch  9 Batch  361 / 525  Training Loss  0.05816973000764847\n",
            "Epoch  9 Batch  362 / 525  Training Loss  0.0498492456972599\n",
            "Epoch  9 Batch  363 / 525  Training Loss  0.05330859497189522\n",
            "Epoch  9 Batch  364 / 525  Training Loss  0.06055354326963425\n",
            "Epoch  9 Batch  365 / 525  Training Loss  0.0681898295879364\n",
            "Epoch  9 Batch  366 / 525  Training Loss  0.055320046842098236\n",
            "Epoch  9 Batch  367 / 525  Training Loss  0.06651097536087036\n",
            "Epoch  9 Batch  368 / 525  Training Loss  0.051774632185697556\n",
            "Epoch  9 Batch  369 / 525  Training Loss  0.05197439715266228\n",
            "Epoch  9 Batch  370 / 525  Training Loss  0.05802253633737564\n",
            "Epoch  9 Batch  371 / 525  Training Loss  0.07536064833402634\n",
            "Epoch  9 Batch  372 / 525  Training Loss  0.054871249943971634\n",
            "Epoch  9 Batch  373 / 525  Training Loss  0.06000228971242905\n",
            "Epoch  9 Batch  374 / 525  Training Loss  0.05428394675254822\n",
            "Epoch  9 Batch  375 / 525  Training Loss  0.06610938161611557\n",
            "Epoch  9 Batch  376 / 525  Training Loss  0.06406162679195404\n",
            "Epoch  9 Batch  377 / 525  Training Loss  0.07916621118783951\n",
            "Epoch  9 Batch  378 / 525  Training Loss  0.07478360831737518\n",
            "Epoch  9 Batch  379 / 525  Training Loss  0.05377425625920296\n",
            "Epoch  9 Batch  380 / 525  Training Loss  0.07586847245693207\n",
            "Epoch  9 Batch  381 / 525  Training Loss  0.061961740255355835\n",
            "Epoch  9 Batch  382 / 525  Training Loss  0.06101863458752632\n",
            "Epoch  9 Batch  383 / 525  Training Loss  0.06643253564834595\n",
            "Epoch  9 Batch  384 / 525  Training Loss  0.07921206206083298\n",
            "Epoch  9 Batch  385 / 525  Training Loss  0.047602660953998566\n",
            "Epoch  9 Batch  386 / 525  Training Loss  0.0462762787938118\n",
            "Epoch  9 Batch  387 / 525  Training Loss  0.07872314006090164\n",
            "Epoch  9 Batch  388 / 525  Training Loss  0.05706150457262993\n",
            "Epoch  9 Batch  389 / 525  Training Loss  0.06352204829454422\n",
            "Epoch  9 Batch  390 / 525  Training Loss  0.07120026648044586\n",
            "Epoch  9 Batch  391 / 525  Training Loss  0.05079848691821098\n",
            "Epoch  9 Batch  392 / 525  Training Loss  0.05638410896062851\n",
            "Epoch  9 Batch  393 / 525  Training Loss  0.06662503629922867\n",
            "Epoch  9 Batch  394 / 525  Training Loss  0.051710497587919235\n",
            "Epoch  9 Batch  395 / 525  Training Loss  0.08412569016218185\n",
            "Epoch  9 Batch  396 / 525  Training Loss  0.05079643055796623\n",
            "Epoch  9 Batch  397 / 525  Training Loss  0.050114817917346954\n",
            "Epoch  9 Batch  398 / 525  Training Loss  0.05703287199139595\n",
            "Epoch  9 Batch  399 / 525  Training Loss  0.04536319524049759\n",
            "Epoch  9 Batch  400 / 525  Training Loss  0.0467064194381237\n",
            "Epoch  9 Batch  401 / 525  Training Loss  0.04623977467417717\n",
            "Epoch  9 Batch  402 / 525  Training Loss  0.04999033361673355\n",
            "Epoch  9 Batch  403 / 525  Training Loss  0.04494387283921242\n",
            "Epoch  9 Batch  404 / 525  Training Loss  0.05642787739634514\n",
            "Epoch  9 Batch  405 / 525  Training Loss  0.05153433606028557\n",
            "Epoch  9 Batch  406 / 525  Training Loss  0.07884819060564041\n",
            "Epoch  9 Batch  407 / 525  Training Loss  0.04211181402206421\n",
            "Epoch  9 Batch  408 / 525  Training Loss  0.06674586981534958\n",
            "Epoch  9 Batch  409 / 525  Training Loss  0.07278215885162354\n",
            "Epoch  9 Batch  410 / 525  Training Loss  0.057657111436128616\n",
            "Epoch  9 Batch  411 / 525  Training Loss  0.06454559415578842\n",
            "Epoch  9 Batch  412 / 525  Training Loss  0.06073852255940437\n",
            "Epoch  9 Batch  413 / 525  Training Loss  0.06143425777554512\n",
            "Epoch  9 Batch  414 / 525  Training Loss  0.08058090507984161\n",
            "Epoch  9 Batch  415 / 525  Training Loss  0.049440547823905945\n",
            "Epoch  9 Batch  416 / 525  Training Loss  0.05507882311940193\n",
            "Epoch  9 Batch  417 / 525  Training Loss  0.0623454824090004\n",
            "Epoch  9 Batch  418 / 525  Training Loss  0.059761565178632736\n",
            "Epoch  9 Batch  419 / 525  Training Loss  0.06092030927538872\n",
            "Epoch  9 Batch  420 / 525  Training Loss  0.04185892641544342\n",
            "Epoch  9 Batch  421 / 525  Training Loss  0.07850904017686844\n",
            "Epoch  9 Batch  422 / 525  Training Loss  0.07056345790624619\n",
            "Epoch  9 Batch  423 / 525  Training Loss  0.06689898669719696\n",
            "Epoch  9 Batch  424 / 525  Training Loss  0.05414675548672676\n",
            "Epoch  9 Batch  425 / 525  Training Loss  0.0617656335234642\n",
            "Epoch  9 Batch  426 / 525  Training Loss  0.05740465968847275\n",
            "Epoch  9 Batch  427 / 525  Training Loss  0.06726305186748505\n",
            "Epoch  9 Batch  428 / 525  Training Loss  0.048126887530088425\n",
            "Epoch  9 Batch  429 / 525  Training Loss  0.07439355552196503\n",
            "Epoch  9 Batch  430 / 525  Training Loss  0.05421127751469612\n",
            "Epoch  9 Batch  431 / 525  Training Loss  0.07431017607450485\n",
            "Epoch  9 Batch  432 / 525  Training Loss  0.0524555966258049\n",
            "Epoch  9 Batch  433 / 525  Training Loss  0.05550248175859451\n",
            "Epoch  9 Batch  434 / 525  Training Loss  0.0454021617770195\n",
            "Epoch  9 Batch  435 / 525  Training Loss  0.048767246305942535\n",
            "Epoch  9 Batch  436 / 525  Training Loss  0.05972833186388016\n",
            "Epoch  9 Batch  437 / 525  Training Loss  0.07370935380458832\n",
            "Epoch  9 Batch  438 / 525  Training Loss  0.06446067243814468\n",
            "Epoch  9 Batch  439 / 525  Training Loss  0.07174341380596161\n",
            "Epoch  9 Batch  440 / 525  Training Loss  0.07192789763212204\n",
            "Epoch  9 Batch  441 / 525  Training Loss  0.07175128906965256\n",
            "Epoch  9 Batch  442 / 525  Training Loss  0.057591505348682404\n",
            "Epoch  9 Batch  443 / 525  Training Loss  0.05717717483639717\n",
            "Epoch  9 Batch  444 / 525  Training Loss  0.07884129881858826\n",
            "Epoch  9 Batch  445 / 525  Training Loss  0.06316892802715302\n",
            "Epoch  9 Batch  446 / 525  Training Loss  0.05000927299261093\n",
            "Epoch  9 Batch  447 / 525  Training Loss  0.06728872656822205\n",
            "Epoch  9 Batch  448 / 525  Training Loss  0.058404792100191116\n",
            "Epoch  9 Batch  449 / 525  Training Loss  0.053485654294490814\n",
            "Epoch  9 Batch  450 / 525  Training Loss  0.059790533035993576\n",
            "Epoch  9 Batch  451 / 525  Training Loss  0.05538683384656906\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  9 Batch  452 / 525  Training Loss  0.06637957692146301\n",
            "Epoch  9 Batch  453 / 525  Training Loss  0.07670241594314575\n",
            "Epoch  9 Batch  454 / 525  Training Loss  0.05636846274137497\n",
            "Epoch  9 Batch  455 / 525  Training Loss  0.06865493953227997\n",
            "Epoch  9 Batch  456 / 525  Training Loss  0.072609543800354\n",
            "Epoch  9 Batch  457 / 525  Training Loss  0.05439195781946182\n",
            "Epoch  9 Batch  458 / 525  Training Loss  0.060756247490644455\n",
            "Epoch  9 Batch  459 / 525  Training Loss  0.06692109256982803\n",
            "Epoch  9 Batch  460 / 525  Training Loss  0.06610501557588577\n",
            "Epoch  9 Batch  461 / 525  Training Loss  0.0635782927274704\n",
            "Epoch  9 Batch  462 / 525  Training Loss  0.0444965586066246\n",
            "Epoch  9 Batch  463 / 525  Training Loss  0.06096864491701126\n",
            "Epoch  9 Batch  464 / 525  Training Loss  0.04722600430250168\n",
            "Epoch  9 Batch  465 / 525  Training Loss  0.0678342804312706\n",
            "Epoch  9 Batch  466 / 525  Training Loss  0.05736589431762695\n",
            "Epoch  9 Batch  467 / 525  Training Loss  0.05474184826016426\n",
            "Epoch  9 Batch  468 / 525  Training Loss  0.06366782635450363\n",
            "Epoch  9 Batch  469 / 525  Training Loss  0.061096977442502975\n",
            "Epoch  9 Batch  470 / 525  Training Loss  0.05622067302465439\n",
            "Epoch  9 Batch  471 / 525  Training Loss  0.0573943629860878\n",
            "Epoch  9 Batch  472 / 525  Training Loss  0.08120644837617874\n",
            "Epoch  9 Batch  473 / 525  Training Loss  0.06759004294872284\n",
            "Epoch  9 Batch  474 / 525  Training Loss  0.07413123548030853\n",
            "Epoch  9 Batch  475 / 525  Training Loss  0.05527586489915848\n",
            "Epoch  9 Batch  476 / 525  Training Loss  0.0656500905752182\n",
            "Epoch  9 Batch  477 / 525  Training Loss  0.062073368579149246\n",
            "Epoch  9 Batch  478 / 525  Training Loss  0.05855352431535721\n",
            "Epoch  9 Batch  479 / 525  Training Loss  0.05430782586336136\n",
            "Epoch  9 Batch  480 / 525  Training Loss  0.06351878494024277\n",
            "Epoch  9 Batch  481 / 525  Training Loss  0.05636110156774521\n",
            "Epoch  9 Batch  482 / 525  Training Loss  0.07032284140586853\n",
            "Epoch  9 Batch  483 / 525  Training Loss  0.07709507644176483\n",
            "Epoch  9 Batch  484 / 525  Training Loss  0.05152161046862602\n",
            "Epoch  9 Batch  485 / 525  Training Loss  0.06078415364027023\n",
            "Epoch  9 Batch  486 / 525  Training Loss  0.061242587864398956\n",
            "Epoch  9 Batch  487 / 525  Training Loss  0.05220697075128555\n",
            "Epoch  9 Batch  488 / 525  Training Loss  0.059372156858444214\n",
            "Epoch  9 Batch  489 / 525  Training Loss  0.0537310354411602\n",
            "Epoch  9 Batch  490 / 525  Training Loss  0.05189059302210808\n",
            "Epoch  9 Batch  491 / 525  Training Loss  0.06427344679832458\n",
            "Epoch  9 Batch  492 / 525  Training Loss  0.07356090098619461\n",
            "Epoch  9 Batch  493 / 525  Training Loss  0.0741821750998497\n",
            "Epoch  9 Batch  494 / 525  Training Loss  0.07704225182533264\n",
            "Epoch  9 Batch  495 / 525  Training Loss  0.039708204567432404\n",
            "Epoch  9 Batch  496 / 525  Training Loss  0.06233587861061096\n",
            "Epoch  9 Batch  497 / 525  Training Loss  0.04084530100226402\n",
            "Epoch  9 Batch  498 / 525  Training Loss  0.0659407526254654\n",
            "Epoch  9 Batch  499 / 525  Training Loss  0.06000395491719246\n",
            "Epoch  9 Batch  500 / 525  Training Loss  0.057750195264816284\n",
            "Epoch  9 Batch  501 / 525  Training Loss  0.056670188903808594\n",
            "Epoch  9 Batch  502 / 525  Training Loss  0.05544354394078255\n",
            "Epoch  9 Batch  503 / 525  Training Loss  0.05060026794672012\n",
            "Epoch  9 Batch  504 / 525  Training Loss  0.05296851322054863\n",
            "Epoch  9 Batch  505 / 525  Training Loss  0.06124844402074814\n",
            "Epoch  9 Batch  506 / 525  Training Loss  0.06032801419496536\n",
            "Epoch  9 Batch  507 / 525  Training Loss  0.04469199478626251\n",
            "Epoch  9 Batch  508 / 525  Training Loss  0.06875183433294296\n",
            "Epoch  9 Batch  509 / 525  Training Loss  0.07020489871501923\n",
            "Epoch  9 Batch  510 / 525  Training Loss  0.042414236813783646\n",
            "Epoch  9 Batch  511 / 525  Training Loss  0.059149909764528275\n",
            "Epoch  9 Batch  512 / 525  Training Loss  0.0610903799533844\n",
            "Epoch  9 Batch  513 / 525  Training Loss  0.05180588364601135\n",
            "Epoch  9 Batch  514 / 525  Training Loss  0.07200215756893158\n",
            "Epoch  9 Batch  515 / 525  Training Loss  0.060771726071834564\n",
            "Epoch  9 Batch  516 / 525  Training Loss  0.0631176084280014\n",
            "Epoch  9 Batch  517 / 525  Training Loss  0.055818021297454834\n",
            "Epoch  9 Batch  518 / 525  Training Loss  0.09006153047084808\n",
            "Epoch  9 Batch  519 / 525  Training Loss  0.04684784263372421\n",
            "Epoch  9 Batch  520 / 525  Training Loss  0.07191498577594757\n",
            "Epoch  9 Batch  521 / 525  Training Loss  0.052328187972307205\n",
            "Epoch  9 Batch  522 / 525  Training Loss  0.04515004903078079\n",
            "Epoch  9 Batch  523 / 525  Training Loss  0.058765340596437454\n",
            "Epoch  9 Batch  524 / 525  Training Loss  0.0510827898979187\n",
            "  10    |    -    |   0.059600   | 44.850000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 10\n",
            "Epoch  10 Batch  0 / 525  Training Loss  0.055049024522304535\n",
            "Epoch  10 Batch  1 / 525  Training Loss  0.040183257311582565\n",
            "Epoch  10 Batch  2 / 525  Training Loss  0.03721056133508682\n",
            "Epoch  10 Batch  3 / 525  Training Loss  0.03881528973579407\n",
            "Epoch  10 Batch  4 / 525  Training Loss  0.04993991181254387\n",
            "Epoch  10 Batch  5 / 525  Training Loss  0.07038465887308121\n",
            "Epoch  10 Batch  6 / 525  Training Loss  0.04932133108377457\n",
            "Epoch  10 Batch  7 / 525  Training Loss  0.04848744720220566\n",
            "Epoch  10 Batch  8 / 525  Training Loss  0.055605947971343994\n",
            "Epoch  10 Batch  9 / 525  Training Loss  0.04668936878442764\n",
            "Epoch  10 Batch  10 / 525  Training Loss  0.04217619448900223\n",
            "Epoch  10 Batch  11 / 525  Training Loss  0.05050239711999893\n",
            "Epoch  10 Batch  12 / 525  Training Loss  0.0449039526283741\n",
            "Epoch  10 Batch  13 / 525  Training Loss  0.04287756234407425\n",
            "Epoch  10 Batch  14 / 525  Training Loss  0.04386717826128006\n",
            "Epoch  10 Batch  15 / 525  Training Loss  0.04378053918480873\n",
            "Epoch  10 Batch  16 / 525  Training Loss  0.049311019480228424\n",
            "Epoch  10 Batch  17 / 525  Training Loss  0.041381217539310455\n",
            "Epoch  10 Batch  18 / 525  Training Loss  0.04805801436305046\n",
            "Epoch  10 Batch  19 / 525  Training Loss  0.03996293991804123\n",
            "Epoch  10 Batch  20 / 525  Training Loss  0.043774619698524475\n",
            "Epoch  10 Batch  21 / 525  Training Loss  0.029127657413482666\n",
            "Epoch  10 Batch  22 / 525  Training Loss  0.045806679874658585\n",
            "Epoch  10 Batch  23 / 525  Training Loss  0.05940543860197067\n",
            "Epoch  10 Batch  24 / 525  Training Loss  0.05423882603645325\n",
            "Epoch  10 Batch  25 / 525  Training Loss  0.05350079387426376\n",
            "Epoch  10 Batch  26 / 525  Training Loss  0.04524094983935356\n",
            "Epoch  10 Batch  27 / 525  Training Loss  0.043902479112148285\n",
            "Epoch  10 Batch  28 / 525  Training Loss  0.05932687968015671\n",
            "Epoch  10 Batch  29 / 525  Training Loss  0.06023303419351578\n",
            "Epoch  10 Batch  30 / 525  Training Loss  0.04338216036558151\n",
            "Epoch  10 Batch  31 / 525  Training Loss  0.05037827044725418\n",
            "Epoch  10 Batch  32 / 525  Training Loss  0.05814116448163986\n",
            "Epoch  10 Batch  33 / 525  Training Loss  0.04628855362534523\n",
            "Epoch  10 Batch  34 / 525  Training Loss  0.05201756954193115\n",
            "Epoch  10 Batch  35 / 525  Training Loss  0.04991535097360611\n",
            "Epoch  10 Batch  36 / 525  Training Loss  0.036153800785541534\n",
            "Epoch  10 Batch  37 / 525  Training Loss  0.042080771178007126\n",
            "Epoch  10 Batch  38 / 525  Training Loss  0.04672922566533089\n",
            "Epoch  10 Batch  39 / 525  Training Loss  0.05029504746198654\n",
            "Epoch  10 Batch  40 / 525  Training Loss  0.06116052344441414\n",
            "Epoch  10 Batch  41 / 525  Training Loss  0.04660408943891525\n",
            "Epoch  10 Batch  42 / 525  Training Loss  0.041565023362636566\n",
            "Epoch  10 Batch  43 / 525  Training Loss  0.050876837223768234\n",
            "Epoch  10 Batch  44 / 525  Training Loss  0.04151878505945206\n",
            "Epoch  10 Batch  45 / 525  Training Loss  0.04838018864393234\n",
            "Epoch  10 Batch  46 / 525  Training Loss  0.04030049964785576\n",
            "Epoch  10 Batch  47 / 525  Training Loss  0.04906919598579407\n",
            "Epoch  10 Batch  48 / 525  Training Loss  0.05380936712026596\n",
            "Epoch  10 Batch  49 / 525  Training Loss  0.06775131076574326\n",
            "Epoch  10 Batch  50 / 525  Training Loss  0.04735000431537628\n",
            "Epoch  10 Batch  51 / 525  Training Loss  0.04231339320540428\n",
            "Epoch  10 Batch  52 / 525  Training Loss  0.04054839536547661\n",
            "Epoch  10 Batch  53 / 525  Training Loss  0.04231572151184082\n",
            "Epoch  10 Batch  54 / 525  Training Loss  0.059868890792131424\n",
            "Epoch  10 Batch  55 / 525  Training Loss  0.0465569794178009\n",
            "Epoch  10 Batch  56 / 525  Training Loss  0.05697724223136902\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  10 Batch  57 / 525  Training Loss  0.03545934334397316\n",
            "Epoch  10 Batch  58 / 525  Training Loss  0.04547543078660965\n",
            "Epoch  10 Batch  59 / 525  Training Loss  0.054597001522779465\n",
            "Epoch  10 Batch  60 / 525  Training Loss  0.05485004186630249\n",
            "Epoch  10 Batch  61 / 525  Training Loss  0.04379592090845108\n",
            "Epoch  10 Batch  62 / 525  Training Loss  0.04476159065961838\n",
            "Epoch  10 Batch  63 / 525  Training Loss  0.04192078113555908\n",
            "Epoch  10 Batch  64 / 525  Training Loss  0.040405649691820145\n",
            "Epoch  10 Batch  65 / 525  Training Loss  0.05330617353320122\n",
            "Epoch  10 Batch  66 / 525  Training Loss  0.047404371201992035\n",
            "Epoch  10 Batch  67 / 525  Training Loss  0.0449872687458992\n",
            "Epoch  10 Batch  68 / 525  Training Loss  0.05757193639874458\n",
            "Epoch  10 Batch  69 / 525  Training Loss  0.05585157871246338\n",
            "Epoch  10 Batch  70 / 525  Training Loss  0.058485329151153564\n",
            "Epoch  10 Batch  71 / 525  Training Loss  0.07108324766159058\n",
            "Epoch  10 Batch  72 / 525  Training Loss  0.058880776166915894\n",
            "Epoch  10 Batch  73 / 525  Training Loss  0.06138302758336067\n",
            "Epoch  10 Batch  74 / 525  Training Loss  0.05441879481077194\n",
            "Epoch  10 Batch  75 / 525  Training Loss  0.0576302707195282\n",
            "Epoch  10 Batch  76 / 525  Training Loss  0.04994482919573784\n",
            "Epoch  10 Batch  77 / 525  Training Loss  0.05561946704983711\n",
            "Epoch  10 Batch  78 / 525  Training Loss  0.04762730374932289\n",
            "Epoch  10 Batch  79 / 525  Training Loss  0.04455838352441788\n",
            "Epoch  10 Batch  80 / 525  Training Loss  0.04643786698579788\n",
            "Epoch  10 Batch  81 / 525  Training Loss  0.06052171438932419\n",
            "Epoch  10 Batch  82 / 525  Training Loss  0.04532666876912117\n",
            "Epoch  10 Batch  83 / 525  Training Loss  0.04858231544494629\n",
            "Epoch  10 Batch  84 / 525  Training Loss  0.04468482732772827\n",
            "Epoch  10 Batch  85 / 525  Training Loss  0.050865303725004196\n",
            "Epoch  10 Batch  86 / 525  Training Loss  0.0569089837372303\n",
            "Epoch  10 Batch  87 / 525  Training Loss  0.04554716497659683\n",
            "Epoch  10 Batch  88 / 525  Training Loss  0.06611064821481705\n",
            "Epoch  10 Batch  89 / 525  Training Loss  0.0552784688770771\n",
            "Epoch  10 Batch  90 / 525  Training Loss  0.05366225913167\n",
            "Epoch  10 Batch  91 / 525  Training Loss  0.0719657763838768\n",
            "Epoch  10 Batch  92 / 525  Training Loss  0.050789378583431244\n",
            "Epoch  10 Batch  93 / 525  Training Loss  0.04286954179406166\n",
            "Epoch  10 Batch  94 / 525  Training Loss  0.038569413125514984\n",
            "Epoch  10 Batch  95 / 525  Training Loss  0.06208469718694687\n",
            "Epoch  10 Batch  96 / 525  Training Loss  0.03475571423768997\n",
            "Epoch  10 Batch  97 / 525  Training Loss  0.046246111392974854\n",
            "Epoch  10 Batch  98 / 525  Training Loss  0.06728421151638031\n",
            "Epoch  10 Batch  99 / 525  Training Loss  0.040835924446582794\n",
            "Epoch  10 Batch  100 / 525  Training Loss  0.07445009797811508\n",
            "Epoch  10 Batch  101 / 525  Training Loss  0.04945039376616478\n",
            "Epoch  10 Batch  102 / 525  Training Loss  0.04374160245060921\n",
            "Epoch  10 Batch  103 / 525  Training Loss  0.04900987818837166\n",
            "Epoch  10 Batch  104 / 525  Training Loss  0.031317561864852905\n",
            "Epoch  10 Batch  105 / 525  Training Loss  0.03395334631204605\n",
            "Epoch  10 Batch  106 / 525  Training Loss  0.05169796943664551\n",
            "Epoch  10 Batch  107 / 525  Training Loss  0.06683670729398727\n",
            "Epoch  10 Batch  108 / 525  Training Loss  0.05359721928834915\n",
            "Epoch  10 Batch  109 / 525  Training Loss  0.05099605768918991\n",
            "Epoch  10 Batch  110 / 525  Training Loss  0.06633903086185455\n",
            "Epoch  10 Batch  111 / 525  Training Loss  0.04934963956475258\n",
            "Epoch  10 Batch  112 / 525  Training Loss  0.06727637350559235\n",
            "Epoch  10 Batch  113 / 525  Training Loss  0.0671141967177391\n",
            "Epoch  10 Batch  114 / 525  Training Loss  0.069618359208107\n",
            "Epoch  10 Batch  115 / 525  Training Loss  0.035346828401088715\n",
            "Epoch  10 Batch  116 / 525  Training Loss  0.0604880154132843\n",
            "Epoch  10 Batch  117 / 525  Training Loss  0.05307589843869209\n",
            "Epoch  10 Batch  118 / 525  Training Loss  0.0502583310008049\n",
            "Epoch  10 Batch  119 / 525  Training Loss  0.0561666302382946\n",
            "Epoch  10 Batch  120 / 525  Training Loss  0.04034621641039848\n",
            "Epoch  10 Batch  121 / 525  Training Loss  0.04363280534744263\n",
            "Epoch  10 Batch  122 / 525  Training Loss  0.044585347175598145\n",
            "Epoch  10 Batch  123 / 525  Training Loss  0.03622986376285553\n",
            "Epoch  10 Batch  124 / 525  Training Loss  0.046062640845775604\n",
            "Epoch  10 Batch  125 / 525  Training Loss  0.054509103298187256\n",
            "Epoch  10 Batch  126 / 525  Training Loss  0.051963258534669876\n",
            "Epoch  10 Batch  127 / 525  Training Loss  0.04288726672530174\n",
            "Epoch  10 Batch  128 / 525  Training Loss  0.03494781628251076\n",
            "Epoch  10 Batch  129 / 525  Training Loss  0.05358697846531868\n",
            "Epoch  10 Batch  130 / 525  Training Loss  0.043448660522699356\n",
            "Epoch  10 Batch  131 / 525  Training Loss  0.05302027612924576\n",
            "Epoch  10 Batch  132 / 525  Training Loss  0.0500551275908947\n",
            "Epoch  10 Batch  133 / 525  Training Loss  0.036412011831998825\n",
            "Epoch  10 Batch  134 / 525  Training Loss  0.059642206877470016\n",
            "Epoch  10 Batch  135 / 525  Training Loss  0.049223631620407104\n",
            "Epoch  10 Batch  136 / 525  Training Loss  0.05126867815852165\n",
            "Epoch  10 Batch  137 / 525  Training Loss  0.06279843300580978\n",
            "Epoch  10 Batch  138 / 525  Training Loss  0.05343734472990036\n",
            "Epoch  10 Batch  139 / 525  Training Loss  0.06544254720211029\n",
            "Epoch  10 Batch  140 / 525  Training Loss  0.042999573051929474\n",
            "Epoch  10 Batch  141 / 525  Training Loss  0.05170302838087082\n",
            "Epoch  10 Batch  142 / 525  Training Loss  0.05403737351298332\n",
            "Epoch  10 Batch  143 / 525  Training Loss  0.05631541460752487\n",
            "Epoch  10 Batch  144 / 525  Training Loss  0.05782715231180191\n",
            "Epoch  10 Batch  145 / 525  Training Loss  0.059375327080488205\n",
            "Epoch  10 Batch  146 / 525  Training Loss  0.05397515743970871\n",
            "Epoch  10 Batch  147 / 525  Training Loss  0.06115902215242386\n",
            "Epoch  10 Batch  148 / 525  Training Loss  0.06362365186214447\n",
            "Epoch  10 Batch  149 / 525  Training Loss  0.05817968398332596\n",
            "Epoch  10 Batch  150 / 525  Training Loss  0.059790097177028656\n",
            "Epoch  10 Batch  151 / 525  Training Loss  0.061831437051296234\n",
            "Epoch  10 Batch  152 / 525  Training Loss  0.046864353120326996\n",
            "Epoch  10 Batch  153 / 525  Training Loss  0.058598704636096954\n",
            "Epoch  10 Batch  154 / 525  Training Loss  0.049447692930698395\n",
            "Epoch  10 Batch  155 / 525  Training Loss  0.043904662132263184\n",
            "Epoch  10 Batch  156 / 525  Training Loss  0.052853506058454514\n",
            "Epoch  10 Batch  157 / 525  Training Loss  0.05855804681777954\n",
            "Epoch  10 Batch  158 / 525  Training Loss  0.06789284199476242\n",
            "Epoch  10 Batch  159 / 525  Training Loss  0.0483674630522728\n",
            "Epoch  10 Batch  160 / 525  Training Loss  0.059416670352220535\n",
            "Epoch  10 Batch  161 / 525  Training Loss  0.045037876814603806\n",
            "Epoch  10 Batch  162 / 525  Training Loss  0.04236605018377304\n",
            "Epoch  10 Batch  163 / 525  Training Loss  0.054988063871860504\n",
            "Epoch  10 Batch  164 / 525  Training Loss  0.054107893258333206\n",
            "Epoch  10 Batch  165 / 525  Training Loss  0.04994814470410347\n",
            "Epoch  10 Batch  166 / 525  Training Loss  0.06499119848012924\n",
            "Epoch  10 Batch  167 / 525  Training Loss  0.07277090102434158\n",
            "Epoch  10 Batch  168 / 525  Training Loss  0.05711844563484192\n",
            "Epoch  10 Batch  169 / 525  Training Loss  0.05029463768005371\n",
            "Epoch  10 Batch  170 / 525  Training Loss  0.0357523038983345\n",
            "Epoch  10 Batch  171 / 525  Training Loss  0.033386215567588806\n",
            "Epoch  10 Batch  172 / 525  Training Loss  0.040590982884168625\n",
            "Epoch  10 Batch  173 / 525  Training Loss  0.060346342623233795\n",
            "Epoch  10 Batch  174 / 525  Training Loss  0.05614323168992996\n",
            "Epoch  10 Batch  175 / 525  Training Loss  0.05022571608424187\n",
            "Epoch  10 Batch  176 / 525  Training Loss  0.041890330612659454\n",
            "Epoch  10 Batch  177 / 525  Training Loss  0.057658545672893524\n",
            "Epoch  10 Batch  178 / 525  Training Loss  0.04823516309261322\n",
            "Epoch  10 Batch  179 / 525  Training Loss  0.04417228698730469\n",
            "Epoch  10 Batch  180 / 525  Training Loss  0.05588715523481369\n",
            "Epoch  10 Batch  181 / 525  Training Loss  0.037110067903995514\n",
            "Epoch  10 Batch  182 / 525  Training Loss  0.05358125641942024\n",
            "Epoch  10 Batch  183 / 525  Training Loss  0.03754350543022156\n",
            "Epoch  10 Batch  184 / 525  Training Loss  0.057243842631578445\n",
            "Epoch  10 Batch  185 / 525  Training Loss  0.03835911676287651\n",
            "Epoch  10 Batch  186 / 525  Training Loss  0.04954356700181961\n",
            "Epoch  10 Batch  187 / 525  Training Loss  0.07361404597759247\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  10 Batch  188 / 525  Training Loss  0.05075199156999588\n",
            "Epoch  10 Batch  189 / 525  Training Loss  0.06603727489709854\n",
            "Epoch  10 Batch  190 / 525  Training Loss  0.06427565217018127\n",
            "Epoch  10 Batch  191 / 525  Training Loss  0.055672794580459595\n",
            "Epoch  10 Batch  192 / 525  Training Loss  0.05726957321166992\n",
            "Epoch  10 Batch  193 / 525  Training Loss  0.04291379451751709\n",
            "Epoch  10 Batch  194 / 525  Training Loss  0.0388994999229908\n",
            "Epoch  10 Batch  195 / 525  Training Loss  0.03366748243570328\n",
            "Epoch  10 Batch  196 / 525  Training Loss  0.04576893523335457\n",
            "Epoch  10 Batch  197 / 525  Training Loss  0.05758591741323471\n",
            "Epoch  10 Batch  198 / 525  Training Loss  0.03228394687175751\n",
            "Epoch  10 Batch  199 / 525  Training Loss  0.0596817247569561\n",
            "Epoch  10 Batch  200 / 525  Training Loss  0.052337467670440674\n",
            "Epoch  10 Batch  201 / 525  Training Loss  0.04990527033805847\n",
            "Epoch  10 Batch  202 / 525  Training Loss  0.05033645033836365\n",
            "Epoch  10 Batch  203 / 525  Training Loss  0.04872361943125725\n",
            "Epoch  10 Batch  204 / 525  Training Loss  0.06254693865776062\n",
            "Epoch  10 Batch  205 / 525  Training Loss  0.06060654670000076\n",
            "Epoch  10 Batch  206 / 525  Training Loss  0.05458592623472214\n",
            "Epoch  10 Batch  207 / 525  Training Loss  0.05894668772816658\n",
            "Epoch  10 Batch  208 / 525  Training Loss  0.0544792003929615\n",
            "Epoch  10 Batch  209 / 525  Training Loss  0.04356331378221512\n",
            "Epoch  10 Batch  210 / 525  Training Loss  0.0757499411702156\n",
            "Epoch  10 Batch  211 / 525  Training Loss  0.040886811912059784\n",
            "Epoch  10 Batch  212 / 525  Training Loss  0.0598890483379364\n",
            "Epoch  10 Batch  213 / 525  Training Loss  0.04432079195976257\n",
            "Epoch  10 Batch  214 / 525  Training Loss  0.057398032397031784\n",
            "Epoch  10 Batch  215 / 525  Training Loss  0.06340043991804123\n",
            "Epoch  10 Batch  216 / 525  Training Loss  0.053650714457035065\n",
            "Epoch  10 Batch  217 / 525  Training Loss  0.06629206240177155\n",
            "Epoch  10 Batch  218 / 525  Training Loss  0.0576760396361351\n",
            "Epoch  10 Batch  219 / 525  Training Loss  0.050180792808532715\n",
            "Epoch  10 Batch  220 / 525  Training Loss  0.045083969831466675\n",
            "Epoch  10 Batch  221 / 525  Training Loss  0.06187180429697037\n",
            "Epoch  10 Batch  222 / 525  Training Loss  0.04647291079163551\n",
            "Epoch  10 Batch  223 / 525  Training Loss  0.04801769182085991\n",
            "Epoch  10 Batch  224 / 525  Training Loss  0.04146401584148407\n",
            "Epoch  10 Batch  225 / 525  Training Loss  0.041099123656749725\n",
            "Epoch  10 Batch  226 / 525  Training Loss  0.035987235605716705\n",
            "Epoch  10 Batch  227 / 525  Training Loss  0.04022333398461342\n",
            "Epoch  10 Batch  228 / 525  Training Loss  0.05881313234567642\n",
            "Epoch  10 Batch  229 / 525  Training Loss  0.04507356137037277\n",
            "Epoch  10 Batch  230 / 525  Training Loss  0.04618707299232483\n",
            "Epoch  10 Batch  231 / 525  Training Loss  0.03988002613186836\n",
            "Epoch  10 Batch  232 / 525  Training Loss  0.03307672217488289\n",
            "Epoch  10 Batch  233 / 525  Training Loss  0.06217627599835396\n",
            "Epoch  10 Batch  234 / 525  Training Loss  0.07028092443943024\n",
            "Epoch  10 Batch  235 / 525  Training Loss  0.044726066291332245\n",
            "Epoch  10 Batch  236 / 525  Training Loss  0.04946797341108322\n",
            "Epoch  10 Batch  237 / 525  Training Loss  0.06527922302484512\n",
            "Epoch  10 Batch  238 / 525  Training Loss  0.05620212480425835\n",
            "Epoch  10 Batch  239 / 525  Training Loss  0.05836411193013191\n",
            "Epoch  10 Batch  240 / 525  Training Loss  0.03275780379772186\n",
            "Epoch  10 Batch  241 / 525  Training Loss  0.0623960867524147\n",
            "Epoch  10 Batch  242 / 525  Training Loss  0.045577362179756165\n",
            "Epoch  10 Batch  243 / 525  Training Loss  0.05682476609945297\n",
            "Epoch  10 Batch  244 / 525  Training Loss  0.049483705312013626\n",
            "Epoch  10 Batch  245 / 525  Training Loss  0.06357620656490326\n",
            "Epoch  10 Batch  246 / 525  Training Loss  0.0465383306145668\n",
            "Epoch  10 Batch  247 / 525  Training Loss  0.0729893371462822\n",
            "Epoch  10 Batch  248 / 525  Training Loss  0.04512019827961922\n",
            "Epoch  10 Batch  249 / 525  Training Loss  0.07383933663368225\n",
            "Epoch  10 Batch  250 / 525  Training Loss  0.05022284388542175\n",
            "Epoch  10 Batch  251 / 525  Training Loss  0.05404467135667801\n",
            "Epoch  10 Batch  252 / 525  Training Loss  0.06014205142855644\n",
            "Epoch  10 Batch  253 / 525  Training Loss  0.0404064878821373\n",
            "Epoch  10 Batch  254 / 525  Training Loss  0.0545319989323616\n",
            "Epoch  10 Batch  255 / 525  Training Loss  0.05525880306959152\n",
            "Epoch  10 Batch  256 / 525  Training Loss  0.054256219416856766\n",
            "Epoch  10 Batch  257 / 525  Training Loss  0.04723336547613144\n",
            "Epoch  10 Batch  258 / 525  Training Loss  0.03931697830557823\n",
            "Epoch  10 Batch  259 / 525  Training Loss  0.040216635912656784\n",
            "Epoch  10 Batch  260 / 525  Training Loss  0.04704786092042923\n",
            "Epoch  10 Batch  261 / 525  Training Loss  0.05225290730595589\n",
            "Epoch  10 Batch  262 / 525  Training Loss  0.06216645985841751\n",
            "Epoch  10 Batch  263 / 525  Training Loss  0.058423034846782684\n",
            "Epoch  10 Batch  264 / 525  Training Loss  0.04682965204119682\n",
            "Epoch  10 Batch  265 / 525  Training Loss  0.05272597074508667\n",
            "Epoch  10 Batch  266 / 525  Training Loss  0.04983107000589371\n",
            "Epoch  10 Batch  267 / 525  Training Loss  0.07067921757698059\n",
            "Epoch  10 Batch  268 / 525  Training Loss  0.0746345967054367\n",
            "Epoch  10 Batch  269 / 525  Training Loss  0.07305888831615448\n",
            "Epoch  10 Batch  270 / 525  Training Loss  0.04250643774867058\n",
            "Epoch  10 Batch  271 / 525  Training Loss  0.055777061730623245\n",
            "Epoch  10 Batch  272 / 525  Training Loss  0.05396019294857979\n",
            "Epoch  10 Batch  273 / 525  Training Loss  0.06265904009342194\n",
            "Epoch  10 Batch  274 / 525  Training Loss  0.05772214010357857\n",
            "Epoch  10 Batch  275 / 525  Training Loss  0.041367270052433014\n",
            "Epoch  10 Batch  276 / 525  Training Loss  0.0381179004907608\n",
            "Epoch  10 Batch  277 / 525  Training Loss  0.05850418284535408\n",
            "Epoch  10 Batch  278 / 525  Training Loss  0.03969342261552811\n",
            "Epoch  10 Batch  279 / 525  Training Loss  0.04016932472586632\n",
            "Epoch  10 Batch  280 / 525  Training Loss  0.06347457319498062\n",
            "Epoch  10 Batch  281 / 525  Training Loss  0.05310659855604172\n",
            "Epoch  10 Batch  282 / 525  Training Loss  0.03878173604607582\n",
            "Epoch  10 Batch  283 / 525  Training Loss  0.038727499544620514\n",
            "Epoch  10 Batch  284 / 525  Training Loss  0.03781463950872421\n",
            "Epoch  10 Batch  285 / 525  Training Loss  0.036804817616939545\n",
            "Epoch  10 Batch  286 / 525  Training Loss  0.04873886704444885\n",
            "Epoch  10 Batch  287 / 525  Training Loss  0.04704175144433975\n",
            "Epoch  10 Batch  288 / 525  Training Loss  0.05105823278427124\n",
            "Epoch  10 Batch  289 / 525  Training Loss  0.047667138278484344\n",
            "Epoch  10 Batch  290 / 525  Training Loss  0.05904589965939522\n",
            "Epoch  10 Batch  291 / 525  Training Loss  0.048372089862823486\n",
            "Epoch  10 Batch  292 / 525  Training Loss  0.05599762871861458\n",
            "Epoch  10 Batch  293 / 525  Training Loss  0.04818452149629593\n",
            "Epoch  10 Batch  294 / 525  Training Loss  0.0537635013461113\n",
            "Epoch  10 Batch  295 / 525  Training Loss  0.04961466044187546\n",
            "Epoch  10 Batch  296 / 525  Training Loss  0.04848429560661316\n",
            "Epoch  10 Batch  297 / 525  Training Loss  0.058012206107378006\n",
            "Epoch  10 Batch  298 / 525  Training Loss  0.04372686892747879\n",
            "Epoch  10 Batch  299 / 525  Training Loss  0.07275860011577606\n",
            "Epoch  10 Batch  300 / 525  Training Loss  0.05198255926370621\n",
            "Epoch  10 Batch  301 / 525  Training Loss  0.0521903932094574\n",
            "Epoch  10 Batch  302 / 525  Training Loss  0.04432648792862892\n",
            "Epoch  10 Batch  303 / 525  Training Loss  0.04556642100214958\n",
            "Epoch  10 Batch  304 / 525  Training Loss  0.0490049347281456\n",
            "Epoch  10 Batch  305 / 525  Training Loss  0.03565707057714462\n",
            "Epoch  10 Batch  306 / 525  Training Loss  0.06690647453069687\n",
            "Epoch  10 Batch  307 / 525  Training Loss  0.059721000492572784\n",
            "Epoch  10 Batch  308 / 525  Training Loss  0.056110598146915436\n",
            "Epoch  10 Batch  309 / 525  Training Loss  0.06603436172008514\n",
            "Epoch  10 Batch  310 / 525  Training Loss  0.05116959661245346\n",
            "Epoch  10 Batch  311 / 525  Training Loss  0.051231659948825836\n",
            "Epoch  10 Batch  312 / 525  Training Loss  0.047069743275642395\n",
            "Epoch  10 Batch  313 / 525  Training Loss  0.05995047837495804\n",
            "Epoch  10 Batch  314 / 525  Training Loss  0.05155961960554123\n",
            "Epoch  10 Batch  315 / 525  Training Loss  0.0558154359459877\n",
            "Epoch  10 Batch  316 / 525  Training Loss  0.04248368740081787\n",
            "Epoch  10 Batch  317 / 525  Training Loss  0.06578055024147034\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  10 Batch  318 / 525  Training Loss  0.07165361940860748\n",
            "Epoch  10 Batch  319 / 525  Training Loss  0.04150044545531273\n",
            "Epoch  10 Batch  320 / 525  Training Loss  0.05095608904957771\n",
            "Epoch  10 Batch  321 / 525  Training Loss  0.04407893121242523\n",
            "Epoch  10 Batch  322 / 525  Training Loss  0.0536024272441864\n",
            "Epoch  10 Batch  323 / 525  Training Loss  0.06376398354768753\n",
            "Epoch  10 Batch  324 / 525  Training Loss  0.05210813879966736\n",
            "Epoch  10 Batch  325 / 525  Training Loss  0.05334718897938728\n",
            "Epoch  10 Batch  326 / 525  Training Loss  0.04659734666347504\n",
            "Epoch  10 Batch  327 / 525  Training Loss  0.06269117444753647\n",
            "Epoch  10 Batch  328 / 525  Training Loss  0.042807530611753464\n",
            "Epoch  10 Batch  329 / 525  Training Loss  0.04951096326112747\n",
            "Epoch  10 Batch  330 / 525  Training Loss  0.052865706384181976\n",
            "Epoch  10 Batch  331 / 525  Training Loss  0.0462816022336483\n",
            "Epoch  10 Batch  332 / 525  Training Loss  0.04532182589173317\n",
            "Epoch  10 Batch  333 / 525  Training Loss  0.07220941036939621\n",
            "Epoch  10 Batch  334 / 525  Training Loss  0.04889392480254173\n",
            "Epoch  10 Batch  335 / 525  Training Loss  0.07316935807466507\n",
            "Epoch  10 Batch  336 / 525  Training Loss  0.05714816972613335\n",
            "Epoch  10 Batch  337 / 525  Training Loss  0.05188925191760063\n",
            "Epoch  10 Batch  338 / 525  Training Loss  0.06467003375291824\n",
            "Epoch  10 Batch  339 / 525  Training Loss  0.042054034769535065\n",
            "Epoch  10 Batch  340 / 525  Training Loss  0.0593327172100544\n",
            "Epoch  10 Batch  341 / 525  Training Loss  0.03886069357395172\n",
            "Epoch  10 Batch  342 / 525  Training Loss  0.04447626322507858\n",
            "Epoch  10 Batch  343 / 525  Training Loss  0.04292590916156769\n",
            "Epoch  10 Batch  344 / 525  Training Loss  0.05726444721221924\n",
            "Epoch  10 Batch  345 / 525  Training Loss  0.045872222632169724\n",
            "Epoch  10 Batch  346 / 525  Training Loss  0.04428154230117798\n",
            "Epoch  10 Batch  347 / 525  Training Loss  0.04351093992590904\n",
            "Epoch  10 Batch  348 / 525  Training Loss  0.05010274052619934\n",
            "Epoch  10 Batch  349 / 525  Training Loss  0.05216619372367859\n",
            "Epoch  10 Batch  350 / 525  Training Loss  0.05985991284251213\n",
            "Epoch  10 Batch  351 / 525  Training Loss  0.05070497468113899\n",
            "Epoch  10 Batch  352 / 525  Training Loss  0.05615811422467232\n",
            "Epoch  10 Batch  353 / 525  Training Loss  0.05209546163678169\n",
            "Epoch  10 Batch  354 / 525  Training Loss  0.05897447466850281\n",
            "Epoch  10 Batch  355 / 525  Training Loss  0.0693972259759903\n",
            "Epoch  10 Batch  356 / 525  Training Loss  0.06406330317258835\n",
            "Epoch  10 Batch  357 / 525  Training Loss  0.0634518563747406\n",
            "Epoch  10 Batch  358 / 525  Training Loss  0.05774545669555664\n",
            "Epoch  10 Batch  359 / 525  Training Loss  0.07332050055265427\n",
            "Epoch  10 Batch  360 / 525  Training Loss  0.06754352897405624\n",
            "Epoch  10 Batch  361 / 525  Training Loss  0.044357843697071075\n",
            "Epoch  10 Batch  362 / 525  Training Loss  0.06631125509738922\n",
            "Epoch  10 Batch  363 / 525  Training Loss  0.04519657418131828\n",
            "Epoch  10 Batch  364 / 525  Training Loss  0.034582674503326416\n",
            "Epoch  10 Batch  365 / 525  Training Loss  0.04315399378538132\n",
            "Epoch  10 Batch  366 / 525  Training Loss  0.043419115245342255\n",
            "Epoch  10 Batch  367 / 525  Training Loss  0.060761094093322754\n",
            "Epoch  10 Batch  368 / 525  Training Loss  0.06359279900789261\n",
            "Epoch  10 Batch  369 / 525  Training Loss  0.040454111993312836\n",
            "Epoch  10 Batch  370 / 525  Training Loss  0.05712636560201645\n",
            "Epoch  10 Batch  371 / 525  Training Loss  0.062074411660432816\n",
            "Epoch  10 Batch  372 / 525  Training Loss  0.03402199223637581\n",
            "Epoch  10 Batch  373 / 525  Training Loss  0.05338660627603531\n",
            "Epoch  10 Batch  374 / 525  Training Loss  0.07190598547458649\n",
            "Epoch  10 Batch  375 / 525  Training Loss  0.07056344300508499\n",
            "Epoch  10 Batch  376 / 525  Training Loss  0.05001574009656906\n",
            "Epoch  10 Batch  377 / 525  Training Loss  0.037975139915943146\n",
            "Epoch  10 Batch  378 / 525  Training Loss  0.06106388568878174\n",
            "Epoch  10 Batch  379 / 525  Training Loss  0.04406141862273216\n",
            "Epoch  10 Batch  380 / 525  Training Loss  0.04407244175672531\n",
            "Epoch  10 Batch  381 / 525  Training Loss  0.0376012846827507\n",
            "Epoch  10 Batch  382 / 525  Training Loss  0.04974807798862457\n",
            "Epoch  10 Batch  383 / 525  Training Loss  0.039748623967170715\n",
            "Epoch  10 Batch  384 / 525  Training Loss  0.06462869048118591\n",
            "Epoch  10 Batch  385 / 525  Training Loss  0.04232782498002052\n",
            "Epoch  10 Batch  386 / 525  Training Loss  0.05110520124435425\n",
            "Epoch  10 Batch  387 / 525  Training Loss  0.042886681854724884\n",
            "Epoch  10 Batch  388 / 525  Training Loss  0.0583098903298378\n",
            "Epoch  10 Batch  389 / 525  Training Loss  0.04364684969186783\n",
            "Epoch  10 Batch  390 / 525  Training Loss  0.04271528869867325\n",
            "Epoch  10 Batch  391 / 525  Training Loss  0.04238767549395561\n",
            "Epoch  10 Batch  392 / 525  Training Loss  0.04622132331132889\n",
            "Epoch  10 Batch  393 / 525  Training Loss  0.04729374870657921\n",
            "Epoch  10 Batch  394 / 525  Training Loss  0.05337602645158768\n",
            "Epoch  10 Batch  395 / 525  Training Loss  0.03891520947217941\n",
            "Epoch  10 Batch  396 / 525  Training Loss  0.044503867626190186\n",
            "Epoch  10 Batch  397 / 525  Training Loss  0.04482085630297661\n",
            "Epoch  10 Batch  398 / 525  Training Loss  0.05109144374728203\n",
            "Epoch  10 Batch  399 / 525  Training Loss  0.04149339348077774\n",
            "Epoch  10 Batch  400 / 525  Training Loss  0.04519011080265045\n",
            "Epoch  10 Batch  401 / 525  Training Loss  0.06462924927473068\n",
            "Epoch  10 Batch  402 / 525  Training Loss  0.053160227835178375\n",
            "Epoch  10 Batch  403 / 525  Training Loss  0.03403455764055252\n",
            "Epoch  10 Batch  404 / 525  Training Loss  0.0370265431702137\n",
            "Epoch  10 Batch  405 / 525  Training Loss  0.06603388488292694\n",
            "Epoch  10 Batch  406 / 525  Training Loss  0.06096748262643814\n",
            "Epoch  10 Batch  407 / 525  Training Loss  0.061254240572452545\n",
            "Epoch  10 Batch  408 / 525  Training Loss  0.05460622161626816\n",
            "Epoch  10 Batch  409 / 525  Training Loss  0.05439590662717819\n",
            "Epoch  10 Batch  410 / 525  Training Loss  0.06410743296146393\n",
            "Epoch  10 Batch  411 / 525  Training Loss  0.04480772837996483\n",
            "Epoch  10 Batch  412 / 525  Training Loss  0.052404336631298065\n",
            "Epoch  10 Batch  413 / 525  Training Loss  0.059168655425310135\n",
            "Epoch  10 Batch  414 / 525  Training Loss  0.05209597945213318\n",
            "Epoch  10 Batch  415 / 525  Training Loss  0.038830168545246124\n",
            "Epoch  10 Batch  416 / 525  Training Loss  0.07024593651294708\n",
            "Epoch  10 Batch  417 / 525  Training Loss  0.0504017099738121\n",
            "Epoch  10 Batch  418 / 525  Training Loss  0.05399100109934807\n",
            "Epoch  10 Batch  419 / 525  Training Loss  0.05455104634165764\n",
            "Epoch  10 Batch  420 / 525  Training Loss  0.05107919126749039\n",
            "Epoch  10 Batch  421 / 525  Training Loss  0.0658772736787796\n",
            "Epoch  10 Batch  422 / 525  Training Loss  0.04712046682834625\n",
            "Epoch  10 Batch  423 / 525  Training Loss  0.04616114869713783\n",
            "Epoch  10 Batch  424 / 525  Training Loss  0.049181707203388214\n",
            "Epoch  10 Batch  425 / 525  Training Loss  0.0641433522105217\n",
            "Epoch  10 Batch  426 / 525  Training Loss  0.04757947474718094\n",
            "Epoch  10 Batch  427 / 525  Training Loss  0.04064067453145981\n",
            "Epoch  10 Batch  428 / 525  Training Loss  0.06673857569694519\n",
            "Epoch  10 Batch  429 / 525  Training Loss  0.07149706780910492\n",
            "Epoch  10 Batch  430 / 525  Training Loss  0.051264744251966476\n",
            "Epoch  10 Batch  431 / 525  Training Loss  0.05299313738942146\n",
            "Epoch  10 Batch  432 / 525  Training Loss  0.05492739751935005\n",
            "Epoch  10 Batch  433 / 525  Training Loss  0.05865911766886711\n",
            "Epoch  10 Batch  434 / 525  Training Loss  0.060248859226703644\n",
            "Epoch  10 Batch  435 / 525  Training Loss  0.05150686576962471\n",
            "Epoch  10 Batch  436 / 525  Training Loss  0.04410431534051895\n",
            "Epoch  10 Batch  437 / 525  Training Loss  0.05741449445486069\n",
            "Epoch  10 Batch  438 / 525  Training Loss  0.043364960700273514\n",
            "Epoch  10 Batch  439 / 525  Training Loss  0.04877489060163498\n",
            "Epoch  10 Batch  440 / 525  Training Loss  0.05474141240119934\n",
            "Epoch  10 Batch  441 / 525  Training Loss  0.054545193910598755\n",
            "Epoch  10 Batch  442 / 525  Training Loss  0.059733450412750244\n",
            "Epoch  10 Batch  443 / 525  Training Loss  0.055158983916044235\n",
            "Epoch  10 Batch  444 / 525  Training Loss  0.06579551845788956\n",
            "Epoch  10 Batch  445 / 525  Training Loss  0.0633019283413887\n",
            "Epoch  10 Batch  446 / 525  Training Loss  0.05104643106460571\n",
            "Epoch  10 Batch  447 / 525  Training Loss  0.03752643242478371\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  10 Batch  448 / 525  Training Loss  0.03630083054304123\n",
            "Epoch  10 Batch  449 / 525  Training Loss  0.05575614050030708\n",
            "Epoch  10 Batch  450 / 525  Training Loss  0.043954845517873764\n",
            "Epoch  10 Batch  451 / 525  Training Loss  0.05313124135136604\n",
            "Epoch  10 Batch  452 / 525  Training Loss  0.056343965232372284\n",
            "Epoch  10 Batch  453 / 525  Training Loss  0.06406110525131226\n",
            "Epoch  10 Batch  454 / 525  Training Loss  0.052104122936725616\n",
            "Epoch  10 Batch  455 / 525  Training Loss  0.046871766448020935\n",
            "Epoch  10 Batch  456 / 525  Training Loss  0.05880361050367355\n",
            "Epoch  10 Batch  457 / 525  Training Loss  0.05609018728137016\n",
            "Epoch  10 Batch  458 / 525  Training Loss  0.05952416732907295\n",
            "Epoch  10 Batch  459 / 525  Training Loss  0.047348689287900925\n",
            "Epoch  10 Batch  460 / 525  Training Loss  0.053683310747146606\n",
            "Epoch  10 Batch  461 / 525  Training Loss  0.05963513255119324\n",
            "Epoch  10 Batch  462 / 525  Training Loss  0.05517522618174553\n",
            "Epoch  10 Batch  463 / 525  Training Loss  0.07142740488052368\n",
            "Epoch  10 Batch  464 / 525  Training Loss  0.05181838199496269\n",
            "Epoch  10 Batch  465 / 525  Training Loss  0.06080011650919914\n",
            "Epoch  10 Batch  466 / 525  Training Loss  0.05228464677929878\n",
            "Epoch  10 Batch  467 / 525  Training Loss  0.06899980455636978\n",
            "Epoch  10 Batch  468 / 525  Training Loss  0.05171986669301987\n",
            "Epoch  10 Batch  469 / 525  Training Loss  0.05142435431480408\n",
            "Epoch  10 Batch  470 / 525  Training Loss  0.05013083666563034\n",
            "Epoch  10 Batch  471 / 525  Training Loss  0.04409334808588028\n",
            "Epoch  10 Batch  472 / 525  Training Loss  0.04275042563676834\n",
            "Epoch  10 Batch  473 / 525  Training Loss  0.04446423053741455\n",
            "Epoch  10 Batch  474 / 525  Training Loss  0.042618442326784134\n",
            "Epoch  10 Batch  475 / 525  Training Loss  0.060945551842451096\n",
            "Epoch  10 Batch  476 / 525  Training Loss  0.048198092728853226\n",
            "Epoch  10 Batch  477 / 525  Training Loss  0.04923788830637932\n",
            "Epoch  10 Batch  478 / 525  Training Loss  0.05716560408473015\n",
            "Epoch  10 Batch  479 / 525  Training Loss  0.059841256588697433\n",
            "Epoch  10 Batch  480 / 525  Training Loss  0.041389960795640945\n",
            "Epoch  10 Batch  481 / 525  Training Loss  0.04942403361201286\n",
            "Epoch  10 Batch  482 / 525  Training Loss  0.06269523501396179\n",
            "Epoch  10 Batch  483 / 525  Training Loss  0.045892372727394104\n",
            "Epoch  10 Batch  484 / 525  Training Loss  0.05371718481183052\n",
            "Epoch  10 Batch  485 / 525  Training Loss  0.04982360452413559\n",
            "Epoch  10 Batch  486 / 525  Training Loss  0.04378143325448036\n",
            "Epoch  10 Batch  487 / 525  Training Loss  0.05519842356443405\n",
            "Epoch  10 Batch  488 / 525  Training Loss  0.06019023060798645\n",
            "Epoch  10 Batch  489 / 525  Training Loss  0.0479140467941761\n",
            "Epoch  10 Batch  490 / 525  Training Loss  0.05587267875671387\n",
            "Epoch  10 Batch  491 / 525  Training Loss  0.05203891918063164\n",
            "Epoch  10 Batch  492 / 525  Training Loss  0.07567363232374191\n",
            "Epoch  10 Batch  493 / 525  Training Loss  0.06098899990320206\n",
            "Epoch  10 Batch  494 / 525  Training Loss  0.051772356033325195\n",
            "Epoch  10 Batch  495 / 525  Training Loss  0.059864990413188934\n",
            "Epoch  10 Batch  496 / 525  Training Loss  0.04796205461025238\n",
            "Epoch  10 Batch  497 / 525  Training Loss  0.056104887276887894\n",
            "Epoch  10 Batch  498 / 525  Training Loss  0.057744115591049194\n",
            "Epoch  10 Batch  499 / 525  Training Loss  0.0461643822491169\n",
            "Epoch  10 Batch  500 / 525  Training Loss  0.04303688183426857\n",
            "Epoch  10 Batch  501 / 525  Training Loss  0.06998525559902191\n",
            "Epoch  10 Batch  502 / 525  Training Loss  0.05500148609280586\n",
            "Epoch  10 Batch  503 / 525  Training Loss  0.06218883395195007\n",
            "Epoch  10 Batch  504 / 525  Training Loss  0.045369744300842285\n",
            "Epoch  10 Batch  505 / 525  Training Loss  0.059674061834812164\n",
            "Epoch  10 Batch  506 / 525  Training Loss  0.05365315079689026\n",
            "Epoch  10 Batch  507 / 525  Training Loss  0.05231146886944771\n",
            "Epoch  10 Batch  508 / 525  Training Loss  0.056093256920576096\n",
            "Epoch  10 Batch  509 / 525  Training Loss  0.05607719346880913\n",
            "Epoch  10 Batch  510 / 525  Training Loss  0.042675044387578964\n",
            "Epoch  10 Batch  511 / 525  Training Loss  0.049518387764692307\n",
            "Epoch  10 Batch  512 / 525  Training Loss  0.06654585897922516\n",
            "Epoch  10 Batch  513 / 525  Training Loss  0.061756569892168045\n",
            "Epoch  10 Batch  514 / 525  Training Loss  0.054158855229616165\n",
            "Epoch  10 Batch  515 / 525  Training Loss  0.03187605366110802\n",
            "Epoch  10 Batch  516 / 525  Training Loss  0.06055184081196785\n",
            "Epoch  10 Batch  517 / 525  Training Loss  0.052679985761642456\n",
            "Epoch  10 Batch  518 / 525  Training Loss  0.06608988344669342\n",
            "Epoch  10 Batch  519 / 525  Training Loss  0.05461110919713974\n",
            "Epoch  10 Batch  520 / 525  Training Loss  0.035814136266708374\n",
            "Epoch  10 Batch  521 / 525  Training Loss  0.051161687821149826\n",
            "Epoch  10 Batch  522 / 525  Training Loss  0.06587155908346176\n",
            "Epoch  10 Batch  523 / 525  Training Loss  0.058855555951595306\n",
            "Epoch  10 Batch  524 / 525  Training Loss  0.03597085177898407\n",
            "  11    |    -    |   0.051951   | 47.825000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 11\n",
            "Epoch  11 Batch  0 / 525  Training Loss  0.05130673572421074\n",
            "Epoch  11 Batch  1 / 525  Training Loss  0.04222024232149124\n",
            "Epoch  11 Batch  2 / 525  Training Loss  0.05769013613462448\n",
            "Epoch  11 Batch  3 / 525  Training Loss  0.048561424016952515\n",
            "Epoch  11 Batch  4 / 525  Training Loss  0.055653661489486694\n",
            "Epoch  11 Batch  5 / 525  Training Loss  0.05162721127271652\n",
            "Epoch  11 Batch  6 / 525  Training Loss  0.03443170338869095\n",
            "Epoch  11 Batch  7 / 525  Training Loss  0.042788490653038025\n",
            "Epoch  11 Batch  8 / 525  Training Loss  0.032296665012836456\n",
            "Epoch  11 Batch  9 / 525  Training Loss  0.046648189425468445\n",
            "Epoch  11 Batch  10 / 525  Training Loss  0.04187744855880737\n",
            "Epoch  11 Batch  11 / 525  Training Loss  0.05428576469421387\n",
            "Epoch  11 Batch  12 / 525  Training Loss  0.048804573714733124\n",
            "Epoch  11 Batch  13 / 525  Training Loss  0.03934719040989876\n",
            "Epoch  11 Batch  14 / 525  Training Loss  0.04525668919086456\n",
            "Epoch  11 Batch  15 / 525  Training Loss  0.0492096021771431\n",
            "Epoch  11 Batch  16 / 525  Training Loss  0.043852441012859344\n",
            "Epoch  11 Batch  17 / 525  Training Loss  0.04184894263744354\n",
            "Epoch  11 Batch  18 / 525  Training Loss  0.03700734302401543\n",
            "Epoch  11 Batch  19 / 525  Training Loss  0.033189598470926285\n",
            "Epoch  11 Batch  20 / 525  Training Loss  0.06157469004392624\n",
            "Epoch  11 Batch  21 / 525  Training Loss  0.04596362262964249\n",
            "Epoch  11 Batch  22 / 525  Training Loss  0.04102813079953194\n",
            "Epoch  11 Batch  23 / 525  Training Loss  0.03640297055244446\n",
            "Epoch  11 Batch  24 / 525  Training Loss  0.03277924656867981\n",
            "Epoch  11 Batch  25 / 525  Training Loss  0.047859806567430496\n",
            "Epoch  11 Batch  26 / 525  Training Loss  0.032211463898420334\n",
            "Epoch  11 Batch  27 / 525  Training Loss  0.02605920471251011\n",
            "Epoch  11 Batch  28 / 525  Training Loss  0.049486733973026276\n",
            "Epoch  11 Batch  29 / 525  Training Loss  0.0628744512796402\n",
            "Epoch  11 Batch  30 / 525  Training Loss  0.04396230727434158\n",
            "Epoch  11 Batch  31 / 525  Training Loss  0.028857767581939697\n",
            "Epoch  11 Batch  32 / 525  Training Loss  0.03663865476846695\n",
            "Epoch  11 Batch  33 / 525  Training Loss  0.03340853378176689\n",
            "Epoch  11 Batch  34 / 525  Training Loss  0.04254252091050148\n",
            "Epoch  11 Batch  35 / 525  Training Loss  0.04118701443076134\n",
            "Epoch  11 Batch  36 / 525  Training Loss  0.048012860119342804\n",
            "Epoch  11 Batch  37 / 525  Training Loss  0.031791478395462036\n",
            "Epoch  11 Batch  38 / 525  Training Loss  0.037113457918167114\n",
            "Epoch  11 Batch  39 / 525  Training Loss  0.03236410394310951\n",
            "Epoch  11 Batch  40 / 525  Training Loss  0.031686075031757355\n",
            "Epoch  11 Batch  41 / 525  Training Loss  0.04175097495317459\n",
            "Epoch  11 Batch  42 / 525  Training Loss  0.04943837970495224\n",
            "Epoch  11 Batch  43 / 525  Training Loss  0.04182019829750061\n",
            "Epoch  11 Batch  44 / 525  Training Loss  0.050129182636737823\n",
            "Epoch  11 Batch  45 / 525  Training Loss  0.03804191201925278\n",
            "Epoch  11 Batch  46 / 525  Training Loss  0.051791198551654816\n",
            "Epoch  11 Batch  47 / 525  Training Loss  0.04501195624470711\n",
            "Epoch  11 Batch  48 / 525  Training Loss  0.0531047098338604\n",
            "Epoch  11 Batch  49 / 525  Training Loss  0.04990529641509056\n",
            "Epoch  11 Batch  50 / 525  Training Loss  0.0381590761244297\n",
            "Epoch  11 Batch  51 / 525  Training Loss  0.04254092276096344\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  11 Batch  52 / 525  Training Loss  0.05120253562927246\n",
            "Epoch  11 Batch  53 / 525  Training Loss  0.04412315785884857\n",
            "Epoch  11 Batch  54 / 525  Training Loss  0.039471182972192764\n",
            "Epoch  11 Batch  55 / 525  Training Loss  0.03569359332323074\n",
            "Epoch  11 Batch  56 / 525  Training Loss  0.040306396782398224\n",
            "Epoch  11 Batch  57 / 525  Training Loss  0.03088558278977871\n",
            "Epoch  11 Batch  58 / 525  Training Loss  0.040354929864406586\n",
            "Epoch  11 Batch  59 / 525  Training Loss  0.042910136282444\n",
            "Epoch  11 Batch  60 / 525  Training Loss  0.029061395674943924\n",
            "Epoch  11 Batch  61 / 525  Training Loss  0.037230756133794785\n",
            "Epoch  11 Batch  62 / 525  Training Loss  0.042105548083782196\n",
            "Epoch  11 Batch  63 / 525  Training Loss  0.026691514998674393\n",
            "Epoch  11 Batch  64 / 525  Training Loss  0.03868325427174568\n",
            "Epoch  11 Batch  65 / 525  Training Loss  0.03813115879893303\n",
            "Epoch  11 Batch  66 / 525  Training Loss  0.030106117948889732\n",
            "Epoch  11 Batch  67 / 525  Training Loss  0.04117908701300621\n",
            "Epoch  11 Batch  68 / 525  Training Loss  0.03624914959073067\n",
            "Epoch  11 Batch  69 / 525  Training Loss  0.032301709055900574\n",
            "Epoch  11 Batch  70 / 525  Training Loss  0.04103358834981918\n",
            "Epoch  11 Batch  71 / 525  Training Loss  0.03287532925605774\n",
            "Epoch  11 Batch  72 / 525  Training Loss  0.04622720927000046\n",
            "Epoch  11 Batch  73 / 525  Training Loss  0.04567950591444969\n",
            "Epoch  11 Batch  74 / 525  Training Loss  0.044456180185079575\n",
            "Epoch  11 Batch  75 / 525  Training Loss  0.050487715750932693\n",
            "Epoch  11 Batch  76 / 525  Training Loss  0.030668998137116432\n",
            "Epoch  11 Batch  77 / 525  Training Loss  0.030901800841093063\n",
            "Epoch  11 Batch  78 / 525  Training Loss  0.055775780230760574\n",
            "Epoch  11 Batch  79 / 525  Training Loss  0.056318242102861404\n",
            "Epoch  11 Batch  80 / 525  Training Loss  0.0398360937833786\n",
            "Epoch  11 Batch  81 / 525  Training Loss  0.046999845653772354\n",
            "Epoch  11 Batch  82 / 525  Training Loss  0.04799891263246536\n",
            "Epoch  11 Batch  83 / 525  Training Loss  0.027890164405107498\n",
            "Epoch  11 Batch  84 / 525  Training Loss  0.0335933193564415\n",
            "Epoch  11 Batch  85 / 525  Training Loss  0.05499964952468872\n",
            "Epoch  11 Batch  86 / 525  Training Loss  0.04180667921900749\n",
            "Epoch  11 Batch  87 / 525  Training Loss  0.04675598442554474\n",
            "Epoch  11 Batch  88 / 525  Training Loss  0.04855386167764664\n",
            "Epoch  11 Batch  89 / 525  Training Loss  0.047025278210639954\n",
            "Epoch  11 Batch  90 / 525  Training Loss  0.05078968405723572\n",
            "Epoch  11 Batch  91 / 525  Training Loss  0.04120272025465965\n",
            "Epoch  11 Batch  92 / 525  Training Loss  0.03697171434760094\n",
            "Epoch  11 Batch  93 / 525  Training Loss  0.04918207600712776\n",
            "Epoch  11 Batch  94 / 525  Training Loss  0.033217448741197586\n",
            "Epoch  11 Batch  95 / 525  Training Loss  0.03418462350964546\n",
            "Epoch  11 Batch  96 / 525  Training Loss  0.03926422446966171\n",
            "Epoch  11 Batch  97 / 525  Training Loss  0.0421854630112648\n",
            "Epoch  11 Batch  98 / 525  Training Loss  0.04552378132939339\n",
            "Epoch  11 Batch  99 / 525  Training Loss  0.03655927628278732\n",
            "Epoch  11 Batch  100 / 525  Training Loss  0.05394585058093071\n",
            "Epoch  11 Batch  101 / 525  Training Loss  0.03388053923845291\n",
            "Epoch  11 Batch  102 / 525  Training Loss  0.05042470246553421\n",
            "Epoch  11 Batch  103 / 525  Training Loss  0.04835420846939087\n",
            "Epoch  11 Batch  104 / 525  Training Loss  0.06850937753915787\n",
            "Epoch  11 Batch  105 / 525  Training Loss  0.03898613527417183\n",
            "Epoch  11 Batch  106 / 525  Training Loss  0.030419429764151573\n",
            "Epoch  11 Batch  107 / 525  Training Loss  0.03136831894516945\n",
            "Epoch  11 Batch  108 / 525  Training Loss  0.04305339232087135\n",
            "Epoch  11 Batch  109 / 525  Training Loss  0.03137406334280968\n",
            "Epoch  11 Batch  110 / 525  Training Loss  0.03361635282635689\n",
            "Epoch  11 Batch  111 / 525  Training Loss  0.049506090581417084\n",
            "Epoch  11 Batch  112 / 525  Training Loss  0.0435028150677681\n",
            "Epoch  11 Batch  113 / 525  Training Loss  0.04900696128606796\n",
            "Epoch  11 Batch  114 / 525  Training Loss  0.036468036472797394\n",
            "Epoch  11 Batch  115 / 525  Training Loss  0.030665624886751175\n",
            "Epoch  11 Batch  116 / 525  Training Loss  0.03921530768275261\n",
            "Epoch  11 Batch  117 / 525  Training Loss  0.049095094203948975\n",
            "Epoch  11 Batch  118 / 525  Training Loss  0.045061077922582626\n",
            "Epoch  11 Batch  119 / 525  Training Loss  0.040758781135082245\n",
            "Epoch  11 Batch  120 / 525  Training Loss  0.041765790432691574\n",
            "Epoch  11 Batch  121 / 525  Training Loss  0.03672220930457115\n",
            "Epoch  11 Batch  122 / 525  Training Loss  0.04147247225046158\n",
            "Epoch  11 Batch  123 / 525  Training Loss  0.04878197982907295\n",
            "Epoch  11 Batch  124 / 525  Training Loss  0.03954257816076279\n",
            "Epoch  11 Batch  125 / 525  Training Loss  0.046949464827775955\n",
            "Epoch  11 Batch  126 / 525  Training Loss  0.03631145507097244\n",
            "Epoch  11 Batch  127 / 525  Training Loss  0.04498278349637985\n",
            "Epoch  11 Batch  128 / 525  Training Loss  0.04304461181163788\n",
            "Epoch  11 Batch  129 / 525  Training Loss  0.03860080987215042\n",
            "Epoch  11 Batch  130 / 525  Training Loss  0.03797189146280289\n",
            "Epoch  11 Batch  131 / 525  Training Loss  0.04132939130067825\n",
            "Epoch  11 Batch  132 / 525  Training Loss  0.04201831668615341\n",
            "Epoch  11 Batch  133 / 525  Training Loss  0.032599303871393204\n",
            "Epoch  11 Batch  134 / 525  Training Loss  0.03284379839897156\n",
            "Epoch  11 Batch  135 / 525  Training Loss  0.029151950031518936\n",
            "Epoch  11 Batch  136 / 525  Training Loss  0.04425518587231636\n",
            "Epoch  11 Batch  137 / 525  Training Loss  0.057006508111953735\n",
            "Epoch  11 Batch  138 / 525  Training Loss  0.04560251906514168\n",
            "Epoch  11 Batch  139 / 525  Training Loss  0.03427959606051445\n",
            "Epoch  11 Batch  140 / 525  Training Loss  0.055784501135349274\n",
            "Epoch  11 Batch  141 / 525  Training Loss  0.045733969658613205\n",
            "Epoch  11 Batch  142 / 525  Training Loss  0.04698450490832329\n",
            "Epoch  11 Batch  143 / 525  Training Loss  0.038623910397291183\n",
            "Epoch  11 Batch  144 / 525  Training Loss  0.052925508469343185\n",
            "Epoch  11 Batch  145 / 525  Training Loss  0.05378764867782593\n",
            "Epoch  11 Batch  146 / 525  Training Loss  0.05823798105120659\n",
            "Epoch  11 Batch  147 / 525  Training Loss  0.06428402662277222\n",
            "Epoch  11 Batch  148 / 525  Training Loss  0.03586166352033615\n",
            "Epoch  11 Batch  149 / 525  Training Loss  0.05002651736140251\n",
            "Epoch  11 Batch  150 / 525  Training Loss  0.04053216427564621\n",
            "Epoch  11 Batch  151 / 525  Training Loss  0.04110941290855408\n",
            "Epoch  11 Batch  152 / 525  Training Loss  0.05464392155408859\n",
            "Epoch  11 Batch  153 / 525  Training Loss  0.040394384413957596\n",
            "Epoch  11 Batch  154 / 525  Training Loss  0.044186923652887344\n",
            "Epoch  11 Batch  155 / 525  Training Loss  0.050105493515729904\n",
            "Epoch  11 Batch  156 / 525  Training Loss  0.04863850399851799\n",
            "Epoch  11 Batch  157 / 525  Training Loss  0.055477552115917206\n",
            "Epoch  11 Batch  158 / 525  Training Loss  0.029198363423347473\n",
            "Epoch  11 Batch  159 / 525  Training Loss  0.0333898700773716\n",
            "Epoch  11 Batch  160 / 525  Training Loss  0.045702967792749405\n",
            "Epoch  11 Batch  161 / 525  Training Loss  0.0347173735499382\n",
            "Epoch  11 Batch  162 / 525  Training Loss  0.03894899785518646\n",
            "Epoch  11 Batch  163 / 525  Training Loss  0.042529840022325516\n",
            "Epoch  11 Batch  164 / 525  Training Loss  0.040338531136512756\n",
            "Epoch  11 Batch  165 / 525  Training Loss  0.04704602435231209\n",
            "Epoch  11 Batch  166 / 525  Training Loss  0.04783345013856888\n",
            "Epoch  11 Batch  167 / 525  Training Loss  0.04169765114784241\n",
            "Epoch  11 Batch  168 / 525  Training Loss  0.056121669709682465\n",
            "Epoch  11 Batch  169 / 525  Training Loss  0.036189183592796326\n",
            "Epoch  11 Batch  170 / 525  Training Loss  0.03904765099287033\n",
            "Epoch  11 Batch  171 / 525  Training Loss  0.034919075667858124\n",
            "Epoch  11 Batch  172 / 525  Training Loss  0.044285763055086136\n",
            "Epoch  11 Batch  173 / 525  Training Loss  0.0451301671564579\n",
            "Epoch  11 Batch  174 / 525  Training Loss  0.036732159554958344\n",
            "Epoch  11 Batch  175 / 525  Training Loss  0.03298615664243698\n",
            "Epoch  11 Batch  176 / 525  Training Loss  0.03980167955160141\n",
            "Epoch  11 Batch  177 / 525  Training Loss  0.03788585960865021\n",
            "Epoch  11 Batch  178 / 525  Training Loss  0.0404895581305027\n",
            "Epoch  11 Batch  179 / 525  Training Loss  0.05928505212068558\n",
            "Epoch  11 Batch  180 / 525  Training Loss  0.05048605799674988\n",
            "Epoch  11 Batch  181 / 525  Training Loss  0.06300070136785507\n",
            "Epoch  11 Batch  182 / 525  Training Loss  0.0620393231511116\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  11 Batch  183 / 525  Training Loss  0.04827576130628586\n",
            "Epoch  11 Batch  184 / 525  Training Loss  0.049974530935287476\n",
            "Epoch  11 Batch  185 / 525  Training Loss  0.038090698421001434\n",
            "Epoch  11 Batch  186 / 525  Training Loss  0.049310170114040375\n",
            "Epoch  11 Batch  187 / 525  Training Loss  0.03790911287069321\n",
            "Epoch  11 Batch  188 / 525  Training Loss  0.0346941277384758\n",
            "Epoch  11 Batch  189 / 525  Training Loss  0.044953830540180206\n",
            "Epoch  11 Batch  190 / 525  Training Loss  0.034220140427351\n",
            "Epoch  11 Batch  191 / 525  Training Loss  0.05214209109544754\n",
            "Epoch  11 Batch  192 / 525  Training Loss  0.04060647636651993\n",
            "Epoch  11 Batch  193 / 525  Training Loss  0.04206272214651108\n",
            "Epoch  11 Batch  194 / 525  Training Loss  0.04953629523515701\n",
            "Epoch  11 Batch  195 / 525  Training Loss  0.03447793051600456\n",
            "Epoch  11 Batch  196 / 525  Training Loss  0.05918952077627182\n",
            "Epoch  11 Batch  197 / 525  Training Loss  0.04653964936733246\n",
            "Epoch  11 Batch  198 / 525  Training Loss  0.038081299513578415\n",
            "Epoch  11 Batch  199 / 525  Training Loss  0.039447467774152756\n",
            "Epoch  11 Batch  200 / 525  Training Loss  0.04599110409617424\n",
            "Epoch  11 Batch  201 / 525  Training Loss  0.04254751652479172\n",
            "Epoch  11 Batch  202 / 525  Training Loss  0.03690480440855026\n",
            "Epoch  11 Batch  203 / 525  Training Loss  0.05499449372291565\n",
            "Epoch  11 Batch  204 / 525  Training Loss  0.04006543383002281\n",
            "Epoch  11 Batch  205 / 525  Training Loss  0.03649040311574936\n",
            "Epoch  11 Batch  206 / 525  Training Loss  0.032337795943021774\n",
            "Epoch  11 Batch  207 / 525  Training Loss  0.04737706109881401\n",
            "Epoch  11 Batch  208 / 525  Training Loss  0.036122266203165054\n",
            "Epoch  11 Batch  209 / 525  Training Loss  0.0415896475315094\n",
            "Epoch  11 Batch  210 / 525  Training Loss  0.04845821112394333\n",
            "Epoch  11 Batch  211 / 525  Training Loss  0.03906053304672241\n",
            "Epoch  11 Batch  212 / 525  Training Loss  0.04763329029083252\n",
            "Epoch  11 Batch  213 / 525  Training Loss  0.04966138303279877\n",
            "Epoch  11 Batch  214 / 525  Training Loss  0.049018267542123795\n",
            "Epoch  11 Batch  215 / 525  Training Loss  0.045123837888240814\n",
            "Epoch  11 Batch  216 / 525  Training Loss  0.04602036252617836\n",
            "Epoch  11 Batch  217 / 525  Training Loss  0.037071872502565384\n",
            "Epoch  11 Batch  218 / 525  Training Loss  0.042813338339328766\n",
            "Epoch  11 Batch  219 / 525  Training Loss  0.04077068716287613\n",
            "Epoch  11 Batch  220 / 525  Training Loss  0.04606918618083\n",
            "Epoch  11 Batch  221 / 525  Training Loss  0.048641808331012726\n",
            "Epoch  11 Batch  222 / 525  Training Loss  0.05411761254072189\n",
            "Epoch  11 Batch  223 / 525  Training Loss  0.04529831185936928\n",
            "Epoch  11 Batch  224 / 525  Training Loss  0.04352782294154167\n",
            "Epoch  11 Batch  225 / 525  Training Loss  0.040235213935375214\n",
            "Epoch  11 Batch  226 / 525  Training Loss  0.04373229295015335\n",
            "Epoch  11 Batch  227 / 525  Training Loss  0.03401655703783035\n",
            "Epoch  11 Batch  228 / 525  Training Loss  0.0519818440079689\n",
            "Epoch  11 Batch  229 / 525  Training Loss  0.03817623481154442\n",
            "Epoch  11 Batch  230 / 525  Training Loss  0.04513726383447647\n",
            "Epoch  11 Batch  231 / 525  Training Loss  0.04806581884622574\n",
            "Epoch  11 Batch  232 / 525  Training Loss  0.051468800753355026\n",
            "Epoch  11 Batch  233 / 525  Training Loss  0.04560093581676483\n",
            "Epoch  11 Batch  234 / 525  Training Loss  0.05408511683344841\n",
            "Epoch  11 Batch  235 / 525  Training Loss  0.04167112708091736\n",
            "Epoch  11 Batch  236 / 525  Training Loss  0.036015696823596954\n",
            "Epoch  11 Batch  237 / 525  Training Loss  0.045924246311187744\n",
            "Epoch  11 Batch  238 / 525  Training Loss  0.051071397960186005\n",
            "Epoch  11 Batch  239 / 525  Training Loss  0.032136689871549606\n",
            "Epoch  11 Batch  240 / 525  Training Loss  0.04623717814683914\n",
            "Epoch  11 Batch  241 / 525  Training Loss  0.046841930598020554\n",
            "Epoch  11 Batch  242 / 525  Training Loss  0.04834495857357979\n",
            "Epoch  11 Batch  243 / 525  Training Loss  0.029723186045885086\n",
            "Epoch  11 Batch  244 / 525  Training Loss  0.040532663464546204\n",
            "Epoch  11 Batch  245 / 525  Training Loss  0.04921623691916466\n",
            "Epoch  11 Batch  246 / 525  Training Loss  0.0317908450961113\n",
            "Epoch  11 Batch  247 / 525  Training Loss  0.030357802286744118\n",
            "Epoch  11 Batch  248 / 525  Training Loss  0.04032401740550995\n",
            "Epoch  11 Batch  249 / 525  Training Loss  0.030310381203889847\n",
            "Epoch  11 Batch  250 / 525  Training Loss  0.04508981481194496\n",
            "Epoch  11 Batch  251 / 525  Training Loss  0.02925870753824711\n",
            "Epoch  11 Batch  252 / 525  Training Loss  0.04082841798663139\n",
            "Epoch  11 Batch  253 / 525  Training Loss  0.05064669996500015\n",
            "Epoch  11 Batch  254 / 525  Training Loss  0.044815804809331894\n",
            "Epoch  11 Batch  255 / 525  Training Loss  0.040587928146123886\n",
            "Epoch  11 Batch  256 / 525  Training Loss  0.042035892605781555\n",
            "Epoch  11 Batch  257 / 525  Training Loss  0.054122280329465866\n",
            "Epoch  11 Batch  258 / 525  Training Loss  0.050037283450365067\n",
            "Epoch  11 Batch  259 / 525  Training Loss  0.04792419821023941\n",
            "Epoch  11 Batch  260 / 525  Training Loss  0.03320785611867905\n",
            "Epoch  11 Batch  261 / 525  Training Loss  0.028293153271079063\n",
            "Epoch  11 Batch  262 / 525  Training Loss  0.0331975519657135\n",
            "Epoch  11 Batch  263 / 525  Training Loss  0.050063811242580414\n",
            "Epoch  11 Batch  264 / 525  Training Loss  0.05800748988986015\n",
            "Epoch  11 Batch  265 / 525  Training Loss  0.05658971145749092\n",
            "Epoch  11 Batch  266 / 525  Training Loss  0.040180616080760956\n",
            "Epoch  11 Batch  267 / 525  Training Loss  0.06459672003984451\n",
            "Epoch  11 Batch  268 / 525  Training Loss  0.05448801442980766\n",
            "Epoch  11 Batch  269 / 525  Training Loss  0.04877059906721115\n",
            "Epoch  11 Batch  270 / 525  Training Loss  0.0479385070502758\n",
            "Epoch  11 Batch  271 / 525  Training Loss  0.04640952870249748\n",
            "Epoch  11 Batch  272 / 525  Training Loss  0.05252814292907715\n",
            "Epoch  11 Batch  273 / 525  Training Loss  0.04873085767030716\n",
            "Epoch  11 Batch  274 / 525  Training Loss  0.03514258563518524\n",
            "Epoch  11 Batch  275 / 525  Training Loss  0.03966923803091049\n",
            "Epoch  11 Batch  276 / 525  Training Loss  0.029164502397179604\n",
            "Epoch  11 Batch  277 / 525  Training Loss  0.033884257078170776\n",
            "Epoch  11 Batch  278 / 525  Training Loss  0.03433077782392502\n",
            "Epoch  11 Batch  279 / 525  Training Loss  0.03324710205197334\n",
            "Epoch  11 Batch  280 / 525  Training Loss  0.04627339914441109\n",
            "Epoch  11 Batch  281 / 525  Training Loss  0.059131067246198654\n",
            "Epoch  11 Batch  282 / 525  Training Loss  0.07315938174724579\n",
            "Epoch  11 Batch  283 / 525  Training Loss  0.03370222449302673\n",
            "Epoch  11 Batch  284 / 525  Training Loss  0.061110593378543854\n",
            "Epoch  11 Batch  285 / 525  Training Loss  0.06208102032542229\n",
            "Epoch  11 Batch  286 / 525  Training Loss  0.03588050603866577\n",
            "Epoch  11 Batch  287 / 525  Training Loss  0.04398388788104057\n",
            "Epoch  11 Batch  288 / 525  Training Loss  0.03579532355070114\n",
            "Epoch  11 Batch  289 / 525  Training Loss  0.037065934389829636\n",
            "Epoch  11 Batch  290 / 525  Training Loss  0.047062113881111145\n",
            "Epoch  11 Batch  291 / 525  Training Loss  0.04080568999052048\n",
            "Epoch  11 Batch  292 / 525  Training Loss  0.03343838453292847\n",
            "Epoch  11 Batch  293 / 525  Training Loss  0.06332935392856598\n",
            "Epoch  11 Batch  294 / 525  Training Loss  0.041537780314683914\n",
            "Epoch  11 Batch  295 / 525  Training Loss  0.054778922349214554\n",
            "Epoch  11 Batch  296 / 525  Training Loss  0.04454060643911362\n",
            "Epoch  11 Batch  297 / 525  Training Loss  0.02824356220662594\n",
            "Epoch  11 Batch  298 / 525  Training Loss  0.03870813921093941\n",
            "Epoch  11 Batch  299 / 525  Training Loss  0.035507820546627045\n",
            "Epoch  11 Batch  300 / 525  Training Loss  0.03597545623779297\n",
            "Epoch  11 Batch  301 / 525  Training Loss  0.03991325572133064\n",
            "Epoch  11 Batch  302 / 525  Training Loss  0.030995240435004234\n",
            "Epoch  11 Batch  303 / 525  Training Loss  0.06993426382541656\n",
            "Epoch  11 Batch  304 / 525  Training Loss  0.05859943479299545\n",
            "Epoch  11 Batch  305 / 525  Training Loss  0.0393746979534626\n",
            "Epoch  11 Batch  306 / 525  Training Loss  0.04298694059252739\n",
            "Epoch  11 Batch  307 / 525  Training Loss  0.034440260380506516\n",
            "Epoch  11 Batch  308 / 525  Training Loss  0.04314279556274414\n",
            "Epoch  11 Batch  309 / 525  Training Loss  0.04650512710213661\n",
            "Epoch  11 Batch  310 / 525  Training Loss  0.04539176821708679\n",
            "Epoch  11 Batch  311 / 525  Training Loss  0.04301482066512108\n",
            "Epoch  11 Batch  312 / 525  Training Loss  0.03381755203008652\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  11 Batch  313 / 525  Training Loss  0.036879949271678925\n",
            "Epoch  11 Batch  314 / 525  Training Loss  0.03709451109170914\n",
            "Epoch  11 Batch  315 / 525  Training Loss  0.06187314912676811\n",
            "Epoch  11 Batch  316 / 525  Training Loss  0.0368272140622139\n",
            "Epoch  11 Batch  317 / 525  Training Loss  0.048232633620500565\n",
            "Epoch  11 Batch  318 / 525  Training Loss  0.041945669800043106\n",
            "Epoch  11 Batch  319 / 525  Training Loss  0.04579533264040947\n",
            "Epoch  11 Batch  320 / 525  Training Loss  0.06569111347198486\n",
            "Epoch  11 Batch  321 / 525  Training Loss  0.04537156969308853\n",
            "Epoch  11 Batch  322 / 525  Training Loss  0.043564312160015106\n",
            "Epoch  11 Batch  323 / 525  Training Loss  0.04388093575835228\n",
            "Epoch  11 Batch  324 / 525  Training Loss  0.04322313144803047\n",
            "Epoch  11 Batch  325 / 525  Training Loss  0.03908095508813858\n",
            "Epoch  11 Batch  326 / 525  Training Loss  0.06616196036338806\n",
            "Epoch  11 Batch  327 / 525  Training Loss  0.05073463171720505\n",
            "Epoch  11 Batch  328 / 525  Training Loss  0.0423261895775795\n",
            "Epoch  11 Batch  329 / 525  Training Loss  0.053785037249326706\n",
            "Epoch  11 Batch  330 / 525  Training Loss  0.04718707501888275\n",
            "Epoch  11 Batch  331 / 525  Training Loss  0.03843450918793678\n",
            "Epoch  11 Batch  332 / 525  Training Loss  0.05838388949632645\n",
            "Epoch  11 Batch  333 / 525  Training Loss  0.043535586446523666\n",
            "Epoch  11 Batch  334 / 525  Training Loss  0.0399477444589138\n",
            "Epoch  11 Batch  335 / 525  Training Loss  0.05018429830670357\n",
            "Epoch  11 Batch  336 / 525  Training Loss  0.04665757715702057\n",
            "Epoch  11 Batch  337 / 525  Training Loss  0.045595213770866394\n",
            "Epoch  11 Batch  338 / 525  Training Loss  0.05098484084010124\n",
            "Epoch  11 Batch  339 / 525  Training Loss  0.050231318920850754\n",
            "Epoch  11 Batch  340 / 525  Training Loss  0.035123199224472046\n",
            "Epoch  11 Batch  341 / 525  Training Loss  0.04075345769524574\n",
            "Epoch  11 Batch  342 / 525  Training Loss  0.04476447030901909\n",
            "Epoch  11 Batch  343 / 525  Training Loss  0.03126612678170204\n",
            "Epoch  11 Batch  344 / 525  Training Loss  0.03650512546300888\n",
            "Epoch  11 Batch  345 / 525  Training Loss  0.03622734174132347\n",
            "Epoch  11 Batch  346 / 525  Training Loss  0.04377081245183945\n",
            "Epoch  11 Batch  347 / 525  Training Loss  0.03423726558685303\n",
            "Epoch  11 Batch  348 / 525  Training Loss  0.04008970409631729\n",
            "Epoch  11 Batch  349 / 525  Training Loss  0.04801853001117706\n",
            "Epoch  11 Batch  350 / 525  Training Loss  0.03565888851881027\n",
            "Epoch  11 Batch  351 / 525  Training Loss  0.031317710876464844\n",
            "Epoch  11 Batch  352 / 525  Training Loss  0.052539318799972534\n",
            "Epoch  11 Batch  353 / 525  Training Loss  0.0377015545964241\n",
            "Epoch  11 Batch  354 / 525  Training Loss  0.03963788226246834\n",
            "Epoch  11 Batch  355 / 525  Training Loss  0.05002366751432419\n",
            "Epoch  11 Batch  356 / 525  Training Loss  0.041128434240818024\n",
            "Epoch  11 Batch  357 / 525  Training Loss  0.04279041662812233\n",
            "Epoch  11 Batch  358 / 525  Training Loss  0.05458065867424011\n",
            "Epoch  11 Batch  359 / 525  Training Loss  0.05955106019973755\n",
            "Epoch  11 Batch  360 / 525  Training Loss  0.04172486439347267\n",
            "Epoch  11 Batch  361 / 525  Training Loss  0.04865066707134247\n",
            "Epoch  11 Batch  362 / 525  Training Loss  0.048600997775793076\n",
            "Epoch  11 Batch  363 / 525  Training Loss  0.04980016499757767\n",
            "Epoch  11 Batch  364 / 525  Training Loss  0.03067765012383461\n",
            "Epoch  11 Batch  365 / 525  Training Loss  0.03084479831159115\n",
            "Epoch  11 Batch  366 / 525  Training Loss  0.04701750725507736\n",
            "Epoch  11 Batch  367 / 525  Training Loss  0.058329902589321136\n",
            "Epoch  11 Batch  368 / 525  Training Loss  0.03850553184747696\n",
            "Epoch  11 Batch  369 / 525  Training Loss  0.03160112351179123\n",
            "Epoch  11 Batch  370 / 525  Training Loss  0.05588201433420181\n",
            "Epoch  11 Batch  371 / 525  Training Loss  0.04060749709606171\n",
            "Epoch  11 Batch  372 / 525  Training Loss  0.04212397709488869\n",
            "Epoch  11 Batch  373 / 525  Training Loss  0.0471896268427372\n",
            "Epoch  11 Batch  374 / 525  Training Loss  0.038848258554935455\n",
            "Epoch  11 Batch  375 / 525  Training Loss  0.04122938960790634\n",
            "Epoch  11 Batch  376 / 525  Training Loss  0.033168382942676544\n",
            "Epoch  11 Batch  377 / 525  Training Loss  0.029676180332899094\n",
            "Epoch  11 Batch  378 / 525  Training Loss  0.057697802782058716\n",
            "Epoch  11 Batch  379 / 525  Training Loss  0.03766687959432602\n",
            "Epoch  11 Batch  380 / 525  Training Loss  0.05542496591806412\n",
            "Epoch  11 Batch  381 / 525  Training Loss  0.05109115317463875\n",
            "Epoch  11 Batch  382 / 525  Training Loss  0.04426845163106918\n",
            "Epoch  11 Batch  383 / 525  Training Loss  0.036556974053382874\n",
            "Epoch  11 Batch  384 / 525  Training Loss  0.05616523697972298\n",
            "Epoch  11 Batch  385 / 525  Training Loss  0.05308476835489273\n",
            "Epoch  11 Batch  386 / 525  Training Loss  0.050509847700595856\n",
            "Epoch  11 Batch  387 / 525  Training Loss  0.047900740057229996\n",
            "Epoch  11 Batch  388 / 525  Training Loss  0.06024838611483574\n",
            "Epoch  11 Batch  389 / 525  Training Loss  0.05601862818002701\n",
            "Epoch  11 Batch  390 / 525  Training Loss  0.03463215380907059\n",
            "Epoch  11 Batch  391 / 525  Training Loss  0.040290601551532745\n",
            "Epoch  11 Batch  392 / 525  Training Loss  0.04817153513431549\n",
            "Epoch  11 Batch  393 / 525  Training Loss  0.04717700928449631\n",
            "Epoch  11 Batch  394 / 525  Training Loss  0.06255953013896942\n",
            "Epoch  11 Batch  395 / 525  Training Loss  0.05863914638757706\n",
            "Epoch  11 Batch  396 / 525  Training Loss  0.02848024107515812\n",
            "Epoch  11 Batch  397 / 525  Training Loss  0.053318090736866\n",
            "Epoch  11 Batch  398 / 525  Training Loss  0.037757258862257004\n",
            "Epoch  11 Batch  399 / 525  Training Loss  0.0480077862739563\n",
            "Epoch  11 Batch  400 / 525  Training Loss  0.04193075746297836\n",
            "Epoch  11 Batch  401 / 525  Training Loss  0.055644165724515915\n",
            "Epoch  11 Batch  402 / 525  Training Loss  0.04150910675525665\n",
            "Epoch  11 Batch  403 / 525  Training Loss  0.04763618856668472\n",
            "Epoch  11 Batch  404 / 525  Training Loss  0.04199335724115372\n",
            "Epoch  11 Batch  405 / 525  Training Loss  0.04552600532770157\n",
            "Epoch  11 Batch  406 / 525  Training Loss  0.04161304235458374\n",
            "Epoch  11 Batch  407 / 525  Training Loss  0.04859808087348938\n",
            "Epoch  11 Batch  408 / 525  Training Loss  0.049103397876024246\n",
            "Epoch  11 Batch  409 / 525  Training Loss  0.04757368192076683\n",
            "Epoch  11 Batch  410 / 525  Training Loss  0.0381651371717453\n",
            "Epoch  11 Batch  411 / 525  Training Loss  0.06885383278131485\n",
            "Epoch  11 Batch  412 / 525  Training Loss  0.04222302511334419\n",
            "Epoch  11 Batch  413 / 525  Training Loss  0.04733510687947273\n",
            "Epoch  11 Batch  414 / 525  Training Loss  0.04790544882416725\n",
            "Epoch  11 Batch  415 / 525  Training Loss  0.05263323709368706\n",
            "Epoch  11 Batch  416 / 525  Training Loss  0.0443028099834919\n",
            "Epoch  11 Batch  417 / 525  Training Loss  0.04472701996564865\n",
            "Epoch  11 Batch  418 / 525  Training Loss  0.038207218050956726\n",
            "Epoch  11 Batch  419 / 525  Training Loss  0.03720784932374954\n",
            "Epoch  11 Batch  420 / 525  Training Loss  0.047564081847667694\n",
            "Epoch  11 Batch  421 / 525  Training Loss  0.03471638262271881\n",
            "Epoch  11 Batch  422 / 525  Training Loss  0.05506821349263191\n",
            "Epoch  11 Batch  423 / 525  Training Loss  0.041081033647060394\n",
            "Epoch  11 Batch  424 / 525  Training Loss  0.04739125818014145\n",
            "Epoch  11 Batch  425 / 525  Training Loss  0.04158893972635269\n",
            "Epoch  11 Batch  426 / 525  Training Loss  0.048036616295576096\n",
            "Epoch  11 Batch  427 / 525  Training Loss  0.036759939044713974\n",
            "Epoch  11 Batch  428 / 525  Training Loss  0.04606138914823532\n",
            "Epoch  11 Batch  429 / 525  Training Loss  0.037705160677433014\n",
            "Epoch  11 Batch  430 / 525  Training Loss  0.0379810594022274\n",
            "Epoch  11 Batch  431 / 525  Training Loss  0.04362575337290764\n",
            "Epoch  11 Batch  432 / 525  Training Loss  0.05017826706171036\n",
            "Epoch  11 Batch  433 / 525  Training Loss  0.049985453486442566\n",
            "Epoch  11 Batch  434 / 525  Training Loss  0.03511764854192734\n",
            "Epoch  11 Batch  435 / 525  Training Loss  0.039616893976926804\n",
            "Epoch  11 Batch  436 / 525  Training Loss  0.040743302553892136\n",
            "Epoch  11 Batch  437 / 525  Training Loss  0.04700368270277977\n",
            "Epoch  11 Batch  438 / 525  Training Loss  0.03301617503166199\n",
            "Epoch  11 Batch  439 / 525  Training Loss  0.047251664102077484\n",
            "Epoch  11 Batch  440 / 525  Training Loss  0.04122726619243622\n",
            "Epoch  11 Batch  441 / 525  Training Loss  0.03910989314317703\n",
            "Epoch  11 Batch  442 / 525  Training Loss  0.05965697020292282\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  11 Batch  443 / 525  Training Loss  0.055524785071611404\n",
            "Epoch  11 Batch  444 / 525  Training Loss  0.05248064920306206\n",
            "Epoch  11 Batch  445 / 525  Training Loss  0.05805080011487007\n",
            "Epoch  11 Batch  446 / 525  Training Loss  0.03533425182104111\n",
            "Epoch  11 Batch  447 / 525  Training Loss  0.041214197874069214\n",
            "Epoch  11 Batch  448 / 525  Training Loss  0.04325104504823685\n",
            "Epoch  11 Batch  449 / 525  Training Loss  0.03947480022907257\n",
            "Epoch  11 Batch  450 / 525  Training Loss  0.05470455810427666\n",
            "Epoch  11 Batch  451 / 525  Training Loss  0.033318061381578445\n",
            "Epoch  11 Batch  452 / 525  Training Loss  0.05797913670539856\n",
            "Epoch  11 Batch  453 / 525  Training Loss  0.024834711104631424\n",
            "Epoch  11 Batch  454 / 525  Training Loss  0.052874673157930374\n",
            "Epoch  11 Batch  455 / 525  Training Loss  0.04779665172100067\n",
            "Epoch  11 Batch  456 / 525  Training Loss  0.04455266520380974\n",
            "Epoch  11 Batch  457 / 525  Training Loss  0.044221483170986176\n",
            "Epoch  11 Batch  458 / 525  Training Loss  0.055281154811382294\n",
            "Epoch  11 Batch  459 / 525  Training Loss  0.043731383979320526\n",
            "Epoch  11 Batch  460 / 525  Training Loss  0.04499158635735512\n",
            "Epoch  11 Batch  461 / 525  Training Loss  0.05268298462033272\n",
            "Epoch  11 Batch  462 / 525  Training Loss  0.03731277957558632\n",
            "Epoch  11 Batch  463 / 525  Training Loss  0.042642299085855484\n",
            "Epoch  11 Batch  464 / 525  Training Loss  0.04524124413728714\n",
            "Epoch  11 Batch  465 / 525  Training Loss  0.054654985666275024\n",
            "Epoch  11 Batch  466 / 525  Training Loss  0.040686361491680145\n",
            "Epoch  11 Batch  467 / 525  Training Loss  0.03925019130110741\n",
            "Epoch  11 Batch  468 / 525  Training Loss  0.03224782273173332\n",
            "Epoch  11 Batch  469 / 525  Training Loss  0.03424344211816788\n",
            "Epoch  11 Batch  470 / 525  Training Loss  0.04752194136381149\n",
            "Epoch  11 Batch  471 / 525  Training Loss  0.04990434646606445\n",
            "Epoch  11 Batch  472 / 525  Training Loss  0.03633570298552513\n",
            "Epoch  11 Batch  473 / 525  Training Loss  0.036096639931201935\n",
            "Epoch  11 Batch  474 / 525  Training Loss  0.055163271725177765\n",
            "Epoch  11 Batch  475 / 525  Training Loss  0.05072224140167236\n",
            "Epoch  11 Batch  476 / 525  Training Loss  0.042664896696805954\n",
            "Epoch  11 Batch  477 / 525  Training Loss  0.04828999936580658\n",
            "Epoch  11 Batch  478 / 525  Training Loss  0.04520564526319504\n",
            "Epoch  11 Batch  479 / 525  Training Loss  0.030709270387887955\n",
            "Epoch  11 Batch  480 / 525  Training Loss  0.054115645587444305\n",
            "Epoch  11 Batch  481 / 525  Training Loss  0.03743697330355644\n",
            "Epoch  11 Batch  482 / 525  Training Loss  0.05776460841298103\n",
            "Epoch  11 Batch  483 / 525  Training Loss  0.03281135857105255\n",
            "Epoch  11 Batch  484 / 525  Training Loss  0.06324451416730881\n",
            "Epoch  11 Batch  485 / 525  Training Loss  0.043457526713609695\n",
            "Epoch  11 Batch  486 / 525  Training Loss  0.04296423867344856\n",
            "Epoch  11 Batch  487 / 525  Training Loss  0.04274886101484299\n",
            "Epoch  11 Batch  488 / 525  Training Loss  0.050871409475803375\n",
            "Epoch  11 Batch  489 / 525  Training Loss  0.041725847870111465\n",
            "Epoch  11 Batch  490 / 525  Training Loss  0.045750804245471954\n",
            "Epoch  11 Batch  491 / 525  Training Loss  0.06134793907403946\n",
            "Epoch  11 Batch  492 / 525  Training Loss  0.05058889836072922\n",
            "Epoch  11 Batch  493 / 525  Training Loss  0.05805550888180733\n",
            "Epoch  11 Batch  494 / 525  Training Loss  0.05859031528234482\n",
            "Epoch  11 Batch  495 / 525  Training Loss  0.037926267832517624\n",
            "Epoch  11 Batch  496 / 525  Training Loss  0.047503530979156494\n",
            "Epoch  11 Batch  497 / 525  Training Loss  0.037652190774679184\n",
            "Epoch  11 Batch  498 / 525  Training Loss  0.03465719148516655\n",
            "Epoch  11 Batch  499 / 525  Training Loss  0.04698652774095535\n",
            "Epoch  11 Batch  500 / 525  Training Loss  0.0484386682510376\n",
            "Epoch  11 Batch  501 / 525  Training Loss  0.04359706491231918\n",
            "Epoch  11 Batch  502 / 525  Training Loss  0.053665269166231155\n",
            "Epoch  11 Batch  503 / 525  Training Loss  0.06434588134288788\n",
            "Epoch  11 Batch  504 / 525  Training Loss  0.037666164338588715\n",
            "Epoch  11 Batch  505 / 525  Training Loss  0.05369694158434868\n",
            "Epoch  11 Batch  506 / 525  Training Loss  0.05262488126754761\n",
            "Epoch  11 Batch  507 / 525  Training Loss  0.059538714587688446\n",
            "Epoch  11 Batch  508 / 525  Training Loss  0.052961982786655426\n",
            "Epoch  11 Batch  509 / 525  Training Loss  0.05481129139661789\n",
            "Epoch  11 Batch  510 / 525  Training Loss  0.06258013844490051\n",
            "Epoch  11 Batch  511 / 525  Training Loss  0.04188117757439613\n",
            "Epoch  11 Batch  512 / 525  Training Loss  0.044891756027936935\n",
            "Epoch  11 Batch  513 / 525  Training Loss  0.060636006295681\n",
            "Epoch  11 Batch  514 / 525  Training Loss  0.04335038736462593\n",
            "Epoch  11 Batch  515 / 525  Training Loss  0.051306016743183136\n",
            "Epoch  11 Batch  516 / 525  Training Loss  0.05973575636744499\n",
            "Epoch  11 Batch  517 / 525  Training Loss  0.05010797828435898\n",
            "Epoch  11 Batch  518 / 525  Training Loss  0.04848075658082962\n",
            "Epoch  11 Batch  519 / 525  Training Loss  0.03938679024577141\n",
            "Epoch  11 Batch  520 / 525  Training Loss  0.06180564686655998\n",
            "Epoch  11 Batch  521 / 525  Training Loss  0.041605014353990555\n",
            "Epoch  11 Batch  522 / 525  Training Loss  0.05552755668759346\n",
            "Epoch  11 Batch  523 / 525  Training Loss  0.04168541729450226\n",
            "Epoch  11 Batch  524 / 525  Training Loss  0.06129327416419983\n",
            "  12    |    -    |   0.044264   | 49.275000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 12\n",
            "Epoch  12 Batch  0 / 525  Training Loss  0.02865903452038765\n",
            "Epoch  12 Batch  1 / 525  Training Loss  0.03681893274188042\n",
            "Epoch  12 Batch  2 / 525  Training Loss  0.033704839646816254\n",
            "Epoch  12 Batch  3 / 525  Training Loss  0.0404839850962162\n",
            "Epoch  12 Batch  4 / 525  Training Loss  0.030963268131017685\n",
            "Epoch  12 Batch  5 / 525  Training Loss  0.03172928839921951\n",
            "Epoch  12 Batch  6 / 525  Training Loss  0.02246096357703209\n",
            "Epoch  12 Batch  7 / 525  Training Loss  0.03443178907036781\n",
            "Epoch  12 Batch  8 / 525  Training Loss  0.043567877262830734\n",
            "Epoch  12 Batch  9 / 525  Training Loss  0.03406303748488426\n",
            "Epoch  12 Batch  10 / 525  Training Loss  0.036623578518629074\n",
            "Epoch  12 Batch  11 / 525  Training Loss  0.05333280563354492\n",
            "Epoch  12 Batch  12 / 525  Training Loss  0.038050293922424316\n",
            "Epoch  12 Batch  13 / 525  Training Loss  0.042941607534885406\n",
            "Epoch  12 Batch  14 / 525  Training Loss  0.04492072016000748\n",
            "Epoch  12 Batch  15 / 525  Training Loss  0.03936922550201416\n",
            "Epoch  12 Batch  16 / 525  Training Loss  0.038735032081604004\n",
            "Epoch  12 Batch  17 / 525  Training Loss  0.029169585555791855\n",
            "Epoch  12 Batch  18 / 525  Training Loss  0.03140648454427719\n",
            "Epoch  12 Batch  19 / 525  Training Loss  0.0394049733877182\n",
            "Epoch  12 Batch  20 / 525  Training Loss  0.033846355974674225\n",
            "Epoch  12 Batch  21 / 525  Training Loss  0.026733627542853355\n",
            "Epoch  12 Batch  22 / 525  Training Loss  0.03337555378675461\n",
            "Epoch  12 Batch  23 / 525  Training Loss  0.02663489803671837\n",
            "Epoch  12 Batch  24 / 525  Training Loss  0.032209478318691254\n",
            "Epoch  12 Batch  25 / 525  Training Loss  0.032203465700149536\n",
            "Epoch  12 Batch  26 / 525  Training Loss  0.03681289777159691\n",
            "Epoch  12 Batch  27 / 525  Training Loss  0.04537802189588547\n",
            "Epoch  12 Batch  28 / 525  Training Loss  0.04331502318382263\n",
            "Epoch  12 Batch  29 / 525  Training Loss  0.03255955129861832\n",
            "Epoch  12 Batch  30 / 525  Training Loss  0.027640407904982567\n",
            "Epoch  12 Batch  31 / 525  Training Loss  0.032524146139621735\n",
            "Epoch  12 Batch  32 / 525  Training Loss  0.022307582199573517\n",
            "Epoch  12 Batch  33 / 525  Training Loss  0.030083054676651955\n",
            "Epoch  12 Batch  34 / 525  Training Loss  0.02625214122235775\n",
            "Epoch  12 Batch  35 / 525  Training Loss  0.034830544143915176\n",
            "Epoch  12 Batch  36 / 525  Training Loss  0.04096730798482895\n",
            "Epoch  12 Batch  37 / 525  Training Loss  0.028148621320724487\n",
            "Epoch  12 Batch  38 / 525  Training Loss  0.021353770047426224\n",
            "Epoch  12 Batch  39 / 525  Training Loss  0.03078893944621086\n",
            "Epoch  12 Batch  40 / 525  Training Loss  0.033638548105955124\n",
            "Epoch  12 Batch  41 / 525  Training Loss  0.033432722091674805\n",
            "Epoch  12 Batch  42 / 525  Training Loss  0.03209664672613144\n",
            "Epoch  12 Batch  43 / 525  Training Loss  0.03227346017956734\n",
            "Epoch  12 Batch  44 / 525  Training Loss  0.030081626027822495\n",
            "Epoch  12 Batch  45 / 525  Training Loss  0.033552058041095734\n",
            "Epoch  12 Batch  46 / 525  Training Loss  0.022655446082353592\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  12 Batch  47 / 525  Training Loss  0.02784302830696106\n",
            "Epoch  12 Batch  48 / 525  Training Loss  0.023348284885287285\n",
            "Epoch  12 Batch  49 / 525  Training Loss  0.025966739282011986\n",
            "Epoch  12 Batch  50 / 525  Training Loss  0.01871286705136299\n",
            "Epoch  12 Batch  51 / 525  Training Loss  0.028240341693162918\n",
            "Epoch  12 Batch  52 / 525  Training Loss  0.040978822857141495\n",
            "Epoch  12 Batch  53 / 525  Training Loss  0.028116261586546898\n",
            "Epoch  12 Batch  54 / 525  Training Loss  0.026987260207533836\n",
            "Epoch  12 Batch  55 / 525  Training Loss  0.026364851742982864\n",
            "Epoch  12 Batch  56 / 525  Training Loss  0.03428855538368225\n",
            "Epoch  12 Batch  57 / 525  Training Loss  0.043321967124938965\n",
            "Epoch  12 Batch  58 / 525  Training Loss  0.03818131238222122\n",
            "Epoch  12 Batch  59 / 525  Training Loss  0.020918842405080795\n",
            "Epoch  12 Batch  60 / 525  Training Loss  0.03006865084171295\n",
            "Epoch  12 Batch  61 / 525  Training Loss  0.03315906599164009\n",
            "Epoch  12 Batch  62 / 525  Training Loss  0.047151658684015274\n",
            "Epoch  12 Batch  63 / 525  Training Loss  0.03376007452607155\n",
            "Epoch  12 Batch  64 / 525  Training Loss  0.01857879012823105\n",
            "Epoch  12 Batch  65 / 525  Training Loss  0.03034958615899086\n",
            "Epoch  12 Batch  66 / 525  Training Loss  0.03600594401359558\n",
            "Epoch  12 Batch  67 / 525  Training Loss  0.03929547965526581\n",
            "Epoch  12 Batch  68 / 525  Training Loss  0.0387905016541481\n",
            "Epoch  12 Batch  69 / 525  Training Loss  0.03213055804371834\n",
            "Epoch  12 Batch  70 / 525  Training Loss  0.03693109378218651\n",
            "Epoch  12 Batch  71 / 525  Training Loss  0.026807114481925964\n",
            "Epoch  12 Batch  72 / 525  Training Loss  0.02449069917201996\n",
            "Epoch  12 Batch  73 / 525  Training Loss  0.018395673483610153\n",
            "Epoch  12 Batch  74 / 525  Training Loss  0.024544106796383858\n",
            "Epoch  12 Batch  75 / 525  Training Loss  0.031476497650146484\n",
            "Epoch  12 Batch  76 / 525  Training Loss  0.028838729485869408\n",
            "Epoch  12 Batch  77 / 525  Training Loss  0.028583437204360962\n",
            "Epoch  12 Batch  78 / 525  Training Loss  0.02778792381286621\n",
            "Epoch  12 Batch  79 / 525  Training Loss  0.030649056658148766\n",
            "Epoch  12 Batch  80 / 525  Training Loss  0.03821513056755066\n",
            "Epoch  12 Batch  81 / 525  Training Loss  0.04845144599676132\n",
            "Epoch  12 Batch  82 / 525  Training Loss  0.02620202675461769\n",
            "Epoch  12 Batch  83 / 525  Training Loss  0.02432788535952568\n",
            "Epoch  12 Batch  84 / 525  Training Loss  0.031515996903181076\n",
            "Epoch  12 Batch  85 / 525  Training Loss  0.03183870017528534\n",
            "Epoch  12 Batch  86 / 525  Training Loss  0.05128812789916992\n",
            "Epoch  12 Batch  87 / 525  Training Loss  0.02895846590399742\n",
            "Epoch  12 Batch  88 / 525  Training Loss  0.02115446887910366\n",
            "Epoch  12 Batch  89 / 525  Training Loss  0.037950754165649414\n",
            "Epoch  12 Batch  90 / 525  Training Loss  0.03474569693207741\n",
            "Epoch  12 Batch  91 / 525  Training Loss  0.04396793246269226\n",
            "Epoch  12 Batch  92 / 525  Training Loss  0.04101447016000748\n",
            "Epoch  12 Batch  93 / 525  Training Loss  0.03303501754999161\n",
            "Epoch  12 Batch  94 / 525  Training Loss  0.030476132407784462\n",
            "Epoch  12 Batch  95 / 525  Training Loss  0.04034874215722084\n",
            "Epoch  12 Batch  96 / 525  Training Loss  0.031432438641786575\n",
            "Epoch  12 Batch  97 / 525  Training Loss  0.04127772897481918\n",
            "Epoch  12 Batch  98 / 525  Training Loss  0.03581026941537857\n",
            "Epoch  12 Batch  99 / 525  Training Loss  0.023672904819250107\n",
            "Epoch  12 Batch  100 / 525  Training Loss  0.036696214228868484\n",
            "Epoch  12 Batch  101 / 525  Training Loss  0.03015575185418129\n",
            "Epoch  12 Batch  102 / 525  Training Loss  0.041416235268116\n",
            "Epoch  12 Batch  103 / 525  Training Loss  0.031043250113725662\n",
            "Epoch  12 Batch  104 / 525  Training Loss  0.030776768922805786\n",
            "Epoch  12 Batch  105 / 525  Training Loss  0.031365759670734406\n",
            "Epoch  12 Batch  106 / 525  Training Loss  0.03607535362243652\n",
            "Epoch  12 Batch  107 / 525  Training Loss  0.027386989444494247\n",
            "Epoch  12 Batch  108 / 525  Training Loss  0.0388374850153923\n",
            "Epoch  12 Batch  109 / 525  Training Loss  0.02538013458251953\n",
            "Epoch  12 Batch  110 / 525  Training Loss  0.033392827957868576\n",
            "Epoch  12 Batch  111 / 525  Training Loss  0.04725714400410652\n",
            "Epoch  12 Batch  112 / 525  Training Loss  0.02855202555656433\n",
            "Epoch  12 Batch  113 / 525  Training Loss  0.03547872602939606\n",
            "Epoch  12 Batch  114 / 525  Training Loss  0.026455800980329514\n",
            "Epoch  12 Batch  115 / 525  Training Loss  0.032469216734170914\n",
            "Epoch  12 Batch  116 / 525  Training Loss  0.032018017023801804\n",
            "Epoch  12 Batch  117 / 525  Training Loss  0.028087342157959938\n",
            "Epoch  12 Batch  118 / 525  Training Loss  0.030013957992196083\n",
            "Epoch  12 Batch  119 / 525  Training Loss  0.024485254660248756\n",
            "Epoch  12 Batch  120 / 525  Training Loss  0.049687616527080536\n",
            "Epoch  12 Batch  121 / 525  Training Loss  0.033661436289548874\n",
            "Epoch  12 Batch  122 / 525  Training Loss  0.03091512992978096\n",
            "Epoch  12 Batch  123 / 525  Training Loss  0.031247470527887344\n",
            "Epoch  12 Batch  124 / 525  Training Loss  0.025399455800652504\n",
            "Epoch  12 Batch  125 / 525  Training Loss  0.030598780140280724\n",
            "Epoch  12 Batch  126 / 525  Training Loss  0.041473351418972015\n",
            "Epoch  12 Batch  127 / 525  Training Loss  0.02673441544175148\n",
            "Epoch  12 Batch  128 / 525  Training Loss  0.03333563357591629\n",
            "Epoch  12 Batch  129 / 525  Training Loss  0.031863532960414886\n",
            "Epoch  12 Batch  130 / 525  Training Loss  0.031000997871160507\n",
            "Epoch  12 Batch  131 / 525  Training Loss  0.02165140211582184\n",
            "Epoch  12 Batch  132 / 525  Training Loss  0.029657578095793724\n",
            "Epoch  12 Batch  133 / 525  Training Loss  0.02239873632788658\n",
            "Epoch  12 Batch  134 / 525  Training Loss  0.02735503576695919\n",
            "Epoch  12 Batch  135 / 525  Training Loss  0.03660927712917328\n",
            "Epoch  12 Batch  136 / 525  Training Loss  0.03159749507904053\n",
            "Epoch  12 Batch  137 / 525  Training Loss  0.02591615542769432\n",
            "Epoch  12 Batch  138 / 525  Training Loss  0.03326689451932907\n",
            "Epoch  12 Batch  139 / 525  Training Loss  0.034489430487155914\n",
            "Epoch  12 Batch  140 / 525  Training Loss  0.02724846825003624\n",
            "Epoch  12 Batch  141 / 525  Training Loss  0.021328184753656387\n",
            "Epoch  12 Batch  142 / 525  Training Loss  0.023576583713293076\n",
            "Epoch  12 Batch  143 / 525  Training Loss  0.03265318274497986\n",
            "Epoch  12 Batch  144 / 525  Training Loss  0.035647597163915634\n",
            "Epoch  12 Batch  145 / 525  Training Loss  0.03713192790746689\n",
            "Epoch  12 Batch  146 / 525  Training Loss  0.03973842412233353\n",
            "Epoch  12 Batch  147 / 525  Training Loss  0.03298913314938545\n",
            "Epoch  12 Batch  148 / 525  Training Loss  0.037599463015794754\n",
            "Epoch  12 Batch  149 / 525  Training Loss  0.03132248669862747\n",
            "Epoch  12 Batch  150 / 525  Training Loss  0.03656736761331558\n",
            "Epoch  12 Batch  151 / 525  Training Loss  0.025325005874037743\n",
            "Epoch  12 Batch  152 / 525  Training Loss  0.03676420450210571\n",
            "Epoch  12 Batch  153 / 525  Training Loss  0.03518658131361008\n",
            "Epoch  12 Batch  154 / 525  Training Loss  0.02971750870347023\n",
            "Epoch  12 Batch  155 / 525  Training Loss  0.031000902876257896\n",
            "Epoch  12 Batch  156 / 525  Training Loss  0.030132602900266647\n",
            "Epoch  12 Batch  157 / 525  Training Loss  0.04366500675678253\n",
            "Epoch  12 Batch  158 / 525  Training Loss  0.03033958375453949\n",
            "Epoch  12 Batch  159 / 525  Training Loss  0.026999961584806442\n",
            "Epoch  12 Batch  160 / 525  Training Loss  0.029559290036559105\n",
            "Epoch  12 Batch  161 / 525  Training Loss  0.027989119291305542\n",
            "Epoch  12 Batch  162 / 525  Training Loss  0.04022273048758507\n",
            "Epoch  12 Batch  163 / 525  Training Loss  0.036301493644714355\n",
            "Epoch  12 Batch  164 / 525  Training Loss  0.04301641136407852\n",
            "Epoch  12 Batch  165 / 525  Training Loss  0.025946572422981262\n",
            "Epoch  12 Batch  166 / 525  Training Loss  0.04848051071166992\n",
            "Epoch  12 Batch  167 / 525  Training Loss  0.03356409817934036\n",
            "Epoch  12 Batch  168 / 525  Training Loss  0.036375004798173904\n",
            "Epoch  12 Batch  169 / 525  Training Loss  0.03530404344201088\n",
            "Epoch  12 Batch  170 / 525  Training Loss  0.032961420714855194\n",
            "Epoch  12 Batch  171 / 525  Training Loss  0.046289727091789246\n",
            "Epoch  12 Batch  172 / 525  Training Loss  0.027127927169203758\n",
            "Epoch  12 Batch  173 / 525  Training Loss  0.028980528935790062\n",
            "Epoch  12 Batch  174 / 525  Training Loss  0.025501709431409836\n",
            "Epoch  12 Batch  175 / 525  Training Loss  0.03269460052251816\n",
            "Epoch  12 Batch  176 / 525  Training Loss  0.02895677648484707\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  12 Batch  177 / 525  Training Loss  0.04103532060980797\n",
            "Epoch  12 Batch  178 / 525  Training Loss  0.023894749581813812\n",
            "Epoch  12 Batch  179 / 525  Training Loss  0.03959871083498001\n",
            "Epoch  12 Batch  180 / 525  Training Loss  0.04120315611362457\n",
            "Epoch  12 Batch  181 / 525  Training Loss  0.0405140146613121\n",
            "Epoch  12 Batch  182 / 525  Training Loss  0.051199860870838165\n",
            "Epoch  12 Batch  183 / 525  Training Loss  0.029858145862817764\n",
            "Epoch  12 Batch  184 / 525  Training Loss  0.04999920353293419\n",
            "Epoch  12 Batch  185 / 525  Training Loss  0.04800198972225189\n",
            "Epoch  12 Batch  186 / 525  Training Loss  0.03154464438557625\n",
            "Epoch  12 Batch  187 / 525  Training Loss  0.03451800346374512\n",
            "Epoch  12 Batch  188 / 525  Training Loss  0.04008673503994942\n",
            "Epoch  12 Batch  189 / 525  Training Loss  0.038429033011198044\n",
            "Epoch  12 Batch  190 / 525  Training Loss  0.046748314052820206\n",
            "Epoch  12 Batch  191 / 525  Training Loss  0.05433511734008789\n",
            "Epoch  12 Batch  192 / 525  Training Loss  0.03708774596452713\n",
            "Epoch  12 Batch  193 / 525  Training Loss  0.05212988704442978\n",
            "Epoch  12 Batch  194 / 525  Training Loss  0.04173935204744339\n",
            "Epoch  12 Batch  195 / 525  Training Loss  0.030476044863462448\n",
            "Epoch  12 Batch  196 / 525  Training Loss  0.05507587268948555\n",
            "Epoch  12 Batch  197 / 525  Training Loss  0.03743553161621094\n",
            "Epoch  12 Batch  198 / 525  Training Loss  0.03418242186307907\n",
            "Epoch  12 Batch  199 / 525  Training Loss  0.036742933094501495\n",
            "Epoch  12 Batch  200 / 525  Training Loss  0.031106268987059593\n",
            "Epoch  12 Batch  201 / 525  Training Loss  0.03736953064799309\n",
            "Epoch  12 Batch  202 / 525  Training Loss  0.04446982592344284\n",
            "Epoch  12 Batch  203 / 525  Training Loss  0.02829497680068016\n",
            "Epoch  12 Batch  204 / 525  Training Loss  0.022069161757826805\n",
            "Epoch  12 Batch  205 / 525  Training Loss  0.030930519104003906\n",
            "Epoch  12 Batch  206 / 525  Training Loss  0.034853495657444\n",
            "Epoch  12 Batch  207 / 525  Training Loss  0.046030085533857346\n",
            "Epoch  12 Batch  208 / 525  Training Loss  0.04379533603787422\n",
            "Epoch  12 Batch  209 / 525  Training Loss  0.032842934131622314\n",
            "Epoch  12 Batch  210 / 525  Training Loss  0.032121628522872925\n",
            "Epoch  12 Batch  211 / 525  Training Loss  0.02926817536354065\n",
            "Epoch  12 Batch  212 / 525  Training Loss  0.03367037698626518\n",
            "Epoch  12 Batch  213 / 525  Training Loss  0.03578319773077965\n",
            "Epoch  12 Batch  214 / 525  Training Loss  0.03627211973071098\n",
            "Epoch  12 Batch  215 / 525  Training Loss  0.02771054580807686\n",
            "Epoch  12 Batch  216 / 525  Training Loss  0.037717677652835846\n",
            "Epoch  12 Batch  217 / 525  Training Loss  0.04186226800084114\n",
            "Epoch  12 Batch  218 / 525  Training Loss  0.03176233544945717\n",
            "Epoch  12 Batch  219 / 525  Training Loss  0.04077005758881569\n",
            "Epoch  12 Batch  220 / 525  Training Loss  0.039736174046993256\n",
            "Epoch  12 Batch  221 / 525  Training Loss  0.03481121361255646\n",
            "Epoch  12 Batch  222 / 525  Training Loss  0.035877399146556854\n",
            "Epoch  12 Batch  223 / 525  Training Loss  0.0316687636077404\n",
            "Epoch  12 Batch  224 / 525  Training Loss  0.039141908288002014\n",
            "Epoch  12 Batch  225 / 525  Training Loss  0.030117660760879517\n",
            "Epoch  12 Batch  226 / 525  Training Loss  0.05147052928805351\n",
            "Epoch  12 Batch  227 / 525  Training Loss  0.027838122099637985\n",
            "Epoch  12 Batch  228 / 525  Training Loss  0.04548414424061775\n",
            "Epoch  12 Batch  229 / 525  Training Loss  0.03819802775979042\n",
            "Epoch  12 Batch  230 / 525  Training Loss  0.03862005099654198\n",
            "Epoch  12 Batch  231 / 525  Training Loss  0.021667877212166786\n",
            "Epoch  12 Batch  232 / 525  Training Loss  0.032229531556367874\n",
            "Epoch  12 Batch  233 / 525  Training Loss  0.02746303379535675\n",
            "Epoch  12 Batch  234 / 525  Training Loss  0.03215600550174713\n",
            "Epoch  12 Batch  235 / 525  Training Loss  0.030483046546578407\n",
            "Epoch  12 Batch  236 / 525  Training Loss  0.03767247870564461\n",
            "Epoch  12 Batch  237 / 525  Training Loss  0.03474307432770729\n",
            "Epoch  12 Batch  238 / 525  Training Loss  0.0340004563331604\n",
            "Epoch  12 Batch  239 / 525  Training Loss  0.04849272221326828\n",
            "Epoch  12 Batch  240 / 525  Training Loss  0.02292434498667717\n",
            "Epoch  12 Batch  241 / 525  Training Loss  0.028652528300881386\n",
            "Epoch  12 Batch  242 / 525  Training Loss  0.037461619824171066\n",
            "Epoch  12 Batch  243 / 525  Training Loss  0.028960874304175377\n",
            "Epoch  12 Batch  244 / 525  Training Loss  0.03746624290943146\n",
            "Epoch  12 Batch  245 / 525  Training Loss  0.03152921050786972\n",
            "Epoch  12 Batch  246 / 525  Training Loss  0.0428890585899353\n",
            "Epoch  12 Batch  247 / 525  Training Loss  0.030307743698358536\n",
            "Epoch  12 Batch  248 / 525  Training Loss  0.03574252128601074\n",
            "Epoch  12 Batch  249 / 525  Training Loss  0.03429720923304558\n",
            "Epoch  12 Batch  250 / 525  Training Loss  0.04442812129855156\n",
            "Epoch  12 Batch  251 / 525  Training Loss  0.04680966958403587\n",
            "Epoch  12 Batch  252 / 525  Training Loss  0.026446467265486717\n",
            "Epoch  12 Batch  253 / 525  Training Loss  0.0656336098909378\n",
            "Epoch  12 Batch  254 / 525  Training Loss  0.042046919465065\n",
            "Epoch  12 Batch  255 / 525  Training Loss  0.036898016929626465\n",
            "Epoch  12 Batch  256 / 525  Training Loss  0.04260920733213425\n",
            "Epoch  12 Batch  257 / 525  Training Loss  0.031340938061475754\n",
            "Epoch  12 Batch  258 / 525  Training Loss  0.03213999792933464\n",
            "Epoch  12 Batch  259 / 525  Training Loss  0.04092264175415039\n",
            "Epoch  12 Batch  260 / 525  Training Loss  0.05457623675465584\n",
            "Epoch  12 Batch  261 / 525  Training Loss  0.03440912067890167\n",
            "Epoch  12 Batch  262 / 525  Training Loss  0.030838286504149437\n",
            "Epoch  12 Batch  263 / 525  Training Loss  0.04115138202905655\n",
            "Epoch  12 Batch  264 / 525  Training Loss  0.03706322982907295\n",
            "Epoch  12 Batch  265 / 525  Training Loss  0.03888470679521561\n",
            "Epoch  12 Batch  266 / 525  Training Loss  0.03975173458456993\n",
            "Epoch  12 Batch  267 / 525  Training Loss  0.026432152837514877\n",
            "Epoch  12 Batch  268 / 525  Training Loss  0.03803760185837746\n",
            "Epoch  12 Batch  269 / 525  Training Loss  0.03487754240632057\n",
            "Epoch  12 Batch  270 / 525  Training Loss  0.04824983328580856\n",
            "Epoch  12 Batch  271 / 525  Training Loss  0.044998109340667725\n",
            "Epoch  12 Batch  272 / 525  Training Loss  0.03478996828198433\n",
            "Epoch  12 Batch  273 / 525  Training Loss  0.04370247572660446\n",
            "Epoch  12 Batch  274 / 525  Training Loss  0.041025981307029724\n",
            "Epoch  12 Batch  275 / 525  Training Loss  0.038648754358291626\n",
            "Epoch  12 Batch  276 / 525  Training Loss  0.03781204670667648\n",
            "Epoch  12 Batch  277 / 525  Training Loss  0.03301463648676872\n",
            "Epoch  12 Batch  278 / 525  Training Loss  0.034847550094127655\n",
            "Epoch  12 Batch  279 / 525  Training Loss  0.0313115119934082\n",
            "Epoch  12 Batch  280 / 525  Training Loss  0.030264172703027725\n",
            "Epoch  12 Batch  281 / 525  Training Loss  0.031001683324575424\n",
            "Epoch  12 Batch  282 / 525  Training Loss  0.032900117337703705\n",
            "Epoch  12 Batch  283 / 525  Training Loss  0.032678160816431046\n",
            "Epoch  12 Batch  284 / 525  Training Loss  0.036362893879413605\n",
            "Epoch  12 Batch  285 / 525  Training Loss  0.0672871470451355\n",
            "Epoch  12 Batch  286 / 525  Training Loss  0.035485316067934036\n",
            "Epoch  12 Batch  287 / 525  Training Loss  0.03263187035918236\n",
            "Epoch  12 Batch  288 / 525  Training Loss  0.0355444997549057\n",
            "Epoch  12 Batch  289 / 525  Training Loss  0.0295577235519886\n",
            "Epoch  12 Batch  290 / 525  Training Loss  0.03305468708276749\n",
            "Epoch  12 Batch  291 / 525  Training Loss  0.03742092475295067\n",
            "Epoch  12 Batch  292 / 525  Training Loss  0.04162956029176712\n",
            "Epoch  12 Batch  293 / 525  Training Loss  0.023402411490678787\n",
            "Epoch  12 Batch  294 / 525  Training Loss  0.04960496351122856\n",
            "Epoch  12 Batch  295 / 525  Training Loss  0.047551389783620834\n",
            "Epoch  12 Batch  296 / 525  Training Loss  0.027359798550605774\n",
            "Epoch  12 Batch  297 / 525  Training Loss  0.02588273026049137\n",
            "Epoch  12 Batch  298 / 525  Training Loss  0.03261205181479454\n",
            "Epoch  12 Batch  299 / 525  Training Loss  0.04678432643413544\n",
            "Epoch  12 Batch  300 / 525  Training Loss  0.03545834869146347\n",
            "Epoch  12 Batch  301 / 525  Training Loss  0.05692882463335991\n",
            "Epoch  12 Batch  302 / 525  Training Loss  0.05026048421859741\n",
            "Epoch  12 Batch  303 / 525  Training Loss  0.029776299372315407\n",
            "Epoch  12 Batch  304 / 525  Training Loss  0.04167177528142929\n",
            "Epoch  12 Batch  305 / 525  Training Loss  0.04072142392396927\n",
            "Epoch  12 Batch  306 / 525  Training Loss  0.029871800914406776\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  12 Batch  307 / 525  Training Loss  0.03231611102819443\n",
            "Epoch  12 Batch  308 / 525  Training Loss  0.03809932619333267\n",
            "Epoch  12 Batch  309 / 525  Training Loss  0.02579633519053459\n",
            "Epoch  12 Batch  310 / 525  Training Loss  0.04361208528280258\n",
            "Epoch  12 Batch  311 / 525  Training Loss  0.03248104825615883\n",
            "Epoch  12 Batch  312 / 525  Training Loss  0.021611109375953674\n",
            "Epoch  12 Batch  313 / 525  Training Loss  0.01842999830842018\n",
            "Epoch  12 Batch  314 / 525  Training Loss  0.03697416931390762\n",
            "Epoch  12 Batch  315 / 525  Training Loss  0.04863298311829567\n",
            "Epoch  12 Batch  316 / 525  Training Loss  0.045613739639520645\n",
            "Epoch  12 Batch  317 / 525  Training Loss  0.023878250271081924\n",
            "Epoch  12 Batch  318 / 525  Training Loss  0.04866701364517212\n",
            "Epoch  12 Batch  319 / 525  Training Loss  0.03484338894486427\n",
            "Epoch  12 Batch  320 / 525  Training Loss  0.043265871703624725\n",
            "Epoch  12 Batch  321 / 525  Training Loss  0.025693129748106003\n",
            "Epoch  12 Batch  322 / 525  Training Loss  0.03653234615921974\n",
            "Epoch  12 Batch  323 / 525  Training Loss  0.031847938895225525\n",
            "Epoch  12 Batch  324 / 525  Training Loss  0.04517839103937149\n",
            "Epoch  12 Batch  325 / 525  Training Loss  0.03761470690369606\n",
            "Epoch  12 Batch  326 / 525  Training Loss  0.02741384506225586\n",
            "Epoch  12 Batch  327 / 525  Training Loss  0.03883979469537735\n",
            "Epoch  12 Batch  328 / 525  Training Loss  0.04510657489299774\n",
            "Epoch  12 Batch  329 / 525  Training Loss  0.03633519634604454\n",
            "Epoch  12 Batch  330 / 525  Training Loss  0.0351373627781868\n",
            "Epoch  12 Batch  331 / 525  Training Loss  0.029181499034166336\n",
            "Epoch  12 Batch  332 / 525  Training Loss  0.048871010541915894\n",
            "Epoch  12 Batch  333 / 525  Training Loss  0.03621440380811691\n",
            "Epoch  12 Batch  334 / 525  Training Loss  0.05686670541763306\n",
            "Epoch  12 Batch  335 / 525  Training Loss  0.02875697612762451\n",
            "Epoch  12 Batch  336 / 525  Training Loss  0.04289401322603226\n",
            "Epoch  12 Batch  337 / 525  Training Loss  0.05629346892237663\n",
            "Epoch  12 Batch  338 / 525  Training Loss  0.03775778412818909\n",
            "Epoch  12 Batch  339 / 525  Training Loss  0.03134326636791229\n",
            "Epoch  12 Batch  340 / 525  Training Loss  0.03925487399101257\n",
            "Epoch  12 Batch  341 / 525  Training Loss  0.027585521340370178\n",
            "Epoch  12 Batch  342 / 525  Training Loss  0.035649728029966354\n",
            "Epoch  12 Batch  343 / 525  Training Loss  0.029181379824876785\n",
            "Epoch  12 Batch  344 / 525  Training Loss  0.025729890912771225\n",
            "Epoch  12 Batch  345 / 525  Training Loss  0.042759981006383896\n",
            "Epoch  12 Batch  346 / 525  Training Loss  0.04125964269042015\n",
            "Epoch  12 Batch  347 / 525  Training Loss  0.04050881788134575\n",
            "Epoch  12 Batch  348 / 525  Training Loss  0.03710665926337242\n",
            "Epoch  12 Batch  349 / 525  Training Loss  0.03435036167502403\n",
            "Epoch  12 Batch  350 / 525  Training Loss  0.04340924695134163\n",
            "Epoch  12 Batch  351 / 525  Training Loss  0.0380471833050251\n",
            "Epoch  12 Batch  352 / 525  Training Loss  0.04353097826242447\n",
            "Epoch  12 Batch  353 / 525  Training Loss  0.04949718341231346\n",
            "Epoch  12 Batch  354 / 525  Training Loss  0.032296665012836456\n",
            "Epoch  12 Batch  355 / 525  Training Loss  0.06481501460075378\n",
            "Epoch  12 Batch  356 / 525  Training Loss  0.05560048669576645\n",
            "Epoch  12 Batch  357 / 525  Training Loss  0.030182605609297752\n",
            "Epoch  12 Batch  358 / 525  Training Loss  0.03985050693154335\n",
            "Epoch  12 Batch  359 / 525  Training Loss  0.04363688454031944\n",
            "Epoch  12 Batch  360 / 525  Training Loss  0.032417960464954376\n",
            "Epoch  12 Batch  361 / 525  Training Loss  0.04037028178572655\n",
            "Epoch  12 Batch  362 / 525  Training Loss  0.02812911942601204\n",
            "Epoch  12 Batch  363 / 525  Training Loss  0.040924057364463806\n",
            "Epoch  12 Batch  364 / 525  Training Loss  0.02629305049777031\n",
            "Epoch  12 Batch  365 / 525  Training Loss  0.02427172288298607\n",
            "Epoch  12 Batch  366 / 525  Training Loss  0.027184396982192993\n",
            "Epoch  12 Batch  367 / 525  Training Loss  0.02442484349012375\n",
            "Epoch  12 Batch  368 / 525  Training Loss  0.023643162101507187\n",
            "Epoch  12 Batch  369 / 525  Training Loss  0.04590669274330139\n",
            "Epoch  12 Batch  370 / 525  Training Loss  0.0384981706738472\n",
            "Epoch  12 Batch  371 / 525  Training Loss  0.04578135162591934\n",
            "Epoch  12 Batch  372 / 525  Training Loss  0.03394169360399246\n",
            "Epoch  12 Batch  373 / 525  Training Loss  0.031049072742462158\n",
            "Epoch  12 Batch  374 / 525  Training Loss  0.025287490338087082\n",
            "Epoch  12 Batch  375 / 525  Training Loss  0.03014751337468624\n",
            "Epoch  12 Batch  376 / 525  Training Loss  0.04842355102300644\n",
            "Epoch  12 Batch  377 / 525  Training Loss  0.04049207642674446\n",
            "Epoch  12 Batch  378 / 525  Training Loss  0.038675691932439804\n",
            "Epoch  12 Batch  379 / 525  Training Loss  0.03836355358362198\n",
            "Epoch  12 Batch  380 / 525  Training Loss  0.051192570477724075\n",
            "Epoch  12 Batch  381 / 525  Training Loss  0.030868342146277428\n",
            "Epoch  12 Batch  382 / 525  Training Loss  0.02485116943717003\n",
            "Epoch  12 Batch  383 / 525  Training Loss  0.054588574916124344\n",
            "Epoch  12 Batch  384 / 525  Training Loss  0.055497366935014725\n",
            "Epoch  12 Batch  385 / 525  Training Loss  0.03720444440841675\n",
            "Epoch  12 Batch  386 / 525  Training Loss  0.03462499752640724\n",
            "Epoch  12 Batch  387 / 525  Training Loss  0.03831222653388977\n",
            "Epoch  12 Batch  388 / 525  Training Loss  0.0318533256649971\n",
            "Epoch  12 Batch  389 / 525  Training Loss  0.031124329194426537\n",
            "Epoch  12 Batch  390 / 525  Training Loss  0.04829591140151024\n",
            "Epoch  12 Batch  391 / 525  Training Loss  0.03863566368818283\n",
            "Epoch  12 Batch  392 / 525  Training Loss  0.027589473873376846\n",
            "Epoch  12 Batch  393 / 525  Training Loss  0.032068975269794464\n",
            "Epoch  12 Batch  394 / 525  Training Loss  0.04602853208780289\n",
            "Epoch  12 Batch  395 / 525  Training Loss  0.04528816044330597\n",
            "Epoch  12 Batch  396 / 525  Training Loss  0.03313659876585007\n",
            "Epoch  12 Batch  397 / 525  Training Loss  0.03858669474720955\n",
            "Epoch  12 Batch  398 / 525  Training Loss  0.04146706312894821\n",
            "Epoch  12 Batch  399 / 525  Training Loss  0.03440281003713608\n",
            "Epoch  12 Batch  400 / 525  Training Loss  0.042665526270866394\n",
            "Epoch  12 Batch  401 / 525  Training Loss  0.03351933881640434\n",
            "Epoch  12 Batch  402 / 525  Training Loss  0.03831855207681656\n",
            "Epoch  12 Batch  403 / 525  Training Loss  0.03265809267759323\n",
            "Epoch  12 Batch  404 / 525  Training Loss  0.05132834240794182\n",
            "Epoch  12 Batch  405 / 525  Training Loss  0.02669568359851837\n",
            "Epoch  12 Batch  406 / 525  Training Loss  0.0413319393992424\n",
            "Epoch  12 Batch  407 / 525  Training Loss  0.03643268719315529\n",
            "Epoch  12 Batch  408 / 525  Training Loss  0.02949748933315277\n",
            "Epoch  12 Batch  409 / 525  Training Loss  0.04383300989866257\n",
            "Epoch  12 Batch  410 / 525  Training Loss  0.04295923560857773\n",
            "Epoch  12 Batch  411 / 525  Training Loss  0.027708372101187706\n",
            "Epoch  12 Batch  412 / 525  Training Loss  0.034903399646282196\n",
            "Epoch  12 Batch  413 / 525  Training Loss  0.04026084765791893\n",
            "Epoch  12 Batch  414 / 525  Training Loss  0.026515189558267593\n",
            "Epoch  12 Batch  415 / 525  Training Loss  0.031439293175935745\n",
            "Epoch  12 Batch  416 / 525  Training Loss  0.036390818655490875\n",
            "Epoch  12 Batch  417 / 525  Training Loss  0.034153178334236145\n",
            "Epoch  12 Batch  418 / 525  Training Loss  0.05062531679868698\n",
            "Epoch  12 Batch  419 / 525  Training Loss  0.03708156198263168\n",
            "Epoch  12 Batch  420 / 525  Training Loss  0.033995479345321655\n",
            "Epoch  12 Batch  421 / 525  Training Loss  0.03095940686762333\n",
            "Epoch  12 Batch  422 / 525  Training Loss  0.0500725582242012\n",
            "Epoch  12 Batch  423 / 525  Training Loss  0.04304615035653114\n",
            "Epoch  12 Batch  424 / 525  Training Loss  0.04020633548498154\n",
            "Epoch  12 Batch  425 / 525  Training Loss  0.0413573682308197\n",
            "Epoch  12 Batch  426 / 525  Training Loss  0.027148697525262833\n",
            "Epoch  12 Batch  427 / 525  Training Loss  0.042505837976932526\n",
            "Epoch  12 Batch  428 / 525  Training Loss  0.021062586456537247\n",
            "Epoch  12 Batch  429 / 525  Training Loss  0.03217077627778053\n",
            "Epoch  12 Batch  430 / 525  Training Loss  0.04267210513353348\n",
            "Epoch  12 Batch  431 / 525  Training Loss  0.031276948750019073\n",
            "Epoch  12 Batch  432 / 525  Training Loss  0.03905322402715683\n",
            "Epoch  12 Batch  433 / 525  Training Loss  0.039894185960292816\n",
            "Epoch  12 Batch  434 / 525  Training Loss  0.04180363565683365\n",
            "Epoch  12 Batch  435 / 525  Training Loss  0.04008017107844353\n",
            "Epoch  12 Batch  436 / 525  Training Loss  0.03853059560060501\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  12 Batch  437 / 525  Training Loss  0.03994534909725189\n",
            "Epoch  12 Batch  438 / 525  Training Loss  0.05094828084111214\n",
            "Epoch  12 Batch  439 / 525  Training Loss  0.030215105041861534\n",
            "Epoch  12 Batch  440 / 525  Training Loss  0.04081537574529648\n",
            "Epoch  12 Batch  441 / 525  Training Loss  0.03452184423804283\n",
            "Epoch  12 Batch  442 / 525  Training Loss  0.03491375595331192\n",
            "Epoch  12 Batch  443 / 525  Training Loss  0.031074170023202896\n",
            "Epoch  12 Batch  444 / 525  Training Loss  0.04393583536148071\n",
            "Epoch  12 Batch  445 / 525  Training Loss  0.03800979629158974\n",
            "Epoch  12 Batch  446 / 525  Training Loss  0.03334296494722366\n",
            "Epoch  12 Batch  447 / 525  Training Loss  0.04055808112025261\n",
            "Epoch  12 Batch  448 / 525  Training Loss  0.04941679909825325\n",
            "Epoch  12 Batch  449 / 525  Training Loss  0.03607732802629471\n",
            "Epoch  12 Batch  450 / 525  Training Loss  0.03698279336094856\n",
            "Epoch  12 Batch  451 / 525  Training Loss  0.04484478384256363\n",
            "Epoch  12 Batch  452 / 525  Training Loss  0.04951684921979904\n",
            "Epoch  12 Batch  453 / 525  Training Loss  0.03573411703109741\n",
            "Epoch  12 Batch  454 / 525  Training Loss  0.03761616721749306\n",
            "Epoch  12 Batch  455 / 525  Training Loss  0.021480053663253784\n",
            "Epoch  12 Batch  456 / 525  Training Loss  0.015398917719721794\n",
            "Epoch  12 Batch  457 / 525  Training Loss  0.03628560155630112\n",
            "Epoch  12 Batch  458 / 525  Training Loss  0.035656560212373734\n",
            "Epoch  12 Batch  459 / 525  Training Loss  0.02520270273089409\n",
            "Epoch  12 Batch  460 / 525  Training Loss  0.041676782071590424\n",
            "Epoch  12 Batch  461 / 525  Training Loss  0.02863665297627449\n",
            "Epoch  12 Batch  462 / 525  Training Loss  0.03723948448896408\n",
            "Epoch  12 Batch  463 / 525  Training Loss  0.05277874320745468\n",
            "Epoch  12 Batch  464 / 525  Training Loss  0.041824135929346085\n",
            "Epoch  12 Batch  465 / 525  Training Loss  0.03024875745177269\n",
            "Epoch  12 Batch  466 / 525  Training Loss  0.022548042237758636\n",
            "Epoch  12 Batch  467 / 525  Training Loss  0.051886409521102905\n",
            "Epoch  12 Batch  468 / 525  Training Loss  0.04570910334587097\n",
            "Epoch  12 Batch  469 / 525  Training Loss  0.02656422182917595\n",
            "Epoch  12 Batch  470 / 525  Training Loss  0.04493265599012375\n",
            "Epoch  12 Batch  471 / 525  Training Loss  0.040532901883125305\n",
            "Epoch  12 Batch  472 / 525  Training Loss  0.03802695870399475\n",
            "Epoch  12 Batch  473 / 525  Training Loss  0.03519232198596001\n",
            "Epoch  12 Batch  474 / 525  Training Loss  0.03419817239046097\n",
            "Epoch  12 Batch  475 / 525  Training Loss  0.028861677274107933\n",
            "Epoch  12 Batch  476 / 525  Training Loss  0.04065234214067459\n",
            "Epoch  12 Batch  477 / 525  Training Loss  0.034522589296102524\n",
            "Epoch  12 Batch  478 / 525  Training Loss  0.02858496643602848\n",
            "Epoch  12 Batch  479 / 525  Training Loss  0.04460211098194122\n",
            "Epoch  12 Batch  480 / 525  Training Loss  0.04161844030022621\n",
            "Epoch  12 Batch  481 / 525  Training Loss  0.05809181183576584\n",
            "Epoch  12 Batch  482 / 525  Training Loss  0.05153902247548103\n",
            "Epoch  12 Batch  483 / 525  Training Loss  0.0399012491106987\n",
            "Epoch  12 Batch  484 / 525  Training Loss  0.05316799134016037\n",
            "Epoch  12 Batch  485 / 525  Training Loss  0.04555448144674301\n",
            "Epoch  12 Batch  486 / 525  Training Loss  0.031216716393828392\n",
            "Epoch  12 Batch  487 / 525  Training Loss  0.03677714616060257\n",
            "Epoch  12 Batch  488 / 525  Training Loss  0.04443584755063057\n",
            "Epoch  12 Batch  489 / 525  Training Loss  0.030167877674102783\n",
            "Epoch  12 Batch  490 / 525  Training Loss  0.028715679422020912\n",
            "Epoch  12 Batch  491 / 525  Training Loss  0.04528967663645744\n",
            "Epoch  12 Batch  492 / 525  Training Loss  0.021960904821753502\n",
            "Epoch  12 Batch  493 / 525  Training Loss  0.018291903659701347\n",
            "Epoch  12 Batch  494 / 525  Training Loss  0.02865528129041195\n",
            "Epoch  12 Batch  495 / 525  Training Loss  0.04169219732284546\n",
            "Epoch  12 Batch  496 / 525  Training Loss  0.04485369473695755\n",
            "Epoch  12 Batch  497 / 525  Training Loss  0.0426906943321228\n",
            "Epoch  12 Batch  498 / 525  Training Loss  0.03578166291117668\n",
            "Epoch  12 Batch  499 / 525  Training Loss  0.046570390462875366\n",
            "Epoch  12 Batch  500 / 525  Training Loss  0.03121226653456688\n",
            "Epoch  12 Batch  501 / 525  Training Loss  0.030759206041693687\n",
            "Epoch  12 Batch  502 / 525  Training Loss  0.028945010155439377\n",
            "Epoch  12 Batch  503 / 525  Training Loss  0.03658376634120941\n",
            "Epoch  12 Batch  504 / 525  Training Loss  0.03036462888121605\n",
            "Epoch  12 Batch  505 / 525  Training Loss  0.03343141824007034\n",
            "Epoch  12 Batch  506 / 525  Training Loss  0.039737213402986526\n",
            "Epoch  12 Batch  507 / 525  Training Loss  0.05860112980008125\n",
            "Epoch  12 Batch  508 / 525  Training Loss  0.05441216751933098\n",
            "Epoch  12 Batch  509 / 525  Training Loss  0.040307752788066864\n",
            "Epoch  12 Batch  510 / 525  Training Loss  0.03250804916024208\n",
            "Epoch  12 Batch  511 / 525  Training Loss  0.04367116093635559\n",
            "Epoch  12 Batch  512 / 525  Training Loss  0.04104876145720482\n",
            "Epoch  12 Batch  513 / 525  Training Loss  0.03920053690671921\n",
            "Epoch  12 Batch  514 / 525  Training Loss  0.027226706966757774\n",
            "Epoch  12 Batch  515 / 525  Training Loss  0.029238039627671242\n",
            "Epoch  12 Batch  516 / 525  Training Loss  0.03045491874217987\n",
            "Epoch  12 Batch  517 / 525  Training Loss  0.026564840227365494\n",
            "Epoch  12 Batch  518 / 525  Training Loss  0.033996038138866425\n",
            "Epoch  12 Batch  519 / 525  Training Loss  0.04270229861140251\n",
            "Epoch  12 Batch  520 / 525  Training Loss  0.03642047941684723\n",
            "Epoch  12 Batch  521 / 525  Training Loss  0.05438769981265068\n",
            "Epoch  12 Batch  522 / 525  Training Loss  0.061687447130680084\n",
            "Epoch  12 Batch  523 / 525  Training Loss  0.03634599223732948\n",
            "Epoch  12 Batch  524 / 525  Training Loss  0.019653379917144775\n",
            "  13    |    -    |   0.035840   | 51.275000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 13\n",
            "Epoch  13 Batch  0 / 525  Training Loss  0.0257802065461874\n",
            "Epoch  13 Batch  1 / 525  Training Loss  0.024758994579315186\n",
            "Epoch  13 Batch  2 / 525  Training Loss  0.0235911812633276\n",
            "Epoch  13 Batch  3 / 525  Training Loss  0.019730787724256516\n",
            "Epoch  13 Batch  4 / 525  Training Loss  0.02559453248977661\n",
            "Epoch  13 Batch  5 / 525  Training Loss  0.03199762850999832\n",
            "Epoch  13 Batch  6 / 525  Training Loss  0.039853401482105255\n",
            "Epoch  13 Batch  7 / 525  Training Loss  0.028775081038475037\n",
            "Epoch  13 Batch  8 / 525  Training Loss  0.03295205906033516\n",
            "Epoch  13 Batch  9 / 525  Training Loss  0.028235524892807007\n",
            "Epoch  13 Batch  10 / 525  Training Loss  0.033199701458215714\n",
            "Epoch  13 Batch  11 / 525  Training Loss  0.022723861038684845\n",
            "Epoch  13 Batch  12 / 525  Training Loss  0.027040835469961166\n",
            "Epoch  13 Batch  13 / 525  Training Loss  0.03380023315548897\n",
            "Epoch  13 Batch  14 / 525  Training Loss  0.025911007076501846\n",
            "Epoch  13 Batch  15 / 525  Training Loss  0.02620985172688961\n",
            "Epoch  13 Batch  16 / 525  Training Loss  0.02189749851822853\n",
            "Epoch  13 Batch  17 / 525  Training Loss  0.026778435334563255\n",
            "Epoch  13 Batch  18 / 525  Training Loss  0.027592357248067856\n",
            "Epoch  13 Batch  19 / 525  Training Loss  0.025693152099847794\n",
            "Epoch  13 Batch  20 / 525  Training Loss  0.036045610904693604\n",
            "Epoch  13 Batch  21 / 525  Training Loss  0.020287424325942993\n",
            "Epoch  13 Batch  22 / 525  Training Loss  0.02026895247399807\n",
            "Epoch  13 Batch  23 / 525  Training Loss  0.025701221078634262\n",
            "Epoch  13 Batch  24 / 525  Training Loss  0.03390952944755554\n",
            "Epoch  13 Batch  25 / 525  Training Loss  0.03266776353120804\n",
            "Epoch  13 Batch  26 / 525  Training Loss  0.02065122127532959\n",
            "Epoch  13 Batch  27 / 525  Training Loss  0.02387107163667679\n",
            "Epoch  13 Batch  28 / 525  Training Loss  0.027212029322981834\n",
            "Epoch  13 Batch  29 / 525  Training Loss  0.022121716290712357\n",
            "Epoch  13 Batch  30 / 525  Training Loss  0.022829299792647362\n",
            "Epoch  13 Batch  31 / 525  Training Loss  0.022334028035402298\n",
            "Epoch  13 Batch  32 / 525  Training Loss  0.02772395871579647\n",
            "Epoch  13 Batch  33 / 525  Training Loss  0.01837536133825779\n",
            "Epoch  13 Batch  34 / 525  Training Loss  0.03261515125632286\n",
            "Epoch  13 Batch  35 / 525  Training Loss  0.04151872545480728\n",
            "Epoch  13 Batch  36 / 525  Training Loss  0.021021578460931778\n",
            "Epoch  13 Batch  37 / 525  Training Loss  0.02884114719927311\n",
            "Epoch  13 Batch  38 / 525  Training Loss  0.028565216809511185\n",
            "Epoch  13 Batch  39 / 525  Training Loss  0.016938772052526474\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  13 Batch  40 / 525  Training Loss  0.03112422488629818\n",
            "Epoch  13 Batch  41 / 525  Training Loss  0.02444354258477688\n",
            "Epoch  13 Batch  42 / 525  Training Loss  0.02180301956832409\n",
            "Epoch  13 Batch  43 / 525  Training Loss  0.018195051699876785\n",
            "Epoch  13 Batch  44 / 525  Training Loss  0.014818862080574036\n",
            "Epoch  13 Batch  45 / 525  Training Loss  0.030476409941911697\n",
            "Epoch  13 Batch  46 / 525  Training Loss  0.02426699921488762\n",
            "Epoch  13 Batch  47 / 525  Training Loss  0.020928742364048958\n",
            "Epoch  13 Batch  48 / 525  Training Loss  0.04033466801047325\n",
            "Epoch  13 Batch  49 / 525  Training Loss  0.022319447249174118\n",
            "Epoch  13 Batch  50 / 525  Training Loss  0.031098609790205956\n",
            "Epoch  13 Batch  51 / 525  Training Loss  0.022161956876516342\n",
            "Epoch  13 Batch  52 / 525  Training Loss  0.026489023119211197\n",
            "Epoch  13 Batch  53 / 525  Training Loss  0.025009239092469215\n",
            "Epoch  13 Batch  54 / 525  Training Loss  0.02639407478272915\n",
            "Epoch  13 Batch  55 / 525  Training Loss  0.025316257029771805\n",
            "Epoch  13 Batch  56 / 525  Training Loss  0.020291071385145187\n",
            "Epoch  13 Batch  57 / 525  Training Loss  0.021787013858556747\n",
            "Epoch  13 Batch  58 / 525  Training Loss  0.0347813256084919\n",
            "Epoch  13 Batch  59 / 525  Training Loss  0.02818576991558075\n",
            "Epoch  13 Batch  60 / 525  Training Loss  0.02605332061648369\n",
            "Epoch  13 Batch  61 / 525  Training Loss  0.0334116593003273\n",
            "Epoch  13 Batch  62 / 525  Training Loss  0.029838159680366516\n",
            "Epoch  13 Batch  63 / 525  Training Loss  0.019195016473531723\n",
            "Epoch  13 Batch  64 / 525  Training Loss  0.04251658171415329\n",
            "Epoch  13 Batch  65 / 525  Training Loss  0.024605903774499893\n",
            "Epoch  13 Batch  66 / 525  Training Loss  0.029449310153722763\n",
            "Epoch  13 Batch  67 / 525  Training Loss  0.028526505455374718\n",
            "Epoch  13 Batch  68 / 525  Training Loss  0.023134978488087654\n",
            "Epoch  13 Batch  69 / 525  Training Loss  0.024027666077017784\n",
            "Epoch  13 Batch  70 / 525  Training Loss  0.02866513654589653\n",
            "Epoch  13 Batch  71 / 525  Training Loss  0.03241519629955292\n",
            "Epoch  13 Batch  72 / 525  Training Loss  0.03465353697538376\n",
            "Epoch  13 Batch  73 / 525  Training Loss  0.023719007149338722\n",
            "Epoch  13 Batch  74 / 525  Training Loss  0.021763712167739868\n",
            "Epoch  13 Batch  75 / 525  Training Loss  0.022034136578440666\n",
            "Epoch  13 Batch  76 / 525  Training Loss  0.041519977152347565\n",
            "Epoch  13 Batch  77 / 525  Training Loss  0.024714741855859756\n",
            "Epoch  13 Batch  78 / 525  Training Loss  0.0250334981828928\n",
            "Epoch  13 Batch  79 / 525  Training Loss  0.019990095868706703\n",
            "Epoch  13 Batch  80 / 525  Training Loss  0.01792956329882145\n",
            "Epoch  13 Batch  81 / 525  Training Loss  0.02846834622323513\n",
            "Epoch  13 Batch  82 / 525  Training Loss  0.0182334091514349\n",
            "Epoch  13 Batch  83 / 525  Training Loss  0.02818954922258854\n",
            "Epoch  13 Batch  84 / 525  Training Loss  0.016596036031842232\n",
            "Epoch  13 Batch  85 / 525  Training Loss  0.030018985271453857\n",
            "Epoch  13 Batch  86 / 525  Training Loss  0.030571838840842247\n",
            "Epoch  13 Batch  87 / 525  Training Loss  0.02014324441552162\n",
            "Epoch  13 Batch  88 / 525  Training Loss  0.041768770664930344\n",
            "Epoch  13 Batch  89 / 525  Training Loss  0.028322821483016014\n",
            "Epoch  13 Batch  90 / 525  Training Loss  0.015325434505939484\n",
            "Epoch  13 Batch  91 / 525  Training Loss  0.018204230815172195\n",
            "Epoch  13 Batch  92 / 525  Training Loss  0.02489522472023964\n",
            "Epoch  13 Batch  93 / 525  Training Loss  0.02913283184170723\n",
            "Epoch  13 Batch  94 / 525  Training Loss  0.028886327520012856\n",
            "Epoch  13 Batch  95 / 525  Training Loss  0.02521928772330284\n",
            "Epoch  13 Batch  96 / 525  Training Loss  0.03590566664934158\n",
            "Epoch  13 Batch  97 / 525  Training Loss  0.03233562782406807\n",
            "Epoch  13 Batch  98 / 525  Training Loss  0.022856786847114563\n",
            "Epoch  13 Batch  99 / 525  Training Loss  0.033247269690036774\n",
            "Epoch  13 Batch  100 / 525  Training Loss  0.029613753780722618\n",
            "Epoch  13 Batch  101 / 525  Training Loss  0.03091944195330143\n",
            "Epoch  13 Batch  102 / 525  Training Loss  0.03343111649155617\n",
            "Epoch  13 Batch  103 / 525  Training Loss  0.026190856471657753\n",
            "Epoch  13 Batch  104 / 525  Training Loss  0.013809256255626678\n",
            "Epoch  13 Batch  105 / 525  Training Loss  0.01887940987944603\n",
            "Epoch  13 Batch  106 / 525  Training Loss  0.021596958860754967\n",
            "Epoch  13 Batch  107 / 525  Training Loss  0.02042560651898384\n",
            "Epoch  13 Batch  108 / 525  Training Loss  0.034325938671827316\n",
            "Epoch  13 Batch  109 / 525  Training Loss  0.029288753867149353\n",
            "Epoch  13 Batch  110 / 525  Training Loss  0.027091359719634056\n",
            "Epoch  13 Batch  111 / 525  Training Loss  0.014931140467524529\n",
            "Epoch  13 Batch  112 / 525  Training Loss  0.02880561351776123\n",
            "Epoch  13 Batch  113 / 525  Training Loss  0.01972906105220318\n",
            "Epoch  13 Batch  114 / 525  Training Loss  0.03393141180276871\n",
            "Epoch  13 Batch  115 / 525  Training Loss  0.025494808331131935\n",
            "Epoch  13 Batch  116 / 525  Training Loss  0.025137916207313538\n",
            "Epoch  13 Batch  117 / 525  Training Loss  0.030886143445968628\n",
            "Epoch  13 Batch  118 / 525  Training Loss  0.02017548866569996\n",
            "Epoch  13 Batch  119 / 525  Training Loss  0.033619172871112823\n",
            "Epoch  13 Batch  120 / 525  Training Loss  0.03763408213853836\n",
            "Epoch  13 Batch  121 / 525  Training Loss  0.02408715710043907\n",
            "Epoch  13 Batch  122 / 525  Training Loss  0.03347858786582947\n",
            "Epoch  13 Batch  123 / 525  Training Loss  0.020465027540922165\n",
            "Epoch  13 Batch  124 / 525  Training Loss  0.031926319003105164\n",
            "Epoch  13 Batch  125 / 525  Training Loss  0.013786937110126019\n",
            "Epoch  13 Batch  126 / 525  Training Loss  0.017943216487765312\n",
            "Epoch  13 Batch  127 / 525  Training Loss  0.028087148442864418\n",
            "Epoch  13 Batch  128 / 525  Training Loss  0.038907818496227264\n",
            "Epoch  13 Batch  129 / 525  Training Loss  0.029741177335381508\n",
            "Epoch  13 Batch  130 / 525  Training Loss  0.04848144203424454\n",
            "Epoch  13 Batch  131 / 525  Training Loss  0.025978881865739822\n",
            "Epoch  13 Batch  132 / 525  Training Loss  0.035503312945365906\n",
            "Epoch  13 Batch  133 / 525  Training Loss  0.016789201647043228\n",
            "Epoch  13 Batch  134 / 525  Training Loss  0.027047863230109215\n",
            "Epoch  13 Batch  135 / 525  Training Loss  0.04259894788265228\n",
            "Epoch  13 Batch  136 / 525  Training Loss  0.030595874413847923\n",
            "Epoch  13 Batch  137 / 525  Training Loss  0.03729269653558731\n",
            "Epoch  13 Batch  138 / 525  Training Loss  0.019342463463544846\n",
            "Epoch  13 Batch  139 / 525  Training Loss  0.03342920169234276\n",
            "Epoch  13 Batch  140 / 525  Training Loss  0.0363805778324604\n",
            "Epoch  13 Batch  141 / 525  Training Loss  0.028118187561631203\n",
            "Epoch  13 Batch  142 / 525  Training Loss  0.04017375409603119\n",
            "Epoch  13 Batch  143 / 525  Training Loss  0.025589460507035255\n",
            "Epoch  13 Batch  144 / 525  Training Loss  0.02773945964872837\n",
            "Epoch  13 Batch  145 / 525  Training Loss  0.03922301530838013\n",
            "Epoch  13 Batch  146 / 525  Training Loss  0.028009191155433655\n",
            "Epoch  13 Batch  147 / 525  Training Loss  0.03633425012230873\n",
            "Epoch  13 Batch  148 / 525  Training Loss  0.02048015035688877\n",
            "Epoch  13 Batch  149 / 525  Training Loss  0.028110289946198463\n",
            "Epoch  13 Batch  150 / 525  Training Loss  0.02776276506483555\n",
            "Epoch  13 Batch  151 / 525  Training Loss  0.03599992394447327\n",
            "Epoch  13 Batch  152 / 525  Training Loss  0.0247381292283535\n",
            "Epoch  13 Batch  153 / 525  Training Loss  0.029829178005456924\n",
            "Epoch  13 Batch  154 / 525  Training Loss  0.035544268786907196\n",
            "Epoch  13 Batch  155 / 525  Training Loss  0.030552048236131668\n",
            "Epoch  13 Batch  156 / 525  Training Loss  0.020778052508831024\n",
            "Epoch  13 Batch  157 / 525  Training Loss  0.035235486924648285\n",
            "Epoch  13 Batch  158 / 525  Training Loss  0.02512669935822487\n",
            "Epoch  13 Batch  159 / 525  Training Loss  0.03224553167819977\n",
            "Epoch  13 Batch  160 / 525  Training Loss  0.0416904054582119\n",
            "Epoch  13 Batch  161 / 525  Training Loss  0.02701714262366295\n",
            "Epoch  13 Batch  162 / 525  Training Loss  0.02890949510037899\n",
            "Epoch  13 Batch  163 / 525  Training Loss  0.03921186551451683\n",
            "Epoch  13 Batch  164 / 525  Training Loss  0.026284778490662575\n",
            "Epoch  13 Batch  165 / 525  Training Loss  0.03466435521841049\n",
            "Epoch  13 Batch  166 / 525  Training Loss  0.0337722972035408\n",
            "Epoch  13 Batch  167 / 525  Training Loss  0.04292871430516243\n",
            "Epoch  13 Batch  168 / 525  Training Loss  0.03781411796808243\n",
            "Epoch  13 Batch  169 / 525  Training Loss  0.025103654712438583\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  13 Batch  170 / 525  Training Loss  0.020946333184838295\n",
            "Epoch  13 Batch  171 / 525  Training Loss  0.024229608476161957\n",
            "Epoch  13 Batch  172 / 525  Training Loss  0.03066929802298546\n",
            "Epoch  13 Batch  173 / 525  Training Loss  0.0151451351121068\n",
            "Epoch  13 Batch  174 / 525  Training Loss  0.032449621707201004\n",
            "Epoch  13 Batch  175 / 525  Training Loss  0.027287542819976807\n",
            "Epoch  13 Batch  176 / 525  Training Loss  0.024782724678516388\n",
            "Epoch  13 Batch  177 / 525  Training Loss  0.018461918458342552\n",
            "Epoch  13 Batch  178 / 525  Training Loss  0.025967180728912354\n",
            "Epoch  13 Batch  179 / 525  Training Loss  0.022781891748309135\n",
            "Epoch  13 Batch  180 / 525  Training Loss  0.024048063904047012\n",
            "Epoch  13 Batch  181 / 525  Training Loss  0.041719935834407806\n",
            "Epoch  13 Batch  182 / 525  Training Loss  0.046822551637887955\n",
            "Epoch  13 Batch  183 / 525  Training Loss  0.03387932851910591\n",
            "Epoch  13 Batch  184 / 525  Training Loss  0.018401334062218666\n",
            "Epoch  13 Batch  185 / 525  Training Loss  0.026399124413728714\n",
            "Epoch  13 Batch  186 / 525  Training Loss  0.03487634286284447\n",
            "Epoch  13 Batch  187 / 525  Training Loss  0.027502909302711487\n",
            "Epoch  13 Batch  188 / 525  Training Loss  0.04007812589406967\n",
            "Epoch  13 Batch  189 / 525  Training Loss  0.027691835537552834\n",
            "Epoch  13 Batch  190 / 525  Training Loss  0.03276504576206207\n",
            "Epoch  13 Batch  191 / 525  Training Loss  0.03542551398277283\n",
            "Epoch  13 Batch  192 / 525  Training Loss  0.015173422172665596\n",
            "Epoch  13 Batch  193 / 525  Training Loss  0.04272867366671562\n",
            "Epoch  13 Batch  194 / 525  Training Loss  0.024658719077706337\n",
            "Epoch  13 Batch  195 / 525  Training Loss  0.026588812470436096\n",
            "Epoch  13 Batch  196 / 525  Training Loss  0.024864066392183304\n",
            "Epoch  13 Batch  197 / 525  Training Loss  0.029240746051073074\n",
            "Epoch  13 Batch  198 / 525  Training Loss  0.03417952358722687\n",
            "Epoch  13 Batch  199 / 525  Training Loss  0.02372913248836994\n",
            "Epoch  13 Batch  200 / 525  Training Loss  0.026148146018385887\n",
            "Epoch  13 Batch  201 / 525  Training Loss  0.02534104511141777\n",
            "Epoch  13 Batch  202 / 525  Training Loss  0.029333684593439102\n",
            "Epoch  13 Batch  203 / 525  Training Loss  0.01805831678211689\n",
            "Epoch  13 Batch  204 / 525  Training Loss  0.03120126947760582\n",
            "Epoch  13 Batch  205 / 525  Training Loss  0.042008981108665466\n",
            "Epoch  13 Batch  206 / 525  Training Loss  0.029512906447052956\n",
            "Epoch  13 Batch  207 / 525  Training Loss  0.025866439566016197\n",
            "Epoch  13 Batch  208 / 525  Training Loss  0.03206869214773178\n",
            "Epoch  13 Batch  209 / 525  Training Loss  0.016305292025208473\n",
            "Epoch  13 Batch  210 / 525  Training Loss  0.03734617680311203\n",
            "Epoch  13 Batch  211 / 525  Training Loss  0.017348768189549446\n",
            "Epoch  13 Batch  212 / 525  Training Loss  0.02088993787765503\n",
            "Epoch  13 Batch  213 / 525  Training Loss  0.026115376502275467\n",
            "Epoch  13 Batch  214 / 525  Training Loss  0.031827010214328766\n",
            "Epoch  13 Batch  215 / 525  Training Loss  0.019982393831014633\n",
            "Epoch  13 Batch  216 / 525  Training Loss  0.020158376544713974\n",
            "Epoch  13 Batch  217 / 525  Training Loss  0.028973307460546494\n",
            "Epoch  13 Batch  218 / 525  Training Loss  0.02565157413482666\n",
            "Epoch  13 Batch  219 / 525  Training Loss  0.029983511194586754\n",
            "Epoch  13 Batch  220 / 525  Training Loss  0.024979395791888237\n",
            "Epoch  13 Batch  221 / 525  Training Loss  0.03404707461595535\n",
            "Epoch  13 Batch  222 / 525  Training Loss  0.03231306001543999\n",
            "Epoch  13 Batch  223 / 525  Training Loss  0.021321695297956467\n",
            "Epoch  13 Batch  224 / 525  Training Loss  0.032870180904865265\n",
            "Epoch  13 Batch  225 / 525  Training Loss  0.016186680644750595\n",
            "Epoch  13 Batch  226 / 525  Training Loss  0.024401528760790825\n",
            "Epoch  13 Batch  227 / 525  Training Loss  0.030109023675322533\n",
            "Epoch  13 Batch  228 / 525  Training Loss  0.012800527736544609\n",
            "Epoch  13 Batch  229 / 525  Training Loss  0.03692808747291565\n",
            "Epoch  13 Batch  230 / 525  Training Loss  0.02131636068224907\n",
            "Epoch  13 Batch  231 / 525  Training Loss  0.0301282349973917\n",
            "Epoch  13 Batch  232 / 525  Training Loss  0.02952156588435173\n",
            "Epoch  13 Batch  233 / 525  Training Loss  0.018500110134482384\n",
            "Epoch  13 Batch  234 / 525  Training Loss  0.03437107428908348\n",
            "Epoch  13 Batch  235 / 525  Training Loss  0.022803742438554764\n",
            "Epoch  13 Batch  236 / 525  Training Loss  0.03298193961381912\n",
            "Epoch  13 Batch  237 / 525  Training Loss  0.020487310364842415\n",
            "Epoch  13 Batch  238 / 525  Training Loss  0.01591319777071476\n",
            "Epoch  13 Batch  239 / 525  Training Loss  0.03925548121333122\n",
            "Epoch  13 Batch  240 / 525  Training Loss  0.021565532311797142\n",
            "Epoch  13 Batch  241 / 525  Training Loss  0.03457809239625931\n",
            "Epoch  13 Batch  242 / 525  Training Loss  0.033222731202840805\n",
            "Epoch  13 Batch  243 / 525  Training Loss  0.035183582454919815\n",
            "Epoch  13 Batch  244 / 525  Training Loss  0.016556713730096817\n",
            "Epoch  13 Batch  245 / 525  Training Loss  0.041662633419036865\n",
            "Epoch  13 Batch  246 / 525  Training Loss  0.02051415853202343\n",
            "Epoch  13 Batch  247 / 525  Training Loss  0.015839621424674988\n",
            "Epoch  13 Batch  248 / 525  Training Loss  0.03636936843395233\n",
            "Epoch  13 Batch  249 / 525  Training Loss  0.02681804820895195\n",
            "Epoch  13 Batch  250 / 525  Training Loss  0.03232105076313019\n",
            "Epoch  13 Batch  251 / 525  Training Loss  0.048320259898900986\n",
            "Epoch  13 Batch  252 / 525  Training Loss  0.027768123894929886\n",
            "Epoch  13 Batch  253 / 525  Training Loss  0.03186706453561783\n",
            "Epoch  13 Batch  254 / 525  Training Loss  0.0330389142036438\n",
            "Epoch  13 Batch  255 / 525  Training Loss  0.03133848309516907\n",
            "Epoch  13 Batch  256 / 525  Training Loss  0.025137081742286682\n",
            "Epoch  13 Batch  257 / 525  Training Loss  0.02228836715221405\n",
            "Epoch  13 Batch  258 / 525  Training Loss  0.035034313797950745\n",
            "Epoch  13 Batch  259 / 525  Training Loss  0.039249926805496216\n",
            "Epoch  13 Batch  260 / 525  Training Loss  0.039959315210580826\n",
            "Epoch  13 Batch  261 / 525  Training Loss  0.030133971944451332\n",
            "Epoch  13 Batch  262 / 525  Training Loss  0.027624744921922684\n",
            "Epoch  13 Batch  263 / 525  Training Loss  0.03253853693604469\n",
            "Epoch  13 Batch  264 / 525  Training Loss  0.025690555572509766\n",
            "Epoch  13 Batch  265 / 525  Training Loss  0.030276089906692505\n",
            "Epoch  13 Batch  266 / 525  Training Loss  0.036922235041856766\n",
            "Epoch  13 Batch  267 / 525  Training Loss  0.029651233926415443\n",
            "Epoch  13 Batch  268 / 525  Training Loss  0.028824593871831894\n",
            "Epoch  13 Batch  269 / 525  Training Loss  0.02962614595890045\n",
            "Epoch  13 Batch  270 / 525  Training Loss  0.030361656099557877\n",
            "Epoch  13 Batch  271 / 525  Training Loss  0.03269873559474945\n",
            "Epoch  13 Batch  272 / 525  Training Loss  0.029109936207532883\n",
            "Epoch  13 Batch  273 / 525  Training Loss  0.025850096717476845\n",
            "Epoch  13 Batch  274 / 525  Training Loss  0.026715774089097977\n",
            "Epoch  13 Batch  275 / 525  Training Loss  0.023275578394532204\n",
            "Epoch  13 Batch  276 / 525  Training Loss  0.018468093127012253\n",
            "Epoch  13 Batch  277 / 525  Training Loss  0.02224363386631012\n",
            "Epoch  13 Batch  278 / 525  Training Loss  0.03447971120476723\n",
            "Epoch  13 Batch  279 / 525  Training Loss  0.034914348274469376\n",
            "Epoch  13 Batch  280 / 525  Training Loss  0.025596097111701965\n",
            "Epoch  13 Batch  281 / 525  Training Loss  0.028635108843445778\n",
            "Epoch  13 Batch  282 / 525  Training Loss  0.02647697553038597\n",
            "Epoch  13 Batch  283 / 525  Training Loss  0.02516058087348938\n",
            "Epoch  13 Batch  284 / 525  Training Loss  0.023995403200387955\n",
            "Epoch  13 Batch  285 / 525  Training Loss  0.02084392122924328\n",
            "Epoch  13 Batch  286 / 525  Training Loss  0.02648344077169895\n",
            "Epoch  13 Batch  287 / 525  Training Loss  0.03864297270774841\n",
            "Epoch  13 Batch  288 / 525  Training Loss  0.022151794284582138\n",
            "Epoch  13 Batch  289 / 525  Training Loss  0.028804689645767212\n",
            "Epoch  13 Batch  290 / 525  Training Loss  0.02873917482793331\n",
            "Epoch  13 Batch  291 / 525  Training Loss  0.02975510060787201\n",
            "Epoch  13 Batch  292 / 525  Training Loss  0.02886771783232689\n",
            "Epoch  13 Batch  293 / 525  Training Loss  0.018506184220314026\n",
            "Epoch  13 Batch  294 / 525  Training Loss  0.02022252418100834\n",
            "Epoch  13 Batch  295 / 525  Training Loss  0.026850704103708267\n",
            "Epoch  13 Batch  296 / 525  Training Loss  0.02792435511946678\n",
            "Epoch  13 Batch  297 / 525  Training Loss  0.039977043867111206\n",
            "Epoch  13 Batch  298 / 525  Training Loss  0.027700860053300858\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  13 Batch  299 / 525  Training Loss  0.024784009903669357\n",
            "Epoch  13 Batch  300 / 525  Training Loss  0.04013827443122864\n",
            "Epoch  13 Batch  301 / 525  Training Loss  0.024183424189686775\n",
            "Epoch  13 Batch  302 / 525  Training Loss  0.018642384558916092\n",
            "Epoch  13 Batch  303 / 525  Training Loss  0.025688499212265015\n",
            "Epoch  13 Batch  304 / 525  Training Loss  0.028945913538336754\n",
            "Epoch  13 Batch  305 / 525  Training Loss  0.034254372119903564\n",
            "Epoch  13 Batch  306 / 525  Training Loss  0.03354031965136528\n",
            "Epoch  13 Batch  307 / 525  Training Loss  0.026758521795272827\n",
            "Epoch  13 Batch  308 / 525  Training Loss  0.021368233487010002\n",
            "Epoch  13 Batch  309 / 525  Training Loss  0.027407193556427956\n",
            "Epoch  13 Batch  310 / 525  Training Loss  0.04230808466672897\n",
            "Epoch  13 Batch  311 / 525  Training Loss  0.012228138744831085\n",
            "Epoch  13 Batch  312 / 525  Training Loss  0.0333993136882782\n",
            "Epoch  13 Batch  313 / 525  Training Loss  0.028239360079169273\n",
            "Epoch  13 Batch  314 / 525  Training Loss  0.03986814245581627\n",
            "Epoch  13 Batch  315 / 525  Training Loss  0.0243453998118639\n",
            "Epoch  13 Batch  316 / 525  Training Loss  0.026596378535032272\n",
            "Epoch  13 Batch  317 / 525  Training Loss  0.044156450778245926\n",
            "Epoch  13 Batch  318 / 525  Training Loss  0.01822080835700035\n",
            "Epoch  13 Batch  319 / 525  Training Loss  0.02689899131655693\n",
            "Epoch  13 Batch  320 / 525  Training Loss  0.03758423030376434\n",
            "Epoch  13 Batch  321 / 525  Training Loss  0.03092655912041664\n",
            "Epoch  13 Batch  322 / 525  Training Loss  0.02802693285048008\n",
            "Epoch  13 Batch  323 / 525  Training Loss  0.03741418570280075\n",
            "Epoch  13 Batch  324 / 525  Training Loss  0.03033093549311161\n",
            "Epoch  13 Batch  325 / 525  Training Loss  0.026008307933807373\n",
            "Epoch  13 Batch  326 / 525  Training Loss  0.0325709767639637\n",
            "Epoch  13 Batch  327 / 525  Training Loss  0.01751914992928505\n",
            "Epoch  13 Batch  328 / 525  Training Loss  0.034031420946121216\n",
            "Epoch  13 Batch  329 / 525  Training Loss  0.02684895694255829\n",
            "Epoch  13 Batch  330 / 525  Training Loss  0.03799828514456749\n",
            "Epoch  13 Batch  331 / 525  Training Loss  0.026016831398010254\n",
            "Epoch  13 Batch  332 / 525  Training Loss  0.023391535505652428\n",
            "Epoch  13 Batch  333 / 525  Training Loss  0.03602231666445732\n",
            "Epoch  13 Batch  334 / 525  Training Loss  0.029513493180274963\n",
            "Epoch  13 Batch  335 / 525  Training Loss  0.033202946186065674\n",
            "Epoch  13 Batch  336 / 525  Training Loss  0.02399158850312233\n",
            "Epoch  13 Batch  337 / 525  Training Loss  0.026803631335496902\n",
            "Epoch  13 Batch  338 / 525  Training Loss  0.03414077311754227\n",
            "Epoch  13 Batch  339 / 525  Training Loss  0.02914808690547943\n",
            "Epoch  13 Batch  340 / 525  Training Loss  0.0251086987555027\n",
            "Epoch  13 Batch  341 / 525  Training Loss  0.026069235056638718\n",
            "Epoch  13 Batch  342 / 525  Training Loss  0.02073095552623272\n",
            "Epoch  13 Batch  343 / 525  Training Loss  0.016271080821752548\n",
            "Epoch  13 Batch  344 / 525  Training Loss  0.029768679291009903\n",
            "Epoch  13 Batch  345 / 525  Training Loss  0.03802679106593132\n",
            "Epoch  13 Batch  346 / 525  Training Loss  0.028363710269331932\n",
            "Epoch  13 Batch  347 / 525  Training Loss  0.032910339534282684\n",
            "Epoch  13 Batch  348 / 525  Training Loss  0.022106491029262543\n",
            "Epoch  13 Batch  349 / 525  Training Loss  0.02329506352543831\n",
            "Epoch  13 Batch  350 / 525  Training Loss  0.035088904201984406\n",
            "Epoch  13 Batch  351 / 525  Training Loss  0.04063253849744797\n",
            "Epoch  13 Batch  352 / 525  Training Loss  0.025268878787755966\n",
            "Epoch  13 Batch  353 / 525  Training Loss  0.022960800677537918\n",
            "Epoch  13 Batch  354 / 525  Training Loss  0.02689407765865326\n",
            "Epoch  13 Batch  355 / 525  Training Loss  0.02935464307665825\n",
            "Epoch  13 Batch  356 / 525  Training Loss  0.0356898233294487\n",
            "Epoch  13 Batch  357 / 525  Training Loss  0.02327529527246952\n",
            "Epoch  13 Batch  358 / 525  Training Loss  0.02565237320959568\n",
            "Epoch  13 Batch  359 / 525  Training Loss  0.02819071337580681\n",
            "Epoch  13 Batch  360 / 525  Training Loss  0.03768053650856018\n",
            "Epoch  13 Batch  361 / 525  Training Loss  0.016934456303715706\n",
            "Epoch  13 Batch  362 / 525  Training Loss  0.030842874199151993\n",
            "Epoch  13 Batch  363 / 525  Training Loss  0.039557524025440216\n",
            "Epoch  13 Batch  364 / 525  Training Loss  0.038201797753572464\n",
            "Epoch  13 Batch  365 / 525  Training Loss  0.025231042876839638\n",
            "Epoch  13 Batch  366 / 525  Training Loss  0.034869320690631866\n",
            "Epoch  13 Batch  367 / 525  Training Loss  0.023906337097287178\n",
            "Epoch  13 Batch  368 / 525  Training Loss  0.028927406296133995\n",
            "Epoch  13 Batch  369 / 525  Training Loss  0.025617215782403946\n",
            "Epoch  13 Batch  370 / 525  Training Loss  0.02446288987994194\n",
            "Epoch  13 Batch  371 / 525  Training Loss  0.014094509184360504\n",
            "Epoch  13 Batch  372 / 525  Training Loss  0.04075082018971443\n",
            "Epoch  13 Batch  373 / 525  Training Loss  0.028921663761138916\n",
            "Epoch  13 Batch  374 / 525  Training Loss  0.032944612205028534\n",
            "Epoch  13 Batch  375 / 525  Training Loss  0.02902672067284584\n",
            "Epoch  13 Batch  376 / 525  Training Loss  0.028769757598638535\n",
            "Epoch  13 Batch  377 / 525  Training Loss  0.020365675911307335\n",
            "Epoch  13 Batch  378 / 525  Training Loss  0.03672712296247482\n",
            "Epoch  13 Batch  379 / 525  Training Loss  0.029500996693968773\n",
            "Epoch  13 Batch  380 / 525  Training Loss  0.02639215812087059\n",
            "Epoch  13 Batch  381 / 525  Training Loss  0.029388314113020897\n",
            "Epoch  13 Batch  382 / 525  Training Loss  0.030477646738290787\n",
            "Epoch  13 Batch  383 / 525  Training Loss  0.021295860409736633\n",
            "Epoch  13 Batch  384 / 525  Training Loss  0.0271606408059597\n",
            "Epoch  13 Batch  385 / 525  Training Loss  0.04508339613676071\n",
            "Epoch  13 Batch  386 / 525  Training Loss  0.037787165492773056\n",
            "Epoch  13 Batch  387 / 525  Training Loss  0.033079490065574646\n",
            "Epoch  13 Batch  388 / 525  Training Loss  0.03434529900550842\n",
            "Epoch  13 Batch  389 / 525  Training Loss  0.03557198494672775\n",
            "Epoch  13 Batch  390 / 525  Training Loss  0.035654548555612564\n",
            "Epoch  13 Batch  391 / 525  Training Loss  0.02382020279765129\n",
            "Epoch  13 Batch  392 / 525  Training Loss  0.04412118345499039\n",
            "Epoch  13 Batch  393 / 525  Training Loss  0.040433306246995926\n",
            "Epoch  13 Batch  394 / 525  Training Loss  0.04559500887989998\n",
            "Epoch  13 Batch  395 / 525  Training Loss  0.029507741332054138\n",
            "Epoch  13 Batch  396 / 525  Training Loss  0.03472629189491272\n",
            "Epoch  13 Batch  397 / 525  Training Loss  0.02919548749923706\n",
            "Epoch  13 Batch  398 / 525  Training Loss  0.022642990574240685\n",
            "Epoch  13 Batch  399 / 525  Training Loss  0.02422463521361351\n",
            "Epoch  13 Batch  400 / 525  Training Loss  0.04190943390130997\n",
            "Epoch  13 Batch  401 / 525  Training Loss  0.03165917098522186\n",
            "Epoch  13 Batch  402 / 525  Training Loss  0.02441653236746788\n",
            "Epoch  13 Batch  403 / 525  Training Loss  0.021708549931645393\n",
            "Epoch  13 Batch  404 / 525  Training Loss  0.044114090502262115\n",
            "Epoch  13 Batch  405 / 525  Training Loss  0.03367581218481064\n",
            "Epoch  13 Batch  406 / 525  Training Loss  0.02700072154402733\n",
            "Epoch  13 Batch  407 / 525  Training Loss  0.03877659887075424\n",
            "Epoch  13 Batch  408 / 525  Training Loss  0.02810410037636757\n",
            "Epoch  13 Batch  409 / 525  Training Loss  0.03154539316892624\n",
            "Epoch  13 Batch  410 / 525  Training Loss  0.04298863187432289\n",
            "Epoch  13 Batch  411 / 525  Training Loss  0.039368849247694016\n",
            "Epoch  13 Batch  412 / 525  Training Loss  0.028439346700906754\n",
            "Epoch  13 Batch  413 / 525  Training Loss  0.028549840673804283\n",
            "Epoch  13 Batch  414 / 525  Training Loss  0.028857136145234108\n",
            "Epoch  13 Batch  415 / 525  Training Loss  0.03248400613665581\n",
            "Epoch  13 Batch  416 / 525  Training Loss  0.020936738699674606\n",
            "Epoch  13 Batch  417 / 525  Training Loss  0.03130088001489639\n",
            "Epoch  13 Batch  418 / 525  Training Loss  0.029617974534630775\n",
            "Epoch  13 Batch  419 / 525  Training Loss  0.0337352529168129\n",
            "Epoch  13 Batch  420 / 525  Training Loss  0.03532939404249191\n",
            "Epoch  13 Batch  421 / 525  Training Loss  0.027142399922013283\n",
            "Epoch  13 Batch  422 / 525  Training Loss  0.014033155515789986\n",
            "Epoch  13 Batch  423 / 525  Training Loss  0.02882554568350315\n",
            "Epoch  13 Batch  424 / 525  Training Loss  0.022721434012055397\n",
            "Epoch  13 Batch  425 / 525  Training Loss  0.039991773664951324\n",
            "Epoch  13 Batch  426 / 525  Training Loss  0.03669947013258934\n",
            "Epoch  13 Batch  427 / 525  Training Loss  0.03773966431617737\n",
            "Epoch  13 Batch  428 / 525  Training Loss  0.03342211991548538\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  13 Batch  429 / 525  Training Loss  0.03653698414564133\n",
            "Epoch  13 Batch  430 / 525  Training Loss  0.04031846672296524\n",
            "Epoch  13 Batch  431 / 525  Training Loss  0.021626321598887444\n",
            "Epoch  13 Batch  432 / 525  Training Loss  0.033635225147008896\n",
            "Epoch  13 Batch  433 / 525  Training Loss  0.0510399229824543\n",
            "Epoch  13 Batch  434 / 525  Training Loss  0.02755805291235447\n",
            "Epoch  13 Batch  435 / 525  Training Loss  0.040380798280239105\n",
            "Epoch  13 Batch  436 / 525  Training Loss  0.0370369628071785\n",
            "Epoch  13 Batch  437 / 525  Training Loss  0.04237201064825058\n",
            "Epoch  13 Batch  438 / 525  Training Loss  0.03645417094230652\n",
            "Epoch  13 Batch  439 / 525  Training Loss  0.025133347138762474\n",
            "Epoch  13 Batch  440 / 525  Training Loss  0.021067354828119278\n",
            "Epoch  13 Batch  441 / 525  Training Loss  0.02844124473631382\n",
            "Epoch  13 Batch  442 / 525  Training Loss  0.030218277126550674\n",
            "Epoch  13 Batch  443 / 525  Training Loss  0.02493896335363388\n",
            "Epoch  13 Batch  444 / 525  Training Loss  0.021061737090349197\n",
            "Epoch  13 Batch  445 / 525  Training Loss  0.019825175404548645\n",
            "Epoch  13 Batch  446 / 525  Training Loss  0.035608332604169846\n",
            "Epoch  13 Batch  447 / 525  Training Loss  0.027602266520261765\n",
            "Epoch  13 Batch  448 / 525  Training Loss  0.03600403666496277\n",
            "Epoch  13 Batch  449 / 525  Training Loss  0.028061965480446815\n",
            "Epoch  13 Batch  450 / 525  Training Loss  0.025253405794501305\n",
            "Epoch  13 Batch  451 / 525  Training Loss  0.0355619378387928\n",
            "Epoch  13 Batch  452 / 525  Training Loss  0.0235578715801239\n",
            "Epoch  13 Batch  453 / 525  Training Loss  0.03462200611829758\n",
            "Epoch  13 Batch  454 / 525  Training Loss  0.036297015845775604\n",
            "Epoch  13 Batch  455 / 525  Training Loss  0.03283314406871796\n",
            "Epoch  13 Batch  456 / 525  Training Loss  0.03403227776288986\n",
            "Epoch  13 Batch  457 / 525  Training Loss  0.04783575236797333\n",
            "Epoch  13 Batch  458 / 525  Training Loss  0.0295221209526062\n",
            "Epoch  13 Batch  459 / 525  Training Loss  0.039852552115917206\n",
            "Epoch  13 Batch  460 / 525  Training Loss  0.020603325217962265\n",
            "Epoch  13 Batch  461 / 525  Training Loss  0.039413467049598694\n",
            "Epoch  13 Batch  462 / 525  Training Loss  0.04015327990055084\n",
            "Epoch  13 Batch  463 / 525  Training Loss  0.043258968740701675\n",
            "Epoch  13 Batch  464 / 525  Training Loss  0.030960867181420326\n",
            "Epoch  13 Batch  465 / 525  Training Loss  0.042941879481077194\n",
            "Epoch  13 Batch  466 / 525  Training Loss  0.032951947301626205\n",
            "Epoch  13 Batch  467 / 525  Training Loss  0.028840910643339157\n",
            "Epoch  13 Batch  468 / 525  Training Loss  0.03272907808423042\n",
            "Epoch  13 Batch  469 / 525  Training Loss  0.038028087466955185\n",
            "Epoch  13 Batch  470 / 525  Training Loss  0.0398218147456646\n",
            "Epoch  13 Batch  471 / 525  Training Loss  0.02372022531926632\n",
            "Epoch  13 Batch  472 / 525  Training Loss  0.028775837272405624\n",
            "Epoch  13 Batch  473 / 525  Training Loss  0.037650253623723984\n",
            "Epoch  13 Batch  474 / 525  Training Loss  0.03337054327130318\n",
            "Epoch  13 Batch  475 / 525  Training Loss  0.034414082765579224\n",
            "Epoch  13 Batch  476 / 525  Training Loss  0.030893202871084213\n",
            "Epoch  13 Batch  477 / 525  Training Loss  0.033614058047533035\n",
            "Epoch  13 Batch  478 / 525  Training Loss  0.04823513701558113\n",
            "Epoch  13 Batch  479 / 525  Training Loss  0.03328440338373184\n",
            "Epoch  13 Batch  480 / 525  Training Loss  0.03605624660849571\n",
            "Epoch  13 Batch  481 / 525  Training Loss  0.023053841665387154\n",
            "Epoch  13 Batch  482 / 525  Training Loss  0.022680135443806648\n",
            "Epoch  13 Batch  483 / 525  Training Loss  0.04040181636810303\n",
            "Epoch  13 Batch  484 / 525  Training Loss  0.027926329523324966\n",
            "Epoch  13 Batch  485 / 525  Training Loss  0.030924726277589798\n",
            "Epoch  13 Batch  486 / 525  Training Loss  0.037279918789863586\n",
            "Epoch  13 Batch  487 / 525  Training Loss  0.03931728005409241\n",
            "Epoch  13 Batch  488 / 525  Training Loss  0.036305021494627\n",
            "Epoch  13 Batch  489 / 525  Training Loss  0.024688709527254105\n",
            "Epoch  13 Batch  490 / 525  Training Loss  0.03062678501009941\n",
            "Epoch  13 Batch  491 / 525  Training Loss  0.018062790855765343\n",
            "Epoch  13 Batch  492 / 525  Training Loss  0.04717128723859787\n",
            "Epoch  13 Batch  493 / 525  Training Loss  0.03634165972471237\n",
            "Epoch  13 Batch  494 / 525  Training Loss  0.023475777357816696\n",
            "Epoch  13 Batch  495 / 525  Training Loss  0.012520136311650276\n",
            "Epoch  13 Batch  496 / 525  Training Loss  0.02560054138302803\n",
            "Epoch  13 Batch  497 / 525  Training Loss  0.02851460874080658\n",
            "Epoch  13 Batch  498 / 525  Training Loss  0.05012757331132889\n",
            "Epoch  13 Batch  499 / 525  Training Loss  0.02956441044807434\n",
            "Epoch  13 Batch  500 / 525  Training Loss  0.02771468460559845\n",
            "Epoch  13 Batch  501 / 525  Training Loss  0.030019288882613182\n",
            "Epoch  13 Batch  502 / 525  Training Loss  0.02949364483356476\n",
            "Epoch  13 Batch  503 / 525  Training Loss  0.030301058664917946\n",
            "Epoch  13 Batch  504 / 525  Training Loss  0.039447613060474396\n",
            "Epoch  13 Batch  505 / 525  Training Loss  0.05268176272511482\n",
            "Epoch  13 Batch  506 / 525  Training Loss  0.03495463728904724\n",
            "Epoch  13 Batch  507 / 525  Training Loss  0.03253373131155968\n",
            "Epoch  13 Batch  508 / 525  Training Loss  0.02783489227294922\n",
            "Epoch  13 Batch  509 / 525  Training Loss  0.020245017483830452\n",
            "Epoch  13 Batch  510 / 525  Training Loss  0.02206551842391491\n",
            "Epoch  13 Batch  511 / 525  Training Loss  0.032724589109420776\n",
            "Epoch  13 Batch  512 / 525  Training Loss  0.029390286654233932\n",
            "Epoch  13 Batch  513 / 525  Training Loss  0.031040092930197716\n",
            "Epoch  13 Batch  514 / 525  Training Loss  0.039475440979003906\n",
            "Epoch  13 Batch  515 / 525  Training Loss  0.035203032195568085\n",
            "Epoch  13 Batch  516 / 525  Training Loss  0.03488237410783768\n",
            "Epoch  13 Batch  517 / 525  Training Loss  0.03182537481188774\n",
            "Epoch  13 Batch  518 / 525  Training Loss  0.02939631976187229\n",
            "Epoch  13 Batch  519 / 525  Training Loss  0.02699081227183342\n",
            "Epoch  13 Batch  520 / 525  Training Loss  0.03109092451632023\n",
            "Epoch  13 Batch  521 / 525  Training Loss  0.024026475846767426\n",
            "Epoch  13 Batch  522 / 525  Training Loss  0.04149995744228363\n",
            "Epoch  13 Batch  523 / 525  Training Loss  0.03044707700610161\n",
            "Epoch  13 Batch  524 / 525  Training Loss  0.030534710735082626\n",
            "  14    |    -    |   0.029301   | 53.041667\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 14\n",
            "Epoch  14 Batch  0 / 525  Training Loss  0.02544093132019043\n",
            "Epoch  14 Batch  1 / 525  Training Loss  0.021672740578651428\n",
            "Epoch  14 Batch  2 / 525  Training Loss  0.02575247921049595\n",
            "Epoch  14 Batch  3 / 525  Training Loss  0.026524562388658524\n",
            "Epoch  14 Batch  4 / 525  Training Loss  0.01913902908563614\n",
            "Epoch  14 Batch  5 / 525  Training Loss  0.02585209533572197\n",
            "Epoch  14 Batch  6 / 525  Training Loss  0.02120792306959629\n",
            "Epoch  14 Batch  7 / 525  Training Loss  0.025923380628228188\n",
            "Epoch  14 Batch  8 / 525  Training Loss  0.032745253294706345\n",
            "Epoch  14 Batch  9 / 525  Training Loss  0.018859075382351875\n",
            "Epoch  14 Batch  10 / 525  Training Loss  0.014016598463058472\n",
            "Epoch  14 Batch  11 / 525  Training Loss  0.028751512989401817\n",
            "Epoch  14 Batch  12 / 525  Training Loss  0.041610416024923325\n",
            "Epoch  14 Batch  13 / 525  Training Loss  0.016633586958050728\n",
            "Epoch  14 Batch  14 / 525  Training Loss  0.02790255844593048\n",
            "Epoch  14 Batch  15 / 525  Training Loss  0.01811164990067482\n",
            "Epoch  14 Batch  16 / 525  Training Loss  0.010916532017290592\n",
            "Epoch  14 Batch  17 / 525  Training Loss  0.03274397924542427\n",
            "Epoch  14 Batch  18 / 525  Training Loss  0.025431465357542038\n",
            "Epoch  14 Batch  19 / 525  Training Loss  0.018808763474225998\n",
            "Epoch  14 Batch  20 / 525  Training Loss  0.018550261855125427\n",
            "Epoch  14 Batch  21 / 525  Training Loss  0.022763825953006744\n",
            "Epoch  14 Batch  22 / 525  Training Loss  0.029755592346191406\n",
            "Epoch  14 Batch  23 / 525  Training Loss  0.012413566932082176\n",
            "Epoch  14 Batch  24 / 525  Training Loss  0.014764313586056232\n",
            "Epoch  14 Batch  25 / 525  Training Loss  0.014757181517779827\n",
            "Epoch  14 Batch  26 / 525  Training Loss  0.018729763105511665\n",
            "Epoch  14 Batch  27 / 525  Training Loss  0.022517304867506027\n",
            "Epoch  14 Batch  28 / 525  Training Loss  0.015524161979556084\n",
            "Epoch  14 Batch  29 / 525  Training Loss  0.009950564242899418\n",
            "Epoch  14 Batch  30 / 525  Training Loss  0.009372679516673088\n",
            "Epoch  14 Batch  31 / 525  Training Loss  0.025831643491983414\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  14 Batch  32 / 525  Training Loss  0.011984855867922306\n",
            "Epoch  14 Batch  33 / 525  Training Loss  0.020306546241044998\n",
            "Epoch  14 Batch  34 / 525  Training Loss  0.01296051312237978\n",
            "Epoch  14 Batch  35 / 525  Training Loss  0.015429692342877388\n",
            "Epoch  14 Batch  36 / 525  Training Loss  0.02869468368589878\n",
            "Epoch  14 Batch  37 / 525  Training Loss  0.021985532715916634\n",
            "Epoch  14 Batch  38 / 525  Training Loss  0.018120206892490387\n",
            "Epoch  14 Batch  39 / 525  Training Loss  0.022529806941747665\n",
            "Epoch  14 Batch  40 / 525  Training Loss  0.01731441169977188\n",
            "Epoch  14 Batch  41 / 525  Training Loss  0.01547857653349638\n",
            "Epoch  14 Batch  42 / 525  Training Loss  0.017365161329507828\n",
            "Epoch  14 Batch  43 / 525  Training Loss  0.019670141860842705\n",
            "Epoch  14 Batch  44 / 525  Training Loss  0.010428063571453094\n",
            "Epoch  14 Batch  45 / 525  Training Loss  0.015180421061813831\n",
            "Epoch  14 Batch  46 / 525  Training Loss  0.026219144463539124\n",
            "Epoch  14 Batch  47 / 525  Training Loss  0.018990475684404373\n",
            "Epoch  14 Batch  48 / 525  Training Loss  0.018303021788597107\n",
            "Epoch  14 Batch  49 / 525  Training Loss  0.01280529797077179\n",
            "Epoch  14 Batch  50 / 525  Training Loss  0.01801365427672863\n",
            "Epoch  14 Batch  51 / 525  Training Loss  0.01917860098183155\n",
            "Epoch  14 Batch  52 / 525  Training Loss  0.013706671074032784\n",
            "Epoch  14 Batch  53 / 525  Training Loss  0.019384203478693962\n",
            "Epoch  14 Batch  54 / 525  Training Loss  0.014839676208794117\n",
            "Epoch  14 Batch  55 / 525  Training Loss  0.01664891466498375\n",
            "Epoch  14 Batch  56 / 525  Training Loss  0.022452915087342262\n",
            "Epoch  14 Batch  57 / 525  Training Loss  0.02129691280424595\n",
            "Epoch  14 Batch  58 / 525  Training Loss  0.020335828885436058\n",
            "Epoch  14 Batch  59 / 525  Training Loss  0.020650500431656837\n",
            "Epoch  14 Batch  60 / 525  Training Loss  0.017401069402694702\n",
            "Epoch  14 Batch  61 / 525  Training Loss  0.01169314794242382\n",
            "Epoch  14 Batch  62 / 525  Training Loss  0.025795063003897667\n",
            "Epoch  14 Batch  63 / 525  Training Loss  0.02758667804300785\n",
            "Epoch  14 Batch  64 / 525  Training Loss  0.02288907766342163\n",
            "Epoch  14 Batch  65 / 525  Training Loss  0.03069758415222168\n",
            "Epoch  14 Batch  66 / 525  Training Loss  0.02831217274069786\n",
            "Epoch  14 Batch  67 / 525  Training Loss  0.009668216109275818\n",
            "Epoch  14 Batch  68 / 525  Training Loss  0.020249178633093834\n",
            "Epoch  14 Batch  69 / 525  Training Loss  0.022334452718496323\n",
            "Epoch  14 Batch  70 / 525  Training Loss  0.021970907226204872\n",
            "Epoch  14 Batch  71 / 525  Training Loss  0.025579556822776794\n",
            "Epoch  14 Batch  72 / 525  Training Loss  0.03587966039776802\n",
            "Epoch  14 Batch  73 / 525  Training Loss  0.01675119809806347\n",
            "Epoch  14 Batch  74 / 525  Training Loss  0.027320584282279015\n",
            "Epoch  14 Batch  75 / 525  Training Loss  0.02355446107685566\n",
            "Epoch  14 Batch  76 / 525  Training Loss  0.018138092011213303\n",
            "Epoch  14 Batch  77 / 525  Training Loss  0.031925641000270844\n",
            "Epoch  14 Batch  78 / 525  Training Loss  0.011484372429549694\n",
            "Epoch  14 Batch  79 / 525  Training Loss  0.01996634155511856\n",
            "Epoch  14 Batch  80 / 525  Training Loss  0.021935489028692245\n",
            "Epoch  14 Batch  81 / 525  Training Loss  0.027244433760643005\n",
            "Epoch  14 Batch  82 / 525  Training Loss  0.01992839016020298\n",
            "Epoch  14 Batch  83 / 525  Training Loss  0.01500091701745987\n",
            "Epoch  14 Batch  84 / 525  Training Loss  0.01513546984642744\n",
            "Epoch  14 Batch  85 / 525  Training Loss  0.024120282381772995\n",
            "Epoch  14 Batch  86 / 525  Training Loss  0.016100909560918808\n",
            "Epoch  14 Batch  87 / 525  Training Loss  0.021454473957419395\n",
            "Epoch  14 Batch  88 / 525  Training Loss  0.015071103349328041\n",
            "Epoch  14 Batch  89 / 525  Training Loss  0.015482638962566853\n",
            "Epoch  14 Batch  90 / 525  Training Loss  0.02172042429447174\n",
            "Epoch  14 Batch  91 / 525  Training Loss  0.014833000488579273\n",
            "Epoch  14 Batch  92 / 525  Training Loss  0.024836815893650055\n",
            "Epoch  14 Batch  93 / 525  Training Loss  0.022647926583886147\n",
            "Epoch  14 Batch  94 / 525  Training Loss  0.01900453120470047\n",
            "Epoch  14 Batch  95 / 525  Training Loss  0.011419132351875305\n",
            "Epoch  14 Batch  96 / 525  Training Loss  0.015555349178612232\n",
            "Epoch  14 Batch  97 / 525  Training Loss  0.023023031651973724\n",
            "Epoch  14 Batch  98 / 525  Training Loss  0.01348657626658678\n",
            "Epoch  14 Batch  99 / 525  Training Loss  0.02017178386449814\n",
            "Epoch  14 Batch  100 / 525  Training Loss  0.02883434295654297\n",
            "Epoch  14 Batch  101 / 525  Training Loss  0.023512762039899826\n",
            "Epoch  14 Batch  102 / 525  Training Loss  0.018295738846063614\n",
            "Epoch  14 Batch  103 / 525  Training Loss  0.02509208954870701\n",
            "Epoch  14 Batch  104 / 525  Training Loss  0.019641220569610596\n",
            "Epoch  14 Batch  105 / 525  Training Loss  0.018525442108511925\n",
            "Epoch  14 Batch  106 / 525  Training Loss  0.02179970219731331\n",
            "Epoch  14 Batch  107 / 525  Training Loss  0.013144256547093391\n",
            "Epoch  14 Batch  108 / 525  Training Loss  0.01639973185956478\n",
            "Epoch  14 Batch  109 / 525  Training Loss  0.019124740734696388\n",
            "Epoch  14 Batch  110 / 525  Training Loss  0.021484432741999626\n",
            "Epoch  14 Batch  111 / 525  Training Loss  0.033066801726818085\n",
            "Epoch  14 Batch  112 / 525  Training Loss  0.027132892981171608\n",
            "Epoch  14 Batch  113 / 525  Training Loss  0.028276067227125168\n",
            "Epoch  14 Batch  114 / 525  Training Loss  0.037388719618320465\n",
            "Epoch  14 Batch  115 / 525  Training Loss  0.027665913105010986\n",
            "Epoch  14 Batch  116 / 525  Training Loss  0.03133893758058548\n",
            "Epoch  14 Batch  117 / 525  Training Loss  0.014905105344951153\n",
            "Epoch  14 Batch  118 / 525  Training Loss  0.0200363639742136\n",
            "Epoch  14 Batch  119 / 525  Training Loss  0.01867547258734703\n",
            "Epoch  14 Batch  120 / 525  Training Loss  0.027553701773285866\n",
            "Epoch  14 Batch  121 / 525  Training Loss  0.01578284054994583\n",
            "Epoch  14 Batch  122 / 525  Training Loss  0.020935965701937675\n",
            "Epoch  14 Batch  123 / 525  Training Loss  0.027446845546364784\n",
            "Epoch  14 Batch  124 / 525  Training Loss  0.019490154460072517\n",
            "Epoch  14 Batch  125 / 525  Training Loss  0.01766068860888481\n",
            "Epoch  14 Batch  126 / 525  Training Loss  0.022638538852334023\n",
            "Epoch  14 Batch  127 / 525  Training Loss  0.02078976109623909\n",
            "Epoch  14 Batch  128 / 525  Training Loss  0.015673968940973282\n",
            "Epoch  14 Batch  129 / 525  Training Loss  0.03927694261074066\n",
            "Epoch  14 Batch  130 / 525  Training Loss  0.029724646359682083\n",
            "Epoch  14 Batch  131 / 525  Training Loss  0.024080345407128334\n",
            "Epoch  14 Batch  132 / 525  Training Loss  0.021718228235840797\n",
            "Epoch  14 Batch  133 / 525  Training Loss  0.01319202035665512\n",
            "Epoch  14 Batch  134 / 525  Training Loss  0.02269556373357773\n",
            "Epoch  14 Batch  135 / 525  Training Loss  0.013944387435913086\n",
            "Epoch  14 Batch  136 / 525  Training Loss  0.018836332485079765\n",
            "Epoch  14 Batch  137 / 525  Training Loss  0.017774643376469612\n",
            "Epoch  14 Batch  138 / 525  Training Loss  0.017359986901283264\n",
            "Epoch  14 Batch  139 / 525  Training Loss  0.018737252801656723\n",
            "Epoch  14 Batch  140 / 525  Training Loss  0.017469527199864388\n",
            "Epoch  14 Batch  141 / 525  Training Loss  0.022380001842975616\n",
            "Epoch  14 Batch  142 / 525  Training Loss  0.023691866546869278\n",
            "Epoch  14 Batch  143 / 525  Training Loss  0.02293642796576023\n",
            "Epoch  14 Batch  144 / 525  Training Loss  0.01284761168062687\n",
            "Epoch  14 Batch  145 / 525  Training Loss  0.021565603092312813\n",
            "Epoch  14 Batch  146 / 525  Training Loss  0.02917417325079441\n",
            "Epoch  14 Batch  147 / 525  Training Loss  0.026627162471413612\n",
            "Epoch  14 Batch  148 / 525  Training Loss  0.012555569410324097\n",
            "Epoch  14 Batch  149 / 525  Training Loss  0.015295252203941345\n",
            "Epoch  14 Batch  150 / 525  Training Loss  0.020688213407993317\n",
            "Epoch  14 Batch  151 / 525  Training Loss  0.030754204839468002\n",
            "Epoch  14 Batch  152 / 525  Training Loss  0.02468356117606163\n",
            "Epoch  14 Batch  153 / 525  Training Loss  0.018566112965345383\n",
            "Epoch  14 Batch  154 / 525  Training Loss  0.016961421817541122\n",
            "Epoch  14 Batch  155 / 525  Training Loss  0.014490336179733276\n",
            "Epoch  14 Batch  156 / 525  Training Loss  0.017387624830007553\n",
            "Epoch  14 Batch  157 / 525  Training Loss  0.016168178990483284\n",
            "Epoch  14 Batch  158 / 525  Training Loss  0.02208833582699299\n",
            "Epoch  14 Batch  159 / 525  Training Loss  0.020538892596960068\n",
            "Epoch  14 Batch  160 / 525  Training Loss  0.01429763250052929\n",
            "Epoch  14 Batch  161 / 525  Training Loss  0.018550649285316467\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  14 Batch  162 / 525  Training Loss  0.01362035982310772\n",
            "Epoch  14 Batch  163 / 525  Training Loss  0.036393262445926666\n",
            "Epoch  14 Batch  164 / 525  Training Loss  0.017395881935954094\n",
            "Epoch  14 Batch  165 / 525  Training Loss  0.02044438198208809\n",
            "Epoch  14 Batch  166 / 525  Training Loss  0.025182466953992844\n",
            "Epoch  14 Batch  167 / 525  Training Loss  0.03529297187924385\n",
            "Epoch  14 Batch  168 / 525  Training Loss  0.02288980223238468\n",
            "Epoch  14 Batch  169 / 525  Training Loss  0.03328569978475571\n",
            "Epoch  14 Batch  170 / 525  Training Loss  0.024820255115628242\n",
            "Epoch  14 Batch  171 / 525  Training Loss  0.03347913920879364\n",
            "Epoch  14 Batch  172 / 525  Training Loss  0.027598787099123\n",
            "Epoch  14 Batch  173 / 525  Training Loss  0.02328285202383995\n",
            "Epoch  14 Batch  174 / 525  Training Loss  0.009425327181816101\n",
            "Epoch  14 Batch  175 / 525  Training Loss  0.025250151753425598\n",
            "Epoch  14 Batch  176 / 525  Training Loss  0.029448026791214943\n",
            "Epoch  14 Batch  177 / 525  Training Loss  0.022644542157649994\n",
            "Epoch  14 Batch  178 / 525  Training Loss  0.014917932450771332\n",
            "Epoch  14 Batch  179 / 525  Training Loss  0.015479981899261475\n",
            "Epoch  14 Batch  180 / 525  Training Loss  0.01904614083468914\n",
            "Epoch  14 Batch  181 / 525  Training Loss  0.02709110639989376\n",
            "Epoch  14 Batch  182 / 525  Training Loss  0.02671726606786251\n",
            "Epoch  14 Batch  183 / 525  Training Loss  0.018437927588820457\n",
            "Epoch  14 Batch  184 / 525  Training Loss  0.0169085543602705\n",
            "Epoch  14 Batch  185 / 525  Training Loss  0.0252362247556448\n",
            "Epoch  14 Batch  186 / 525  Training Loss  0.024760793894529343\n",
            "Epoch  14 Batch  187 / 525  Training Loss  0.024337712675333023\n",
            "Epoch  14 Batch  188 / 525  Training Loss  0.009373718872666359\n",
            "Epoch  14 Batch  189 / 525  Training Loss  0.02828357182443142\n",
            "Epoch  14 Batch  190 / 525  Training Loss  0.028375785797834396\n",
            "Epoch  14 Batch  191 / 525  Training Loss  0.028873929753899574\n",
            "Epoch  14 Batch  192 / 525  Training Loss  0.03714122250676155\n",
            "Epoch  14 Batch  193 / 525  Training Loss  0.014571240171790123\n",
            "Epoch  14 Batch  194 / 525  Training Loss  0.027055904269218445\n",
            "Epoch  14 Batch  195 / 525  Training Loss  0.018054604530334473\n",
            "Epoch  14 Batch  196 / 525  Training Loss  0.018164100125432014\n",
            "Epoch  14 Batch  197 / 525  Training Loss  0.022861970588564873\n",
            "Epoch  14 Batch  198 / 525  Training Loss  0.02321675792336464\n",
            "Epoch  14 Batch  199 / 525  Training Loss  0.02080417238175869\n",
            "Epoch  14 Batch  200 / 525  Training Loss  0.02405976504087448\n",
            "Epoch  14 Batch  201 / 525  Training Loss  0.019667038694024086\n",
            "Epoch  14 Batch  202 / 525  Training Loss  0.022614533081650734\n",
            "Epoch  14 Batch  203 / 525  Training Loss  0.024788737297058105\n",
            "Epoch  14 Batch  204 / 525  Training Loss  0.04017413407564163\n",
            "Epoch  14 Batch  205 / 525  Training Loss  0.02534230425953865\n",
            "Epoch  14 Batch  206 / 525  Training Loss  0.022729219868779182\n",
            "Epoch  14 Batch  207 / 525  Training Loss  0.024702496826648712\n",
            "Epoch  14 Batch  208 / 525  Training Loss  0.022668246179819107\n",
            "Epoch  14 Batch  209 / 525  Training Loss  0.032022859901189804\n",
            "Epoch  14 Batch  210 / 525  Training Loss  0.034312985837459564\n",
            "Epoch  14 Batch  211 / 525  Training Loss  0.026055002585053444\n",
            "Epoch  14 Batch  212 / 525  Training Loss  0.017201364040374756\n",
            "Epoch  14 Batch  213 / 525  Training Loss  0.018999936059117317\n",
            "Epoch  14 Batch  214 / 525  Training Loss  0.02530738338828087\n",
            "Epoch  14 Batch  215 / 525  Training Loss  0.029675092548131943\n",
            "Epoch  14 Batch  216 / 525  Training Loss  0.033839210867881775\n",
            "Epoch  14 Batch  217 / 525  Training Loss  0.025105655193328857\n",
            "Epoch  14 Batch  218 / 525  Training Loss  0.027525026351213455\n",
            "Epoch  14 Batch  219 / 525  Training Loss  0.03445779159665108\n",
            "Epoch  14 Batch  220 / 525  Training Loss  0.02676568552851677\n",
            "Epoch  14 Batch  221 / 525  Training Loss  0.020800059661269188\n",
            "Epoch  14 Batch  222 / 525  Training Loss  0.027434464544057846\n",
            "Epoch  14 Batch  223 / 525  Training Loss  0.024411968886852264\n",
            "Epoch  14 Batch  224 / 525  Training Loss  0.01964743807911873\n",
            "Epoch  14 Batch  225 / 525  Training Loss  0.025297174230217934\n",
            "Epoch  14 Batch  226 / 525  Training Loss  0.023378023877739906\n",
            "Epoch  14 Batch  227 / 525  Training Loss  0.014791302382946014\n",
            "Epoch  14 Batch  228 / 525  Training Loss  0.017629731446504593\n",
            "Epoch  14 Batch  229 / 525  Training Loss  0.017519574612379074\n",
            "Epoch  14 Batch  230 / 525  Training Loss  0.021370109170675278\n",
            "Epoch  14 Batch  231 / 525  Training Loss  0.01658393256366253\n",
            "Epoch  14 Batch  232 / 525  Training Loss  0.01755906268954277\n",
            "Epoch  14 Batch  233 / 525  Training Loss  0.029910380020737648\n",
            "Epoch  14 Batch  234 / 525  Training Loss  0.02017836645245552\n",
            "Epoch  14 Batch  235 / 525  Training Loss  0.019327905029058456\n",
            "Epoch  14 Batch  236 / 525  Training Loss  0.0111628333106637\n",
            "Epoch  14 Batch  237 / 525  Training Loss  0.029206734150648117\n",
            "Epoch  14 Batch  238 / 525  Training Loss  0.027675915509462357\n",
            "Epoch  14 Batch  239 / 525  Training Loss  0.017252106219530106\n",
            "Epoch  14 Batch  240 / 525  Training Loss  0.024189263582229614\n",
            "Epoch  14 Batch  241 / 525  Training Loss  0.019919374957680702\n",
            "Epoch  14 Batch  242 / 525  Training Loss  0.025839626789093018\n",
            "Epoch  14 Batch  243 / 525  Training Loss  0.016373226419091225\n",
            "Epoch  14 Batch  244 / 525  Training Loss  0.017942309379577637\n",
            "Epoch  14 Batch  245 / 525  Training Loss  0.019480662420392036\n",
            "Epoch  14 Batch  246 / 525  Training Loss  0.019981635734438896\n",
            "Epoch  14 Batch  247 / 525  Training Loss  0.03622892498970032\n",
            "Epoch  14 Batch  248 / 525  Training Loss  0.016580810770392418\n",
            "Epoch  14 Batch  249 / 525  Training Loss  0.036015190184116364\n",
            "Epoch  14 Batch  250 / 525  Training Loss  0.022177722305059433\n",
            "Epoch  14 Batch  251 / 525  Training Loss  0.016712751239538193\n",
            "Epoch  14 Batch  252 / 525  Training Loss  0.027424195781350136\n",
            "Epoch  14 Batch  253 / 525  Training Loss  0.021490156650543213\n",
            "Epoch  14 Batch  254 / 525  Training Loss  0.015912136062979698\n",
            "Epoch  14 Batch  255 / 525  Training Loss  0.022352783009409904\n",
            "Epoch  14 Batch  256 / 525  Training Loss  0.020968396216630936\n",
            "Epoch  14 Batch  257 / 525  Training Loss  0.029426854103803635\n",
            "Epoch  14 Batch  258 / 525  Training Loss  0.025281693786382675\n",
            "Epoch  14 Batch  259 / 525  Training Loss  0.020061487331986427\n",
            "Epoch  14 Batch  260 / 525  Training Loss  0.027542058378458023\n",
            "Epoch  14 Batch  261 / 525  Training Loss  0.01764708384871483\n",
            "Epoch  14 Batch  262 / 525  Training Loss  0.019088882952928543\n",
            "Epoch  14 Batch  263 / 525  Training Loss  0.022819355130195618\n",
            "Epoch  14 Batch  264 / 525  Training Loss  0.026623282581567764\n",
            "Epoch  14 Batch  265 / 525  Training Loss  0.02276742085814476\n",
            "Epoch  14 Batch  266 / 525  Training Loss  0.02234910987317562\n",
            "Epoch  14 Batch  267 / 525  Training Loss  0.01775220036506653\n",
            "Epoch  14 Batch  268 / 525  Training Loss  0.029259800910949707\n",
            "Epoch  14 Batch  269 / 525  Training Loss  0.02929896116256714\n",
            "Epoch  14 Batch  270 / 525  Training Loss  0.020753856748342514\n",
            "Epoch  14 Batch  271 / 525  Training Loss  0.01837734505534172\n",
            "Epoch  14 Batch  272 / 525  Training Loss  0.021336117759346962\n",
            "Epoch  14 Batch  273 / 525  Training Loss  0.0225178524851799\n",
            "Epoch  14 Batch  274 / 525  Training Loss  0.021791938692331314\n",
            "Epoch  14 Batch  275 / 525  Training Loss  0.024091577157378197\n",
            "Epoch  14 Batch  276 / 525  Training Loss  0.02664726972579956\n",
            "Epoch  14 Batch  277 / 525  Training Loss  0.025531571358442307\n",
            "Epoch  14 Batch  278 / 525  Training Loss  0.02927950583398342\n",
            "Epoch  14 Batch  279 / 525  Training Loss  0.021621767431497574\n",
            "Epoch  14 Batch  280 / 525  Training Loss  0.021266799420118332\n",
            "Epoch  14 Batch  281 / 525  Training Loss  0.034801676869392395\n",
            "Epoch  14 Batch  282 / 525  Training Loss  0.017639407888054848\n",
            "Epoch  14 Batch  283 / 525  Training Loss  0.019807005301117897\n",
            "Epoch  14 Batch  284 / 525  Training Loss  0.030991625040769577\n",
            "Epoch  14 Batch  285 / 525  Training Loss  0.018550265580415726\n",
            "Epoch  14 Batch  286 / 525  Training Loss  0.023111948743462563\n",
            "Epoch  14 Batch  287 / 525  Training Loss  0.022272488102316856\n",
            "Epoch  14 Batch  288 / 525  Training Loss  0.022041048854589462\n",
            "Epoch  14 Batch  289 / 525  Training Loss  0.023831065744161606\n",
            "Epoch  14 Batch  290 / 525  Training Loss  0.015985194593667984\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  14 Batch  291 / 525  Training Loss  0.012104162946343422\n",
            "Epoch  14 Batch  292 / 525  Training Loss  0.01424807496368885\n",
            "Epoch  14 Batch  293 / 525  Training Loss  0.01566663756966591\n",
            "Epoch  14 Batch  294 / 525  Training Loss  0.029018664732575417\n",
            "Epoch  14 Batch  295 / 525  Training Loss  0.028097059577703476\n",
            "Epoch  14 Batch  296 / 525  Training Loss  0.027048926800489426\n",
            "Epoch  14 Batch  297 / 525  Training Loss  0.0214122012257576\n",
            "Epoch  14 Batch  298 / 525  Training Loss  0.02439693547785282\n",
            "Epoch  14 Batch  299 / 525  Training Loss  0.02278490923345089\n",
            "Epoch  14 Batch  300 / 525  Training Loss  0.02331138588488102\n",
            "Epoch  14 Batch  301 / 525  Training Loss  0.021955683827400208\n",
            "Epoch  14 Batch  302 / 525  Training Loss  0.025508061051368713\n",
            "Epoch  14 Batch  303 / 525  Training Loss  0.02634660340845585\n",
            "Epoch  14 Batch  304 / 525  Training Loss  0.01355746854096651\n",
            "Epoch  14 Batch  305 / 525  Training Loss  0.02949027717113495\n",
            "Epoch  14 Batch  306 / 525  Training Loss  0.024237433448433876\n",
            "Epoch  14 Batch  307 / 525  Training Loss  0.0243634432554245\n",
            "Epoch  14 Batch  308 / 525  Training Loss  0.021374566480517387\n",
            "Epoch  14 Batch  309 / 525  Training Loss  0.01915627531707287\n",
            "Epoch  14 Batch  310 / 525  Training Loss  0.009159870445728302\n",
            "Epoch  14 Batch  311 / 525  Training Loss  0.02024795114994049\n",
            "Epoch  14 Batch  312 / 525  Training Loss  0.01746727153658867\n",
            "Epoch  14 Batch  313 / 525  Training Loss  0.05128331109881401\n",
            "Epoch  14 Batch  314 / 525  Training Loss  0.020346958190202713\n",
            "Epoch  14 Batch  315 / 525  Training Loss  0.023157907649874687\n",
            "Epoch  14 Batch  316 / 525  Training Loss  0.018882840871810913\n",
            "Epoch  14 Batch  317 / 525  Training Loss  0.023877795785665512\n",
            "Epoch  14 Batch  318 / 525  Training Loss  0.01063626166433096\n",
            "Epoch  14 Batch  319 / 525  Training Loss  0.03334704414010048\n",
            "Epoch  14 Batch  320 / 525  Training Loss  0.03244784101843834\n",
            "Epoch  14 Batch  321 / 525  Training Loss  0.021774636581540108\n",
            "Epoch  14 Batch  322 / 525  Training Loss  0.020118355751037598\n",
            "Epoch  14 Batch  323 / 525  Training Loss  0.013743257150053978\n",
            "Epoch  14 Batch  324 / 525  Training Loss  0.01145723182708025\n",
            "Epoch  14 Batch  325 / 525  Training Loss  0.014956245198845863\n",
            "Epoch  14 Batch  326 / 525  Training Loss  0.030393952503800392\n",
            "Epoch  14 Batch  327 / 525  Training Loss  0.022364478558301926\n",
            "Epoch  14 Batch  328 / 525  Training Loss  0.02800895646214485\n",
            "Epoch  14 Batch  329 / 525  Training Loss  0.02016863040626049\n",
            "Epoch  14 Batch  330 / 525  Training Loss  0.013351981528103352\n",
            "Epoch  14 Batch  331 / 525  Training Loss  0.02753199264407158\n",
            "Epoch  14 Batch  332 / 525  Training Loss  0.02958102896809578\n",
            "Epoch  14 Batch  333 / 525  Training Loss  0.024343471974134445\n",
            "Epoch  14 Batch  334 / 525  Training Loss  0.02993984892964363\n",
            "Epoch  14 Batch  335 / 525  Training Loss  0.02712782844901085\n",
            "Epoch  14 Batch  336 / 525  Training Loss  0.024391133338212967\n",
            "Epoch  14 Batch  337 / 525  Training Loss  0.027096396312117577\n",
            "Epoch  14 Batch  338 / 525  Training Loss  0.015775952488183975\n",
            "Epoch  14 Batch  339 / 525  Training Loss  0.013514146208763123\n",
            "Epoch  14 Batch  340 / 525  Training Loss  0.026442816480994225\n",
            "Epoch  14 Batch  341 / 525  Training Loss  0.01679874025285244\n",
            "Epoch  14 Batch  342 / 525  Training Loss  0.020756300538778305\n",
            "Epoch  14 Batch  343 / 525  Training Loss  0.02442028373479843\n",
            "Epoch  14 Batch  344 / 525  Training Loss  0.025700649246573448\n",
            "Epoch  14 Batch  345 / 525  Training Loss  0.023162174969911575\n",
            "Epoch  14 Batch  346 / 525  Training Loss  0.016668768599629402\n",
            "Epoch  14 Batch  347 / 525  Training Loss  0.019533727318048477\n",
            "Epoch  14 Batch  348 / 525  Training Loss  0.01523058582097292\n",
            "Epoch  14 Batch  349 / 525  Training Loss  0.029057422652840614\n",
            "Epoch  14 Batch  350 / 525  Training Loss  0.021251961588859558\n",
            "Epoch  14 Batch  351 / 525  Training Loss  0.02013682760298252\n",
            "Epoch  14 Batch  352 / 525  Training Loss  0.027455857023596764\n",
            "Epoch  14 Batch  353 / 525  Training Loss  0.033444054424762726\n",
            "Epoch  14 Batch  354 / 525  Training Loss  0.01862034946680069\n",
            "Epoch  14 Batch  355 / 525  Training Loss  0.026305492967367172\n",
            "Epoch  14 Batch  356 / 525  Training Loss  0.02896895632147789\n",
            "Epoch  14 Batch  357 / 525  Training Loss  0.02661730721592903\n",
            "Epoch  14 Batch  358 / 525  Training Loss  0.015071490779519081\n",
            "Epoch  14 Batch  359 / 525  Training Loss  0.02547357976436615\n",
            "Epoch  14 Batch  360 / 525  Training Loss  0.02506597898900509\n",
            "Epoch  14 Batch  361 / 525  Training Loss  0.036976031959056854\n",
            "Epoch  14 Batch  362 / 525  Training Loss  0.030527949333190918\n",
            "Epoch  14 Batch  363 / 525  Training Loss  0.033824335783720016\n",
            "Epoch  14 Batch  364 / 525  Training Loss  0.032273296266794205\n",
            "Epoch  14 Batch  365 / 525  Training Loss  0.026468485593795776\n",
            "Epoch  14 Batch  366 / 525  Training Loss  0.031983159482479095\n",
            "Epoch  14 Batch  367 / 525  Training Loss  0.025751594454050064\n",
            "Epoch  14 Batch  368 / 525  Training Loss  0.02948041632771492\n",
            "Epoch  14 Batch  369 / 525  Training Loss  0.016239222139120102\n",
            "Epoch  14 Batch  370 / 525  Training Loss  0.025748779997229576\n",
            "Epoch  14 Batch  371 / 525  Training Loss  0.019730625674128532\n",
            "Epoch  14 Batch  372 / 525  Training Loss  0.020947258919477463\n",
            "Epoch  14 Batch  373 / 525  Training Loss  0.029140207916498184\n",
            "Epoch  14 Batch  374 / 525  Training Loss  0.03898784890770912\n",
            "Epoch  14 Batch  375 / 525  Training Loss  0.029543006792664528\n",
            "Epoch  14 Batch  376 / 525  Training Loss  0.02439608797430992\n",
            "Epoch  14 Batch  377 / 525  Training Loss  0.029357824474573135\n",
            "Epoch  14 Batch  378 / 525  Training Loss  0.017135601490736008\n",
            "Epoch  14 Batch  379 / 525  Training Loss  0.03720429912209511\n",
            "Epoch  14 Batch  380 / 525  Training Loss  0.017705822363495827\n",
            "Epoch  14 Batch  381 / 525  Training Loss  0.017133232206106186\n",
            "Epoch  14 Batch  382 / 525  Training Loss  0.018283959478139877\n",
            "Epoch  14 Batch  383 / 525  Training Loss  0.029831642284989357\n",
            "Epoch  14 Batch  384 / 525  Training Loss  0.027809809893369675\n",
            "Epoch  14 Batch  385 / 525  Training Loss  0.03270866721868515\n",
            "Epoch  14 Batch  386 / 525  Training Loss  0.03900337591767311\n",
            "Epoch  14 Batch  387 / 525  Training Loss  0.03309088572859764\n",
            "Epoch  14 Batch  388 / 525  Training Loss  0.031635574996471405\n",
            "Epoch  14 Batch  389 / 525  Training Loss  0.02054525353014469\n",
            "Epoch  14 Batch  390 / 525  Training Loss  0.019458111375570297\n",
            "Epoch  14 Batch  391 / 525  Training Loss  0.016833676025271416\n",
            "Epoch  14 Batch  392 / 525  Training Loss  0.014299836941063404\n",
            "Epoch  14 Batch  393 / 525  Training Loss  0.03904252499341965\n",
            "Epoch  14 Batch  394 / 525  Training Loss  0.014934802427887917\n",
            "Epoch  14 Batch  395 / 525  Training Loss  0.027942169457674026\n",
            "Epoch  14 Batch  396 / 525  Training Loss  0.023333480581641197\n",
            "Epoch  14 Batch  397 / 525  Training Loss  0.026811962947249413\n",
            "Epoch  14 Batch  398 / 525  Training Loss  0.03808194771409035\n",
            "Epoch  14 Batch  399 / 525  Training Loss  0.025313396006822586\n",
            "Epoch  14 Batch  400 / 525  Training Loss  0.025750363245606422\n",
            "Epoch  14 Batch  401 / 525  Training Loss  0.023671619594097137\n",
            "Epoch  14 Batch  402 / 525  Training Loss  0.015170899219810963\n",
            "Epoch  14 Batch  403 / 525  Training Loss  0.01458450872451067\n",
            "Epoch  14 Batch  404 / 525  Training Loss  0.01600358821451664\n",
            "Epoch  14 Batch  405 / 525  Training Loss  0.029372278600931168\n",
            "Epoch  14 Batch  406 / 525  Training Loss  0.029849940910935402\n",
            "Epoch  14 Batch  407 / 525  Training Loss  0.023032644763588905\n",
            "Epoch  14 Batch  408 / 525  Training Loss  0.017120709642767906\n",
            "Epoch  14 Batch  409 / 525  Training Loss  0.025448229163885117\n",
            "Epoch  14 Batch  410 / 525  Training Loss  0.022279953584074974\n",
            "Epoch  14 Batch  411 / 525  Training Loss  0.027799123898148537\n",
            "Epoch  14 Batch  412 / 525  Training Loss  0.033421989530324936\n",
            "Epoch  14 Batch  413 / 525  Training Loss  0.023509293794631958\n",
            "Epoch  14 Batch  414 / 525  Training Loss  0.02360742725431919\n",
            "Epoch  14 Batch  415 / 525  Training Loss  0.0281654205173254\n",
            "Epoch  14 Batch  416 / 525  Training Loss  0.025721749290823936\n",
            "Epoch  14 Batch  417 / 525  Training Loss  0.017445676028728485\n",
            "Epoch  14 Batch  418 / 525  Training Loss  0.015209583565592766\n",
            "Epoch  14 Batch  419 / 525  Training Loss  0.031623475253582\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  14 Batch  420 / 525  Training Loss  0.04720812290906906\n",
            "Epoch  14 Batch  421 / 525  Training Loss  0.031109219416975975\n",
            "Epoch  14 Batch  422 / 525  Training Loss  0.023906497284770012\n",
            "Epoch  14 Batch  423 / 525  Training Loss  0.02768735960125923\n",
            "Epoch  14 Batch  424 / 525  Training Loss  0.020584430545568466\n",
            "Epoch  14 Batch  425 / 525  Training Loss  0.027888720855116844\n",
            "Epoch  14 Batch  426 / 525  Training Loss  0.021214326843619347\n",
            "Epoch  14 Batch  427 / 525  Training Loss  0.015054471790790558\n",
            "Epoch  14 Batch  428 / 525  Training Loss  0.023457597941160202\n",
            "Epoch  14 Batch  429 / 525  Training Loss  0.015168257057666779\n",
            "Epoch  14 Batch  430 / 525  Training Loss  0.009695672430098057\n",
            "Epoch  14 Batch  431 / 525  Training Loss  0.026798183098435402\n",
            "Epoch  14 Batch  432 / 525  Training Loss  0.013923381455242634\n",
            "Epoch  14 Batch  433 / 525  Training Loss  0.019461732357740402\n",
            "Epoch  14 Batch  434 / 525  Training Loss  0.030042443424463272\n",
            "Epoch  14 Batch  435 / 525  Training Loss  0.030037572607398033\n",
            "Epoch  14 Batch  436 / 525  Training Loss  0.022025126963853836\n",
            "Epoch  14 Batch  437 / 525  Training Loss  0.013818047940731049\n",
            "Epoch  14 Batch  438 / 525  Training Loss  0.02158370427787304\n",
            "Epoch  14 Batch  439 / 525  Training Loss  0.017894629389047623\n",
            "Epoch  14 Batch  440 / 525  Training Loss  0.0304994136095047\n",
            "Epoch  14 Batch  441 / 525  Training Loss  0.018630634993314743\n",
            "Epoch  14 Batch  442 / 525  Training Loss  0.015749933198094368\n",
            "Epoch  14 Batch  443 / 525  Training Loss  0.019014468416571617\n",
            "Epoch  14 Batch  444 / 525  Training Loss  0.01427591871470213\n",
            "Epoch  14 Batch  445 / 525  Training Loss  0.036361079663038254\n",
            "Epoch  14 Batch  446 / 525  Training Loss  0.013998542912304401\n",
            "Epoch  14 Batch  447 / 525  Training Loss  0.01915908232331276\n",
            "Epoch  14 Batch  448 / 525  Training Loss  0.029837345704436302\n",
            "Epoch  14 Batch  449 / 525  Training Loss  0.02627182938158512\n",
            "Epoch  14 Batch  450 / 525  Training Loss  0.03163415938615799\n",
            "Epoch  14 Batch  451 / 525  Training Loss  0.02821199595928192\n",
            "Epoch  14 Batch  452 / 525  Training Loss  0.034960243850946426\n",
            "Epoch  14 Batch  453 / 525  Training Loss  0.03665231168270111\n",
            "Epoch  14 Batch  454 / 525  Training Loss  0.03351398929953575\n",
            "Epoch  14 Batch  455 / 525  Training Loss  0.033769115805625916\n",
            "Epoch  14 Batch  456 / 525  Training Loss  0.029171636328101158\n",
            "Epoch  14 Batch  457 / 525  Training Loss  0.0119076669216156\n",
            "Epoch  14 Batch  458 / 525  Training Loss  0.03390100598335266\n",
            "Epoch  14 Batch  459 / 525  Training Loss  0.019450301304459572\n",
            "Epoch  14 Batch  460 / 525  Training Loss  0.03290969505906105\n",
            "Epoch  14 Batch  461 / 525  Training Loss  0.025474488735198975\n",
            "Epoch  14 Batch  462 / 525  Training Loss  0.0166225153952837\n",
            "Epoch  14 Batch  463 / 525  Training Loss  0.02462082915008068\n",
            "Epoch  14 Batch  464 / 525  Training Loss  0.01442372053861618\n",
            "Epoch  14 Batch  465 / 525  Training Loss  0.020370442420244217\n",
            "Epoch  14 Batch  466 / 525  Training Loss  0.0284752044826746\n",
            "Epoch  14 Batch  467 / 525  Training Loss  0.037698522210121155\n",
            "Epoch  14 Batch  468 / 525  Training Loss  0.0292510986328125\n",
            "Epoch  14 Batch  469 / 525  Training Loss  0.0197477824985981\n",
            "Epoch  14 Batch  470 / 525  Training Loss  0.030802667140960693\n",
            "Epoch  14 Batch  471 / 525  Training Loss  0.03443421423435211\n",
            "Epoch  14 Batch  472 / 525  Training Loss  0.032225601375103\n",
            "Epoch  14 Batch  473 / 525  Training Loss  0.02941860817372799\n",
            "Epoch  14 Batch  474 / 525  Training Loss  0.023052938282489777\n",
            "Epoch  14 Batch  475 / 525  Training Loss  0.019595380872488022\n",
            "Epoch  14 Batch  476 / 525  Training Loss  0.0251136627048254\n",
            "Epoch  14 Batch  477 / 525  Training Loss  0.02360893413424492\n",
            "Epoch  14 Batch  478 / 525  Training Loss  0.029614290222525597\n",
            "Epoch  14 Batch  479 / 525  Training Loss  0.016564276069402695\n",
            "Epoch  14 Batch  480 / 525  Training Loss  0.023617874830961227\n",
            "Epoch  14 Batch  481 / 525  Training Loss  0.024985287338495255\n",
            "Epoch  14 Batch  482 / 525  Training Loss  0.02663366124033928\n",
            "Epoch  14 Batch  483 / 525  Training Loss  0.021350350230932236\n",
            "Epoch  14 Batch  484 / 525  Training Loss  0.022713907063007355\n",
            "Epoch  14 Batch  485 / 525  Training Loss  0.03482041135430336\n",
            "Epoch  14 Batch  486 / 525  Training Loss  0.01761256530880928\n",
            "Epoch  14 Batch  487 / 525  Training Loss  0.029846038669347763\n",
            "Epoch  14 Batch  488 / 525  Training Loss  0.020218500867486\n",
            "Epoch  14 Batch  489 / 525  Training Loss  0.028830762952566147\n",
            "Epoch  14 Batch  490 / 525  Training Loss  0.024073656648397446\n",
            "Epoch  14 Batch  491 / 525  Training Loss  0.018313074484467506\n",
            "Epoch  14 Batch  492 / 525  Training Loss  0.013915067538619041\n",
            "Epoch  14 Batch  493 / 525  Training Loss  0.014116376638412476\n",
            "Epoch  14 Batch  494 / 525  Training Loss  0.015407259576022625\n",
            "Epoch  14 Batch  495 / 525  Training Loss  0.034053776413202286\n",
            "Epoch  14 Batch  496 / 525  Training Loss  0.02804619073867798\n",
            "Epoch  14 Batch  497 / 525  Training Loss  0.02611495926976204\n",
            "Epoch  14 Batch  498 / 525  Training Loss  0.024408699944615364\n",
            "Epoch  14 Batch  499 / 525  Training Loss  0.029032662510871887\n",
            "Epoch  14 Batch  500 / 525  Training Loss  0.02940138801932335\n",
            "Epoch  14 Batch  501 / 525  Training Loss  0.029528895393013954\n",
            "Epoch  14 Batch  502 / 525  Training Loss  0.020660068839788437\n",
            "Epoch  14 Batch  503 / 525  Training Loss  0.019771941006183624\n",
            "Epoch  14 Batch  504 / 525  Training Loss  0.015952300280332565\n",
            "Epoch  14 Batch  505 / 525  Training Loss  0.03084748424589634\n",
            "Epoch  14 Batch  506 / 525  Training Loss  0.03175788000226021\n",
            "Epoch  14 Batch  507 / 525  Training Loss  0.03226327896118164\n",
            "Epoch  14 Batch  508 / 525  Training Loss  0.015306557528674603\n",
            "Epoch  14 Batch  509 / 525  Training Loss  0.026664933189749718\n",
            "Epoch  14 Batch  510 / 525  Training Loss  0.01865144446492195\n",
            "Epoch  14 Batch  511 / 525  Training Loss  0.02232508920133114\n",
            "Epoch  14 Batch  512 / 525  Training Loss  0.020793482661247253\n",
            "Epoch  14 Batch  513 / 525  Training Loss  0.020589705556631088\n",
            "Epoch  14 Batch  514 / 525  Training Loss  0.016088683158159256\n",
            "Epoch  14 Batch  515 / 525  Training Loss  0.023508472368121147\n",
            "Epoch  14 Batch  516 / 525  Training Loss  0.027281757444143295\n",
            "Epoch  14 Batch  517 / 525  Training Loss  0.03719129040837288\n",
            "Epoch  14 Batch  518 / 525  Training Loss  0.02183162048459053\n",
            "Epoch  14 Batch  519 / 525  Training Loss  0.023913003504276276\n",
            "Epoch  14 Batch  520 / 525  Training Loss  0.023864535614848137\n",
            "Epoch  14 Batch  521 / 525  Training Loss  0.028697600588202477\n",
            "Epoch  14 Batch  522 / 525  Training Loss  0.0328657329082489\n",
            "Epoch  14 Batch  523 / 525  Training Loss  0.02684573456645012\n",
            "Epoch  14 Batch  524 / 525  Training Loss  0.034160904586315155\n",
            "  15    |    -    |   0.023018   | 54.625000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 15\n",
            "Epoch  15 Batch  0 / 525  Training Loss  0.020282933488488197\n",
            "Epoch  15 Batch  1 / 525  Training Loss  0.013674924150109291\n",
            "Epoch  15 Batch  2 / 525  Training Loss  0.008304961957037449\n",
            "Epoch  15 Batch  3 / 525  Training Loss  0.016704952344298363\n",
            "Epoch  15 Batch  4 / 525  Training Loss  0.02038988471031189\n",
            "Epoch  15 Batch  5 / 525  Training Loss  0.019349608570337296\n",
            "Epoch  15 Batch  6 / 525  Training Loss  0.007684799376875162\n",
            "Epoch  15 Batch  7 / 525  Training Loss  0.01566602662205696\n",
            "Epoch  15 Batch  8 / 525  Training Loss  0.013447357341647148\n",
            "Epoch  15 Batch  9 / 525  Training Loss  0.0219131987541914\n",
            "Epoch  15 Batch  10 / 525  Training Loss  0.013247346505522728\n",
            "Epoch  15 Batch  11 / 525  Training Loss  0.01760682463645935\n",
            "Epoch  15 Batch  12 / 525  Training Loss  0.015765242278575897\n",
            "Epoch  15 Batch  13 / 525  Training Loss  0.014649340882897377\n",
            "Epoch  15 Batch  14 / 525  Training Loss  0.014268802478909492\n",
            "Epoch  15 Batch  15 / 525  Training Loss  0.018958749249577522\n",
            "Epoch  15 Batch  16 / 525  Training Loss  0.020971421152353287\n",
            "Epoch  15 Batch  17 / 525  Training Loss  0.02453692816197872\n",
            "Epoch  15 Batch  18 / 525  Training Loss  0.013425610959529877\n",
            "Epoch  15 Batch  19 / 525  Training Loss  0.014777427539229393\n",
            "Epoch  15 Batch  20 / 525  Training Loss  0.008402258157730103\n",
            "Epoch  15 Batch  21 / 525  Training Loss  0.017965106293559074\n",
            "Epoch  15 Batch  22 / 525  Training Loss  0.013127734884619713\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  15 Batch  23 / 525  Training Loss  0.00894118007272482\n",
            "Epoch  15 Batch  24 / 525  Training Loss  0.0176502987742424\n",
            "Epoch  15 Batch  25 / 525  Training Loss  0.010949771851301193\n",
            "Epoch  15 Batch  26 / 525  Training Loss  0.014499098062515259\n",
            "Epoch  15 Batch  27 / 525  Training Loss  0.015563363209366798\n",
            "Epoch  15 Batch  28 / 525  Training Loss  0.017955604940652847\n",
            "Epoch  15 Batch  29 / 525  Training Loss  0.014656653627753258\n",
            "Epoch  15 Batch  30 / 525  Training Loss  0.008761873468756676\n",
            "Epoch  15 Batch  31 / 525  Training Loss  0.02244538627564907\n",
            "Epoch  15 Batch  32 / 525  Training Loss  0.01638680137693882\n",
            "Epoch  15 Batch  33 / 525  Training Loss  0.013011532835662365\n",
            "Epoch  15 Batch  34 / 525  Training Loss  0.010384270921349525\n",
            "Epoch  15 Batch  35 / 525  Training Loss  0.03175616264343262\n",
            "Epoch  15 Batch  36 / 525  Training Loss  0.014233991503715515\n",
            "Epoch  15 Batch  37 / 525  Training Loss  0.01119500957429409\n",
            "Epoch  15 Batch  38 / 525  Training Loss  0.014969095587730408\n",
            "Epoch  15 Batch  39 / 525  Training Loss  0.02543141320347786\n",
            "Epoch  15 Batch  40 / 525  Training Loss  0.01754939928650856\n",
            "Epoch  15 Batch  41 / 525  Training Loss  0.008986763656139374\n",
            "Epoch  15 Batch  42 / 525  Training Loss  0.012173888273537159\n",
            "Epoch  15 Batch  43 / 525  Training Loss  0.01915094256401062\n",
            "Epoch  15 Batch  44 / 525  Training Loss  0.016679441556334496\n",
            "Epoch  15 Batch  45 / 525  Training Loss  0.02920171245932579\n",
            "Epoch  15 Batch  46 / 525  Training Loss  0.015908699482679367\n",
            "Epoch  15 Batch  47 / 525  Training Loss  0.011572057381272316\n",
            "Epoch  15 Batch  48 / 525  Training Loss  0.006796577479690313\n",
            "Epoch  15 Batch  49 / 525  Training Loss  0.011897197924554348\n",
            "Epoch  15 Batch  50 / 525  Training Loss  0.011686218902468681\n",
            "Epoch  15 Batch  51 / 525  Training Loss  0.016627248376607895\n",
            "Epoch  15 Batch  52 / 525  Training Loss  0.017722394317388535\n",
            "Epoch  15 Batch  53 / 525  Training Loss  0.020604494959115982\n",
            "Epoch  15 Batch  54 / 525  Training Loss  0.01582374796271324\n",
            "Epoch  15 Batch  55 / 525  Training Loss  0.01598339155316353\n",
            "Epoch  15 Batch  56 / 525  Training Loss  0.013206186704337597\n",
            "Epoch  15 Batch  57 / 525  Training Loss  0.011503389105200768\n",
            "Epoch  15 Batch  58 / 525  Training Loss  0.013706833124160767\n",
            "Epoch  15 Batch  59 / 525  Training Loss  0.024798419326543808\n",
            "Epoch  15 Batch  60 / 525  Training Loss  0.018772535026073456\n",
            "Epoch  15 Batch  61 / 525  Training Loss  0.02617904543876648\n",
            "Epoch  15 Batch  62 / 525  Training Loss  0.01762024685740471\n",
            "Epoch  15 Batch  63 / 525  Training Loss  0.017583470791578293\n",
            "Epoch  15 Batch  64 / 525  Training Loss  0.013717216439545155\n",
            "Epoch  15 Batch  65 / 525  Training Loss  0.01553414948284626\n",
            "Epoch  15 Batch  66 / 525  Training Loss  0.02288573794066906\n",
            "Epoch  15 Batch  67 / 525  Training Loss  0.020725809037685394\n",
            "Epoch  15 Batch  68 / 525  Training Loss  0.018109982833266258\n",
            "Epoch  15 Batch  69 / 525  Training Loss  0.022024577483534813\n",
            "Epoch  15 Batch  70 / 525  Training Loss  0.022032493725419044\n",
            "Epoch  15 Batch  71 / 525  Training Loss  0.01692705601453781\n",
            "Epoch  15 Batch  72 / 525  Training Loss  0.013212611898779869\n",
            "Epoch  15 Batch  73 / 525  Training Loss  0.010911261662840843\n",
            "Epoch  15 Batch  74 / 525  Training Loss  0.01685183122754097\n",
            "Epoch  15 Batch  75 / 525  Training Loss  0.02078324928879738\n",
            "Epoch  15 Batch  76 / 525  Training Loss  0.01468880008906126\n",
            "Epoch  15 Batch  77 / 525  Training Loss  0.015875840559601784\n",
            "Epoch  15 Batch  78 / 525  Training Loss  0.010839050635695457\n",
            "Epoch  15 Batch  79 / 525  Training Loss  0.020755821838974953\n",
            "Epoch  15 Batch  80 / 525  Training Loss  0.018206264823675156\n",
            "Epoch  15 Batch  81 / 525  Training Loss  0.009274780750274658\n",
            "Epoch  15 Batch  82 / 525  Training Loss  0.02240602672100067\n",
            "Epoch  15 Batch  83 / 525  Training Loss  0.017253395169973373\n",
            "Epoch  15 Batch  84 / 525  Training Loss  0.016182933002710342\n",
            "Epoch  15 Batch  85 / 525  Training Loss  0.00843852385878563\n",
            "Epoch  15 Batch  86 / 525  Training Loss  0.017383668571710587\n",
            "Epoch  15 Batch  87 / 525  Training Loss  0.01272490806877613\n",
            "Epoch  15 Batch  88 / 525  Training Loss  0.01363630872219801\n",
            "Epoch  15 Batch  89 / 525  Training Loss  0.014685861766338348\n",
            "Epoch  15 Batch  90 / 525  Training Loss  0.011874242685735226\n",
            "Epoch  15 Batch  91 / 525  Training Loss  0.01975952833890915\n",
            "Epoch  15 Batch  92 / 525  Training Loss  0.02386012114584446\n",
            "Epoch  15 Batch  93 / 525  Training Loss  0.008288806304335594\n",
            "Epoch  15 Batch  94 / 525  Training Loss  0.008191016502678394\n",
            "Epoch  15 Batch  95 / 525  Training Loss  0.015201729722321033\n",
            "Epoch  15 Batch  96 / 525  Training Loss  0.017743179574608803\n",
            "Epoch  15 Batch  97 / 525  Training Loss  0.01791592314839363\n",
            "Epoch  15 Batch  98 / 525  Training Loss  0.02063356526196003\n",
            "Epoch  15 Batch  99 / 525  Training Loss  0.01294042356312275\n",
            "Epoch  15 Batch  100 / 525  Training Loss  0.012974655255675316\n",
            "Epoch  15 Batch  101 / 525  Training Loss  0.020711366087198257\n",
            "Epoch  15 Batch  102 / 525  Training Loss  0.014310549013316631\n",
            "Epoch  15 Batch  103 / 525  Training Loss  0.008127199485898018\n",
            "Epoch  15 Batch  104 / 525  Training Loss  0.01810542121529579\n",
            "Epoch  15 Batch  105 / 525  Training Loss  0.01705608330667019\n",
            "Epoch  15 Batch  106 / 525  Training Loss  0.019555281847715378\n",
            "Epoch  15 Batch  107 / 525  Training Loss  0.02169404737651348\n",
            "Epoch  15 Batch  108 / 525  Training Loss  0.013402057811617851\n",
            "Epoch  15 Batch  109 / 525  Training Loss  0.009893250651657581\n",
            "Epoch  15 Batch  110 / 525  Training Loss  0.016630006954073906\n",
            "Epoch  15 Batch  111 / 525  Training Loss  0.021429140120744705\n",
            "Epoch  15 Batch  112 / 525  Training Loss  0.015296513214707375\n",
            "Epoch  15 Batch  113 / 525  Training Loss  0.01740935817360878\n",
            "Epoch  15 Batch  114 / 525  Training Loss  0.01656986214220524\n",
            "Epoch  15 Batch  115 / 525  Training Loss  0.016526231542229652\n",
            "Epoch  15 Batch  116 / 525  Training Loss  0.010805051773786545\n",
            "Epoch  15 Batch  117 / 525  Training Loss  0.01872211880981922\n",
            "Epoch  15 Batch  118 / 525  Training Loss  0.01420525461435318\n",
            "Epoch  15 Batch  119 / 525  Training Loss  0.014362597838044167\n",
            "Epoch  15 Batch  120 / 525  Training Loss  0.0216328464448452\n",
            "Epoch  15 Batch  121 / 525  Training Loss  0.012937523424625397\n",
            "Epoch  15 Batch  122 / 525  Training Loss  0.007789391092956066\n",
            "Epoch  15 Batch  123 / 525  Training Loss  0.016994668170809746\n",
            "Epoch  15 Batch  124 / 525  Training Loss  0.018231360241770744\n",
            "Epoch  15 Batch  125 / 525  Training Loss  0.011899404227733612\n",
            "Epoch  15 Batch  126 / 525  Training Loss  0.02323562279343605\n",
            "Epoch  15 Batch  127 / 525  Training Loss  0.019527796655893326\n",
            "Epoch  15 Batch  128 / 525  Training Loss  0.02382308803498745\n",
            "Epoch  15 Batch  129 / 525  Training Loss  0.01762160286307335\n",
            "Epoch  15 Batch  130 / 525  Training Loss  0.024218890815973282\n",
            "Epoch  15 Batch  131 / 525  Training Loss  0.016193373128771782\n",
            "Epoch  15 Batch  132 / 525  Training Loss  0.0212418120354414\n",
            "Epoch  15 Batch  133 / 525  Training Loss  0.015719609335064888\n",
            "Epoch  15 Batch  134 / 525  Training Loss  0.015945283696055412\n",
            "Epoch  15 Batch  135 / 525  Training Loss  0.009340633638203144\n",
            "Epoch  15 Batch  136 / 525  Training Loss  0.023842837661504745\n",
            "Epoch  15 Batch  137 / 525  Training Loss  0.018134446814656258\n",
            "Epoch  15 Batch  138 / 525  Training Loss  0.012875725515186787\n",
            "Epoch  15 Batch  139 / 525  Training Loss  0.0077153099700808525\n",
            "Epoch  15 Batch  140 / 525  Training Loss  0.02402043528854847\n",
            "Epoch  15 Batch  141 / 525  Training Loss  0.02400868758559227\n",
            "Epoch  15 Batch  142 / 525  Training Loss  0.015268169343471527\n",
            "Epoch  15 Batch  143 / 525  Training Loss  0.01640135422348976\n",
            "Epoch  15 Batch  144 / 525  Training Loss  0.01815510168671608\n",
            "Epoch  15 Batch  145 / 525  Training Loss  0.016812535002827644\n",
            "Epoch  15 Batch  146 / 525  Training Loss  0.021064819768071175\n",
            "Epoch  15 Batch  147 / 525  Training Loss  0.011527551338076591\n",
            "Epoch  15 Batch  148 / 525  Training Loss  0.014479284174740314\n",
            "Epoch  15 Batch  149 / 525  Training Loss  0.011405076831579208\n",
            "Epoch  15 Batch  150 / 525  Training Loss  0.010304572992026806\n",
            "Epoch  15 Batch  151 / 525  Training Loss  0.006875643040984869\n",
            "Epoch  15 Batch  152 / 525  Training Loss  0.019886638969182968\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  15 Batch  153 / 525  Training Loss  0.019739225506782532\n",
            "Epoch  15 Batch  154 / 525  Training Loss  0.013277341611683369\n",
            "Epoch  15 Batch  155 / 525  Training Loss  0.007428938057273626\n",
            "Epoch  15 Batch  156 / 525  Training Loss  0.024270707741379738\n",
            "Epoch  15 Batch  157 / 525  Training Loss  0.010814246721565723\n",
            "Epoch  15 Batch  158 / 525  Training Loss  0.010726540349423885\n",
            "Epoch  15 Batch  159 / 525  Training Loss  0.015257418155670166\n",
            "Epoch  15 Batch  160 / 525  Training Loss  0.014183488674461842\n",
            "Epoch  15 Batch  161 / 525  Training Loss  0.012760380282998085\n",
            "Epoch  15 Batch  162 / 525  Training Loss  0.01981007121503353\n",
            "Epoch  15 Batch  163 / 525  Training Loss  0.029269477352499962\n",
            "Epoch  15 Batch  164 / 525  Training Loss  0.02120041847229004\n",
            "Epoch  15 Batch  165 / 525  Training Loss  0.013489456847310066\n",
            "Epoch  15 Batch  166 / 525  Training Loss  0.019722385331988335\n",
            "Epoch  15 Batch  167 / 525  Training Loss  0.012392277829349041\n",
            "Epoch  15 Batch  168 / 525  Training Loss  0.020504826679825783\n",
            "Epoch  15 Batch  169 / 525  Training Loss  0.015738042071461678\n",
            "Epoch  15 Batch  170 / 525  Training Loss  0.00809570960700512\n",
            "Epoch  15 Batch  171 / 525  Training Loss  0.015138139016926289\n",
            "Epoch  15 Batch  172 / 525  Training Loss  0.007064078003168106\n",
            "Epoch  15 Batch  173 / 525  Training Loss  0.013033715076744556\n",
            "Epoch  15 Batch  174 / 525  Training Loss  0.021346278488636017\n",
            "Epoch  15 Batch  175 / 525  Training Loss  0.018214086070656776\n",
            "Epoch  15 Batch  176 / 525  Training Loss  0.025627832859754562\n",
            "Epoch  15 Batch  177 / 525  Training Loss  0.014708153903484344\n",
            "Epoch  15 Batch  178 / 525  Training Loss  0.015348481014370918\n",
            "Epoch  15 Batch  179 / 525  Training Loss  0.01773861236870289\n",
            "Epoch  15 Batch  180 / 525  Training Loss  0.00866498239338398\n",
            "Epoch  15 Batch  181 / 525  Training Loss  0.015256847254931927\n",
            "Epoch  15 Batch  182 / 525  Training Loss  0.007743318565189838\n",
            "Epoch  15 Batch  183 / 525  Training Loss  0.011425112374126911\n",
            "Epoch  15 Batch  184 / 525  Training Loss  0.014152789488434792\n",
            "Epoch  15 Batch  185 / 525  Training Loss  0.009447457268834114\n",
            "Epoch  15 Batch  186 / 525  Training Loss  0.006818824913352728\n",
            "Epoch  15 Batch  187 / 525  Training Loss  0.011406468227505684\n",
            "Epoch  15 Batch  188 / 525  Training Loss  0.019758030772209167\n",
            "Epoch  15 Batch  189 / 525  Training Loss  0.023368649184703827\n",
            "Epoch  15 Batch  190 / 525  Training Loss  0.027444710955023766\n",
            "Epoch  15 Batch  191 / 525  Training Loss  0.01670125685632229\n",
            "Epoch  15 Batch  192 / 525  Training Loss  0.01636398769915104\n",
            "Epoch  15 Batch  193 / 525  Training Loss  0.011949330568313599\n",
            "Epoch  15 Batch  194 / 525  Training Loss  0.01780209317803383\n",
            "Epoch  15 Batch  195 / 525  Training Loss  0.012740922160446644\n",
            "Epoch  15 Batch  196 / 525  Training Loss  0.02830050326883793\n",
            "Epoch  15 Batch  197 / 525  Training Loss  0.023334935307502747\n",
            "Epoch  15 Batch  198 / 525  Training Loss  0.022752325981855392\n",
            "Epoch  15 Batch  199 / 525  Training Loss  0.012170396745204926\n",
            "Epoch  15 Batch  200 / 525  Training Loss  0.025446459650993347\n",
            "Epoch  15 Batch  201 / 525  Training Loss  0.012019788846373558\n",
            "Epoch  15 Batch  202 / 525  Training Loss  0.014444222673773766\n",
            "Epoch  15 Batch  203 / 525  Training Loss  0.009965501725673676\n",
            "Epoch  15 Batch  204 / 525  Training Loss  0.021747592836618423\n",
            "Epoch  15 Batch  205 / 525  Training Loss  0.03011690452694893\n",
            "Epoch  15 Batch  206 / 525  Training Loss  0.017306338995695114\n",
            "Epoch  15 Batch  207 / 525  Training Loss  0.022720063105225563\n",
            "Epoch  15 Batch  208 / 525  Training Loss  0.014362946152687073\n",
            "Epoch  15 Batch  209 / 525  Training Loss  0.023388799279928207\n",
            "Epoch  15 Batch  210 / 525  Training Loss  0.015593908727169037\n",
            "Epoch  15 Batch  211 / 525  Training Loss  0.01948470249772072\n",
            "Epoch  15 Batch  212 / 525  Training Loss  0.013298260048031807\n",
            "Epoch  15 Batch  213 / 525  Training Loss  0.014451704919338226\n",
            "Epoch  15 Batch  214 / 525  Training Loss  0.025533026084303856\n",
            "Epoch  15 Batch  215 / 525  Training Loss  0.01651996374130249\n",
            "Epoch  15 Batch  216 / 525  Training Loss  0.013940388336777687\n",
            "Epoch  15 Batch  217 / 525  Training Loss  0.011865432374179363\n",
            "Epoch  15 Batch  218 / 525  Training Loss  0.02549637481570244\n",
            "Epoch  15 Batch  219 / 525  Training Loss  0.0167952049523592\n",
            "Epoch  15 Batch  220 / 525  Training Loss  0.02219516411423683\n",
            "Epoch  15 Batch  221 / 525  Training Loss  0.006691484246402979\n",
            "Epoch  15 Batch  222 / 525  Training Loss  0.006370748393237591\n",
            "Epoch  15 Batch  223 / 525  Training Loss  0.02761428989470005\n",
            "Epoch  15 Batch  224 / 525  Training Loss  0.013812720775604248\n",
            "Epoch  15 Batch  225 / 525  Training Loss  0.012007581070065498\n",
            "Epoch  15 Batch  226 / 525  Training Loss  0.011845597997307777\n",
            "Epoch  15 Batch  227 / 525  Training Loss  0.009698567911982536\n",
            "Epoch  15 Batch  228 / 525  Training Loss  0.008951710537075996\n",
            "Epoch  15 Batch  229 / 525  Training Loss  0.01937699131667614\n",
            "Epoch  15 Batch  230 / 525  Training Loss  0.02208181843161583\n",
            "Epoch  15 Batch  231 / 525  Training Loss  0.01880798302590847\n",
            "Epoch  15 Batch  232 / 525  Training Loss  0.013870744034647942\n",
            "Epoch  15 Batch  233 / 525  Training Loss  0.030545899644494057\n",
            "Epoch  15 Batch  234 / 525  Training Loss  0.016480736434459686\n",
            "Epoch  15 Batch  235 / 525  Training Loss  0.032899655401706696\n",
            "Epoch  15 Batch  236 / 525  Training Loss  0.01282478403300047\n",
            "Epoch  15 Batch  237 / 525  Training Loss  0.022859835997223854\n",
            "Epoch  15 Batch  238 / 525  Training Loss  0.017755139619112015\n",
            "Epoch  15 Batch  239 / 525  Training Loss  0.017422359436750412\n",
            "Epoch  15 Batch  240 / 525  Training Loss  0.017334196716547012\n",
            "Epoch  15 Batch  241 / 525  Training Loss  0.008464647457003593\n",
            "Epoch  15 Batch  242 / 525  Training Loss  0.017564354464411736\n",
            "Epoch  15 Batch  243 / 525  Training Loss  0.009347495622932911\n",
            "Epoch  15 Batch  244 / 525  Training Loss  0.018438663333654404\n",
            "Epoch  15 Batch  245 / 525  Training Loss  0.012526188977062702\n",
            "Epoch  15 Batch  246 / 525  Training Loss  0.022330494597554207\n",
            "Epoch  15 Batch  247 / 525  Training Loss  0.01075734756886959\n",
            "Epoch  15 Batch  248 / 525  Training Loss  0.016518864780664444\n",
            "Epoch  15 Batch  249 / 525  Training Loss  0.018185624852776527\n",
            "Epoch  15 Batch  250 / 525  Training Loss  0.02046782337129116\n",
            "Epoch  15 Batch  251 / 525  Training Loss  0.032750453799963\n",
            "Epoch  15 Batch  252 / 525  Training Loss  0.01247306540608406\n",
            "Epoch  15 Batch  253 / 525  Training Loss  0.029493633657693863\n",
            "Epoch  15 Batch  254 / 525  Training Loss  0.02247679978609085\n",
            "Epoch  15 Batch  255 / 525  Training Loss  0.019531184807419777\n",
            "Epoch  15 Batch  256 / 525  Training Loss  0.017693527042865753\n",
            "Epoch  15 Batch  257 / 525  Training Loss  0.02740141749382019\n",
            "Epoch  15 Batch  258 / 525  Training Loss  0.014443404972553253\n",
            "Epoch  15 Batch  259 / 525  Training Loss  0.024648819118738174\n",
            "Epoch  15 Batch  260 / 525  Training Loss  0.018925726413726807\n",
            "Epoch  15 Batch  261 / 525  Training Loss  0.014235831797122955\n",
            "Epoch  15 Batch  262 / 525  Training Loss  0.011935796588659286\n",
            "Epoch  15 Batch  263 / 525  Training Loss  0.014859406277537346\n",
            "Epoch  15 Batch  264 / 525  Training Loss  0.01407712697982788\n",
            "Epoch  15 Batch  265 / 525  Training Loss  0.016398342326283455\n",
            "Epoch  15 Batch  266 / 525  Training Loss  0.01688760332763195\n",
            "Epoch  15 Batch  267 / 525  Training Loss  0.01532371062785387\n",
            "Epoch  15 Batch  268 / 525  Training Loss  0.025655299425125122\n",
            "Epoch  15 Batch  269 / 525  Training Loss  0.014039183966815472\n",
            "Epoch  15 Batch  270 / 525  Training Loss  0.023169655352830887\n",
            "Epoch  15 Batch  271 / 525  Training Loss  0.01743376813828945\n",
            "Epoch  15 Batch  272 / 525  Training Loss  0.019282963126897812\n",
            "Epoch  15 Batch  273 / 525  Training Loss  0.018648702651262283\n",
            "Epoch  15 Batch  274 / 525  Training Loss  0.015811214223504066\n",
            "Epoch  15 Batch  275 / 525  Training Loss  0.010471763089299202\n",
            "Epoch  15 Batch  276 / 525  Training Loss  0.017300201579928398\n",
            "Epoch  15 Batch  277 / 525  Training Loss  0.015044177882373333\n",
            "Epoch  15 Batch  278 / 525  Training Loss  0.012433118186891079\n",
            "Epoch  15 Batch  279 / 525  Training Loss  0.013339415192604065\n",
            "Epoch  15 Batch  280 / 525  Training Loss  0.022738473489880562\n",
            "Epoch  15 Batch  281 / 525  Training Loss  0.017759649083018303\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  15 Batch  282 / 525  Training Loss  0.010294430889189243\n",
            "Epoch  15 Batch  283 / 525  Training Loss  0.018647152930498123\n",
            "Epoch  15 Batch  284 / 525  Training Loss  0.01585552841424942\n",
            "Epoch  15 Batch  285 / 525  Training Loss  0.018963292241096497\n",
            "Epoch  15 Batch  286 / 525  Training Loss  0.007852037437260151\n",
            "Epoch  15 Batch  287 / 525  Training Loss  0.012970706447958946\n",
            "Epoch  15 Batch  288 / 525  Training Loss  0.015620818361639977\n",
            "Epoch  15 Batch  289 / 525  Training Loss  0.016135651618242264\n",
            "Epoch  15 Batch  290 / 525  Training Loss  0.01513150054961443\n",
            "Epoch  15 Batch  291 / 525  Training Loss  0.019567951560020447\n",
            "Epoch  15 Batch  292 / 525  Training Loss  0.026869606226682663\n",
            "Epoch  15 Batch  293 / 525  Training Loss  0.012726390734314919\n",
            "Epoch  15 Batch  294 / 525  Training Loss  0.010086851194500923\n",
            "Epoch  15 Batch  295 / 525  Training Loss  0.023775603622198105\n",
            "Epoch  15 Batch  296 / 525  Training Loss  0.024059705436229706\n",
            "Epoch  15 Batch  297 / 525  Training Loss  0.014310169033706188\n",
            "Epoch  15 Batch  298 / 525  Training Loss  0.02448849566280842\n",
            "Epoch  15 Batch  299 / 525  Training Loss  0.015436339192092419\n",
            "Epoch  15 Batch  300 / 525  Training Loss  0.017778821289539337\n",
            "Epoch  15 Batch  301 / 525  Training Loss  0.015799179673194885\n",
            "Epoch  15 Batch  302 / 525  Training Loss  0.015271589159965515\n",
            "Epoch  15 Batch  303 / 525  Training Loss  0.02104189060628414\n",
            "Epoch  15 Batch  304 / 525  Training Loss  0.019384250044822693\n",
            "Epoch  15 Batch  305 / 525  Training Loss  0.011515388265252113\n",
            "Epoch  15 Batch  306 / 525  Training Loss  0.013232584111392498\n",
            "Epoch  15 Batch  307 / 525  Training Loss  0.009671451523900032\n",
            "Epoch  15 Batch  308 / 525  Training Loss  0.020853323861956596\n",
            "Epoch  15 Batch  309 / 525  Training Loss  0.013802153058350086\n",
            "Epoch  15 Batch  310 / 525  Training Loss  0.018077652901411057\n",
            "Epoch  15 Batch  311 / 525  Training Loss  0.02136906422674656\n",
            "Epoch  15 Batch  312 / 525  Training Loss  0.017607709392905235\n",
            "Epoch  15 Batch  313 / 525  Training Loss  0.017238769680261612\n",
            "Epoch  15 Batch  314 / 525  Training Loss  0.012634689919650555\n",
            "Epoch  15 Batch  315 / 525  Training Loss  0.0124512929469347\n",
            "Epoch  15 Batch  316 / 525  Training Loss  0.01526733674108982\n",
            "Epoch  15 Batch  317 / 525  Training Loss  0.021569665521383286\n",
            "Epoch  15 Batch  318 / 525  Training Loss  0.013447381556034088\n",
            "Epoch  15 Batch  319 / 525  Training Loss  0.01286996714770794\n",
            "Epoch  15 Batch  320 / 525  Training Loss  0.012717139907181263\n",
            "Epoch  15 Batch  321 / 525  Training Loss  0.01422025728970766\n",
            "Epoch  15 Batch  322 / 525  Training Loss  0.017618665471673012\n",
            "Epoch  15 Batch  323 / 525  Training Loss  0.018169552087783813\n",
            "Epoch  15 Batch  324 / 525  Training Loss  0.017824653536081314\n",
            "Epoch  15 Batch  325 / 525  Training Loss  0.021485209465026855\n",
            "Epoch  15 Batch  326 / 525  Training Loss  0.014378910884261131\n",
            "Epoch  15 Batch  327 / 525  Training Loss  0.024209944531321526\n",
            "Epoch  15 Batch  328 / 525  Training Loss  0.017683466896414757\n",
            "Epoch  15 Batch  329 / 525  Training Loss  0.02563057281076908\n",
            "Epoch  15 Batch  330 / 525  Training Loss  0.014591378159821033\n",
            "Epoch  15 Batch  331 / 525  Training Loss  0.01960829645395279\n",
            "Epoch  15 Batch  332 / 525  Training Loss  0.01856914348900318\n",
            "Epoch  15 Batch  333 / 525  Training Loss  0.011905505321919918\n",
            "Epoch  15 Batch  334 / 525  Training Loss  0.018339049071073532\n",
            "Epoch  15 Batch  335 / 525  Training Loss  0.02389831282198429\n",
            "Epoch  15 Batch  336 / 525  Training Loss  0.015565020963549614\n",
            "Epoch  15 Batch  337 / 525  Training Loss  0.018061364069581032\n",
            "Epoch  15 Batch  338 / 525  Training Loss  0.016175314784049988\n",
            "Epoch  15 Batch  339 / 525  Training Loss  0.018576961010694504\n",
            "Epoch  15 Batch  340 / 525  Training Loss  0.015968482941389084\n",
            "Epoch  15 Batch  341 / 525  Training Loss  0.01339210756123066\n",
            "Epoch  15 Batch  342 / 525  Training Loss  0.016792403534054756\n",
            "Epoch  15 Batch  343 / 525  Training Loss  0.014034068211913109\n",
            "Epoch  15 Batch  344 / 525  Training Loss  0.014587928541004658\n",
            "Epoch  15 Batch  345 / 525  Training Loss  0.02001490816473961\n",
            "Epoch  15 Batch  346 / 525  Training Loss  0.011363297700881958\n",
            "Epoch  15 Batch  347 / 525  Training Loss  0.011479050852358341\n",
            "Epoch  15 Batch  348 / 525  Training Loss  0.012004593387246132\n",
            "Epoch  15 Batch  349 / 525  Training Loss  0.026134375482797623\n",
            "Epoch  15 Batch  350 / 525  Training Loss  0.010607213713228703\n",
            "Epoch  15 Batch  351 / 525  Training Loss  0.016330499202013016\n",
            "Epoch  15 Batch  352 / 525  Training Loss  0.014084038324654102\n",
            "Epoch  15 Batch  353 / 525  Training Loss  0.018131980672478676\n",
            "Epoch  15 Batch  354 / 525  Training Loss  0.027321195229887962\n",
            "Epoch  15 Batch  355 / 525  Training Loss  0.01891094632446766\n",
            "Epoch  15 Batch  356 / 525  Training Loss  0.013021282851696014\n",
            "Epoch  15 Batch  357 / 525  Training Loss  0.01295510120689869\n",
            "Epoch  15 Batch  358 / 525  Training Loss  0.006224862765520811\n",
            "Epoch  15 Batch  359 / 525  Training Loss  0.011039922945201397\n",
            "Epoch  15 Batch  360 / 525  Training Loss  0.021176163107156754\n",
            "Epoch  15 Batch  361 / 525  Training Loss  0.011027957312762737\n",
            "Epoch  15 Batch  362 / 525  Training Loss  0.028406372293829918\n",
            "Epoch  15 Batch  363 / 525  Training Loss  0.01826692372560501\n",
            "Epoch  15 Batch  364 / 525  Training Loss  0.011950182728469372\n",
            "Epoch  15 Batch  365 / 525  Training Loss  0.011257020756602287\n",
            "Epoch  15 Batch  366 / 525  Training Loss  0.008484682068228722\n",
            "Epoch  15 Batch  367 / 525  Training Loss  0.010848305188119411\n",
            "Epoch  15 Batch  368 / 525  Training Loss  0.01498510129749775\n",
            "Epoch  15 Batch  369 / 525  Training Loss  0.008597365580499172\n",
            "Epoch  15 Batch  370 / 525  Training Loss  0.02481023781001568\n",
            "Epoch  15 Batch  371 / 525  Training Loss  0.009246019646525383\n",
            "Epoch  15 Batch  372 / 525  Training Loss  0.022645317018032074\n",
            "Epoch  15 Batch  373 / 525  Training Loss  0.01463761180639267\n",
            "Epoch  15 Batch  374 / 525  Training Loss  0.011870583519339561\n",
            "Epoch  15 Batch  375 / 525  Training Loss  0.017181377857923508\n",
            "Epoch  15 Batch  376 / 525  Training Loss  0.027763551101088524\n",
            "Epoch  15 Batch  377 / 525  Training Loss  0.019470645114779472\n",
            "Epoch  15 Batch  378 / 525  Training Loss  0.011320354416966438\n",
            "Epoch  15 Batch  379 / 525  Training Loss  0.012041417881846428\n",
            "Epoch  15 Batch  380 / 525  Training Loss  0.018062133342027664\n",
            "Epoch  15 Batch  381 / 525  Training Loss  0.025376558303833008\n",
            "Epoch  15 Batch  382 / 525  Training Loss  0.019700806587934494\n",
            "Epoch  15 Batch  383 / 525  Training Loss  0.014915022067725658\n",
            "Epoch  15 Batch  384 / 525  Training Loss  0.01284318882972002\n",
            "Epoch  15 Batch  385 / 525  Training Loss  0.010997800156474113\n",
            "Epoch  15 Batch  386 / 525  Training Loss  0.011038856580853462\n",
            "Epoch  15 Batch  387 / 525  Training Loss  0.019679995253682137\n",
            "Epoch  15 Batch  388 / 525  Training Loss  0.011314734816551208\n",
            "Epoch  15 Batch  389 / 525  Training Loss  0.012574235908687115\n",
            "Epoch  15 Batch  390 / 525  Training Loss  0.019870270043611526\n",
            "Epoch  15 Batch  391 / 525  Training Loss  0.01068500243127346\n",
            "Epoch  15 Batch  392 / 525  Training Loss  0.024462416768074036\n",
            "Epoch  15 Batch  393 / 525  Training Loss  0.011682146228849888\n",
            "Epoch  15 Batch  394 / 525  Training Loss  0.015867963433265686\n",
            "Epoch  15 Batch  395 / 525  Training Loss  0.025439033284783363\n",
            "Epoch  15 Batch  396 / 525  Training Loss  0.01425904966890812\n",
            "Epoch  15 Batch  397 / 525  Training Loss  0.018351051956415176\n",
            "Epoch  15 Batch  398 / 525  Training Loss  0.016220061108469963\n",
            "Epoch  15 Batch  399 / 525  Training Loss  0.019234206527471542\n",
            "Epoch  15 Batch  400 / 525  Training Loss  0.013941126875579357\n",
            "Epoch  15 Batch  401 / 525  Training Loss  0.03498261794447899\n",
            "Epoch  15 Batch  402 / 525  Training Loss  0.017299819737672806\n",
            "Epoch  15 Batch  403 / 525  Training Loss  0.02277105487883091\n",
            "Epoch  15 Batch  404 / 525  Training Loss  0.022466707974672318\n",
            "Epoch  15 Batch  405 / 525  Training Loss  0.010738765820860863\n",
            "Epoch  15 Batch  406 / 525  Training Loss  0.018316607922315598\n",
            "Epoch  15 Batch  407 / 525  Training Loss  0.021825972944498062\n",
            "Epoch  15 Batch  408 / 525  Training Loss  0.010660656727850437\n",
            "Epoch  15 Batch  409 / 525  Training Loss  0.017129825428128242\n",
            "Epoch  15 Batch  410 / 525  Training Loss  0.011966816149652004\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  15 Batch  411 / 525  Training Loss  0.015660319477319717\n",
            "Epoch  15 Batch  412 / 525  Training Loss  0.018955420702695847\n",
            "Epoch  15 Batch  413 / 525  Training Loss  0.02547067403793335\n",
            "Epoch  15 Batch  414 / 525  Training Loss  0.020830761641263962\n",
            "Epoch  15 Batch  415 / 525  Training Loss  0.016370896250009537\n",
            "Epoch  15 Batch  416 / 525  Training Loss  0.03044239617884159\n",
            "Epoch  15 Batch  417 / 525  Training Loss  0.01842438615858555\n",
            "Epoch  15 Batch  418 / 525  Training Loss  0.024287501350045204\n",
            "Epoch  15 Batch  419 / 525  Training Loss  0.0224197618663311\n",
            "Epoch  15 Batch  420 / 525  Training Loss  0.01938031241297722\n",
            "Epoch  15 Batch  421 / 525  Training Loss  0.014347079209983349\n",
            "Epoch  15 Batch  422 / 525  Training Loss  0.008678935468196869\n",
            "Epoch  15 Batch  423 / 525  Training Loss  0.020669274032115936\n",
            "Epoch  15 Batch  424 / 525  Training Loss  0.014557352289557457\n",
            "Epoch  15 Batch  425 / 525  Training Loss  0.014660258777439594\n",
            "Epoch  15 Batch  426 / 525  Training Loss  0.016671117395162582\n",
            "Epoch  15 Batch  427 / 525  Training Loss  0.011867617256939411\n",
            "Epoch  15 Batch  428 / 525  Training Loss  0.014263729564845562\n",
            "Epoch  15 Batch  429 / 525  Training Loss  0.008359139785170555\n",
            "Epoch  15 Batch  430 / 525  Training Loss  0.016794797033071518\n",
            "Epoch  15 Batch  431 / 525  Training Loss  0.027847666293382645\n",
            "Epoch  15 Batch  432 / 525  Training Loss  0.02341119758784771\n",
            "Epoch  15 Batch  433 / 525  Training Loss  0.012506937608122826\n",
            "Epoch  15 Batch  434 / 525  Training Loss  0.019034143537282944\n",
            "Epoch  15 Batch  435 / 525  Training Loss  0.021629177033901215\n",
            "Epoch  15 Batch  436 / 525  Training Loss  0.013564249500632286\n",
            "Epoch  15 Batch  437 / 525  Training Loss  0.010961727239191532\n",
            "Epoch  15 Batch  438 / 525  Training Loss  0.01315735001116991\n",
            "Epoch  15 Batch  439 / 525  Training Loss  0.013568560592830181\n",
            "Epoch  15 Batch  440 / 525  Training Loss  0.014739908277988434\n",
            "Epoch  15 Batch  441 / 525  Training Loss  0.008255677297711372\n",
            "Epoch  15 Batch  442 / 525  Training Loss  0.010084109380841255\n",
            "Epoch  15 Batch  443 / 525  Training Loss  0.009971028193831444\n",
            "Epoch  15 Batch  444 / 525  Training Loss  0.019247284159064293\n",
            "Epoch  15 Batch  445 / 525  Training Loss  0.015339704230427742\n",
            "Epoch  15 Batch  446 / 525  Training Loss  0.022508028894662857\n",
            "Epoch  15 Batch  447 / 525  Training Loss  0.01415135245770216\n",
            "Epoch  15 Batch  448 / 525  Training Loss  0.017399238422513008\n",
            "Epoch  15 Batch  449 / 525  Training Loss  0.024220194667577744\n",
            "Epoch  15 Batch  450 / 525  Training Loss  0.02029218152165413\n",
            "Epoch  15 Batch  451 / 525  Training Loss  0.02347511425614357\n",
            "Epoch  15 Batch  452 / 525  Training Loss  0.02469879947602749\n",
            "Epoch  15 Batch  453 / 525  Training Loss  0.027151096612215042\n",
            "Epoch  15 Batch  454 / 525  Training Loss  0.016433577984571457\n",
            "Epoch  15 Batch  455 / 525  Training Loss  0.014139761216938496\n",
            "Epoch  15 Batch  456 / 525  Training Loss  0.021364161744713783\n",
            "Epoch  15 Batch  457 / 525  Training Loss  0.017247730866074562\n",
            "Epoch  15 Batch  458 / 525  Training Loss  0.011220118030905724\n",
            "Epoch  15 Batch  459 / 525  Training Loss  0.022209474816918373\n",
            "Epoch  15 Batch  460 / 525  Training Loss  0.012779489159584045\n",
            "Epoch  15 Batch  461 / 525  Training Loss  0.024396859109401703\n",
            "Epoch  15 Batch  462 / 525  Training Loss  0.020252244547009468\n",
            "Epoch  15 Batch  463 / 525  Training Loss  0.0334412045776844\n",
            "Epoch  15 Batch  464 / 525  Training Loss  0.02891525626182556\n",
            "Epoch  15 Batch  465 / 525  Training Loss  0.01555363368242979\n",
            "Epoch  15 Batch  466 / 525  Training Loss  0.01898854970932007\n",
            "Epoch  15 Batch  467 / 525  Training Loss  0.014542755670845509\n",
            "Epoch  15 Batch  468 / 525  Training Loss  0.02947336994111538\n",
            "Epoch  15 Batch  469 / 525  Training Loss  0.03489215299487114\n",
            "Epoch  15 Batch  470 / 525  Training Loss  0.02553407847881317\n",
            "Epoch  15 Batch  471 / 525  Training Loss  0.030477559193968773\n",
            "Epoch  15 Batch  472 / 525  Training Loss  0.023534037172794342\n",
            "Epoch  15 Batch  473 / 525  Training Loss  0.02017313987016678\n",
            "Epoch  15 Batch  474 / 525  Training Loss  0.012493284419178963\n",
            "Epoch  15 Batch  475 / 525  Training Loss  0.02264154702425003\n",
            "Epoch  15 Batch  476 / 525  Training Loss  0.01957293041050434\n",
            "Epoch  15 Batch  477 / 525  Training Loss  0.015042613260447979\n",
            "Epoch  15 Batch  478 / 525  Training Loss  0.01964743249118328\n",
            "Epoch  15 Batch  479 / 525  Training Loss  0.024233652278780937\n",
            "Epoch  15 Batch  480 / 525  Training Loss  0.022059448063373566\n",
            "Epoch  15 Batch  481 / 525  Training Loss  0.017272457480430603\n",
            "Epoch  15 Batch  482 / 525  Training Loss  0.02003992721438408\n",
            "Epoch  15 Batch  483 / 525  Training Loss  0.01591463014483452\n",
            "Epoch  15 Batch  484 / 525  Training Loss  0.010045414790511131\n",
            "Epoch  15 Batch  485 / 525  Training Loss  0.021374939009547234\n",
            "Epoch  15 Batch  486 / 525  Training Loss  0.02138865366578102\n",
            "Epoch  15 Batch  487 / 525  Training Loss  0.012620720081031322\n",
            "Epoch  15 Batch  488 / 525  Training Loss  0.018762296065688133\n",
            "Epoch  15 Batch  489 / 525  Training Loss  0.02358403429389\n",
            "Epoch  15 Batch  490 / 525  Training Loss  0.0232431348413229\n",
            "Epoch  15 Batch  491 / 525  Training Loss  0.01298573799431324\n",
            "Epoch  15 Batch  492 / 525  Training Loss  0.024853821843862534\n",
            "Epoch  15 Batch  493 / 525  Training Loss  0.018750673159956932\n",
            "Epoch  15 Batch  494 / 525  Training Loss  0.02658149227499962\n",
            "Epoch  15 Batch  495 / 525  Training Loss  0.019631335511803627\n",
            "Epoch  15 Batch  496 / 525  Training Loss  0.01852964609861374\n",
            "Epoch  15 Batch  497 / 525  Training Loss  0.016218654811382294\n",
            "Epoch  15 Batch  498 / 525  Training Loss  0.011860818602144718\n",
            "Epoch  15 Batch  499 / 525  Training Loss  0.008996797725558281\n",
            "Epoch  15 Batch  500 / 525  Training Loss  0.0225350484251976\n",
            "Epoch  15 Batch  501 / 525  Training Loss  0.020983248949050903\n",
            "Epoch  15 Batch  502 / 525  Training Loss  0.026883158832788467\n",
            "Epoch  15 Batch  503 / 525  Training Loss  0.022981081157922745\n",
            "Epoch  15 Batch  504 / 525  Training Loss  0.016657263040542603\n",
            "Epoch  15 Batch  505 / 525  Training Loss  0.013991132378578186\n",
            "Epoch  15 Batch  506 / 525  Training Loss  0.02861068584024906\n",
            "Epoch  15 Batch  507 / 525  Training Loss  0.019192975014448166\n",
            "Epoch  15 Batch  508 / 525  Training Loss  0.01182399783283472\n",
            "Epoch  15 Batch  509 / 525  Training Loss  0.013104921206831932\n",
            "Epoch  15 Batch  510 / 525  Training Loss  0.020780639722943306\n",
            "Epoch  15 Batch  511 / 525  Training Loss  0.025082558393478394\n",
            "Epoch  15 Batch  512 / 525  Training Loss  0.02724352851510048\n",
            "Epoch  15 Batch  513 / 525  Training Loss  0.016081223264336586\n",
            "Epoch  15 Batch  514 / 525  Training Loss  0.025872010737657547\n",
            "Epoch  15 Batch  515 / 525  Training Loss  0.01814979687333107\n",
            "Epoch  15 Batch  516 / 525  Training Loss  0.0090395612642169\n",
            "Epoch  15 Batch  517 / 525  Training Loss  0.016112394630908966\n",
            "Epoch  15 Batch  518 / 525  Training Loss  0.016397805884480476\n",
            "Epoch  15 Batch  519 / 525  Training Loss  0.013013715855777264\n",
            "Epoch  15 Batch  520 / 525  Training Loss  0.021542344242334366\n",
            "Epoch  15 Batch  521 / 525  Training Loss  0.03239878639578819\n",
            "Epoch  15 Batch  522 / 525  Training Loss  0.023429181426763535\n",
            "Epoch  15 Batch  523 / 525  Training Loss  0.02091333642601967\n",
            "Epoch  15 Batch  524 / 525  Training Loss  0.015019448474049568\n",
            "  16    |    -    |   0.017118   | 56.866667\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 16\n",
            "Epoch  16 Batch  0 / 525  Training Loss  0.01326691173017025\n",
            "Epoch  16 Batch  1 / 525  Training Loss  0.01189485751092434\n",
            "Epoch  16 Batch  2 / 525  Training Loss  0.013431076891720295\n",
            "Epoch  16 Batch  3 / 525  Training Loss  0.01711558923125267\n",
            "Epoch  16 Batch  4 / 525  Training Loss  0.011092403903603554\n",
            "Epoch  16 Batch  5 / 525  Training Loss  0.009612035006284714\n",
            "Epoch  16 Batch  6 / 525  Training Loss  0.012520444579422474\n",
            "Epoch  16 Batch  7 / 525  Training Loss  0.008490292355418205\n",
            "Epoch  16 Batch  8 / 525  Training Loss  0.007950409315526485\n",
            "Epoch  16 Batch  9 / 525  Training Loss  0.008610750548541546\n",
            "Epoch  16 Batch  10 / 525  Training Loss  0.010508507490158081\n",
            "Epoch  16 Batch  11 / 525  Training Loss  0.008817153982818127\n",
            "Epoch  16 Batch  12 / 525  Training Loss  0.012918638996779919\n",
            "Epoch  16 Batch  13 / 525  Training Loss  0.01082009356468916\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  16 Batch  14 / 525  Training Loss  0.00838826596736908\n",
            "Epoch  16 Batch  15 / 525  Training Loss  0.01429828442633152\n",
            "Epoch  16 Batch  16 / 525  Training Loss  0.009151231497526169\n",
            "Epoch  16 Batch  17 / 525  Training Loss  0.006561679299920797\n",
            "Epoch  16 Batch  18 / 525  Training Loss  0.009421126917004585\n",
            "Epoch  16 Batch  19 / 525  Training Loss  0.004509274382144213\n",
            "Epoch  16 Batch  20 / 525  Training Loss  0.02101113088428974\n",
            "Epoch  16 Batch  21 / 525  Training Loss  0.010435552336275578\n",
            "Epoch  16 Batch  22 / 525  Training Loss  0.013207169249653816\n",
            "Epoch  16 Batch  23 / 525  Training Loss  0.007185121066868305\n",
            "Epoch  16 Batch  24 / 525  Training Loss  0.011285520158708096\n",
            "Epoch  16 Batch  25 / 525  Training Loss  0.009047407656908035\n",
            "Epoch  16 Batch  26 / 525  Training Loss  0.008188776671886444\n",
            "Epoch  16 Batch  27 / 525  Training Loss  0.00809613335877657\n",
            "Epoch  16 Batch  28 / 525  Training Loss  0.008894719183444977\n",
            "Epoch  16 Batch  29 / 525  Training Loss  0.014793716371059418\n",
            "Epoch  16 Batch  30 / 525  Training Loss  0.024932626634836197\n",
            "Epoch  16 Batch  31 / 525  Training Loss  0.016310468316078186\n",
            "Epoch  16 Batch  32 / 525  Training Loss  0.010297263972461224\n",
            "Epoch  16 Batch  33 / 525  Training Loss  0.005640716757625341\n",
            "Epoch  16 Batch  34 / 525  Training Loss  0.00970078632235527\n",
            "Epoch  16 Batch  35 / 525  Training Loss  0.01158793456852436\n",
            "Epoch  16 Batch  36 / 525  Training Loss  0.004915398545563221\n",
            "Epoch  16 Batch  37 / 525  Training Loss  0.009934899397194386\n",
            "Epoch  16 Batch  38 / 525  Training Loss  0.008747510612010956\n",
            "Epoch  16 Batch  39 / 525  Training Loss  0.005021857563406229\n",
            "Epoch  16 Batch  40 / 525  Training Loss  0.01870511844754219\n",
            "Epoch  16 Batch  41 / 525  Training Loss  0.012877330183982849\n",
            "Epoch  16 Batch  42 / 525  Training Loss  0.010761196725070477\n",
            "Epoch  16 Batch  43 / 525  Training Loss  0.013029450550675392\n",
            "Epoch  16 Batch  44 / 525  Training Loss  0.005140435416251421\n",
            "Epoch  16 Batch  45 / 525  Training Loss  0.012857506982982159\n",
            "Epoch  16 Batch  46 / 525  Training Loss  0.007148648612201214\n",
            "Epoch  16 Batch  47 / 525  Training Loss  0.012468991801142693\n",
            "Epoch  16 Batch  48 / 525  Training Loss  0.012306556105613708\n",
            "Epoch  16 Batch  49 / 525  Training Loss  0.01637592911720276\n",
            "Epoch  16 Batch  50 / 525  Training Loss  0.005285781808197498\n",
            "Epoch  16 Batch  51 / 525  Training Loss  0.010500007309019566\n",
            "Epoch  16 Batch  52 / 525  Training Loss  0.013408273458480835\n",
            "Epoch  16 Batch  53 / 525  Training Loss  0.004181066062301397\n",
            "Epoch  16 Batch  54 / 525  Training Loss  0.015300299040973186\n",
            "Epoch  16 Batch  55 / 525  Training Loss  0.008531110361218452\n",
            "Epoch  16 Batch  56 / 525  Training Loss  0.011626704595983028\n",
            "Epoch  16 Batch  57 / 525  Training Loss  0.0075758821330964565\n",
            "Epoch  16 Batch  58 / 525  Training Loss  0.007457866333425045\n",
            "Epoch  16 Batch  59 / 525  Training Loss  0.010808438062667847\n",
            "Epoch  16 Batch  60 / 525  Training Loss  0.015546729788184166\n",
            "Epoch  16 Batch  61 / 525  Training Loss  0.01629878580570221\n",
            "Epoch  16 Batch  62 / 525  Training Loss  0.014879639260470867\n",
            "Epoch  16 Batch  63 / 525  Training Loss  0.00738910585641861\n",
            "Epoch  16 Batch  64 / 525  Training Loss  0.004639878403395414\n",
            "Epoch  16 Batch  65 / 525  Training Loss  0.007031948771327734\n",
            "Epoch  16 Batch  66 / 525  Training Loss  0.025523383170366287\n",
            "Epoch  16 Batch  67 / 525  Training Loss  0.008579042740166187\n",
            "Epoch  16 Batch  68 / 525  Training Loss  0.007485965732485056\n",
            "Epoch  16 Batch  69 / 525  Training Loss  0.005956719629466534\n",
            "Epoch  16 Batch  70 / 525  Training Loss  0.00785929523408413\n",
            "Epoch  16 Batch  71 / 525  Training Loss  0.007455436047166586\n",
            "Epoch  16 Batch  72 / 525  Training Loss  0.009846392087638378\n",
            "Epoch  16 Batch  73 / 525  Training Loss  0.010083651170134544\n",
            "Epoch  16 Batch  74 / 525  Training Loss  0.007766192313283682\n",
            "Epoch  16 Batch  75 / 525  Training Loss  0.013435070402920246\n",
            "Epoch  16 Batch  76 / 525  Training Loss  0.01151265762746334\n",
            "Epoch  16 Batch  77 / 525  Training Loss  0.007826163433492184\n",
            "Epoch  16 Batch  78 / 525  Training Loss  0.015031745657324791\n",
            "Epoch  16 Batch  79 / 525  Training Loss  0.008999799378216267\n",
            "Epoch  16 Batch  80 / 525  Training Loss  0.011495089158415794\n",
            "Epoch  16 Batch  81 / 525  Training Loss  0.008807879872620106\n",
            "Epoch  16 Batch  82 / 525  Training Loss  0.024646349251270294\n",
            "Epoch  16 Batch  83 / 525  Training Loss  0.01293223351240158\n",
            "Epoch  16 Batch  84 / 525  Training Loss  0.009887350723147392\n",
            "Epoch  16 Batch  85 / 525  Training Loss  0.010404406115412712\n",
            "Epoch  16 Batch  86 / 525  Training Loss  0.009699507616460323\n",
            "Epoch  16 Batch  87 / 525  Training Loss  0.010434062220156193\n",
            "Epoch  16 Batch  88 / 525  Training Loss  0.01474722195416689\n",
            "Epoch  16 Batch  89 / 525  Training Loss  0.005904891528189182\n",
            "Epoch  16 Batch  90 / 525  Training Loss  0.009626712650060654\n",
            "Epoch  16 Batch  91 / 525  Training Loss  0.02059241198003292\n",
            "Epoch  16 Batch  92 / 525  Training Loss  0.012287968769669533\n",
            "Epoch  16 Batch  93 / 525  Training Loss  0.010103237815201283\n",
            "Epoch  16 Batch  94 / 525  Training Loss  0.0109064606949687\n",
            "Epoch  16 Batch  95 / 525  Training Loss  0.009221932850778103\n",
            "Epoch  16 Batch  96 / 525  Training Loss  0.010443250648677349\n",
            "Epoch  16 Batch  97 / 525  Training Loss  0.014601258561015129\n",
            "Epoch  16 Batch  98 / 525  Training Loss  0.015091369859874249\n",
            "Epoch  16 Batch  99 / 525  Training Loss  0.016267288476228714\n",
            "Epoch  16 Batch  100 / 525  Training Loss  0.009239045903086662\n",
            "Epoch  16 Batch  101 / 525  Training Loss  0.019144881516695023\n",
            "Epoch  16 Batch  102 / 525  Training Loss  0.010770506225526333\n",
            "Epoch  16 Batch  103 / 525  Training Loss  0.012072144076228142\n",
            "Epoch  16 Batch  104 / 525  Training Loss  0.017502272501587868\n",
            "Epoch  16 Batch  105 / 525  Training Loss  0.00797936599701643\n",
            "Epoch  16 Batch  106 / 525  Training Loss  0.021084750071167946\n",
            "Epoch  16 Batch  107 / 525  Training Loss  0.016421597450971603\n",
            "Epoch  16 Batch  108 / 525  Training Loss  0.01866232417523861\n",
            "Epoch  16 Batch  109 / 525  Training Loss  0.012488720938563347\n",
            "Epoch  16 Batch  110 / 525  Training Loss  0.00684704864397645\n",
            "Epoch  16 Batch  111 / 525  Training Loss  0.01884327456355095\n",
            "Epoch  16 Batch  112 / 525  Training Loss  0.009237093850970268\n",
            "Epoch  16 Batch  113 / 525  Training Loss  0.007811953779309988\n",
            "Epoch  16 Batch  114 / 525  Training Loss  0.01764325425028801\n",
            "Epoch  16 Batch  115 / 525  Training Loss  0.0183623805642128\n",
            "Epoch  16 Batch  116 / 525  Training Loss  0.007816245779395103\n",
            "Epoch  16 Batch  117 / 525  Training Loss  0.009315107017755508\n",
            "Epoch  16 Batch  118 / 525  Training Loss  0.008875982835888863\n",
            "Epoch  16 Batch  119 / 525  Training Loss  0.013107212260365486\n",
            "Epoch  16 Batch  120 / 525  Training Loss  0.017253993079066277\n",
            "Epoch  16 Batch  121 / 525  Training Loss  0.011596227996051311\n",
            "Epoch  16 Batch  122 / 525  Training Loss  0.012352783232927322\n",
            "Epoch  16 Batch  123 / 525  Training Loss  0.01599314622581005\n",
            "Epoch  16 Batch  124 / 525  Training Loss  0.016676578670740128\n",
            "Epoch  16 Batch  125 / 525  Training Loss  0.015041658654808998\n",
            "Epoch  16 Batch  126 / 525  Training Loss  0.013028496876358986\n",
            "Epoch  16 Batch  127 / 525  Training Loss  0.007298066280782223\n",
            "Epoch  16 Batch  128 / 525  Training Loss  0.011477191932499409\n",
            "Epoch  16 Batch  129 / 525  Training Loss  0.013279914855957031\n",
            "Epoch  16 Batch  130 / 525  Training Loss  0.009192051365971565\n",
            "Epoch  16 Batch  131 / 525  Training Loss  0.013491963036358356\n",
            "Epoch  16 Batch  132 / 525  Training Loss  0.014183943159878254\n",
            "Epoch  16 Batch  133 / 525  Training Loss  0.011538838036358356\n",
            "Epoch  16 Batch  134 / 525  Training Loss  0.010548451915383339\n",
            "Epoch  16 Batch  135 / 525  Training Loss  0.008856167085468769\n",
            "Epoch  16 Batch  136 / 525  Training Loss  0.008799415081739426\n",
            "Epoch  16 Batch  137 / 525  Training Loss  0.016058586537837982\n",
            "Epoch  16 Batch  138 / 525  Training Loss  0.00802117120474577\n",
            "Epoch  16 Batch  139 / 525  Training Loss  0.0071157352067530155\n",
            "Epoch  16 Batch  140 / 525  Training Loss  0.00959586352109909\n",
            "Epoch  16 Batch  141 / 525  Training Loss  0.007714288774877787\n",
            "Epoch  16 Batch  142 / 525  Training Loss  0.006851681508123875\n",
            "Epoch  16 Batch  143 / 525  Training Loss  0.008489137515425682\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  16 Batch  144 / 525  Training Loss  0.011606045067310333\n",
            "Epoch  16 Batch  145 / 525  Training Loss  0.009896287694573402\n",
            "Epoch  16 Batch  146 / 525  Training Loss  0.017599252983927727\n",
            "Epoch  16 Batch  147 / 525  Training Loss  0.010604992508888245\n",
            "Epoch  16 Batch  148 / 525  Training Loss  0.0107649527490139\n",
            "Epoch  16 Batch  149 / 525  Training Loss  0.014143926091492176\n",
            "Epoch  16 Batch  150 / 525  Training Loss  0.01321496069431305\n",
            "Epoch  16 Batch  151 / 525  Training Loss  0.014535029418766499\n",
            "Epoch  16 Batch  152 / 525  Training Loss  0.00596999004483223\n",
            "Epoch  16 Batch  153 / 525  Training Loss  0.013519661501049995\n",
            "Epoch  16 Batch  154 / 525  Training Loss  0.0066204434260725975\n",
            "Epoch  16 Batch  155 / 525  Training Loss  0.012573244981467724\n",
            "Epoch  16 Batch  156 / 525  Training Loss  0.01387736201286316\n",
            "Epoch  16 Batch  157 / 525  Training Loss  0.007921549491584301\n",
            "Epoch  16 Batch  158 / 525  Training Loss  0.011777222156524658\n",
            "Epoch  16 Batch  159 / 525  Training Loss  0.013904303312301636\n",
            "Epoch  16 Batch  160 / 525  Training Loss  0.01243877038359642\n",
            "Epoch  16 Batch  161 / 525  Training Loss  0.013335401192307472\n",
            "Epoch  16 Batch  162 / 525  Training Loss  0.008024502545595169\n",
            "Epoch  16 Batch  163 / 525  Training Loss  0.012137209996581078\n",
            "Epoch  16 Batch  164 / 525  Training Loss  0.008862793445587158\n",
            "Epoch  16 Batch  165 / 525  Training Loss  0.011992087587714195\n",
            "Epoch  16 Batch  166 / 525  Training Loss  0.005270094610750675\n",
            "Epoch  16 Batch  167 / 525  Training Loss  0.010843207128345966\n",
            "Epoch  16 Batch  168 / 525  Training Loss  0.012194426730275154\n",
            "Epoch  16 Batch  169 / 525  Training Loss  0.008578361012041569\n",
            "Epoch  16 Batch  170 / 525  Training Loss  0.012634540908038616\n",
            "Epoch  16 Batch  171 / 525  Training Loss  0.00915949884802103\n",
            "Epoch  16 Batch  172 / 525  Training Loss  0.011023606173694134\n",
            "Epoch  16 Batch  173 / 525  Training Loss  0.01524650864303112\n",
            "Epoch  16 Batch  174 / 525  Training Loss  0.007898983545601368\n",
            "Epoch  16 Batch  175 / 525  Training Loss  0.015073900111019611\n",
            "Epoch  16 Batch  176 / 525  Training Loss  0.012397505342960358\n",
            "Epoch  16 Batch  177 / 525  Training Loss  0.01043819822371006\n",
            "Epoch  16 Batch  178 / 525  Training Loss  0.011993472464382648\n",
            "Epoch  16 Batch  179 / 525  Training Loss  0.018051574006676674\n",
            "Epoch  16 Batch  180 / 525  Training Loss  0.013962747529149055\n",
            "Epoch  16 Batch  181 / 525  Training Loss  0.009635353460907936\n",
            "Epoch  16 Batch  182 / 525  Training Loss  0.02362147718667984\n",
            "Epoch  16 Batch  183 / 525  Training Loss  0.0107810003682971\n",
            "Epoch  16 Batch  184 / 525  Training Loss  0.010341986082494259\n",
            "Epoch  16 Batch  185 / 525  Training Loss  0.011017544195055962\n",
            "Epoch  16 Batch  186 / 525  Training Loss  0.009166194126009941\n",
            "Epoch  16 Batch  187 / 525  Training Loss  0.019181568175554276\n",
            "Epoch  16 Batch  188 / 525  Training Loss  0.01778949424624443\n",
            "Epoch  16 Batch  189 / 525  Training Loss  0.010510224848985672\n",
            "Epoch  16 Batch  190 / 525  Training Loss  0.009076358750462532\n",
            "Epoch  16 Batch  191 / 525  Training Loss  0.015075253322720528\n",
            "Epoch  16 Batch  192 / 525  Training Loss  0.013020527549088001\n",
            "Epoch  16 Batch  193 / 525  Training Loss  0.02350471168756485\n",
            "Epoch  16 Batch  194 / 525  Training Loss  0.01866805925965309\n",
            "Epoch  16 Batch  195 / 525  Training Loss  0.030370647087693214\n",
            "Epoch  16 Batch  196 / 525  Training Loss  0.01662922278046608\n",
            "Epoch  16 Batch  197 / 525  Training Loss  0.01505452673882246\n",
            "Epoch  16 Batch  198 / 525  Training Loss  0.015270894393324852\n",
            "Epoch  16 Batch  199 / 525  Training Loss  0.026000667363405228\n",
            "Epoch  16 Batch  200 / 525  Training Loss  0.013463596813380718\n",
            "Epoch  16 Batch  201 / 525  Training Loss  0.027188703417778015\n",
            "Epoch  16 Batch  202 / 525  Training Loss  0.014855960384011269\n",
            "Epoch  16 Batch  203 / 525  Training Loss  0.019414354115724564\n",
            "Epoch  16 Batch  204 / 525  Training Loss  0.01720261014997959\n",
            "Epoch  16 Batch  205 / 525  Training Loss  0.014974595978856087\n",
            "Epoch  16 Batch  206 / 525  Training Loss  0.02380267158150673\n",
            "Epoch  16 Batch  207 / 525  Training Loss  0.006169677712023258\n",
            "Epoch  16 Batch  208 / 525  Training Loss  0.010088106617331505\n",
            "Epoch  16 Batch  209 / 525  Training Loss  0.009994296357035637\n",
            "Epoch  16 Batch  210 / 525  Training Loss  0.018371565267443657\n",
            "Epoch  16 Batch  211 / 525  Training Loss  0.018022987991571426\n",
            "Epoch  16 Batch  212 / 525  Training Loss  0.021502256393432617\n",
            "Epoch  16 Batch  213 / 525  Training Loss  0.01443841028958559\n",
            "Epoch  16 Batch  214 / 525  Training Loss  0.01756606064736843\n",
            "Epoch  16 Batch  215 / 525  Training Loss  0.015349755063652992\n",
            "Epoch  16 Batch  216 / 525  Training Loss  0.008853508159518242\n",
            "Epoch  16 Batch  217 / 525  Training Loss  0.014062391594052315\n",
            "Epoch  16 Batch  218 / 525  Training Loss  0.014582397416234016\n",
            "Epoch  16 Batch  219 / 525  Training Loss  0.02254355326294899\n",
            "Epoch  16 Batch  220 / 525  Training Loss  0.013791568577289581\n",
            "Epoch  16 Batch  221 / 525  Training Loss  0.008301224559545517\n",
            "Epoch  16 Batch  222 / 525  Training Loss  0.012136789038777351\n",
            "Epoch  16 Batch  223 / 525  Training Loss  0.009974917396903038\n",
            "Epoch  16 Batch  224 / 525  Training Loss  0.017215298488736153\n",
            "Epoch  16 Batch  225 / 525  Training Loss  0.015043571591377258\n",
            "Epoch  16 Batch  226 / 525  Training Loss  0.01245799008756876\n",
            "Epoch  16 Batch  227 / 525  Training Loss  0.013671797700226307\n",
            "Epoch  16 Batch  228 / 525  Training Loss  0.0198365431278944\n",
            "Epoch  16 Batch  229 / 525  Training Loss  0.014926346018910408\n",
            "Epoch  16 Batch  230 / 525  Training Loss  0.007419978268444538\n",
            "Epoch  16 Batch  231 / 525  Training Loss  0.008848262950778008\n",
            "Epoch  16 Batch  232 / 525  Training Loss  0.008415382355451584\n",
            "Epoch  16 Batch  233 / 525  Training Loss  0.03859419375658035\n",
            "Epoch  16 Batch  234 / 525  Training Loss  0.008211417123675346\n",
            "Epoch  16 Batch  235 / 525  Training Loss  0.01481143943965435\n",
            "Epoch  16 Batch  236 / 525  Training Loss  0.015810362994670868\n",
            "Epoch  16 Batch  237 / 525  Training Loss  0.02793816104531288\n",
            "Epoch  16 Batch  238 / 525  Training Loss  0.013313847593963146\n",
            "Epoch  16 Batch  239 / 525  Training Loss  0.02265414223074913\n",
            "Epoch  16 Batch  240 / 525  Training Loss  0.011245667934417725\n",
            "Epoch  16 Batch  241 / 525  Training Loss  0.012405315414071083\n",
            "Epoch  16 Batch  242 / 525  Training Loss  0.011254746466875076\n",
            "Epoch  16 Batch  243 / 525  Training Loss  0.007916001603007317\n",
            "Epoch  16 Batch  244 / 525  Training Loss  0.015947850421071053\n",
            "Epoch  16 Batch  245 / 525  Training Loss  0.023027127608656883\n",
            "Epoch  16 Batch  246 / 525  Training Loss  0.013921715319156647\n",
            "Epoch  16 Batch  247 / 525  Training Loss  0.014046932570636272\n",
            "Epoch  16 Batch  248 / 525  Training Loss  0.01689114235341549\n",
            "Epoch  16 Batch  249 / 525  Training Loss  0.02057427540421486\n",
            "Epoch  16 Batch  250 / 525  Training Loss  0.0126191396266222\n",
            "Epoch  16 Batch  251 / 525  Training Loss  0.023706838488578796\n",
            "Epoch  16 Batch  252 / 525  Training Loss  0.013580633327364922\n",
            "Epoch  16 Batch  253 / 525  Training Loss  0.017114894464612007\n",
            "Epoch  16 Batch  254 / 525  Training Loss  0.016388311982154846\n",
            "Epoch  16 Batch  255 / 525  Training Loss  0.013295238837599754\n",
            "Epoch  16 Batch  256 / 525  Training Loss  0.014919722452759743\n",
            "Epoch  16 Batch  257 / 525  Training Loss  0.008918814361095428\n",
            "Epoch  16 Batch  258 / 525  Training Loss  0.021230623126029968\n",
            "Epoch  16 Batch  259 / 525  Training Loss  0.015361016616225243\n",
            "Epoch  16 Batch  260 / 525  Training Loss  0.007456289138644934\n",
            "Epoch  16 Batch  261 / 525  Training Loss  0.01758781261742115\n",
            "Epoch  16 Batch  262 / 525  Training Loss  0.016870606690645218\n",
            "Epoch  16 Batch  263 / 525  Training Loss  0.032467231154441833\n",
            "Epoch  16 Batch  264 / 525  Training Loss  0.032899998128414154\n",
            "Epoch  16 Batch  265 / 525  Training Loss  0.012315256521105766\n",
            "Epoch  16 Batch  266 / 525  Training Loss  0.015262654051184654\n",
            "Epoch  16 Batch  267 / 525  Training Loss  0.016979701817035675\n",
            "Epoch  16 Batch  268 / 525  Training Loss  0.010776249691843987\n",
            "Epoch  16 Batch  269 / 525  Training Loss  0.011250951327383518\n",
            "Epoch  16 Batch  270 / 525  Training Loss  0.01722959242761135\n",
            "Epoch  16 Batch  271 / 525  Training Loss  0.016701703891158104\n",
            "Epoch  16 Batch  272 / 525  Training Loss  0.01697569712996483\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  16 Batch  273 / 525  Training Loss  0.00899745523929596\n",
            "Epoch  16 Batch  274 / 525  Training Loss  0.004945795051753521\n",
            "Epoch  16 Batch  275 / 525  Training Loss  0.017374344170093536\n",
            "Epoch  16 Batch  276 / 525  Training Loss  0.0074891685508191586\n",
            "Epoch  16 Batch  277 / 525  Training Loss  0.016495222225785255\n",
            "Epoch  16 Batch  278 / 525  Training Loss  0.008725879713892937\n",
            "Epoch  16 Batch  279 / 525  Training Loss  0.010152134113013744\n",
            "Epoch  16 Batch  280 / 525  Training Loss  0.020923245698213577\n",
            "Epoch  16 Batch  281 / 525  Training Loss  0.019564224407076836\n",
            "Epoch  16 Batch  282 / 525  Training Loss  0.008754816837608814\n",
            "Epoch  16 Batch  283 / 525  Training Loss  0.011777536943554878\n",
            "Epoch  16 Batch  284 / 525  Training Loss  0.013915265910327435\n",
            "Epoch  16 Batch  285 / 525  Training Loss  0.02439279481768608\n",
            "Epoch  16 Batch  286 / 525  Training Loss  0.02001248486340046\n",
            "Epoch  16 Batch  287 / 525  Training Loss  0.009619479067623615\n",
            "Epoch  16 Batch  288 / 525  Training Loss  0.011004330590367317\n",
            "Epoch  16 Batch  289 / 525  Training Loss  0.007938800379633904\n",
            "Epoch  16 Batch  290 / 525  Training Loss  0.019717222079634666\n",
            "Epoch  16 Batch  291 / 525  Training Loss  0.007916825823485851\n",
            "Epoch  16 Batch  292 / 525  Training Loss  0.02054448239505291\n",
            "Epoch  16 Batch  293 / 525  Training Loss  0.00881665013730526\n",
            "Epoch  16 Batch  294 / 525  Training Loss  0.00936980452388525\n",
            "Epoch  16 Batch  295 / 525  Training Loss  0.020313899964094162\n",
            "Epoch  16 Batch  296 / 525  Training Loss  0.014486625790596008\n",
            "Epoch  16 Batch  297 / 525  Training Loss  0.011271059513092041\n",
            "Epoch  16 Batch  298 / 525  Training Loss  0.024835264310240746\n",
            "Epoch  16 Batch  299 / 525  Training Loss  0.009683936834335327\n",
            "Epoch  16 Batch  300 / 525  Training Loss  0.013875814154744148\n",
            "Epoch  16 Batch  301 / 525  Training Loss  0.00792604684829712\n",
            "Epoch  16 Batch  302 / 525  Training Loss  0.01924859546124935\n",
            "Epoch  16 Batch  303 / 525  Training Loss  0.01258418895304203\n",
            "Epoch  16 Batch  304 / 525  Training Loss  0.005008414853364229\n",
            "Epoch  16 Batch  305 / 525  Training Loss  0.011065611615777016\n",
            "Epoch  16 Batch  306 / 525  Training Loss  0.015896033495664597\n",
            "Epoch  16 Batch  307 / 525  Training Loss  0.01738744229078293\n",
            "Epoch  16 Batch  308 / 525  Training Loss  0.012662656605243683\n",
            "Epoch  16 Batch  309 / 525  Training Loss  0.008426683023571968\n",
            "Epoch  16 Batch  310 / 525  Training Loss  0.017689738422632217\n",
            "Epoch  16 Batch  311 / 525  Training Loss  0.00513181509450078\n",
            "Epoch  16 Batch  312 / 525  Training Loss  0.01319826114922762\n",
            "Epoch  16 Batch  313 / 525  Training Loss  0.018182547762989998\n",
            "Epoch  16 Batch  314 / 525  Training Loss  0.014873308129608631\n",
            "Epoch  16 Batch  315 / 525  Training Loss  0.012997886165976524\n",
            "Epoch  16 Batch  316 / 525  Training Loss  0.009685461409389973\n",
            "Epoch  16 Batch  317 / 525  Training Loss  0.007706175558269024\n",
            "Epoch  16 Batch  318 / 525  Training Loss  0.009224507957696915\n",
            "Epoch  16 Batch  319 / 525  Training Loss  0.014870861545205116\n",
            "Epoch  16 Batch  320 / 525  Training Loss  0.0256634708493948\n",
            "Epoch  16 Batch  321 / 525  Training Loss  0.016307951882481575\n",
            "Epoch  16 Batch  322 / 525  Training Loss  0.010950589552521706\n",
            "Epoch  16 Batch  323 / 525  Training Loss  0.009443147107958794\n",
            "Epoch  16 Batch  324 / 525  Training Loss  0.00902710109949112\n",
            "Epoch  16 Batch  325 / 525  Training Loss  0.012561872601509094\n",
            "Epoch  16 Batch  326 / 525  Training Loss  0.012325991876423359\n",
            "Epoch  16 Batch  327 / 525  Training Loss  0.0167898777872324\n",
            "Epoch  16 Batch  328 / 525  Training Loss  0.012603109702467918\n",
            "Epoch  16 Batch  329 / 525  Training Loss  0.010844116099178791\n",
            "Epoch  16 Batch  330 / 525  Training Loss  0.009348804131150246\n",
            "Epoch  16 Batch  331 / 525  Training Loss  0.014650081284344196\n",
            "Epoch  16 Batch  332 / 525  Training Loss  0.011145212687551975\n",
            "Epoch  16 Batch  333 / 525  Training Loss  0.012647293508052826\n",
            "Epoch  16 Batch  334 / 525  Training Loss  0.01730206049978733\n",
            "Epoch  16 Batch  335 / 525  Training Loss  0.012839818373322487\n",
            "Epoch  16 Batch  336 / 525  Training Loss  0.009638786315917969\n",
            "Epoch  16 Batch  337 / 525  Training Loss  0.009369341656565666\n",
            "Epoch  16 Batch  338 / 525  Training Loss  0.014483785256743431\n",
            "Epoch  16 Batch  339 / 525  Training Loss  0.024429725483059883\n",
            "Epoch  16 Batch  340 / 525  Training Loss  0.017178233712911606\n",
            "Epoch  16 Batch  341 / 525  Training Loss  0.012161390855908394\n",
            "Epoch  16 Batch  342 / 525  Training Loss  0.014902323484420776\n",
            "Epoch  16 Batch  343 / 525  Training Loss  0.01846635900437832\n",
            "Epoch  16 Batch  344 / 525  Training Loss  0.020461011677980423\n",
            "Epoch  16 Batch  345 / 525  Training Loss  0.015702689066529274\n",
            "Epoch  16 Batch  346 / 525  Training Loss  0.010658575221896172\n",
            "Epoch  16 Batch  347 / 525  Training Loss  0.007024120539426804\n",
            "Epoch  16 Batch  348 / 525  Training Loss  0.00899164192378521\n",
            "Epoch  16 Batch  349 / 525  Training Loss  0.009507263079285622\n",
            "Epoch  16 Batch  350 / 525  Training Loss  0.01142204087227583\n",
            "Epoch  16 Batch  351 / 525  Training Loss  0.00891832821071148\n",
            "Epoch  16 Batch  352 / 525  Training Loss  0.00706201558932662\n",
            "Epoch  16 Batch  353 / 525  Training Loss  0.01215490885078907\n",
            "Epoch  16 Batch  354 / 525  Training Loss  0.012247098609805107\n",
            "Epoch  16 Batch  355 / 525  Training Loss  0.004532181657850742\n",
            "Epoch  16 Batch  356 / 525  Training Loss  0.015362454578280449\n",
            "Epoch  16 Batch  357 / 525  Training Loss  0.019432177767157555\n",
            "Epoch  16 Batch  358 / 525  Training Loss  0.014328229241073132\n",
            "Epoch  16 Batch  359 / 525  Training Loss  0.007205665111541748\n",
            "Epoch  16 Batch  360 / 525  Training Loss  0.0125958900898695\n",
            "Epoch  16 Batch  361 / 525  Training Loss  0.014832569286227226\n",
            "Epoch  16 Batch  362 / 525  Training Loss  0.018763117492198944\n",
            "Epoch  16 Batch  363 / 525  Training Loss  0.0041125500574707985\n",
            "Epoch  16 Batch  364 / 525  Training Loss  0.0077418917790055275\n",
            "Epoch  16 Batch  365 / 525  Training Loss  0.007123944815248251\n",
            "Epoch  16 Batch  366 / 525  Training Loss  0.016368187963962555\n",
            "Epoch  16 Batch  367 / 525  Training Loss  0.014133019372820854\n",
            "Epoch  16 Batch  368 / 525  Training Loss  0.018693892285227776\n",
            "Epoch  16 Batch  369 / 525  Training Loss  0.008917347528040409\n",
            "Epoch  16 Batch  370 / 525  Training Loss  0.008334828540682793\n",
            "Epoch  16 Batch  371 / 525  Training Loss  0.021013157442212105\n",
            "Epoch  16 Batch  372 / 525  Training Loss  0.00851517915725708\n",
            "Epoch  16 Batch  373 / 525  Training Loss  0.014663146808743477\n",
            "Epoch  16 Batch  374 / 525  Training Loss  0.01005215011537075\n",
            "Epoch  16 Batch  375 / 525  Training Loss  0.005773440934717655\n",
            "Epoch  16 Batch  376 / 525  Training Loss  0.009651726111769676\n",
            "Epoch  16 Batch  377 / 525  Training Loss  0.01987445540726185\n",
            "Epoch  16 Batch  378 / 525  Training Loss  0.011489443480968475\n",
            "Epoch  16 Batch  379 / 525  Training Loss  0.015740254893898964\n",
            "Epoch  16 Batch  380 / 525  Training Loss  0.004886639770120382\n",
            "Epoch  16 Batch  381 / 525  Training Loss  0.008980552665889263\n",
            "Epoch  16 Batch  382 / 525  Training Loss  0.014074255712330341\n",
            "Epoch  16 Batch  383 / 525  Training Loss  0.01181346271187067\n",
            "Epoch  16 Batch  384 / 525  Training Loss  0.00995059497654438\n",
            "Epoch  16 Batch  385 / 525  Training Loss  0.01871654763817787\n",
            "Epoch  16 Batch  386 / 525  Training Loss  0.017996540293097496\n",
            "Epoch  16 Batch  387 / 525  Training Loss  0.015692876651883125\n",
            "Epoch  16 Batch  388 / 525  Training Loss  0.01960202120244503\n",
            "Epoch  16 Batch  389 / 525  Training Loss  0.01574297621846199\n",
            "Epoch  16 Batch  390 / 525  Training Loss  0.00893449317663908\n",
            "Epoch  16 Batch  391 / 525  Training Loss  0.016846569254994392\n",
            "Epoch  16 Batch  392 / 525  Training Loss  0.013744408264756203\n",
            "Epoch  16 Batch  393 / 525  Training Loss  0.009586112573742867\n",
            "Epoch  16 Batch  394 / 525  Training Loss  0.014283375814557076\n",
            "Epoch  16 Batch  395 / 525  Training Loss  0.01359452586621046\n",
            "Epoch  16 Batch  396 / 525  Training Loss  0.010322126559913158\n",
            "Epoch  16 Batch  397 / 525  Training Loss  0.015102893114089966\n",
            "Epoch  16 Batch  398 / 525  Training Loss  0.013050520792603493\n",
            "Epoch  16 Batch  399 / 525  Training Loss  0.02585112676024437\n",
            "Epoch  16 Batch  400 / 525  Training Loss  0.01897547021508217\n",
            "Epoch  16 Batch  401 / 525  Training Loss  0.006687376648187637\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  16 Batch  402 / 525  Training Loss  0.010604998096823692\n",
            "Epoch  16 Batch  403 / 525  Training Loss  0.009672430343925953\n",
            "Epoch  16 Batch  404 / 525  Training Loss  0.014621501788496971\n",
            "Epoch  16 Batch  405 / 525  Training Loss  0.011049536988139153\n",
            "Epoch  16 Batch  406 / 525  Training Loss  0.018572453409433365\n",
            "Epoch  16 Batch  407 / 525  Training Loss  0.015196789987385273\n",
            "Epoch  16 Batch  408 / 525  Training Loss  0.02160465344786644\n",
            "Epoch  16 Batch  409 / 525  Training Loss  0.012150038965046406\n",
            "Epoch  16 Batch  410 / 525  Training Loss  0.008558693341910839\n",
            "Epoch  16 Batch  411 / 525  Training Loss  0.01299549825489521\n",
            "Epoch  16 Batch  412 / 525  Training Loss  0.010154826566576958\n",
            "Epoch  16 Batch  413 / 525  Training Loss  0.0173332579433918\n",
            "Epoch  16 Batch  414 / 525  Training Loss  0.011656062677502632\n",
            "Epoch  16 Batch  415 / 525  Training Loss  0.014524644240736961\n",
            "Epoch  16 Batch  416 / 525  Training Loss  0.01694815419614315\n",
            "Epoch  16 Batch  417 / 525  Training Loss  0.01856214553117752\n",
            "Epoch  16 Batch  418 / 525  Training Loss  0.01355928648263216\n",
            "Epoch  16 Batch  419 / 525  Training Loss  0.008984211832284927\n",
            "Epoch  16 Batch  420 / 525  Training Loss  0.021690938621759415\n",
            "Epoch  16 Batch  421 / 525  Training Loss  0.024162963032722473\n",
            "Epoch  16 Batch  422 / 525  Training Loss  0.013890224508941174\n",
            "Epoch  16 Batch  423 / 525  Training Loss  0.00796126201748848\n",
            "Epoch  16 Batch  424 / 525  Training Loss  0.012042004615068436\n",
            "Epoch  16 Batch  425 / 525  Training Loss  0.00847635231912136\n",
            "Epoch  16 Batch  426 / 525  Training Loss  0.01592036709189415\n",
            "Epoch  16 Batch  427 / 525  Training Loss  0.012362380512058735\n",
            "Epoch  16 Batch  428 / 525  Training Loss  0.015645282343029976\n",
            "Epoch  16 Batch  429 / 525  Training Loss  0.014622810296714306\n",
            "Epoch  16 Batch  430 / 525  Training Loss  0.01723857782781124\n",
            "Epoch  16 Batch  431 / 525  Training Loss  0.01164296269416809\n",
            "Epoch  16 Batch  432 / 525  Training Loss  0.017513882368803024\n",
            "Epoch  16 Batch  433 / 525  Training Loss  0.018030483275651932\n",
            "Epoch  16 Batch  434 / 525  Training Loss  0.015535076148808002\n",
            "Epoch  16 Batch  435 / 525  Training Loss  0.01612129807472229\n",
            "Epoch  16 Batch  436 / 525  Training Loss  0.013256272301077843\n",
            "Epoch  16 Batch  437 / 525  Training Loss  0.01649065874516964\n",
            "Epoch  16 Batch  438 / 525  Training Loss  0.010606620460748672\n",
            "Epoch  16 Batch  439 / 525  Training Loss  0.017926547676324844\n",
            "Epoch  16 Batch  440 / 525  Training Loss  0.024528823792934418\n",
            "Epoch  16 Batch  441 / 525  Training Loss  0.01306955236941576\n",
            "Epoch  16 Batch  442 / 525  Training Loss  0.013041853904724121\n",
            "Epoch  16 Batch  443 / 525  Training Loss  0.013515695929527283\n",
            "Epoch  16 Batch  444 / 525  Training Loss  0.01848459616303444\n",
            "Epoch  16 Batch  445 / 525  Training Loss  0.009533469565212727\n",
            "Epoch  16 Batch  446 / 525  Training Loss  0.019560465589165688\n",
            "Epoch  16 Batch  447 / 525  Training Loss  0.013451935723423958\n",
            "Epoch  16 Batch  448 / 525  Training Loss  0.021187501028180122\n",
            "Epoch  16 Batch  449 / 525  Training Loss  0.007813003845512867\n",
            "Epoch  16 Batch  450 / 525  Training Loss  0.022006003186106682\n",
            "Epoch  16 Batch  451 / 525  Training Loss  0.02247176691889763\n",
            "Epoch  16 Batch  452 / 525  Training Loss  0.009701400995254517\n",
            "Epoch  16 Batch  453 / 525  Training Loss  0.007559537887573242\n",
            "Epoch  16 Batch  454 / 525  Training Loss  0.02511649765074253\n",
            "Epoch  16 Batch  455 / 525  Training Loss  0.01583431288599968\n",
            "Epoch  16 Batch  456 / 525  Training Loss  0.01778164878487587\n",
            "Epoch  16 Batch  457 / 525  Training Loss  0.013573966920375824\n",
            "Epoch  16 Batch  458 / 525  Training Loss  0.017097149044275284\n",
            "Epoch  16 Batch  459 / 525  Training Loss  0.013799173757433891\n",
            "Epoch  16 Batch  460 / 525  Training Loss  0.019213857129216194\n",
            "Epoch  16 Batch  461 / 525  Training Loss  0.01710132509469986\n",
            "Epoch  16 Batch  462 / 525  Training Loss  0.0101320780813694\n",
            "Epoch  16 Batch  463 / 525  Training Loss  0.01831119880080223\n",
            "Epoch  16 Batch  464 / 525  Training Loss  0.009852024726569653\n",
            "Epoch  16 Batch  465 / 525  Training Loss  0.01359036285430193\n",
            "Epoch  16 Batch  466 / 525  Training Loss  0.018598688766360283\n",
            "Epoch  16 Batch  467 / 525  Training Loss  0.018028896301984787\n",
            "Epoch  16 Batch  468 / 525  Training Loss  0.017048517242074013\n",
            "Epoch  16 Batch  469 / 525  Training Loss  0.017541032284498215\n",
            "Epoch  16 Batch  470 / 525  Training Loss  0.011446891352534294\n",
            "Epoch  16 Batch  471 / 525  Training Loss  0.012499317526817322\n",
            "Epoch  16 Batch  472 / 525  Training Loss  0.010766235180199146\n",
            "Epoch  16 Batch  473 / 525  Training Loss  0.020202213898301125\n",
            "Epoch  16 Batch  474 / 525  Training Loss  0.020032556727528572\n",
            "Epoch  16 Batch  475 / 525  Training Loss  0.025869155302643776\n",
            "Epoch  16 Batch  476 / 525  Training Loss  0.020696893334388733\n",
            "Epoch  16 Batch  477 / 525  Training Loss  0.016443630680441856\n",
            "Epoch  16 Batch  478 / 525  Training Loss  0.017097465693950653\n",
            "Epoch  16 Batch  479 / 525  Training Loss  0.02143305167555809\n",
            "Epoch  16 Batch  480 / 525  Training Loss  0.012113961391150951\n",
            "Epoch  16 Batch  481 / 525  Training Loss  0.016703398898243904\n",
            "Epoch  16 Batch  482 / 525  Training Loss  0.015436245128512383\n",
            "Epoch  16 Batch  483 / 525  Training Loss  0.01061803288757801\n",
            "Epoch  16 Batch  484 / 525  Training Loss  0.00949861854314804\n",
            "Epoch  16 Batch  485 / 525  Training Loss  0.013611646369099617\n",
            "Epoch  16 Batch  486 / 525  Training Loss  0.01109425537288189\n",
            "Epoch  16 Batch  487 / 525  Training Loss  0.006469030864536762\n",
            "Epoch  16 Batch  488 / 525  Training Loss  0.01579873636364937\n",
            "Epoch  16 Batch  489 / 525  Training Loss  0.00841009896248579\n",
            "Epoch  16 Batch  490 / 525  Training Loss  0.01388112735003233\n",
            "Epoch  16 Batch  491 / 525  Training Loss  0.02602650225162506\n",
            "Epoch  16 Batch  492 / 525  Training Loss  0.014166849665343761\n",
            "Epoch  16 Batch  493 / 525  Training Loss  0.01961265131831169\n",
            "Epoch  16 Batch  494 / 525  Training Loss  0.013820992782711983\n",
            "Epoch  16 Batch  495 / 525  Training Loss  0.01785123720765114\n",
            "Epoch  16 Batch  496 / 525  Training Loss  0.019324248656630516\n",
            "Epoch  16 Batch  497 / 525  Training Loss  0.007366479840129614\n",
            "Epoch  16 Batch  498 / 525  Training Loss  0.011897670105099678\n",
            "Epoch  16 Batch  499 / 525  Training Loss  0.01085933018475771\n",
            "Epoch  16 Batch  500 / 525  Training Loss  0.019616764038801193\n",
            "Epoch  16 Batch  501 / 525  Training Loss  0.019800616428256035\n",
            "Epoch  16 Batch  502 / 525  Training Loss  0.01664692535996437\n",
            "Epoch  16 Batch  503 / 525  Training Loss  0.009798967279493809\n",
            "Epoch  16 Batch  504 / 525  Training Loss  0.010934150777757168\n",
            "Epoch  16 Batch  505 / 525  Training Loss  0.010362343862652779\n",
            "Epoch  16 Batch  506 / 525  Training Loss  0.028860192745923996\n",
            "Epoch  16 Batch  507 / 525  Training Loss  0.014891189523041248\n",
            "Epoch  16 Batch  508 / 525  Training Loss  0.020744793117046356\n",
            "Epoch  16 Batch  509 / 525  Training Loss  0.010382922366261482\n",
            "Epoch  16 Batch  510 / 525  Training Loss  0.02970009110867977\n",
            "Epoch  16 Batch  511 / 525  Training Loss  0.025015462189912796\n",
            "Epoch  16 Batch  512 / 525  Training Loss  0.01756490394473076\n",
            "Epoch  16 Batch  513 / 525  Training Loss  0.006547910161316395\n",
            "Epoch  16 Batch  514 / 525  Training Loss  0.016596872359514236\n",
            "Epoch  16 Batch  515 / 525  Training Loss  0.03107290342450142\n",
            "Epoch  16 Batch  516 / 525  Training Loss  0.01925678923726082\n",
            "Epoch  16 Batch  517 / 525  Training Loss  0.015758845955133438\n",
            "Epoch  16 Batch  518 / 525  Training Loss  0.016344856470823288\n",
            "Epoch  16 Batch  519 / 525  Training Loss  0.011317028664052486\n",
            "Epoch  16 Batch  520 / 525  Training Loss  0.01897074654698372\n",
            "Epoch  16 Batch  521 / 525  Training Loss  0.011721733026206493\n",
            "Epoch  16 Batch  522 / 525  Training Loss  0.022575249895453453\n",
            "Epoch  16 Batch  523 / 525  Training Loss  0.013453508727252483\n",
            "Epoch  16 Batch  524 / 525  Training Loss  0.02323976904153824\n",
            "  17    |    -    |   0.013604   | 55.275000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 17\n",
            "Epoch  17 Batch  0 / 525  Training Loss  0.0072127440944314\n",
            "Epoch  17 Batch  1 / 525  Training Loss  0.005196219310164452\n",
            "Epoch  17 Batch  2 / 525  Training Loss  0.023988377302885056\n",
            "Epoch  17 Batch  3 / 525  Training Loss  0.007131412625312805\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  17 Batch  4 / 525  Training Loss  0.008642749860882759\n",
            "Epoch  17 Batch  5 / 525  Training Loss  0.008085177280008793\n",
            "Epoch  17 Batch  6 / 525  Training Loss  0.012069299817085266\n",
            "Epoch  17 Batch  7 / 525  Training Loss  0.010540359653532505\n",
            "Epoch  17 Batch  8 / 525  Training Loss  0.005525634624063969\n",
            "Epoch  17 Batch  9 / 525  Training Loss  0.017125453799962997\n",
            "Epoch  17 Batch  10 / 525  Training Loss  0.02197895012795925\n",
            "Epoch  17 Batch  11 / 525  Training Loss  0.021919626742601395\n",
            "Epoch  17 Batch  12 / 525  Training Loss  0.01594533585011959\n",
            "Epoch  17 Batch  13 / 525  Training Loss  0.013111638836562634\n",
            "Epoch  17 Batch  14 / 525  Training Loss  0.00494362460449338\n",
            "Epoch  17 Batch  15 / 525  Training Loss  0.007956033572554588\n",
            "Epoch  17 Batch  16 / 525  Training Loss  0.008477059192955494\n",
            "Epoch  17 Batch  17 / 525  Training Loss  0.005962028168141842\n",
            "Epoch  17 Batch  18 / 525  Training Loss  0.009248493239283562\n",
            "Epoch  17 Batch  19 / 525  Training Loss  0.011312632821500301\n",
            "Epoch  17 Batch  20 / 525  Training Loss  0.012164553627371788\n",
            "Epoch  17 Batch  21 / 525  Training Loss  0.010515555739402771\n",
            "Epoch  17 Batch  22 / 525  Training Loss  0.014768123626708984\n",
            "Epoch  17 Batch  23 / 525  Training Loss  0.020705968141555786\n",
            "Epoch  17 Batch  24 / 525  Training Loss  0.01348467729985714\n",
            "Epoch  17 Batch  25 / 525  Training Loss  0.014334172010421753\n",
            "Epoch  17 Batch  26 / 525  Training Loss  0.014459672383964062\n",
            "Epoch  17 Batch  27 / 525  Training Loss  0.01302823144942522\n",
            "Epoch  17 Batch  28 / 525  Training Loss  0.012925183400511742\n",
            "Epoch  17 Batch  29 / 525  Training Loss  0.0072615137323737144\n",
            "Epoch  17 Batch  30 / 525  Training Loss  0.008953775279223919\n",
            "Epoch  17 Batch  31 / 525  Training Loss  0.007363708224147558\n",
            "Epoch  17 Batch  32 / 525  Training Loss  0.006722381804138422\n",
            "Epoch  17 Batch  33 / 525  Training Loss  0.006143863312900066\n",
            "Epoch  17 Batch  34 / 525  Training Loss  0.007514926139265299\n",
            "Epoch  17 Batch  35 / 525  Training Loss  0.01027766428887844\n",
            "Epoch  17 Batch  36 / 525  Training Loss  0.00896220188587904\n",
            "Epoch  17 Batch  37 / 525  Training Loss  0.010918175801634789\n",
            "Epoch  17 Batch  38 / 525  Training Loss  0.0045008230954408646\n",
            "Epoch  17 Batch  39 / 525  Training Loss  0.018971648067235947\n",
            "Epoch  17 Batch  40 / 525  Training Loss  0.00775251304730773\n",
            "Epoch  17 Batch  41 / 525  Training Loss  0.004149871878325939\n",
            "Epoch  17 Batch  42 / 525  Training Loss  0.006691254675388336\n",
            "Epoch  17 Batch  43 / 525  Training Loss  0.01164300087839365\n",
            "Epoch  17 Batch  44 / 525  Training Loss  0.007326686289161444\n",
            "Epoch  17 Batch  45 / 525  Training Loss  0.006341471336781979\n",
            "Epoch  17 Batch  46 / 525  Training Loss  0.00995154120028019\n",
            "Epoch  17 Batch  47 / 525  Training Loss  0.006405753083527088\n",
            "Epoch  17 Batch  48 / 525  Training Loss  0.007469619624316692\n",
            "Epoch  17 Batch  49 / 525  Training Loss  0.006075255572795868\n",
            "Epoch  17 Batch  50 / 525  Training Loss  0.007070659194141626\n",
            "Epoch  17 Batch  51 / 525  Training Loss  0.005437985062599182\n",
            "Epoch  17 Batch  52 / 525  Training Loss  0.007895909249782562\n",
            "Epoch  17 Batch  53 / 525  Training Loss  0.004238790366798639\n",
            "Epoch  17 Batch  54 / 525  Training Loss  0.005387560930103064\n",
            "Epoch  17 Batch  55 / 525  Training Loss  0.0029354130383580923\n",
            "Epoch  17 Batch  56 / 525  Training Loss  0.018240846693515778\n",
            "Epoch  17 Batch  57 / 525  Training Loss  0.005175638012588024\n",
            "Epoch  17 Batch  58 / 525  Training Loss  0.006187536288052797\n",
            "Epoch  17 Batch  59 / 525  Training Loss  0.004446367267519236\n",
            "Epoch  17 Batch  60 / 525  Training Loss  0.0080705052241683\n",
            "Epoch  17 Batch  61 / 525  Training Loss  0.0103480014950037\n",
            "Epoch  17 Batch  62 / 525  Training Loss  0.007747878320515156\n",
            "Epoch  17 Batch  63 / 525  Training Loss  0.003842109115794301\n",
            "Epoch  17 Batch  64 / 525  Training Loss  0.005106403026729822\n",
            "Epoch  17 Batch  65 / 525  Training Loss  0.005579105578362942\n",
            "Epoch  17 Batch  66 / 525  Training Loss  0.007935376837849617\n",
            "Epoch  17 Batch  67 / 525  Training Loss  0.004046070389449596\n",
            "Epoch  17 Batch  68 / 525  Training Loss  0.009322235360741615\n",
            "Epoch  17 Batch  69 / 525  Training Loss  0.009845264256000519\n",
            "Epoch  17 Batch  70 / 525  Training Loss  0.007963059470057487\n",
            "Epoch  17 Batch  71 / 525  Training Loss  0.006960563361644745\n",
            "Epoch  17 Batch  72 / 525  Training Loss  0.006716486066579819\n",
            "Epoch  17 Batch  73 / 525  Training Loss  0.007423184812068939\n",
            "Epoch  17 Batch  74 / 525  Training Loss  0.011624455451965332\n",
            "Epoch  17 Batch  75 / 525  Training Loss  0.012579764239490032\n",
            "Epoch  17 Batch  76 / 525  Training Loss  0.0069220117293298244\n",
            "Epoch  17 Batch  77 / 525  Training Loss  0.006462103221565485\n",
            "Epoch  17 Batch  78 / 525  Training Loss  0.0068896254524588585\n",
            "Epoch  17 Batch  79 / 525  Training Loss  0.008247138932347298\n",
            "Epoch  17 Batch  80 / 525  Training Loss  0.007635111454874277\n",
            "Epoch  17 Batch  81 / 525  Training Loss  0.014676639810204506\n",
            "Epoch  17 Batch  82 / 525  Training Loss  0.010927452705800533\n",
            "Epoch  17 Batch  83 / 525  Training Loss  0.0033530634827911854\n",
            "Epoch  17 Batch  84 / 525  Training Loss  0.005927900318056345\n",
            "Epoch  17 Batch  85 / 525  Training Loss  0.01161962654441595\n",
            "Epoch  17 Batch  86 / 525  Training Loss  0.006763122975826263\n",
            "Epoch  17 Batch  87 / 525  Training Loss  0.003938924986869097\n",
            "Epoch  17 Batch  88 / 525  Training Loss  0.013302301056683064\n",
            "Epoch  17 Batch  89 / 525  Training Loss  0.013689892366528511\n",
            "Epoch  17 Batch  90 / 525  Training Loss  0.02030171826481819\n",
            "Epoch  17 Batch  91 / 525  Training Loss  0.0030812877230346203\n",
            "Epoch  17 Batch  92 / 525  Training Loss  0.007902798242866993\n",
            "Epoch  17 Batch  93 / 525  Training Loss  0.01550997979938984\n",
            "Epoch  17 Batch  94 / 525  Training Loss  0.008113941177725792\n",
            "Epoch  17 Batch  95 / 525  Training Loss  0.011705641634762287\n",
            "Epoch  17 Batch  96 / 525  Training Loss  0.008601121604442596\n",
            "Epoch  17 Batch  97 / 525  Training Loss  0.009341560304164886\n",
            "Epoch  17 Batch  98 / 525  Training Loss  0.006332223303616047\n",
            "Epoch  17 Batch  99 / 525  Training Loss  0.00841965340077877\n",
            "Epoch  17 Batch  100 / 525  Training Loss  0.0073495106771588326\n",
            "Epoch  17 Batch  101 / 525  Training Loss  0.007481941021978855\n",
            "Epoch  17 Batch  102 / 525  Training Loss  0.005536601413041353\n",
            "Epoch  17 Batch  103 / 525  Training Loss  0.007845407351851463\n",
            "Epoch  17 Batch  104 / 525  Training Loss  0.011426001787185669\n",
            "Epoch  17 Batch  105 / 525  Training Loss  0.013213880360126495\n",
            "Epoch  17 Batch  106 / 525  Training Loss  0.007549495901912451\n",
            "Epoch  17 Batch  107 / 525  Training Loss  0.006166327279061079\n",
            "Epoch  17 Batch  108 / 525  Training Loss  0.006659722421318293\n",
            "Epoch  17 Batch  109 / 525  Training Loss  0.006662401370704174\n",
            "Epoch  17 Batch  110 / 525  Training Loss  0.010292602702975273\n",
            "Epoch  17 Batch  111 / 525  Training Loss  0.005395549815148115\n",
            "Epoch  17 Batch  112 / 525  Training Loss  0.004870188422501087\n",
            "Epoch  17 Batch  113 / 525  Training Loss  0.004634489770978689\n",
            "Epoch  17 Batch  114 / 525  Training Loss  0.010884647257626057\n",
            "Epoch  17 Batch  115 / 525  Training Loss  0.005153196398168802\n",
            "Epoch  17 Batch  116 / 525  Training Loss  0.005834517069160938\n",
            "Epoch  17 Batch  117 / 525  Training Loss  0.009960255585610867\n",
            "Epoch  17 Batch  118 / 525  Training Loss  0.00441285315901041\n",
            "Epoch  17 Batch  119 / 525  Training Loss  0.005559446290135384\n",
            "Epoch  17 Batch  120 / 525  Training Loss  0.011049083434045315\n",
            "Epoch  17 Batch  121 / 525  Training Loss  0.010952831245958805\n",
            "Epoch  17 Batch  122 / 525  Training Loss  0.009271102026104927\n",
            "Epoch  17 Batch  123 / 525  Training Loss  0.01155591569840908\n",
            "Epoch  17 Batch  124 / 525  Training Loss  0.006140328943729401\n",
            "Epoch  17 Batch  125 / 525  Training Loss  0.005009221378713846\n",
            "Epoch  17 Batch  126 / 525  Training Loss  0.004506970290094614\n",
            "Epoch  17 Batch  127 / 525  Training Loss  0.005662556737661362\n",
            "Epoch  17 Batch  128 / 525  Training Loss  0.00614975718781352\n",
            "Epoch  17 Batch  129 / 525  Training Loss  0.0067392378114163876\n",
            "Epoch  17 Batch  130 / 525  Training Loss  0.007273866329342127\n",
            "Epoch  17 Batch  131 / 525  Training Loss  0.008573169820010662\n",
            "Epoch  17 Batch  132 / 525  Training Loss  0.00463618291541934\n",
            "Epoch  17 Batch  133 / 525  Training Loss  0.00403437577188015\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  17 Batch  134 / 525  Training Loss  0.008139240555465221\n",
            "Epoch  17 Batch  135 / 525  Training Loss  0.008989554829895496\n",
            "Epoch  17 Batch  136 / 525  Training Loss  0.008360160514712334\n",
            "Epoch  17 Batch  137 / 525  Training Loss  0.006095550954341888\n",
            "Epoch  17 Batch  138 / 525  Training Loss  0.011585621163249016\n",
            "Epoch  17 Batch  139 / 525  Training Loss  0.006005135830491781\n",
            "Epoch  17 Batch  140 / 525  Training Loss  0.006955637596547604\n",
            "Epoch  17 Batch  141 / 525  Training Loss  0.007343982346355915\n",
            "Epoch  17 Batch  142 / 525  Training Loss  0.004994390066713095\n",
            "Epoch  17 Batch  143 / 525  Training Loss  0.011249670758843422\n",
            "Epoch  17 Batch  144 / 525  Training Loss  0.0042340741492807865\n",
            "Epoch  17 Batch  145 / 525  Training Loss  0.005605874117463827\n",
            "Epoch  17 Batch  146 / 525  Training Loss  0.007455657236278057\n",
            "Epoch  17 Batch  147 / 525  Training Loss  0.010800333693623543\n",
            "Epoch  17 Batch  148 / 525  Training Loss  0.007632619235664606\n",
            "Epoch  17 Batch  149 / 525  Training Loss  0.008974412456154823\n",
            "Epoch  17 Batch  150 / 525  Training Loss  0.0177397970110178\n",
            "Epoch  17 Batch  151 / 525  Training Loss  0.007721446454524994\n",
            "Epoch  17 Batch  152 / 525  Training Loss  0.01425234042108059\n",
            "Epoch  17 Batch  153 / 525  Training Loss  0.010991322807967663\n",
            "Epoch  17 Batch  154 / 525  Training Loss  0.005839761812239885\n",
            "Epoch  17 Batch  155 / 525  Training Loss  0.005641797557473183\n",
            "Epoch  17 Batch  156 / 525  Training Loss  0.01184057630598545\n",
            "Epoch  17 Batch  157 / 525  Training Loss  0.00687835831195116\n",
            "Epoch  17 Batch  158 / 525  Training Loss  0.008756675757467747\n",
            "Epoch  17 Batch  159 / 525  Training Loss  0.01212456077337265\n",
            "Epoch  17 Batch  160 / 525  Training Loss  0.010318479500710964\n",
            "Epoch  17 Batch  161 / 525  Training Loss  0.007713768631219864\n",
            "Epoch  17 Batch  162 / 525  Training Loss  0.0069602252915501595\n",
            "Epoch  17 Batch  163 / 525  Training Loss  0.008017636835575104\n",
            "Epoch  17 Batch  164 / 525  Training Loss  0.006467738188803196\n",
            "Epoch  17 Batch  165 / 525  Training Loss  0.004421954974532127\n",
            "Epoch  17 Batch  166 / 525  Training Loss  0.006349734961986542\n",
            "Epoch  17 Batch  167 / 525  Training Loss  0.005203832406550646\n",
            "Epoch  17 Batch  168 / 525  Training Loss  0.013286197558045387\n",
            "Epoch  17 Batch  169 / 525  Training Loss  0.006858810782432556\n",
            "Epoch  17 Batch  170 / 525  Training Loss  0.00566840497776866\n",
            "Epoch  17 Batch  171 / 525  Training Loss  0.00877449195832014\n",
            "Epoch  17 Batch  172 / 525  Training Loss  0.006652506999671459\n",
            "Epoch  17 Batch  173 / 525  Training Loss  0.005226552952080965\n",
            "Epoch  17 Batch  174 / 525  Training Loss  0.004405079409480095\n",
            "Epoch  17 Batch  175 / 525  Training Loss  0.014811431989073753\n",
            "Epoch  17 Batch  176 / 525  Training Loss  0.0071319215930998325\n",
            "Epoch  17 Batch  177 / 525  Training Loss  0.006174750626087189\n",
            "Epoch  17 Batch  178 / 525  Training Loss  0.01712777093052864\n",
            "Epoch  17 Batch  179 / 525  Training Loss  0.008814510889351368\n",
            "Epoch  17 Batch  180 / 525  Training Loss  0.008044550195336342\n",
            "Epoch  17 Batch  181 / 525  Training Loss  0.0035436085890978575\n",
            "Epoch  17 Batch  182 / 525  Training Loss  0.0075576030649244785\n",
            "Epoch  17 Batch  183 / 525  Training Loss  0.004005261696875095\n",
            "Epoch  17 Batch  184 / 525  Training Loss  0.013394045643508434\n",
            "Epoch  17 Batch  185 / 525  Training Loss  0.00710730254650116\n",
            "Epoch  17 Batch  186 / 525  Training Loss  0.009727953001856804\n",
            "Epoch  17 Batch  187 / 525  Training Loss  0.005381654016673565\n",
            "Epoch  17 Batch  188 / 525  Training Loss  0.011789337731897831\n",
            "Epoch  17 Batch  189 / 525  Training Loss  0.0070508262142539024\n",
            "Epoch  17 Batch  190 / 525  Training Loss  0.00966616254299879\n",
            "Epoch  17 Batch  191 / 525  Training Loss  0.007021797355264425\n",
            "Epoch  17 Batch  192 / 525  Training Loss  0.006348484195768833\n",
            "Epoch  17 Batch  193 / 525  Training Loss  0.011274779215455055\n",
            "Epoch  17 Batch  194 / 525  Training Loss  0.005267825908958912\n",
            "Epoch  17 Batch  195 / 525  Training Loss  0.017914239317178726\n",
            "Epoch  17 Batch  196 / 525  Training Loss  0.007118735462427139\n",
            "Epoch  17 Batch  197 / 525  Training Loss  0.008920615538954735\n",
            "Epoch  17 Batch  198 / 525  Training Loss  0.011922724545001984\n",
            "Epoch  17 Batch  199 / 525  Training Loss  0.013306309469044209\n",
            "Epoch  17 Batch  200 / 525  Training Loss  0.008184177801012993\n",
            "Epoch  17 Batch  201 / 525  Training Loss  0.028242677450180054\n",
            "Epoch  17 Batch  202 / 525  Training Loss  0.023597192019224167\n",
            "Epoch  17 Batch  203 / 525  Training Loss  0.009863866493105888\n",
            "Epoch  17 Batch  204 / 525  Training Loss  0.006057018414139748\n",
            "Epoch  17 Batch  205 / 525  Training Loss  0.008537657558918\n",
            "Epoch  17 Batch  206 / 525  Training Loss  0.014267681166529655\n",
            "Epoch  17 Batch  207 / 525  Training Loss  0.009722976945340633\n",
            "Epoch  17 Batch  208 / 525  Training Loss  0.027204643934965134\n",
            "Epoch  17 Batch  209 / 525  Training Loss  0.007542538456618786\n",
            "Epoch  17 Batch  210 / 525  Training Loss  0.00937623344361782\n",
            "Epoch  17 Batch  211 / 525  Training Loss  0.006620883941650391\n",
            "Epoch  17 Batch  212 / 525  Training Loss  0.009804853238165379\n",
            "Epoch  17 Batch  213 / 525  Training Loss  0.010540669783949852\n",
            "Epoch  17 Batch  214 / 525  Training Loss  0.022269558161497116\n",
            "Epoch  17 Batch  215 / 525  Training Loss  0.009846972301602364\n",
            "Epoch  17 Batch  216 / 525  Training Loss  0.017135903239250183\n",
            "Epoch  17 Batch  217 / 525  Training Loss  0.010812302120029926\n",
            "Epoch  17 Batch  218 / 525  Training Loss  0.011495949700474739\n",
            "Epoch  17 Batch  219 / 525  Training Loss  0.011069445870816708\n",
            "Epoch  17 Batch  220 / 525  Training Loss  0.013096952810883522\n",
            "Epoch  17 Batch  221 / 525  Training Loss  0.017894061282277107\n",
            "Epoch  17 Batch  222 / 525  Training Loss  0.029856044799089432\n",
            "Epoch  17 Batch  223 / 525  Training Loss  0.006587529089301825\n",
            "Epoch  17 Batch  224 / 525  Training Loss  0.0094512440264225\n",
            "Epoch  17 Batch  225 / 525  Training Loss  0.015868347138166428\n",
            "Epoch  17 Batch  226 / 525  Training Loss  0.007109194062650204\n",
            "Epoch  17 Batch  227 / 525  Training Loss  0.011817246675491333\n",
            "Epoch  17 Batch  228 / 525  Training Loss  0.004254755098372698\n",
            "Epoch  17 Batch  229 / 525  Training Loss  0.011060762219130993\n",
            "Epoch  17 Batch  230 / 525  Training Loss  0.01614903286099434\n",
            "Epoch  17 Batch  231 / 525  Training Loss  0.011985677294433117\n",
            "Epoch  17 Batch  232 / 525  Training Loss  0.012545408681035042\n",
            "Epoch  17 Batch  233 / 525  Training Loss  0.009885357692837715\n",
            "Epoch  17 Batch  234 / 525  Training Loss  0.008088858798146248\n",
            "Epoch  17 Batch  235 / 525  Training Loss  0.006448777858167887\n",
            "Epoch  17 Batch  236 / 525  Training Loss  0.007927880622446537\n",
            "Epoch  17 Batch  237 / 525  Training Loss  0.007523874286562204\n",
            "Epoch  17 Batch  238 / 525  Training Loss  0.010137628763914108\n",
            "Epoch  17 Batch  239 / 525  Training Loss  0.00635267049074173\n",
            "Epoch  17 Batch  240 / 525  Training Loss  0.007253820542246103\n",
            "Epoch  17 Batch  241 / 525  Training Loss  0.0103843305259943\n",
            "Epoch  17 Batch  242 / 525  Training Loss  0.013394351117312908\n",
            "Epoch  17 Batch  243 / 525  Training Loss  0.01139514148235321\n",
            "Epoch  17 Batch  244 / 525  Training Loss  0.005999201443046331\n",
            "Epoch  17 Batch  245 / 525  Training Loss  0.006980095990002155\n",
            "Epoch  17 Batch  246 / 525  Training Loss  0.01470695436000824\n",
            "Epoch  17 Batch  247 / 525  Training Loss  0.020281117409467697\n",
            "Epoch  17 Batch  248 / 525  Training Loss  0.01532540749758482\n",
            "Epoch  17 Batch  249 / 525  Training Loss  0.020720886066555977\n",
            "Epoch  17 Batch  250 / 525  Training Loss  0.009195568040013313\n",
            "Epoch  17 Batch  251 / 525  Training Loss  0.014599588699638844\n",
            "Epoch  17 Batch  252 / 525  Training Loss  0.004530270583927631\n",
            "Epoch  17 Batch  253 / 525  Training Loss  0.012532164342701435\n",
            "Epoch  17 Batch  254 / 525  Training Loss  0.011514876037836075\n",
            "Epoch  17 Batch  255 / 525  Training Loss  0.0077088745310902596\n",
            "Epoch  17 Batch  256 / 525  Training Loss  0.008318090811371803\n",
            "Epoch  17 Batch  257 / 525  Training Loss  0.007498875260353088\n",
            "Epoch  17 Batch  258 / 525  Training Loss  0.006113910116255283\n",
            "Epoch  17 Batch  259 / 525  Training Loss  0.005637980066239834\n",
            "Epoch  17 Batch  260 / 525  Training Loss  0.014613611623644829\n",
            "Epoch  17 Batch  261 / 525  Training Loss  0.013435306027531624\n",
            "Epoch  17 Batch  262 / 525  Training Loss  0.00836868304759264\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  17 Batch  263 / 525  Training Loss  0.01131308265030384\n",
            "Epoch  17 Batch  264 / 525  Training Loss  0.007688420359045267\n",
            "Epoch  17 Batch  265 / 525  Training Loss  0.006344574503600597\n",
            "Epoch  17 Batch  266 / 525  Training Loss  0.004755742847919464\n",
            "Epoch  17 Batch  267 / 525  Training Loss  0.008414175361394882\n",
            "Epoch  17 Batch  268 / 525  Training Loss  0.0054581440053880215\n",
            "Epoch  17 Batch  269 / 525  Training Loss  0.008702589198946953\n",
            "Epoch  17 Batch  270 / 525  Training Loss  0.0045425379648804665\n",
            "Epoch  17 Batch  271 / 525  Training Loss  0.01233193650841713\n",
            "Epoch  17 Batch  272 / 525  Training Loss  0.018530579283833504\n",
            "Epoch  17 Batch  273 / 525  Training Loss  0.011372828856110573\n",
            "Epoch  17 Batch  274 / 525  Training Loss  0.009994368068873882\n",
            "Epoch  17 Batch  275 / 525  Training Loss  0.013216959312558174\n",
            "Epoch  17 Batch  276 / 525  Training Loss  0.006221123971045017\n",
            "Epoch  17 Batch  277 / 525  Training Loss  0.010883113369345665\n",
            "Epoch  17 Batch  278 / 525  Training Loss  0.011260556057095528\n",
            "Epoch  17 Batch  279 / 525  Training Loss  0.017812345176935196\n",
            "Epoch  17 Batch  280 / 525  Training Loss  0.009029262699186802\n",
            "Epoch  17 Batch  281 / 525  Training Loss  0.009065083228051662\n",
            "Epoch  17 Batch  282 / 525  Training Loss  0.01462642103433609\n",
            "Epoch  17 Batch  283 / 525  Training Loss  0.010564032010734081\n",
            "Epoch  17 Batch  284 / 525  Training Loss  0.011939095333218575\n",
            "Epoch  17 Batch  285 / 525  Training Loss  0.019857257604599\n",
            "Epoch  17 Batch  286 / 525  Training Loss  0.009612014517188072\n",
            "Epoch  17 Batch  287 / 525  Training Loss  0.009059904143214226\n",
            "Epoch  17 Batch  288 / 525  Training Loss  0.008452184498310089\n",
            "Epoch  17 Batch  289 / 525  Training Loss  0.010524479672312737\n",
            "Epoch  17 Batch  290 / 525  Training Loss  0.010816617868840694\n",
            "Epoch  17 Batch  291 / 525  Training Loss  0.008520747534930706\n",
            "Epoch  17 Batch  292 / 525  Training Loss  0.008465783670544624\n",
            "Epoch  17 Batch  293 / 525  Training Loss  0.012673867866396904\n",
            "Epoch  17 Batch  294 / 525  Training Loss  0.019920028746128082\n",
            "Epoch  17 Batch  295 / 525  Training Loss  0.01614881493151188\n",
            "Epoch  17 Batch  296 / 525  Training Loss  0.007963837124407291\n",
            "Epoch  17 Batch  297 / 525  Training Loss  0.018788162618875504\n",
            "Epoch  17 Batch  298 / 525  Training Loss  0.011564040556550026\n",
            "Epoch  17 Batch  299 / 525  Training Loss  0.005922826938331127\n",
            "Epoch  17 Batch  300 / 525  Training Loss  0.012100370600819588\n",
            "Epoch  17 Batch  301 / 525  Training Loss  0.007544183172285557\n",
            "Epoch  17 Batch  302 / 525  Training Loss  0.009303607046604156\n",
            "Epoch  17 Batch  303 / 525  Training Loss  0.008289085701107979\n",
            "Epoch  17 Batch  304 / 525  Training Loss  0.006025578826665878\n",
            "Epoch  17 Batch  305 / 525  Training Loss  0.00827930960804224\n",
            "Epoch  17 Batch  306 / 525  Training Loss  0.010927537456154823\n",
            "Epoch  17 Batch  307 / 525  Training Loss  0.01228674128651619\n",
            "Epoch  17 Batch  308 / 525  Training Loss  0.014218570664525032\n",
            "Epoch  17 Batch  309 / 525  Training Loss  0.01243068277835846\n",
            "Epoch  17 Batch  310 / 525  Training Loss  0.01044397708028555\n",
            "Epoch  17 Batch  311 / 525  Training Loss  0.007595685310661793\n",
            "Epoch  17 Batch  312 / 525  Training Loss  0.009945482946932316\n",
            "Epoch  17 Batch  313 / 525  Training Loss  0.010113904252648354\n",
            "Epoch  17 Batch  314 / 525  Training Loss  0.015857288613915443\n",
            "Epoch  17 Batch  315 / 525  Training Loss  0.011469319462776184\n",
            "Epoch  17 Batch  316 / 525  Training Loss  0.007009903900325298\n",
            "Epoch  17 Batch  317 / 525  Training Loss  0.008998192846775055\n",
            "Epoch  17 Batch  318 / 525  Training Loss  0.011558203026652336\n",
            "Epoch  17 Batch  319 / 525  Training Loss  0.013203014619648457\n",
            "Epoch  17 Batch  320 / 525  Training Loss  0.011301426216959953\n",
            "Epoch  17 Batch  321 / 525  Training Loss  0.008738701231777668\n",
            "Epoch  17 Batch  322 / 525  Training Loss  0.020870139822363853\n",
            "Epoch  17 Batch  323 / 525  Training Loss  0.014105089008808136\n",
            "Epoch  17 Batch  324 / 525  Training Loss  0.00674345251172781\n",
            "Epoch  17 Batch  325 / 525  Training Loss  0.012731368653476238\n",
            "Epoch  17 Batch  326 / 525  Training Loss  0.010998407378792763\n",
            "Epoch  17 Batch  327 / 525  Training Loss  0.012284702621400356\n",
            "Epoch  17 Batch  328 / 525  Training Loss  0.010437960736453533\n",
            "Epoch  17 Batch  329 / 525  Training Loss  0.01019459217786789\n",
            "Epoch  17 Batch  330 / 525  Training Loss  0.008815732784569263\n",
            "Epoch  17 Batch  331 / 525  Training Loss  0.0035835430026054382\n",
            "Epoch  17 Batch  332 / 525  Training Loss  0.011058622971177101\n",
            "Epoch  17 Batch  333 / 525  Training Loss  0.008178316056728363\n",
            "Epoch  17 Batch  334 / 525  Training Loss  0.006715954281389713\n",
            "Epoch  17 Batch  335 / 525  Training Loss  0.022491734474897385\n",
            "Epoch  17 Batch  336 / 525  Training Loss  0.011222407221794128\n",
            "Epoch  17 Batch  337 / 525  Training Loss  0.005937498062849045\n",
            "Epoch  17 Batch  338 / 525  Training Loss  0.004466749262064695\n",
            "Epoch  17 Batch  339 / 525  Training Loss  0.016436152160167694\n",
            "Epoch  17 Batch  340 / 525  Training Loss  0.007754410617053509\n",
            "Epoch  17 Batch  341 / 525  Training Loss  0.01166593935340643\n",
            "Epoch  17 Batch  342 / 525  Training Loss  0.018380366265773773\n",
            "Epoch  17 Batch  343 / 525  Training Loss  0.008362882770597935\n",
            "Epoch  17 Batch  344 / 525  Training Loss  0.008265340700745583\n",
            "Epoch  17 Batch  345 / 525  Training Loss  0.00462455814704299\n",
            "Epoch  17 Batch  346 / 525  Training Loss  0.009999609552323818\n",
            "Epoch  17 Batch  347 / 525  Training Loss  0.006872246507555246\n",
            "Epoch  17 Batch  348 / 525  Training Loss  0.004575912840664387\n",
            "Epoch  17 Batch  349 / 525  Training Loss  0.015201111324131489\n",
            "Epoch  17 Batch  350 / 525  Training Loss  0.009321862831711769\n",
            "Epoch  17 Batch  351 / 525  Training Loss  0.007195409387350082\n",
            "Epoch  17 Batch  352 / 525  Training Loss  0.011450441554188728\n",
            "Epoch  17 Batch  353 / 525  Training Loss  0.012936057522892952\n",
            "Epoch  17 Batch  354 / 525  Training Loss  0.008681543171405792\n",
            "Epoch  17 Batch  355 / 525  Training Loss  0.011863029561936855\n",
            "Epoch  17 Batch  356 / 525  Training Loss  0.015746237710118294\n",
            "Epoch  17 Batch  357 / 525  Training Loss  0.006720507983118296\n",
            "Epoch  17 Batch  358 / 525  Training Loss  0.015095588751137257\n",
            "Epoch  17 Batch  359 / 525  Training Loss  0.0062482585199177265\n",
            "Epoch  17 Batch  360 / 525  Training Loss  0.01035973522812128\n",
            "Epoch  17 Batch  361 / 525  Training Loss  0.00823573675006628\n",
            "Epoch  17 Batch  362 / 525  Training Loss  0.012197280302643776\n",
            "Epoch  17 Batch  363 / 525  Training Loss  0.01064993254840374\n",
            "Epoch  17 Batch  364 / 525  Training Loss  0.017243165522813797\n",
            "Epoch  17 Batch  365 / 525  Training Loss  0.023988476023077965\n",
            "Epoch  17 Batch  366 / 525  Training Loss  0.007304134778678417\n",
            "Epoch  17 Batch  367 / 525  Training Loss  0.014731612987816334\n",
            "Epoch  17 Batch  368 / 525  Training Loss  0.010920997709035873\n",
            "Epoch  17 Batch  369 / 525  Training Loss  0.01324276439845562\n",
            "Epoch  17 Batch  370 / 525  Training Loss  0.005461298860609531\n",
            "Epoch  17 Batch  371 / 525  Training Loss  0.007033953908830881\n",
            "Epoch  17 Batch  372 / 525  Training Loss  0.0194248016923666\n",
            "Epoch  17 Batch  373 / 525  Training Loss  0.006559035740792751\n",
            "Epoch  17 Batch  374 / 525  Training Loss  0.009486156515777111\n",
            "Epoch  17 Batch  375 / 525  Training Loss  0.005936247296631336\n",
            "Epoch  17 Batch  376 / 525  Training Loss  0.010084236972033978\n",
            "Epoch  17 Batch  377 / 525  Training Loss  0.009764938615262508\n",
            "Epoch  17 Batch  378 / 525  Training Loss  0.01055008266121149\n",
            "Epoch  17 Batch  379 / 525  Training Loss  0.007894305512309074\n",
            "Epoch  17 Batch  380 / 525  Training Loss  0.004040941596031189\n",
            "Epoch  17 Batch  381 / 525  Training Loss  0.008149703033268452\n",
            "Epoch  17 Batch  382 / 525  Training Loss  0.013292551040649414\n",
            "Epoch  17 Batch  383 / 525  Training Loss  0.0075897471979260445\n",
            "Epoch  17 Batch  384 / 525  Training Loss  0.007769792340695858\n",
            "Epoch  17 Batch  385 / 525  Training Loss  0.0021502298768609762\n",
            "Epoch  17 Batch  386 / 525  Training Loss  0.008472530171275139\n",
            "Epoch  17 Batch  387 / 525  Training Loss  0.007732369005680084\n",
            "Epoch  17 Batch  388 / 525  Training Loss  0.007089839782565832\n",
            "Epoch  17 Batch  389 / 525  Training Loss  0.010502149350941181\n",
            "Epoch  17 Batch  390 / 525  Training Loss  0.009372097440063953\n",
            "Epoch  17 Batch  391 / 525  Training Loss  0.010914986953139305\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  17 Batch  392 / 525  Training Loss  0.01082952506840229\n",
            "Epoch  17 Batch  393 / 525  Training Loss  0.008629834279417992\n",
            "Epoch  17 Batch  394 / 525  Training Loss  0.006369070615619421\n",
            "Epoch  17 Batch  395 / 525  Training Loss  0.008002230897545815\n",
            "Epoch  17 Batch  396 / 525  Training Loss  0.015136323869228363\n",
            "Epoch  17 Batch  397 / 525  Training Loss  0.013874515891075134\n",
            "Epoch  17 Batch  398 / 525  Training Loss  0.011090871877968311\n",
            "Epoch  17 Batch  399 / 525  Training Loss  0.00811643898487091\n",
            "Epoch  17 Batch  400 / 525  Training Loss  0.019678562879562378\n",
            "Epoch  17 Batch  401 / 525  Training Loss  0.014781096950173378\n",
            "Epoch  17 Batch  402 / 525  Training Loss  0.0050044492818415165\n",
            "Epoch  17 Batch  403 / 525  Training Loss  0.006031122524291277\n",
            "Epoch  17 Batch  404 / 525  Training Loss  0.011862915009260178\n",
            "Epoch  17 Batch  405 / 525  Training Loss  0.007058992981910706\n",
            "Epoch  17 Batch  406 / 525  Training Loss  0.004961805418133736\n",
            "Epoch  17 Batch  407 / 525  Training Loss  0.01715218462049961\n",
            "Epoch  17 Batch  408 / 525  Training Loss  0.024215396493673325\n",
            "Epoch  17 Batch  409 / 525  Training Loss  0.009147671982645988\n",
            "Epoch  17 Batch  410 / 525  Training Loss  0.00950460322201252\n",
            "Epoch  17 Batch  411 / 525  Training Loss  0.0066522532142698765\n",
            "Epoch  17 Batch  412 / 525  Training Loss  0.014703948982059956\n",
            "Epoch  17 Batch  413 / 525  Training Loss  0.0044538481160998344\n",
            "Epoch  17 Batch  414 / 525  Training Loss  0.007357118185609579\n",
            "Epoch  17 Batch  415 / 525  Training Loss  0.015172494575381279\n",
            "Epoch  17 Batch  416 / 525  Training Loss  0.010859310626983643\n",
            "Epoch  17 Batch  417 / 525  Training Loss  0.012997262179851532\n",
            "Epoch  17 Batch  418 / 525  Training Loss  0.0127726374194026\n",
            "Epoch  17 Batch  419 / 525  Training Loss  0.01439213939011097\n",
            "Epoch  17 Batch  420 / 525  Training Loss  0.008893945254385471\n",
            "Epoch  17 Batch  421 / 525  Training Loss  0.021594401448965073\n",
            "Epoch  17 Batch  422 / 525  Training Loss  0.008981535211205482\n",
            "Epoch  17 Batch  423 / 525  Training Loss  0.012300047092139721\n",
            "Epoch  17 Batch  424 / 525  Training Loss  0.025670981034636497\n",
            "Epoch  17 Batch  425 / 525  Training Loss  0.016142327338457108\n",
            "Epoch  17 Batch  426 / 525  Training Loss  0.009812934324145317\n",
            "Epoch  17 Batch  427 / 525  Training Loss  0.01166137307882309\n",
            "Epoch  17 Batch  428 / 525  Training Loss  0.008475476875901222\n",
            "Epoch  17 Batch  429 / 525  Training Loss  0.006349554751068354\n",
            "Epoch  17 Batch  430 / 525  Training Loss  0.006830611266195774\n",
            "Epoch  17 Batch  431 / 525  Training Loss  0.013274121098220348\n",
            "Epoch  17 Batch  432 / 525  Training Loss  0.014343485236167908\n",
            "Epoch  17 Batch  433 / 525  Training Loss  0.004884416703134775\n",
            "Epoch  17 Batch  434 / 525  Training Loss  0.010195168666541576\n",
            "Epoch  17 Batch  435 / 525  Training Loss  0.005440147593617439\n",
            "Epoch  17 Batch  436 / 525  Training Loss  0.0073702372610569\n",
            "Epoch  17 Batch  437 / 525  Training Loss  0.00809966679662466\n",
            "Epoch  17 Batch  438 / 525  Training Loss  0.008105860091745853\n",
            "Epoch  17 Batch  439 / 525  Training Loss  0.00912792794406414\n",
            "Epoch  17 Batch  440 / 525  Training Loss  0.016719719395041466\n",
            "Epoch  17 Batch  441 / 525  Training Loss  0.009648329578340054\n",
            "Epoch  17 Batch  442 / 525  Training Loss  0.012656341306865215\n",
            "Epoch  17 Batch  443 / 525  Training Loss  0.012689284980297089\n",
            "Epoch  17 Batch  444 / 525  Training Loss  0.010104423388838768\n",
            "Epoch  17 Batch  445 / 525  Training Loss  0.018651612102985382\n",
            "Epoch  17 Batch  446 / 525  Training Loss  0.009308845736086369\n",
            "Epoch  17 Batch  447 / 525  Training Loss  0.006170277949422598\n",
            "Epoch  17 Batch  448 / 525  Training Loss  0.009504076093435287\n",
            "Epoch  17 Batch  449 / 525  Training Loss  0.009546715766191483\n",
            "Epoch  17 Batch  450 / 525  Training Loss  0.006176686845719814\n",
            "Epoch  17 Batch  451 / 525  Training Loss  0.0036554362159222364\n",
            "Epoch  17 Batch  452 / 525  Training Loss  0.011318070814013481\n",
            "Epoch  17 Batch  453 / 525  Training Loss  0.006322974804788828\n",
            "Epoch  17 Batch  454 / 525  Training Loss  0.0076982625760138035\n",
            "Epoch  17 Batch  455 / 525  Training Loss  0.02988293394446373\n",
            "Epoch  17 Batch  456 / 525  Training Loss  0.013035356998443604\n",
            "Epoch  17 Batch  457 / 525  Training Loss  0.006316890008747578\n",
            "Epoch  17 Batch  458 / 525  Training Loss  0.010865719988942146\n",
            "Epoch  17 Batch  459 / 525  Training Loss  0.007845145650207996\n",
            "Epoch  17 Batch  460 / 525  Training Loss  0.009064007550477982\n",
            "Epoch  17 Batch  461 / 525  Training Loss  0.006441070698201656\n",
            "Epoch  17 Batch  462 / 525  Training Loss  0.003826932981610298\n",
            "Epoch  17 Batch  463 / 525  Training Loss  0.018315810710191727\n",
            "Epoch  17 Batch  464 / 525  Training Loss  0.012205301783978939\n",
            "Epoch  17 Batch  465 / 525  Training Loss  0.0053169988095760345\n",
            "Epoch  17 Batch  466 / 525  Training Loss  0.009463698603212833\n",
            "Epoch  17 Batch  467 / 525  Training Loss  0.017508408054709435\n",
            "Epoch  17 Batch  468 / 525  Training Loss  0.014343738555908203\n",
            "Epoch  17 Batch  469 / 525  Training Loss  0.018298108130693436\n",
            "Epoch  17 Batch  470 / 525  Training Loss  0.009332607500255108\n",
            "Epoch  17 Batch  471 / 525  Training Loss  0.014291338622570038\n",
            "Epoch  17 Batch  472 / 525  Training Loss  0.005869312211871147\n",
            "Epoch  17 Batch  473 / 525  Training Loss  0.005533379502594471\n",
            "Epoch  17 Batch  474 / 525  Training Loss  0.00534015940502286\n",
            "Epoch  17 Batch  475 / 525  Training Loss  0.008716041222214699\n",
            "Epoch  17 Batch  476 / 525  Training Loss  0.011749720200896263\n",
            "Epoch  17 Batch  477 / 525  Training Loss  0.010875644162297249\n",
            "Epoch  17 Batch  478 / 525  Training Loss  0.012770384550094604\n",
            "Epoch  17 Batch  479 / 525  Training Loss  0.008945145644247532\n",
            "Epoch  17 Batch  480 / 525  Training Loss  0.009138587862253189\n",
            "Epoch  17 Batch  481 / 525  Training Loss  0.006486999802291393\n",
            "Epoch  17 Batch  482 / 525  Training Loss  0.007030507083982229\n",
            "Epoch  17 Batch  483 / 525  Training Loss  0.008385324850678444\n",
            "Epoch  17 Batch  484 / 525  Training Loss  0.01608116924762726\n",
            "Epoch  17 Batch  485 / 525  Training Loss  0.01535506546497345\n",
            "Epoch  17 Batch  486 / 525  Training Loss  0.007783735636621714\n",
            "Epoch  17 Batch  487 / 525  Training Loss  0.007778736297041178\n",
            "Epoch  17 Batch  488 / 525  Training Loss  0.011923535726964474\n",
            "Epoch  17 Batch  489 / 525  Training Loss  0.013187184929847717\n",
            "Epoch  17 Batch  490 / 525  Training Loss  0.004241612274199724\n",
            "Epoch  17 Batch  491 / 525  Training Loss  0.010066790506243706\n",
            "Epoch  17 Batch  492 / 525  Training Loss  0.00703651225194335\n",
            "Epoch  17 Batch  493 / 525  Training Loss  0.01809815689921379\n",
            "Epoch  17 Batch  494 / 525  Training Loss  0.009147007949650288\n",
            "Epoch  17 Batch  495 / 525  Training Loss  0.01935146562755108\n",
            "Epoch  17 Batch  496 / 525  Training Loss  0.014473426155745983\n",
            "Epoch  17 Batch  497 / 525  Training Loss  0.005033839028328657\n",
            "Epoch  17 Batch  498 / 525  Training Loss  0.015166612342000008\n",
            "Epoch  17 Batch  499 / 525  Training Loss  0.016747627407312393\n",
            "Epoch  17 Batch  500 / 525  Training Loss  0.009890454821288586\n",
            "Epoch  17 Batch  501 / 525  Training Loss  0.009318448603153229\n",
            "Epoch  17 Batch  502 / 525  Training Loss  0.008446979336440563\n",
            "Epoch  17 Batch  503 / 525  Training Loss  0.006383523344993591\n",
            "Epoch  17 Batch  504 / 525  Training Loss  0.011914514005184174\n",
            "Epoch  17 Batch  505 / 525  Training Loss  0.008312136866152287\n",
            "Epoch  17 Batch  506 / 525  Training Loss  0.010039012879133224\n",
            "Epoch  17 Batch  507 / 525  Training Loss  0.014500349760055542\n",
            "Epoch  17 Batch  508 / 525  Training Loss  0.015879366546869278\n",
            "Epoch  17 Batch  509 / 525  Training Loss  0.01343661081045866\n",
            "Epoch  17 Batch  510 / 525  Training Loss  0.01443113386631012\n",
            "Epoch  17 Batch  511 / 525  Training Loss  0.015946947038173676\n",
            "Epoch  17 Batch  512 / 525  Training Loss  0.008362038061022758\n",
            "Epoch  17 Batch  513 / 525  Training Loss  0.012917647138237953\n",
            "Epoch  17 Batch  514 / 525  Training Loss  0.005871572531759739\n",
            "Epoch  17 Batch  515 / 525  Training Loss  0.006973152048885822\n",
            "Epoch  17 Batch  516 / 525  Training Loss  0.010697124525904655\n",
            "Epoch  17 Batch  517 / 525  Training Loss  0.005644018761813641\n",
            "Epoch  17 Batch  518 / 525  Training Loss  0.020525207743048668\n",
            "Epoch  17 Batch  519 / 525  Training Loss  0.013806156814098358\n",
            "Epoch  17 Batch  520 / 525  Training Loss  0.010596723295748234\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  17 Batch  521 / 525  Training Loss  0.007104522082954645\n",
            "Epoch  17 Batch  522 / 525  Training Loss  0.009809020906686783\n",
            "Epoch  17 Batch  523 / 525  Training Loss  0.01061384566128254\n",
            "Epoch  17 Batch  524 / 525  Training Loss  0.0068515511229634285\n",
            "  18    |    -    |   0.010049   | 58.633333\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 18\n",
            "Epoch  18 Batch  0 / 525  Training Loss  0.001728432485833764\n",
            "Epoch  18 Batch  1 / 525  Training Loss  0.0024853674694895744\n",
            "Epoch  18 Batch  2 / 525  Training Loss  0.006737914867699146\n",
            "Epoch  18 Batch  3 / 525  Training Loss  0.007601728197187185\n",
            "Epoch  18 Batch  4 / 525  Training Loss  0.0082265455275774\n",
            "Epoch  18 Batch  5 / 525  Training Loss  0.0032767518423497677\n",
            "Epoch  18 Batch  6 / 525  Training Loss  0.004650076851248741\n",
            "Epoch  18 Batch  7 / 525  Training Loss  0.01238285843282938\n",
            "Epoch  18 Batch  8 / 525  Training Loss  0.006559205707162619\n",
            "Epoch  18 Batch  9 / 525  Training Loss  0.005306788720190525\n",
            "Epoch  18 Batch  10 / 525  Training Loss  0.00871672760695219\n",
            "Epoch  18 Batch  11 / 525  Training Loss  0.004823689814656973\n",
            "Epoch  18 Batch  12 / 525  Training Loss  0.007698545698076487\n",
            "Epoch  18 Batch  13 / 525  Training Loss  0.00579255074262619\n",
            "Epoch  18 Batch  14 / 525  Training Loss  0.004210790619254112\n",
            "Epoch  18 Batch  15 / 525  Training Loss  0.005402120295912027\n",
            "Epoch  18 Batch  16 / 525  Training Loss  0.010886294767260551\n",
            "Epoch  18 Batch  17 / 525  Training Loss  0.005854901857674122\n",
            "Epoch  18 Batch  18 / 525  Training Loss  0.020117120817303658\n",
            "Epoch  18 Batch  19 / 525  Training Loss  0.021766141057014465\n",
            "Epoch  18 Batch  20 / 525  Training Loss  0.008467728272080421\n",
            "Epoch  18 Batch  21 / 525  Training Loss  0.01009906455874443\n",
            "Epoch  18 Batch  22 / 525  Training Loss  0.010703066363930702\n",
            "Epoch  18 Batch  23 / 525  Training Loss  0.003754641395062208\n",
            "Epoch  18 Batch  24 / 525  Training Loss  0.008883176371455193\n",
            "Epoch  18 Batch  25 / 525  Training Loss  0.004064420238137245\n",
            "Epoch  18 Batch  26 / 525  Training Loss  0.011321237310767174\n",
            "Epoch  18 Batch  27 / 525  Training Loss  0.0038652829825878143\n",
            "Epoch  18 Batch  28 / 525  Training Loss  0.003647299949079752\n",
            "Epoch  18 Batch  29 / 525  Training Loss  0.016228964552283287\n",
            "Epoch  18 Batch  30 / 525  Training Loss  0.0026693283580243587\n",
            "Epoch  18 Batch  31 / 525  Training Loss  0.004965420812368393\n",
            "Epoch  18 Batch  32 / 525  Training Loss  0.01525585912168026\n",
            "Epoch  18 Batch  33 / 525  Training Loss  0.006445030216127634\n",
            "Epoch  18 Batch  34 / 525  Training Loss  0.007816322147846222\n",
            "Epoch  18 Batch  35 / 525  Training Loss  0.013046815991401672\n",
            "Epoch  18 Batch  36 / 525  Training Loss  0.0021761315874755383\n",
            "Epoch  18 Batch  37 / 525  Training Loss  0.003803455037996173\n",
            "Epoch  18 Batch  38 / 525  Training Loss  0.0064393081702291965\n",
            "Epoch  18 Batch  39 / 525  Training Loss  0.00773391779512167\n",
            "Epoch  18 Batch  40 / 525  Training Loss  0.008018587715923786\n",
            "Epoch  18 Batch  41 / 525  Training Loss  0.003409555647522211\n",
            "Epoch  18 Batch  42 / 525  Training Loss  0.005301588214933872\n",
            "Epoch  18 Batch  43 / 525  Training Loss  0.004771356470882893\n",
            "Epoch  18 Batch  44 / 525  Training Loss  0.00587352504953742\n",
            "Epoch  18 Batch  45 / 525  Training Loss  0.0050905244424939156\n",
            "Epoch  18 Batch  46 / 525  Training Loss  0.009662638418376446\n",
            "Epoch  18 Batch  47 / 525  Training Loss  0.006045404355973005\n",
            "Epoch  18 Batch  48 / 525  Training Loss  0.002907004440203309\n",
            "Epoch  18 Batch  49 / 525  Training Loss  0.004138353280723095\n",
            "Epoch  18 Batch  50 / 525  Training Loss  0.0077975750900805\n",
            "Epoch  18 Batch  51 / 525  Training Loss  0.008634781464934349\n",
            "Epoch  18 Batch  52 / 525  Training Loss  0.0018736403435468674\n",
            "Epoch  18 Batch  53 / 525  Training Loss  0.0037195507902652025\n",
            "Epoch  18 Batch  54 / 525  Training Loss  0.005720390006899834\n",
            "Epoch  18 Batch  55 / 525  Training Loss  0.00943942554295063\n",
            "Epoch  18 Batch  56 / 525  Training Loss  0.006576894782483578\n",
            "Epoch  18 Batch  57 / 525  Training Loss  0.00709883775562048\n",
            "Epoch  18 Batch  58 / 525  Training Loss  0.010046396404504776\n",
            "Epoch  18 Batch  59 / 525  Training Loss  0.0069602616131305695\n",
            "Epoch  18 Batch  60 / 525  Training Loss  0.004429535008966923\n",
            "Epoch  18 Batch  61 / 525  Training Loss  0.007993923500180244\n",
            "Epoch  18 Batch  62 / 525  Training Loss  0.0016783755272626877\n",
            "Epoch  18 Batch  63 / 525  Training Loss  0.007428708020597696\n",
            "Epoch  18 Batch  64 / 525  Training Loss  0.0032190761994570494\n",
            "Epoch  18 Batch  65 / 525  Training Loss  0.0024368998128920794\n",
            "Epoch  18 Batch  66 / 525  Training Loss  0.006225650664418936\n",
            "Epoch  18 Batch  67 / 525  Training Loss  0.01021572109311819\n",
            "Epoch  18 Batch  68 / 525  Training Loss  0.0047887577675282955\n",
            "Epoch  18 Batch  69 / 525  Training Loss  0.00562474736943841\n",
            "Epoch  18 Batch  70 / 525  Training Loss  0.005293603055179119\n",
            "Epoch  18 Batch  71 / 525  Training Loss  0.004406369756907225\n",
            "Epoch  18 Batch  72 / 525  Training Loss  0.005289716180413961\n",
            "Epoch  18 Batch  73 / 525  Training Loss  0.005368643905967474\n",
            "Epoch  18 Batch  74 / 525  Training Loss  0.004388351924717426\n",
            "Epoch  18 Batch  75 / 525  Training Loss  0.002490751910954714\n",
            "Epoch  18 Batch  76 / 525  Training Loss  0.004913858138024807\n",
            "Epoch  18 Batch  77 / 525  Training Loss  0.009242339059710503\n",
            "Epoch  18 Batch  78 / 525  Training Loss  0.00744247529655695\n",
            "Epoch  18 Batch  79 / 525  Training Loss  0.0072633447125554085\n",
            "Epoch  18 Batch  80 / 525  Training Loss  0.0069419643841683865\n",
            "Epoch  18 Batch  81 / 525  Training Loss  0.00529917236417532\n",
            "Epoch  18 Batch  82 / 525  Training Loss  0.0044006058014929295\n",
            "Epoch  18 Batch  83 / 525  Training Loss  0.008788436651229858\n",
            "Epoch  18 Batch  84 / 525  Training Loss  0.00739135080948472\n",
            "Epoch  18 Batch  85 / 525  Training Loss  0.004745329264551401\n",
            "Epoch  18 Batch  86 / 525  Training Loss  0.007204816676676273\n",
            "Epoch  18 Batch  87 / 525  Training Loss  0.0036546173505485058\n",
            "Epoch  18 Batch  88 / 525  Training Loss  0.006912646349519491\n",
            "Epoch  18 Batch  89 / 525  Training Loss  0.005040191579610109\n",
            "Epoch  18 Batch  90 / 525  Training Loss  0.007857483811676502\n",
            "Epoch  18 Batch  91 / 525  Training Loss  0.0025527640245854855\n",
            "Epoch  18 Batch  92 / 525  Training Loss  0.0046533457934856415\n",
            "Epoch  18 Batch  93 / 525  Training Loss  0.005684013944119215\n",
            "Epoch  18 Batch  94 / 525  Training Loss  0.004949185065925121\n",
            "Epoch  18 Batch  95 / 525  Training Loss  0.005260656587779522\n",
            "Epoch  18 Batch  96 / 525  Training Loss  0.0018225476378574967\n",
            "Epoch  18 Batch  97 / 525  Training Loss  0.006475470960140228\n",
            "Epoch  18 Batch  98 / 525  Training Loss  0.005931210238486528\n",
            "Epoch  18 Batch  99 / 525  Training Loss  0.005363821052014828\n",
            "Epoch  18 Batch  100 / 525  Training Loss  0.008579706773161888\n",
            "Epoch  18 Batch  101 / 525  Training Loss  0.002888964954763651\n",
            "Epoch  18 Batch  102 / 525  Training Loss  0.008599798195064068\n",
            "Epoch  18 Batch  103 / 525  Training Loss  0.004334391560405493\n",
            "Epoch  18 Batch  104 / 525  Training Loss  0.004649532027542591\n",
            "Epoch  18 Batch  105 / 525  Training Loss  0.0021159911993891\n",
            "Epoch  18 Batch  106 / 525  Training Loss  0.009171480312943459\n",
            "Epoch  18 Batch  107 / 525  Training Loss  0.012076991610229015\n",
            "Epoch  18 Batch  108 / 525  Training Loss  0.0038908265996724367\n",
            "Epoch  18 Batch  109 / 525  Training Loss  0.0017365457024425268\n",
            "Epoch  18 Batch  110 / 525  Training Loss  0.004101064521819353\n",
            "Epoch  18 Batch  111 / 525  Training Loss  0.010432888753712177\n",
            "Epoch  18 Batch  112 / 525  Training Loss  0.00399835966527462\n",
            "Epoch  18 Batch  113 / 525  Training Loss  0.004267789889127016\n",
            "Epoch  18 Batch  114 / 525  Training Loss  0.007719042710959911\n",
            "Epoch  18 Batch  115 / 525  Training Loss  0.010305114090442657\n",
            "Epoch  18 Batch  116 / 525  Training Loss  0.003976547159254551\n",
            "Epoch  18 Batch  117 / 525  Training Loss  0.004263770766556263\n",
            "Epoch  18 Batch  118 / 525  Training Loss  0.0017953921342268586\n",
            "Epoch  18 Batch  119 / 525  Training Loss  0.007309259381145239\n",
            "Epoch  18 Batch  120 / 525  Training Loss  0.005828986410051584\n",
            "Epoch  18 Batch  121 / 525  Training Loss  0.01361403800547123\n",
            "Epoch  18 Batch  122 / 525  Training Loss  0.004360222723335028\n",
            "Epoch  18 Batch  123 / 525  Training Loss  0.005200671963393688\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  18 Batch  124 / 525  Training Loss  0.005499250255525112\n",
            "Epoch  18 Batch  125 / 525  Training Loss  0.01254189945757389\n",
            "Epoch  18 Batch  126 / 525  Training Loss  0.005746920593082905\n",
            "Epoch  18 Batch  127 / 525  Training Loss  0.007586188614368439\n",
            "Epoch  18 Batch  128 / 525  Training Loss  0.007518504746258259\n",
            "Epoch  18 Batch  129 / 525  Training Loss  0.007265241350978613\n",
            "Epoch  18 Batch  130 / 525  Training Loss  0.005629928316920996\n",
            "Epoch  18 Batch  131 / 525  Training Loss  0.02485789731144905\n",
            "Epoch  18 Batch  132 / 525  Training Loss  0.006586037576198578\n",
            "Epoch  18 Batch  133 / 525  Training Loss  0.002207244513556361\n",
            "Epoch  18 Batch  134 / 525  Training Loss  0.007943231612443924\n",
            "Epoch  18 Batch  135 / 525  Training Loss  0.003788057714700699\n",
            "Epoch  18 Batch  136 / 525  Training Loss  0.0037418834399431944\n",
            "Epoch  18 Batch  137 / 525  Training Loss  0.003943084739148617\n",
            "Epoch  18 Batch  138 / 525  Training Loss  0.00749459583312273\n",
            "Epoch  18 Batch  139 / 525  Training Loss  0.0033730915747582912\n",
            "Epoch  18 Batch  140 / 525  Training Loss  0.0028347275219857693\n",
            "Epoch  18 Batch  141 / 525  Training Loss  0.005302756559103727\n",
            "Epoch  18 Batch  142 / 525  Training Loss  0.0051467097364366055\n",
            "Epoch  18 Batch  143 / 525  Training Loss  0.0019417727598920465\n",
            "Epoch  18 Batch  144 / 525  Training Loss  0.007393574807792902\n",
            "Epoch  18 Batch  145 / 525  Training Loss  0.009050458669662476\n",
            "Epoch  18 Batch  146 / 525  Training Loss  0.0045599243603646755\n",
            "Epoch  18 Batch  147 / 525  Training Loss  0.004083889536559582\n",
            "Epoch  18 Batch  148 / 525  Training Loss  0.006060493644326925\n",
            "Epoch  18 Batch  149 / 525  Training Loss  0.009154858067631721\n",
            "Epoch  18 Batch  150 / 525  Training Loss  0.00554901547729969\n",
            "Epoch  18 Batch  151 / 525  Training Loss  0.004605643916875124\n",
            "Epoch  18 Batch  152 / 525  Training Loss  0.0033720526844263077\n",
            "Epoch  18 Batch  153 / 525  Training Loss  0.004338535945862532\n",
            "Epoch  18 Batch  154 / 525  Training Loss  0.0031806272454559803\n",
            "Epoch  18 Batch  155 / 525  Training Loss  0.005254040472209454\n",
            "Epoch  18 Batch  156 / 525  Training Loss  0.010799920186400414\n",
            "Epoch  18 Batch  157 / 525  Training Loss  0.003504245774820447\n",
            "Epoch  18 Batch  158 / 525  Training Loss  0.014103716239333153\n",
            "Epoch  18 Batch  159 / 525  Training Loss  0.002713955007493496\n",
            "Epoch  18 Batch  160 / 525  Training Loss  0.0035521097015589476\n",
            "Epoch  18 Batch  161 / 525  Training Loss  0.0043162014335393906\n",
            "Epoch  18 Batch  162 / 525  Training Loss  0.003486682428047061\n",
            "Epoch  18 Batch  163 / 525  Training Loss  0.005690381862223148\n",
            "Epoch  18 Batch  164 / 525  Training Loss  0.005248106084764004\n",
            "Epoch  18 Batch  165 / 525  Training Loss  0.004421532154083252\n",
            "Epoch  18 Batch  166 / 525  Training Loss  0.0031647372525185347\n",
            "Epoch  18 Batch  167 / 525  Training Loss  0.004181896802037954\n",
            "Epoch  18 Batch  168 / 525  Training Loss  0.006758827716112137\n",
            "Epoch  18 Batch  169 / 525  Training Loss  0.009845963679254055\n",
            "Epoch  18 Batch  170 / 525  Training Loss  0.005074700340628624\n",
            "Epoch  18 Batch  171 / 525  Training Loss  0.005887586623430252\n",
            "Epoch  18 Batch  172 / 525  Training Loss  0.003018514020368457\n",
            "Epoch  18 Batch  173 / 525  Training Loss  0.0049921199679374695\n",
            "Epoch  18 Batch  174 / 525  Training Loss  0.005499421618878841\n",
            "Epoch  18 Batch  175 / 525  Training Loss  0.004048462025821209\n",
            "Epoch  18 Batch  176 / 525  Training Loss  0.0041051143780350685\n",
            "Epoch  18 Batch  177 / 525  Training Loss  0.007557627744972706\n",
            "Epoch  18 Batch  178 / 525  Training Loss  0.005721869878470898\n",
            "Epoch  18 Batch  179 / 525  Training Loss  0.006480872631072998\n",
            "Epoch  18 Batch  180 / 525  Training Loss  0.004918665159493685\n",
            "Epoch  18 Batch  181 / 525  Training Loss  0.008478168398141861\n",
            "Epoch  18 Batch  182 / 525  Training Loss  0.004234012216329575\n",
            "Epoch  18 Batch  183 / 525  Training Loss  0.011482341215014458\n",
            "Epoch  18 Batch  184 / 525  Training Loss  0.014018339104950428\n",
            "Epoch  18 Batch  185 / 525  Training Loss  0.00504303677007556\n",
            "Epoch  18 Batch  186 / 525  Training Loss  0.007139356341212988\n",
            "Epoch  18 Batch  187 / 525  Training Loss  0.01072258036583662\n",
            "Epoch  18 Batch  188 / 525  Training Loss  0.006862147245556116\n",
            "Epoch  18 Batch  189 / 525  Training Loss  0.004260609392076731\n",
            "Epoch  18 Batch  190 / 525  Training Loss  0.009203938767313957\n",
            "Epoch  18 Batch  191 / 525  Training Loss  0.013274826109409332\n",
            "Epoch  18 Batch  192 / 525  Training Loss  0.00489672739058733\n",
            "Epoch  18 Batch  193 / 525  Training Loss  0.005910849664360285\n",
            "Epoch  18 Batch  194 / 525  Training Loss  0.02215711586177349\n",
            "Epoch  18 Batch  195 / 525  Training Loss  0.006249700672924519\n",
            "Epoch  18 Batch  196 / 525  Training Loss  0.00538885360583663\n",
            "Epoch  18 Batch  197 / 525  Training Loss  0.01173847820609808\n",
            "Epoch  18 Batch  198 / 525  Training Loss  0.005018940661102533\n",
            "Epoch  18 Batch  199 / 525  Training Loss  0.010103648528456688\n",
            "Epoch  18 Batch  200 / 525  Training Loss  0.009588555432856083\n",
            "Epoch  18 Batch  201 / 525  Training Loss  0.00618025753647089\n",
            "Epoch  18 Batch  202 / 525  Training Loss  0.005097676534205675\n",
            "Epoch  18 Batch  203 / 525  Training Loss  0.006044648587703705\n",
            "Epoch  18 Batch  204 / 525  Training Loss  0.0034695733338594437\n",
            "Epoch  18 Batch  205 / 525  Training Loss  0.0012553518172353506\n",
            "Epoch  18 Batch  206 / 525  Training Loss  0.004258658271282911\n",
            "Epoch  18 Batch  207 / 525  Training Loss  0.004494500812143087\n",
            "Epoch  18 Batch  208 / 525  Training Loss  0.0024856519885361195\n",
            "Epoch  18 Batch  209 / 525  Training Loss  0.005776678211987019\n",
            "Epoch  18 Batch  210 / 525  Training Loss  0.006562316324561834\n",
            "Epoch  18 Batch  211 / 525  Training Loss  0.0016958927735686302\n",
            "Epoch  18 Batch  212 / 525  Training Loss  0.002041851868852973\n",
            "Epoch  18 Batch  213 / 525  Training Loss  0.0025179055519402027\n",
            "Epoch  18 Batch  214 / 525  Training Loss  0.011720429174602032\n",
            "Epoch  18 Batch  215 / 525  Training Loss  0.004478774033486843\n",
            "Epoch  18 Batch  216 / 525  Training Loss  0.009759720414876938\n",
            "Epoch  18 Batch  217 / 525  Training Loss  0.011003765277564526\n",
            "Epoch  18 Batch  218 / 525  Training Loss  0.006390614900738001\n",
            "Epoch  18 Batch  219 / 525  Training Loss  0.002944407518953085\n",
            "Epoch  18 Batch  220 / 525  Training Loss  0.009591459296643734\n",
            "Epoch  18 Batch  221 / 525  Training Loss  0.006469905376434326\n",
            "Epoch  18 Batch  222 / 525  Training Loss  0.0038069081492722034\n",
            "Epoch  18 Batch  223 / 525  Training Loss  0.005027831997722387\n",
            "Epoch  18 Batch  224 / 525  Training Loss  0.014176389202475548\n",
            "Epoch  18 Batch  225 / 525  Training Loss  0.004220101051032543\n",
            "Epoch  18 Batch  226 / 525  Training Loss  0.007770547177642584\n",
            "Epoch  18 Batch  227 / 525  Training Loss  0.004491084720939398\n",
            "Epoch  18 Batch  228 / 525  Training Loss  0.008422253653407097\n",
            "Epoch  18 Batch  229 / 525  Training Loss  0.008392821066081524\n",
            "Epoch  18 Batch  230 / 525  Training Loss  0.0032694037072360516\n",
            "Epoch  18 Batch  231 / 525  Training Loss  0.008781882002949715\n",
            "Epoch  18 Batch  232 / 525  Training Loss  0.00393760297447443\n",
            "Epoch  18 Batch  233 / 525  Training Loss  0.00660549383610487\n",
            "Epoch  18 Batch  234 / 525  Training Loss  0.0032559882383793592\n",
            "Epoch  18 Batch  235 / 525  Training Loss  0.009203070774674416\n",
            "Epoch  18 Batch  236 / 525  Training Loss  0.011365557089447975\n",
            "Epoch  18 Batch  237 / 525  Training Loss  0.005209175404161215\n",
            "Epoch  18 Batch  238 / 525  Training Loss  0.004810805898159742\n",
            "Epoch  18 Batch  239 / 525  Training Loss  0.005749498959630728\n",
            "Epoch  18 Batch  240 / 525  Training Loss  0.00954101700335741\n",
            "Epoch  18 Batch  241 / 525  Training Loss  0.0031263078562915325\n",
            "Epoch  18 Batch  242 / 525  Training Loss  0.005993206519633532\n",
            "Epoch  18 Batch  243 / 525  Training Loss  0.00555761530995369\n",
            "Epoch  18 Batch  244 / 525  Training Loss  0.006603647954761982\n",
            "Epoch  18 Batch  245 / 525  Training Loss  0.012028452008962631\n",
            "Epoch  18 Batch  246 / 525  Training Loss  0.006104250438511372\n",
            "Epoch  18 Batch  247 / 525  Training Loss  0.013621270656585693\n",
            "Epoch  18 Batch  248 / 525  Training Loss  0.007100907154381275\n",
            "Epoch  18 Batch  249 / 525  Training Loss  0.005889797583222389\n",
            "Epoch  18 Batch  250 / 525  Training Loss  0.004090491216629744\n",
            "Epoch  18 Batch  251 / 525  Training Loss  0.005496828816831112\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  18 Batch  252 / 525  Training Loss  0.005612843669950962\n",
            "Epoch  18 Batch  253 / 525  Training Loss  0.010861506685614586\n",
            "Epoch  18 Batch  254 / 525  Training Loss  0.015032718889415264\n",
            "Epoch  18 Batch  255 / 525  Training Loss  0.0048525696620345116\n",
            "Epoch  18 Batch  256 / 525  Training Loss  0.004287889692932367\n",
            "Epoch  18 Batch  257 / 525  Training Loss  0.006692830473184586\n",
            "Epoch  18 Batch  258 / 525  Training Loss  0.004069031216204166\n",
            "Epoch  18 Batch  259 / 525  Training Loss  0.005966675467789173\n",
            "Epoch  18 Batch  260 / 525  Training Loss  0.006349280476570129\n",
            "Epoch  18 Batch  261 / 525  Training Loss  0.004108370281755924\n",
            "Epoch  18 Batch  262 / 525  Training Loss  0.005911735352128744\n",
            "Epoch  18 Batch  263 / 525  Training Loss  0.0034027956426143646\n",
            "Epoch  18 Batch  264 / 525  Training Loss  0.007120901253074408\n",
            "Epoch  18 Batch  265 / 525  Training Loss  0.003347675083205104\n",
            "Epoch  18 Batch  266 / 525  Training Loss  0.007542776875197887\n",
            "Epoch  18 Batch  267 / 525  Training Loss  0.0034452788531780243\n",
            "Epoch  18 Batch  268 / 525  Training Loss  0.0050552585162222385\n",
            "Epoch  18 Batch  269 / 525  Training Loss  0.01397341676056385\n",
            "Epoch  18 Batch  270 / 525  Training Loss  0.0018323229160159826\n",
            "Epoch  18 Batch  271 / 525  Training Loss  0.0030013707000762224\n",
            "Epoch  18 Batch  272 / 525  Training Loss  0.005118416156619787\n",
            "Epoch  18 Batch  273 / 525  Training Loss  0.002906264504417777\n",
            "Epoch  18 Batch  274 / 525  Training Loss  0.009603805840015411\n",
            "Epoch  18 Batch  275 / 525  Training Loss  0.008640839718282223\n",
            "Epoch  18 Batch  276 / 525  Training Loss  0.0062695154920220375\n",
            "Epoch  18 Batch  277 / 525  Training Loss  0.011619588360190392\n",
            "Epoch  18 Batch  278 / 525  Training Loss  0.005111696198582649\n",
            "Epoch  18 Batch  279 / 525  Training Loss  0.007515156175941229\n",
            "Epoch  18 Batch  280 / 525  Training Loss  0.007296551950275898\n",
            "Epoch  18 Batch  281 / 525  Training Loss  0.006316156592220068\n",
            "Epoch  18 Batch  282 / 525  Training Loss  0.010840345174074173\n",
            "Epoch  18 Batch  283 / 525  Training Loss  0.0046405796892941\n",
            "Epoch  18 Batch  284 / 525  Training Loss  0.00265784515067935\n",
            "Epoch  18 Batch  285 / 525  Training Loss  0.011967645958065987\n",
            "Epoch  18 Batch  286 / 525  Training Loss  0.004282849840819836\n",
            "Epoch  18 Batch  287 / 525  Training Loss  0.019323188811540604\n",
            "Epoch  18 Batch  288 / 525  Training Loss  0.004744329955428839\n",
            "Epoch  18 Batch  289 / 525  Training Loss  0.01830451190471649\n",
            "Epoch  18 Batch  290 / 525  Training Loss  0.004182972013950348\n",
            "Epoch  18 Batch  291 / 525  Training Loss  0.00525157293304801\n",
            "Epoch  18 Batch  292 / 525  Training Loss  0.007710396312177181\n",
            "Epoch  18 Batch  293 / 525  Training Loss  0.006296674255281687\n",
            "Epoch  18 Batch  294 / 525  Training Loss  0.005732080899178982\n",
            "Epoch  18 Batch  295 / 525  Training Loss  0.012900421395897865\n",
            "Epoch  18 Batch  296 / 525  Training Loss  0.005929489620029926\n",
            "Epoch  18 Batch  297 / 525  Training Loss  0.013854434713721275\n",
            "Epoch  18 Batch  298 / 525  Training Loss  0.00376107357442379\n",
            "Epoch  18 Batch  299 / 525  Training Loss  0.007709482219070196\n",
            "Epoch  18 Batch  300 / 525  Training Loss  0.004846491850912571\n",
            "Epoch  18 Batch  301 / 525  Training Loss  0.006299552507698536\n",
            "Epoch  18 Batch  302 / 525  Training Loss  0.0034091647248715162\n",
            "Epoch  18 Batch  303 / 525  Training Loss  0.006830727215856314\n",
            "Epoch  18 Batch  304 / 525  Training Loss  0.007159012369811535\n",
            "Epoch  18 Batch  305 / 525  Training Loss  0.008790615014731884\n",
            "Epoch  18 Batch  306 / 525  Training Loss  0.003800997044891119\n",
            "Epoch  18 Batch  307 / 525  Training Loss  0.003284379141405225\n",
            "Epoch  18 Batch  308 / 525  Training Loss  0.0069832755252718925\n",
            "Epoch  18 Batch  309 / 525  Training Loss  0.014540405943989754\n",
            "Epoch  18 Batch  310 / 525  Training Loss  0.004959479905664921\n",
            "Epoch  18 Batch  311 / 525  Training Loss  0.007307254709303379\n",
            "Epoch  18 Batch  312 / 525  Training Loss  0.004466149024665356\n",
            "Epoch  18 Batch  313 / 525  Training Loss  0.008012596517801285\n",
            "Epoch  18 Batch  314 / 525  Training Loss  0.006288912147283554\n",
            "Epoch  18 Batch  315 / 525  Training Loss  0.010039528831839561\n",
            "Epoch  18 Batch  316 / 525  Training Loss  0.004957901779562235\n",
            "Epoch  18 Batch  317 / 525  Training Loss  0.004646011628210545\n",
            "Epoch  18 Batch  318 / 525  Training Loss  0.004833778832107782\n",
            "Epoch  18 Batch  319 / 525  Training Loss  0.005013725720345974\n",
            "Epoch  18 Batch  320 / 525  Training Loss  0.005815853364765644\n",
            "Epoch  18 Batch  321 / 525  Training Loss  0.003547538770362735\n",
            "Epoch  18 Batch  322 / 525  Training Loss  0.003943697549402714\n",
            "Epoch  18 Batch  323 / 525  Training Loss  0.008585031144320965\n",
            "Epoch  18 Batch  324 / 525  Training Loss  0.005916711874306202\n",
            "Epoch  18 Batch  325 / 525  Training Loss  0.007934440858662128\n",
            "Epoch  18 Batch  326 / 525  Training Loss  0.009316859766840935\n",
            "Epoch  18 Batch  327 / 525  Training Loss  0.010747261345386505\n",
            "Epoch  18 Batch  328 / 525  Training Loss  0.004296010360121727\n",
            "Epoch  18 Batch  329 / 525  Training Loss  0.0087770139798522\n",
            "Epoch  18 Batch  330 / 525  Training Loss  0.007053539156913757\n",
            "Epoch  18 Batch  331 / 525  Training Loss  0.005532420240342617\n",
            "Epoch  18 Batch  332 / 525  Training Loss  0.01397775113582611\n",
            "Epoch  18 Batch  333 / 525  Training Loss  0.0025817682035267353\n",
            "Epoch  18 Batch  334 / 525  Training Loss  0.0033360663801431656\n",
            "Epoch  18 Batch  335 / 525  Training Loss  0.0052864127792418\n",
            "Epoch  18 Batch  336 / 525  Training Loss  0.005770836491137743\n",
            "Epoch  18 Batch  337 / 525  Training Loss  0.00651448592543602\n",
            "Epoch  18 Batch  338 / 525  Training Loss  0.003466567490249872\n",
            "Epoch  18 Batch  339 / 525  Training Loss  0.00407378189265728\n",
            "Epoch  18 Batch  340 / 525  Training Loss  0.009032709524035454\n",
            "Epoch  18 Batch  341 / 525  Training Loss  0.00658117001876235\n",
            "Epoch  18 Batch  342 / 525  Training Loss  0.005549429915845394\n",
            "Epoch  18 Batch  343 / 525  Training Loss  0.00847136601805687\n",
            "Epoch  18 Batch  344 / 525  Training Loss  0.0016754036769270897\n",
            "Epoch  18 Batch  345 / 525  Training Loss  0.011747888289391994\n",
            "Epoch  18 Batch  346 / 525  Training Loss  0.0036411595065146685\n",
            "Epoch  18 Batch  347 / 525  Training Loss  0.004925708752125502\n",
            "Epoch  18 Batch  348 / 525  Training Loss  0.009163142181932926\n",
            "Epoch  18 Batch  349 / 525  Training Loss  0.00478674191981554\n",
            "Epoch  18 Batch  350 / 525  Training Loss  0.004908873233944178\n",
            "Epoch  18 Batch  351 / 525  Training Loss  0.005308124702423811\n",
            "Epoch  18 Batch  352 / 525  Training Loss  0.007811689283698797\n",
            "Epoch  18 Batch  353 / 525  Training Loss  0.007607961539179087\n",
            "Epoch  18 Batch  354 / 525  Training Loss  0.005896980408579111\n",
            "Epoch  18 Batch  355 / 525  Training Loss  0.0049578421749174595\n",
            "Epoch  18 Batch  356 / 525  Training Loss  0.003495600773021579\n",
            "Epoch  18 Batch  357 / 525  Training Loss  0.006043888628482819\n",
            "Epoch  18 Batch  358 / 525  Training Loss  0.0026660640724003315\n",
            "Epoch  18 Batch  359 / 525  Training Loss  0.002665990963578224\n",
            "Epoch  18 Batch  360 / 525  Training Loss  0.005161564331501722\n",
            "Epoch  18 Batch  361 / 525  Training Loss  0.0032948229927569628\n",
            "Epoch  18 Batch  362 / 525  Training Loss  0.0049407207407057285\n",
            "Epoch  18 Batch  363 / 525  Training Loss  0.004415503237396479\n",
            "Epoch  18 Batch  364 / 525  Training Loss  0.006847165524959564\n",
            "Epoch  18 Batch  365 / 525  Training Loss  0.006820896174758673\n",
            "Epoch  18 Batch  366 / 525  Training Loss  0.0040318286046385765\n",
            "Epoch  18 Batch  367 / 525  Training Loss  0.0033228695392608643\n",
            "Epoch  18 Batch  368 / 525  Training Loss  0.004210018552839756\n",
            "Epoch  18 Batch  369 / 525  Training Loss  0.004352647345513105\n",
            "Epoch  18 Batch  370 / 525  Training Loss  0.008151274174451828\n",
            "Epoch  18 Batch  371 / 525  Training Loss  0.016288524493575096\n",
            "Epoch  18 Batch  372 / 525  Training Loss  0.006184843368828297\n",
            "Epoch  18 Batch  373 / 525  Training Loss  0.002683828817680478\n",
            "Epoch  18 Batch  374 / 525  Training Loss  0.008965114131569862\n",
            "Epoch  18 Batch  375 / 525  Training Loss  0.009768233634531498\n",
            "Epoch  18 Batch  376 / 525  Training Loss  0.005379404406994581\n",
            "Epoch  18 Batch  377 / 525  Training Loss  0.006865018513053656\n",
            "Epoch  18 Batch  378 / 525  Training Loss  0.009351586923003197\n",
            "Epoch  18 Batch  379 / 525  Training Loss  0.0033946309704333544\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  18 Batch  380 / 525  Training Loss  0.008123415522277355\n",
            "Epoch  18 Batch  381 / 525  Training Loss  0.00187845213804394\n",
            "Epoch  18 Batch  382 / 525  Training Loss  0.002518725348636508\n",
            "Epoch  18 Batch  383 / 525  Training Loss  0.011247559450566769\n",
            "Epoch  18 Batch  384 / 525  Training Loss  0.0070153893902897835\n",
            "Epoch  18 Batch  385 / 525  Training Loss  0.010376552119851112\n",
            "Epoch  18 Batch  386 / 525  Training Loss  0.00602306704968214\n",
            "Epoch  18 Batch  387 / 525  Training Loss  0.009332241490483284\n",
            "Epoch  18 Batch  388 / 525  Training Loss  0.006253362633287907\n",
            "Epoch  18 Batch  389 / 525  Training Loss  0.006321650929749012\n",
            "Epoch  18 Batch  390 / 525  Training Loss  0.013666396029293537\n",
            "Epoch  18 Batch  391 / 525  Training Loss  0.0077845193445682526\n",
            "Epoch  18 Batch  392 / 525  Training Loss  0.007533190306276083\n",
            "Epoch  18 Batch  393 / 525  Training Loss  0.010352837853133678\n",
            "Epoch  18 Batch  394 / 525  Training Loss  0.011765756644308567\n",
            "Epoch  18 Batch  395 / 525  Training Loss  0.008709196001291275\n",
            "Epoch  18 Batch  396 / 525  Training Loss  0.006642132066190243\n",
            "Epoch  18 Batch  397 / 525  Training Loss  0.00637898501008749\n",
            "Epoch  18 Batch  398 / 525  Training Loss  0.007677524350583553\n",
            "Epoch  18 Batch  399 / 525  Training Loss  0.013213211670517921\n",
            "Epoch  18 Batch  400 / 525  Training Loss  0.0065002902410924435\n",
            "Epoch  18 Batch  401 / 525  Training Loss  0.002460220130160451\n",
            "Epoch  18 Batch  402 / 525  Training Loss  0.007156041916459799\n",
            "Epoch  18 Batch  403 / 525  Training Loss  0.004763749428093433\n",
            "Epoch  18 Batch  404 / 525  Training Loss  0.012912491336464882\n",
            "Epoch  18 Batch  405 / 525  Training Loss  0.004891322460025549\n",
            "Epoch  18 Batch  406 / 525  Training Loss  0.012550096027553082\n",
            "Epoch  18 Batch  407 / 525  Training Loss  0.0075461529195308685\n",
            "Epoch  18 Batch  408 / 525  Training Loss  0.0100806700065732\n",
            "Epoch  18 Batch  409 / 525  Training Loss  0.00986472237855196\n",
            "Epoch  18 Batch  410 / 525  Training Loss  0.006112111732363701\n",
            "Epoch  18 Batch  411 / 525  Training Loss  0.008833822794258595\n",
            "Epoch  18 Batch  412 / 525  Training Loss  0.008506986312568188\n",
            "Epoch  18 Batch  413 / 525  Training Loss  0.004473721142858267\n",
            "Epoch  18 Batch  414 / 525  Training Loss  0.0027979945298284292\n",
            "Epoch  18 Batch  415 / 525  Training Loss  0.004485120996832848\n",
            "Epoch  18 Batch  416 / 525  Training Loss  0.027550149708986282\n",
            "Epoch  18 Batch  417 / 525  Training Loss  0.011200015433132648\n",
            "Epoch  18 Batch  418 / 525  Training Loss  0.003563458099961281\n",
            "Epoch  18 Batch  419 / 525  Training Loss  0.002803964540362358\n",
            "Epoch  18 Batch  420 / 525  Training Loss  0.013032302260398865\n",
            "Epoch  18 Batch  421 / 525  Training Loss  0.003859396558254957\n",
            "Epoch  18 Batch  422 / 525  Training Loss  0.004194098524749279\n",
            "Epoch  18 Batch  423 / 525  Training Loss  0.011022555641829967\n",
            "Epoch  18 Batch  424 / 525  Training Loss  0.0033987872302532196\n",
            "Epoch  18 Batch  425 / 525  Training Loss  0.005327453371137381\n",
            "Epoch  18 Batch  426 / 525  Training Loss  0.007532218005508184\n",
            "Epoch  18 Batch  427 / 525  Training Loss  0.008840782567858696\n",
            "Epoch  18 Batch  428 / 525  Training Loss  0.008510706946253777\n",
            "Epoch  18 Batch  429 / 525  Training Loss  0.003166786627843976\n",
            "Epoch  18 Batch  430 / 525  Training Loss  0.008427564986050129\n",
            "Epoch  18 Batch  431 / 525  Training Loss  0.00411397498100996\n",
            "Epoch  18 Batch  432 / 525  Training Loss  0.00540819251909852\n",
            "Epoch  18 Batch  433 / 525  Training Loss  0.004947681911289692\n",
            "Epoch  18 Batch  434 / 525  Training Loss  0.0068471068516373634\n",
            "Epoch  18 Batch  435 / 525  Training Loss  0.004567118361592293\n",
            "Epoch  18 Batch  436 / 525  Training Loss  0.006522455718368292\n",
            "Epoch  18 Batch  437 / 525  Training Loss  0.004117070697247982\n",
            "Epoch  18 Batch  438 / 525  Training Loss  0.005462910048663616\n",
            "Epoch  18 Batch  439 / 525  Training Loss  0.005603047553449869\n",
            "Epoch  18 Batch  440 / 525  Training Loss  0.005960758309811354\n",
            "Epoch  18 Batch  441 / 525  Training Loss  0.0067411670461297035\n",
            "Epoch  18 Batch  442 / 525  Training Loss  0.005799874197691679\n",
            "Epoch  18 Batch  443 / 525  Training Loss  0.006719072815030813\n",
            "Epoch  18 Batch  444 / 525  Training Loss  0.007267544977366924\n",
            "Epoch  18 Batch  445 / 525  Training Loss  0.005237790755927563\n",
            "Epoch  18 Batch  446 / 525  Training Loss  0.008839712478220463\n",
            "Epoch  18 Batch  447 / 525  Training Loss  0.005250697024166584\n",
            "Epoch  18 Batch  448 / 525  Training Loss  0.003041533287614584\n",
            "Epoch  18 Batch  449 / 525  Training Loss  0.005822551436722279\n",
            "Epoch  18 Batch  450 / 525  Training Loss  0.006219362374395132\n",
            "Epoch  18 Batch  451 / 525  Training Loss  0.008062160573899746\n",
            "Epoch  18 Batch  452 / 525  Training Loss  0.011846261098980904\n",
            "Epoch  18 Batch  453 / 525  Training Loss  0.004930456634610891\n",
            "Epoch  18 Batch  454 / 525  Training Loss  0.008995105512440205\n",
            "Epoch  18 Batch  455 / 525  Training Loss  0.007052614353597164\n",
            "Epoch  18 Batch  456 / 525  Training Loss  0.005296397488564253\n",
            "Epoch  18 Batch  457 / 525  Training Loss  0.011038783006370068\n",
            "Epoch  18 Batch  458 / 525  Training Loss  0.011412660591304302\n",
            "Epoch  18 Batch  459 / 525  Training Loss  0.00755729153752327\n",
            "Epoch  18 Batch  460 / 525  Training Loss  0.01240979041904211\n",
            "Epoch  18 Batch  461 / 525  Training Loss  0.0024932639207690954\n",
            "Epoch  18 Batch  462 / 525  Training Loss  0.00760263204574585\n",
            "Epoch  18 Batch  463 / 525  Training Loss  0.00850007589906454\n",
            "Epoch  18 Batch  464 / 525  Training Loss  0.005230485461652279\n",
            "Epoch  18 Batch  465 / 525  Training Loss  0.0059248534962534904\n",
            "Epoch  18 Batch  466 / 525  Training Loss  0.01203137170523405\n",
            "Epoch  18 Batch  467 / 525  Training Loss  0.02067757025361061\n",
            "Epoch  18 Batch  468 / 525  Training Loss  0.00832358282059431\n",
            "Epoch  18 Batch  469 / 525  Training Loss  0.005186061374843121\n",
            "Epoch  18 Batch  470 / 525  Training Loss  0.005364222452044487\n",
            "Epoch  18 Batch  471 / 525  Training Loss  0.011807793751358986\n",
            "Epoch  18 Batch  472 / 525  Training Loss  0.013243680819869041\n",
            "Epoch  18 Batch  473 / 525  Training Loss  0.0088551240041852\n",
            "Epoch  18 Batch  474 / 525  Training Loss  0.011693660169839859\n",
            "Epoch  18 Batch  475 / 525  Training Loss  0.006800143979489803\n",
            "Epoch  18 Batch  476 / 525  Training Loss  0.006014677695930004\n",
            "Epoch  18 Batch  477 / 525  Training Loss  0.007879085838794708\n",
            "Epoch  18 Batch  478 / 525  Training Loss  0.0065552652813494205\n",
            "Epoch  18 Batch  479 / 525  Training Loss  0.00839798990637064\n",
            "Epoch  18 Batch  480 / 525  Training Loss  0.0024237786419689655\n",
            "Epoch  18 Batch  481 / 525  Training Loss  0.009312416426837444\n",
            "Epoch  18 Batch  482 / 525  Training Loss  0.002548166085034609\n",
            "Epoch  18 Batch  483 / 525  Training Loss  0.0022971895523369312\n",
            "Epoch  18 Batch  484 / 525  Training Loss  0.01190516073256731\n",
            "Epoch  18 Batch  485 / 525  Training Loss  0.00828329287469387\n",
            "Epoch  18 Batch  486 / 525  Training Loss  0.009862542152404785\n",
            "Epoch  18 Batch  487 / 525  Training Loss  0.0034371796064078808\n",
            "Epoch  18 Batch  488 / 525  Training Loss  0.0034524076618254185\n",
            "Epoch  18 Batch  489 / 525  Training Loss  0.005622184835374355\n",
            "Epoch  18 Batch  490 / 525  Training Loss  0.004313018172979355\n",
            "Epoch  18 Batch  491 / 525  Training Loss  0.004317835904657841\n",
            "Epoch  18 Batch  492 / 525  Training Loss  0.01087727677077055\n",
            "Epoch  18 Batch  493 / 525  Training Loss  0.004329643677920103\n",
            "Epoch  18 Batch  494 / 525  Training Loss  0.008952675387263298\n",
            "Epoch  18 Batch  495 / 525  Training Loss  0.006335035897791386\n",
            "Epoch  18 Batch  496 / 525  Training Loss  0.0071940659545362\n",
            "Epoch  18 Batch  497 / 525  Training Loss  0.012094145640730858\n",
            "Epoch  18 Batch  498 / 525  Training Loss  0.009912768378853798\n",
            "Epoch  18 Batch  499 / 525  Training Loss  0.002468600869178772\n",
            "Epoch  18 Batch  500 / 525  Training Loss  0.007324926555156708\n",
            "Epoch  18 Batch  501 / 525  Training Loss  0.0023099177051335573\n",
            "Epoch  18 Batch  502 / 525  Training Loss  0.007120695896446705\n",
            "Epoch  18 Batch  503 / 525  Training Loss  0.00887315534055233\n",
            "Epoch  18 Batch  504 / 525  Training Loss  0.009371972642838955\n",
            "Epoch  18 Batch  505 / 525  Training Loss  0.005321485921740532\n",
            "Epoch  18 Batch  506 / 525  Training Loss  0.004481137730181217\n",
            "Epoch  18 Batch  507 / 525  Training Loss  0.015985539183020592\n",
            "Epoch  18 Batch  508 / 525  Training Loss  0.0036364588886499405\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  18 Batch  509 / 525  Training Loss  0.007153225131332874\n",
            "Epoch  18 Batch  510 / 525  Training Loss  0.005744231399148703\n",
            "Epoch  18 Batch  511 / 525  Training Loss  0.00748699763789773\n",
            "Epoch  18 Batch  512 / 525  Training Loss  0.005928763188421726\n",
            "Epoch  18 Batch  513 / 525  Training Loss  0.006164351478219032\n",
            "Epoch  18 Batch  514 / 525  Training Loss  0.005211045499891043\n",
            "Epoch  18 Batch  515 / 525  Training Loss  0.011729102581739426\n",
            "Epoch  18 Batch  516 / 525  Training Loss  0.004908409435302019\n",
            "Epoch  18 Batch  517 / 525  Training Loss  0.00819060206413269\n",
            "Epoch  18 Batch  518 / 525  Training Loss  0.009987392462790012\n",
            "Epoch  18 Batch  519 / 525  Training Loss  0.01068040356040001\n",
            "Epoch  18 Batch  520 / 525  Training Loss  0.004049098584800959\n",
            "Epoch  18 Batch  521 / 525  Training Loss  0.007511536590754986\n",
            "Epoch  18 Batch  522 / 525  Training Loss  0.008564895018935204\n",
            "Epoch  18 Batch  523 / 525  Training Loss  0.01297931931912899\n",
            "Epoch  18 Batch  524 / 525  Training Loss  0.007177490741014481\n",
            "  19    |    -    |   0.006753   | 58.833333\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 19\n",
            "Epoch  19 Batch  0 / 525  Training Loss  0.012997342273592949\n",
            "Epoch  19 Batch  1 / 525  Training Loss  0.0062892199493944645\n",
            "Epoch  19 Batch  2 / 525  Training Loss  0.004490661434829235\n",
            "Epoch  19 Batch  3 / 525  Training Loss  0.005985267460346222\n",
            "Epoch  19 Batch  4 / 525  Training Loss  0.0024478959385305643\n",
            "Epoch  19 Batch  5 / 525  Training Loss  0.0030831252224743366\n",
            "Epoch  19 Batch  6 / 525  Training Loss  0.01606196165084839\n",
            "Epoch  19 Batch  7 / 525  Training Loss  0.008559145964682102\n",
            "Epoch  19 Batch  8 / 525  Training Loss  0.010820160619914532\n",
            "Epoch  19 Batch  9 / 525  Training Loss  0.005120088346302509\n",
            "Epoch  19 Batch  10 / 525  Training Loss  0.007862916216254234\n",
            "Epoch  19 Batch  11 / 525  Training Loss  0.005110260099172592\n",
            "Epoch  19 Batch  12 / 525  Training Loss  0.0015542872715741396\n",
            "Epoch  19 Batch  13 / 525  Training Loss  0.0036558390129357576\n",
            "Epoch  19 Batch  14 / 525  Training Loss  0.003097750712186098\n",
            "Epoch  19 Batch  15 / 525  Training Loss  0.0069407811388373375\n",
            "Epoch  19 Batch  16 / 525  Training Loss  0.0032305915374308825\n",
            "Epoch  19 Batch  17 / 525  Training Loss  0.0030803270637989044\n",
            "Epoch  19 Batch  18 / 525  Training Loss  0.0031183194369077682\n",
            "Epoch  19 Batch  19 / 525  Training Loss  0.0022689749021083117\n",
            "Epoch  19 Batch  20 / 525  Training Loss  0.0016474518924951553\n",
            "Epoch  19 Batch  21 / 525  Training Loss  0.005080569069832563\n",
            "Epoch  19 Batch  22 / 525  Training Loss  0.005625386722385883\n",
            "Epoch  19 Batch  23 / 525  Training Loss  0.004868931137025356\n",
            "Epoch  19 Batch  24 / 525  Training Loss  0.002551295096054673\n",
            "Epoch  19 Batch  25 / 525  Training Loss  0.0034798074048012495\n",
            "Epoch  19 Batch  26 / 525  Training Loss  0.0032714400440454483\n",
            "Epoch  19 Batch  27 / 525  Training Loss  0.003168660681694746\n",
            "Epoch  19 Batch  28 / 525  Training Loss  0.004038300830870867\n",
            "Epoch  19 Batch  29 / 525  Training Loss  0.005432491190731525\n",
            "Epoch  19 Batch  30 / 525  Training Loss  0.00686871912330389\n",
            "Epoch  19 Batch  31 / 525  Training Loss  0.007383446209132671\n",
            "Epoch  19 Batch  32 / 525  Training Loss  0.0033142995089292526\n",
            "Epoch  19 Batch  33 / 525  Training Loss  0.004440452437847853\n",
            "Epoch  19 Batch  34 / 525  Training Loss  0.0017425533151254058\n",
            "Epoch  19 Batch  35 / 525  Training Loss  0.003909694030880928\n",
            "Epoch  19 Batch  36 / 525  Training Loss  0.0026860549114644527\n",
            "Epoch  19 Batch  37 / 525  Training Loss  0.0034313369542360306\n",
            "Epoch  19 Batch  38 / 525  Training Loss  0.005075656808912754\n",
            "Epoch  19 Batch  39 / 525  Training Loss  0.008760091848671436\n",
            "Epoch  19 Batch  40 / 525  Training Loss  0.0019989788997918367\n",
            "Epoch  19 Batch  41 / 525  Training Loss  0.006097582168877125\n",
            "Epoch  19 Batch  42 / 525  Training Loss  0.0024673733860254288\n",
            "Epoch  19 Batch  43 / 525  Training Loss  0.0026082717813551426\n",
            "Epoch  19 Batch  44 / 525  Training Loss  0.003888275008648634\n",
            "Epoch  19 Batch  45 / 525  Training Loss  0.001935059204697609\n",
            "Epoch  19 Batch  46 / 525  Training Loss  0.005609593354165554\n",
            "Epoch  19 Batch  47 / 525  Training Loss  0.005646898411214352\n",
            "Epoch  19 Batch  48 / 525  Training Loss  0.0030578081496059895\n",
            "Epoch  19 Batch  49 / 525  Training Loss  0.010521354153752327\n",
            "Epoch  19 Batch  50 / 525  Training Loss  0.005157810635864735\n",
            "Epoch  19 Batch  51 / 525  Training Loss  0.010431477800011635\n",
            "Epoch  19 Batch  52 / 525  Training Loss  0.0035356120206415653\n",
            "Epoch  19 Batch  53 / 525  Training Loss  0.002309198025614023\n",
            "Epoch  19 Batch  54 / 525  Training Loss  0.003828521817922592\n",
            "Epoch  19 Batch  55 / 525  Training Loss  0.0054895030334591866\n",
            "Epoch  19 Batch  56 / 525  Training Loss  0.002951185218989849\n",
            "Epoch  19 Batch  57 / 525  Training Loss  0.00820672232657671\n",
            "Epoch  19 Batch  58 / 525  Training Loss  0.004410720895975828\n",
            "Epoch  19 Batch  59 / 525  Training Loss  0.0018533862894400954\n",
            "Epoch  19 Batch  60 / 525  Training Loss  0.0016187040600925684\n",
            "Epoch  19 Batch  61 / 525  Training Loss  0.0027015618979930878\n",
            "Epoch  19 Batch  62 / 525  Training Loss  0.003269815817475319\n",
            "Epoch  19 Batch  63 / 525  Training Loss  0.004562509711831808\n",
            "Epoch  19 Batch  64 / 525  Training Loss  0.0075021227821707726\n",
            "Epoch  19 Batch  65 / 525  Training Loss  0.004550914280116558\n",
            "Epoch  19 Batch  66 / 525  Training Loss  0.0022030631080269814\n",
            "Epoch  19 Batch  67 / 525  Training Loss  0.0027695612516254187\n",
            "Epoch  19 Batch  68 / 525  Training Loss  0.002605025889351964\n",
            "Epoch  19 Batch  69 / 525  Training Loss  0.0027896747924387455\n",
            "Epoch  19 Batch  70 / 525  Training Loss  0.0012156845768913627\n",
            "Epoch  19 Batch  71 / 525  Training Loss  0.0022755744867026806\n",
            "Epoch  19 Batch  72 / 525  Training Loss  0.0022105330135673285\n",
            "Epoch  19 Batch  73 / 525  Training Loss  0.001407312462106347\n",
            "Epoch  19 Batch  74 / 525  Training Loss  0.0019301858264952898\n",
            "Epoch  19 Batch  75 / 525  Training Loss  0.0020899600349366665\n",
            "Epoch  19 Batch  76 / 525  Training Loss  0.003850906388834119\n",
            "Epoch  19 Batch  77 / 525  Training Loss  0.0038246936164796352\n",
            "Epoch  19 Batch  78 / 525  Training Loss  0.002921252977102995\n",
            "Epoch  19 Batch  79 / 525  Training Loss  0.0015301934909075499\n",
            "Epoch  19 Batch  80 / 525  Training Loss  0.0034617215860635042\n",
            "Epoch  19 Batch  81 / 525  Training Loss  0.0013917033793404698\n",
            "Epoch  19 Batch  82 / 525  Training Loss  0.005432554520666599\n",
            "Epoch  19 Batch  83 / 525  Training Loss  0.002347910776734352\n",
            "Epoch  19 Batch  84 / 525  Training Loss  0.0027642378117889166\n",
            "Epoch  19 Batch  85 / 525  Training Loss  0.0022000339813530445\n",
            "Epoch  19 Batch  86 / 525  Training Loss  0.004611847456544638\n",
            "Epoch  19 Batch  87 / 525  Training Loss  0.005575446877628565\n",
            "Epoch  19 Batch  88 / 525  Training Loss  0.0013221537228673697\n",
            "Epoch  19 Batch  89 / 525  Training Loss  0.0016514675226062536\n",
            "Epoch  19 Batch  90 / 525  Training Loss  0.001586178899742663\n",
            "Epoch  19 Batch  91 / 525  Training Loss  0.008321447297930717\n",
            "Epoch  19 Batch  92 / 525  Training Loss  0.0030555662233382463\n",
            "Epoch  19 Batch  93 / 525  Training Loss  0.004327595233917236\n",
            "Epoch  19 Batch  94 / 525  Training Loss  0.006414114031940699\n",
            "Epoch  19 Batch  95 / 525  Training Loss  0.0018445294117555022\n",
            "Epoch  19 Batch  96 / 525  Training Loss  0.0019339981954544783\n",
            "Epoch  19 Batch  97 / 525  Training Loss  0.0014009816804900765\n",
            "Epoch  19 Batch  98 / 525  Training Loss  0.0028539025224745274\n",
            "Epoch  19 Batch  99 / 525  Training Loss  0.0029590653721243143\n",
            "Epoch  19 Batch  100 / 525  Training Loss  0.0021299365907907486\n",
            "Epoch  19 Batch  101 / 525  Training Loss  0.001101747970096767\n",
            "Epoch  19 Batch  102 / 525  Training Loss  0.007337459363043308\n",
            "Epoch  19 Batch  103 / 525  Training Loss  0.0022124561946839094\n",
            "Epoch  19 Batch  104 / 525  Training Loss  0.004411191679537296\n",
            "Epoch  19 Batch  105 / 525  Training Loss  0.0035401969216763973\n",
            "Epoch  19 Batch  106 / 525  Training Loss  0.0016595743363723159\n",
            "Epoch  19 Batch  107 / 525  Training Loss  0.00194483099039644\n",
            "Epoch  19 Batch  108 / 525  Training Loss  0.0031874533742666245\n",
            "Epoch  19 Batch  109 / 525  Training Loss  0.0024196698796004057\n",
            "Epoch  19 Batch  110 / 525  Training Loss  0.00801057368516922\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  19 Batch  111 / 525  Training Loss  0.0027062990702688694\n",
            "Epoch  19 Batch  112 / 525  Training Loss  0.004583514295518398\n",
            "Epoch  19 Batch  113 / 525  Training Loss  0.00399034284055233\n",
            "Epoch  19 Batch  114 / 525  Training Loss  0.007877010852098465\n",
            "Epoch  19 Batch  115 / 525  Training Loss  0.0060731153935194016\n",
            "Epoch  19 Batch  116 / 525  Training Loss  0.0011849135626107454\n",
            "Epoch  19 Batch  117 / 525  Training Loss  0.015131724998354912\n",
            "Epoch  19 Batch  118 / 525  Training Loss  0.008247869089245796\n",
            "Epoch  19 Batch  119 / 525  Training Loss  0.0019324219319969416\n",
            "Epoch  19 Batch  120 / 525  Training Loss  0.006694232579320669\n",
            "Epoch  19 Batch  121 / 525  Training Loss  0.0020772258285433054\n",
            "Epoch  19 Batch  122 / 525  Training Loss  0.0032849572598934174\n",
            "Epoch  19 Batch  123 / 525  Training Loss  0.00289865187369287\n",
            "Epoch  19 Batch  124 / 525  Training Loss  0.00513866264373064\n",
            "Epoch  19 Batch  125 / 525  Training Loss  0.0028531223069876432\n",
            "Epoch  19 Batch  126 / 525  Training Loss  0.0010317107662558556\n",
            "Epoch  19 Batch  127 / 525  Training Loss  0.0021369727328419685\n",
            "Epoch  19 Batch  128 / 525  Training Loss  0.003943993244320154\n",
            "Epoch  19 Batch  129 / 525  Training Loss  0.0043162074871361256\n",
            "Epoch  19 Batch  130 / 525  Training Loss  0.001956065185368061\n",
            "Epoch  19 Batch  131 / 525  Training Loss  0.0051677473820745945\n",
            "Epoch  19 Batch  132 / 525  Training Loss  0.00415660347789526\n",
            "Epoch  19 Batch  133 / 525  Training Loss  0.0035208524204790592\n",
            "Epoch  19 Batch  134 / 525  Training Loss  0.0028570869471877813\n",
            "Epoch  19 Batch  135 / 525  Training Loss  0.002631222130730748\n",
            "Epoch  19 Batch  136 / 525  Training Loss  0.007427793927490711\n",
            "Epoch  19 Batch  137 / 525  Training Loss  0.0022658237721771\n",
            "Epoch  19 Batch  138 / 525  Training Loss  0.004312336444854736\n",
            "Epoch  19 Batch  139 / 525  Training Loss  0.018117867410182953\n",
            "Epoch  19 Batch  140 / 525  Training Loss  0.01350688748061657\n",
            "Epoch  19 Batch  141 / 525  Training Loss  0.0019847569055855274\n",
            "Epoch  19 Batch  142 / 525  Training Loss  0.008716916665434837\n",
            "Epoch  19 Batch  143 / 525  Training Loss  0.004138036631047726\n",
            "Epoch  19 Batch  144 / 525  Training Loss  0.003990416415035725\n",
            "Epoch  19 Batch  145 / 525  Training Loss  0.0012487201020121574\n",
            "Epoch  19 Batch  146 / 525  Training Loss  0.004001378081738949\n",
            "Epoch  19 Batch  147 / 525  Training Loss  0.006207251455634832\n",
            "Epoch  19 Batch  148 / 525  Training Loss  0.0012096045538783073\n",
            "Epoch  19 Batch  149 / 525  Training Loss  0.0041375719010829926\n",
            "Epoch  19 Batch  150 / 525  Training Loss  0.0019673649221658707\n",
            "Epoch  19 Batch  151 / 525  Training Loss  0.002480396069586277\n",
            "Epoch  19 Batch  152 / 525  Training Loss  0.003824504790827632\n",
            "Epoch  19 Batch  153 / 525  Training Loss  0.0029670167714357376\n",
            "Epoch  19 Batch  154 / 525  Training Loss  0.002440627897158265\n",
            "Epoch  19 Batch  155 / 525  Training Loss  0.0018974372651427984\n",
            "Epoch  19 Batch  156 / 525  Training Loss  0.004548703320324421\n",
            "Epoch  19 Batch  157 / 525  Training Loss  0.001760078244842589\n",
            "Epoch  19 Batch  158 / 525  Training Loss  0.004828744567930698\n",
            "Epoch  19 Batch  159 / 525  Training Loss  0.0022978128399699926\n",
            "Epoch  19 Batch  160 / 525  Training Loss  0.006157664582133293\n",
            "Epoch  19 Batch  161 / 525  Training Loss  0.006299961358308792\n",
            "Epoch  19 Batch  162 / 525  Training Loss  0.0035425000824034214\n",
            "Epoch  19 Batch  163 / 525  Training Loss  0.007123976945877075\n",
            "Epoch  19 Batch  164 / 525  Training Loss  0.0027724727988243103\n",
            "Epoch  19 Batch  165 / 525  Training Loss  0.0020175238605588675\n",
            "Epoch  19 Batch  166 / 525  Training Loss  0.0021407348103821278\n",
            "Epoch  19 Batch  167 / 525  Training Loss  0.002663579536601901\n",
            "Epoch  19 Batch  168 / 525  Training Loss  0.0037251512985676527\n",
            "Epoch  19 Batch  169 / 525  Training Loss  0.0047057019546628\n",
            "Epoch  19 Batch  170 / 525  Training Loss  0.002035051118582487\n",
            "Epoch  19 Batch  171 / 525  Training Loss  0.0018820802215486765\n",
            "Epoch  19 Batch  172 / 525  Training Loss  0.0027168123051524162\n",
            "Epoch  19 Batch  173 / 525  Training Loss  0.008524415083229542\n",
            "Epoch  19 Batch  174 / 525  Training Loss  0.0041702864691615105\n",
            "Epoch  19 Batch  175 / 525  Training Loss  0.00440923823043704\n",
            "Epoch  19 Batch  176 / 525  Training Loss  0.005335623398423195\n",
            "Epoch  19 Batch  177 / 525  Training Loss  0.010907664895057678\n",
            "Epoch  19 Batch  178 / 525  Training Loss  0.016397695988416672\n",
            "Epoch  19 Batch  179 / 525  Training Loss  0.00754562858492136\n",
            "Epoch  19 Batch  180 / 525  Training Loss  0.0048295361921191216\n",
            "Epoch  19 Batch  181 / 525  Training Loss  0.0031398285645991564\n",
            "Epoch  19 Batch  182 / 525  Training Loss  0.0026895604096353054\n",
            "Epoch  19 Batch  183 / 525  Training Loss  0.0022038298193365335\n",
            "Epoch  19 Batch  184 / 525  Training Loss  0.0028328499756753445\n",
            "Epoch  19 Batch  185 / 525  Training Loss  0.0022364361211657524\n",
            "Epoch  19 Batch  186 / 525  Training Loss  0.0029316903091967106\n",
            "Epoch  19 Batch  187 / 525  Training Loss  0.008154002949595451\n",
            "Epoch  19 Batch  188 / 525  Training Loss  0.003064369084313512\n",
            "Epoch  19 Batch  189 / 525  Training Loss  0.0038111265748739243\n",
            "Epoch  19 Batch  190 / 525  Training Loss  0.00439764279872179\n",
            "Epoch  19 Batch  191 / 525  Training Loss  0.006793490145355463\n",
            "Epoch  19 Batch  192 / 525  Training Loss  0.005140649154782295\n",
            "Epoch  19 Batch  193 / 525  Training Loss  0.002764145378023386\n",
            "Epoch  19 Batch  194 / 525  Training Loss  0.0022056330926716328\n",
            "Epoch  19 Batch  195 / 525  Training Loss  0.002355040516704321\n",
            "Epoch  19 Batch  196 / 525  Training Loss  0.0025274555664509535\n",
            "Epoch  19 Batch  197 / 525  Training Loss  0.0036684535443782806\n",
            "Epoch  19 Batch  198 / 525  Training Loss  0.0059111518785357475\n",
            "Epoch  19 Batch  199 / 525  Training Loss  0.001818929216824472\n",
            "Epoch  19 Batch  200 / 525  Training Loss  0.00650755176320672\n",
            "Epoch  19 Batch  201 / 525  Training Loss  0.002704146783798933\n",
            "Epoch  19 Batch  202 / 525  Training Loss  0.00937402993440628\n",
            "Epoch  19 Batch  203 / 525  Training Loss  0.003926408011466265\n",
            "Epoch  19 Batch  204 / 525  Training Loss  0.0020577055402100086\n",
            "Epoch  19 Batch  205 / 525  Training Loss  0.004170751199126244\n",
            "Epoch  19 Batch  206 / 525  Training Loss  0.0049221208319067955\n",
            "Epoch  19 Batch  207 / 525  Training Loss  0.0059125469997525215\n",
            "Epoch  19 Batch  208 / 525  Training Loss  0.00647271191701293\n",
            "Epoch  19 Batch  209 / 525  Training Loss  0.010099225677549839\n",
            "Epoch  19 Batch  210 / 525  Training Loss  0.003087287303060293\n",
            "Epoch  19 Batch  211 / 525  Training Loss  0.0042345523834228516\n",
            "Epoch  19 Batch  212 / 525  Training Loss  0.0020628678612411022\n",
            "Epoch  19 Batch  213 / 525  Training Loss  0.003623340977355838\n",
            "Epoch  19 Batch  214 / 525  Training Loss  0.008260130882263184\n",
            "Epoch  19 Batch  215 / 525  Training Loss  0.008231459185481071\n",
            "Epoch  19 Batch  216 / 525  Training Loss  0.008299540728330612\n",
            "Epoch  19 Batch  217 / 525  Training Loss  0.009318102151155472\n",
            "Epoch  19 Batch  218 / 525  Training Loss  0.0035774740390479565\n",
            "Epoch  19 Batch  219 / 525  Training Loss  0.0033940435387194157\n",
            "Epoch  19 Batch  220 / 525  Training Loss  0.004277005326002836\n",
            "Epoch  19 Batch  221 / 525  Training Loss  0.002072015078738332\n",
            "Epoch  19 Batch  222 / 525  Training Loss  0.004758730530738831\n",
            "Epoch  19 Batch  223 / 525  Training Loss  0.004006775561720133\n",
            "Epoch  19 Batch  224 / 525  Training Loss  0.0035287602804601192\n",
            "Epoch  19 Batch  225 / 525  Training Loss  0.005433499813079834\n",
            "Epoch  19 Batch  226 / 525  Training Loss  0.003042973345145583\n",
            "Epoch  19 Batch  227 / 525  Training Loss  0.002602232387289405\n",
            "Epoch  19 Batch  228 / 525  Training Loss  0.0028307184111326933\n",
            "Epoch  19 Batch  229 / 525  Training Loss  0.0027357779908925295\n",
            "Epoch  19 Batch  230 / 525  Training Loss  0.0032470892183482647\n",
            "Epoch  19 Batch  231 / 525  Training Loss  0.00504513643682003\n",
            "Epoch  19 Batch  232 / 525  Training Loss  0.004789985250681639\n",
            "Epoch  19 Batch  233 / 525  Training Loss  0.009135696105659008\n",
            "Epoch  19 Batch  234 / 525  Training Loss  0.0028779690619558096\n",
            "Epoch  19 Batch  235 / 525  Training Loss  0.002879937645047903\n",
            "Epoch  19 Batch  236 / 525  Training Loss  0.004800287541002035\n",
            "Epoch  19 Batch  237 / 525  Training Loss  0.0061629777774214745\n",
            "Epoch  19 Batch  238 / 525  Training Loss  0.0036433502100408077\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  19 Batch  239 / 525  Training Loss  0.002667330438271165\n",
            "Epoch  19 Batch  240 / 525  Training Loss  0.0030124341137707233\n",
            "Epoch  19 Batch  241 / 525  Training Loss  0.002193666994571686\n",
            "Epoch  19 Batch  242 / 525  Training Loss  0.0056537603959441185\n",
            "Epoch  19 Batch  243 / 525  Training Loss  0.004641302861273289\n",
            "Epoch  19 Batch  244 / 525  Training Loss  0.005769312381744385\n",
            "Epoch  19 Batch  245 / 525  Training Loss  0.002917767269536853\n",
            "Epoch  19 Batch  246 / 525  Training Loss  0.005901167169213295\n",
            "Epoch  19 Batch  247 / 525  Training Loss  0.0058201453648507595\n",
            "Epoch  19 Batch  248 / 525  Training Loss  0.0072628287598490715\n",
            "Epoch  19 Batch  249 / 525  Training Loss  0.010718327015638351\n",
            "Epoch  19 Batch  250 / 525  Training Loss  0.0029393674340099096\n",
            "Epoch  19 Batch  251 / 525  Training Loss  0.005476527847349644\n",
            "Epoch  19 Batch  252 / 525  Training Loss  0.003338546957820654\n",
            "Epoch  19 Batch  253 / 525  Training Loss  0.0031284026335924864\n",
            "Epoch  19 Batch  254 / 525  Training Loss  0.0034685928840190172\n",
            "Epoch  19 Batch  255 / 525  Training Loss  0.0047601996921002865\n",
            "Epoch  19 Batch  256 / 525  Training Loss  0.009097209200263023\n",
            "Epoch  19 Batch  257 / 525  Training Loss  0.005048100836575031\n",
            "Epoch  19 Batch  258 / 525  Training Loss  0.0018227063119411469\n",
            "Epoch  19 Batch  259 / 525  Training Loss  0.00951940007507801\n",
            "Epoch  19 Batch  260 / 525  Training Loss  0.010244742967188358\n",
            "Epoch  19 Batch  261 / 525  Training Loss  0.0035123981069773436\n",
            "Epoch  19 Batch  262 / 525  Training Loss  0.00938461534678936\n",
            "Epoch  19 Batch  263 / 525  Training Loss  0.007838726975023746\n",
            "Epoch  19 Batch  264 / 525  Training Loss  0.0059463465586304665\n",
            "Epoch  19 Batch  265 / 525  Training Loss  0.00858191680163145\n",
            "Epoch  19 Batch  266 / 525  Training Loss  0.0015931632369756699\n",
            "Epoch  19 Batch  267 / 525  Training Loss  0.0038089193403720856\n",
            "Epoch  19 Batch  268 / 525  Training Loss  0.004210746847093105\n",
            "Epoch  19 Batch  269 / 525  Training Loss  0.00597250834107399\n",
            "Epoch  19 Batch  270 / 525  Training Loss  0.004515320062637329\n",
            "Epoch  19 Batch  271 / 525  Training Loss  0.007592770271003246\n",
            "Epoch  19 Batch  272 / 525  Training Loss  0.01013839803636074\n",
            "Epoch  19 Batch  273 / 525  Training Loss  0.004798850975930691\n",
            "Epoch  19 Batch  274 / 525  Training Loss  0.010352126322686672\n",
            "Epoch  19 Batch  275 / 525  Training Loss  0.005696074105799198\n",
            "Epoch  19 Batch  276 / 525  Training Loss  0.005270111374557018\n",
            "Epoch  19 Batch  277 / 525  Training Loss  0.009972784668207169\n",
            "Epoch  19 Batch  278 / 525  Training Loss  0.005785099696367979\n",
            "Epoch  19 Batch  279 / 525  Training Loss  0.005560420919209719\n",
            "Epoch  19 Batch  280 / 525  Training Loss  0.010034671053290367\n",
            "Epoch  19 Batch  281 / 525  Training Loss  0.006040423177182674\n",
            "Epoch  19 Batch  282 / 525  Training Loss  0.0031374935060739517\n",
            "Epoch  19 Batch  283 / 525  Training Loss  0.004550586454570293\n",
            "Epoch  19 Batch  284 / 525  Training Loss  0.0019407046493142843\n",
            "Epoch  19 Batch  285 / 525  Training Loss  0.007880773395299911\n",
            "Epoch  19 Batch  286 / 525  Training Loss  0.0032228517811745405\n",
            "Epoch  19 Batch  287 / 525  Training Loss  0.003574877977371216\n",
            "Epoch  19 Batch  288 / 525  Training Loss  0.0029612709768116474\n",
            "Epoch  19 Batch  289 / 525  Training Loss  0.008353631012141705\n",
            "Epoch  19 Batch  290 / 525  Training Loss  0.0019425296923145652\n",
            "Epoch  19 Batch  291 / 525  Training Loss  0.0028053936548531055\n",
            "Epoch  19 Batch  292 / 525  Training Loss  0.006708481814712286\n",
            "Epoch  19 Batch  293 / 525  Training Loss  0.011081114411354065\n",
            "Epoch  19 Batch  294 / 525  Training Loss  0.00896981731057167\n",
            "Epoch  19 Batch  295 / 525  Training Loss  0.00807040836662054\n",
            "Epoch  19 Batch  296 / 525  Training Loss  0.01121486909687519\n",
            "Epoch  19 Batch  297 / 525  Training Loss  0.0023461738601326942\n",
            "Epoch  19 Batch  298 / 525  Training Loss  0.005785380490124226\n",
            "Epoch  19 Batch  299 / 525  Training Loss  0.003960845526307821\n",
            "Epoch  19 Batch  300 / 525  Training Loss  0.0040158918127417564\n",
            "Epoch  19 Batch  301 / 525  Training Loss  0.002094149822369218\n",
            "Epoch  19 Batch  302 / 525  Training Loss  0.004745663143694401\n",
            "Epoch  19 Batch  303 / 525  Training Loss  0.004430446308106184\n",
            "Epoch  19 Batch  304 / 525  Training Loss  0.0023098750971257687\n",
            "Epoch  19 Batch  305 / 525  Training Loss  0.00512709142640233\n",
            "Epoch  19 Batch  306 / 525  Training Loss  0.0032168023753911257\n",
            "Epoch  19 Batch  307 / 525  Training Loss  0.0027238528709858656\n",
            "Epoch  19 Batch  308 / 525  Training Loss  0.004977192264050245\n",
            "Epoch  19 Batch  309 / 525  Training Loss  0.002713700057938695\n",
            "Epoch  19 Batch  310 / 525  Training Loss  0.010046284645795822\n",
            "Epoch  19 Batch  311 / 525  Training Loss  0.009366976097226143\n",
            "Epoch  19 Batch  312 / 525  Training Loss  0.004372714087367058\n",
            "Epoch  19 Batch  313 / 525  Training Loss  0.006235987413674593\n",
            "Epoch  19 Batch  314 / 525  Training Loss  0.0032986558508127928\n",
            "Epoch  19 Batch  315 / 525  Training Loss  0.008204219862818718\n",
            "Epoch  19 Batch  316 / 525  Training Loss  0.017478901892900467\n",
            "Epoch  19 Batch  317 / 525  Training Loss  0.00352880684658885\n",
            "Epoch  19 Batch  318 / 525  Training Loss  0.005292873829603195\n",
            "Epoch  19 Batch  319 / 525  Training Loss  0.005460926331579685\n",
            "Epoch  19 Batch  320 / 525  Training Loss  0.007930170744657516\n",
            "Epoch  19 Batch  321 / 525  Training Loss  0.0027594859711825848\n",
            "Epoch  19 Batch  322 / 525  Training Loss  0.018215253949165344\n",
            "Epoch  19 Batch  323 / 525  Training Loss  0.004840488079935312\n",
            "Epoch  19 Batch  324 / 525  Training Loss  0.00484220776706934\n",
            "Epoch  19 Batch  325 / 525  Training Loss  0.0037007268983870745\n",
            "Epoch  19 Batch  326 / 525  Training Loss  0.0061883507296442986\n",
            "Epoch  19 Batch  327 / 525  Training Loss  0.005336376838386059\n",
            "Epoch  19 Batch  328 / 525  Training Loss  0.00739577692002058\n",
            "Epoch  19 Batch  329 / 525  Training Loss  0.006569133140146732\n",
            "Epoch  19 Batch  330 / 525  Training Loss  0.005873719695955515\n",
            "Epoch  19 Batch  331 / 525  Training Loss  0.002770540304481983\n",
            "Epoch  19 Batch  332 / 525  Training Loss  0.00900142453610897\n",
            "Epoch  19 Batch  333 / 525  Training Loss  0.008180996403098106\n",
            "Epoch  19 Batch  334 / 525  Training Loss  0.007889480330049992\n",
            "Epoch  19 Batch  335 / 525  Training Loss  0.004206300713121891\n",
            "Epoch  19 Batch  336 / 525  Training Loss  0.003650107653811574\n",
            "Epoch  19 Batch  337 / 525  Training Loss  0.006452036555856466\n",
            "Epoch  19 Batch  338 / 525  Training Loss  0.004781351424753666\n",
            "Epoch  19 Batch  339 / 525  Training Loss  0.006723677273839712\n",
            "Epoch  19 Batch  340 / 525  Training Loss  0.006388568785041571\n",
            "Epoch  19 Batch  341 / 525  Training Loss  0.005389646161347628\n",
            "Epoch  19 Batch  342 / 525  Training Loss  0.010603586211800575\n",
            "Epoch  19 Batch  343 / 525  Training Loss  0.003485997673124075\n",
            "Epoch  19 Batch  344 / 525  Training Loss  0.0034405055921524763\n",
            "Epoch  19 Batch  345 / 525  Training Loss  0.005636103451251984\n",
            "Epoch  19 Batch  346 / 525  Training Loss  0.0029571871273219585\n",
            "Epoch  19 Batch  347 / 525  Training Loss  0.010782741010189056\n",
            "Epoch  19 Batch  348 / 525  Training Loss  0.0037127428222447634\n",
            "Epoch  19 Batch  349 / 525  Training Loss  0.012237017042934895\n",
            "Epoch  19 Batch  350 / 525  Training Loss  0.004962598904967308\n",
            "Epoch  19 Batch  351 / 525  Training Loss  0.008920883759856224\n",
            "Epoch  19 Batch  352 / 525  Training Loss  0.0032592415809631348\n",
            "Epoch  19 Batch  353 / 525  Training Loss  0.005777080077677965\n",
            "Epoch  19 Batch  354 / 525  Training Loss  0.006733562797307968\n",
            "Epoch  19 Batch  355 / 525  Training Loss  0.002623396459966898\n",
            "Epoch  19 Batch  356 / 525  Training Loss  0.0026859361678361893\n",
            "Epoch  19 Batch  357 / 525  Training Loss  0.00491001782938838\n",
            "Epoch  19 Batch  358 / 525  Training Loss  0.006484159734100103\n",
            "Epoch  19 Batch  359 / 525  Training Loss  0.009426426142454147\n",
            "Epoch  19 Batch  360 / 525  Training Loss  0.0061147501692175865\n",
            "Epoch  19 Batch  361 / 525  Training Loss  0.008827838115394115\n",
            "Epoch  19 Batch  362 / 525  Training Loss  0.018175778910517693\n",
            "Epoch  19 Batch  363 / 525  Training Loss  0.010125505737960339\n",
            "Epoch  19 Batch  364 / 525  Training Loss  0.012026859447360039\n",
            "Epoch  19 Batch  365 / 525  Training Loss  0.006377392914146185\n",
            "Epoch  19 Batch  366 / 525  Training Loss  0.0015173132997006178\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  19 Batch  367 / 525  Training Loss  0.01458515040576458\n",
            "Epoch  19 Batch  368 / 525  Training Loss  0.006589620374143124\n",
            "Epoch  19 Batch  369 / 525  Training Loss  0.004512859042733908\n",
            "Epoch  19 Batch  370 / 525  Training Loss  0.013682836666703224\n",
            "Epoch  19 Batch  371 / 525  Training Loss  0.0031015928834676743\n",
            "Epoch  19 Batch  372 / 525  Training Loss  0.004924115724861622\n",
            "Epoch  19 Batch  373 / 525  Training Loss  0.004178017843514681\n",
            "Epoch  19 Batch  374 / 525  Training Loss  0.005132748745381832\n",
            "Epoch  19 Batch  375 / 525  Training Loss  0.0028309798799455166\n",
            "Epoch  19 Batch  376 / 525  Training Loss  0.003460150910541415\n",
            "Epoch  19 Batch  377 / 525  Training Loss  0.005433513317257166\n",
            "Epoch  19 Batch  378 / 525  Training Loss  0.006693814881145954\n",
            "Epoch  19 Batch  379 / 525  Training Loss  0.0034930079709738493\n",
            "Epoch  19 Batch  380 / 525  Training Loss  0.0010964777320623398\n",
            "Epoch  19 Batch  381 / 525  Training Loss  0.006128460168838501\n",
            "Epoch  19 Batch  382 / 525  Training Loss  0.014206996187567711\n",
            "Epoch  19 Batch  383 / 525  Training Loss  0.004483153112232685\n",
            "Epoch  19 Batch  384 / 525  Training Loss  0.005711430683732033\n",
            "Epoch  19 Batch  385 / 525  Training Loss  0.0027246165554970503\n",
            "Epoch  19 Batch  386 / 525  Training Loss  0.003753743367269635\n",
            "Epoch  19 Batch  387 / 525  Training Loss  0.006617444567382336\n",
            "Epoch  19 Batch  388 / 525  Training Loss  0.003710746066644788\n",
            "Epoch  19 Batch  389 / 525  Training Loss  0.0019847718067467213\n",
            "Epoch  19 Batch  390 / 525  Training Loss  0.009325353428721428\n",
            "Epoch  19 Batch  391 / 525  Training Loss  0.004084413405507803\n",
            "Epoch  19 Batch  392 / 525  Training Loss  0.0030336841009557247\n",
            "Epoch  19 Batch  393 / 525  Training Loss  0.00858596246689558\n",
            "Epoch  19 Batch  394 / 525  Training Loss  0.006000456400215626\n",
            "Epoch  19 Batch  395 / 525  Training Loss  0.005481014493852854\n",
            "Epoch  19 Batch  396 / 525  Training Loss  0.003367497120052576\n",
            "Epoch  19 Batch  397 / 525  Training Loss  0.013459657318890095\n",
            "Epoch  19 Batch  398 / 525  Training Loss  0.0017539272084832191\n",
            "Epoch  19 Batch  399 / 525  Training Loss  0.014220942743122578\n",
            "Epoch  19 Batch  400 / 525  Training Loss  0.004784042946994305\n",
            "Epoch  19 Batch  401 / 525  Training Loss  0.018558183684945107\n",
            "Epoch  19 Batch  402 / 525  Training Loss  0.0026409507263451815\n",
            "Epoch  19 Batch  403 / 525  Training Loss  0.007183755747973919\n",
            "Epoch  19 Batch  404 / 525  Training Loss  0.005169058684259653\n",
            "Epoch  19 Batch  405 / 525  Training Loss  0.006229368504136801\n",
            "Epoch  19 Batch  406 / 525  Training Loss  0.0035720490850508213\n",
            "Epoch  19 Batch  407 / 525  Training Loss  0.0031425829511135817\n",
            "Epoch  19 Batch  408 / 525  Training Loss  0.004295523278415203\n",
            "Epoch  19 Batch  409 / 525  Training Loss  0.0034350561909377575\n",
            "Epoch  19 Batch  410 / 525  Training Loss  0.0062416912987828255\n",
            "Epoch  19 Batch  411 / 525  Training Loss  0.004599217791110277\n",
            "Epoch  19 Batch  412 / 525  Training Loss  0.0062410300597548485\n",
            "Epoch  19 Batch  413 / 525  Training Loss  0.008019590750336647\n",
            "Epoch  19 Batch  414 / 525  Training Loss  0.0024865171872079372\n",
            "Epoch  19 Batch  415 / 525  Training Loss  0.00536732655018568\n",
            "Epoch  19 Batch  416 / 525  Training Loss  0.002775168977677822\n",
            "Epoch  19 Batch  417 / 525  Training Loss  0.006028315518051386\n",
            "Epoch  19 Batch  418 / 525  Training Loss  0.0069035813212394714\n",
            "Epoch  19 Batch  419 / 525  Training Loss  0.010021163150668144\n",
            "Epoch  19 Batch  420 / 525  Training Loss  0.0027092956006526947\n",
            "Epoch  19 Batch  421 / 525  Training Loss  0.012444069609045982\n",
            "Epoch  19 Batch  422 / 525  Training Loss  0.0026955553330481052\n",
            "Epoch  19 Batch  423 / 525  Training Loss  0.0035638015251606703\n",
            "Epoch  19 Batch  424 / 525  Training Loss  0.004884236492216587\n",
            "Epoch  19 Batch  425 / 525  Training Loss  0.0054342979565262794\n",
            "Epoch  19 Batch  426 / 525  Training Loss  0.0077742659486830235\n",
            "Epoch  19 Batch  427 / 525  Training Loss  0.010731794871389866\n",
            "Epoch  19 Batch  428 / 525  Training Loss  0.004079871810972691\n",
            "Epoch  19 Batch  429 / 525  Training Loss  0.006237749010324478\n",
            "Epoch  19 Batch  430 / 525  Training Loss  0.003646083641797304\n",
            "Epoch  19 Batch  431 / 525  Training Loss  0.006224732846021652\n",
            "Epoch  19 Batch  432 / 525  Training Loss  0.014097562059760094\n",
            "Epoch  19 Batch  433 / 525  Training Loss  0.004102698527276516\n",
            "Epoch  19 Batch  434 / 525  Training Loss  0.005699521861970425\n",
            "Epoch  19 Batch  435 / 525  Training Loss  0.005765940062701702\n",
            "Epoch  19 Batch  436 / 525  Training Loss  0.012228226289153099\n",
            "Epoch  19 Batch  437 / 525  Training Loss  0.0071485163643956184\n",
            "Epoch  19 Batch  438 / 525  Training Loss  0.006808494217693806\n",
            "Epoch  19 Batch  439 / 525  Training Loss  0.0046288808807730675\n",
            "Epoch  19 Batch  440 / 525  Training Loss  0.008186602964997292\n",
            "Epoch  19 Batch  441 / 525  Training Loss  0.005956556182354689\n",
            "Epoch  19 Batch  442 / 525  Training Loss  0.0025744647718966007\n",
            "Epoch  19 Batch  443 / 525  Training Loss  0.0035403817892074585\n",
            "Epoch  19 Batch  444 / 525  Training Loss  0.004278945736587048\n",
            "Epoch  19 Batch  445 / 525  Training Loss  0.005398422013968229\n",
            "Epoch  19 Batch  446 / 525  Training Loss  0.005207530688494444\n",
            "Epoch  19 Batch  447 / 525  Training Loss  0.003956028260290623\n",
            "Epoch  19 Batch  448 / 525  Training Loss  0.005899760872125626\n",
            "Epoch  19 Batch  449 / 525  Training Loss  0.0039049789775162935\n",
            "Epoch  19 Batch  450 / 525  Training Loss  0.004047409165650606\n",
            "Epoch  19 Batch  451 / 525  Training Loss  0.005906287580728531\n",
            "Epoch  19 Batch  452 / 525  Training Loss  0.004593635443598032\n",
            "Epoch  19 Batch  453 / 525  Training Loss  0.00984948594123125\n",
            "Epoch  19 Batch  454 / 525  Training Loss  0.006873267702758312\n",
            "Epoch  19 Batch  455 / 525  Training Loss  0.004369338974356651\n",
            "Epoch  19 Batch  456 / 525  Training Loss  0.0028723967261612415\n",
            "Epoch  19 Batch  457 / 525  Training Loss  0.004017674829810858\n",
            "Epoch  19 Batch  458 / 525  Training Loss  0.009470057673752308\n",
            "Epoch  19 Batch  459 / 525  Training Loss  0.002437448129057884\n",
            "Epoch  19 Batch  460 / 525  Training Loss  0.005048075225204229\n",
            "Epoch  19 Batch  461 / 525  Training Loss  0.004370321519672871\n",
            "Epoch  19 Batch  462 / 525  Training Loss  0.0040350379422307014\n",
            "Epoch  19 Batch  463 / 525  Training Loss  0.012716459110379219\n",
            "Epoch  19 Batch  464 / 525  Training Loss  0.007021005265414715\n",
            "Epoch  19 Batch  465 / 525  Training Loss  0.0031525366939604282\n",
            "Epoch  19 Batch  466 / 525  Training Loss  0.0068596964702010155\n",
            "Epoch  19 Batch  467 / 525  Training Loss  0.009839404374361038\n",
            "Epoch  19 Batch  468 / 525  Training Loss  0.0015730714658275247\n",
            "Epoch  19 Batch  469 / 525  Training Loss  0.003338651731610298\n",
            "Epoch  19 Batch  470 / 525  Training Loss  0.009656544774770737\n",
            "Epoch  19 Batch  471 / 525  Training Loss  0.0053460439667105675\n",
            "Epoch  19 Batch  472 / 525  Training Loss  0.0021313070319592953\n",
            "Epoch  19 Batch  473 / 525  Training Loss  0.0039719040505588055\n",
            "Epoch  19 Batch  474 / 525  Training Loss  0.0035315901041030884\n",
            "Epoch  19 Batch  475 / 525  Training Loss  0.0017750324914231896\n",
            "Epoch  19 Batch  476 / 525  Training Loss  0.009156057611107826\n",
            "Epoch  19 Batch  477 / 525  Training Loss  0.002933433512225747\n",
            "Epoch  19 Batch  478 / 525  Training Loss  0.01451235730201006\n",
            "Epoch  19 Batch  479 / 525  Training Loss  0.006627972237765789\n",
            "Epoch  19 Batch  480 / 525  Training Loss  0.008341467007994652\n",
            "Epoch  19 Batch  481 / 525  Training Loss  0.013964300975203514\n",
            "Epoch  19 Batch  482 / 525  Training Loss  0.009913543239235878\n",
            "Epoch  19 Batch  483 / 525  Training Loss  0.0032162766437977552\n",
            "Epoch  19 Batch  484 / 525  Training Loss  0.005275305826216936\n",
            "Epoch  19 Batch  485 / 525  Training Loss  0.00406437274068594\n",
            "Epoch  19 Batch  486 / 525  Training Loss  0.002362702274695039\n",
            "Epoch  19 Batch  487 / 525  Training Loss  0.005951958708465099\n",
            "Epoch  19 Batch  488 / 525  Training Loss  0.0035825041122734547\n",
            "Epoch  19 Batch  489 / 525  Training Loss  0.002057833131402731\n",
            "Epoch  19 Batch  490 / 525  Training Loss  0.006711094174534082\n",
            "Epoch  19 Batch  491 / 525  Training Loss  0.003893408225849271\n",
            "Epoch  19 Batch  492 / 525  Training Loss  0.007007176522165537\n",
            "Epoch  19 Batch  493 / 525  Training Loss  0.0215486828237772\n",
            "Epoch  19 Batch  494 / 525  Training Loss  0.01044983696192503\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  19 Batch  495 / 525  Training Loss  0.00709266122430563\n",
            "Epoch  19 Batch  496 / 525  Training Loss  0.0035798526369035244\n",
            "Epoch  19 Batch  497 / 525  Training Loss  0.004079241305589676\n",
            "Epoch  19 Batch  498 / 525  Training Loss  0.003141069319099188\n",
            "Epoch  19 Batch  499 / 525  Training Loss  0.005562533624470234\n",
            "Epoch  19 Batch  500 / 525  Training Loss  0.01077490858733654\n",
            "Epoch  19 Batch  501 / 525  Training Loss  0.0061047752387821674\n",
            "Epoch  19 Batch  502 / 525  Training Loss  0.0029012912418693304\n",
            "Epoch  19 Batch  503 / 525  Training Loss  0.007247384637594223\n",
            "Epoch  19 Batch  504 / 525  Training Loss  0.011345776729285717\n",
            "Epoch  19 Batch  505 / 525  Training Loss  0.0027329889126122\n",
            "Epoch  19 Batch  506 / 525  Training Loss  0.0035082059912383556\n",
            "Epoch  19 Batch  507 / 525  Training Loss  0.004445962607860565\n",
            "Epoch  19 Batch  508 / 525  Training Loss  0.01249321736395359\n",
            "Epoch  19 Batch  509 / 525  Training Loss  0.005174748133867979\n",
            "Epoch  19 Batch  510 / 525  Training Loss  0.002012524288147688\n",
            "Epoch  19 Batch  511 / 525  Training Loss  0.009559383615851402\n",
            "Epoch  19 Batch  512 / 525  Training Loss  0.002975861541926861\n",
            "Epoch  19 Batch  513 / 525  Training Loss  0.0054664211347699165\n",
            "Epoch  19 Batch  514 / 525  Training Loss  0.006093939300626516\n",
            "Epoch  19 Batch  515 / 525  Training Loss  0.006541158072650433\n",
            "Epoch  19 Batch  516 / 525  Training Loss  0.0062020691111683846\n",
            "Epoch  19 Batch  517 / 525  Training Loss  0.004885400179773569\n",
            "Epoch  19 Batch  518 / 525  Training Loss  0.0028729611076414585\n",
            "Epoch  19 Batch  519 / 525  Training Loss  0.0025948488619178534\n",
            "Epoch  19 Batch  520 / 525  Training Loss  0.006359443999826908\n",
            "Epoch  19 Batch  521 / 525  Training Loss  0.00686123501509428\n",
            "Epoch  19 Batch  522 / 525  Training Loss  0.006600984372198582\n",
            "Epoch  19 Batch  523 / 525  Training Loss  0.00497314240783453\n",
            "Epoch  19 Batch  524 / 525  Training Loss  0.0040333932265639305\n",
            "  20    |    -    |   0.005247   | 58.700000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 20\n",
            "Epoch  20 Batch  0 / 525  Training Loss  0.00539611978456378\n",
            "Epoch  20 Batch  1 / 525  Training Loss  0.00679552648216486\n",
            "Epoch  20 Batch  2 / 525  Training Loss  0.006773094646632671\n",
            "Epoch  20 Batch  3 / 525  Training Loss  0.0021445094607770443\n",
            "Epoch  20 Batch  4 / 525  Training Loss  0.0016467297682538629\n",
            "Epoch  20 Batch  5 / 525  Training Loss  0.0028301149141043425\n",
            "Epoch  20 Batch  6 / 525  Training Loss  0.005699724890291691\n",
            "Epoch  20 Batch  7 / 525  Training Loss  0.004631247837096453\n",
            "Epoch  20 Batch  8 / 525  Training Loss  0.00825000461190939\n",
            "Epoch  20 Batch  9 / 525  Training Loss  0.022683657705783844\n",
            "Epoch  20 Batch  10 / 525  Training Loss  0.009741977788507938\n",
            "Epoch  20 Batch  11 / 525  Training Loss  0.008023101836442947\n",
            "Epoch  20 Batch  12 / 525  Training Loss  0.004449787549674511\n",
            "Epoch  20 Batch  13 / 525  Training Loss  0.0035381135530769825\n",
            "Epoch  20 Batch  14 / 525  Training Loss  0.0044074710458517075\n",
            "Epoch  20 Batch  15 / 525  Training Loss  0.0018582635093480349\n",
            "Epoch  20 Batch  16 / 525  Training Loss  0.012252571061253548\n",
            "Epoch  20 Batch  17 / 525  Training Loss  0.005057238973677158\n",
            "Epoch  20 Batch  18 / 525  Training Loss  0.0027160351164638996\n",
            "Epoch  20 Batch  19 / 525  Training Loss  0.004268527030944824\n",
            "Epoch  20 Batch  20 / 525  Training Loss  0.0016624906565994024\n",
            "Epoch  20 Batch  21 / 525  Training Loss  0.004567903932183981\n",
            "Epoch  20 Batch  22 / 525  Training Loss  0.004051185213029385\n",
            "Epoch  20 Batch  23 / 525  Training Loss  0.011618122458457947\n",
            "Epoch  20 Batch  24 / 525  Training Loss  0.0021201004274189472\n",
            "Epoch  20 Batch  25 / 525  Training Loss  0.017398815602064133\n",
            "Epoch  20 Batch  26 / 525  Training Loss  0.0053121005184948444\n",
            "Epoch  20 Batch  27 / 525  Training Loss  0.0029633385129272938\n",
            "Epoch  20 Batch  28 / 525  Training Loss  0.003031852189451456\n",
            "Epoch  20 Batch  29 / 525  Training Loss  0.013412468135356903\n",
            "Epoch  20 Batch  30 / 525  Training Loss  0.012045553885400295\n",
            "Epoch  20 Batch  31 / 525  Training Loss  0.004571974277496338\n",
            "Epoch  20 Batch  32 / 525  Training Loss  0.012186713516712189\n",
            "Epoch  20 Batch  33 / 525  Training Loss  0.0025262213312089443\n",
            "Epoch  20 Batch  34 / 525  Training Loss  0.009312931448221207\n",
            "Epoch  20 Batch  35 / 525  Training Loss  0.004778483882546425\n",
            "Epoch  20 Batch  36 / 525  Training Loss  0.0056944903917610645\n",
            "Epoch  20 Batch  37 / 525  Training Loss  0.01338844932615757\n",
            "Epoch  20 Batch  38 / 525  Training Loss  0.0011597198899835348\n",
            "Epoch  20 Batch  39 / 525  Training Loss  0.0013142123352736235\n",
            "Epoch  20 Batch  40 / 525  Training Loss  0.0014570655766874552\n",
            "Epoch  20 Batch  41 / 525  Training Loss  0.0037960060872137547\n",
            "Epoch  20 Batch  42 / 525  Training Loss  0.003953880630433559\n",
            "Epoch  20 Batch  43 / 525  Training Loss  0.0038517643697559834\n",
            "Epoch  20 Batch  44 / 525  Training Loss  0.0018652162980288267\n",
            "Epoch  20 Batch  45 / 525  Training Loss  0.002368119079619646\n",
            "Epoch  20 Batch  46 / 525  Training Loss  0.0011072477791458368\n",
            "Epoch  20 Batch  47 / 525  Training Loss  0.0014095937367528677\n",
            "Epoch  20 Batch  48 / 525  Training Loss  0.0022272015921771526\n",
            "Epoch  20 Batch  49 / 525  Training Loss  0.003127229865640402\n",
            "Epoch  20 Batch  50 / 525  Training Loss  0.003848282154649496\n",
            "Epoch  20 Batch  51 / 525  Training Loss  0.016207922250032425\n",
            "Epoch  20 Batch  52 / 525  Training Loss  0.010494998656213284\n",
            "Epoch  20 Batch  53 / 525  Training Loss  0.002430420368909836\n",
            "Epoch  20 Batch  54 / 525  Training Loss  0.007342448923736811\n",
            "Epoch  20 Batch  55 / 525  Training Loss  0.012069927528500557\n",
            "Epoch  20 Batch  56 / 525  Training Loss  0.003756975755095482\n",
            "Epoch  20 Batch  57 / 525  Training Loss  0.0011863743420690298\n",
            "Epoch  20 Batch  58 / 525  Training Loss  0.005901440046727657\n",
            "Epoch  20 Batch  59 / 525  Training Loss  0.0009522375767119229\n",
            "Epoch  20 Batch  60 / 525  Training Loss  0.002346742432564497\n",
            "Epoch  20 Batch  61 / 525  Training Loss  0.0021120025776326656\n",
            "Epoch  20 Batch  62 / 525  Training Loss  0.005779179744422436\n",
            "Epoch  20 Batch  63 / 525  Training Loss  0.0012511733220890164\n",
            "Epoch  20 Batch  64 / 525  Training Loss  0.007942642085254192\n",
            "Epoch  20 Batch  65 / 525  Training Loss  0.0013315797550603747\n",
            "Epoch  20 Batch  66 / 525  Training Loss  0.004391875118017197\n",
            "Epoch  20 Batch  67 / 525  Training Loss  0.002397013595327735\n",
            "Epoch  20 Batch  68 / 525  Training Loss  0.0022509940899908543\n",
            "Epoch  20 Batch  69 / 525  Training Loss  0.0029412475414574146\n",
            "Epoch  20 Batch  70 / 525  Training Loss  0.004454714711755514\n",
            "Epoch  20 Batch  71 / 525  Training Loss  0.0009763228590600193\n",
            "Epoch  20 Batch  72 / 525  Training Loss  0.003852151334285736\n",
            "Epoch  20 Batch  73 / 525  Training Loss  0.0026397849433124065\n",
            "Epoch  20 Batch  74 / 525  Training Loss  0.0035001314245164394\n",
            "Epoch  20 Batch  75 / 525  Training Loss  0.0023957150988280773\n",
            "Epoch  20 Batch  76 / 525  Training Loss  0.007059148512780666\n",
            "Epoch  20 Batch  77 / 525  Training Loss  0.0048176804557442665\n",
            "Epoch  20 Batch  78 / 525  Training Loss  0.001836805255152285\n",
            "Epoch  20 Batch  79 / 525  Training Loss  0.002291616750881076\n",
            "Epoch  20 Batch  80 / 525  Training Loss  0.006525064818561077\n",
            "Epoch  20 Batch  81 / 525  Training Loss  0.007592475973069668\n",
            "Epoch  20 Batch  82 / 525  Training Loss  0.013467863202095032\n",
            "Epoch  20 Batch  83 / 525  Training Loss  0.008747415617108345\n",
            "Epoch  20 Batch  84 / 525  Training Loss  0.0023939248640090227\n",
            "Epoch  20 Batch  85 / 525  Training Loss  0.0059699686244130135\n",
            "Epoch  20 Batch  86 / 525  Training Loss  0.001897577429190278\n",
            "Epoch  20 Batch  87 / 525  Training Loss  0.009050741791725159\n",
            "Epoch  20 Batch  88 / 525  Training Loss  0.003646392375230789\n",
            "Epoch  20 Batch  89 / 525  Training Loss  0.006323643960058689\n",
            "Epoch  20 Batch  90 / 525  Training Loss  0.001157364808022976\n",
            "Epoch  20 Batch  91 / 525  Training Loss  0.004120582714676857\n",
            "Epoch  20 Batch  92 / 525  Training Loss  0.006456018425524235\n",
            "Epoch  20 Batch  93 / 525  Training Loss  0.002184312790632248\n",
            "Epoch  20 Batch  94 / 525  Training Loss  0.002343847882002592\n",
            "Epoch  20 Batch  95 / 525  Training Loss  0.000736609916202724\n",
            "Epoch  20 Batch  96 / 525  Training Loss  0.0014623000752180815\n",
            "Epoch  20 Batch  97 / 525  Training Loss  0.007499237544834614\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  20 Batch  98 / 525  Training Loss  0.0032678123097866774\n",
            "Epoch  20 Batch  99 / 525  Training Loss  0.0013534983154386282\n",
            "Epoch  20 Batch  100 / 525  Training Loss  0.0028373231180012226\n",
            "Epoch  20 Batch  101 / 525  Training Loss  0.004273704718798399\n",
            "Epoch  20 Batch  102 / 525  Training Loss  0.0037389807403087616\n",
            "Epoch  20 Batch  103 / 525  Training Loss  0.00893828459084034\n",
            "Epoch  20 Batch  104 / 525  Training Loss  0.009105154313147068\n",
            "Epoch  20 Batch  105 / 525  Training Loss  0.001954158768057823\n",
            "Epoch  20 Batch  106 / 525  Training Loss  0.004044864792376757\n",
            "Epoch  20 Batch  107 / 525  Training Loss  0.004106762353330851\n",
            "Epoch  20 Batch  108 / 525  Training Loss  0.0031434218399226665\n",
            "Epoch  20 Batch  109 / 525  Training Loss  0.004272335674613714\n",
            "Epoch  20 Batch  110 / 525  Training Loss  0.0036185644567012787\n",
            "Epoch  20 Batch  111 / 525  Training Loss  0.002763258758932352\n",
            "Epoch  20 Batch  112 / 525  Training Loss  0.0027188314124941826\n",
            "Epoch  20 Batch  113 / 525  Training Loss  0.0034914438147097826\n",
            "Epoch  20 Batch  114 / 525  Training Loss  0.009887500666081905\n",
            "Epoch  20 Batch  115 / 525  Training Loss  0.002741460222750902\n",
            "Epoch  20 Batch  116 / 525  Training Loss  0.0024566343054175377\n",
            "Epoch  20 Batch  117 / 525  Training Loss  0.004219792317599058\n",
            "Epoch  20 Batch  118 / 525  Training Loss  0.009167450480163097\n",
            "Epoch  20 Batch  119 / 525  Training Loss  0.002798905363306403\n",
            "Epoch  20 Batch  120 / 525  Training Loss  0.001985863782465458\n",
            "Epoch  20 Batch  121 / 525  Training Loss  0.0028676032088696957\n",
            "Epoch  20 Batch  122 / 525  Training Loss  0.0014573469525203109\n",
            "Epoch  20 Batch  123 / 525  Training Loss  0.005629026331007481\n",
            "Epoch  20 Batch  124 / 525  Training Loss  0.0018362931441515684\n",
            "Epoch  20 Batch  125 / 525  Training Loss  0.0029681497253477573\n",
            "Epoch  20 Batch  126 / 525  Training Loss  0.0016680419212207198\n",
            "Epoch  20 Batch  127 / 525  Training Loss  0.0014674717094749212\n",
            "Epoch  20 Batch  128 / 525  Training Loss  0.0013647868763655424\n",
            "Epoch  20 Batch  129 / 525  Training Loss  0.012090140953660011\n",
            "Epoch  20 Batch  130 / 525  Training Loss  0.0035865064710378647\n",
            "Epoch  20 Batch  131 / 525  Training Loss  0.000754422158934176\n",
            "Epoch  20 Batch  132 / 525  Training Loss  0.000867878261487931\n",
            "Epoch  20 Batch  133 / 525  Training Loss  0.011561352759599686\n",
            "Epoch  20 Batch  134 / 525  Training Loss  0.004788445308804512\n",
            "Epoch  20 Batch  135 / 525  Training Loss  0.003427244024351239\n",
            "Epoch  20 Batch  136 / 525  Training Loss  0.008401123806834221\n",
            "Epoch  20 Batch  137 / 525  Training Loss  0.00865607988089323\n",
            "Epoch  20 Batch  138 / 525  Training Loss  0.005292989779263735\n",
            "Epoch  20 Batch  139 / 525  Training Loss  0.0026561773847788572\n",
            "Epoch  20 Batch  140 / 525  Training Loss  0.012448490597307682\n",
            "Epoch  20 Batch  141 / 525  Training Loss  0.008817399851977825\n",
            "Epoch  20 Batch  142 / 525  Training Loss  0.002269966993480921\n",
            "Epoch  20 Batch  143 / 525  Training Loss  0.0016996593913063407\n",
            "Epoch  20 Batch  144 / 525  Training Loss  0.002011756645515561\n",
            "Epoch  20 Batch  145 / 525  Training Loss  0.004320909734815359\n",
            "Epoch  20 Batch  146 / 525  Training Loss  0.0017157674301415682\n",
            "Epoch  20 Batch  147 / 525  Training Loss  0.0037792450748384\n",
            "Epoch  20 Batch  148 / 525  Training Loss  0.0030613357666879892\n",
            "Epoch  20 Batch  149 / 525  Training Loss  0.00640362873673439\n",
            "Epoch  20 Batch  150 / 525  Training Loss  0.0032941042445600033\n",
            "Epoch  20 Batch  151 / 525  Training Loss  0.001550828805193305\n",
            "Epoch  20 Batch  152 / 525  Training Loss  0.0037376254331320524\n",
            "Epoch  20 Batch  153 / 525  Training Loss  0.00977161806076765\n",
            "Epoch  20 Batch  154 / 525  Training Loss  0.0029277929570525885\n",
            "Epoch  20 Batch  155 / 525  Training Loss  0.006453420035541058\n",
            "Epoch  20 Batch  156 / 525  Training Loss  0.005830977577716112\n",
            "Epoch  20 Batch  157 / 525  Training Loss  0.0026831829454749823\n",
            "Epoch  20 Batch  158 / 525  Training Loss  0.004747550003230572\n",
            "Epoch  20 Batch  159 / 525  Training Loss  0.005342529155313969\n",
            "Epoch  20 Batch  160 / 525  Training Loss  0.008544426411390305\n",
            "Epoch  20 Batch  161 / 525  Training Loss  0.006342602428048849\n",
            "Epoch  20 Batch  162 / 525  Training Loss  0.004166824277490377\n",
            "Epoch  20 Batch  163 / 525  Training Loss  0.002618916565552354\n",
            "Epoch  20 Batch  164 / 525  Training Loss  0.0029567915480583906\n",
            "Epoch  20 Batch  165 / 525  Training Loss  0.003360278205946088\n",
            "Epoch  20 Batch  166 / 525  Training Loss  0.0017349779373034835\n",
            "Epoch  20 Batch  167 / 525  Training Loss  0.002726241946220398\n",
            "Epoch  20 Batch  168 / 525  Training Loss  0.003971494734287262\n",
            "Epoch  20 Batch  169 / 525  Training Loss  0.006601785309612751\n",
            "Epoch  20 Batch  170 / 525  Training Loss  0.007081881165504456\n",
            "Epoch  20 Batch  171 / 525  Training Loss  0.0028442651964724064\n",
            "Epoch  20 Batch  172 / 525  Training Loss  0.0019805142655968666\n",
            "Epoch  20 Batch  173 / 525  Training Loss  0.004717448726296425\n",
            "Epoch  20 Batch  174 / 525  Training Loss  0.004105809610337019\n",
            "Epoch  20 Batch  175 / 525  Training Loss  0.0035446814727038145\n",
            "Epoch  20 Batch  176 / 525  Training Loss  0.0038465335965156555\n",
            "Epoch  20 Batch  177 / 525  Training Loss  0.005670429207384586\n",
            "Epoch  20 Batch  178 / 525  Training Loss  0.00544769736006856\n",
            "Epoch  20 Batch  179 / 525  Training Loss  0.004107545595616102\n",
            "Epoch  20 Batch  180 / 525  Training Loss  0.0030494360253214836\n",
            "Epoch  20 Batch  181 / 525  Training Loss  0.001704733120277524\n",
            "Epoch  20 Batch  182 / 525  Training Loss  0.002181549556553364\n",
            "Epoch  20 Batch  183 / 525  Training Loss  0.002213291358202696\n",
            "Epoch  20 Batch  184 / 525  Training Loss  0.00385674019344151\n",
            "Epoch  20 Batch  185 / 525  Training Loss  0.0029441507067531347\n",
            "Epoch  20 Batch  186 / 525  Training Loss  0.005401909351348877\n",
            "Epoch  20 Batch  187 / 525  Training Loss  0.0030158162117004395\n",
            "Epoch  20 Batch  188 / 525  Training Loss  0.0033706631511449814\n",
            "Epoch  20 Batch  189 / 525  Training Loss  0.005420565139502287\n",
            "Epoch  20 Batch  190 / 525  Training Loss  0.013084940612316132\n",
            "Epoch  20 Batch  191 / 525  Training Loss  0.011442193761467934\n",
            "Epoch  20 Batch  192 / 525  Training Loss  0.009461050853133202\n",
            "Epoch  20 Batch  193 / 525  Training Loss  0.00414025504142046\n",
            "Epoch  20 Batch  194 / 525  Training Loss  0.0038627583999186754\n",
            "Epoch  20 Batch  195 / 525  Training Loss  0.00418411148712039\n",
            "Epoch  20 Batch  196 / 525  Training Loss  0.009456472471356392\n",
            "Epoch  20 Batch  197 / 525  Training Loss  0.004790258593857288\n",
            "Epoch  20 Batch  198 / 525  Training Loss  0.010988232679665089\n",
            "Epoch  20 Batch  199 / 525  Training Loss  0.0060839238576591015\n",
            "Epoch  20 Batch  200 / 525  Training Loss  0.01013057678937912\n",
            "Epoch  20 Batch  201 / 525  Training Loss  0.0020448637660592794\n",
            "Epoch  20 Batch  202 / 525  Training Loss  0.005535642150789499\n",
            "Epoch  20 Batch  203 / 525  Training Loss  0.004757932852953672\n",
            "Epoch  20 Batch  204 / 525  Training Loss  0.007297406904399395\n",
            "Epoch  20 Batch  205 / 525  Training Loss  0.00501553900539875\n",
            "Epoch  20 Batch  206 / 525  Training Loss  0.004662280436605215\n",
            "Epoch  20 Batch  207 / 525  Training Loss  0.0059593552723526955\n",
            "Epoch  20 Batch  208 / 525  Training Loss  0.002312503755092621\n",
            "Epoch  20 Batch  209 / 525  Training Loss  0.005469269119203091\n",
            "Epoch  20 Batch  210 / 525  Training Loss  0.006507118232548237\n",
            "Epoch  20 Batch  211 / 525  Training Loss  0.0054663559421896935\n",
            "Epoch  20 Batch  212 / 525  Training Loss  0.003820318030193448\n",
            "Epoch  20 Batch  213 / 525  Training Loss  0.0027708932757377625\n",
            "Epoch  20 Batch  214 / 525  Training Loss  0.0030534544494003057\n",
            "Epoch  20 Batch  215 / 525  Training Loss  0.0037795542739331722\n",
            "Epoch  20 Batch  216 / 525  Training Loss  0.0012240544892847538\n",
            "Epoch  20 Batch  217 / 525  Training Loss  0.0020484570413827896\n",
            "Epoch  20 Batch  218 / 525  Training Loss  0.004531771410256624\n",
            "Epoch  20 Batch  219 / 525  Training Loss  0.003944683820009232\n",
            "Epoch  20 Batch  220 / 525  Training Loss  0.0038664997555315495\n",
            "Epoch  20 Batch  221 / 525  Training Loss  0.00405879458412528\n",
            "Epoch  20 Batch  222 / 525  Training Loss  0.0041391244158148766\n",
            "Epoch  20 Batch  223 / 525  Training Loss  0.005745754111558199\n",
            "Epoch  20 Batch  224 / 525  Training Loss  0.004516866523772478\n",
            "Epoch  20 Batch  225 / 525  Training Loss  0.003209418151527643\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  20 Batch  226 / 525  Training Loss  0.005459901411086321\n",
            "Epoch  20 Batch  227 / 525  Training Loss  0.008101249113678932\n",
            "Epoch  20 Batch  228 / 525  Training Loss  0.002356127602979541\n",
            "Epoch  20 Batch  229 / 525  Training Loss  0.004222770221531391\n",
            "Epoch  20 Batch  230 / 525  Training Loss  0.002484448254108429\n",
            "Epoch  20 Batch  231 / 525  Training Loss  0.0038866691756993532\n",
            "Epoch  20 Batch  232 / 525  Training Loss  0.003664296818897128\n",
            "Epoch  20 Batch  233 / 525  Training Loss  0.005350061692297459\n",
            "Epoch  20 Batch  234 / 525  Training Loss  0.002621654886752367\n",
            "Epoch  20 Batch  235 / 525  Training Loss  0.0038646485190838575\n",
            "Epoch  20 Batch  236 / 525  Training Loss  0.0041099777445197105\n",
            "Epoch  20 Batch  237 / 525  Training Loss  0.0006903590401634574\n",
            "Epoch  20 Batch  238 / 525  Training Loss  0.0011454970808699727\n",
            "Epoch  20 Batch  239 / 525  Training Loss  0.005535799078643322\n",
            "Epoch  20 Batch  240 / 525  Training Loss  0.0023052364122122526\n",
            "Epoch  20 Batch  241 / 525  Training Loss  0.0032734288834035397\n",
            "Epoch  20 Batch  242 / 525  Training Loss  0.0026496178470551968\n",
            "Epoch  20 Batch  243 / 525  Training Loss  0.005700360052287579\n",
            "Epoch  20 Batch  244 / 525  Training Loss  0.01165239792317152\n",
            "Epoch  20 Batch  245 / 525  Training Loss  0.0023393009323626757\n",
            "Epoch  20 Batch  246 / 525  Training Loss  0.004069575108587742\n",
            "Epoch  20 Batch  247 / 525  Training Loss  0.0026341849006712437\n",
            "Epoch  20 Batch  248 / 525  Training Loss  0.007129550911486149\n",
            "Epoch  20 Batch  249 / 525  Training Loss  0.004340686369687319\n",
            "Epoch  20 Batch  250 / 525  Training Loss  0.023048661649227142\n",
            "Epoch  20 Batch  251 / 525  Training Loss  0.010225523263216019\n",
            "Epoch  20 Batch  252 / 525  Training Loss  0.0058343843556940556\n",
            "Epoch  20 Batch  253 / 525  Training Loss  0.0052346098236739635\n",
            "Epoch  20 Batch  254 / 525  Training Loss  0.020404033362865448\n",
            "Epoch  20 Batch  255 / 525  Training Loss  0.0024047628976404667\n",
            "Epoch  20 Batch  256 / 525  Training Loss  0.008840287104249\n",
            "Epoch  20 Batch  257 / 525  Training Loss  0.004293432924896479\n",
            "Epoch  20 Batch  258 / 525  Training Loss  0.007530075963586569\n",
            "Epoch  20 Batch  259 / 525  Training Loss  0.00492905592545867\n",
            "Epoch  20 Batch  260 / 525  Training Loss  0.001493997173383832\n",
            "Epoch  20 Batch  261 / 525  Training Loss  0.0019895494915544987\n",
            "Epoch  20 Batch  262 / 525  Training Loss  0.0024880121927708387\n",
            "Epoch  20 Batch  263 / 525  Training Loss  0.009979868307709694\n",
            "Epoch  20 Batch  264 / 525  Training Loss  0.0019580882508307695\n",
            "Epoch  20 Batch  265 / 525  Training Loss  0.010405536741018295\n",
            "Epoch  20 Batch  266 / 525  Training Loss  0.00223939074203372\n",
            "Epoch  20 Batch  267 / 525  Training Loss  0.005552508868277073\n",
            "Epoch  20 Batch  268 / 525  Training Loss  0.0021017969120293856\n",
            "Epoch  20 Batch  269 / 525  Training Loss  0.0029556823428720236\n",
            "Epoch  20 Batch  270 / 525  Training Loss  0.0019907623063772917\n",
            "Epoch  20 Batch  271 / 525  Training Loss  0.004533541388809681\n",
            "Epoch  20 Batch  272 / 525  Training Loss  0.0014054279308766127\n",
            "Epoch  20 Batch  273 / 525  Training Loss  0.004216431640088558\n",
            "Epoch  20 Batch  274 / 525  Training Loss  0.00697963684797287\n",
            "Epoch  20 Batch  275 / 525  Training Loss  0.004354267381131649\n",
            "Epoch  20 Batch  276 / 525  Training Loss  0.005673185922205448\n",
            "Epoch  20 Batch  277 / 525  Training Loss  0.013209424912929535\n",
            "Epoch  20 Batch  278 / 525  Training Loss  0.006444394588470459\n",
            "Epoch  20 Batch  279 / 525  Training Loss  0.014006453566253185\n",
            "Epoch  20 Batch  280 / 525  Training Loss  0.004764707759022713\n",
            "Epoch  20 Batch  281 / 525  Training Loss  0.0028876890428364277\n",
            "Epoch  20 Batch  282 / 525  Training Loss  0.0017910415772348642\n",
            "Epoch  20 Batch  283 / 525  Training Loss  0.0029719911981374025\n",
            "Epoch  20 Batch  284 / 525  Training Loss  0.004019724205136299\n",
            "Epoch  20 Batch  285 / 525  Training Loss  0.005850187968462706\n",
            "Epoch  20 Batch  286 / 525  Training Loss  0.0018833575304597616\n",
            "Epoch  20 Batch  287 / 525  Training Loss  0.0042105489410459995\n",
            "Epoch  20 Batch  288 / 525  Training Loss  0.003420429304242134\n",
            "Epoch  20 Batch  289 / 525  Training Loss  0.0032338444143533707\n",
            "Epoch  20 Batch  290 / 525  Training Loss  0.005145175848156214\n",
            "Epoch  20 Batch  291 / 525  Training Loss  0.004627052694559097\n",
            "Epoch  20 Batch  292 / 525  Training Loss  0.0037607294507324696\n",
            "Epoch  20 Batch  293 / 525  Training Loss  0.006474591791629791\n",
            "Epoch  20 Batch  294 / 525  Training Loss  0.00862495694309473\n",
            "Epoch  20 Batch  295 / 525  Training Loss  0.010087983682751656\n",
            "Epoch  20 Batch  296 / 525  Training Loss  0.013544229790568352\n",
            "Epoch  20 Batch  297 / 525  Training Loss  0.014784055761992931\n",
            "Epoch  20 Batch  298 / 525  Training Loss  0.008224530145525932\n",
            "Epoch  20 Batch  299 / 525  Training Loss  0.003468174021691084\n",
            "Epoch  20 Batch  300 / 525  Training Loss  0.021394604817032814\n",
            "Epoch  20 Batch  301 / 525  Training Loss  0.005125884898006916\n",
            "Epoch  20 Batch  302 / 525  Training Loss  0.01419870089739561\n",
            "Epoch  20 Batch  303 / 525  Training Loss  0.0042306589893996716\n",
            "Epoch  20 Batch  304 / 525  Training Loss  0.007690434344112873\n",
            "Epoch  20 Batch  305 / 525  Training Loss  0.004655430093407631\n",
            "Epoch  20 Batch  306 / 525  Training Loss  0.007085442543029785\n",
            "Epoch  20 Batch  307 / 525  Training Loss  0.002215457148849964\n",
            "Epoch  20 Batch  308 / 525  Training Loss  0.006875202059745789\n",
            "Epoch  20 Batch  309 / 525  Training Loss  0.008748245425522327\n",
            "Epoch  20 Batch  310 / 525  Training Loss  0.004510894417762756\n",
            "Epoch  20 Batch  311 / 525  Training Loss  0.00723268324509263\n",
            "Epoch  20 Batch  312 / 525  Training Loss  0.001993223326280713\n",
            "Epoch  20 Batch  313 / 525  Training Loss  0.002099704695865512\n",
            "Epoch  20 Batch  314 / 525  Training Loss  0.0020820102654397488\n",
            "Epoch  20 Batch  315 / 525  Training Loss  0.006645196117460728\n",
            "Epoch  20 Batch  316 / 525  Training Loss  0.007592582143843174\n",
            "Epoch  20 Batch  317 / 525  Training Loss  0.005212407559156418\n",
            "Epoch  20 Batch  318 / 525  Training Loss  0.006514945533126593\n",
            "Epoch  20 Batch  319 / 525  Training Loss  0.0014821330551058054\n",
            "Epoch  20 Batch  320 / 525  Training Loss  0.014324034564197063\n",
            "Epoch  20 Batch  321 / 525  Training Loss  0.015240554697811604\n",
            "Epoch  20 Batch  322 / 525  Training Loss  0.02364339679479599\n",
            "Epoch  20 Batch  323 / 525  Training Loss  0.01779409497976303\n",
            "Epoch  20 Batch  324 / 525  Training Loss  0.0071900165639817715\n",
            "Epoch  20 Batch  325 / 525  Training Loss  0.018502842634916306\n",
            "Epoch  20 Batch  326 / 525  Training Loss  0.009764989838004112\n",
            "Epoch  20 Batch  327 / 525  Training Loss  0.007082242518663406\n",
            "Epoch  20 Batch  328 / 525  Training Loss  0.022050414234399796\n",
            "Epoch  20 Batch  329 / 525  Training Loss  0.003519328311085701\n",
            "Epoch  20 Batch  330 / 525  Training Loss  0.008842810988426208\n",
            "Epoch  20 Batch  331 / 525  Training Loss  0.0084391413256526\n",
            "Epoch  20 Batch  332 / 525  Training Loss  0.0027942568995058537\n",
            "Epoch  20 Batch  333 / 525  Training Loss  0.006969082169234753\n",
            "Epoch  20 Batch  334 / 525  Training Loss  0.004390588961541653\n",
            "Epoch  20 Batch  335 / 525  Training Loss  0.0024411077611148357\n",
            "Epoch  20 Batch  336 / 525  Training Loss  0.00540337897837162\n",
            "Epoch  20 Batch  337 / 525  Training Loss  0.008608502335846424\n",
            "Epoch  20 Batch  338 / 525  Training Loss  0.005167397670447826\n",
            "Epoch  20 Batch  339 / 525  Training Loss  0.001706084469333291\n",
            "Epoch  20 Batch  340 / 525  Training Loss  0.013610722497105598\n",
            "Epoch  20 Batch  341 / 525  Training Loss  0.011325071565806866\n",
            "Epoch  20 Batch  342 / 525  Training Loss  0.00996323674917221\n",
            "Epoch  20 Batch  343 / 525  Training Loss  0.006486176513135433\n",
            "Epoch  20 Batch  344 / 525  Training Loss  0.002997667295858264\n",
            "Epoch  20 Batch  345 / 525  Training Loss  0.011363277211785316\n",
            "Epoch  20 Batch  346 / 525  Training Loss  0.004053008742630482\n",
            "Epoch  20 Batch  347 / 525  Training Loss  0.016949286684393883\n",
            "Epoch  20 Batch  348 / 525  Training Loss  0.006465567741543055\n",
            "Epoch  20 Batch  349 / 525  Training Loss  0.005606635939329863\n",
            "Epoch  20 Batch  350 / 525  Training Loss  0.006343143992125988\n",
            "Epoch  20 Batch  351 / 525  Training Loss  0.014424296095967293\n",
            "Epoch  20 Batch  352 / 525  Training Loss  0.00530050415545702\n",
            "Epoch  20 Batch  353 / 525  Training Loss  0.003076172899454832\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  20 Batch  354 / 525  Training Loss  0.012301626615226269\n",
            "Epoch  20 Batch  355 / 525  Training Loss  0.01551318634301424\n",
            "Epoch  20 Batch  356 / 525  Training Loss  0.00259673735126853\n",
            "Epoch  20 Batch  357 / 525  Training Loss  0.009113093838095665\n",
            "Epoch  20 Batch  358 / 525  Training Loss  0.0050321826711297035\n",
            "Epoch  20 Batch  359 / 525  Training Loss  0.012483608908951283\n",
            "Epoch  20 Batch  360 / 525  Training Loss  0.005091369152069092\n",
            "Epoch  20 Batch  361 / 525  Training Loss  0.002306757029145956\n",
            "Epoch  20 Batch  362 / 525  Training Loss  0.005060323514044285\n",
            "Epoch  20 Batch  363 / 525  Training Loss  0.003798349993303418\n",
            "Epoch  20 Batch  364 / 525  Training Loss  0.009426115080714226\n",
            "Epoch  20 Batch  365 / 525  Training Loss  0.0027853650972247124\n",
            "Epoch  20 Batch  366 / 525  Training Loss  0.006280797533690929\n",
            "Epoch  20 Batch  367 / 525  Training Loss  0.00782007910311222\n",
            "Epoch  20 Batch  368 / 525  Training Loss  0.008227883838117123\n",
            "Epoch  20 Batch  369 / 525  Training Loss  0.003040642011910677\n",
            "Epoch  20 Batch  370 / 525  Training Loss  0.006229323334991932\n",
            "Epoch  20 Batch  371 / 525  Training Loss  0.006760678254067898\n",
            "Epoch  20 Batch  372 / 525  Training Loss  0.0037325345911085606\n",
            "Epoch  20 Batch  373 / 525  Training Loss  0.006036343984305859\n",
            "Epoch  20 Batch  374 / 525  Training Loss  0.00340215303003788\n",
            "Epoch  20 Batch  375 / 525  Training Loss  0.014499510638415813\n",
            "Epoch  20 Batch  376 / 525  Training Loss  0.010212310589849949\n",
            "Epoch  20 Batch  377 / 525  Training Loss  0.003066320437937975\n",
            "Epoch  20 Batch  378 / 525  Training Loss  0.006384381093084812\n",
            "Epoch  20 Batch  379 / 525  Training Loss  0.005581867881119251\n",
            "Epoch  20 Batch  380 / 525  Training Loss  0.016533084213733673\n",
            "Epoch  20 Batch  381 / 525  Training Loss  0.0097342012450099\n",
            "Epoch  20 Batch  382 / 525  Training Loss  0.00985078327357769\n",
            "Epoch  20 Batch  383 / 525  Training Loss  0.00527540035545826\n",
            "Epoch  20 Batch  384 / 525  Training Loss  0.0033968710340559483\n",
            "Epoch  20 Batch  385 / 525  Training Loss  0.004620910622179508\n",
            "Epoch  20 Batch  386 / 525  Training Loss  0.003082876792177558\n",
            "Epoch  20 Batch  387 / 525  Training Loss  0.0016546010738238692\n",
            "Epoch  20 Batch  388 / 525  Training Loss  0.005171851720660925\n",
            "Epoch  20 Batch  389 / 525  Training Loss  0.005755947437137365\n",
            "Epoch  20 Batch  390 / 525  Training Loss  0.01130762230604887\n",
            "Epoch  20 Batch  391 / 525  Training Loss  0.0044515980407595634\n",
            "Epoch  20 Batch  392 / 525  Training Loss  0.0016007503727450967\n",
            "Epoch  20 Batch  393 / 525  Training Loss  0.009343871846795082\n",
            "Epoch  20 Batch  394 / 525  Training Loss  0.002222364069893956\n",
            "Epoch  20 Batch  395 / 525  Training Loss  0.005102735478430986\n",
            "Epoch  20 Batch  396 / 525  Training Loss  0.005110251251608133\n",
            "Epoch  20 Batch  397 / 525  Training Loss  0.0029150687623769045\n",
            "Epoch  20 Batch  398 / 525  Training Loss  0.009204254485666752\n",
            "Epoch  20 Batch  399 / 525  Training Loss  0.008495406247675419\n",
            "Epoch  20 Batch  400 / 525  Training Loss  0.005242585204541683\n",
            "Epoch  20 Batch  401 / 525  Training Loss  0.007189235184341669\n",
            "Epoch  20 Batch  402 / 525  Training Loss  0.019271425902843475\n",
            "Epoch  20 Batch  403 / 525  Training Loss  0.0034574936144053936\n",
            "Epoch  20 Batch  404 / 525  Training Loss  0.002479210263118148\n",
            "Epoch  20 Batch  405 / 525  Training Loss  0.010266778990626335\n",
            "Epoch  20 Batch  406 / 525  Training Loss  0.004338768310844898\n",
            "Epoch  20 Batch  407 / 525  Training Loss  0.004132987465709448\n",
            "Epoch  20 Batch  408 / 525  Training Loss  0.008179130032658577\n",
            "Epoch  20 Batch  409 / 525  Training Loss  0.0026832683943212032\n",
            "Epoch  20 Batch  410 / 525  Training Loss  0.00588801596313715\n",
            "Epoch  20 Batch  411 / 525  Training Loss  0.016519267112016678\n",
            "Epoch  20 Batch  412 / 525  Training Loss  0.012363002635538578\n",
            "Epoch  20 Batch  413 / 525  Training Loss  0.004543927498161793\n",
            "Epoch  20 Batch  414 / 525  Training Loss  0.02390475571155548\n",
            "Epoch  20 Batch  415 / 525  Training Loss  0.0032388544641435146\n",
            "Epoch  20 Batch  416 / 525  Training Loss  0.0026134257204830647\n",
            "Epoch  20 Batch  417 / 525  Training Loss  0.006623597349971533\n",
            "Epoch  20 Batch  418 / 525  Training Loss  0.0038498644717037678\n",
            "Epoch  20 Batch  419 / 525  Training Loss  0.008376342244446278\n",
            "Epoch  20 Batch  420 / 525  Training Loss  0.004824761301279068\n",
            "Epoch  20 Batch  421 / 525  Training Loss  0.0036158431321382523\n",
            "Epoch  20 Batch  422 / 525  Training Loss  0.004070597235113382\n",
            "Epoch  20 Batch  423 / 525  Training Loss  0.006801155861467123\n",
            "Epoch  20 Batch  424 / 525  Training Loss  0.0027943220920860767\n",
            "Epoch  20 Batch  425 / 525  Training Loss  0.005854933988302946\n",
            "Epoch  20 Batch  426 / 525  Training Loss  0.004722720943391323\n",
            "Epoch  20 Batch  427 / 525  Training Loss  0.011044012382626534\n",
            "Epoch  20 Batch  428 / 525  Training Loss  0.0030150485690683126\n",
            "Epoch  20 Batch  429 / 525  Training Loss  0.016246872022747993\n",
            "Epoch  20 Batch  430 / 525  Training Loss  0.006825373973697424\n",
            "Epoch  20 Batch  431 / 525  Training Loss  0.003002262907102704\n",
            "Epoch  20 Batch  432 / 525  Training Loss  0.004320561420172453\n",
            "Epoch  20 Batch  433 / 525  Training Loss  0.0035424083471298218\n",
            "Epoch  20 Batch  434 / 525  Training Loss  0.004299873020499945\n",
            "Epoch  20 Batch  435 / 525  Training Loss  0.006001805420964956\n",
            "Epoch  20 Batch  436 / 525  Training Loss  0.009405584074556828\n",
            "Epoch  20 Batch  437 / 525  Training Loss  0.0025641277898103\n",
            "Epoch  20 Batch  438 / 525  Training Loss  0.0026686983183026314\n",
            "Epoch  20 Batch  439 / 525  Training Loss  0.0028661570977419615\n",
            "Epoch  20 Batch  440 / 525  Training Loss  0.008437678217887878\n",
            "Epoch  20 Batch  441 / 525  Training Loss  0.0036043464206159115\n",
            "Epoch  20 Batch  442 / 525  Training Loss  0.0017961416160687804\n",
            "Epoch  20 Batch  443 / 525  Training Loss  0.007302232086658478\n",
            "Epoch  20 Batch  444 / 525  Training Loss  0.003570245113223791\n",
            "Epoch  20 Batch  445 / 525  Training Loss  0.003949986305087805\n",
            "Epoch  20 Batch  446 / 525  Training Loss  0.0011391609441488981\n",
            "Epoch  20 Batch  447 / 525  Training Loss  0.003562040627002716\n",
            "Epoch  20 Batch  448 / 525  Training Loss  0.004417946096509695\n",
            "Epoch  20 Batch  449 / 525  Training Loss  0.010629848577082157\n",
            "Epoch  20 Batch  450 / 525  Training Loss  0.004097767639905214\n",
            "Epoch  20 Batch  451 / 525  Training Loss  0.0053425137884914875\n",
            "Epoch  20 Batch  452 / 525  Training Loss  0.005583841819316149\n",
            "Epoch  20 Batch  453 / 525  Training Loss  0.00497188325971365\n",
            "Epoch  20 Batch  454 / 525  Training Loss  0.0024217318277806044\n",
            "Epoch  20 Batch  455 / 525  Training Loss  0.0038554288912564516\n",
            "Epoch  20 Batch  456 / 525  Training Loss  0.0027736821211874485\n",
            "Epoch  20 Batch  457 / 525  Training Loss  0.0039037843234837055\n",
            "Epoch  20 Batch  458 / 525  Training Loss  0.002056244295090437\n",
            "Epoch  20 Batch  459 / 525  Training Loss  0.0015164443757385015\n",
            "Epoch  20 Batch  460 / 525  Training Loss  0.0041834041476249695\n",
            "Epoch  20 Batch  461 / 525  Training Loss  0.003713484387844801\n",
            "Epoch  20 Batch  462 / 525  Training Loss  0.0020887379068881273\n",
            "Epoch  20 Batch  463 / 525  Training Loss  0.005479681305587292\n",
            "Epoch  20 Batch  464 / 525  Training Loss  0.007945866324007511\n",
            "Epoch  20 Batch  465 / 525  Training Loss  0.005996427498757839\n",
            "Epoch  20 Batch  466 / 525  Training Loss  0.01218706276267767\n",
            "Epoch  20 Batch  467 / 525  Training Loss  0.005413758102804422\n",
            "Epoch  20 Batch  468 / 525  Training Loss  0.0034856931306421757\n",
            "Epoch  20 Batch  469 / 525  Training Loss  0.005033903755247593\n",
            "Epoch  20 Batch  470 / 525  Training Loss  0.0061374688521027565\n",
            "Epoch  20 Batch  471 / 525  Training Loss  0.005176365841180086\n",
            "Epoch  20 Batch  472 / 525  Training Loss  0.009449715726077557\n",
            "Epoch  20 Batch  473 / 525  Training Loss  0.004501719493418932\n",
            "Epoch  20 Batch  474 / 525  Training Loss  0.006080404855310917\n",
            "Epoch  20 Batch  475 / 525  Training Loss  0.004177385475486517\n",
            "Epoch  20 Batch  476 / 525  Training Loss  0.007558293640613556\n",
            "Epoch  20 Batch  477 / 525  Training Loss  0.003879767609760165\n",
            "Epoch  20 Batch  478 / 525  Training Loss  0.009675371460616589\n",
            "Epoch  20 Batch  479 / 525  Training Loss  0.007708079181611538\n",
            "Epoch  20 Batch  480 / 525  Training Loss  0.007233171723783016\n",
            "Epoch  20 Batch  481 / 525  Training Loss  0.01000272948294878\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  20 Batch  482 / 525  Training Loss  0.007022003643214703\n",
            "Epoch  20 Batch  483 / 525  Training Loss  0.010686129331588745\n",
            "Epoch  20 Batch  484 / 525  Training Loss  0.013308984227478504\n",
            "Epoch  20 Batch  485 / 525  Training Loss  0.002358147408813238\n",
            "Epoch  20 Batch  486 / 525  Training Loss  0.005989882163703442\n",
            "Epoch  20 Batch  487 / 525  Training Loss  0.009547209367156029\n",
            "Epoch  20 Batch  488 / 525  Training Loss  0.0071788146160542965\n",
            "Epoch  20 Batch  489 / 525  Training Loss  0.001810944639146328\n",
            "Epoch  20 Batch  490 / 525  Training Loss  0.0037112399004399776\n",
            "Epoch  20 Batch  491 / 525  Training Loss  0.005392597988247871\n",
            "Epoch  20 Batch  492 / 525  Training Loss  0.005419079214334488\n",
            "Epoch  20 Batch  493 / 525  Training Loss  0.011019639670848846\n",
            "Epoch  20 Batch  494 / 525  Training Loss  0.006442192010581493\n",
            "Epoch  20 Batch  495 / 525  Training Loss  0.00610115984454751\n",
            "Epoch  20 Batch  496 / 525  Training Loss  0.007907591760158539\n",
            "Epoch  20 Batch  497 / 525  Training Loss  0.004349180497229099\n",
            "Epoch  20 Batch  498 / 525  Training Loss  0.0034357719123363495\n",
            "Epoch  20 Batch  499 / 525  Training Loss  0.002861859742552042\n",
            "Epoch  20 Batch  500 / 525  Training Loss  0.0017083415295928717\n",
            "Epoch  20 Batch  501 / 525  Training Loss  0.006989260204136372\n",
            "Epoch  20 Batch  502 / 525  Training Loss  0.00649222219362855\n",
            "Epoch  20 Batch  503 / 525  Training Loss  0.009072871878743172\n",
            "Epoch  20 Batch  504 / 525  Training Loss  0.005447313655167818\n",
            "Epoch  20 Batch  505 / 525  Training Loss  0.0040760040283203125\n",
            "Epoch  20 Batch  506 / 525  Training Loss  0.006012520752847195\n",
            "Epoch  20 Batch  507 / 525  Training Loss  0.005534016061574221\n",
            "Epoch  20 Batch  508 / 525  Training Loss  0.0029238436836749315\n",
            "Epoch  20 Batch  509 / 525  Training Loss  0.00254291039891541\n",
            "Epoch  20 Batch  510 / 525  Training Loss  0.0020702837500721216\n",
            "Epoch  20 Batch  511 / 525  Training Loss  0.011221052147448063\n",
            "Epoch  20 Batch  512 / 525  Training Loss  0.008192336186766624\n",
            "Epoch  20 Batch  513 / 525  Training Loss  0.002348058857023716\n",
            "Epoch  20 Batch  514 / 525  Training Loss  0.004353589378297329\n",
            "Epoch  20 Batch  515 / 525  Training Loss  0.006424312945455313\n",
            "Epoch  20 Batch  516 / 525  Training Loss  0.005504596047103405\n",
            "Epoch  20 Batch  517 / 525  Training Loss  0.0035540114622563124\n",
            "Epoch  20 Batch  518 / 525  Training Loss  0.008395376615226269\n",
            "Epoch  20 Batch  519 / 525  Training Loss  0.009340918622910976\n",
            "Epoch  20 Batch  520 / 525  Training Loss  0.007124557159841061\n",
            "Epoch  20 Batch  521 / 525  Training Loss  0.017580553889274597\n",
            "Epoch  20 Batch  522 / 525  Training Loss  0.01885308139026165\n",
            "Epoch  20 Batch  523 / 525  Training Loss  0.005496094934642315\n",
            "Epoch  20 Batch  524 / 525  Training Loss  0.004795043729245663\n",
            "  21    |    -    |   0.005724   | 58.150000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 21\n",
            "Epoch  21 Batch  0 / 525  Training Loss  0.0050115096382796764\n",
            "Epoch  21 Batch  1 / 525  Training Loss  0.004413291346281767\n",
            "Epoch  21 Batch  2 / 525  Training Loss  0.002921136561781168\n",
            "Epoch  21 Batch  3 / 525  Training Loss  0.0021356726065278053\n",
            "Epoch  21 Batch  4 / 525  Training Loss  0.003843804821372032\n",
            "Epoch  21 Batch  5 / 525  Training Loss  0.006553771439939737\n",
            "Epoch  21 Batch  6 / 525  Training Loss  0.0030723237432539463\n",
            "Epoch  21 Batch  7 / 525  Training Loss  0.0028881714679300785\n",
            "Epoch  21 Batch  8 / 525  Training Loss  0.0033772990573197603\n",
            "Epoch  21 Batch  9 / 525  Training Loss  0.001205373671837151\n",
            "Epoch  21 Batch  10 / 525  Training Loss  0.0022745965979993343\n",
            "Epoch  21 Batch  11 / 525  Training Loss  0.0017068099696189165\n",
            "Epoch  21 Batch  12 / 525  Training Loss  0.0021745325066149235\n",
            "Epoch  21 Batch  13 / 525  Training Loss  0.00925616268068552\n",
            "Epoch  21 Batch  14 / 525  Training Loss  0.004682342056185007\n",
            "Epoch  21 Batch  15 / 525  Training Loss  0.0018596937879920006\n",
            "Epoch  21 Batch  16 / 525  Training Loss  0.0035604063887149096\n",
            "Epoch  21 Batch  17 / 525  Training Loss  0.004217431880533695\n",
            "Epoch  21 Batch  18 / 525  Training Loss  0.0018442177679389715\n",
            "Epoch  21 Batch  19 / 525  Training Loss  0.0026118026580661535\n",
            "Epoch  21 Batch  20 / 525  Training Loss  0.0017527714371681213\n",
            "Epoch  21 Batch  21 / 525  Training Loss  0.004465470556169748\n",
            "Epoch  21 Batch  22 / 525  Training Loss  0.00176998320966959\n",
            "Epoch  21 Batch  23 / 525  Training Loss  0.001046185614541173\n",
            "Epoch  21 Batch  24 / 525  Training Loss  0.0015865715686231852\n",
            "Epoch  21 Batch  25 / 525  Training Loss  0.005580129101872444\n",
            "Epoch  21 Batch  26 / 525  Training Loss  0.002865021349862218\n",
            "Epoch  21 Batch  27 / 525  Training Loss  0.0025512571446597576\n",
            "Epoch  21 Batch  28 / 525  Training Loss  0.0029668028000742197\n",
            "Epoch  21 Batch  29 / 525  Training Loss  0.0015370978508144617\n",
            "Epoch  21 Batch  30 / 525  Training Loss  0.0024823613930493593\n",
            "Epoch  21 Batch  31 / 525  Training Loss  0.002612787066027522\n",
            "Epoch  21 Batch  32 / 525  Training Loss  0.0018090646481141448\n",
            "Epoch  21 Batch  33 / 525  Training Loss  0.0033949906937777996\n",
            "Epoch  21 Batch  34 / 525  Training Loss  0.006562275346368551\n",
            "Epoch  21 Batch  35 / 525  Training Loss  0.001973888371139765\n",
            "Epoch  21 Batch  36 / 525  Training Loss  0.0012143119238317013\n",
            "Epoch  21 Batch  37 / 525  Training Loss  0.0019082579528912902\n",
            "Epoch  21 Batch  38 / 525  Training Loss  0.008433337323367596\n",
            "Epoch  21 Batch  39 / 525  Training Loss  0.001153254066593945\n",
            "Epoch  21 Batch  40 / 525  Training Loss  0.0006911146338097751\n",
            "Epoch  21 Batch  41 / 525  Training Loss  0.003771115094423294\n",
            "Epoch  21 Batch  42 / 525  Training Loss  0.0028682623524218798\n",
            "Epoch  21 Batch  43 / 525  Training Loss  0.002030027797445655\n",
            "Epoch  21 Batch  44 / 525  Training Loss  0.001571517437696457\n",
            "Epoch  21 Batch  45 / 525  Training Loss  0.003999998793005943\n",
            "Epoch  21 Batch  46 / 525  Training Loss  0.010134579613804817\n",
            "Epoch  21 Batch  47 / 525  Training Loss  0.004617250524461269\n",
            "Epoch  21 Batch  48 / 525  Training Loss  0.009386544115841389\n",
            "Epoch  21 Batch  49 / 525  Training Loss  0.001406775671057403\n",
            "Epoch  21 Batch  50 / 525  Training Loss  0.003906828351318836\n",
            "Epoch  21 Batch  51 / 525  Training Loss  0.003643087577074766\n",
            "Epoch  21 Batch  52 / 525  Training Loss  0.002260389272123575\n",
            "Epoch  21 Batch  53 / 525  Training Loss  0.0020706842187792063\n",
            "Epoch  21 Batch  54 / 525  Training Loss  0.003268454223871231\n",
            "Epoch  21 Batch  55 / 525  Training Loss  0.0026940940879285336\n",
            "Epoch  21 Batch  56 / 525  Training Loss  0.001183042419143021\n",
            "Epoch  21 Batch  57 / 525  Training Loss  0.002410437446087599\n",
            "Epoch  21 Batch  58 / 525  Training Loss  0.004869677592068911\n",
            "Epoch  21 Batch  59 / 525  Training Loss  0.00671907514333725\n",
            "Epoch  21 Batch  60 / 525  Training Loss  0.002206438686698675\n",
            "Epoch  21 Batch  61 / 525  Training Loss  0.0036903396248817444\n",
            "Epoch  21 Batch  62 / 525  Training Loss  0.01144577655941248\n",
            "Epoch  21 Batch  63 / 525  Training Loss  0.0016704195877537131\n",
            "Epoch  21 Batch  64 / 525  Training Loss  0.004524007439613342\n",
            "Epoch  21 Batch  65 / 525  Training Loss  0.0031266342848539352\n",
            "Epoch  21 Batch  66 / 525  Training Loss  0.004124780651181936\n",
            "Epoch  21 Batch  67 / 525  Training Loss  0.002718785312026739\n",
            "Epoch  21 Batch  68 / 525  Training Loss  0.007312617264688015\n",
            "Epoch  21 Batch  69 / 525  Training Loss  0.004232472740113735\n",
            "Epoch  21 Batch  70 / 525  Training Loss  0.0010338017018511891\n",
            "Epoch  21 Batch  71 / 525  Training Loss  0.0018427211325615644\n",
            "Epoch  21 Batch  72 / 525  Training Loss  0.007622706238180399\n",
            "Epoch  21 Batch  73 / 525  Training Loss  0.00811972189694643\n",
            "Epoch  21 Batch  74 / 525  Training Loss  0.0018176650628447533\n",
            "Epoch  21 Batch  75 / 525  Training Loss  0.003342159790918231\n",
            "Epoch  21 Batch  76 / 525  Training Loss  0.006869602017104626\n",
            "Epoch  21 Batch  77 / 525  Training Loss  0.0024509024806320667\n",
            "Epoch  21 Batch  78 / 525  Training Loss  0.0011927047744393349\n",
            "Epoch  21 Batch  79 / 525  Training Loss  0.0019046736415475607\n",
            "Epoch  21 Batch  80 / 525  Training Loss  0.0007030513952486217\n",
            "Epoch  21 Batch  81 / 525  Training Loss  0.0021574259735643864\n",
            "Epoch  21 Batch  82 / 525  Training Loss  0.005095941945910454\n",
            "Epoch  21 Batch  83 / 525  Training Loss  0.0017464548582211137\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  21 Batch  84 / 525  Training Loss  0.005159427411854267\n",
            "Epoch  21 Batch  85 / 525  Training Loss  0.003363264724612236\n",
            "Epoch  21 Batch  86 / 525  Training Loss  0.0037006493657827377\n",
            "Epoch  21 Batch  87 / 525  Training Loss  0.002507544355466962\n",
            "Epoch  21 Batch  88 / 525  Training Loss  0.01305824052542448\n",
            "Epoch  21 Batch  89 / 525  Training Loss  0.0058904895558953285\n",
            "Epoch  21 Batch  90 / 525  Training Loss  0.0027676234021782875\n",
            "Epoch  21 Batch  91 / 525  Training Loss  0.004308726638555527\n",
            "Epoch  21 Batch  92 / 525  Training Loss  0.010453948751091957\n",
            "Epoch  21 Batch  93 / 525  Training Loss  0.0040712556801736355\n",
            "Epoch  21 Batch  94 / 525  Training Loss  0.004988004919141531\n",
            "Epoch  21 Batch  95 / 525  Training Loss  0.002801988972350955\n",
            "Epoch  21 Batch  96 / 525  Training Loss  0.0019137237686663866\n",
            "Epoch  21 Batch  97 / 525  Training Loss  0.0033915142994374037\n",
            "Epoch  21 Batch  98 / 525  Training Loss  0.0019120967481285334\n",
            "Epoch  21 Batch  99 / 525  Training Loss  0.01026381179690361\n",
            "Epoch  21 Batch  100 / 525  Training Loss  0.01025361567735672\n",
            "Epoch  21 Batch  101 / 525  Training Loss  0.0030208223033696413\n",
            "Epoch  21 Batch  102 / 525  Training Loss  0.0024286333937197924\n",
            "Epoch  21 Batch  103 / 525  Training Loss  0.0025825249031186104\n",
            "Epoch  21 Batch  104 / 525  Training Loss  0.0014632849488407373\n",
            "Epoch  21 Batch  105 / 525  Training Loss  0.0015074091497808695\n",
            "Epoch  21 Batch  106 / 525  Training Loss  0.004547605291008949\n",
            "Epoch  21 Batch  107 / 525  Training Loss  0.001517999917268753\n",
            "Epoch  21 Batch  108 / 525  Training Loss  0.0049970620311796665\n",
            "Epoch  21 Batch  109 / 525  Training Loss  0.005416039377450943\n",
            "Epoch  21 Batch  110 / 525  Training Loss  0.0015130119863897562\n",
            "Epoch  21 Batch  111 / 525  Training Loss  0.006237241439521313\n",
            "Epoch  21 Batch  112 / 525  Training Loss  0.0033981171436607838\n",
            "Epoch  21 Batch  113 / 525  Training Loss  0.022542089223861694\n",
            "Epoch  21 Batch  114 / 525  Training Loss  0.0017406897386536002\n",
            "Epoch  21 Batch  115 / 525  Training Loss  0.003949628211557865\n",
            "Epoch  21 Batch  116 / 525  Training Loss  0.001717593870125711\n",
            "Epoch  21 Batch  117 / 525  Training Loss  0.00967396516352892\n",
            "Epoch  21 Batch  118 / 525  Training Loss  0.007200438529253006\n",
            "Epoch  21 Batch  119 / 525  Training Loss  0.00941559486091137\n",
            "Epoch  21 Batch  120 / 525  Training Loss  0.005922977346926928\n",
            "Epoch  21 Batch  121 / 525  Training Loss  0.005260644480586052\n",
            "Epoch  21 Batch  122 / 525  Training Loss  0.0014116386882960796\n",
            "Epoch  21 Batch  123 / 525  Training Loss  0.014453736133873463\n",
            "Epoch  21 Batch  124 / 525  Training Loss  0.00361266965046525\n",
            "Epoch  21 Batch  125 / 525  Training Loss  0.00887416023761034\n",
            "Epoch  21 Batch  126 / 525  Training Loss  0.0027494607493281364\n",
            "Epoch  21 Batch  127 / 525  Training Loss  0.008520158007740974\n",
            "Epoch  21 Batch  128 / 525  Training Loss  0.002952843438833952\n",
            "Epoch  21 Batch  129 / 525  Training Loss  0.0065270112827420235\n",
            "Epoch  21 Batch  130 / 525  Training Loss  0.003581128316000104\n",
            "Epoch  21 Batch  131 / 525  Training Loss  0.01261304784566164\n",
            "Epoch  21 Batch  132 / 525  Training Loss  0.0005691401893272996\n",
            "Epoch  21 Batch  133 / 525  Training Loss  0.002055781427770853\n",
            "Epoch  21 Batch  134 / 525  Training Loss  0.0014486343134194613\n",
            "Epoch  21 Batch  135 / 525  Training Loss  0.009068720042705536\n",
            "Epoch  21 Batch  136 / 525  Training Loss  0.005692485719919205\n",
            "Epoch  21 Batch  137 / 525  Training Loss  0.004530370235443115\n",
            "Epoch  21 Batch  138 / 525  Training Loss  0.0025508939288556576\n",
            "Epoch  21 Batch  139 / 525  Training Loss  0.010738538578152657\n",
            "Epoch  21 Batch  140 / 525  Training Loss  0.006425526924431324\n",
            "Epoch  21 Batch  141 / 525  Training Loss  0.009839529171586037\n",
            "Epoch  21 Batch  142 / 525  Training Loss  0.006321962922811508\n",
            "Epoch  21 Batch  143 / 525  Training Loss  0.003473816905170679\n",
            "Epoch  21 Batch  144 / 525  Training Loss  0.0025574262253940105\n",
            "Epoch  21 Batch  145 / 525  Training Loss  0.005817250348627567\n",
            "Epoch  21 Batch  146 / 525  Training Loss  0.005890754051506519\n",
            "Epoch  21 Batch  147 / 525  Training Loss  0.002419892232865095\n",
            "Epoch  21 Batch  148 / 525  Training Loss  0.005193436983972788\n",
            "Epoch  21 Batch  149 / 525  Training Loss  0.0029597701504826546\n",
            "Epoch  21 Batch  150 / 525  Training Loss  0.011446040123701096\n",
            "Epoch  21 Batch  151 / 525  Training Loss  0.0029628355987370014\n",
            "Epoch  21 Batch  152 / 525  Training Loss  0.0031076085288077593\n",
            "Epoch  21 Batch  153 / 525  Training Loss  0.0016170140588656068\n",
            "Epoch  21 Batch  154 / 525  Training Loss  0.003229966852813959\n",
            "Epoch  21 Batch  155 / 525  Training Loss  0.0017207078635692596\n",
            "Epoch  21 Batch  156 / 525  Training Loss  0.004029817879199982\n",
            "Epoch  21 Batch  157 / 525  Training Loss  0.0038088925648480654\n",
            "Epoch  21 Batch  158 / 525  Training Loss  0.0013709943741559982\n",
            "Epoch  21 Batch  159 / 525  Training Loss  0.005397281143814325\n",
            "Epoch  21 Batch  160 / 525  Training Loss  0.002912987256422639\n",
            "Epoch  21 Batch  161 / 525  Training Loss  0.0021952716633677483\n",
            "Epoch  21 Batch  162 / 525  Training Loss  0.0034027553629130125\n",
            "Epoch  21 Batch  163 / 525  Training Loss  0.007345117628574371\n",
            "Epoch  21 Batch  164 / 525  Training Loss  0.002570289885625243\n",
            "Epoch  21 Batch  165 / 525  Training Loss  0.003152999561280012\n",
            "Epoch  21 Batch  166 / 525  Training Loss  0.00334911304526031\n",
            "Epoch  21 Batch  167 / 525  Training Loss  0.004562222398817539\n",
            "Epoch  21 Batch  168 / 525  Training Loss  0.00996360182762146\n",
            "Epoch  21 Batch  169 / 525  Training Loss  0.0026946361176669598\n",
            "Epoch  21 Batch  170 / 525  Training Loss  0.0019927092362195253\n",
            "Epoch  21 Batch  171 / 525  Training Loss  0.004466849844902754\n",
            "Epoch  21 Batch  172 / 525  Training Loss  0.007805682718753815\n",
            "Epoch  21 Batch  173 / 525  Training Loss  0.011825572699308395\n",
            "Epoch  21 Batch  174 / 525  Training Loss  0.004174624569714069\n",
            "Epoch  21 Batch  175 / 525  Training Loss  0.004267829470336437\n",
            "Epoch  21 Batch  176 / 525  Training Loss  0.00462385406717658\n",
            "Epoch  21 Batch  177 / 525  Training Loss  0.004618681967258453\n",
            "Epoch  21 Batch  178 / 525  Training Loss  0.007298166397958994\n",
            "Epoch  21 Batch  179 / 525  Training Loss  0.004485768266022205\n",
            "Epoch  21 Batch  180 / 525  Training Loss  0.005045735277235508\n",
            "Epoch  21 Batch  181 / 525  Training Loss  0.0036102798767387867\n",
            "Epoch  21 Batch  182 / 525  Training Loss  0.0036105182953178883\n",
            "Epoch  21 Batch  183 / 525  Training Loss  0.002295432612299919\n",
            "Epoch  21 Batch  184 / 525  Training Loss  0.006331386510282755\n",
            "Epoch  21 Batch  185 / 525  Training Loss  0.0046583060175180435\n",
            "Epoch  21 Batch  186 / 525  Training Loss  0.011957107111811638\n",
            "Epoch  21 Batch  187 / 525  Training Loss  0.004496107809245586\n",
            "Epoch  21 Batch  188 / 525  Training Loss  0.006915506906807423\n",
            "Epoch  21 Batch  189 / 525  Training Loss  0.002855223836377263\n",
            "Epoch  21 Batch  190 / 525  Training Loss  0.003217747900635004\n",
            "Epoch  21 Batch  191 / 525  Training Loss  0.0033907578326761723\n",
            "Epoch  21 Batch  192 / 525  Training Loss  0.002187060657888651\n",
            "Epoch  21 Batch  193 / 525  Training Loss  0.004539567045867443\n",
            "Epoch  21 Batch  194 / 525  Training Loss  0.004365898668766022\n",
            "Epoch  21 Batch  195 / 525  Training Loss  0.0027563082985579967\n",
            "Epoch  21 Batch  196 / 525  Training Loss  0.003152909455820918\n",
            "Epoch  21 Batch  197 / 525  Training Loss  0.006449397653341293\n",
            "Epoch  21 Batch  198 / 525  Training Loss  0.00620283791795373\n",
            "Epoch  21 Batch  199 / 525  Training Loss  0.004619252402335405\n",
            "Epoch  21 Batch  200 / 525  Training Loss  0.013870283961296082\n",
            "Epoch  21 Batch  201 / 525  Training Loss  0.005866541527211666\n",
            "Epoch  21 Batch  202 / 525  Training Loss  0.006112216506153345\n",
            "Epoch  21 Batch  203 / 525  Training Loss  0.0019268735777586699\n",
            "Epoch  21 Batch  204 / 525  Training Loss  0.0014572192449122667\n",
            "Epoch  21 Batch  205 / 525  Training Loss  0.0017213165992870927\n",
            "Epoch  21 Batch  206 / 525  Training Loss  0.005971785634756088\n",
            "Epoch  21 Batch  207 / 525  Training Loss  0.003756096586585045\n",
            "Epoch  21 Batch  208 / 525  Training Loss  0.002910157898440957\n",
            "Epoch  21 Batch  209 / 525  Training Loss  0.015596817247569561\n",
            "Epoch  21 Batch  210 / 525  Training Loss  0.0016062601935118437\n",
            "Epoch  21 Batch  211 / 525  Training Loss  0.006245647557079792\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  21 Batch  212 / 525  Training Loss  0.0016152079915627837\n",
            "Epoch  21 Batch  213 / 525  Training Loss  0.0025276336818933487\n",
            "Epoch  21 Batch  214 / 525  Training Loss  0.005977080203592777\n",
            "Epoch  21 Batch  215 / 525  Training Loss  0.00116508558858186\n",
            "Epoch  21 Batch  216 / 525  Training Loss  0.01073057483881712\n",
            "Epoch  21 Batch  217 / 525  Training Loss  0.013030871748924255\n",
            "Epoch  21 Batch  218 / 525  Training Loss  0.00559166120365262\n",
            "Epoch  21 Batch  219 / 525  Training Loss  0.0056103290989995\n",
            "Epoch  21 Batch  220 / 525  Training Loss  0.004922191612422466\n",
            "Epoch  21 Batch  221 / 525  Training Loss  0.002573941135779023\n",
            "Epoch  21 Batch  222 / 525  Training Loss  0.003675690619274974\n",
            "Epoch  21 Batch  223 / 525  Training Loss  0.007346109487116337\n",
            "Epoch  21 Batch  224 / 525  Training Loss  0.0038255963008850813\n",
            "Epoch  21 Batch  225 / 525  Training Loss  0.00203363923355937\n",
            "Epoch  21 Batch  226 / 525  Training Loss  0.0028523392975330353\n",
            "Epoch  21 Batch  227 / 525  Training Loss  0.013766775839030743\n",
            "Epoch  21 Batch  228 / 525  Training Loss  0.004130903631448746\n",
            "Epoch  21 Batch  229 / 525  Training Loss  0.008215511217713356\n",
            "Epoch  21 Batch  230 / 525  Training Loss  0.007195509038865566\n",
            "Epoch  21 Batch  231 / 525  Training Loss  0.0022977921180427074\n",
            "Epoch  21 Batch  232 / 525  Training Loss  0.008968403562903404\n",
            "Epoch  21 Batch  233 / 525  Training Loss  0.003154369769617915\n",
            "Epoch  21 Batch  234 / 525  Training Loss  0.00999166164547205\n",
            "Epoch  21 Batch  235 / 525  Training Loss  0.004553377162665129\n",
            "Epoch  21 Batch  236 / 525  Training Loss  0.005757912527769804\n",
            "Epoch  21 Batch  237 / 525  Training Loss  0.006522233597934246\n",
            "Epoch  21 Batch  238 / 525  Training Loss  0.003398808417841792\n",
            "Epoch  21 Batch  239 / 525  Training Loss  0.0036493216175585985\n",
            "Epoch  21 Batch  240 / 525  Training Loss  0.007116868160665035\n",
            "Epoch  21 Batch  241 / 525  Training Loss  0.0028925156220793724\n",
            "Epoch  21 Batch  242 / 525  Training Loss  0.0036025703884661198\n",
            "Epoch  21 Batch  243 / 525  Training Loss  0.0010862740455195308\n",
            "Epoch  21 Batch  244 / 525  Training Loss  0.003686729120090604\n",
            "Epoch  21 Batch  245 / 525  Training Loss  0.003667389275506139\n",
            "Epoch  21 Batch  246 / 525  Training Loss  0.0033458652906119823\n",
            "Epoch  21 Batch  247 / 525  Training Loss  0.012663553468883038\n",
            "Epoch  21 Batch  248 / 525  Training Loss  0.001271639484912157\n",
            "Epoch  21 Batch  249 / 525  Training Loss  0.002267212141305208\n",
            "Epoch  21 Batch  250 / 525  Training Loss  0.0036145877093076706\n",
            "Epoch  21 Batch  251 / 525  Training Loss  0.002871528035029769\n",
            "Epoch  21 Batch  252 / 525  Training Loss  0.0017078437376767397\n",
            "Epoch  21 Batch  253 / 525  Training Loss  0.0014419134240597486\n",
            "Epoch  21 Batch  254 / 525  Training Loss  0.006040363572537899\n",
            "Epoch  21 Batch  255 / 525  Training Loss  0.0038275967817753553\n",
            "Epoch  21 Batch  256 / 525  Training Loss  0.0033009822946041822\n",
            "Epoch  21 Batch  257 / 525  Training Loss  0.0031689845491200686\n",
            "Epoch  21 Batch  258 / 525  Training Loss  0.006863209418952465\n",
            "Epoch  21 Batch  259 / 525  Training Loss  0.004986974876374006\n",
            "Epoch  21 Batch  260 / 525  Training Loss  0.002500042086467147\n",
            "Epoch  21 Batch  261 / 525  Training Loss  0.005223508458584547\n",
            "Epoch  21 Batch  262 / 525  Training Loss  0.007584030739963055\n",
            "Epoch  21 Batch  263 / 525  Training Loss  0.0037232625763863325\n",
            "Epoch  21 Batch  264 / 525  Training Loss  0.012109806761145592\n",
            "Epoch  21 Batch  265 / 525  Training Loss  0.006357296369969845\n",
            "Epoch  21 Batch  266 / 525  Training Loss  0.002481856383383274\n",
            "Epoch  21 Batch  267 / 525  Training Loss  0.01455592643469572\n",
            "Epoch  21 Batch  268 / 525  Training Loss  0.004944582935422659\n",
            "Epoch  21 Batch  269 / 525  Training Loss  0.004905333276838064\n",
            "Epoch  21 Batch  270 / 525  Training Loss  0.008959382772445679\n",
            "Epoch  21 Batch  271 / 525  Training Loss  0.004207090009003878\n",
            "Epoch  21 Batch  272 / 525  Training Loss  0.00824669562280178\n",
            "Epoch  21 Batch  273 / 525  Training Loss  0.011871566995978355\n",
            "Epoch  21 Batch  274 / 525  Training Loss  0.005393018946051598\n",
            "Epoch  21 Batch  275 / 525  Training Loss  0.004673630930483341\n",
            "Epoch  21 Batch  276 / 525  Training Loss  0.005096472334116697\n",
            "Epoch  21 Batch  277 / 525  Training Loss  0.003928016405552626\n",
            "Epoch  21 Batch  278 / 525  Training Loss  0.004250661935657263\n",
            "Epoch  21 Batch  279 / 525  Training Loss  0.003400445682927966\n",
            "Epoch  21 Batch  280 / 525  Training Loss  0.002176815178245306\n",
            "Epoch  21 Batch  281 / 525  Training Loss  0.004534461069852114\n",
            "Epoch  21 Batch  282 / 525  Training Loss  0.006824232637882233\n",
            "Epoch  21 Batch  283 / 525  Training Loss  0.0025792322121560574\n",
            "Epoch  21 Batch  284 / 525  Training Loss  0.0017067261505872011\n",
            "Epoch  21 Batch  285 / 525  Training Loss  0.006249881815165281\n",
            "Epoch  21 Batch  286 / 525  Training Loss  0.003552458481863141\n",
            "Epoch  21 Batch  287 / 525  Training Loss  0.002088329056277871\n",
            "Epoch  21 Batch  288 / 525  Training Loss  0.002830491168424487\n",
            "Epoch  21 Batch  289 / 525  Training Loss  0.002953146118670702\n",
            "Epoch  21 Batch  290 / 525  Training Loss  0.0019962864462286234\n",
            "Epoch  21 Batch  291 / 525  Training Loss  0.006353355012834072\n",
            "Epoch  21 Batch  292 / 525  Training Loss  0.002373818540945649\n",
            "Epoch  21 Batch  293 / 525  Training Loss  0.010132776573300362\n",
            "Epoch  21 Batch  294 / 525  Training Loss  0.004254714120179415\n",
            "Epoch  21 Batch  295 / 525  Training Loss  0.0024977242574095726\n",
            "Epoch  21 Batch  296 / 525  Training Loss  0.009168039076030254\n",
            "Epoch  21 Batch  297 / 525  Training Loss  0.006830788217484951\n",
            "Epoch  21 Batch  298 / 525  Training Loss  0.0036565617192536592\n",
            "Epoch  21 Batch  299 / 525  Training Loss  0.001820195117034018\n",
            "Epoch  21 Batch  300 / 525  Training Loss  0.007064666599035263\n",
            "Epoch  21 Batch  301 / 525  Training Loss  0.0028476451989263296\n",
            "Epoch  21 Batch  302 / 525  Training Loss  0.003933719824999571\n",
            "Epoch  21 Batch  303 / 525  Training Loss  0.006849846336990595\n",
            "Epoch  21 Batch  304 / 525  Training Loss  0.0055253636091947556\n",
            "Epoch  21 Batch  305 / 525  Training Loss  0.0027203706558793783\n",
            "Epoch  21 Batch  306 / 525  Training Loss  0.007656763307750225\n",
            "Epoch  21 Batch  307 / 525  Training Loss  0.006801077164709568\n",
            "Epoch  21 Batch  308 / 525  Training Loss  0.003547561587765813\n",
            "Epoch  21 Batch  309 / 525  Training Loss  0.004594328813254833\n",
            "Epoch  21 Batch  310 / 525  Training Loss  0.0033014011569321156\n",
            "Epoch  21 Batch  311 / 525  Training Loss  0.003345263423398137\n",
            "Epoch  21 Batch  312 / 525  Training Loss  0.004876972176134586\n",
            "Epoch  21 Batch  313 / 525  Training Loss  0.003353224601596594\n",
            "Epoch  21 Batch  314 / 525  Training Loss  0.005715060979127884\n",
            "Epoch  21 Batch  315 / 525  Training Loss  0.0020617865957319736\n",
            "Epoch  21 Batch  316 / 525  Training Loss  0.009631166234612465\n",
            "Epoch  21 Batch  317 / 525  Training Loss  0.002415518509224057\n",
            "Epoch  21 Batch  318 / 525  Training Loss  0.0040988316759467125\n",
            "Epoch  21 Batch  319 / 525  Training Loss  0.001501475227996707\n",
            "Epoch  21 Batch  320 / 525  Training Loss  0.002376882592216134\n",
            "Epoch  21 Batch  321 / 525  Training Loss  0.0038953423500061035\n",
            "Epoch  21 Batch  322 / 525  Training Loss  0.0034923446364700794\n",
            "Epoch  21 Batch  323 / 525  Training Loss  0.006453814450651407\n",
            "Epoch  21 Batch  324 / 525  Training Loss  0.010149671696126461\n",
            "Epoch  21 Batch  325 / 525  Training Loss  0.010535943321883678\n",
            "Epoch  21 Batch  326 / 525  Training Loss  0.024557199329137802\n",
            "Epoch  21 Batch  327 / 525  Training Loss  0.0035253199748694897\n",
            "Epoch  21 Batch  328 / 525  Training Loss  0.003960727714002132\n",
            "Epoch  21 Batch  329 / 525  Training Loss  0.007548396475613117\n",
            "Epoch  21 Batch  330 / 525  Training Loss  0.0035357302986085415\n",
            "Epoch  21 Batch  331 / 525  Training Loss  0.001312181120738387\n",
            "Epoch  21 Batch  332 / 525  Training Loss  0.001970339100807905\n",
            "Epoch  21 Batch  333 / 525  Training Loss  0.004660805221647024\n",
            "Epoch  21 Batch  334 / 525  Training Loss  0.002379131270572543\n",
            "Epoch  21 Batch  335 / 525  Training Loss  0.009609905071556568\n",
            "Epoch  21 Batch  336 / 525  Training Loss  0.0014401397202163935\n",
            "Epoch  21 Batch  337 / 525  Training Loss  0.0019859778694808483\n",
            "Epoch  21 Batch  338 / 525  Training Loss  0.0018191039562225342\n",
            "Epoch  21 Batch  339 / 525  Training Loss  0.0014157902915030718\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  21 Batch  340 / 525  Training Loss  0.001715265796519816\n",
            "Epoch  21 Batch  341 / 525  Training Loss  0.014139552600681782\n",
            "Epoch  21 Batch  342 / 525  Training Loss  0.005053854547441006\n",
            "Epoch  21 Batch  343 / 525  Training Loss  0.004206683952361345\n",
            "Epoch  21 Batch  344 / 525  Training Loss  0.0037955574225634336\n",
            "Epoch  21 Batch  345 / 525  Training Loss  0.004941376857459545\n",
            "Epoch  21 Batch  346 / 525  Training Loss  0.004513171501457691\n",
            "Epoch  21 Batch  347 / 525  Training Loss  0.006163829006254673\n",
            "Epoch  21 Batch  348 / 525  Training Loss  0.00266180164180696\n",
            "Epoch  21 Batch  349 / 525  Training Loss  0.005989390891045332\n",
            "Epoch  21 Batch  350 / 525  Training Loss  0.002591554308310151\n",
            "Epoch  21 Batch  351 / 525  Training Loss  0.0035702914465218782\n",
            "Epoch  21 Batch  352 / 525  Training Loss  0.005494673270732164\n",
            "Epoch  21 Batch  353 / 525  Training Loss  0.0012091693934053183\n",
            "Epoch  21 Batch  354 / 525  Training Loss  0.0035553909838199615\n",
            "Epoch  21 Batch  355 / 525  Training Loss  0.004981782753020525\n",
            "Epoch  21 Batch  356 / 525  Training Loss  0.0038009067066013813\n",
            "Epoch  21 Batch  357 / 525  Training Loss  0.0030474872328341007\n",
            "Epoch  21 Batch  358 / 525  Training Loss  0.009038792923092842\n",
            "Epoch  21 Batch  359 / 525  Training Loss  0.006544310599565506\n",
            "Epoch  21 Batch  360 / 525  Training Loss  0.004029635805636644\n",
            "Epoch  21 Batch  361 / 525  Training Loss  0.0024341559037566185\n",
            "Epoch  21 Batch  362 / 525  Training Loss  0.0027030992787331343\n",
            "Epoch  21 Batch  363 / 525  Training Loss  0.003835715353488922\n",
            "Epoch  21 Batch  364 / 525  Training Loss  0.006641673389822245\n",
            "Epoch  21 Batch  365 / 525  Training Loss  0.00594724016264081\n",
            "Epoch  21 Batch  366 / 525  Training Loss  0.002219267189502716\n",
            "Epoch  21 Batch  367 / 525  Training Loss  0.0036920937709510326\n",
            "Epoch  21 Batch  368 / 525  Training Loss  0.004769274033606052\n",
            "Epoch  21 Batch  369 / 525  Training Loss  0.001990195130929351\n",
            "Epoch  21 Batch  370 / 525  Training Loss  0.008204079233109951\n",
            "Epoch  21 Batch  371 / 525  Training Loss  0.0029128114692866802\n",
            "Epoch  21 Batch  372 / 525  Training Loss  0.004952433984726667\n",
            "Epoch  21 Batch  373 / 525  Training Loss  0.006127510219812393\n",
            "Epoch  21 Batch  374 / 525  Training Loss  0.002266235649585724\n",
            "Epoch  21 Batch  375 / 525  Training Loss  0.004630947019904852\n",
            "Epoch  21 Batch  376 / 525  Training Loss  0.002218421082943678\n",
            "Epoch  21 Batch  377 / 525  Training Loss  0.0024577619042247534\n",
            "Epoch  21 Batch  378 / 525  Training Loss  0.001348973484709859\n",
            "Epoch  21 Batch  379 / 525  Training Loss  0.006508789956569672\n",
            "Epoch  21 Batch  380 / 525  Training Loss  0.006629136856645346\n",
            "Epoch  21 Batch  381 / 525  Training Loss  0.004798735026270151\n",
            "Epoch  21 Batch  382 / 525  Training Loss  0.007576528936624527\n",
            "Epoch  21 Batch  383 / 525  Training Loss  0.00551088061183691\n",
            "Epoch  21 Batch  384 / 525  Training Loss  0.004080870188772678\n",
            "Epoch  21 Batch  385 / 525  Training Loss  0.002314523095265031\n",
            "Epoch  21 Batch  386 / 525  Training Loss  0.0022162566892802715\n",
            "Epoch  21 Batch  387 / 525  Training Loss  0.004652530886232853\n",
            "Epoch  21 Batch  388 / 525  Training Loss  0.004304719157516956\n",
            "Epoch  21 Batch  389 / 525  Training Loss  0.0055140238255262375\n",
            "Epoch  21 Batch  390 / 525  Training Loss  0.004926086403429508\n",
            "Epoch  21 Batch  391 / 525  Training Loss  0.0038748763035982847\n",
            "Epoch  21 Batch  392 / 525  Training Loss  0.004131112713366747\n",
            "Epoch  21 Batch  393 / 525  Training Loss  0.005629455670714378\n",
            "Epoch  21 Batch  394 / 525  Training Loss  0.0023105377331376076\n",
            "Epoch  21 Batch  395 / 525  Training Loss  0.0016438744496554136\n",
            "Epoch  21 Batch  396 / 525  Training Loss  0.0027861555572599173\n",
            "Epoch  21 Batch  397 / 525  Training Loss  0.012493853457272053\n",
            "Epoch  21 Batch  398 / 525  Training Loss  0.003773573087528348\n",
            "Epoch  21 Batch  399 / 525  Training Loss  0.0021083205938339233\n",
            "Epoch  21 Batch  400 / 525  Training Loss  0.003517728764563799\n",
            "Epoch  21 Batch  401 / 525  Training Loss  0.0038985672872513533\n",
            "Epoch  21 Batch  402 / 525  Training Loss  0.004983872175216675\n",
            "Epoch  21 Batch  403 / 525  Training Loss  0.004200940020382404\n",
            "Epoch  21 Batch  404 / 525  Training Loss  0.0016745831817388535\n",
            "Epoch  21 Batch  405 / 525  Training Loss  0.001784987049177289\n",
            "Epoch  21 Batch  406 / 525  Training Loss  0.0013411445543169975\n",
            "Epoch  21 Batch  407 / 525  Training Loss  0.0012619586195796728\n",
            "Epoch  21 Batch  408 / 525  Training Loss  0.001261745230294764\n",
            "Epoch  21 Batch  409 / 525  Training Loss  0.001483485335484147\n",
            "Epoch  21 Batch  410 / 525  Training Loss  0.002658386481925845\n",
            "Epoch  21 Batch  411 / 525  Training Loss  0.0021980477031320333\n",
            "Epoch  21 Batch  412 / 525  Training Loss  0.006846611853688955\n",
            "Epoch  21 Batch  413 / 525  Training Loss  0.004418537952005863\n",
            "Epoch  21 Batch  414 / 525  Training Loss  0.0017664784099906683\n",
            "Epoch  21 Batch  415 / 525  Training Loss  0.007710994686931372\n",
            "Epoch  21 Batch  416 / 525  Training Loss  0.005250506568700075\n",
            "Epoch  21 Batch  417 / 525  Training Loss  0.00221699895337224\n",
            "Epoch  21 Batch  418 / 525  Training Loss  0.002127330517396331\n",
            "Epoch  21 Batch  419 / 525  Training Loss  0.00367502449080348\n",
            "Epoch  21 Batch  420 / 525  Training Loss  0.0037942989729344845\n",
            "Epoch  21 Batch  421 / 525  Training Loss  0.010937577113509178\n",
            "Epoch  21 Batch  422 / 525  Training Loss  0.00354938511736691\n",
            "Epoch  21 Batch  423 / 525  Training Loss  0.0015452209627255797\n",
            "Epoch  21 Batch  424 / 525  Training Loss  0.010778801515698433\n",
            "Epoch  21 Batch  425 / 525  Training Loss  0.004831095226109028\n",
            "Epoch  21 Batch  426 / 525  Training Loss  0.00709114596247673\n",
            "Epoch  21 Batch  427 / 525  Training Loss  0.0017238855361938477\n",
            "Epoch  21 Batch  428 / 525  Training Loss  0.007002640515565872\n",
            "Epoch  21 Batch  429 / 525  Training Loss  0.0033350929152220488\n",
            "Epoch  21 Batch  430 / 525  Training Loss  0.0036037650424987078\n",
            "Epoch  21 Batch  431 / 525  Training Loss  0.003294855821877718\n",
            "Epoch  21 Batch  432 / 525  Training Loss  0.004748865962028503\n",
            "Epoch  21 Batch  433 / 525  Training Loss  0.0026540211401879787\n",
            "Epoch  21 Batch  434 / 525  Training Loss  0.0010497033363208175\n",
            "Epoch  21 Batch  435 / 525  Training Loss  0.0009854895761236548\n",
            "Epoch  21 Batch  436 / 525  Training Loss  0.007660083472728729\n",
            "Epoch  21 Batch  437 / 525  Training Loss  0.002844870090484619\n",
            "Epoch  21 Batch  438 / 525  Training Loss  0.0029892362654209137\n",
            "Epoch  21 Batch  439 / 525  Training Loss  0.005727195180952549\n",
            "Epoch  21 Batch  440 / 525  Training Loss  0.005034592933952808\n",
            "Epoch  21 Batch  441 / 525  Training Loss  0.0014744291547685862\n",
            "Epoch  21 Batch  442 / 525  Training Loss  0.005383585579693317\n",
            "Epoch  21 Batch  443 / 525  Training Loss  0.01054092962294817\n",
            "Epoch  21 Batch  444 / 525  Training Loss  0.0034158292692154646\n",
            "Epoch  21 Batch  445 / 525  Training Loss  0.012571143917739391\n",
            "Epoch  21 Batch  446 / 525  Training Loss  0.0026679427828639746\n",
            "Epoch  21 Batch  447 / 525  Training Loss  0.005608548875898123\n",
            "Epoch  21 Batch  448 / 525  Training Loss  0.0024458318948745728\n",
            "Epoch  21 Batch  449 / 525  Training Loss  0.0020144390873610973\n",
            "Epoch  21 Batch  450 / 525  Training Loss  0.006980039179325104\n",
            "Epoch  21 Batch  451 / 525  Training Loss  0.0064149899408221245\n",
            "Epoch  21 Batch  452 / 525  Training Loss  0.010720985010266304\n",
            "Epoch  21 Batch  453 / 525  Training Loss  0.011946147307753563\n",
            "Epoch  21 Batch  454 / 525  Training Loss  0.0062815239652991295\n",
            "Epoch  21 Batch  455 / 525  Training Loss  0.00564298452809453\n",
            "Epoch  21 Batch  456 / 525  Training Loss  0.013342341408133507\n",
            "Epoch  21 Batch  457 / 525  Training Loss  0.002452627057209611\n",
            "Epoch  21 Batch  458 / 525  Training Loss  0.004206677433103323\n",
            "Epoch  21 Batch  459 / 525  Training Loss  0.004038664977997541\n",
            "Epoch  21 Batch  460 / 525  Training Loss  0.002508495468646288\n",
            "Epoch  21 Batch  461 / 525  Training Loss  0.001929632155224681\n",
            "Epoch  21 Batch  462 / 525  Training Loss  0.00809352844953537\n",
            "Epoch  21 Batch  463 / 525  Training Loss  0.0145730497315526\n",
            "Epoch  21 Batch  464 / 525  Training Loss  0.004173463210463524\n",
            "Epoch  21 Batch  465 / 525  Training Loss  0.005552773363888264\n",
            "Epoch  21 Batch  466 / 525  Training Loss  0.003869174513965845\n",
            "Epoch  21 Batch  467 / 525  Training Loss  0.0014335419982671738\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  21 Batch  468 / 525  Training Loss  0.007353578694164753\n",
            "Epoch  21 Batch  469 / 525  Training Loss  0.003982786554843187\n",
            "Epoch  21 Batch  470 / 525  Training Loss  0.0034430227242410183\n",
            "Epoch  21 Batch  471 / 525  Training Loss  0.003933997824788094\n",
            "Epoch  21 Batch  472 / 525  Training Loss  0.001695370301604271\n",
            "Epoch  21 Batch  473 / 525  Training Loss  0.005932028405368328\n",
            "Epoch  21 Batch  474 / 525  Training Loss  0.001718505984172225\n",
            "Epoch  21 Batch  475 / 525  Training Loss  0.0034495205618441105\n",
            "Epoch  21 Batch  476 / 525  Training Loss  0.003055851673707366\n",
            "Epoch  21 Batch  477 / 525  Training Loss  0.004821839742362499\n",
            "Epoch  21 Batch  478 / 525  Training Loss  0.006312665995210409\n",
            "Epoch  21 Batch  479 / 525  Training Loss  0.006900124251842499\n",
            "Epoch  21 Batch  480 / 525  Training Loss  0.004177482333034277\n",
            "Epoch  21 Batch  481 / 525  Training Loss  0.0023862591478973627\n",
            "Epoch  21 Batch  482 / 525  Training Loss  0.002007274655625224\n",
            "Epoch  21 Batch  483 / 525  Training Loss  0.006156532093882561\n",
            "Epoch  21 Batch  484 / 525  Training Loss  0.003078601323068142\n",
            "Epoch  21 Batch  485 / 525  Training Loss  0.006032702513039112\n",
            "Epoch  21 Batch  486 / 525  Training Loss  0.003047062549740076\n",
            "Epoch  21 Batch  487 / 525  Training Loss  0.002545825205743313\n",
            "Epoch  21 Batch  488 / 525  Training Loss  0.005746840033680201\n",
            "Epoch  21 Batch  489 / 525  Training Loss  0.003223641309887171\n",
            "Epoch  21 Batch  490 / 525  Training Loss  0.0027824845165014267\n",
            "Epoch  21 Batch  491 / 525  Training Loss  0.008970196358859539\n",
            "Epoch  21 Batch  492 / 525  Training Loss  0.002900071907788515\n",
            "Epoch  21 Batch  493 / 525  Training Loss  0.008204316720366478\n",
            "Epoch  21 Batch  494 / 525  Training Loss  0.004331521224230528\n",
            "Epoch  21 Batch  495 / 525  Training Loss  0.020472770556807518\n",
            "Epoch  21 Batch  496 / 525  Training Loss  0.006230051629245281\n",
            "Epoch  21 Batch  497 / 525  Training Loss  0.0013813652331009507\n",
            "Epoch  21 Batch  498 / 525  Training Loss  0.010547458194196224\n",
            "Epoch  21 Batch  499 / 525  Training Loss  0.004658552352339029\n",
            "Epoch  21 Batch  500 / 525  Training Loss  0.004501366056501865\n",
            "Epoch  21 Batch  501 / 525  Training Loss  0.006018391810357571\n",
            "Epoch  21 Batch  502 / 525  Training Loss  0.004273485857993364\n",
            "Epoch  21 Batch  503 / 525  Training Loss  0.007040438242256641\n",
            "Epoch  21 Batch  504 / 525  Training Loss  0.002879532752558589\n",
            "Epoch  21 Batch  505 / 525  Training Loss  0.005128240212798119\n",
            "Epoch  21 Batch  506 / 525  Training Loss  0.009473766200244427\n",
            "Epoch  21 Batch  507 / 525  Training Loss  0.006099627818912268\n",
            "Epoch  21 Batch  508 / 525  Training Loss  0.015428337268531322\n",
            "Epoch  21 Batch  509 / 525  Training Loss  0.00762088131159544\n",
            "Epoch  21 Batch  510 / 525  Training Loss  0.003802717197686434\n",
            "Epoch  21 Batch  511 / 525  Training Loss  0.00617696437984705\n",
            "Epoch  21 Batch  512 / 525  Training Loss  0.0051991017535328865\n",
            "Epoch  21 Batch  513 / 525  Training Loss  0.002910854062065482\n",
            "Epoch  21 Batch  514 / 525  Training Loss  0.005608187522739172\n",
            "Epoch  21 Batch  515 / 525  Training Loss  0.004898455925285816\n",
            "Epoch  21 Batch  516 / 525  Training Loss  0.0048793014138937\n",
            "Epoch  21 Batch  517 / 525  Training Loss  0.00909839291125536\n",
            "Epoch  21 Batch  518 / 525  Training Loss  0.0063148485496640205\n",
            "Epoch  21 Batch  519 / 525  Training Loss  0.006521475967019796\n",
            "Epoch  21 Batch  520 / 525  Training Loss  0.002706431318074465\n",
            "Epoch  21 Batch  521 / 525  Training Loss  0.007931379601359367\n",
            "Epoch  21 Batch  522 / 525  Training Loss  0.003251412184908986\n",
            "Epoch  21 Batch  523 / 525  Training Loss  0.006518765352666378\n",
            "Epoch  21 Batch  524 / 525  Training Loss  0.009712466970086098\n",
            "  22    |    -    |   0.004729   | 59.125000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 22\n",
            "Epoch  22 Batch  0 / 525  Training Loss  0.002058745361864567\n",
            "Epoch  22 Batch  1 / 525  Training Loss  0.0013416614383459091\n",
            "Epoch  22 Batch  2 / 525  Training Loss  0.004728698171675205\n",
            "Epoch  22 Batch  3 / 525  Training Loss  0.00532688619568944\n",
            "Epoch  22 Batch  4 / 525  Training Loss  0.008713442832231522\n",
            "Epoch  22 Batch  5 / 525  Training Loss  0.0017995495581999421\n",
            "Epoch  22 Batch  6 / 525  Training Loss  0.002621884923428297\n",
            "Epoch  22 Batch  7 / 525  Training Loss  0.0020785422530025244\n",
            "Epoch  22 Batch  8 / 525  Training Loss  0.0006314884521998465\n",
            "Epoch  22 Batch  9 / 525  Training Loss  0.0038814612198621035\n",
            "Epoch  22 Batch  10 / 525  Training Loss  0.001855825656093657\n",
            "Epoch  22 Batch  11 / 525  Training Loss  0.001451632590033114\n",
            "Epoch  22 Batch  12 / 525  Training Loss  0.008030319586396217\n",
            "Epoch  22 Batch  13 / 525  Training Loss  0.009378934279084206\n",
            "Epoch  22 Batch  14 / 525  Training Loss  0.002127754967659712\n",
            "Epoch  22 Batch  15 / 525  Training Loss  0.0026930454187095165\n",
            "Epoch  22 Batch  16 / 525  Training Loss  0.0046036699786782265\n",
            "Epoch  22 Batch  17 / 525  Training Loss  0.0027934859972447157\n",
            "Epoch  22 Batch  18 / 525  Training Loss  0.005237567704170942\n",
            "Epoch  22 Batch  19 / 525  Training Loss  0.0026257040444761515\n",
            "Epoch  22 Batch  20 / 525  Training Loss  0.001590268686413765\n",
            "Epoch  22 Batch  21 / 525  Training Loss  0.009563332423567772\n",
            "Epoch  22 Batch  22 / 525  Training Loss  0.004106651991605759\n",
            "Epoch  22 Batch  23 / 525  Training Loss  0.005075212102383375\n",
            "Epoch  22 Batch  24 / 525  Training Loss  0.0043878210708498955\n",
            "Epoch  22 Batch  25 / 525  Training Loss  0.0008238480659201741\n",
            "Epoch  22 Batch  26 / 525  Training Loss  0.007407722529023886\n",
            "Epoch  22 Batch  27 / 525  Training Loss  0.005339261144399643\n",
            "Epoch  22 Batch  28 / 525  Training Loss  0.0032772249542176723\n",
            "Epoch  22 Batch  29 / 525  Training Loss  0.00126334757078439\n",
            "Epoch  22 Batch  30 / 525  Training Loss  0.004072701092809439\n",
            "Epoch  22 Batch  31 / 525  Training Loss  0.0048808082938194275\n",
            "Epoch  22 Batch  32 / 525  Training Loss  0.0031497343443334103\n",
            "Epoch  22 Batch  33 / 525  Training Loss  0.0014348997501656413\n",
            "Epoch  22 Batch  34 / 525  Training Loss  0.0012301577953621745\n",
            "Epoch  22 Batch  35 / 525  Training Loss  0.0025639766827225685\n",
            "Epoch  22 Batch  36 / 525  Training Loss  0.0009022809681482613\n",
            "Epoch  22 Batch  37 / 525  Training Loss  0.0053925360552966595\n",
            "Epoch  22 Batch  38 / 525  Training Loss  0.0029484094120562077\n",
            "Epoch  22 Batch  39 / 525  Training Loss  0.004117406439036131\n",
            "Epoch  22 Batch  40 / 525  Training Loss  0.0021162587217986584\n",
            "Epoch  22 Batch  41 / 525  Training Loss  0.00401244917884469\n",
            "Epoch  22 Batch  42 / 525  Training Loss  0.0036992006935179234\n",
            "Epoch  22 Batch  43 / 525  Training Loss  0.004209526814520359\n",
            "Epoch  22 Batch  44 / 525  Training Loss  0.0023048794828355312\n",
            "Epoch  22 Batch  45 / 525  Training Loss  0.00540901767089963\n",
            "Epoch  22 Batch  46 / 525  Training Loss  0.002713132183998823\n",
            "Epoch  22 Batch  47 / 525  Training Loss  0.003248917404562235\n",
            "Epoch  22 Batch  48 / 525  Training Loss  0.004781682975590229\n",
            "Epoch  22 Batch  49 / 525  Training Loss  0.0020227390341460705\n",
            "Epoch  22 Batch  50 / 525  Training Loss  0.0034292254131287336\n",
            "Epoch  22 Batch  51 / 525  Training Loss  0.0039645167998969555\n",
            "Epoch  22 Batch  52 / 525  Training Loss  0.005868813022971153\n",
            "Epoch  22 Batch  53 / 525  Training Loss  0.0008724381332285702\n",
            "Epoch  22 Batch  54 / 525  Training Loss  0.0027000997215509415\n",
            "Epoch  22 Batch  55 / 525  Training Loss  0.0012205637758597732\n",
            "Epoch  22 Batch  56 / 525  Training Loss  0.001747363479807973\n",
            "Epoch  22 Batch  57 / 525  Training Loss  0.0011870890157297254\n",
            "Epoch  22 Batch  58 / 525  Training Loss  0.0020783429499715567\n",
            "Epoch  22 Batch  59 / 525  Training Loss  0.00158085732255131\n",
            "Epoch  22 Batch  60 / 525  Training Loss  0.006527441553771496\n",
            "Epoch  22 Batch  61 / 525  Training Loss  0.0024592559784650803\n",
            "Epoch  22 Batch  62 / 525  Training Loss  0.0005850087618455291\n",
            "Epoch  22 Batch  63 / 525  Training Loss  0.0008104103617370129\n",
            "Epoch  22 Batch  64 / 525  Training Loss  0.001722955727018416\n",
            "Epoch  22 Batch  65 / 525  Training Loss  0.0033298656344413757\n",
            "Epoch  22 Batch  66 / 525  Training Loss  0.003917995374649763\n",
            "Epoch  22 Batch  67 / 525  Training Loss  0.0019540246576070786\n",
            "Epoch  22 Batch  68 / 525  Training Loss  0.0019728499464690685\n",
            "Epoch  22 Batch  69 / 525  Training Loss  0.0015495754778385162\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  22 Batch  70 / 525  Training Loss  0.0005920151015743613\n",
            "Epoch  22 Batch  71 / 525  Training Loss  0.0035591579508036375\n",
            "Epoch  22 Batch  72 / 525  Training Loss  0.0030826893635094166\n",
            "Epoch  22 Batch  73 / 525  Training Loss  0.007093754597008228\n",
            "Epoch  22 Batch  74 / 525  Training Loss  0.0050771841779351234\n",
            "Epoch  22 Batch  75 / 525  Training Loss  0.001421211170963943\n",
            "Epoch  22 Batch  76 / 525  Training Loss  0.0025434172712266445\n",
            "Epoch  22 Batch  77 / 525  Training Loss  0.0037970920093357563\n",
            "Epoch  22 Batch  78 / 525  Training Loss  0.006273157894611359\n",
            "Epoch  22 Batch  79 / 525  Training Loss  0.0064166029915213585\n",
            "Epoch  22 Batch  80 / 525  Training Loss  0.011651886627078056\n",
            "Epoch  22 Batch  81 / 525  Training Loss  0.001134159741923213\n",
            "Epoch  22 Batch  82 / 525  Training Loss  0.005178781691938639\n",
            "Epoch  22 Batch  83 / 525  Training Loss  0.001585164456628263\n",
            "Epoch  22 Batch  84 / 525  Training Loss  0.016239050775766373\n",
            "Epoch  22 Batch  85 / 525  Training Loss  0.0021109364461153746\n",
            "Epoch  22 Batch  86 / 525  Training Loss  0.005969596095383167\n",
            "Epoch  22 Batch  87 / 525  Training Loss  0.01519382931292057\n",
            "Epoch  22 Batch  88 / 525  Training Loss  0.004166821017861366\n",
            "Epoch  22 Batch  89 / 525  Training Loss  0.0030520600266754627\n",
            "Epoch  22 Batch  90 / 525  Training Loss  0.0025720628909766674\n",
            "Epoch  22 Batch  91 / 525  Training Loss  0.0035940338857471943\n",
            "Epoch  22 Batch  92 / 525  Training Loss  0.002366043161600828\n",
            "Epoch  22 Batch  93 / 525  Training Loss  0.0082448311150074\n",
            "Epoch  22 Batch  94 / 525  Training Loss  0.002312621334567666\n",
            "Epoch  22 Batch  95 / 525  Training Loss  0.005610709544271231\n",
            "Epoch  22 Batch  96 / 525  Training Loss  0.0033811009488999844\n",
            "Epoch  22 Batch  97 / 525  Training Loss  0.011631233617663383\n",
            "Epoch  22 Batch  98 / 525  Training Loss  0.0013063017977401614\n",
            "Epoch  22 Batch  99 / 525  Training Loss  0.011433160863816738\n",
            "Epoch  22 Batch  100 / 525  Training Loss  0.0018103901529684663\n",
            "Epoch  22 Batch  101 / 525  Training Loss  0.003522491781041026\n",
            "Epoch  22 Batch  102 / 525  Training Loss  0.0041905734688043594\n",
            "Epoch  22 Batch  103 / 525  Training Loss  0.0018534358823671937\n",
            "Epoch  22 Batch  104 / 525  Training Loss  0.0025472037959843874\n",
            "Epoch  22 Batch  105 / 525  Training Loss  0.004368353635072708\n",
            "Epoch  22 Batch  106 / 525  Training Loss  0.0037971653509885073\n",
            "Epoch  22 Batch  107 / 525  Training Loss  0.00282453210093081\n",
            "Epoch  22 Batch  108 / 525  Training Loss  0.004700378514826298\n",
            "Epoch  22 Batch  109 / 525  Training Loss  0.0010110781295225024\n",
            "Epoch  22 Batch  110 / 525  Training Loss  0.004159156233072281\n",
            "Epoch  22 Batch  111 / 525  Training Loss  0.003992344252765179\n",
            "Epoch  22 Batch  112 / 525  Training Loss  0.005985904484987259\n",
            "Epoch  22 Batch  113 / 525  Training Loss  0.002391950925812125\n",
            "Epoch  22 Batch  114 / 525  Training Loss  0.0025494550354778767\n",
            "Epoch  22 Batch  115 / 525  Training Loss  0.003941331524401903\n",
            "Epoch  22 Batch  116 / 525  Training Loss  0.004359702114015818\n",
            "Epoch  22 Batch  117 / 525  Training Loss  0.004728246014565229\n",
            "Epoch  22 Batch  118 / 525  Training Loss  0.0016835713759064674\n",
            "Epoch  22 Batch  119 / 525  Training Loss  0.002653185511007905\n",
            "Epoch  22 Batch  120 / 525  Training Loss  0.005482390522956848\n",
            "Epoch  22 Batch  121 / 525  Training Loss  0.0028886948712170124\n",
            "Epoch  22 Batch  122 / 525  Training Loss  0.0019231733167544007\n",
            "Epoch  22 Batch  123 / 525  Training Loss  0.004443001933395863\n",
            "Epoch  22 Batch  124 / 525  Training Loss  0.002537140157073736\n",
            "Epoch  22 Batch  125 / 525  Training Loss  0.001623503863811493\n",
            "Epoch  22 Batch  126 / 525  Training Loss  0.0013387050712481141\n",
            "Epoch  22 Batch  127 / 525  Training Loss  0.005192185752093792\n",
            "Epoch  22 Batch  128 / 525  Training Loss  0.0037637080531567335\n",
            "Epoch  22 Batch  129 / 525  Training Loss  0.009130945429205894\n",
            "Epoch  22 Batch  130 / 525  Training Loss  0.004530004225671291\n",
            "Epoch  22 Batch  131 / 525  Training Loss  0.0015980640891939402\n",
            "Epoch  22 Batch  132 / 525  Training Loss  0.001917679444886744\n",
            "Epoch  22 Batch  133 / 525  Training Loss  0.0028103638906031847\n",
            "Epoch  22 Batch  134 / 525  Training Loss  0.008227136917412281\n",
            "Epoch  22 Batch  135 / 525  Training Loss  0.003881995100528002\n",
            "Epoch  22 Batch  136 / 525  Training Loss  0.007685893680900335\n",
            "Epoch  22 Batch  137 / 525  Training Loss  0.001670396188274026\n",
            "Epoch  22 Batch  138 / 525  Training Loss  0.004589513409882784\n",
            "Epoch  22 Batch  139 / 525  Training Loss  0.0058033582754433155\n",
            "Epoch  22 Batch  140 / 525  Training Loss  0.0021264017559587955\n",
            "Epoch  22 Batch  141 / 525  Training Loss  0.004965257830917835\n",
            "Epoch  22 Batch  142 / 525  Training Loss  0.003090901765972376\n",
            "Epoch  22 Batch  143 / 525  Training Loss  0.003157323691993952\n",
            "Epoch  22 Batch  144 / 525  Training Loss  0.004499502945691347\n",
            "Epoch  22 Batch  145 / 525  Training Loss  0.003434643615037203\n",
            "Epoch  22 Batch  146 / 525  Training Loss  0.0028680574614554644\n",
            "Epoch  22 Batch  147 / 525  Training Loss  0.0024382746778428555\n",
            "Epoch  22 Batch  148 / 525  Training Loss  0.002064510015770793\n",
            "Epoch  22 Batch  149 / 525  Training Loss  0.0007292725495062768\n",
            "Epoch  22 Batch  150 / 525  Training Loss  0.007133325096219778\n",
            "Epoch  22 Batch  151 / 525  Training Loss  0.001750062219798565\n",
            "Epoch  22 Batch  152 / 525  Training Loss  0.0008705103537067771\n",
            "Epoch  22 Batch  153 / 525  Training Loss  0.0046352120116353035\n",
            "Epoch  22 Batch  154 / 525  Training Loss  0.001706733019091189\n",
            "Epoch  22 Batch  155 / 525  Training Loss  0.019762009382247925\n",
            "Epoch  22 Batch  156 / 525  Training Loss  0.003294028341770172\n",
            "Epoch  22 Batch  157 / 525  Training Loss  0.002394298557192087\n",
            "Epoch  22 Batch  158 / 525  Training Loss  0.009870240464806557\n",
            "Epoch  22 Batch  159 / 525  Training Loss  0.003632510779425502\n",
            "Epoch  22 Batch  160 / 525  Training Loss  0.0036550909280776978\n",
            "Epoch  22 Batch  161 / 525  Training Loss  0.0017143858131021261\n",
            "Epoch  22 Batch  162 / 525  Training Loss  0.0017477096989750862\n",
            "Epoch  22 Batch  163 / 525  Training Loss  0.0016309816855937243\n",
            "Epoch  22 Batch  164 / 525  Training Loss  0.011581877246499062\n",
            "Epoch  22 Batch  165 / 525  Training Loss  0.00362643925473094\n",
            "Epoch  22 Batch  166 / 525  Training Loss  0.0011346290120854974\n",
            "Epoch  22 Batch  167 / 525  Training Loss  0.002354909898713231\n",
            "Epoch  22 Batch  168 / 525  Training Loss  0.0020002531819045544\n",
            "Epoch  22 Batch  169 / 525  Training Loss  0.008052831515669823\n",
            "Epoch  22 Batch  170 / 525  Training Loss  0.002154534449800849\n",
            "Epoch  22 Batch  171 / 525  Training Loss  0.0032655007671564817\n",
            "Epoch  22 Batch  172 / 525  Training Loss  0.005688393022865057\n",
            "Epoch  22 Batch  173 / 525  Training Loss  0.010823260992765427\n",
            "Epoch  22 Batch  174 / 525  Training Loss  0.004573942627757788\n",
            "Epoch  22 Batch  175 / 525  Training Loss  0.0050527858547866344\n",
            "Epoch  22 Batch  176 / 525  Training Loss  0.0045156823471188545\n",
            "Epoch  22 Batch  177 / 525  Training Loss  0.009851383045315742\n",
            "Epoch  22 Batch  178 / 525  Training Loss  0.0033712084405124187\n",
            "Epoch  22 Batch  179 / 525  Training Loss  0.003762008622288704\n",
            "Epoch  22 Batch  180 / 525  Training Loss  0.0075669437646865845\n",
            "Epoch  22 Batch  181 / 525  Training Loss  0.005578092765063047\n",
            "Epoch  22 Batch  182 / 525  Training Loss  0.005610097665339708\n",
            "Epoch  22 Batch  183 / 525  Training Loss  0.006096214521676302\n",
            "Epoch  22 Batch  184 / 525  Training Loss  0.0011264682980254292\n",
            "Epoch  22 Batch  185 / 525  Training Loss  0.001609895029105246\n",
            "Epoch  22 Batch  186 / 525  Training Loss  0.0011180269066244364\n",
            "Epoch  22 Batch  187 / 525  Training Loss  0.002958812518045306\n",
            "Epoch  22 Batch  188 / 525  Training Loss  0.003034075954928994\n",
            "Epoch  22 Batch  189 / 525  Training Loss  0.004028262570500374\n",
            "Epoch  22 Batch  190 / 525  Training Loss  0.003349318401888013\n",
            "Epoch  22 Batch  191 / 525  Training Loss  0.001638973830267787\n",
            "Epoch  22 Batch  192 / 525  Training Loss  0.001887179329060018\n",
            "Epoch  22 Batch  193 / 525  Training Loss  0.0030602584592998028\n",
            "Epoch  22 Batch  194 / 525  Training Loss  0.0008192750392481685\n",
            "Epoch  22 Batch  195 / 525  Training Loss  0.0019021686166524887\n",
            "Epoch  22 Batch  196 / 525  Training Loss  0.0024611952248960733\n",
            "Epoch  22 Batch  197 / 525  Training Loss  0.00235949270427227\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  22 Batch  198 / 525  Training Loss  0.002062174491584301\n",
            "Epoch  22 Batch  199 / 525  Training Loss  0.005418164189904928\n",
            "Epoch  22 Batch  200 / 525  Training Loss  0.0033456534147262573\n",
            "Epoch  22 Batch  201 / 525  Training Loss  0.0012824984733015299\n",
            "Epoch  22 Batch  202 / 525  Training Loss  0.0027197101153433323\n",
            "Epoch  22 Batch  203 / 525  Training Loss  0.0031049202661961317\n",
            "Epoch  22 Batch  204 / 525  Training Loss  0.0007471871213056147\n",
            "Epoch  22 Batch  205 / 525  Training Loss  0.001284571597352624\n",
            "Epoch  22 Batch  206 / 525  Training Loss  0.006507948972284794\n",
            "Epoch  22 Batch  207 / 525  Training Loss  0.0012402035063132644\n",
            "Epoch  22 Batch  208 / 525  Training Loss  0.0012900392757728696\n",
            "Epoch  22 Batch  209 / 525  Training Loss  0.006496091838926077\n",
            "Epoch  22 Batch  210 / 525  Training Loss  0.004219691734761\n",
            "Epoch  22 Batch  211 / 525  Training Loss  0.011899255216121674\n",
            "Epoch  22 Batch  212 / 525  Training Loss  0.0029638740234076977\n",
            "Epoch  22 Batch  213 / 525  Training Loss  0.003551424713805318\n",
            "Epoch  22 Batch  214 / 525  Training Loss  0.013692110776901245\n",
            "Epoch  22 Batch  215 / 525  Training Loss  0.005305681377649307\n",
            "Epoch  22 Batch  216 / 525  Training Loss  0.00468259584158659\n",
            "Epoch  22 Batch  217 / 525  Training Loss  0.0020795557647943497\n",
            "Epoch  22 Batch  218 / 525  Training Loss  0.001282611396163702\n",
            "Epoch  22 Batch  219 / 525  Training Loss  0.00865047238767147\n",
            "Epoch  22 Batch  220 / 525  Training Loss  0.009135660715401173\n",
            "Epoch  22 Batch  221 / 525  Training Loss  0.005623897537589073\n",
            "Epoch  22 Batch  222 / 525  Training Loss  0.0016615450149402022\n",
            "Epoch  22 Batch  223 / 525  Training Loss  0.0016692683566361666\n",
            "Epoch  22 Batch  224 / 525  Training Loss  0.001734333811327815\n",
            "Epoch  22 Batch  225 / 525  Training Loss  0.004815034102648497\n",
            "Epoch  22 Batch  226 / 525  Training Loss  0.0028578187339007854\n",
            "Epoch  22 Batch  227 / 525  Training Loss  0.001391009078361094\n",
            "Epoch  22 Batch  228 / 525  Training Loss  0.0013553387252613902\n",
            "Epoch  22 Batch  229 / 525  Training Loss  0.002064319094642997\n",
            "Epoch  22 Batch  230 / 525  Training Loss  0.00482254708185792\n",
            "Epoch  22 Batch  231 / 525  Training Loss  0.0009298909571953118\n",
            "Epoch  22 Batch  232 / 525  Training Loss  0.0026540274266153574\n",
            "Epoch  22 Batch  233 / 525  Training Loss  0.00317769986577332\n",
            "Epoch  22 Batch  234 / 525  Training Loss  0.0034020915627479553\n",
            "Epoch  22 Batch  235 / 525  Training Loss  0.004156267736107111\n",
            "Epoch  22 Batch  236 / 525  Training Loss  0.003413975238800049\n",
            "Epoch  22 Batch  237 / 525  Training Loss  0.0017815601313486695\n",
            "Epoch  22 Batch  238 / 525  Training Loss  0.001033631619066\n",
            "Epoch  22 Batch  239 / 525  Training Loss  0.0024459424894303083\n",
            "Epoch  22 Batch  240 / 525  Training Loss  0.004292053636163473\n",
            "Epoch  22 Batch  241 / 525  Training Loss  0.0008292556740343571\n",
            "Epoch  22 Batch  242 / 525  Training Loss  0.006080291699618101\n",
            "Epoch  22 Batch  243 / 525  Training Loss  0.0020109987817704678\n",
            "Epoch  22 Batch  244 / 525  Training Loss  0.0016471619019284844\n",
            "Epoch  22 Batch  245 / 525  Training Loss  0.0021604218054562807\n",
            "Epoch  22 Batch  246 / 525  Training Loss  0.004885645117610693\n",
            "Epoch  22 Batch  247 / 525  Training Loss  0.0035525269340723753\n",
            "Epoch  22 Batch  248 / 525  Training Loss  0.0048707472160458565\n",
            "Epoch  22 Batch  249 / 525  Training Loss  0.008503077551722527\n",
            "Epoch  22 Batch  250 / 525  Training Loss  0.0017873980104923248\n",
            "Epoch  22 Batch  251 / 525  Training Loss  0.0029093255288898945\n",
            "Epoch  22 Batch  252 / 525  Training Loss  0.0025473267305642366\n",
            "Epoch  22 Batch  253 / 525  Training Loss  0.0117401834577322\n",
            "Epoch  22 Batch  254 / 525  Training Loss  0.0018411076162010431\n",
            "Epoch  22 Batch  255 / 525  Training Loss  0.0011611341033130884\n",
            "Epoch  22 Batch  256 / 525  Training Loss  0.003920647781342268\n",
            "Epoch  22 Batch  257 / 525  Training Loss  0.007665477693080902\n",
            "Epoch  22 Batch  258 / 525  Training Loss  0.0016607281286269426\n",
            "Epoch  22 Batch  259 / 525  Training Loss  0.0041585215367376804\n",
            "Epoch  22 Batch  260 / 525  Training Loss  0.0010836772853508592\n",
            "Epoch  22 Batch  261 / 525  Training Loss  0.002527109580114484\n",
            "Epoch  22 Batch  262 / 525  Training Loss  0.0035820361226797104\n",
            "Epoch  22 Batch  263 / 525  Training Loss  0.004116876982152462\n",
            "Epoch  22 Batch  264 / 525  Training Loss  0.002004600130021572\n",
            "Epoch  22 Batch  265 / 525  Training Loss  0.0022658181842416525\n",
            "Epoch  22 Batch  266 / 525  Training Loss  0.007657031062990427\n",
            "Epoch  22 Batch  267 / 525  Training Loss  0.0038121365942060947\n",
            "Epoch  22 Batch  268 / 525  Training Loss  0.0023380578495562077\n",
            "Epoch  22 Batch  269 / 525  Training Loss  0.0005964612355455756\n",
            "Epoch  22 Batch  270 / 525  Training Loss  0.004873739555478096\n",
            "Epoch  22 Batch  271 / 525  Training Loss  0.004158584401011467\n",
            "Epoch  22 Batch  272 / 525  Training Loss  0.0023400606587529182\n",
            "Epoch  22 Batch  273 / 525  Training Loss  0.004550714045763016\n",
            "Epoch  22 Batch  274 / 525  Training Loss  0.005209723487496376\n",
            "Epoch  22 Batch  275 / 525  Training Loss  0.0013487429823726416\n",
            "Epoch  22 Batch  276 / 525  Training Loss  0.004579354077577591\n",
            "Epoch  22 Batch  277 / 525  Training Loss  0.007452730089426041\n",
            "Epoch  22 Batch  278 / 525  Training Loss  0.0034334175288677216\n",
            "Epoch  22 Batch  279 / 525  Training Loss  0.007662675343453884\n",
            "Epoch  22 Batch  280 / 525  Training Loss  0.002885280642658472\n",
            "Epoch  22 Batch  281 / 525  Training Loss  0.007219887338578701\n",
            "Epoch  22 Batch  282 / 525  Training Loss  0.003621842246502638\n",
            "Epoch  22 Batch  283 / 525  Training Loss  0.0016780246514827013\n",
            "Epoch  22 Batch  284 / 525  Training Loss  0.001938389614224434\n",
            "Epoch  22 Batch  285 / 525  Training Loss  0.003742425935342908\n",
            "Epoch  22 Batch  286 / 525  Training Loss  0.0020091156475245953\n",
            "Epoch  22 Batch  287 / 525  Training Loss  0.0038494921755045652\n",
            "Epoch  22 Batch  288 / 525  Training Loss  0.005472215823829174\n",
            "Epoch  22 Batch  289 / 525  Training Loss  0.0010698291007429361\n",
            "Epoch  22 Batch  290 / 525  Training Loss  0.0029025545809417963\n",
            "Epoch  22 Batch  291 / 525  Training Loss  0.0028604951221495867\n",
            "Epoch  22 Batch  292 / 525  Training Loss  0.0030540393199771643\n",
            "Epoch  22 Batch  293 / 525  Training Loss  0.0016850715037435293\n",
            "Epoch  22 Batch  294 / 525  Training Loss  0.00516851432621479\n",
            "Epoch  22 Batch  295 / 525  Training Loss  0.001739105791784823\n",
            "Epoch  22 Batch  296 / 525  Training Loss  0.003103451570495963\n",
            "Epoch  22 Batch  297 / 525  Training Loss  0.004263821057975292\n",
            "Epoch  22 Batch  298 / 525  Training Loss  0.00421445956453681\n",
            "Epoch  22 Batch  299 / 525  Training Loss  0.004229872487485409\n",
            "Epoch  22 Batch  300 / 525  Training Loss  0.002504660515114665\n",
            "Epoch  22 Batch  301 / 525  Training Loss  0.010788632556796074\n",
            "Epoch  22 Batch  302 / 525  Training Loss  0.007385811302810907\n",
            "Epoch  22 Batch  303 / 525  Training Loss  0.0019320513820275664\n",
            "Epoch  22 Batch  304 / 525  Training Loss  0.001715189777314663\n",
            "Epoch  22 Batch  305 / 525  Training Loss  0.0031628217548131943\n",
            "Epoch  22 Batch  306 / 525  Training Loss  0.0032747765071690083\n",
            "Epoch  22 Batch  307 / 525  Training Loss  0.003402258502319455\n",
            "Epoch  22 Batch  308 / 525  Training Loss  0.0039285579696297646\n",
            "Epoch  22 Batch  309 / 525  Training Loss  0.005509522743523121\n",
            "Epoch  22 Batch  310 / 525  Training Loss  0.0013621357502415776\n",
            "Epoch  22 Batch  311 / 525  Training Loss  0.009768287651240826\n",
            "Epoch  22 Batch  312 / 525  Training Loss  0.002922042505815625\n",
            "Epoch  22 Batch  313 / 525  Training Loss  0.007170086260885\n",
            "Epoch  22 Batch  314 / 525  Training Loss  0.008814767003059387\n",
            "Epoch  22 Batch  315 / 525  Training Loss  0.0029484438709914684\n",
            "Epoch  22 Batch  316 / 525  Training Loss  0.0033972084056586027\n",
            "Epoch  22 Batch  317 / 525  Training Loss  0.00945503544062376\n",
            "Epoch  22 Batch  318 / 525  Training Loss  0.003545226529240608\n",
            "Epoch  22 Batch  319 / 525  Training Loss  0.010088494047522545\n",
            "Epoch  22 Batch  320 / 525  Training Loss  0.0012021856382489204\n",
            "Epoch  22 Batch  321 / 525  Training Loss  0.0030472627840936184\n",
            "Epoch  22 Batch  322 / 525  Training Loss  0.0029538264498114586\n",
            "Epoch  22 Batch  323 / 525  Training Loss  0.0035746667999774218\n",
            "Epoch  22 Batch  324 / 525  Training Loss  0.002830195939168334\n",
            "Epoch  22 Batch  325 / 525  Training Loss  0.006793317850679159\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  22 Batch  326 / 525  Training Loss  0.003203724743798375\n",
            "Epoch  22 Batch  327 / 525  Training Loss  0.0035623733419924974\n",
            "Epoch  22 Batch  328 / 525  Training Loss  0.001606628648005426\n",
            "Epoch  22 Batch  329 / 525  Training Loss  0.0016874689608812332\n",
            "Epoch  22 Batch  330 / 525  Training Loss  0.0024337838403880596\n",
            "Epoch  22 Batch  331 / 525  Training Loss  0.003993618302047253\n",
            "Epoch  22 Batch  332 / 525  Training Loss  0.0031309097539633512\n",
            "Epoch  22 Batch  333 / 525  Training Loss  0.0031415491830557585\n",
            "Epoch  22 Batch  334 / 525  Training Loss  0.005372466519474983\n",
            "Epoch  22 Batch  335 / 525  Training Loss  0.003001301782205701\n",
            "Epoch  22 Batch  336 / 525  Training Loss  0.006271590944379568\n",
            "Epoch  22 Batch  337 / 525  Training Loss  0.003146339673548937\n",
            "Epoch  22 Batch  338 / 525  Training Loss  0.005082318559288979\n",
            "Epoch  22 Batch  339 / 525  Training Loss  0.003409110475331545\n",
            "Epoch  22 Batch  340 / 525  Training Loss  0.0020373756997287273\n",
            "Epoch  22 Batch  341 / 525  Training Loss  0.0026514797937124968\n",
            "Epoch  22 Batch  342 / 525  Training Loss  0.003967056982219219\n",
            "Epoch  22 Batch  343 / 525  Training Loss  0.001743400702252984\n",
            "Epoch  22 Batch  344 / 525  Training Loss  0.0021359343081712723\n",
            "Epoch  22 Batch  345 / 525  Training Loss  0.0024405326694250107\n",
            "Epoch  22 Batch  346 / 525  Training Loss  0.011532479897141457\n",
            "Epoch  22 Batch  347 / 525  Training Loss  0.0018792671617120504\n",
            "Epoch  22 Batch  348 / 525  Training Loss  0.002050005365163088\n",
            "Epoch  22 Batch  349 / 525  Training Loss  0.005577480886131525\n",
            "Epoch  22 Batch  350 / 525  Training Loss  0.0015815250808373094\n",
            "Epoch  22 Batch  351 / 525  Training Loss  0.002215927466750145\n",
            "Epoch  22 Batch  352 / 525  Training Loss  0.002891488838940859\n",
            "Epoch  22 Batch  353 / 525  Training Loss  0.0015598076861351728\n",
            "Epoch  22 Batch  354 / 525  Training Loss  0.004878147505223751\n",
            "Epoch  22 Batch  355 / 525  Training Loss  0.006129373796284199\n",
            "Epoch  22 Batch  356 / 525  Training Loss  0.002338470658287406\n",
            "Epoch  22 Batch  357 / 525  Training Loss  0.008472215384244919\n",
            "Epoch  22 Batch  358 / 525  Training Loss  0.003967135213315487\n",
            "Epoch  22 Batch  359 / 525  Training Loss  0.0009190229466184974\n",
            "Epoch  22 Batch  360 / 525  Training Loss  0.008631284348666668\n",
            "Epoch  22 Batch  361 / 525  Training Loss  0.002487865975126624\n",
            "Epoch  22 Batch  362 / 525  Training Loss  0.0035735678393393755\n",
            "Epoch  22 Batch  363 / 525  Training Loss  0.0008392479503527284\n",
            "Epoch  22 Batch  364 / 525  Training Loss  0.004377000965178013\n",
            "Epoch  22 Batch  365 / 525  Training Loss  0.0017278749728575349\n",
            "Epoch  22 Batch  366 / 525  Training Loss  0.0037361769936978817\n",
            "Epoch  22 Batch  367 / 525  Training Loss  0.0019786071497946978\n",
            "Epoch  22 Batch  368 / 525  Training Loss  0.004864629823714495\n",
            "Epoch  22 Batch  369 / 525  Training Loss  0.00636373832821846\n",
            "Epoch  22 Batch  370 / 525  Training Loss  0.0013583063846454024\n",
            "Epoch  22 Batch  371 / 525  Training Loss  0.007667975965887308\n",
            "Epoch  22 Batch  372 / 525  Training Loss  0.0017621578881517053\n",
            "Epoch  22 Batch  373 / 525  Training Loss  0.002956176409497857\n",
            "Epoch  22 Batch  374 / 525  Training Loss  0.0014146951725706458\n",
            "Epoch  22 Batch  375 / 525  Training Loss  0.0025809849612414837\n",
            "Epoch  22 Batch  376 / 525  Training Loss  0.0014213267713785172\n",
            "Epoch  22 Batch  377 / 525  Training Loss  0.001901728450320661\n",
            "Epoch  22 Batch  378 / 525  Training Loss  0.0033842124976217747\n",
            "Epoch  22 Batch  379 / 525  Training Loss  0.006977097596973181\n",
            "Epoch  22 Batch  380 / 525  Training Loss  0.001283402438275516\n",
            "Epoch  22 Batch  381 / 525  Training Loss  0.0016883214702829719\n",
            "Epoch  22 Batch  382 / 525  Training Loss  0.004535411950200796\n",
            "Epoch  22 Batch  383 / 525  Training Loss  0.0028032599948346615\n",
            "Epoch  22 Batch  384 / 525  Training Loss  0.0015787782613188028\n",
            "Epoch  22 Batch  385 / 525  Training Loss  0.0013495479943230748\n",
            "Epoch  22 Batch  386 / 525  Training Loss  0.003642593277618289\n",
            "Epoch  22 Batch  387 / 525  Training Loss  0.0022255119401961565\n",
            "Epoch  22 Batch  388 / 525  Training Loss  0.005297850351780653\n",
            "Epoch  22 Batch  389 / 525  Training Loss  0.0025383851025253534\n",
            "Epoch  22 Batch  390 / 525  Training Loss  0.0017812529113143682\n",
            "Epoch  22 Batch  391 / 525  Training Loss  0.000740395684260875\n",
            "Epoch  22 Batch  392 / 525  Training Loss  0.0014331419952213764\n",
            "Epoch  22 Batch  393 / 525  Training Loss  0.003631200175732374\n",
            "Epoch  22 Batch  394 / 525  Training Loss  0.0020777075551450253\n",
            "Epoch  22 Batch  395 / 525  Training Loss  0.0016330633079633117\n",
            "Epoch  22 Batch  396 / 525  Training Loss  0.007236531935632229\n",
            "Epoch  22 Batch  397 / 525  Training Loss  0.0014504326973110437\n",
            "Epoch  22 Batch  398 / 525  Training Loss  0.0016133316094055772\n",
            "Epoch  22 Batch  399 / 525  Training Loss  0.0015031457878649235\n",
            "Epoch  22 Batch  400 / 525  Training Loss  0.005538013763725758\n",
            "Epoch  22 Batch  401 / 525  Training Loss  0.002658512443304062\n",
            "Epoch  22 Batch  402 / 525  Training Loss  0.002911371411755681\n",
            "Epoch  22 Batch  403 / 525  Training Loss  0.002820429392158985\n",
            "Epoch  22 Batch  404 / 525  Training Loss  0.0010403332998976111\n",
            "Epoch  22 Batch  405 / 525  Training Loss  0.007260728627443314\n",
            "Epoch  22 Batch  406 / 525  Training Loss  0.002059528836980462\n",
            "Epoch  22 Batch  407 / 525  Training Loss  0.0023216749541461468\n",
            "Epoch  22 Batch  408 / 525  Training Loss  0.0039709825068712234\n",
            "Epoch  22 Batch  409 / 525  Training Loss  0.0009121097391471267\n",
            "Epoch  22 Batch  410 / 525  Training Loss  0.0041910638101398945\n",
            "Epoch  22 Batch  411 / 525  Training Loss  0.0013022462371736765\n",
            "Epoch  22 Batch  412 / 525  Training Loss  0.00593584356829524\n",
            "Epoch  22 Batch  413 / 525  Training Loss  0.0005893591442145407\n",
            "Epoch  22 Batch  414 / 525  Training Loss  0.0021400346886366606\n",
            "Epoch  22 Batch  415 / 525  Training Loss  0.0012977884616702795\n",
            "Epoch  22 Batch  416 / 525  Training Loss  0.008669417351484299\n",
            "Epoch  22 Batch  417 / 525  Training Loss  0.0024520731531083584\n",
            "Epoch  22 Batch  418 / 525  Training Loss  0.005638372153043747\n",
            "Epoch  22 Batch  419 / 525  Training Loss  0.003942913841456175\n",
            "Epoch  22 Batch  420 / 525  Training Loss  0.004755307920277119\n",
            "Epoch  22 Batch  421 / 525  Training Loss  0.0028575973119586706\n",
            "Epoch  22 Batch  422 / 525  Training Loss  0.0026696890126913786\n",
            "Epoch  22 Batch  423 / 525  Training Loss  0.007786900736391544\n",
            "Epoch  22 Batch  424 / 525  Training Loss  0.0016953905578702688\n",
            "Epoch  22 Batch  425 / 525  Training Loss  0.0061042471788823605\n",
            "Epoch  22 Batch  426 / 525  Training Loss  0.0016210401663556695\n",
            "Epoch  22 Batch  427 / 525  Training Loss  0.0032911933958530426\n",
            "Epoch  22 Batch  428 / 525  Training Loss  0.009440477006137371\n",
            "Epoch  22 Batch  429 / 525  Training Loss  0.003369665937498212\n",
            "Epoch  22 Batch  430 / 525  Training Loss  0.004024099558591843\n",
            "Epoch  22 Batch  431 / 525  Training Loss  0.003966813441365957\n",
            "Epoch  22 Batch  432 / 525  Training Loss  0.0010491885477676988\n",
            "Epoch  22 Batch  433 / 525  Training Loss  0.010305090807378292\n",
            "Epoch  22 Batch  434 / 525  Training Loss  0.0037472762633115053\n",
            "Epoch  22 Batch  435 / 525  Training Loss  0.002734218956902623\n",
            "Epoch  22 Batch  436 / 525  Training Loss  0.005073887296020985\n",
            "Epoch  22 Batch  437 / 525  Training Loss  0.0054016378708183765\n",
            "Epoch  22 Batch  438 / 525  Training Loss  0.002288275631144643\n",
            "Epoch  22 Batch  439 / 525  Training Loss  0.002482928102836013\n",
            "Epoch  22 Batch  440 / 525  Training Loss  0.0034508283715695143\n",
            "Epoch  22 Batch  441 / 525  Training Loss  0.002314684446901083\n",
            "Epoch  22 Batch  442 / 525  Training Loss  0.0013625780120491982\n",
            "Epoch  22 Batch  443 / 525  Training Loss  0.004564801696687937\n",
            "Epoch  22 Batch  444 / 525  Training Loss  0.005772293545305729\n",
            "Epoch  22 Batch  445 / 525  Training Loss  0.005315569695085287\n",
            "Epoch  22 Batch  446 / 525  Training Loss  0.0025648134760558605\n",
            "Epoch  22 Batch  447 / 525  Training Loss  0.0038691009394824505\n",
            "Epoch  22 Batch  448 / 525  Training Loss  0.0013612377224490047\n",
            "Epoch  22 Batch  449 / 525  Training Loss  0.001048243953846395\n",
            "Epoch  22 Batch  450 / 525  Training Loss  0.003001669654622674\n",
            "Epoch  22 Batch  451 / 525  Training Loss  0.001791672082617879\n",
            "Epoch  22 Batch  452 / 525  Training Loss  0.003115652361884713\n",
            "Epoch  22 Batch  453 / 525  Training Loss  0.00333653693087399\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  22 Batch  454 / 525  Training Loss  0.0032608802430331707\n",
            "Epoch  22 Batch  455 / 525  Training Loss  0.0012104574125260115\n",
            "Epoch  22 Batch  456 / 525  Training Loss  0.006577902939170599\n",
            "Epoch  22 Batch  457 / 525  Training Loss  0.003931019455194473\n",
            "Epoch  22 Batch  458 / 525  Training Loss  0.008869482204318047\n",
            "Epoch  22 Batch  459 / 525  Training Loss  0.005505050532519817\n",
            "Epoch  22 Batch  460 / 525  Training Loss  0.010319875553250313\n",
            "Epoch  22 Batch  461 / 525  Training Loss  0.008985677734017372\n",
            "Epoch  22 Batch  462 / 525  Training Loss  0.007969305850565434\n",
            "Epoch  22 Batch  463 / 525  Training Loss  0.004939376376569271\n",
            "Epoch  22 Batch  464 / 525  Training Loss  0.0035989475436508656\n",
            "Epoch  22 Batch  465 / 525  Training Loss  0.0017188589554280043\n",
            "Epoch  22 Batch  466 / 525  Training Loss  0.0042237588204443455\n",
            "Epoch  22 Batch  467 / 525  Training Loss  0.007527968380600214\n",
            "Epoch  22 Batch  468 / 525  Training Loss  0.0010369475930929184\n",
            "Epoch  22 Batch  469 / 525  Training Loss  0.0015901478473097086\n",
            "Epoch  22 Batch  470 / 525  Training Loss  0.008174779824912548\n",
            "Epoch  22 Batch  471 / 525  Training Loss  0.006219417322427034\n",
            "Epoch  22 Batch  472 / 525  Training Loss  0.003398208413273096\n",
            "Epoch  22 Batch  473 / 525  Training Loss  0.0038549720775336027\n",
            "Epoch  22 Batch  474 / 525  Training Loss  0.0042465911246836185\n",
            "Epoch  22 Batch  475 / 525  Training Loss  0.00397639162838459\n",
            "Epoch  22 Batch  476 / 525  Training Loss  0.0027986408676952124\n",
            "Epoch  22 Batch  477 / 525  Training Loss  0.0028316364623606205\n",
            "Epoch  22 Batch  478 / 525  Training Loss  0.002735053887590766\n",
            "Epoch  22 Batch  479 / 525  Training Loss  0.010480853728950024\n",
            "Epoch  22 Batch  480 / 525  Training Loss  0.004220806527882814\n",
            "Epoch  22 Batch  481 / 525  Training Loss  0.004492998123168945\n",
            "Epoch  22 Batch  482 / 525  Training Loss  0.002592098666355014\n",
            "Epoch  22 Batch  483 / 525  Training Loss  0.009631788358092308\n",
            "Epoch  22 Batch  484 / 525  Training Loss  0.004072899464517832\n",
            "Epoch  22 Batch  485 / 525  Training Loss  0.004381036385893822\n",
            "Epoch  22 Batch  486 / 525  Training Loss  0.001854164875112474\n",
            "Epoch  22 Batch  487 / 525  Training Loss  0.000980199663899839\n",
            "Epoch  22 Batch  488 / 525  Training Loss  0.002944502281025052\n",
            "Epoch  22 Batch  489 / 525  Training Loss  0.0038472507148981094\n",
            "Epoch  22 Batch  490 / 525  Training Loss  0.004549947567284107\n",
            "Epoch  22 Batch  491 / 525  Training Loss  0.0027131279930472374\n",
            "Epoch  22 Batch  492 / 525  Training Loss  0.0016845518257468939\n",
            "Epoch  22 Batch  493 / 525  Training Loss  0.007016821298748255\n",
            "Epoch  22 Batch  494 / 525  Training Loss  0.004089668858796358\n",
            "Epoch  22 Batch  495 / 525  Training Loss  0.006635437253862619\n",
            "Epoch  22 Batch  496 / 525  Training Loss  0.003899182425811887\n",
            "Epoch  22 Batch  497 / 525  Training Loss  0.0028345962055027485\n",
            "Epoch  22 Batch  498 / 525  Training Loss  0.002097581047564745\n",
            "Epoch  22 Batch  499 / 525  Training Loss  0.0017461737152189016\n",
            "Epoch  22 Batch  500 / 525  Training Loss  0.001612471416592598\n",
            "Epoch  22 Batch  501 / 525  Training Loss  0.0012064839247614145\n",
            "Epoch  22 Batch  502 / 525  Training Loss  0.004069051705300808\n",
            "Epoch  22 Batch  503 / 525  Training Loss  0.004096143878996372\n",
            "Epoch  22 Batch  504 / 525  Training Loss  0.0018677543848752975\n",
            "Epoch  22 Batch  505 / 525  Training Loss  0.0014159802813082933\n",
            "Epoch  22 Batch  506 / 525  Training Loss  0.005737605970352888\n",
            "Epoch  22 Batch  507 / 525  Training Loss  0.006730372551828623\n",
            "Epoch  22 Batch  508 / 525  Training Loss  0.002108314773067832\n",
            "Epoch  22 Batch  509 / 525  Training Loss  0.0012904061004519463\n",
            "Epoch  22 Batch  510 / 525  Training Loss  0.0018680784851312637\n",
            "Epoch  22 Batch  511 / 525  Training Loss  0.0011074256617575884\n",
            "Epoch  22 Batch  512 / 525  Training Loss  0.007812239229679108\n",
            "Epoch  22 Batch  513 / 525  Training Loss  0.00321086123585701\n",
            "Epoch  22 Batch  514 / 525  Training Loss  0.013209810480475426\n",
            "Epoch  22 Batch  515 / 525  Training Loss  0.0057264091446995735\n",
            "Epoch  22 Batch  516 / 525  Training Loss  0.0016829086234793067\n",
            "Epoch  22 Batch  517 / 525  Training Loss  0.0038167759776115417\n",
            "Epoch  22 Batch  518 / 525  Training Loss  0.0009041969897225499\n",
            "Epoch  22 Batch  519 / 525  Training Loss  0.002534836996346712\n",
            "Epoch  22 Batch  520 / 525  Training Loss  0.0026317951269447803\n",
            "Epoch  22 Batch  521 / 525  Training Loss  0.003606275888159871\n",
            "Epoch  22 Batch  522 / 525  Training Loss  0.0007553741452284157\n",
            "Epoch  22 Batch  523 / 525  Training Loss  0.0077649010345339775\n",
            "Epoch  22 Batch  524 / 525  Training Loss  0.007053200155496597\n",
            "  23    |    -    |   0.003815   | 59.916667\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 23\n",
            "Epoch  23 Batch  0 / 525  Training Loss  0.001789927831850946\n",
            "Epoch  23 Batch  1 / 525  Training Loss  0.001355858868919313\n",
            "Epoch  23 Batch  2 / 525  Training Loss  0.0038274272810667753\n",
            "Epoch  23 Batch  3 / 525  Training Loss  0.0019324434688314795\n",
            "Epoch  23 Batch  4 / 525  Training Loss  0.0012119609164074063\n",
            "Epoch  23 Batch  5 / 525  Training Loss  0.0055250623263418674\n",
            "Epoch  23 Batch  6 / 525  Training Loss  0.001262504025362432\n",
            "Epoch  23 Batch  7 / 525  Training Loss  0.0005565465544350445\n",
            "Epoch  23 Batch  8 / 525  Training Loss  0.0034986878745257854\n",
            "Epoch  23 Batch  9 / 525  Training Loss  0.0031584673561155796\n",
            "Epoch  23 Batch  10 / 525  Training Loss  0.0026935776695609093\n",
            "Epoch  23 Batch  11 / 525  Training Loss  0.0021515279076993465\n",
            "Epoch  23 Batch  12 / 525  Training Loss  0.0010804592166095972\n",
            "Epoch  23 Batch  13 / 525  Training Loss  0.0015385791193693876\n",
            "Epoch  23 Batch  14 / 525  Training Loss  0.0010849782265722752\n",
            "Epoch  23 Batch  15 / 525  Training Loss  0.0030066173058003187\n",
            "Epoch  23 Batch  16 / 525  Training Loss  0.001081337803043425\n",
            "Epoch  23 Batch  17 / 525  Training Loss  0.004848092328757048\n",
            "Epoch  23 Batch  18 / 525  Training Loss  0.0006130937836132944\n",
            "Epoch  23 Batch  19 / 525  Training Loss  0.0010242069838568568\n",
            "Epoch  23 Batch  20 / 525  Training Loss  0.001259074779227376\n",
            "Epoch  23 Batch  21 / 525  Training Loss  0.002277425955981016\n",
            "Epoch  23 Batch  22 / 525  Training Loss  0.0008985972963273525\n",
            "Epoch  23 Batch  23 / 525  Training Loss  0.0011038911761716008\n",
            "Epoch  23 Batch  24 / 525  Training Loss  0.001408201758749783\n",
            "Epoch  23 Batch  25 / 525  Training Loss  0.0004668176989071071\n",
            "Epoch  23 Batch  26 / 525  Training Loss  0.0008060360560193658\n",
            "Epoch  23 Batch  27 / 525  Training Loss  0.0018400304252281785\n",
            "Epoch  23 Batch  28 / 525  Training Loss  0.0025167313870042562\n",
            "Epoch  23 Batch  29 / 525  Training Loss  0.0008465374703519046\n",
            "Epoch  23 Batch  30 / 525  Training Loss  0.0010496517643332481\n",
            "Epoch  23 Batch  31 / 525  Training Loss  0.0021215721499174833\n",
            "Epoch  23 Batch  32 / 525  Training Loss  0.0021247840486466885\n",
            "Epoch  23 Batch  33 / 525  Training Loss  0.0031322408467531204\n",
            "Epoch  23 Batch  34 / 525  Training Loss  0.0006142649217508733\n",
            "Epoch  23 Batch  35 / 525  Training Loss  0.0026832257863134146\n",
            "Epoch  23 Batch  36 / 525  Training Loss  0.002600383246317506\n",
            "Epoch  23 Batch  37 / 525  Training Loss  0.0012506421189755201\n",
            "Epoch  23 Batch  38 / 525  Training Loss  0.0013247245224192739\n",
            "Epoch  23 Batch  39 / 525  Training Loss  0.0011142103467136621\n",
            "Epoch  23 Batch  40 / 525  Training Loss  0.0009601846104487777\n",
            "Epoch  23 Batch  41 / 525  Training Loss  0.0031641169916838408\n",
            "Epoch  23 Batch  42 / 525  Training Loss  0.0016412825789302588\n",
            "Epoch  23 Batch  43 / 525  Training Loss  0.001009683241136372\n",
            "Epoch  23 Batch  44 / 525  Training Loss  0.002282810863107443\n",
            "Epoch  23 Batch  45 / 525  Training Loss  0.0016208036104217172\n",
            "Epoch  23 Batch  46 / 525  Training Loss  0.0013536402257159352\n",
            "Epoch  23 Batch  47 / 525  Training Loss  0.0021053289528936148\n",
            "Epoch  23 Batch  48 / 525  Training Loss  0.0025274630170315504\n",
            "Epoch  23 Batch  49 / 525  Training Loss  0.0011408941354602575\n",
            "Epoch  23 Batch  50 / 525  Training Loss  0.0010112117743119597\n",
            "Epoch  23 Batch  51 / 525  Training Loss  0.0008782789227552712\n",
            "Epoch  23 Batch  52 / 525  Training Loss  0.0018127614166587591\n",
            "Epoch  23 Batch  53 / 525  Training Loss  0.0012057421263307333\n",
            "Epoch  23 Batch  54 / 525  Training Loss  0.008065321482717991\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  23 Batch  55 / 525  Training Loss  0.0007709448109380901\n",
            "Epoch  23 Batch  56 / 525  Training Loss  0.003220055950805545\n",
            "Epoch  23 Batch  57 / 525  Training Loss  0.0014024053234606981\n",
            "Epoch  23 Batch  58 / 525  Training Loss  0.0013431787956506014\n",
            "Epoch  23 Batch  59 / 525  Training Loss  0.003245947416871786\n",
            "Epoch  23 Batch  60 / 525  Training Loss  0.003852900816127658\n",
            "Epoch  23 Batch  61 / 525  Training Loss  0.003703490598127246\n",
            "Epoch  23 Batch  62 / 525  Training Loss  0.007471933960914612\n",
            "Epoch  23 Batch  63 / 525  Training Loss  0.0017092028865590692\n",
            "Epoch  23 Batch  64 / 525  Training Loss  0.002565351314842701\n",
            "Epoch  23 Batch  65 / 525  Training Loss  0.0012164429062977433\n",
            "Epoch  23 Batch  66 / 525  Training Loss  0.0011330314446240664\n",
            "Epoch  23 Batch  67 / 525  Training Loss  0.0019504480296745896\n",
            "Epoch  23 Batch  68 / 525  Training Loss  0.0007217530510388315\n",
            "Epoch  23 Batch  69 / 525  Training Loss  0.000701864599250257\n",
            "Epoch  23 Batch  70 / 525  Training Loss  0.0021945599000900984\n",
            "Epoch  23 Batch  71 / 525  Training Loss  0.0018939420115202665\n",
            "Epoch  23 Batch  72 / 525  Training Loss  0.0006823705043643713\n",
            "Epoch  23 Batch  73 / 525  Training Loss  0.0036637778393924236\n",
            "Epoch  23 Batch  74 / 525  Training Loss  0.0007378676673397422\n",
            "Epoch  23 Batch  75 / 525  Training Loss  0.0019014918943867087\n",
            "Epoch  23 Batch  76 / 525  Training Loss  0.005380817223340273\n",
            "Epoch  23 Batch  77 / 525  Training Loss  0.0012939746957272291\n",
            "Epoch  23 Batch  78 / 525  Training Loss  0.0007531301234848797\n",
            "Epoch  23 Batch  79 / 525  Training Loss  0.0009394095977768302\n",
            "Epoch  23 Batch  80 / 525  Training Loss  0.001227359869517386\n",
            "Epoch  23 Batch  81 / 525  Training Loss  0.0007012281566858292\n",
            "Epoch  23 Batch  82 / 525  Training Loss  0.006236963905394077\n",
            "Epoch  23 Batch  83 / 525  Training Loss  0.0018619894981384277\n",
            "Epoch  23 Batch  84 / 525  Training Loss  0.0018548916559666395\n",
            "Epoch  23 Batch  85 / 525  Training Loss  0.00036268451367504895\n",
            "Epoch  23 Batch  86 / 525  Training Loss  0.0016872922424227\n",
            "Epoch  23 Batch  87 / 525  Training Loss  0.0009023502352647483\n",
            "Epoch  23 Batch  88 / 525  Training Loss  0.0006111374823376536\n",
            "Epoch  23 Batch  89 / 525  Training Loss  0.0005239173769950867\n",
            "Epoch  23 Batch  90 / 525  Training Loss  0.0015530341770499945\n",
            "Epoch  23 Batch  91 / 525  Training Loss  0.00722027663141489\n",
            "Epoch  23 Batch  92 / 525  Training Loss  0.001260388526134193\n",
            "Epoch  23 Batch  93 / 525  Training Loss  0.0008165590697899461\n",
            "Epoch  23 Batch  94 / 525  Training Loss  0.00034782610600814223\n",
            "Epoch  23 Batch  95 / 525  Training Loss  0.004211559426039457\n",
            "Epoch  23 Batch  96 / 525  Training Loss  0.00046061904868111014\n",
            "Epoch  23 Batch  97 / 525  Training Loss  0.002780308248475194\n",
            "Epoch  23 Batch  98 / 525  Training Loss  0.0024670169223099947\n",
            "Epoch  23 Batch  99 / 525  Training Loss  0.004467934835702181\n",
            "Epoch  23 Batch  100 / 525  Training Loss  0.0010813677217811346\n",
            "Epoch  23 Batch  101 / 525  Training Loss  0.0019436152651906013\n",
            "Epoch  23 Batch  102 / 525  Training Loss  0.0025879614986479282\n",
            "Epoch  23 Batch  103 / 525  Training Loss  0.0008422690443694592\n",
            "Epoch  23 Batch  104 / 525  Training Loss  0.0004963685059919953\n",
            "Epoch  23 Batch  105 / 525  Training Loss  0.0032148032914847136\n",
            "Epoch  23 Batch  106 / 525  Training Loss  0.0022411849349737167\n",
            "Epoch  23 Batch  107 / 525  Training Loss  0.002921796403825283\n",
            "Epoch  23 Batch  108 / 525  Training Loss  0.004087063483893871\n",
            "Epoch  23 Batch  109 / 525  Training Loss  0.0004861181660089642\n",
            "Epoch  23 Batch  110 / 525  Training Loss  0.0009504128247499466\n",
            "Epoch  23 Batch  111 / 525  Training Loss  0.002783462405204773\n",
            "Epoch  23 Batch  112 / 525  Training Loss  0.009011260233819485\n",
            "Epoch  23 Batch  113 / 525  Training Loss  0.002894367789849639\n",
            "Epoch  23 Batch  114 / 525  Training Loss  0.004497026093304157\n",
            "Epoch  23 Batch  115 / 525  Training Loss  0.0009094789857044816\n",
            "Epoch  23 Batch  116 / 525  Training Loss  0.0013567155692726374\n",
            "Epoch  23 Batch  117 / 525  Training Loss  0.0008379261707887053\n",
            "Epoch  23 Batch  118 / 525  Training Loss  0.0007984255207702518\n",
            "Epoch  23 Batch  119 / 525  Training Loss  0.0009613618021830916\n",
            "Epoch  23 Batch  120 / 525  Training Loss  0.0008220082963816822\n",
            "Epoch  23 Batch  121 / 525  Training Loss  0.000827799376565963\n",
            "Epoch  23 Batch  122 / 525  Training Loss  0.0005387332057580352\n",
            "Epoch  23 Batch  123 / 525  Training Loss  0.0007917402544990182\n",
            "Epoch  23 Batch  124 / 525  Training Loss  0.0012360436376184225\n",
            "Epoch  23 Batch  125 / 525  Training Loss  0.002244891133159399\n",
            "Epoch  23 Batch  126 / 525  Training Loss  0.00145988748408854\n",
            "Epoch  23 Batch  127 / 525  Training Loss  0.0015782259870320559\n",
            "Epoch  23 Batch  128 / 525  Training Loss  0.0010474787559360266\n",
            "Epoch  23 Batch  129 / 525  Training Loss  0.0043660434894263744\n",
            "Epoch  23 Batch  130 / 525  Training Loss  0.0013130998704582453\n",
            "Epoch  23 Batch  131 / 525  Training Loss  0.0009677502093836665\n",
            "Epoch  23 Batch  132 / 525  Training Loss  0.0027356871869415045\n",
            "Epoch  23 Batch  133 / 525  Training Loss  0.0011186737101525068\n",
            "Epoch  23 Batch  134 / 525  Training Loss  0.0023263157345354557\n",
            "Epoch  23 Batch  135 / 525  Training Loss  0.0008938208920881152\n",
            "Epoch  23 Batch  136 / 525  Training Loss  0.0018597713205963373\n",
            "Epoch  23 Batch  137 / 525  Training Loss  0.0006253247265703976\n",
            "Epoch  23 Batch  138 / 525  Training Loss  0.000915449287276715\n",
            "Epoch  23 Batch  139 / 525  Training Loss  0.0010066296672448516\n",
            "Epoch  23 Batch  140 / 525  Training Loss  0.0004528659919742495\n",
            "Epoch  23 Batch  141 / 525  Training Loss  0.0018652866128832102\n",
            "Epoch  23 Batch  142 / 525  Training Loss  0.0020188300404697657\n",
            "Epoch  23 Batch  143 / 525  Training Loss  0.0012806587619706988\n",
            "Epoch  23 Batch  144 / 525  Training Loss  0.0011801811633631587\n",
            "Epoch  23 Batch  145 / 525  Training Loss  0.0011821825755760074\n",
            "Epoch  23 Batch  146 / 525  Training Loss  0.0018048286437988281\n",
            "Epoch  23 Batch  147 / 525  Training Loss  0.0009915938135236502\n",
            "Epoch  23 Batch  148 / 525  Training Loss  0.001403611502610147\n",
            "Epoch  23 Batch  149 / 525  Training Loss  0.0011343719670549035\n",
            "Epoch  23 Batch  150 / 525  Training Loss  0.0067224325612187386\n",
            "Epoch  23 Batch  151 / 525  Training Loss  0.0009386312449350953\n",
            "Epoch  23 Batch  152 / 525  Training Loss  0.0030454439111053944\n",
            "Epoch  23 Batch  153 / 525  Training Loss  0.003316172631457448\n",
            "Epoch  23 Batch  154 / 525  Training Loss  0.0028330269269645214\n",
            "Epoch  23 Batch  155 / 525  Training Loss  0.0007761694141663611\n",
            "Epoch  23 Batch  156 / 525  Training Loss  0.007268272340297699\n",
            "Epoch  23 Batch  157 / 525  Training Loss  0.0011179590364918113\n",
            "Epoch  23 Batch  158 / 525  Training Loss  0.0048016756772994995\n",
            "Epoch  23 Batch  159 / 525  Training Loss  0.008200056850910187\n",
            "Epoch  23 Batch  160 / 525  Training Loss  0.0010405579814687371\n",
            "Epoch  23 Batch  161 / 525  Training Loss  0.004568254575133324\n",
            "Epoch  23 Batch  162 / 525  Training Loss  0.0009558604215271771\n",
            "Epoch  23 Batch  163 / 525  Training Loss  0.01021136436611414\n",
            "Epoch  23 Batch  164 / 525  Training Loss  0.00295375008136034\n",
            "Epoch  23 Batch  165 / 525  Training Loss  0.0026972258929163218\n",
            "Epoch  23 Batch  166 / 525  Training Loss  0.001605355180799961\n",
            "Epoch  23 Batch  167 / 525  Training Loss  0.0017387833213433623\n",
            "Epoch  23 Batch  168 / 525  Training Loss  0.003580428659915924\n",
            "Epoch  23 Batch  169 / 525  Training Loss  0.00392030319198966\n",
            "Epoch  23 Batch  170 / 525  Training Loss  0.0013541850494220853\n",
            "Epoch  23 Batch  171 / 525  Training Loss  0.0019269265467301011\n",
            "Epoch  23 Batch  172 / 525  Training Loss  0.003590318839997053\n",
            "Epoch  23 Batch  173 / 525  Training Loss  0.0009115862776525319\n",
            "Epoch  23 Batch  174 / 525  Training Loss  0.0009423015872016549\n",
            "Epoch  23 Batch  175 / 525  Training Loss  0.00046920200111344457\n",
            "Epoch  23 Batch  176 / 525  Training Loss  0.000865658454131335\n",
            "Epoch  23 Batch  177 / 525  Training Loss  0.0030695798341184855\n",
            "Epoch  23 Batch  178 / 525  Training Loss  0.001558213378302753\n",
            "Epoch  23 Batch  179 / 525  Training Loss  0.0017506338190287352\n",
            "Epoch  23 Batch  180 / 525  Training Loss  0.0033403136767446995\n",
            "Epoch  23 Batch  181 / 525  Training Loss  0.002671572845429182\n",
            "Epoch  23 Batch  182 / 525  Training Loss  0.001736729172989726\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  23 Batch  183 / 525  Training Loss  0.0014819888165220618\n",
            "Epoch  23 Batch  184 / 525  Training Loss  0.0016290569910779595\n",
            "Epoch  23 Batch  185 / 525  Training Loss  0.0009032180532813072\n",
            "Epoch  23 Batch  186 / 525  Training Loss  0.0024037014227360487\n",
            "Epoch  23 Batch  187 / 525  Training Loss  0.0007502397056668997\n",
            "Epoch  23 Batch  188 / 525  Training Loss  0.0007756089908070862\n",
            "Epoch  23 Batch  189 / 525  Training Loss  0.002062201267108321\n",
            "Epoch  23 Batch  190 / 525  Training Loss  0.0028264787979424\n",
            "Epoch  23 Batch  191 / 525  Training Loss  0.000778177403844893\n",
            "Epoch  23 Batch  192 / 525  Training Loss  0.0013439881149679422\n",
            "Epoch  23 Batch  193 / 525  Training Loss  0.0028306853491812944\n",
            "Epoch  23 Batch  194 / 525  Training Loss  0.0019283313304185867\n",
            "Epoch  23 Batch  195 / 525  Training Loss  0.002743746154010296\n",
            "Epoch  23 Batch  196 / 525  Training Loss  0.0006759942625649273\n",
            "Epoch  23 Batch  197 / 525  Training Loss  0.001026661484502256\n",
            "Epoch  23 Batch  198 / 525  Training Loss  0.004655875731259584\n",
            "Epoch  23 Batch  199 / 525  Training Loss  0.000730286817997694\n",
            "Epoch  23 Batch  200 / 525  Training Loss  0.0047049373388290405\n",
            "Epoch  23 Batch  201 / 525  Training Loss  0.0024070655927062035\n",
            "Epoch  23 Batch  202 / 525  Training Loss  0.0017802448710426688\n",
            "Epoch  23 Batch  203 / 525  Training Loss  0.001614961540326476\n",
            "Epoch  23 Batch  204 / 525  Training Loss  0.0021045836620032787\n",
            "Epoch  23 Batch  205 / 525  Training Loss  0.0032742645125836134\n",
            "Epoch  23 Batch  206 / 525  Training Loss  0.0006753037450835109\n",
            "Epoch  23 Batch  207 / 525  Training Loss  0.0015635559102520347\n",
            "Epoch  23 Batch  208 / 525  Training Loss  0.0017153900116682053\n",
            "Epoch  23 Batch  209 / 525  Training Loss  0.0027046366594731808\n",
            "Epoch  23 Batch  210 / 525  Training Loss  0.0015251301229000092\n",
            "Epoch  23 Batch  211 / 525  Training Loss  0.0011553155491128564\n",
            "Epoch  23 Batch  212 / 525  Training Loss  0.0013563097454607487\n",
            "Epoch  23 Batch  213 / 525  Training Loss  0.0007187175797298551\n",
            "Epoch  23 Batch  214 / 525  Training Loss  0.0016045259544625878\n",
            "Epoch  23 Batch  215 / 525  Training Loss  0.001333882799372077\n",
            "Epoch  23 Batch  216 / 525  Training Loss  0.003330950392410159\n",
            "Epoch  23 Batch  217 / 525  Training Loss  0.0012781597906723619\n",
            "Epoch  23 Batch  218 / 525  Training Loss  0.0007875319570302963\n",
            "Epoch  23 Batch  219 / 525  Training Loss  0.0015529795782640576\n",
            "Epoch  23 Batch  220 / 525  Training Loss  0.0025838646106421947\n",
            "Epoch  23 Batch  221 / 525  Training Loss  0.0007504846435040236\n",
            "Epoch  23 Batch  222 / 525  Training Loss  0.0021063934545964003\n",
            "Epoch  23 Batch  223 / 525  Training Loss  0.000937964185141027\n",
            "Epoch  23 Batch  224 / 525  Training Loss  0.0012696317862719297\n",
            "Epoch  23 Batch  225 / 525  Training Loss  0.0012633216101676226\n",
            "Epoch  23 Batch  226 / 525  Training Loss  0.0015027659246698022\n",
            "Epoch  23 Batch  227 / 525  Training Loss  0.002290172968059778\n",
            "Epoch  23 Batch  228 / 525  Training Loss  0.006450156215578318\n",
            "Epoch  23 Batch  229 / 525  Training Loss  0.005269927904009819\n",
            "Epoch  23 Batch  230 / 525  Training Loss  0.0013305454049259424\n",
            "Epoch  23 Batch  231 / 525  Training Loss  0.0018589090323075652\n",
            "Epoch  23 Batch  232 / 525  Training Loss  0.0011316644959151745\n",
            "Epoch  23 Batch  233 / 525  Training Loss  0.0018044853350147605\n",
            "Epoch  23 Batch  234 / 525  Training Loss  0.000659074867144227\n",
            "Epoch  23 Batch  235 / 525  Training Loss  0.000951190129853785\n",
            "Epoch  23 Batch  236 / 525  Training Loss  0.00318111851811409\n",
            "Epoch  23 Batch  237 / 525  Training Loss  0.0003728116862475872\n",
            "Epoch  23 Batch  238 / 525  Training Loss  0.0012965842615813017\n",
            "Epoch  23 Batch  239 / 525  Training Loss  0.0005406954442150891\n",
            "Epoch  23 Batch  240 / 525  Training Loss  0.0005209665978327394\n",
            "Epoch  23 Batch  241 / 525  Training Loss  0.0007780720479786396\n",
            "Epoch  23 Batch  242 / 525  Training Loss  0.0015801578992977738\n",
            "Epoch  23 Batch  243 / 525  Training Loss  0.004069281741976738\n",
            "Epoch  23 Batch  244 / 525  Training Loss  0.0027899853885173798\n",
            "Epoch  23 Batch  245 / 525  Training Loss  0.0008454184862785041\n",
            "Epoch  23 Batch  246 / 525  Training Loss  0.00223371060565114\n",
            "Epoch  23 Batch  247 / 525  Training Loss  0.0020009400323033333\n",
            "Epoch  23 Batch  248 / 525  Training Loss  0.001363982679322362\n",
            "Epoch  23 Batch  249 / 525  Training Loss  0.001260091783478856\n",
            "Epoch  23 Batch  250 / 525  Training Loss  0.0019739032723009586\n",
            "Epoch  23 Batch  251 / 525  Training Loss  0.0006068564252927899\n",
            "Epoch  23 Batch  252 / 525  Training Loss  0.0011352896690368652\n",
            "Epoch  23 Batch  253 / 525  Training Loss  0.0020116379018872976\n",
            "Epoch  23 Batch  254 / 525  Training Loss  0.0012988843955099583\n",
            "Epoch  23 Batch  255 / 525  Training Loss  0.0027073961682617664\n",
            "Epoch  23 Batch  256 / 525  Training Loss  0.0009229470742866397\n",
            "Epoch  23 Batch  257 / 525  Training Loss  0.0035886410623788834\n",
            "Epoch  23 Batch  258 / 525  Training Loss  0.0023322084452956915\n",
            "Epoch  23 Batch  259 / 525  Training Loss  0.0019539245404303074\n",
            "Epoch  23 Batch  260 / 525  Training Loss  0.0004512332961894572\n",
            "Epoch  23 Batch  261 / 525  Training Loss  0.000968045205809176\n",
            "Epoch  23 Batch  262 / 525  Training Loss  0.0011938249226659536\n",
            "Epoch  23 Batch  263 / 525  Training Loss  0.0007975439657457173\n",
            "Epoch  23 Batch  264 / 525  Training Loss  0.0013668896863237023\n",
            "Epoch  23 Batch  265 / 525  Training Loss  0.0006902816821821034\n",
            "Epoch  23 Batch  266 / 525  Training Loss  0.011199836619198322\n",
            "Epoch  23 Batch  267 / 525  Training Loss  0.0011520085390657187\n",
            "Epoch  23 Batch  268 / 525  Training Loss  0.001895308494567871\n",
            "Epoch  23 Batch  269 / 525  Training Loss  0.0027640429325401783\n",
            "Epoch  23 Batch  270 / 525  Training Loss  0.0008337986655533314\n",
            "Epoch  23 Batch  271 / 525  Training Loss  0.0011430543381720781\n",
            "Epoch  23 Batch  272 / 525  Training Loss  0.0008756316383369267\n",
            "Epoch  23 Batch  273 / 525  Training Loss  0.003789033740758896\n",
            "Epoch  23 Batch  274 / 525  Training Loss  0.001277758041396737\n",
            "Epoch  23 Batch  275 / 525  Training Loss  0.0006655847537331283\n",
            "Epoch  23 Batch  276 / 525  Training Loss  0.0029922204557806253\n",
            "Epoch  23 Batch  277 / 525  Training Loss  0.0009602885693311691\n",
            "Epoch  23 Batch  278 / 525  Training Loss  0.0019919818732887506\n",
            "Epoch  23 Batch  279 / 525  Training Loss  0.00041393499122932553\n",
            "Epoch  23 Batch  280 / 525  Training Loss  0.0018611749401316047\n",
            "Epoch  23 Batch  281 / 525  Training Loss  0.0020008136052638292\n",
            "Epoch  23 Batch  282 / 525  Training Loss  0.0018504969775676727\n",
            "Epoch  23 Batch  283 / 525  Training Loss  0.0009851227514445782\n",
            "Epoch  23 Batch  284 / 525  Training Loss  0.004389212466776371\n",
            "Epoch  23 Batch  285 / 525  Training Loss  0.0010797545546665788\n",
            "Epoch  23 Batch  286 / 525  Training Loss  0.0009229760617017746\n",
            "Epoch  23 Batch  287 / 525  Training Loss  0.0017386816907674074\n",
            "Epoch  23 Batch  288 / 525  Training Loss  0.0014490282628685236\n",
            "Epoch  23 Batch  289 / 525  Training Loss  0.001234796829521656\n",
            "Epoch  23 Batch  290 / 525  Training Loss  0.0005295026930980384\n",
            "Epoch  23 Batch  291 / 525  Training Loss  0.02562379464507103\n",
            "Epoch  23 Batch  292 / 525  Training Loss  0.004124658182263374\n",
            "Epoch  23 Batch  293 / 525  Training Loss  0.005314856301993132\n",
            "Epoch  23 Batch  294 / 525  Training Loss  0.0006188941770233214\n",
            "Epoch  23 Batch  295 / 525  Training Loss  0.0011623011669144034\n",
            "Epoch  23 Batch  296 / 525  Training Loss  0.0029323860071599483\n",
            "Epoch  23 Batch  297 / 525  Training Loss  0.0018250005086883903\n",
            "Epoch  23 Batch  298 / 525  Training Loss  0.0009992136619985104\n",
            "Epoch  23 Batch  299 / 525  Training Loss  0.0011398263741284609\n",
            "Epoch  23 Batch  300 / 525  Training Loss  0.0009159600595012307\n",
            "Epoch  23 Batch  301 / 525  Training Loss  0.0011056424118578434\n",
            "Epoch  23 Batch  302 / 525  Training Loss  0.003259476274251938\n",
            "Epoch  23 Batch  303 / 525  Training Loss  0.0014536201488226652\n",
            "Epoch  23 Batch  304 / 525  Training Loss  0.0010271372739225626\n",
            "Epoch  23 Batch  305 / 525  Training Loss  0.0011579181300476193\n",
            "Epoch  23 Batch  306 / 525  Training Loss  0.0013870105613023043\n",
            "Epoch  23 Batch  307 / 525  Training Loss  0.0068829795345664024\n",
            "Epoch  23 Batch  308 / 525  Training Loss  0.0017497831722721457\n",
            "Epoch  23 Batch  309 / 525  Training Loss  0.0014641237212345004\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  23 Batch  310 / 525  Training Loss  0.0024171804543584585\n",
            "Epoch  23 Batch  311 / 525  Training Loss  0.0010616539511829615\n",
            "Epoch  23 Batch  312 / 525  Training Loss  0.0024015926755964756\n",
            "Epoch  23 Batch  313 / 525  Training Loss  0.0012793625937774777\n",
            "Epoch  23 Batch  314 / 525  Training Loss  0.001102666836231947\n",
            "Epoch  23 Batch  315 / 525  Training Loss  0.005735740065574646\n",
            "Epoch  23 Batch  316 / 525  Training Loss  0.0018905345350503922\n",
            "Epoch  23 Batch  317 / 525  Training Loss  0.0029067290015518665\n",
            "Epoch  23 Batch  318 / 525  Training Loss  0.001737902290187776\n",
            "Epoch  23 Batch  319 / 525  Training Loss  0.0006674012402072549\n",
            "Epoch  23 Batch  320 / 525  Training Loss  0.0006580493645742536\n",
            "Epoch  23 Batch  321 / 525  Training Loss  0.0015653915470466018\n",
            "Epoch  23 Batch  322 / 525  Training Loss  0.004180545918643475\n",
            "Epoch  23 Batch  323 / 525  Training Loss  0.004610806703567505\n",
            "Epoch  23 Batch  324 / 525  Training Loss  0.00415984308347106\n",
            "Epoch  23 Batch  325 / 525  Training Loss  0.000740375486202538\n",
            "Epoch  23 Batch  326 / 525  Training Loss  0.004132061265408993\n",
            "Epoch  23 Batch  327 / 525  Training Loss  0.0017105176812037826\n",
            "Epoch  23 Batch  328 / 525  Training Loss  0.0011976336827501655\n",
            "Epoch  23 Batch  329 / 525  Training Loss  0.0012894400861114264\n",
            "Epoch  23 Batch  330 / 525  Training Loss  0.0009815244702622294\n",
            "Epoch  23 Batch  331 / 525  Training Loss  0.0004901046631857753\n",
            "Epoch  23 Batch  332 / 525  Training Loss  0.006872081663459539\n",
            "Epoch  23 Batch  333 / 525  Training Loss  0.0007190589676611125\n",
            "Epoch  23 Batch  334 / 525  Training Loss  0.015891149640083313\n",
            "Epoch  23 Batch  335 / 525  Training Loss  0.003536481410264969\n",
            "Epoch  23 Batch  336 / 525  Training Loss  0.006220027804374695\n",
            "Epoch  23 Batch  337 / 525  Training Loss  0.0009367297170683742\n",
            "Epoch  23 Batch  338 / 525  Training Loss  0.00551170390099287\n",
            "Epoch  23 Batch  339 / 525  Training Loss  0.0006939388113096356\n",
            "Epoch  23 Batch  340 / 525  Training Loss  0.002082696184515953\n",
            "Epoch  23 Batch  341 / 525  Training Loss  0.0027061770670115948\n",
            "Epoch  23 Batch  342 / 525  Training Loss  0.0033937650732696056\n",
            "Epoch  23 Batch  343 / 525  Training Loss  0.003645210061222315\n",
            "Epoch  23 Batch  344 / 525  Training Loss  0.004160091280937195\n",
            "Epoch  23 Batch  345 / 525  Training Loss  0.0008698414894752204\n",
            "Epoch  23 Batch  346 / 525  Training Loss  0.0016656148945912719\n",
            "Epoch  23 Batch  347 / 525  Training Loss  0.002693957882001996\n",
            "Epoch  23 Batch  348 / 525  Training Loss  0.002836930565536022\n",
            "Epoch  23 Batch  349 / 525  Training Loss  0.0004229508340358734\n",
            "Epoch  23 Batch  350 / 525  Training Loss  0.002379403682425618\n",
            "Epoch  23 Batch  351 / 525  Training Loss  0.0008786889957264066\n",
            "Epoch  23 Batch  352 / 525  Training Loss  0.0007203697459772229\n",
            "Epoch  23 Batch  353 / 525  Training Loss  0.0011157980188727379\n",
            "Epoch  23 Batch  354 / 525  Training Loss  0.0012054808903485537\n",
            "Epoch  23 Batch  355 / 525  Training Loss  0.0012079556472599506\n",
            "Epoch  23 Batch  356 / 525  Training Loss  0.0016530770808458328\n",
            "Epoch  23 Batch  357 / 525  Training Loss  0.002100782934576273\n",
            "Epoch  23 Batch  358 / 525  Training Loss  0.0008607408963143826\n",
            "Epoch  23 Batch  359 / 525  Training Loss  0.002299147890880704\n",
            "Epoch  23 Batch  360 / 525  Training Loss  0.002033839700743556\n",
            "Epoch  23 Batch  361 / 525  Training Loss  0.0012396660167723894\n",
            "Epoch  23 Batch  362 / 525  Training Loss  0.0028393915854394436\n",
            "Epoch  23 Batch  363 / 525  Training Loss  0.001162935746833682\n",
            "Epoch  23 Batch  364 / 525  Training Loss  0.0015875803073868155\n",
            "Epoch  23 Batch  365 / 525  Training Loss  0.00397244468331337\n",
            "Epoch  23 Batch  366 / 525  Training Loss  0.001220579957589507\n",
            "Epoch  23 Batch  367 / 525  Training Loss  0.0013721388531848788\n",
            "Epoch  23 Batch  368 / 525  Training Loss  0.0030994415283203125\n",
            "Epoch  23 Batch  369 / 525  Training Loss  0.0008056815713644028\n",
            "Epoch  23 Batch  370 / 525  Training Loss  0.0075650871731340885\n",
            "Epoch  23 Batch  371 / 525  Training Loss  0.00245877169072628\n",
            "Epoch  23 Batch  372 / 525  Training Loss  0.0015273159369826317\n",
            "Epoch  23 Batch  373 / 525  Training Loss  0.0009180063498206437\n",
            "Epoch  23 Batch  374 / 525  Training Loss  0.0019824854098260403\n",
            "Epoch  23 Batch  375 / 525  Training Loss  0.0008368156850337982\n",
            "Epoch  23 Batch  376 / 525  Training Loss  0.006245761178433895\n",
            "Epoch  23 Batch  377 / 525  Training Loss  0.005871393717825413\n",
            "Epoch  23 Batch  378 / 525  Training Loss  0.008256788365542889\n",
            "Epoch  23 Batch  379 / 525  Training Loss  0.012099208310246468\n",
            "Epoch  23 Batch  380 / 525  Training Loss  0.003764896187931299\n",
            "Epoch  23 Batch  381 / 525  Training Loss  0.00238055526278913\n",
            "Epoch  23 Batch  382 / 525  Training Loss  0.003168610157445073\n",
            "Epoch  23 Batch  383 / 525  Training Loss  0.0022867871448397636\n",
            "Epoch  23 Batch  384 / 525  Training Loss  0.0015753458719700575\n",
            "Epoch  23 Batch  385 / 525  Training Loss  0.0007104186806827784\n",
            "Epoch  23 Batch  386 / 525  Training Loss  0.0014290794497355819\n",
            "Epoch  23 Batch  387 / 525  Training Loss  0.006083846557885408\n",
            "Epoch  23 Batch  388 / 525  Training Loss  0.0007315161637961864\n",
            "Epoch  23 Batch  389 / 525  Training Loss  0.001084079616703093\n",
            "Epoch  23 Batch  390 / 525  Training Loss  0.0015213887672871351\n",
            "Epoch  23 Batch  391 / 525  Training Loss  0.0025137877091765404\n",
            "Epoch  23 Batch  392 / 525  Training Loss  0.001898585818707943\n",
            "Epoch  23 Batch  393 / 525  Training Loss  0.0008994744275696576\n",
            "Epoch  23 Batch  394 / 525  Training Loss  0.0005409341538324952\n",
            "Epoch  23 Batch  395 / 525  Training Loss  0.002601309446617961\n",
            "Epoch  23 Batch  396 / 525  Training Loss  0.0036551826633512974\n",
            "Epoch  23 Batch  397 / 525  Training Loss  0.004485647194087505\n",
            "Epoch  23 Batch  398 / 525  Training Loss  0.0011893885675817728\n",
            "Epoch  23 Batch  399 / 525  Training Loss  0.00561490235850215\n",
            "Epoch  23 Batch  400 / 525  Training Loss  0.0030298870988190174\n",
            "Epoch  23 Batch  401 / 525  Training Loss  0.0068705095909535885\n",
            "Epoch  23 Batch  402 / 525  Training Loss  0.001096443273127079\n",
            "Epoch  23 Batch  403 / 525  Training Loss  0.002657355275005102\n",
            "Epoch  23 Batch  404 / 525  Training Loss  0.0012375838123261929\n",
            "Epoch  23 Batch  405 / 525  Training Loss  0.003173879813402891\n",
            "Epoch  23 Batch  406 / 525  Training Loss  0.0015101328026503325\n",
            "Epoch  23 Batch  407 / 525  Training Loss  0.0024177399463951588\n",
            "Epoch  23 Batch  408 / 525  Training Loss  0.0014956917148083448\n",
            "Epoch  23 Batch  409 / 525  Training Loss  0.008128345012664795\n",
            "Epoch  23 Batch  410 / 525  Training Loss  0.0016939894994720817\n",
            "Epoch  23 Batch  411 / 525  Training Loss  0.0012755899224430323\n",
            "Epoch  23 Batch  412 / 525  Training Loss  0.008245970122516155\n",
            "Epoch  23 Batch  413 / 525  Training Loss  0.00696719903498888\n",
            "Epoch  23 Batch  414 / 525  Training Loss  0.0023634033277630806\n",
            "Epoch  23 Batch  415 / 525  Training Loss  0.0027207275852560997\n",
            "Epoch  23 Batch  416 / 525  Training Loss  0.0023724553175270557\n",
            "Epoch  23 Batch  417 / 525  Training Loss  0.0017666332423686981\n",
            "Epoch  23 Batch  418 / 525  Training Loss  0.0022929206024855375\n",
            "Epoch  23 Batch  419 / 525  Training Loss  0.0013235837686806917\n",
            "Epoch  23 Batch  420 / 525  Training Loss  0.0041421446949243546\n",
            "Epoch  23 Batch  421 / 525  Training Loss  0.002020405139774084\n",
            "Epoch  23 Batch  422 / 525  Training Loss  0.0051343804225325584\n",
            "Epoch  23 Batch  423 / 525  Training Loss  0.0008314436418004334\n",
            "Epoch  23 Batch  424 / 525  Training Loss  0.00505403708666563\n",
            "Epoch  23 Batch  425 / 525  Training Loss  0.00229635345749557\n",
            "Epoch  23 Batch  426 / 525  Training Loss  0.0010253306245431304\n",
            "Epoch  23 Batch  427 / 525  Training Loss  0.001375006977468729\n",
            "Epoch  23 Batch  428 / 525  Training Loss  0.002001826884225011\n",
            "Epoch  23 Batch  429 / 525  Training Loss  0.0019096409669145942\n",
            "Epoch  23 Batch  430 / 525  Training Loss  0.0034219275694340467\n",
            "Epoch  23 Batch  431 / 525  Training Loss  0.00654364749789238\n",
            "Epoch  23 Batch  432 / 525  Training Loss  0.0017940469551831484\n",
            "Epoch  23 Batch  433 / 525  Training Loss  0.005468142684549093\n",
            "Epoch  23 Batch  434 / 525  Training Loss  0.0025406591594219208\n",
            "Epoch  23 Batch  435 / 525  Training Loss  0.0009190038545057178\n",
            "Epoch  23 Batch  436 / 525  Training Loss  0.0017393932212144136\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  23 Batch  437 / 525  Training Loss  0.006745527498424053\n",
            "Epoch  23 Batch  438 / 525  Training Loss  0.0018892328953370452\n",
            "Epoch  23 Batch  439 / 525  Training Loss  0.0014747364912182093\n",
            "Epoch  23 Batch  440 / 525  Training Loss  0.008401977829635143\n",
            "Epoch  23 Batch  441 / 525  Training Loss  0.0030633402056992054\n",
            "Epoch  23 Batch  442 / 525  Training Loss  0.0011559715494513512\n",
            "Epoch  23 Batch  443 / 525  Training Loss  0.0035215909592807293\n",
            "Epoch  23 Batch  444 / 525  Training Loss  0.0002807859855238348\n",
            "Epoch  23 Batch  445 / 525  Training Loss  0.004139742814004421\n",
            "Epoch  23 Batch  446 / 525  Training Loss  0.0025629703886806965\n",
            "Epoch  23 Batch  447 / 525  Training Loss  0.0016665600705891848\n",
            "Epoch  23 Batch  448 / 525  Training Loss  0.009821715764701366\n",
            "Epoch  23 Batch  449 / 525  Training Loss  0.0015622879145666957\n",
            "Epoch  23 Batch  450 / 525  Training Loss  0.005380081944167614\n",
            "Epoch  23 Batch  451 / 525  Training Loss  0.0013775143306702375\n",
            "Epoch  23 Batch  452 / 525  Training Loss  0.0018290020525455475\n",
            "Epoch  23 Batch  453 / 525  Training Loss  0.0016224419232457876\n",
            "Epoch  23 Batch  454 / 525  Training Loss  0.0027119580190628767\n",
            "Epoch  23 Batch  455 / 525  Training Loss  0.0011495222570374608\n",
            "Epoch  23 Batch  456 / 525  Training Loss  0.0017078971723094583\n",
            "Epoch  23 Batch  457 / 525  Training Loss  0.0010276882676407695\n",
            "Epoch  23 Batch  458 / 525  Training Loss  0.003253475297242403\n",
            "Epoch  23 Batch  459 / 525  Training Loss  0.0010830156970769167\n",
            "Epoch  23 Batch  460 / 525  Training Loss  0.0006229617283679545\n",
            "Epoch  23 Batch  461 / 525  Training Loss  0.011010036803781986\n",
            "Epoch  23 Batch  462 / 525  Training Loss  0.0016044335206970572\n",
            "Epoch  23 Batch  463 / 525  Training Loss  0.006112970411777496\n",
            "Epoch  23 Batch  464 / 525  Training Loss  0.0035058625508099794\n",
            "Epoch  23 Batch  465 / 525  Training Loss  0.002088818931952119\n",
            "Epoch  23 Batch  466 / 525  Training Loss  0.0022013308480381966\n",
            "Epoch  23 Batch  467 / 525  Training Loss  0.01253710687160492\n",
            "Epoch  23 Batch  468 / 525  Training Loss  0.013075773604214191\n",
            "Epoch  23 Batch  469 / 525  Training Loss  0.000845933158416301\n",
            "Epoch  23 Batch  470 / 525  Training Loss  0.0005390789592638612\n",
            "Epoch  23 Batch  471 / 525  Training Loss  0.0016405421774834394\n",
            "Epoch  23 Batch  472 / 525  Training Loss  0.002616076497361064\n",
            "Epoch  23 Batch  473 / 525  Training Loss  0.0043675340712070465\n",
            "Epoch  23 Batch  474 / 525  Training Loss  0.0027274349704384804\n",
            "Epoch  23 Batch  475 / 525  Training Loss  0.0030783130787312984\n",
            "Epoch  23 Batch  476 / 525  Training Loss  0.0008633026736788452\n",
            "Epoch  23 Batch  477 / 525  Training Loss  0.0024706735275685787\n",
            "Epoch  23 Batch  478 / 525  Training Loss  0.002475916175171733\n",
            "Epoch  23 Batch  479 / 525  Training Loss  0.0016041822964325547\n",
            "Epoch  23 Batch  480 / 525  Training Loss  0.007138913031667471\n",
            "Epoch  23 Batch  481 / 525  Training Loss  0.0025590755976736546\n",
            "Epoch  23 Batch  482 / 525  Training Loss  0.002054357435554266\n",
            "Epoch  23 Batch  483 / 525  Training Loss  0.0012140193721279502\n",
            "Epoch  23 Batch  484 / 525  Training Loss  0.0021645037923008204\n",
            "Epoch  23 Batch  485 / 525  Training Loss  0.004332251846790314\n",
            "Epoch  23 Batch  486 / 525  Training Loss  0.005384167656302452\n",
            "Epoch  23 Batch  487 / 525  Training Loss  0.005688784644007683\n",
            "Epoch  23 Batch  488 / 525  Training Loss  0.006256157997995615\n",
            "Epoch  23 Batch  489 / 525  Training Loss  0.0019274915102869272\n",
            "Epoch  23 Batch  490 / 525  Training Loss  0.001875390880741179\n",
            "Epoch  23 Batch  491 / 525  Training Loss  0.022333787754178047\n",
            "Epoch  23 Batch  492 / 525  Training Loss  0.005721269175410271\n",
            "Epoch  23 Batch  493 / 525  Training Loss  0.0038988529704511166\n",
            "Epoch  23 Batch  494 / 525  Training Loss  0.002253473736345768\n",
            "Epoch  23 Batch  495 / 525  Training Loss  0.002968735992908478\n",
            "Epoch  23 Batch  496 / 525  Training Loss  0.0035112439654767513\n",
            "Epoch  23 Batch  497 / 525  Training Loss  0.0008472969639115036\n",
            "Epoch  23 Batch  498 / 525  Training Loss  0.006089385598897934\n",
            "Epoch  23 Batch  499 / 525  Training Loss  0.004981529898941517\n",
            "Epoch  23 Batch  500 / 525  Training Loss  0.0018249184358865023\n",
            "Epoch  23 Batch  501 / 525  Training Loss  0.004384705796837807\n",
            "Epoch  23 Batch  502 / 525  Training Loss  0.0017957413801923394\n",
            "Epoch  23 Batch  503 / 525  Training Loss  0.0012887765187770128\n",
            "Epoch  23 Batch  504 / 525  Training Loss  0.002097062300890684\n",
            "Epoch  23 Batch  505 / 525  Training Loss  0.0006501699099317193\n",
            "Epoch  23 Batch  506 / 525  Training Loss  0.00264210207387805\n",
            "Epoch  23 Batch  507 / 525  Training Loss  0.0022948661353439093\n",
            "Epoch  23 Batch  508 / 525  Training Loss  0.0031643614638596773\n",
            "Epoch  23 Batch  509 / 525  Training Loss  0.0014634061371907592\n",
            "Epoch  23 Batch  510 / 525  Training Loss  0.008470943197607994\n",
            "Epoch  23 Batch  511 / 525  Training Loss  0.0031746719032526016\n",
            "Epoch  23 Batch  512 / 525  Training Loss  0.004258121829479933\n",
            "Epoch  23 Batch  513 / 525  Training Loss  0.00915365107357502\n",
            "Epoch  23 Batch  514 / 525  Training Loss  0.003816689131781459\n",
            "Epoch  23 Batch  515 / 525  Training Loss  0.0038948804140090942\n",
            "Epoch  23 Batch  516 / 525  Training Loss  0.002984522609040141\n",
            "Epoch  23 Batch  517 / 525  Training Loss  0.0017795013263821602\n",
            "Epoch  23 Batch  518 / 525  Training Loss  0.0021862336434423923\n",
            "Epoch  23 Batch  519 / 525  Training Loss  0.004090342670679092\n",
            "Epoch  23 Batch  520 / 525  Training Loss  0.0029403080698102713\n",
            "Epoch  23 Batch  521 / 525  Training Loss  0.0027863432187587023\n",
            "Epoch  23 Batch  522 / 525  Training Loss  0.0017925280844792724\n",
            "Epoch  23 Batch  523 / 525  Training Loss  0.00163438287563622\n",
            "Epoch  23 Batch  524 / 525  Training Loss  0.0011360460193827748\n",
            "  24    |    -    |   0.002494   | 59.500000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 24\n",
            "Epoch  24 Batch  0 / 525  Training Loss  0.0010981776285916567\n",
            "Epoch  24 Batch  1 / 525  Training Loss  0.0011093352222815156\n",
            "Epoch  24 Batch  2 / 525  Training Loss  0.0008788389968685806\n",
            "Epoch  24 Batch  3 / 525  Training Loss  0.0032341114711016417\n",
            "Epoch  24 Batch  4 / 525  Training Loss  0.001270549139007926\n",
            "Epoch  24 Batch  5 / 525  Training Loss  0.0004512440937105566\n",
            "Epoch  24 Batch  6 / 525  Training Loss  0.0010107173584401608\n",
            "Epoch  24 Batch  7 / 525  Training Loss  0.002236295258626342\n",
            "Epoch  24 Batch  8 / 525  Training Loss  0.0003280724340584129\n",
            "Epoch  24 Batch  9 / 525  Training Loss  0.0004777629510499537\n",
            "Epoch  24 Batch  10 / 525  Training Loss  0.0011265936773270369\n",
            "Epoch  24 Batch  11 / 525  Training Loss  0.0032234694808721542\n",
            "Epoch  24 Batch  12 / 525  Training Loss  0.0016660674009472132\n",
            "Epoch  24 Batch  13 / 525  Training Loss  0.0015757385408505797\n",
            "Epoch  24 Batch  14 / 525  Training Loss  0.0005536430981010199\n",
            "Epoch  24 Batch  15 / 525  Training Loss  0.005951652303338051\n",
            "Epoch  24 Batch  16 / 525  Training Loss  0.001383487950079143\n",
            "Epoch  24 Batch  17 / 525  Training Loss  0.0006456683622673154\n",
            "Epoch  24 Batch  18 / 525  Training Loss  0.000814288854598999\n",
            "Epoch  24 Batch  19 / 525  Training Loss  0.0007848034729249775\n",
            "Epoch  24 Batch  20 / 525  Training Loss  0.0021596290171146393\n",
            "Epoch  24 Batch  21 / 525  Training Loss  0.002769105602055788\n",
            "Epoch  24 Batch  22 / 525  Training Loss  0.0005281046614982188\n",
            "Epoch  24 Batch  23 / 525  Training Loss  0.0003901778836734593\n",
            "Epoch  24 Batch  24 / 525  Training Loss  0.0005157381528988481\n",
            "Epoch  24 Batch  25 / 525  Training Loss  0.0008792094886302948\n",
            "Epoch  24 Batch  26 / 525  Training Loss  0.0007026867242529988\n",
            "Epoch  24 Batch  27 / 525  Training Loss  0.001471688156016171\n",
            "Epoch  24 Batch  28 / 525  Training Loss  0.0006906382623128593\n",
            "Epoch  24 Batch  29 / 525  Training Loss  0.0004530705919023603\n",
            "Epoch  24 Batch  30 / 525  Training Loss  0.0014847463462501764\n",
            "Epoch  24 Batch  31 / 525  Training Loss  0.0006081672618165612\n",
            "Epoch  24 Batch  32 / 525  Training Loss  0.002769728656858206\n",
            "Epoch  24 Batch  33 / 525  Training Loss  0.0005192583194002509\n",
            "Epoch  24 Batch  34 / 525  Training Loss  0.0010359114967286587\n",
            "Epoch  24 Batch  35 / 525  Training Loss  0.0033814788330346346\n",
            "Epoch  24 Batch  36 / 525  Training Loss  0.001965610310435295\n",
            "Epoch  24 Batch  37 / 525  Training Loss  0.0013653902569785714\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  24 Batch  38 / 525  Training Loss  0.0014852246968075633\n",
            "Epoch  24 Batch  39 / 525  Training Loss  0.001125082839280367\n",
            "Epoch  24 Batch  40 / 525  Training Loss  0.0010463546495884657\n",
            "Epoch  24 Batch  41 / 525  Training Loss  0.002381254220381379\n",
            "Epoch  24 Batch  42 / 525  Training Loss  0.0023830505087971687\n",
            "Epoch  24 Batch  43 / 525  Training Loss  0.0007277883123606443\n",
            "Epoch  24 Batch  44 / 525  Training Loss  0.0007760027656331658\n",
            "Epoch  24 Batch  45 / 525  Training Loss  0.0007546056294813752\n",
            "Epoch  24 Batch  46 / 525  Training Loss  0.0017836341867223382\n",
            "Epoch  24 Batch  47 / 525  Training Loss  0.001422944595105946\n",
            "Epoch  24 Batch  48 / 525  Training Loss  0.0014418067876249552\n",
            "Epoch  24 Batch  49 / 525  Training Loss  0.0007342156604863703\n",
            "Epoch  24 Batch  50 / 525  Training Loss  0.0020838589407503605\n",
            "Epoch  24 Batch  51 / 525  Training Loss  0.0009434728999622166\n",
            "Epoch  24 Batch  52 / 525  Training Loss  0.002310174750164151\n",
            "Epoch  24 Batch  53 / 525  Training Loss  0.006347277667373419\n",
            "Epoch  24 Batch  54 / 525  Training Loss  0.00724668288603425\n",
            "Epoch  24 Batch  55 / 525  Training Loss  0.0021063454914838076\n",
            "Epoch  24 Batch  56 / 525  Training Loss  0.0007615116192027926\n",
            "Epoch  24 Batch  57 / 525  Training Loss  0.0007368670776486397\n",
            "Epoch  24 Batch  58 / 525  Training Loss  0.0008834324544295669\n",
            "Epoch  24 Batch  59 / 525  Training Loss  0.0008494788780808449\n",
            "Epoch  24 Batch  60 / 525  Training Loss  0.0005932382773607969\n",
            "Epoch  24 Batch  61 / 525  Training Loss  0.0010364321060478687\n",
            "Epoch  24 Batch  62 / 525  Training Loss  0.004471351392567158\n",
            "Epoch  24 Batch  63 / 525  Training Loss  0.006366308778524399\n",
            "Epoch  24 Batch  64 / 525  Training Loss  0.0002585586626082659\n",
            "Epoch  24 Batch  65 / 525  Training Loss  0.0005757041508331895\n",
            "Epoch  24 Batch  66 / 525  Training Loss  0.0015752840554341674\n",
            "Epoch  24 Batch  67 / 525  Training Loss  0.0024147455114871264\n",
            "Epoch  24 Batch  68 / 525  Training Loss  0.0007549693109467626\n",
            "Epoch  24 Batch  69 / 525  Training Loss  0.0017516633961349726\n",
            "Epoch  24 Batch  70 / 525  Training Loss  0.0031310797203332186\n",
            "Epoch  24 Batch  71 / 525  Training Loss  0.0032667338382452726\n",
            "Epoch  24 Batch  72 / 525  Training Loss  0.0031667700968682766\n",
            "Epoch  24 Batch  73 / 525  Training Loss  0.0022035331930965185\n",
            "Epoch  24 Batch  74 / 525  Training Loss  0.0013712019426748157\n",
            "Epoch  24 Batch  75 / 525  Training Loss  0.0010393931297585368\n",
            "Epoch  24 Batch  76 / 525  Training Loss  0.001925729215145111\n",
            "Epoch  24 Batch  77 / 525  Training Loss  0.0008946240996010602\n",
            "Epoch  24 Batch  78 / 525  Training Loss  0.0010065248934552073\n",
            "Epoch  24 Batch  79 / 525  Training Loss  0.0020890154410153627\n",
            "Epoch  24 Batch  80 / 525  Training Loss  0.001622212352231145\n",
            "Epoch  24 Batch  81 / 525  Training Loss  0.0007573566399514675\n",
            "Epoch  24 Batch  82 / 525  Training Loss  0.001141932443715632\n",
            "Epoch  24 Batch  83 / 525  Training Loss  0.0015358205419033766\n",
            "Epoch  24 Batch  84 / 525  Training Loss  0.0011504963040351868\n",
            "Epoch  24 Batch  85 / 525  Training Loss  0.00104684685356915\n",
            "Epoch  24 Batch  86 / 525  Training Loss  0.0007291465881280601\n",
            "Epoch  24 Batch  87 / 525  Training Loss  0.0021265035029500723\n",
            "Epoch  24 Batch  88 / 525  Training Loss  0.00300240539945662\n",
            "Epoch  24 Batch  89 / 525  Training Loss  0.002582880901172757\n",
            "Epoch  24 Batch  90 / 525  Training Loss  0.0006230321596376598\n",
            "Epoch  24 Batch  91 / 525  Training Loss  0.0014850269071757793\n",
            "Epoch  24 Batch  92 / 525  Training Loss  0.0008794005843810737\n",
            "Epoch  24 Batch  93 / 525  Training Loss  0.00044248386984691024\n",
            "Epoch  24 Batch  94 / 525  Training Loss  0.0005479399696923792\n",
            "Epoch  24 Batch  95 / 525  Training Loss  0.0023880961816757917\n",
            "Epoch  24 Batch  96 / 525  Training Loss  0.006314533296972513\n",
            "Epoch  24 Batch  97 / 525  Training Loss  0.001349046011455357\n",
            "Epoch  24 Batch  98 / 525  Training Loss  0.0015662172809243202\n",
            "Epoch  24 Batch  99 / 525  Training Loss  0.0014313508290797472\n",
            "Epoch  24 Batch  100 / 525  Training Loss  0.0027996371500194073\n",
            "Epoch  24 Batch  101 / 525  Training Loss  0.0010973757598549128\n",
            "Epoch  24 Batch  102 / 525  Training Loss  0.0008661869214847684\n",
            "Epoch  24 Batch  103 / 525  Training Loss  0.000866839662194252\n",
            "Epoch  24 Batch  104 / 525  Training Loss  0.0033551636151969433\n",
            "Epoch  24 Batch  105 / 525  Training Loss  0.0016106672119349241\n",
            "Epoch  24 Batch  106 / 525  Training Loss  0.0014239639276638627\n",
            "Epoch  24 Batch  107 / 525  Training Loss  0.005716380663216114\n",
            "Epoch  24 Batch  108 / 525  Training Loss  0.0011111742351204157\n",
            "Epoch  24 Batch  109 / 525  Training Loss  0.003803625702857971\n",
            "Epoch  24 Batch  110 / 525  Training Loss  0.0052129835821688175\n",
            "Epoch  24 Batch  111 / 525  Training Loss  0.0033822008408606052\n",
            "Epoch  24 Batch  112 / 525  Training Loss  0.002426878083497286\n",
            "Epoch  24 Batch  113 / 525  Training Loss  0.002287458162754774\n",
            "Epoch  24 Batch  114 / 525  Training Loss  0.0014770376728847623\n",
            "Epoch  24 Batch  115 / 525  Training Loss  0.006733620073646307\n",
            "Epoch  24 Batch  116 / 525  Training Loss  0.004611607640981674\n",
            "Epoch  24 Batch  117 / 525  Training Loss  0.0017982820281758904\n",
            "Epoch  24 Batch  118 / 525  Training Loss  0.0012373409699648619\n",
            "Epoch  24 Batch  119 / 525  Training Loss  0.005136215593665838\n",
            "Epoch  24 Batch  120 / 525  Training Loss  0.0042960443533957005\n",
            "Epoch  24 Batch  121 / 525  Training Loss  0.0009988070232793689\n",
            "Epoch  24 Batch  122 / 525  Training Loss  0.0007690653437748551\n",
            "Epoch  24 Batch  123 / 525  Training Loss  0.001248917425982654\n",
            "Epoch  24 Batch  124 / 525  Training Loss  0.001962880836799741\n",
            "Epoch  24 Batch  125 / 525  Training Loss  0.000616256264038384\n",
            "Epoch  24 Batch  126 / 525  Training Loss  0.0008335229940712452\n",
            "Epoch  24 Batch  127 / 525  Training Loss  0.0020821017678827047\n",
            "Epoch  24 Batch  128 / 525  Training Loss  0.0016281448770314455\n",
            "Epoch  24 Batch  129 / 525  Training Loss  0.0012502127792686224\n",
            "Epoch  24 Batch  130 / 525  Training Loss  0.0005345187382772565\n",
            "Epoch  24 Batch  131 / 525  Training Loss  0.0035530701279640198\n",
            "Epoch  24 Batch  132 / 525  Training Loss  0.0016959758941084146\n",
            "Epoch  24 Batch  133 / 525  Training Loss  0.0026280819438397884\n",
            "Epoch  24 Batch  134 / 525  Training Loss  0.0009943802142515779\n",
            "Epoch  24 Batch  135 / 525  Training Loss  0.000507489952724427\n",
            "Epoch  24 Batch  136 / 525  Training Loss  0.0016410918906331062\n",
            "Epoch  24 Batch  137 / 525  Training Loss  0.001602716976776719\n",
            "Epoch  24 Batch  138 / 525  Training Loss  0.0017803203081712127\n",
            "Epoch  24 Batch  139 / 525  Training Loss  0.004100410733371973\n",
            "Epoch  24 Batch  140 / 525  Training Loss  0.0008086243760772049\n",
            "Epoch  24 Batch  141 / 525  Training Loss  0.0015991248656064272\n",
            "Epoch  24 Batch  142 / 525  Training Loss  0.0016976051265373826\n",
            "Epoch  24 Batch  143 / 525  Training Loss  0.0046208444982767105\n",
            "Epoch  24 Batch  144 / 525  Training Loss  0.0030888572800904512\n",
            "Epoch  24 Batch  145 / 525  Training Loss  0.0031200035009533167\n",
            "Epoch  24 Batch  146 / 525  Training Loss  0.0015734117478132248\n",
            "Epoch  24 Batch  147 / 525  Training Loss  0.001358891255222261\n",
            "Epoch  24 Batch  148 / 525  Training Loss  0.007534373551607132\n",
            "Epoch  24 Batch  149 / 525  Training Loss  0.0009837893303483725\n",
            "Epoch  24 Batch  150 / 525  Training Loss  0.005176948849111795\n",
            "Epoch  24 Batch  151 / 525  Training Loss  0.0018031572690233588\n",
            "Epoch  24 Batch  152 / 525  Training Loss  0.0004880420456174761\n",
            "Epoch  24 Batch  153 / 525  Training Loss  0.004130006767809391\n",
            "Epoch  24 Batch  154 / 525  Training Loss  0.003021752927452326\n",
            "Epoch  24 Batch  155 / 525  Training Loss  0.0006364263826981187\n",
            "Epoch  24 Batch  156 / 525  Training Loss  0.0012671684380620718\n",
            "Epoch  24 Batch  157 / 525  Training Loss  0.0012348946183919907\n",
            "Epoch  24 Batch  158 / 525  Training Loss  0.00639695581048727\n",
            "Epoch  24 Batch  159 / 525  Training Loss  0.0015068025095388293\n",
            "Epoch  24 Batch  160 / 525  Training Loss  0.0010960812214761972\n",
            "Epoch  24 Batch  161 / 525  Training Loss  0.0006747039151377976\n",
            "Epoch  24 Batch  162 / 525  Training Loss  0.0038395137526094913\n",
            "Epoch  24 Batch  163 / 525  Training Loss  0.0006739548407495022\n",
            "Epoch  24 Batch  164 / 525  Training Loss  0.004448738880455494\n",
            "Epoch  24 Batch  165 / 525  Training Loss  0.0007763047469779849\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  24 Batch  166 / 525  Training Loss  0.0008277905872091651\n",
            "Epoch  24 Batch  167 / 525  Training Loss  0.001957531087100506\n",
            "Epoch  24 Batch  168 / 525  Training Loss  0.001144915004260838\n",
            "Epoch  24 Batch  169 / 525  Training Loss  0.0037462704349309206\n",
            "Epoch  24 Batch  170 / 525  Training Loss  0.004055369179695845\n",
            "Epoch  24 Batch  171 / 525  Training Loss  0.003871723311021924\n",
            "Epoch  24 Batch  172 / 525  Training Loss  0.0004706272447947413\n",
            "Epoch  24 Batch  173 / 525  Training Loss  0.0032910876907408237\n",
            "Epoch  24 Batch  174 / 525  Training Loss  0.0008868227596394718\n",
            "Epoch  24 Batch  175 / 525  Training Loss  0.000763987482059747\n",
            "Epoch  24 Batch  176 / 525  Training Loss  0.0007631281623616815\n",
            "Epoch  24 Batch  177 / 525  Training Loss  0.00040401550359092653\n",
            "Epoch  24 Batch  178 / 525  Training Loss  0.001001175376586616\n",
            "Epoch  24 Batch  179 / 525  Training Loss  0.009609982371330261\n",
            "Epoch  24 Batch  180 / 525  Training Loss  0.0010605227435007691\n",
            "Epoch  24 Batch  181 / 525  Training Loss  0.0016135942423716187\n",
            "Epoch  24 Batch  182 / 525  Training Loss  0.0009103979682549834\n",
            "Epoch  24 Batch  183 / 525  Training Loss  0.0020408332347869873\n",
            "Epoch  24 Batch  184 / 525  Training Loss  0.0009562891791574657\n",
            "Epoch  24 Batch  185 / 525  Training Loss  0.004803516902029514\n",
            "Epoch  24 Batch  186 / 525  Training Loss  0.003037423361092806\n",
            "Epoch  24 Batch  187 / 525  Training Loss  0.000901295687071979\n",
            "Epoch  24 Batch  188 / 525  Training Loss  0.0004450229462236166\n",
            "Epoch  24 Batch  189 / 525  Training Loss  0.0006357348756864667\n",
            "Epoch  24 Batch  190 / 525  Training Loss  0.0021992891561239958\n",
            "Epoch  24 Batch  191 / 525  Training Loss  0.0007618928211741149\n",
            "Epoch  24 Batch  192 / 525  Training Loss  0.001024264027364552\n",
            "Epoch  24 Batch  193 / 525  Training Loss  0.004404169507324696\n",
            "Epoch  24 Batch  194 / 525  Training Loss  0.004291108343750238\n",
            "Epoch  24 Batch  195 / 525  Training Loss  0.0003978561144322157\n",
            "Epoch  24 Batch  196 / 525  Training Loss  0.0005811736336909235\n",
            "Epoch  24 Batch  197 / 525  Training Loss  0.0015534131089225411\n",
            "Epoch  24 Batch  198 / 525  Training Loss  0.0015715730842202902\n",
            "Epoch  24 Batch  199 / 525  Training Loss  0.005499628372490406\n",
            "Epoch  24 Batch  200 / 525  Training Loss  0.00095738610252738\n",
            "Epoch  24 Batch  201 / 525  Training Loss  0.0023049963638186455\n",
            "Epoch  24 Batch  202 / 525  Training Loss  0.00020877047791145742\n",
            "Epoch  24 Batch  203 / 525  Training Loss  0.0016485706437379122\n",
            "Epoch  24 Batch  204 / 525  Training Loss  0.003566485596820712\n",
            "Epoch  24 Batch  205 / 525  Training Loss  0.001712780212983489\n",
            "Epoch  24 Batch  206 / 525  Training Loss  0.0014306764351204038\n",
            "Epoch  24 Batch  207 / 525  Training Loss  0.001816021278500557\n",
            "Epoch  24 Batch  208 / 525  Training Loss  0.0012785608414560556\n",
            "Epoch  24 Batch  209 / 525  Training Loss  0.0008192815002985299\n",
            "Epoch  24 Batch  210 / 525  Training Loss  0.0011804297100752592\n",
            "Epoch  24 Batch  211 / 525  Training Loss  0.0074879564344882965\n",
            "Epoch  24 Batch  212 / 525  Training Loss  0.006181344389915466\n",
            "Epoch  24 Batch  213 / 525  Training Loss  0.0010202855337411165\n",
            "Epoch  24 Batch  214 / 525  Training Loss  0.0010912452125921845\n",
            "Epoch  24 Batch  215 / 525  Training Loss  0.000893229735083878\n",
            "Epoch  24 Batch  216 / 525  Training Loss  0.00037621124647557735\n",
            "Epoch  24 Batch  217 / 525  Training Loss  0.0010300533613190055\n",
            "Epoch  24 Batch  218 / 525  Training Loss  0.0008801538497209549\n",
            "Epoch  24 Batch  219 / 525  Training Loss  0.0006773631321266294\n",
            "Epoch  24 Batch  220 / 525  Training Loss  0.0013614202616736293\n",
            "Epoch  24 Batch  221 / 525  Training Loss  0.0009332840563729405\n",
            "Epoch  24 Batch  222 / 525  Training Loss  0.0021221202332526445\n",
            "Epoch  24 Batch  223 / 525  Training Loss  0.0022163032554090023\n",
            "Epoch  24 Batch  224 / 525  Training Loss  0.0003151947457809001\n",
            "Epoch  24 Batch  225 / 525  Training Loss  0.0013122366508468986\n",
            "Epoch  24 Batch  226 / 525  Training Loss  0.001474401680752635\n",
            "Epoch  24 Batch  227 / 525  Training Loss  0.0006763501442037523\n",
            "Epoch  24 Batch  228 / 525  Training Loss  0.0002442260447423905\n",
            "Epoch  24 Batch  229 / 525  Training Loss  0.0006428960477933288\n",
            "Epoch  24 Batch  230 / 525  Training Loss  0.0013656149385496974\n",
            "Epoch  24 Batch  231 / 525  Training Loss  0.0009380046976730227\n",
            "Epoch  24 Batch  232 / 525  Training Loss  0.0007207493763417006\n",
            "Epoch  24 Batch  233 / 525  Training Loss  0.002613555174320936\n",
            "Epoch  24 Batch  234 / 525  Training Loss  0.001870231470093131\n",
            "Epoch  24 Batch  235 / 525  Training Loss  0.003066565375775099\n",
            "Epoch  24 Batch  236 / 525  Training Loss  0.0008905943250283599\n",
            "Epoch  24 Batch  237 / 525  Training Loss  0.0009113902924582362\n",
            "Epoch  24 Batch  238 / 525  Training Loss  0.0010516499169170856\n",
            "Epoch  24 Batch  239 / 525  Training Loss  0.0020782998763024807\n",
            "Epoch  24 Batch  240 / 525  Training Loss  0.0031229027081280947\n",
            "Epoch  24 Batch  241 / 525  Training Loss  0.002204290358349681\n",
            "Epoch  24 Batch  242 / 525  Training Loss  0.0006803862052038312\n",
            "Epoch  24 Batch  243 / 525  Training Loss  0.000558586441911757\n",
            "Epoch  24 Batch  244 / 525  Training Loss  0.0014425793197005987\n",
            "Epoch  24 Batch  245 / 525  Training Loss  0.0014144208980724216\n",
            "Epoch  24 Batch  246 / 525  Training Loss  0.0013779924483969808\n",
            "Epoch  24 Batch  247 / 525  Training Loss  0.0028311649803072214\n",
            "Epoch  24 Batch  248 / 525  Training Loss  0.0010028309188783169\n",
            "Epoch  24 Batch  249 / 525  Training Loss  0.0006318580126389861\n",
            "Epoch  24 Batch  250 / 525  Training Loss  0.001850048080086708\n",
            "Epoch  24 Batch  251 / 525  Training Loss  0.0006285525159910321\n",
            "Epoch  24 Batch  252 / 525  Training Loss  0.0022306276950985193\n",
            "Epoch  24 Batch  253 / 525  Training Loss  0.005496121011674404\n",
            "Epoch  24 Batch  254 / 525  Training Loss  0.0007203997811302543\n",
            "Epoch  24 Batch  255 / 525  Training Loss  0.0009476969134993851\n",
            "Epoch  24 Batch  256 / 525  Training Loss  0.008390718139708042\n",
            "Epoch  24 Batch  257 / 525  Training Loss  0.002636568620800972\n",
            "Epoch  24 Batch  258 / 525  Training Loss  0.001706142327748239\n",
            "Epoch  24 Batch  259 / 525  Training Loss  0.0021488280035555363\n",
            "Epoch  24 Batch  260 / 525  Training Loss  0.0020755035802721977\n",
            "Epoch  24 Batch  261 / 525  Training Loss  0.0018439621198922396\n",
            "Epoch  24 Batch  262 / 525  Training Loss  0.0005318950861692429\n",
            "Epoch  24 Batch  263 / 525  Training Loss  0.002694071037694812\n",
            "Epoch  24 Batch  264 / 525  Training Loss  0.0017538474639877677\n",
            "Epoch  24 Batch  265 / 525  Training Loss  0.0029372340068221092\n",
            "Epoch  24 Batch  266 / 525  Training Loss  0.004268288612365723\n",
            "Epoch  24 Batch  267 / 525  Training Loss  0.0019586149137467146\n",
            "Epoch  24 Batch  268 / 525  Training Loss  0.0011715047294273973\n",
            "Epoch  24 Batch  269 / 525  Training Loss  0.0005111483624204993\n",
            "Epoch  24 Batch  270 / 525  Training Loss  0.00396329490467906\n",
            "Epoch  24 Batch  271 / 525  Training Loss  0.0008047810988500714\n",
            "Epoch  24 Batch  272 / 525  Training Loss  0.0013566563138738275\n",
            "Epoch  24 Batch  273 / 525  Training Loss  0.0007606265135109425\n",
            "Epoch  24 Batch  274 / 525  Training Loss  0.0009081523166969419\n",
            "Epoch  24 Batch  275 / 525  Training Loss  0.00047423294745385647\n",
            "Epoch  24 Batch  276 / 525  Training Loss  0.0003896791604347527\n",
            "Epoch  24 Batch  277 / 525  Training Loss  0.0008326743845827878\n",
            "Epoch  24 Batch  278 / 525  Training Loss  0.00042362845852039754\n",
            "Epoch  24 Batch  279 / 525  Training Loss  0.0007190731121227145\n",
            "Epoch  24 Batch  280 / 525  Training Loss  0.002102764556184411\n",
            "Epoch  24 Batch  281 / 525  Training Loss  0.0009646799298934639\n",
            "Epoch  24 Batch  282 / 525  Training Loss  0.001161816529929638\n",
            "Epoch  24 Batch  283 / 525  Training Loss  0.002380835358053446\n",
            "Epoch  24 Batch  284 / 525  Training Loss  0.002332420088350773\n",
            "Epoch  24 Batch  285 / 525  Training Loss  0.0010780042503029108\n",
            "Epoch  24 Batch  286 / 525  Training Loss  0.0008125027525238693\n",
            "Epoch  24 Batch  287 / 525  Training Loss  0.0010049868142232299\n",
            "Epoch  24 Batch  288 / 525  Training Loss  0.00069277553120628\n",
            "Epoch  24 Batch  289 / 525  Training Loss  0.0009530565585009754\n",
            "Epoch  24 Batch  290 / 525  Training Loss  0.001189100556075573\n",
            "Epoch  24 Batch  291 / 525  Training Loss  0.0009846563916653395\n",
            "Epoch  24 Batch  292 / 525  Training Loss  0.005694488529115915\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  24 Batch  293 / 525  Training Loss  0.0017975231166929007\n",
            "Epoch  24 Batch  294 / 525  Training Loss  0.004900711588561535\n",
            "Epoch  24 Batch  295 / 525  Training Loss  0.0004919047350995243\n",
            "Epoch  24 Batch  296 / 525  Training Loss  0.00045508123002946377\n",
            "Epoch  24 Batch  297 / 525  Training Loss  0.0011057050433009863\n",
            "Epoch  24 Batch  298 / 525  Training Loss  0.0016872311243787408\n",
            "Epoch  24 Batch  299 / 525  Training Loss  0.0012989265378564596\n",
            "Epoch  24 Batch  300 / 525  Training Loss  0.0006374991498887539\n",
            "Epoch  24 Batch  301 / 525  Training Loss  0.0012203531805425882\n",
            "Epoch  24 Batch  302 / 525  Training Loss  0.003677309723570943\n",
            "Epoch  24 Batch  303 / 525  Training Loss  0.0009383188444189727\n",
            "Epoch  24 Batch  304 / 525  Training Loss  0.001388193340972066\n",
            "Epoch  24 Batch  305 / 525  Training Loss  0.00196400610730052\n",
            "Epoch  24 Batch  306 / 525  Training Loss  0.0011279210448265076\n",
            "Epoch  24 Batch  307 / 525  Training Loss  0.0008405117550864816\n",
            "Epoch  24 Batch  308 / 525  Training Loss  0.000624673324637115\n",
            "Epoch  24 Batch  309 / 525  Training Loss  0.002431895351037383\n",
            "Epoch  24 Batch  310 / 525  Training Loss  0.00039729574928060174\n",
            "Epoch  24 Batch  311 / 525  Training Loss  0.000617065466940403\n",
            "Epoch  24 Batch  312 / 525  Training Loss  0.0005069452454335988\n",
            "Epoch  24 Batch  313 / 525  Training Loss  0.0012469654902815819\n",
            "Epoch  24 Batch  314 / 525  Training Loss  0.0006300091044977307\n",
            "Epoch  24 Batch  315 / 525  Training Loss  0.00040392548544332385\n",
            "Epoch  24 Batch  316 / 525  Training Loss  0.000884400331415236\n",
            "Epoch  24 Batch  317 / 525  Training Loss  0.0005542672006413341\n",
            "Epoch  24 Batch  318 / 525  Training Loss  0.0003640694194473326\n",
            "Epoch  24 Batch  319 / 525  Training Loss  0.0014070302713662386\n",
            "Epoch  24 Batch  320 / 525  Training Loss  0.0010875194566324353\n",
            "Epoch  24 Batch  321 / 525  Training Loss  0.0006761816912330687\n",
            "Epoch  24 Batch  322 / 525  Training Loss  0.00042325863614678383\n",
            "Epoch  24 Batch  323 / 525  Training Loss  0.003559837117791176\n",
            "Epoch  24 Batch  324 / 525  Training Loss  0.0015210453420877457\n",
            "Epoch  24 Batch  325 / 525  Training Loss  0.0038376585580408573\n",
            "Epoch  24 Batch  326 / 525  Training Loss  0.00035192881477996707\n",
            "Epoch  24 Batch  327 / 525  Training Loss  0.0021488864440470934\n",
            "Epoch  24 Batch  328 / 525  Training Loss  0.0009861805010586977\n",
            "Epoch  24 Batch  329 / 525  Training Loss  0.002052807714790106\n",
            "Epoch  24 Batch  330 / 525  Training Loss  0.000136379967443645\n",
            "Epoch  24 Batch  331 / 525  Training Loss  0.0030717412009835243\n",
            "Epoch  24 Batch  332 / 525  Training Loss  0.0018180415499955416\n",
            "Epoch  24 Batch  333 / 525  Training Loss  0.005278430413454771\n",
            "Epoch  24 Batch  334 / 525  Training Loss  0.00044797343434765935\n",
            "Epoch  24 Batch  335 / 525  Training Loss  0.010057247243821621\n",
            "Epoch  24 Batch  336 / 525  Training Loss  0.002455683657899499\n",
            "Epoch  24 Batch  337 / 525  Training Loss  0.0011024483246728778\n",
            "Epoch  24 Batch  338 / 525  Training Loss  0.002842442598193884\n",
            "Epoch  24 Batch  339 / 525  Training Loss  0.0012050532968714833\n",
            "Epoch  24 Batch  340 / 525  Training Loss  0.0014205295592546463\n",
            "Epoch  24 Batch  341 / 525  Training Loss  0.0011201432207599282\n",
            "Epoch  24 Batch  342 / 525  Training Loss  0.003690691664814949\n",
            "Epoch  24 Batch  343 / 525  Training Loss  0.0017584199085831642\n",
            "Epoch  24 Batch  344 / 525  Training Loss  0.0020745876245200634\n",
            "Epoch  24 Batch  345 / 525  Training Loss  0.000206590979360044\n",
            "Epoch  24 Batch  346 / 525  Training Loss  0.0017633771058171988\n",
            "Epoch  24 Batch  347 / 525  Training Loss  0.0044538830406963825\n",
            "Epoch  24 Batch  348 / 525  Training Loss  0.00035108564770780504\n",
            "Epoch  24 Batch  349 / 525  Training Loss  0.0009392499923706055\n",
            "Epoch  24 Batch  350 / 525  Training Loss  0.0016780284931883216\n",
            "Epoch  24 Batch  351 / 525  Training Loss  0.0035570901818573475\n",
            "Epoch  24 Batch  352 / 525  Training Loss  0.004028710536658764\n",
            "Epoch  24 Batch  353 / 525  Training Loss  0.001553582726046443\n",
            "Epoch  24 Batch  354 / 525  Training Loss  0.0029573712963610888\n",
            "Epoch  24 Batch  355 / 525  Training Loss  0.0009627504041418433\n",
            "Epoch  24 Batch  356 / 525  Training Loss  0.0017270877724513412\n",
            "Epoch  24 Batch  357 / 525  Training Loss  0.0011736738961189985\n",
            "Epoch  24 Batch  358 / 525  Training Loss  0.0017550801858305931\n",
            "Epoch  24 Batch  359 / 525  Training Loss  0.006121322512626648\n",
            "Epoch  24 Batch  360 / 525  Training Loss  0.0007947823032736778\n",
            "Epoch  24 Batch  361 / 525  Training Loss  0.0085515845566988\n",
            "Epoch  24 Batch  362 / 525  Training Loss  0.0002905722358264029\n",
            "Epoch  24 Batch  363 / 525  Training Loss  0.010146347805857658\n",
            "Epoch  24 Batch  364 / 525  Training Loss  0.005318124778568745\n",
            "Epoch  24 Batch  365 / 525  Training Loss  0.001033246866427362\n",
            "Epoch  24 Batch  366 / 525  Training Loss  0.0006916504353284836\n",
            "Epoch  24 Batch  367 / 525  Training Loss  0.0004988721339032054\n",
            "Epoch  24 Batch  368 / 525  Training Loss  0.0007323600584641099\n",
            "Epoch  24 Batch  369 / 525  Training Loss  0.0009645586833357811\n",
            "Epoch  24 Batch  370 / 525  Training Loss  0.0020933786872774363\n",
            "Epoch  24 Batch  371 / 525  Training Loss  0.001187458517961204\n",
            "Epoch  24 Batch  372 / 525  Training Loss  0.0008120111306197941\n",
            "Epoch  24 Batch  373 / 525  Training Loss  0.004304128233343363\n",
            "Epoch  24 Batch  374 / 525  Training Loss  0.0010707867331802845\n",
            "Epoch  24 Batch  375 / 525  Training Loss  0.0005271799163892865\n",
            "Epoch  24 Batch  376 / 525  Training Loss  0.0007200060645118356\n",
            "Epoch  24 Batch  377 / 525  Training Loss  0.0007435031002387404\n",
            "Epoch  24 Batch  378 / 525  Training Loss  0.008525983430445194\n",
            "Epoch  24 Batch  379 / 525  Training Loss  0.0002435327769489959\n",
            "Epoch  24 Batch  380 / 525  Training Loss  0.0011827682610601187\n",
            "Epoch  24 Batch  381 / 525  Training Loss  0.006981611251831055\n",
            "Epoch  24 Batch  382 / 525  Training Loss  0.011007674969732761\n",
            "Epoch  24 Batch  383 / 525  Training Loss  0.0012841497082263231\n",
            "Epoch  24 Batch  384 / 525  Training Loss  0.0017582286382094026\n",
            "Epoch  24 Batch  385 / 525  Training Loss  0.0021092654205858707\n",
            "Epoch  24 Batch  386 / 525  Training Loss  0.0017235595732927322\n",
            "Epoch  24 Batch  387 / 525  Training Loss  0.0005740639753639698\n",
            "Epoch  24 Batch  388 / 525  Training Loss  0.002159573370590806\n",
            "Epoch  24 Batch  389 / 525  Training Loss  0.005538759753108025\n",
            "Epoch  24 Batch  390 / 525  Training Loss  0.006441631354391575\n",
            "Epoch  24 Batch  391 / 525  Training Loss  0.007289380766451359\n",
            "Epoch  24 Batch  392 / 525  Training Loss  0.0016605776036158204\n",
            "Epoch  24 Batch  393 / 525  Training Loss  0.005738104693591595\n",
            "Epoch  24 Batch  394 / 525  Training Loss  0.004728883504867554\n",
            "Epoch  24 Batch  395 / 525  Training Loss  0.0017130186315625906\n",
            "Epoch  24 Batch  396 / 525  Training Loss  0.0014689891831949353\n",
            "Epoch  24 Batch  397 / 525  Training Loss  0.010957070626318455\n",
            "Epoch  24 Batch  398 / 525  Training Loss  0.0016735680401325226\n",
            "Epoch  24 Batch  399 / 525  Training Loss  0.0012114696437492967\n",
            "Epoch  24 Batch  400 / 525  Training Loss  0.0004956847405992448\n",
            "Epoch  24 Batch  401 / 525  Training Loss  0.0009163888171315193\n",
            "Epoch  24 Batch  402 / 525  Training Loss  0.0003870752116199583\n",
            "Epoch  24 Batch  403 / 525  Training Loss  0.002475969959050417\n",
            "Epoch  24 Batch  404 / 525  Training Loss  0.0016761779552325606\n",
            "Epoch  24 Batch  405 / 525  Training Loss  0.0020470689050853252\n",
            "Epoch  24 Batch  406 / 525  Training Loss  0.0016435828292742372\n",
            "Epoch  24 Batch  407 / 525  Training Loss  0.00032805054797790945\n",
            "Epoch  24 Batch  408 / 525  Training Loss  0.0015994778368622065\n",
            "Epoch  24 Batch  409 / 525  Training Loss  0.00389971025288105\n",
            "Epoch  24 Batch  410 / 525  Training Loss  0.0018614366417750716\n",
            "Epoch  24 Batch  411 / 525  Training Loss  0.0007480594795197248\n",
            "Epoch  24 Batch  412 / 525  Training Loss  0.0008286537486128509\n",
            "Epoch  24 Batch  413 / 525  Training Loss  0.0008755678427405655\n",
            "Epoch  24 Batch  414 / 525  Training Loss  0.0031800686847418547\n",
            "Epoch  24 Batch  415 / 525  Training Loss  0.001271090586669743\n",
            "Epoch  24 Batch  416 / 525  Training Loss  0.002119147451594472\n",
            "Epoch  24 Batch  417 / 525  Training Loss  0.0007124177063815296\n",
            "Epoch  24 Batch  418 / 525  Training Loss  0.0015304053667932749\n",
            "Epoch  24 Batch  419 / 525  Training Loss  0.0015801182016730309\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  24 Batch  420 / 525  Training Loss  0.000986768282018602\n",
            "Epoch  24 Batch  421 / 525  Training Loss  0.0009848561603575945\n",
            "Epoch  24 Batch  422 / 525  Training Loss  0.000776592583861202\n",
            "Epoch  24 Batch  423 / 525  Training Loss  0.000589915900491178\n",
            "Epoch  24 Batch  424 / 525  Training Loss  0.00056126230629161\n",
            "Epoch  24 Batch  425 / 525  Training Loss  0.0008985103340819478\n",
            "Epoch  24 Batch  426 / 525  Training Loss  0.0008721969788894057\n",
            "Epoch  24 Batch  427 / 525  Training Loss  0.0007163850823417306\n",
            "Epoch  24 Batch  428 / 525  Training Loss  0.0008583468152210116\n",
            "Epoch  24 Batch  429 / 525  Training Loss  0.00040612323209643364\n",
            "Epoch  24 Batch  430 / 525  Training Loss  0.000470911298179999\n",
            "Epoch  24 Batch  431 / 525  Training Loss  0.001514015719294548\n",
            "Epoch  24 Batch  432 / 525  Training Loss  0.0005226616049185395\n",
            "Epoch  24 Batch  433 / 525  Training Loss  0.0012265017721801996\n",
            "Epoch  24 Batch  434 / 525  Training Loss  0.000929384957998991\n",
            "Epoch  24 Batch  435 / 525  Training Loss  0.004833155311644077\n",
            "Epoch  24 Batch  436 / 525  Training Loss  0.0007082712836563587\n",
            "Epoch  24 Batch  437 / 525  Training Loss  0.003283347934484482\n",
            "Epoch  24 Batch  438 / 525  Training Loss  0.002300822641700506\n",
            "Epoch  24 Batch  439 / 525  Training Loss  0.0014940343098714948\n",
            "Epoch  24 Batch  440 / 525  Training Loss  0.0020896506030112505\n",
            "Epoch  24 Batch  441 / 525  Training Loss  0.0022687469609081745\n",
            "Epoch  24 Batch  442 / 525  Training Loss  0.000899149221368134\n",
            "Epoch  24 Batch  443 / 525  Training Loss  0.003948420286178589\n",
            "Epoch  24 Batch  444 / 525  Training Loss  0.003281098324805498\n",
            "Epoch  24 Batch  445 / 525  Training Loss  0.0008995686657726765\n",
            "Epoch  24 Batch  446 / 525  Training Loss  0.001306057907640934\n",
            "Epoch  24 Batch  447 / 525  Training Loss  0.0004868923278991133\n",
            "Epoch  24 Batch  448 / 525  Training Loss  0.0012736767530441284\n",
            "Epoch  24 Batch  449 / 525  Training Loss  0.001082073082216084\n",
            "Epoch  24 Batch  450 / 525  Training Loss  0.0049226051196455956\n",
            "Epoch  24 Batch  451 / 525  Training Loss  0.0030466890893876553\n",
            "Epoch  24 Batch  452 / 525  Training Loss  0.0025480291806161404\n",
            "Epoch  24 Batch  453 / 525  Training Loss  0.0011638278374448419\n",
            "Epoch  24 Batch  454 / 525  Training Loss  0.0033461027778685093\n",
            "Epoch  24 Batch  455 / 525  Training Loss  0.0013987834099680185\n",
            "Epoch  24 Batch  456 / 525  Training Loss  0.0011369296116754413\n",
            "Epoch  24 Batch  457 / 525  Training Loss  0.00143039645627141\n",
            "Epoch  24 Batch  458 / 525  Training Loss  0.0007524822140112519\n",
            "Epoch  24 Batch  459 / 525  Training Loss  0.0017878062790259719\n",
            "Epoch  24 Batch  460 / 525  Training Loss  0.00299949967302382\n",
            "Epoch  24 Batch  461 / 525  Training Loss  0.00318100000731647\n",
            "Epoch  24 Batch  462 / 525  Training Loss  0.001328582875430584\n",
            "Epoch  24 Batch  463 / 525  Training Loss  0.004959584679454565\n",
            "Epoch  24 Batch  464 / 525  Training Loss  0.0007530277362093329\n",
            "Epoch  24 Batch  465 / 525  Training Loss  0.0014896776992827654\n",
            "Epoch  24 Batch  466 / 525  Training Loss  0.0007304505561478436\n",
            "Epoch  24 Batch  467 / 525  Training Loss  0.0005048435414209962\n",
            "Epoch  24 Batch  468 / 525  Training Loss  0.006430371198803186\n",
            "Epoch  24 Batch  469 / 525  Training Loss  0.001855713315308094\n",
            "Epoch  24 Batch  470 / 525  Training Loss  0.003957211505621672\n",
            "Epoch  24 Batch  471 / 525  Training Loss  0.0010536175686866045\n",
            "Epoch  24 Batch  472 / 525  Training Loss  0.0017703825142234564\n",
            "Epoch  24 Batch  473 / 525  Training Loss  0.0006313526537269354\n",
            "Epoch  24 Batch  474 / 525  Training Loss  0.0008295040461234748\n",
            "Epoch  24 Batch  475 / 525  Training Loss  0.0009657868067733943\n",
            "Epoch  24 Batch  476 / 525  Training Loss  0.0014452349860221148\n",
            "Epoch  24 Batch  477 / 525  Training Loss  0.0017640364822000265\n",
            "Epoch  24 Batch  478 / 525  Training Loss  0.0008134107338264585\n",
            "Epoch  24 Batch  479 / 525  Training Loss  0.000665745057631284\n",
            "Epoch  24 Batch  480 / 525  Training Loss  0.0023246812634170055\n",
            "Epoch  24 Batch  481 / 525  Training Loss  0.0005035708891227841\n",
            "Epoch  24 Batch  482 / 525  Training Loss  0.0010942619992420077\n",
            "Epoch  24 Batch  483 / 525  Training Loss  0.0007471194840036333\n",
            "Epoch  24 Batch  484 / 525  Training Loss  0.000860952481161803\n",
            "Epoch  24 Batch  485 / 525  Training Loss  0.0014040719252079725\n",
            "Epoch  24 Batch  486 / 525  Training Loss  0.0013094197493046522\n",
            "Epoch  24 Batch  487 / 525  Training Loss  0.0004771490639541298\n",
            "Epoch  24 Batch  488 / 525  Training Loss  0.0006551686674356461\n",
            "Epoch  24 Batch  489 / 525  Training Loss  0.0006414869567379355\n",
            "Epoch  24 Batch  490 / 525  Training Loss  0.0006831875070929527\n",
            "Epoch  24 Batch  491 / 525  Training Loss  0.0005886757862754166\n",
            "Epoch  24 Batch  492 / 525  Training Loss  0.0003954456769861281\n",
            "Epoch  24 Batch  493 / 525  Training Loss  0.0005062927375547588\n",
            "Epoch  24 Batch  494 / 525  Training Loss  0.002922436222434044\n",
            "Epoch  24 Batch  495 / 525  Training Loss  0.0037749577313661575\n",
            "Epoch  24 Batch  496 / 525  Training Loss  0.0006692291353829205\n",
            "Epoch  24 Batch  497 / 525  Training Loss  0.0002620972809381783\n",
            "Epoch  24 Batch  498 / 525  Training Loss  0.0011988789774477482\n",
            "Epoch  24 Batch  499 / 525  Training Loss  0.0004215067601762712\n",
            "Epoch  24 Batch  500 / 525  Training Loss  0.0003885774058289826\n",
            "Epoch  24 Batch  501 / 525  Training Loss  0.0004786618519574404\n",
            "Epoch  24 Batch  502 / 525  Training Loss  0.001978552434593439\n",
            "Epoch  24 Batch  503 / 525  Training Loss  0.005789958871901035\n",
            "Epoch  24 Batch  504 / 525  Training Loss  0.0007784167537465692\n",
            "Epoch  24 Batch  505 / 525  Training Loss  0.0005162638844922185\n",
            "Epoch  24 Batch  506 / 525  Training Loss  0.0020448658615350723\n",
            "Epoch  24 Batch  507 / 525  Training Loss  0.0029971101321280003\n",
            "Epoch  24 Batch  508 / 525  Training Loss  0.003413944272324443\n",
            "Epoch  24 Batch  509 / 525  Training Loss  0.0017796391621232033\n",
            "Epoch  24 Batch  510 / 525  Training Loss  0.0009737358195707202\n",
            "Epoch  24 Batch  511 / 525  Training Loss  0.0028393184766173363\n",
            "Epoch  24 Batch  512 / 525  Training Loss  0.0015796618536114693\n",
            "Epoch  24 Batch  513 / 525  Training Loss  0.0011601357255131006\n",
            "Epoch  24 Batch  514 / 525  Training Loss  0.0036493148654699326\n",
            "Epoch  24 Batch  515 / 525  Training Loss  0.005501534324139357\n",
            "Epoch  24 Batch  516 / 525  Training Loss  0.0020368932746350765\n",
            "Epoch  24 Batch  517 / 525  Training Loss  0.0028024024795740843\n",
            "Epoch  24 Batch  518 / 525  Training Loss  0.0002758482296485454\n",
            "Epoch  24 Batch  519 / 525  Training Loss  0.0016010325634852052\n",
            "Epoch  24 Batch  520 / 525  Training Loss  0.0011417472269386053\n",
            "Epoch  24 Batch  521 / 525  Training Loss  0.0006848570192232728\n",
            "Epoch  24 Batch  522 / 525  Training Loss  0.0033213465940207243\n",
            "Epoch  24 Batch  523 / 525  Training Loss  0.0007677407702431083\n",
            "Epoch  24 Batch  524 / 525  Training Loss  0.0007128507131710649\n",
            "  25    |    -    |   0.001908   | 61.033333\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 25\n",
            "Epoch  25 Batch  0 / 525  Training Loss  0.0005298913456499577\n",
            "Epoch  25 Batch  1 / 525  Training Loss  0.0014861230738461018\n",
            "Epoch  25 Batch  2 / 525  Training Loss  0.0070045022293925285\n",
            "Epoch  25 Batch  3 / 525  Training Loss  0.0003319332899991423\n",
            "Epoch  25 Batch  4 / 525  Training Loss  0.00044227560283616185\n",
            "Epoch  25 Batch  5 / 525  Training Loss  0.00041719814180396497\n",
            "Epoch  25 Batch  6 / 525  Training Loss  0.001392162637785077\n",
            "Epoch  25 Batch  7 / 525  Training Loss  0.0003901605086866766\n",
            "Epoch  25 Batch  8 / 525  Training Loss  0.001033350476063788\n",
            "Epoch  25 Batch  9 / 525  Training Loss  0.000609786540735513\n",
            "Epoch  25 Batch  10 / 525  Training Loss  0.00017527511226944625\n",
            "Epoch  25 Batch  11 / 525  Training Loss  0.0006027456256560981\n",
            "Epoch  25 Batch  12 / 525  Training Loss  0.0009538301965221763\n",
            "Epoch  25 Batch  13 / 525  Training Loss  0.000804601819254458\n",
            "Epoch  25 Batch  14 / 525  Training Loss  0.001488042646087706\n",
            "Epoch  25 Batch  15 / 525  Training Loss  0.0009629345731809735\n",
            "Epoch  25 Batch  16 / 525  Training Loss  0.000593259755987674\n",
            "Epoch  25 Batch  17 / 525  Training Loss  0.00027840412803925574\n",
            "Epoch  25 Batch  18 / 525  Training Loss  0.0006855543470010161\n",
            "Epoch  25 Batch  19 / 525  Training Loss  0.001644951873458922\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  25 Batch  20 / 525  Training Loss  0.0006485084304586053\n",
            "Epoch  25 Batch  21 / 525  Training Loss  0.0056131621822714806\n",
            "Epoch  25 Batch  22 / 525  Training Loss  0.0006391752394847572\n",
            "Epoch  25 Batch  23 / 525  Training Loss  0.0007749319192953408\n",
            "Epoch  25 Batch  24 / 525  Training Loss  0.0011944452999159694\n",
            "Epoch  25 Batch  25 / 525  Training Loss  0.0003122608468402177\n",
            "Epoch  25 Batch  26 / 525  Training Loss  0.0005512942443601787\n",
            "Epoch  25 Batch  27 / 525  Training Loss  0.001452212338335812\n",
            "Epoch  25 Batch  28 / 525  Training Loss  0.00032258330611512065\n",
            "Epoch  25 Batch  29 / 525  Training Loss  0.004653374198824167\n",
            "Epoch  25 Batch  30 / 525  Training Loss  0.0005950461491011083\n",
            "Epoch  25 Batch  31 / 525  Training Loss  0.0005117828259244561\n",
            "Epoch  25 Batch  32 / 525  Training Loss  0.0011525331065058708\n",
            "Epoch  25 Batch  33 / 525  Training Loss  0.000521950307302177\n",
            "Epoch  25 Batch  34 / 525  Training Loss  0.0016137200873345137\n",
            "Epoch  25 Batch  35 / 525  Training Loss  0.0003407215408515185\n",
            "Epoch  25 Batch  36 / 525  Training Loss  0.0006706342101097107\n",
            "Epoch  25 Batch  37 / 525  Training Loss  0.00027680970379151404\n",
            "Epoch  25 Batch  38 / 525  Training Loss  0.0006118974415585399\n",
            "Epoch  25 Batch  39 / 525  Training Loss  0.0021358157973736525\n",
            "Epoch  25 Batch  40 / 525  Training Loss  0.00041941492236219347\n",
            "Epoch  25 Batch  41 / 525  Training Loss  0.0008829950238578022\n",
            "Epoch  25 Batch  42 / 525  Training Loss  0.0012186983367428184\n",
            "Epoch  25 Batch  43 / 525  Training Loss  0.0004847309610340744\n",
            "Epoch  25 Batch  44 / 525  Training Loss  0.00023527329904027283\n",
            "Epoch  25 Batch  45 / 525  Training Loss  0.0009535751305520535\n",
            "Epoch  25 Batch  46 / 525  Training Loss  0.0010085063986480236\n",
            "Epoch  25 Batch  47 / 525  Training Loss  0.0005371397128328681\n",
            "Epoch  25 Batch  48 / 525  Training Loss  0.000882318418007344\n",
            "Epoch  25 Batch  49 / 525  Training Loss  0.0008812194573692977\n",
            "Epoch  25 Batch  50 / 525  Training Loss  0.0006523210904560983\n",
            "Epoch  25 Batch  51 / 525  Training Loss  0.0005423386464826763\n",
            "Epoch  25 Batch  52 / 525  Training Loss  0.00030155639979057014\n",
            "Epoch  25 Batch  53 / 525  Training Loss  0.0008738056640140712\n",
            "Epoch  25 Batch  54 / 525  Training Loss  0.00046263524563983083\n",
            "Epoch  25 Batch  55 / 525  Training Loss  0.00173304439522326\n",
            "Epoch  25 Batch  56 / 525  Training Loss  0.0007846551015973091\n",
            "Epoch  25 Batch  57 / 525  Training Loss  0.0001818544405978173\n",
            "Epoch  25 Batch  58 / 525  Training Loss  0.0005700309411622584\n",
            "Epoch  25 Batch  59 / 525  Training Loss  0.007495523430407047\n",
            "Epoch  25 Batch  60 / 525  Training Loss  0.0008583185262978077\n",
            "Epoch  25 Batch  61 / 525  Training Loss  0.00017253523401450366\n",
            "Epoch  25 Batch  62 / 525  Training Loss  0.006150220520794392\n",
            "Epoch  25 Batch  63 / 525  Training Loss  0.00019186556164640933\n",
            "Epoch  25 Batch  64 / 525  Training Loss  0.00047834281576797366\n",
            "Epoch  25 Batch  65 / 525  Training Loss  0.0006292557227425277\n",
            "Epoch  25 Batch  66 / 525  Training Loss  0.0003716008213814348\n",
            "Epoch  25 Batch  67 / 525  Training Loss  0.0004097230266779661\n",
            "Epoch  25 Batch  68 / 525  Training Loss  0.00033230840926989913\n",
            "Epoch  25 Batch  69 / 525  Training Loss  0.0006441125296987593\n",
            "Epoch  25 Batch  70 / 525  Training Loss  0.00025915339938364923\n",
            "Epoch  25 Batch  71 / 525  Training Loss  0.0014709591632708907\n",
            "Epoch  25 Batch  72 / 525  Training Loss  0.0005571772926487029\n",
            "Epoch  25 Batch  73 / 525  Training Loss  0.00035425813985057175\n",
            "Epoch  25 Batch  74 / 525  Training Loss  0.0004726497863885015\n",
            "Epoch  25 Batch  75 / 525  Training Loss  0.0002703735663089901\n",
            "Epoch  25 Batch  76 / 525  Training Loss  0.010297903791069984\n",
            "Epoch  25 Batch  77 / 525  Training Loss  0.0008913186611607671\n",
            "Epoch  25 Batch  78 / 525  Training Loss  0.0002769748098216951\n",
            "Epoch  25 Batch  79 / 525  Training Loss  0.000342502782586962\n",
            "Epoch  25 Batch  80 / 525  Training Loss  0.0006319712265394628\n",
            "Epoch  25 Batch  81 / 525  Training Loss  0.000665883650071919\n",
            "Epoch  25 Batch  82 / 525  Training Loss  0.0017927226144820452\n",
            "Epoch  25 Batch  83 / 525  Training Loss  0.00028686929726973176\n",
            "Epoch  25 Batch  84 / 525  Training Loss  0.0019905217923223972\n",
            "Epoch  25 Batch  85 / 525  Training Loss  0.00023861046065576375\n",
            "Epoch  25 Batch  86 / 525  Training Loss  0.012978856451809406\n",
            "Epoch  25 Batch  87 / 525  Training Loss  0.003466864349320531\n",
            "Epoch  25 Batch  88 / 525  Training Loss  0.0015251035802066326\n",
            "Epoch  25 Batch  89 / 525  Training Loss  0.0005149539792910218\n",
            "Epoch  25 Batch  90 / 525  Training Loss  0.0014712357660755515\n",
            "Epoch  25 Batch  91 / 525  Training Loss  0.0007481233333237469\n",
            "Epoch  25 Batch  92 / 525  Training Loss  0.0006730289314873517\n",
            "Epoch  25 Batch  93 / 525  Training Loss  0.00036862323759123683\n",
            "Epoch  25 Batch  94 / 525  Training Loss  0.0009898231364786625\n",
            "Epoch  25 Batch  95 / 525  Training Loss  0.0009859410347416997\n",
            "Epoch  25 Batch  96 / 525  Training Loss  0.003945070784538984\n",
            "Epoch  25 Batch  97 / 525  Training Loss  0.000534325372427702\n",
            "Epoch  25 Batch  98 / 525  Training Loss  0.0027515648398548365\n",
            "Epoch  25 Batch  99 / 525  Training Loss  0.0005429484299384058\n",
            "Epoch  25 Batch  100 / 525  Training Loss  0.003176754340529442\n",
            "Epoch  25 Batch  101 / 525  Training Loss  0.0003956272266805172\n",
            "Epoch  25 Batch  102 / 525  Training Loss  0.0004571970202960074\n",
            "Epoch  25 Batch  103 / 525  Training Loss  0.0008229376981034875\n",
            "Epoch  25 Batch  104 / 525  Training Loss  0.0015782639384269714\n",
            "Epoch  25 Batch  105 / 525  Training Loss  0.0008057759259827435\n",
            "Epoch  25 Batch  106 / 525  Training Loss  0.0013288722839206457\n",
            "Epoch  25 Batch  107 / 525  Training Loss  0.0018348386511206627\n",
            "Epoch  25 Batch  108 / 525  Training Loss  0.00042092212243005633\n",
            "Epoch  25 Batch  109 / 525  Training Loss  0.00040195154724642634\n",
            "Epoch  25 Batch  110 / 525  Training Loss  0.0003819255798589438\n",
            "Epoch  25 Batch  111 / 525  Training Loss  0.00046345219016075134\n",
            "Epoch  25 Batch  112 / 525  Training Loss  0.0008108139154501259\n",
            "Epoch  25 Batch  113 / 525  Training Loss  0.001575447036884725\n",
            "Epoch  25 Batch  114 / 525  Training Loss  0.0009510925156064332\n",
            "Epoch  25 Batch  115 / 525  Training Loss  0.0007047781255096197\n",
            "Epoch  25 Batch  116 / 525  Training Loss  0.00027892010984942317\n",
            "Epoch  25 Batch  117 / 525  Training Loss  0.003649716032668948\n",
            "Epoch  25 Batch  118 / 525  Training Loss  0.0032816757448017597\n",
            "Epoch  25 Batch  119 / 525  Training Loss  0.0013722984585911036\n",
            "Epoch  25 Batch  120 / 525  Training Loss  0.001109467470087111\n",
            "Epoch  25 Batch  121 / 525  Training Loss  0.00126869126688689\n",
            "Epoch  25 Batch  122 / 525  Training Loss  0.0009134751744568348\n",
            "Epoch  25 Batch  123 / 525  Training Loss  0.0016507413238286972\n",
            "Epoch  25 Batch  124 / 525  Training Loss  0.0008762350189499557\n",
            "Epoch  25 Batch  125 / 525  Training Loss  0.0011107203317806125\n",
            "Epoch  25 Batch  126 / 525  Training Loss  0.0009370682528242469\n",
            "Epoch  25 Batch  127 / 525  Training Loss  0.0009311506291851401\n",
            "Epoch  25 Batch  128 / 525  Training Loss  0.0008588957716710865\n",
            "Epoch  25 Batch  129 / 525  Training Loss  0.0004579531087074429\n",
            "Epoch  25 Batch  130 / 525  Training Loss  0.0009480760199949145\n",
            "Epoch  25 Batch  131 / 525  Training Loss  0.0003377492248546332\n",
            "Epoch  25 Batch  132 / 525  Training Loss  0.00046304776333272457\n",
            "Epoch  25 Batch  133 / 525  Training Loss  0.0014244072372093797\n",
            "Epoch  25 Batch  134 / 525  Training Loss  0.003412646008655429\n",
            "Epoch  25 Batch  135 / 525  Training Loss  0.000950867950450629\n",
            "Epoch  25 Batch  136 / 525  Training Loss  0.0013044081861153245\n",
            "Epoch  25 Batch  137 / 525  Training Loss  0.006604257971048355\n",
            "Epoch  25 Batch  138 / 525  Training Loss  0.0007683526491746306\n",
            "Epoch  25 Batch  139 / 525  Training Loss  0.0006584424991160631\n",
            "Epoch  25 Batch  140 / 525  Training Loss  0.0031741741113364697\n",
            "Epoch  25 Batch  141 / 525  Training Loss  0.0004530256555881351\n",
            "Epoch  25 Batch  142 / 525  Training Loss  0.0017138414550572634\n",
            "Epoch  25 Batch  143 / 525  Training Loss  0.00028190482407808304\n",
            "Epoch  25 Batch  144 / 525  Training Loss  0.0003006311017088592\n",
            "Epoch  25 Batch  145 / 525  Training Loss  0.0020791778806596994\n",
            "Epoch  25 Batch  146 / 525  Training Loss  0.007677949033677578\n",
            "Epoch  25 Batch  147 / 525  Training Loss  0.0005970724741928279\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  25 Batch  148 / 525  Training Loss  0.0008002423564903438\n",
            "Epoch  25 Batch  149 / 525  Training Loss  0.0014071294572204351\n",
            "Epoch  25 Batch  150 / 525  Training Loss  0.0017108528409153223\n",
            "Epoch  25 Batch  151 / 525  Training Loss  0.00110143912024796\n",
            "Epoch  25 Batch  152 / 525  Training Loss  0.0005426970310509205\n",
            "Epoch  25 Batch  153 / 525  Training Loss  0.0007609609747305512\n",
            "Epoch  25 Batch  154 / 525  Training Loss  0.002269947435706854\n",
            "Epoch  25 Batch  155 / 525  Training Loss  0.0016483950894325972\n",
            "Epoch  25 Batch  156 / 525  Training Loss  0.001030442537739873\n",
            "Epoch  25 Batch  157 / 525  Training Loss  0.0005285103688947856\n",
            "Epoch  25 Batch  158 / 525  Training Loss  0.0009772084886208177\n",
            "Epoch  25 Batch  159 / 525  Training Loss  0.0010100610088557005\n",
            "Epoch  25 Batch  160 / 525  Training Loss  0.0012279724469408393\n",
            "Epoch  25 Batch  161 / 525  Training Loss  0.001277160830795765\n",
            "Epoch  25 Batch  162 / 525  Training Loss  0.00036006237496621907\n",
            "Epoch  25 Batch  163 / 525  Training Loss  0.004712163936346769\n",
            "Epoch  25 Batch  164 / 525  Training Loss  0.0007125950651243329\n",
            "Epoch  25 Batch  165 / 525  Training Loss  0.0011198294814676046\n",
            "Epoch  25 Batch  166 / 525  Training Loss  0.003098126268014312\n",
            "Epoch  25 Batch  167 / 525  Training Loss  0.0010184403508901596\n",
            "Epoch  25 Batch  168 / 525  Training Loss  0.000918144010938704\n",
            "Epoch  25 Batch  169 / 525  Training Loss  0.00037901074392721057\n",
            "Epoch  25 Batch  170 / 525  Training Loss  0.001030028099194169\n",
            "Epoch  25 Batch  171 / 525  Training Loss  0.0012774292845278978\n",
            "Epoch  25 Batch  172 / 525  Training Loss  0.00210947054438293\n",
            "Epoch  25 Batch  173 / 525  Training Loss  0.0050324443727731705\n",
            "Epoch  25 Batch  174 / 525  Training Loss  0.0008634248515591025\n",
            "Epoch  25 Batch  175 / 525  Training Loss  0.0005875789793208241\n",
            "Epoch  25 Batch  176 / 525  Training Loss  0.0033278856426477432\n",
            "Epoch  25 Batch  177 / 525  Training Loss  0.0010641642147675157\n",
            "Epoch  25 Batch  178 / 525  Training Loss  0.002420493634417653\n",
            "Epoch  25 Batch  179 / 525  Training Loss  0.0006197071634232998\n",
            "Epoch  25 Batch  180 / 525  Training Loss  0.001534083392471075\n",
            "Epoch  25 Batch  181 / 525  Training Loss  0.00045377612696029246\n",
            "Epoch  25 Batch  182 / 525  Training Loss  0.003191565629094839\n",
            "Epoch  25 Batch  183 / 525  Training Loss  0.0013045596424490213\n",
            "Epoch  25 Batch  184 / 525  Training Loss  0.0008666810463182628\n",
            "Epoch  25 Batch  185 / 525  Training Loss  0.0015995942521840334\n",
            "Epoch  25 Batch  186 / 525  Training Loss  0.0005559709970839322\n",
            "Epoch  25 Batch  187 / 525  Training Loss  0.0006113573326729238\n",
            "Epoch  25 Batch  188 / 525  Training Loss  0.0005348859122022986\n",
            "Epoch  25 Batch  189 / 525  Training Loss  0.00037734495708718896\n",
            "Epoch  25 Batch  190 / 525  Training Loss  0.0004937553312629461\n",
            "Epoch  25 Batch  191 / 525  Training Loss  0.0012812323402613401\n",
            "Epoch  25 Batch  192 / 525  Training Loss  0.0003255117335356772\n",
            "Epoch  25 Batch  193 / 525  Training Loss  0.0015807764139026403\n",
            "Epoch  25 Batch  194 / 525  Training Loss  0.000516227912157774\n",
            "Epoch  25 Batch  195 / 525  Training Loss  0.00182314682751894\n",
            "Epoch  25 Batch  196 / 525  Training Loss  0.0013759394641965628\n",
            "Epoch  25 Batch  197 / 525  Training Loss  0.0005198665894567966\n",
            "Epoch  25 Batch  198 / 525  Training Loss  0.011476526036858559\n",
            "Epoch  25 Batch  199 / 525  Training Loss  0.0009022745070978999\n",
            "Epoch  25 Batch  200 / 525  Training Loss  0.0003261622041463852\n",
            "Epoch  25 Batch  201 / 525  Training Loss  0.0005719595355913043\n",
            "Epoch  25 Batch  202 / 525  Training Loss  0.0008183352765627205\n",
            "Epoch  25 Batch  203 / 525  Training Loss  0.0004194136708974838\n",
            "Epoch  25 Batch  204 / 525  Training Loss  0.0005299804615788162\n",
            "Epoch  25 Batch  205 / 525  Training Loss  0.0004386153887026012\n",
            "Epoch  25 Batch  206 / 525  Training Loss  0.000856042024679482\n",
            "Epoch  25 Batch  207 / 525  Training Loss  0.00038662488805130124\n",
            "Epoch  25 Batch  208 / 525  Training Loss  0.005431122146546841\n",
            "Epoch  25 Batch  209 / 525  Training Loss  0.0007820217870175838\n",
            "Epoch  25 Batch  210 / 525  Training Loss  0.0007106974953785539\n",
            "Epoch  25 Batch  211 / 525  Training Loss  0.00016646538279019296\n",
            "Epoch  25 Batch  212 / 525  Training Loss  0.0010895021259784698\n",
            "Epoch  25 Batch  213 / 525  Training Loss  0.0010318855056539178\n",
            "Epoch  25 Batch  214 / 525  Training Loss  0.0002648097288329154\n",
            "Epoch  25 Batch  215 / 525  Training Loss  0.0006454252288676798\n",
            "Epoch  25 Batch  216 / 525  Training Loss  0.0005677220178768039\n",
            "Epoch  25 Batch  217 / 525  Training Loss  0.0011340256314724684\n",
            "Epoch  25 Batch  218 / 525  Training Loss  0.0049837203696370125\n",
            "Epoch  25 Batch  219 / 525  Training Loss  0.0006942347972653806\n",
            "Epoch  25 Batch  220 / 525  Training Loss  0.0004956428310833871\n",
            "Epoch  25 Batch  221 / 525  Training Loss  0.0011982700088992715\n",
            "Epoch  25 Batch  222 / 525  Training Loss  0.0020823869854211807\n",
            "Epoch  25 Batch  223 / 525  Training Loss  0.00033721368527039886\n",
            "Epoch  25 Batch  224 / 525  Training Loss  0.000588455586694181\n",
            "Epoch  25 Batch  225 / 525  Training Loss  0.002393159316852689\n",
            "Epoch  25 Batch  226 / 525  Training Loss  0.0006446214392781258\n",
            "Epoch  25 Batch  227 / 525  Training Loss  0.0007769841467961669\n",
            "Epoch  25 Batch  228 / 525  Training Loss  0.0012984664645045996\n",
            "Epoch  25 Batch  229 / 525  Training Loss  0.0033084943424910307\n",
            "Epoch  25 Batch  230 / 525  Training Loss  0.0019160537049174309\n",
            "Epoch  25 Batch  231 / 525  Training Loss  0.0003632844891399145\n",
            "Epoch  25 Batch  232 / 525  Training Loss  0.00020259807934053242\n",
            "Epoch  25 Batch  233 / 525  Training Loss  0.000898788683116436\n",
            "Epoch  25 Batch  234 / 525  Training Loss  0.0005019211093895137\n",
            "Epoch  25 Batch  235 / 525  Training Loss  0.0006533883861266077\n",
            "Epoch  25 Batch  236 / 525  Training Loss  0.0026036135386675596\n",
            "Epoch  25 Batch  237 / 525  Training Loss  0.000523999216966331\n",
            "Epoch  25 Batch  238 / 525  Training Loss  0.0007018197211436927\n",
            "Epoch  25 Batch  239 / 525  Training Loss  0.0008169276406988502\n",
            "Epoch  25 Batch  240 / 525  Training Loss  0.0023033602628856897\n",
            "Epoch  25 Batch  241 / 525  Training Loss  0.0018663096707314253\n",
            "Epoch  25 Batch  242 / 525  Training Loss  0.0011696922592818737\n",
            "Epoch  25 Batch  243 / 525  Training Loss  0.0026661884039640427\n",
            "Epoch  25 Batch  244 / 525  Training Loss  0.0008833798347041011\n",
            "Epoch  25 Batch  245 / 525  Training Loss  0.0019993239548057318\n",
            "Epoch  25 Batch  246 / 525  Training Loss  0.00022902556520421058\n",
            "Epoch  25 Batch  247 / 525  Training Loss  0.0018701112130656838\n",
            "Epoch  25 Batch  248 / 525  Training Loss  0.00038773889536969364\n",
            "Epoch  25 Batch  249 / 525  Training Loss  0.0008736512390896678\n",
            "Epoch  25 Batch  250 / 525  Training Loss  0.000323422224028036\n",
            "Epoch  25 Batch  251 / 525  Training Loss  0.002256957348436117\n",
            "Epoch  25 Batch  252 / 525  Training Loss  0.00027619011234492064\n",
            "Epoch  25 Batch  253 / 525  Training Loss  0.0020586049649864435\n",
            "Epoch  25 Batch  254 / 525  Training Loss  0.000659070152323693\n",
            "Epoch  25 Batch  255 / 525  Training Loss  0.0008900940301828086\n",
            "Epoch  25 Batch  256 / 525  Training Loss  0.0015510034281760454\n",
            "Epoch  25 Batch  257 / 525  Training Loss  0.0006697377539239824\n",
            "Epoch  25 Batch  258 / 525  Training Loss  0.0017569845076650381\n",
            "Epoch  25 Batch  259 / 525  Training Loss  0.0011576266260817647\n",
            "Epoch  25 Batch  260 / 525  Training Loss  0.0003746568108908832\n",
            "Epoch  25 Batch  261 / 525  Training Loss  0.0009582180646248162\n",
            "Epoch  25 Batch  262 / 525  Training Loss  0.0005580103606916964\n",
            "Epoch  25 Batch  263 / 525  Training Loss  0.0005547936889342964\n",
            "Epoch  25 Batch  264 / 525  Training Loss  0.0006373607320711017\n",
            "Epoch  25 Batch  265 / 525  Training Loss  0.0007880161283537745\n",
            "Epoch  25 Batch  266 / 525  Training Loss  0.001457749749533832\n",
            "Epoch  25 Batch  267 / 525  Training Loss  0.0003493401745799929\n",
            "Epoch  25 Batch  268 / 525  Training Loss  0.0005330446292646229\n",
            "Epoch  25 Batch  269 / 525  Training Loss  0.0006212094449438155\n",
            "Epoch  25 Batch  270 / 525  Training Loss  0.00019287856412120163\n",
            "Epoch  25 Batch  271 / 525  Training Loss  0.0009650072315707803\n",
            "Epoch  25 Batch  272 / 525  Training Loss  0.0008200686424970627\n",
            "Epoch  25 Batch  273 / 525  Training Loss  0.002439158270135522\n",
            "Epoch  25 Batch  274 / 525  Training Loss  0.0003482811152935028\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  25 Batch  275 / 525  Training Loss  0.0005064567667432129\n",
            "Epoch  25 Batch  276 / 525  Training Loss  0.0013788515934720635\n",
            "Epoch  25 Batch  277 / 525  Training Loss  0.0011590680805966258\n",
            "Epoch  25 Batch  278 / 525  Training Loss  0.002536953892558813\n",
            "Epoch  25 Batch  279 / 525  Training Loss  0.0020675724372267723\n",
            "Epoch  25 Batch  280 / 525  Training Loss  0.001177932252176106\n",
            "Epoch  25 Batch  281 / 525  Training Loss  0.0010463753715157509\n",
            "Epoch  25 Batch  282 / 525  Training Loss  0.0006588250980712473\n",
            "Epoch  25 Batch  283 / 525  Training Loss  0.00155713502317667\n",
            "Epoch  25 Batch  284 / 525  Training Loss  0.002121167490258813\n",
            "Epoch  25 Batch  285 / 525  Training Loss  0.00036039273254573345\n",
            "Epoch  25 Batch  286 / 525  Training Loss  0.0007585881976410747\n",
            "Epoch  25 Batch  287 / 525  Training Loss  0.0006342645501717925\n",
            "Epoch  25 Batch  288 / 525  Training Loss  0.0017617277335375547\n",
            "Epoch  25 Batch  289 / 525  Training Loss  0.0006446960032917559\n",
            "Epoch  25 Batch  290 / 525  Training Loss  0.00041556969517841935\n",
            "Epoch  25 Batch  291 / 525  Training Loss  0.0014731705887243152\n",
            "Epoch  25 Batch  292 / 525  Training Loss  0.0006831745849922299\n",
            "Epoch  25 Batch  293 / 525  Training Loss  0.002383094048127532\n",
            "Epoch  25 Batch  294 / 525  Training Loss  0.0007750875665806234\n",
            "Epoch  25 Batch  295 / 525  Training Loss  0.000935379124712199\n",
            "Epoch  25 Batch  296 / 525  Training Loss  0.00036490309867076576\n",
            "Epoch  25 Batch  297 / 525  Training Loss  0.00035944534465670586\n",
            "Epoch  25 Batch  298 / 525  Training Loss  0.0006558426539413631\n",
            "Epoch  25 Batch  299 / 525  Training Loss  0.00016231824702117592\n",
            "Epoch  25 Batch  300 / 525  Training Loss  0.0004298813291825354\n",
            "Epoch  25 Batch  301 / 525  Training Loss  0.000692431116476655\n",
            "Epoch  25 Batch  302 / 525  Training Loss  0.0010082043008878827\n",
            "Epoch  25 Batch  303 / 525  Training Loss  0.0004883810179308057\n",
            "Epoch  25 Batch  304 / 525  Training Loss  0.0012222094228491187\n",
            "Epoch  25 Batch  305 / 525  Training Loss  0.004690614063292742\n",
            "Epoch  25 Batch  306 / 525  Training Loss  0.0004623847489710897\n",
            "Epoch  25 Batch  307 / 525  Training Loss  0.0005259145400486887\n",
            "Epoch  25 Batch  308 / 525  Training Loss  0.01972338929772377\n",
            "Epoch  25 Batch  309 / 525  Training Loss  0.0002719006151892245\n",
            "Epoch  25 Batch  310 / 525  Training Loss  0.0009049767395481467\n",
            "Epoch  25 Batch  311 / 525  Training Loss  0.00028113817097619176\n",
            "Epoch  25 Batch  312 / 525  Training Loss  0.0038258905988186598\n",
            "Epoch  25 Batch  313 / 525  Training Loss  0.006204635836184025\n",
            "Epoch  25 Batch  314 / 525  Training Loss  0.0006870016804896295\n",
            "Epoch  25 Batch  315 / 525  Training Loss  0.0023638673592358828\n",
            "Epoch  25 Batch  316 / 525  Training Loss  0.0015261522494256496\n",
            "Epoch  25 Batch  317 / 525  Training Loss  0.0018177542369812727\n",
            "Epoch  25 Batch  318 / 525  Training Loss  0.00035809172550216317\n",
            "Epoch  25 Batch  319 / 525  Training Loss  0.00023688496730756015\n",
            "Epoch  25 Batch  320 / 525  Training Loss  0.00015234171587508172\n",
            "Epoch  25 Batch  321 / 525  Training Loss  0.0006799986003898084\n",
            "Epoch  25 Batch  322 / 525  Training Loss  0.0004301064764149487\n",
            "Epoch  25 Batch  323 / 525  Training Loss  0.004715058021247387\n",
            "Epoch  25 Batch  324 / 525  Training Loss  0.0006209475686773658\n",
            "Epoch  25 Batch  325 / 525  Training Loss  0.0008992349612526596\n",
            "Epoch  25 Batch  326 / 525  Training Loss  0.0012314373161643744\n",
            "Epoch  25 Batch  327 / 525  Training Loss  0.001396579435095191\n",
            "Epoch  25 Batch  328 / 525  Training Loss  0.0005873314221389592\n",
            "Epoch  25 Batch  329 / 525  Training Loss  0.001052661333233118\n",
            "Epoch  25 Batch  330 / 525  Training Loss  0.0007815154967829585\n",
            "Epoch  25 Batch  331 / 525  Training Loss  0.0005059548420831561\n",
            "Epoch  25 Batch  332 / 525  Training Loss  0.0017629277426749468\n",
            "Epoch  25 Batch  333 / 525  Training Loss  0.0005438668304122984\n",
            "Epoch  25 Batch  334 / 525  Training Loss  0.001119061023928225\n",
            "Epoch  25 Batch  335 / 525  Training Loss  0.0003110070829279721\n",
            "Epoch  25 Batch  336 / 525  Training Loss  0.0002444588462822139\n",
            "Epoch  25 Batch  337 / 525  Training Loss  0.0006102159386500716\n",
            "Epoch  25 Batch  338 / 525  Training Loss  0.0005144420429132879\n",
            "Epoch  25 Batch  339 / 525  Training Loss  0.00045716390013694763\n",
            "Epoch  25 Batch  340 / 525  Training Loss  0.001047998433932662\n",
            "Epoch  25 Batch  341 / 525  Training Loss  0.00033757463097572327\n",
            "Epoch  25 Batch  342 / 525  Training Loss  0.000507341290358454\n",
            "Epoch  25 Batch  343 / 525  Training Loss  0.0004814792482648045\n",
            "Epoch  25 Batch  344 / 525  Training Loss  0.000666002044454217\n",
            "Epoch  25 Batch  345 / 525  Training Loss  0.00019276370585430413\n",
            "Epoch  25 Batch  346 / 525  Training Loss  0.0007203280692920089\n",
            "Epoch  25 Batch  347 / 525  Training Loss  0.000554502650629729\n",
            "Epoch  25 Batch  348 / 525  Training Loss  0.0006035752012394369\n",
            "Epoch  25 Batch  349 / 525  Training Loss  0.0070724161341786385\n",
            "Epoch  25 Batch  350 / 525  Training Loss  0.0018280020449310541\n",
            "Epoch  25 Batch  351 / 525  Training Loss  0.0007275509997271001\n",
            "Epoch  25 Batch  352 / 525  Training Loss  0.0007515104953199625\n",
            "Epoch  25 Batch  353 / 525  Training Loss  0.0002863657136913389\n",
            "Epoch  25 Batch  354 / 525  Training Loss  0.0003128298558294773\n",
            "Epoch  25 Batch  355 / 525  Training Loss  0.0017218375578522682\n",
            "Epoch  25 Batch  356 / 525  Training Loss  0.000850742100737989\n",
            "Epoch  25 Batch  357 / 525  Training Loss  0.000802818511147052\n",
            "Epoch  25 Batch  358 / 525  Training Loss  0.0030149389058351517\n",
            "Epoch  25 Batch  359 / 525  Training Loss  0.00038458415656350553\n",
            "Epoch  25 Batch  360 / 525  Training Loss  0.0005378565983846784\n",
            "Epoch  25 Batch  361 / 525  Training Loss  0.0014730412513017654\n",
            "Epoch  25 Batch  362 / 525  Training Loss  0.00037748634349554777\n",
            "Epoch  25 Batch  363 / 525  Training Loss  0.0005278588505461812\n",
            "Epoch  25 Batch  364 / 525  Training Loss  0.0008381091756746173\n",
            "Epoch  25 Batch  365 / 525  Training Loss  0.0009925737977027893\n",
            "Epoch  25 Batch  366 / 525  Training Loss  0.0010615612845867872\n",
            "Epoch  25 Batch  367 / 525  Training Loss  0.0018203925574198365\n",
            "Epoch  25 Batch  368 / 525  Training Loss  0.001517405267804861\n",
            "Epoch  25 Batch  369 / 525  Training Loss  0.00023287921794690192\n",
            "Epoch  25 Batch  370 / 525  Training Loss  0.002411716151982546\n",
            "Epoch  25 Batch  371 / 525  Training Loss  0.0013856214936822653\n",
            "Epoch  25 Batch  372 / 525  Training Loss  0.00031264775316230953\n",
            "Epoch  25 Batch  373 / 525  Training Loss  0.00084726233035326\n",
            "Epoch  25 Batch  374 / 525  Training Loss  0.0003069703816436231\n",
            "Epoch  25 Batch  375 / 525  Training Loss  0.0011352254077792168\n",
            "Epoch  25 Batch  376 / 525  Training Loss  0.0007098495261743665\n",
            "Epoch  25 Batch  377 / 525  Training Loss  0.0004419746692292392\n",
            "Epoch  25 Batch  378 / 525  Training Loss  0.0019389099907130003\n",
            "Epoch  25 Batch  379 / 525  Training Loss  0.0005780180217698216\n",
            "Epoch  25 Batch  380 / 525  Training Loss  0.008899989537894726\n",
            "Epoch  25 Batch  381 / 525  Training Loss  0.00167869555298239\n",
            "Epoch  25 Batch  382 / 525  Training Loss  0.00025010976241901517\n",
            "Epoch  25 Batch  383 / 525  Training Loss  0.004968459717929363\n",
            "Epoch  25 Batch  384 / 525  Training Loss  0.0004943019011989236\n",
            "Epoch  25 Batch  385 / 525  Training Loss  0.000509490433614701\n",
            "Epoch  25 Batch  386 / 525  Training Loss  0.0011602926533669233\n",
            "Epoch  25 Batch  387 / 525  Training Loss  0.0007424842333421111\n",
            "Epoch  25 Batch  388 / 525  Training Loss  0.000254325132118538\n",
            "Epoch  25 Batch  389 / 525  Training Loss  0.00107250374276191\n",
            "Epoch  25 Batch  390 / 525  Training Loss  0.0007404392235912383\n",
            "Epoch  25 Batch  391 / 525  Training Loss  0.0004052896110806614\n",
            "Epoch  25 Batch  392 / 525  Training Loss  0.005952359642833471\n",
            "Epoch  25 Batch  393 / 525  Training Loss  0.0010704394662752748\n",
            "Epoch  25 Batch  394 / 525  Training Loss  0.0006548631936311722\n",
            "Epoch  25 Batch  395 / 525  Training Loss  0.00041785178473219275\n",
            "Epoch  25 Batch  396 / 525  Training Loss  0.0010818433947861195\n",
            "Epoch  25 Batch  397 / 525  Training Loss  0.00040449746302329004\n",
            "Epoch  25 Batch  398 / 525  Training Loss  0.0022094922605901957\n",
            "Epoch  25 Batch  399 / 525  Training Loss  0.0007644199067726731\n",
            "Epoch  25 Batch  400 / 525  Training Loss  0.0004361615574453026\n",
            "Epoch  25 Batch  401 / 525  Training Loss  0.0002563921152614057\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  25 Batch  402 / 525  Training Loss  0.00027041370049118996\n",
            "Epoch  25 Batch  403 / 525  Training Loss  0.0002038267848547548\n",
            "Epoch  25 Batch  404 / 525  Training Loss  0.000294765253784135\n",
            "Epoch  25 Batch  405 / 525  Training Loss  0.0007552419556304812\n",
            "Epoch  25 Batch  406 / 525  Training Loss  0.0005411951569840312\n",
            "Epoch  25 Batch  407 / 525  Training Loss  0.0005523414583876729\n",
            "Epoch  25 Batch  408 / 525  Training Loss  0.0002603696193546057\n",
            "Epoch  25 Batch  409 / 525  Training Loss  0.0008224155753850937\n",
            "Epoch  25 Batch  410 / 525  Training Loss  0.00044372474076226354\n",
            "Epoch  25 Batch  411 / 525  Training Loss  0.0007101315422914922\n",
            "Epoch  25 Batch  412 / 525  Training Loss  0.0013410731917247176\n",
            "Epoch  25 Batch  413 / 525  Training Loss  0.00030593707924708724\n",
            "Epoch  25 Batch  414 / 525  Training Loss  0.0011313949944451451\n",
            "Epoch  25 Batch  415 / 525  Training Loss  0.0005393716273829341\n",
            "Epoch  25 Batch  416 / 525  Training Loss  0.0006576093728654087\n",
            "Epoch  25 Batch  417 / 525  Training Loss  0.016846099868416786\n",
            "Epoch  25 Batch  418 / 525  Training Loss  0.0006960192695260048\n",
            "Epoch  25 Batch  419 / 525  Training Loss  0.0006648245616815984\n",
            "Epoch  25 Batch  420 / 525  Training Loss  0.0007369006052613258\n",
            "Epoch  25 Batch  421 / 525  Training Loss  0.0005324786761775613\n",
            "Epoch  25 Batch  422 / 525  Training Loss  0.0010025000665336847\n",
            "Epoch  25 Batch  423 / 525  Training Loss  0.0003091318649239838\n",
            "Epoch  25 Batch  424 / 525  Training Loss  0.00032582611311227083\n",
            "Epoch  25 Batch  425 / 525  Training Loss  0.0006103423656895757\n",
            "Epoch  25 Batch  426 / 525  Training Loss  0.00130634312517941\n",
            "Epoch  25 Batch  427 / 525  Training Loss  0.001704022055491805\n",
            "Epoch  25 Batch  428 / 525  Training Loss  0.00110563391353935\n",
            "Epoch  25 Batch  429 / 525  Training Loss  0.00034382633748464286\n",
            "Epoch  25 Batch  430 / 525  Training Loss  0.000947280612308532\n",
            "Epoch  25 Batch  431 / 525  Training Loss  0.0004979814402759075\n",
            "Epoch  25 Batch  432 / 525  Training Loss  0.000604002270847559\n",
            "Epoch  25 Batch  433 / 525  Training Loss  0.0006150372209958732\n",
            "Epoch  25 Batch  434 / 525  Training Loss  0.0011963165597990155\n",
            "Epoch  25 Batch  435 / 525  Training Loss  0.00034756766399368644\n",
            "Epoch  25 Batch  436 / 525  Training Loss  0.0021977496799081564\n",
            "Epoch  25 Batch  437 / 525  Training Loss  0.0005095015512779355\n",
            "Epoch  25 Batch  438 / 525  Training Loss  0.000627850356977433\n",
            "Epoch  25 Batch  439 / 525  Training Loss  0.00023883448739070445\n",
            "Epoch  25 Batch  440 / 525  Training Loss  0.002548374468460679\n",
            "Epoch  25 Batch  441 / 525  Training Loss  0.0006751691689714789\n",
            "Epoch  25 Batch  442 / 525  Training Loss  0.003033239394426346\n",
            "Epoch  25 Batch  443 / 525  Training Loss  0.0002897731610573828\n",
            "Epoch  25 Batch  444 / 525  Training Loss  0.0004614323261193931\n",
            "Epoch  25 Batch  445 / 525  Training Loss  0.003000786993652582\n",
            "Epoch  25 Batch  446 / 525  Training Loss  0.0005907240556553006\n",
            "Epoch  25 Batch  447 / 525  Training Loss  0.002661022823303938\n",
            "Epoch  25 Batch  448 / 525  Training Loss  0.0009002627921290696\n",
            "Epoch  25 Batch  449 / 525  Training Loss  0.0009804696310311556\n",
            "Epoch  25 Batch  450 / 525  Training Loss  0.0028690448962152004\n",
            "Epoch  25 Batch  451 / 525  Training Loss  0.0003517436271067709\n",
            "Epoch  25 Batch  452 / 525  Training Loss  0.0009974197018891573\n",
            "Epoch  25 Batch  453 / 525  Training Loss  0.0020485809072852135\n",
            "Epoch  25 Batch  454 / 525  Training Loss  0.0007660124683752656\n",
            "Epoch  25 Batch  455 / 525  Training Loss  0.0006087432848289609\n",
            "Epoch  25 Batch  456 / 525  Training Loss  0.006755840964615345\n",
            "Epoch  25 Batch  457 / 525  Training Loss  0.0002703466161619872\n",
            "Epoch  25 Batch  458 / 525  Training Loss  0.0003955010324716568\n",
            "Epoch  25 Batch  459 / 525  Training Loss  0.0024211439304053783\n",
            "Epoch  25 Batch  460 / 525  Training Loss  0.001329557504504919\n",
            "Epoch  25 Batch  461 / 525  Training Loss  0.0009992761770263314\n",
            "Epoch  25 Batch  462 / 525  Training Loss  0.002842564135789871\n",
            "Epoch  25 Batch  463 / 525  Training Loss  0.00036588372313417494\n",
            "Epoch  25 Batch  464 / 525  Training Loss  0.0013784125912934542\n",
            "Epoch  25 Batch  465 / 525  Training Loss  0.00031384738394990563\n",
            "Epoch  25 Batch  466 / 525  Training Loss  0.0008221299503929913\n",
            "Epoch  25 Batch  467 / 525  Training Loss  0.0004378788289614022\n",
            "Epoch  25 Batch  468 / 525  Training Loss  0.0021290790755301714\n",
            "Epoch  25 Batch  469 / 525  Training Loss  0.0015179449692368507\n",
            "Epoch  25 Batch  470 / 525  Training Loss  0.0005895550712011755\n",
            "Epoch  25 Batch  471 / 525  Training Loss  0.0011621329467743635\n",
            "Epoch  25 Batch  472 / 525  Training Loss  0.002960357815027237\n",
            "Epoch  25 Batch  473 / 525  Training Loss  0.0012710076989606023\n",
            "Epoch  25 Batch  474 / 525  Training Loss  0.007933225482702255\n",
            "Epoch  25 Batch  475 / 525  Training Loss  0.0008257444715127349\n",
            "Epoch  25 Batch  476 / 525  Training Loss  0.0011934845242649317\n",
            "Epoch  25 Batch  477 / 525  Training Loss  0.0010915230959653854\n",
            "Epoch  25 Batch  478 / 525  Training Loss  0.0005324098165147007\n",
            "Epoch  25 Batch  479 / 525  Training Loss  0.0008215238340198994\n",
            "Epoch  25 Batch  480 / 525  Training Loss  0.0012682413216680288\n",
            "Epoch  25 Batch  481 / 525  Training Loss  0.0006222286028787494\n",
            "Epoch  25 Batch  482 / 525  Training Loss  0.0010600818786770105\n",
            "Epoch  25 Batch  483 / 525  Training Loss  0.0010916809551417828\n",
            "Epoch  25 Batch  484 / 525  Training Loss  0.001019892399199307\n",
            "Epoch  25 Batch  485 / 525  Training Loss  0.0003484730259515345\n",
            "Epoch  25 Batch  486 / 525  Training Loss  0.005078216083347797\n",
            "Epoch  25 Batch  487 / 525  Training Loss  0.0036122954916208982\n",
            "Epoch  25 Batch  488 / 525  Training Loss  0.0014763958752155304\n",
            "Epoch  25 Batch  489 / 525  Training Loss  0.0005332233267836273\n",
            "Epoch  25 Batch  490 / 525  Training Loss  0.00042841010144911706\n",
            "Epoch  25 Batch  491 / 525  Training Loss  0.000906822388060391\n",
            "Epoch  25 Batch  492 / 525  Training Loss  0.0007454216829501092\n",
            "Epoch  25 Batch  493 / 525  Training Loss  0.0007265928434208035\n",
            "Epoch  25 Batch  494 / 525  Training Loss  0.0008955536177381873\n",
            "Epoch  25 Batch  495 / 525  Training Loss  0.0005179269355721772\n",
            "Epoch  25 Batch  496 / 525  Training Loss  0.0007644913857802749\n",
            "Epoch  25 Batch  497 / 525  Training Loss  0.02283882349729538\n",
            "Epoch  25 Batch  498 / 525  Training Loss  0.002169004874303937\n",
            "Epoch  25 Batch  499 / 525  Training Loss  0.0016637342050671577\n",
            "Epoch  25 Batch  500 / 525  Training Loss  0.0005270384717732668\n",
            "Epoch  25 Batch  501 / 525  Training Loss  0.0008569903438910842\n",
            "Epoch  25 Batch  502 / 525  Training Loss  0.0008019510423764586\n",
            "Epoch  25 Batch  503 / 525  Training Loss  0.0008365263347513974\n",
            "Epoch  25 Batch  504 / 525  Training Loss  0.0017413778696209192\n",
            "Epoch  25 Batch  505 / 525  Training Loss  0.0011163989547640085\n",
            "Epoch  25 Batch  506 / 525  Training Loss  0.002102621365338564\n",
            "Epoch  25 Batch  507 / 525  Training Loss  0.0007293374510481954\n",
            "Epoch  25 Batch  508 / 525  Training Loss  0.001340115093626082\n",
            "Epoch  25 Batch  509 / 525  Training Loss  0.0031446977518498898\n",
            "Epoch  25 Batch  510 / 525  Training Loss  0.0007565348641946912\n",
            "Epoch  25 Batch  511 / 525  Training Loss  0.0004113113973289728\n",
            "Epoch  25 Batch  512 / 525  Training Loss  0.00037324154982343316\n",
            "Epoch  25 Batch  513 / 525  Training Loss  0.0009736932697705925\n",
            "Epoch  25 Batch  514 / 525  Training Loss  0.0011497648665681481\n",
            "Epoch  25 Batch  515 / 525  Training Loss  0.001292429631575942\n",
            "Epoch  25 Batch  516 / 525  Training Loss  0.0008058931562118232\n",
            "Epoch  25 Batch  517 / 525  Training Loss  0.0004180096148047596\n",
            "Epoch  25 Batch  518 / 525  Training Loss  0.00024348504666704684\n",
            "Epoch  25 Batch  519 / 525  Training Loss  0.0008646162459626794\n",
            "Epoch  25 Batch  520 / 525  Training Loss  0.000427603634307161\n",
            "Epoch  25 Batch  521 / 525  Training Loss  0.00125690340064466\n",
            "Epoch  25 Batch  522 / 525  Training Loss  0.0006351919146254659\n",
            "Epoch  25 Batch  523 / 525  Training Loss  0.000581278232857585\n",
            "Epoch  25 Batch  524 / 525  Training Loss  0.00030160386813804507\n",
            "  26    |    -    |   0.001343   | 62.325000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 26\n",
            "Epoch  26 Batch  0 / 525  Training Loss  0.00020954634237568825\n",
            "Epoch  26 Batch  1 / 525  Training Loss  0.0008345317328348756\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  26 Batch  2 / 525  Training Loss  0.000156769368913956\n",
            "Epoch  26 Batch  3 / 525  Training Loss  0.0002850370656233281\n",
            "Epoch  26 Batch  4 / 525  Training Loss  0.001865266589447856\n",
            "Epoch  26 Batch  5 / 525  Training Loss  0.0004248475597705692\n",
            "Epoch  26 Batch  6 / 525  Training Loss  8.222315955208614e-05\n",
            "Epoch  26 Batch  7 / 525  Training Loss  0.0001768836664268747\n",
            "Epoch  26 Batch  8 / 525  Training Loss  0.00028807439957745373\n",
            "Epoch  26 Batch  9 / 525  Training Loss  0.00017912397743202746\n",
            "Epoch  26 Batch  10 / 525  Training Loss  0.0002110484056174755\n",
            "Epoch  26 Batch  11 / 525  Training Loss  0.0008496840600855649\n",
            "Epoch  26 Batch  12 / 525  Training Loss  0.00042093382216989994\n",
            "Epoch  26 Batch  13 / 525  Training Loss  0.0004399646131787449\n",
            "Epoch  26 Batch  14 / 525  Training Loss  0.0009043168975040317\n",
            "Epoch  26 Batch  15 / 525  Training Loss  0.0002213053376181051\n",
            "Epoch  26 Batch  16 / 525  Training Loss  0.0005351522122509778\n",
            "Epoch  26 Batch  17 / 525  Training Loss  0.0005025282735005021\n",
            "Epoch  26 Batch  18 / 525  Training Loss  0.00023453388712368906\n",
            "Epoch  26 Batch  19 / 525  Training Loss  0.00023740071628708392\n",
            "Epoch  26 Batch  20 / 525  Training Loss  0.0005726803210563958\n",
            "Epoch  26 Batch  21 / 525  Training Loss  0.000255284714512527\n",
            "Epoch  26 Batch  22 / 525  Training Loss  0.00043148495024070144\n",
            "Epoch  26 Batch  23 / 525  Training Loss  0.0004666574241127819\n",
            "Epoch  26 Batch  24 / 525  Training Loss  0.00023703234910499305\n",
            "Epoch  26 Batch  25 / 525  Training Loss  0.0004899726482108235\n",
            "Epoch  26 Batch  26 / 525  Training Loss  0.0002585947513580322\n",
            "Epoch  26 Batch  27 / 525  Training Loss  0.0003882359596900642\n",
            "Epoch  26 Batch  28 / 525  Training Loss  0.0005242707557044923\n",
            "Epoch  26 Batch  29 / 525  Training Loss  0.000511310703586787\n",
            "Epoch  26 Batch  30 / 525  Training Loss  0.00039423987618647516\n",
            "Epoch  26 Batch  31 / 525  Training Loss  0.00047975173220038414\n",
            "Epoch  26 Batch  32 / 525  Training Loss  0.0002813479513861239\n",
            "Epoch  26 Batch  33 / 525  Training Loss  0.0006624103989452124\n",
            "Epoch  26 Batch  34 / 525  Training Loss  0.0002760618517640978\n",
            "Epoch  26 Batch  35 / 525  Training Loss  0.000231504236580804\n",
            "Epoch  26 Batch  36 / 525  Training Loss  0.0005025335121899843\n",
            "Epoch  26 Batch  37 / 525  Training Loss  0.00017309468239545822\n",
            "Epoch  26 Batch  38 / 525  Training Loss  0.0011732429265975952\n",
            "Epoch  26 Batch  39 / 525  Training Loss  0.0004972345777787268\n",
            "Epoch  26 Batch  40 / 525  Training Loss  0.0006109960959292948\n",
            "Epoch  26 Batch  41 / 525  Training Loss  0.0003655531327240169\n",
            "Epoch  26 Batch  42 / 525  Training Loss  0.000284740497590974\n",
            "Epoch  26 Batch  43 / 525  Training Loss  0.0012164167128503323\n",
            "Epoch  26 Batch  44 / 525  Training Loss  0.0001952063466887921\n",
            "Epoch  26 Batch  45 / 525  Training Loss  0.00017147845937870443\n",
            "Epoch  26 Batch  46 / 525  Training Loss  0.00015512664685957134\n",
            "Epoch  26 Batch  47 / 525  Training Loss  0.0002470707695465535\n",
            "Epoch  26 Batch  48 / 525  Training Loss  0.0004631986375898123\n",
            "Epoch  26 Batch  49 / 525  Training Loss  0.00018016032117884606\n",
            "Epoch  26 Batch  50 / 525  Training Loss  0.00033091442310251296\n",
            "Epoch  26 Batch  51 / 525  Training Loss  0.0003506665234453976\n",
            "Epoch  26 Batch  52 / 525  Training Loss  0.00017451404710300267\n",
            "Epoch  26 Batch  53 / 525  Training Loss  0.0005253454437479377\n",
            "Epoch  26 Batch  54 / 525  Training Loss  0.00021406426094472408\n",
            "Epoch  26 Batch  55 / 525  Training Loss  0.0003745105932466686\n",
            "Epoch  26 Batch  56 / 525  Training Loss  0.00040428858483210206\n",
            "Epoch  26 Batch  57 / 525  Training Loss  0.00028939443291164935\n",
            "Epoch  26 Batch  58 / 525  Training Loss  0.0003342697164043784\n",
            "Epoch  26 Batch  59 / 525  Training Loss  0.00042771268635988235\n",
            "Epoch  26 Batch  60 / 525  Training Loss  0.00027616319130174816\n",
            "Epoch  26 Batch  61 / 525  Training Loss  0.0002461968397255987\n",
            "Epoch  26 Batch  62 / 525  Training Loss  0.00015706305566709489\n",
            "Epoch  26 Batch  63 / 525  Training Loss  0.00028115249006077647\n",
            "Epoch  26 Batch  64 / 525  Training Loss  0.00042787916027009487\n",
            "Epoch  26 Batch  65 / 525  Training Loss  0.00027125899214297533\n",
            "Epoch  26 Batch  66 / 525  Training Loss  0.00025070601259358227\n",
            "Epoch  26 Batch  67 / 525  Training Loss  0.00016999423678498715\n",
            "Epoch  26 Batch  68 / 525  Training Loss  0.0007592610782012343\n",
            "Epoch  26 Batch  69 / 525  Training Loss  0.00022590113803744316\n",
            "Epoch  26 Batch  70 / 525  Training Loss  0.00015647335385438055\n",
            "Epoch  26 Batch  71 / 525  Training Loss  0.000861438806168735\n",
            "Epoch  26 Batch  72 / 525  Training Loss  0.0002093813818646595\n",
            "Epoch  26 Batch  73 / 525  Training Loss  0.0019558011554181576\n",
            "Epoch  26 Batch  74 / 525  Training Loss  0.00022195532801561058\n",
            "Epoch  26 Batch  75 / 525  Training Loss  0.00016865661018528044\n",
            "Epoch  26 Batch  76 / 525  Training Loss  0.00012471100490074605\n",
            "Epoch  26 Batch  77 / 525  Training Loss  0.00010552105959504843\n",
            "Epoch  26 Batch  78 / 525  Training Loss  0.0002108005282934755\n",
            "Epoch  26 Batch  79 / 525  Training Loss  0.0007866643136367202\n",
            "Epoch  26 Batch  80 / 525  Training Loss  0.0005898018134757876\n",
            "Epoch  26 Batch  81 / 525  Training Loss  0.00015554488345514983\n",
            "Epoch  26 Batch  82 / 525  Training Loss  0.0004112697788514197\n",
            "Epoch  26 Batch  83 / 525  Training Loss  0.000501239497680217\n",
            "Epoch  26 Batch  84 / 525  Training Loss  0.00045953475637361407\n",
            "Epoch  26 Batch  85 / 525  Training Loss  0.0001708395138848573\n",
            "Epoch  26 Batch  86 / 525  Training Loss  0.00024377941736020148\n",
            "Epoch  26 Batch  87 / 525  Training Loss  0.0004738406278192997\n",
            "Epoch  26 Batch  88 / 525  Training Loss  0.00040504540083929896\n",
            "Epoch  26 Batch  89 / 525  Training Loss  0.0004256057145539671\n",
            "Epoch  26 Batch  90 / 525  Training Loss  0.000273237528745085\n",
            "Epoch  26 Batch  91 / 525  Training Loss  0.00021065988403279334\n",
            "Epoch  26 Batch  92 / 525  Training Loss  0.0003354678046889603\n",
            "Epoch  26 Batch  93 / 525  Training Loss  0.0002553007798269391\n",
            "Epoch  26 Batch  94 / 525  Training Loss  0.00039049741462804377\n",
            "Epoch  26 Batch  95 / 525  Training Loss  0.00010144758562091738\n",
            "Epoch  26 Batch  96 / 525  Training Loss  0.003130530472844839\n",
            "Epoch  26 Batch  97 / 525  Training Loss  0.0004224369768053293\n",
            "Epoch  26 Batch  98 / 525  Training Loss  0.0001181420375360176\n",
            "Epoch  26 Batch  99 / 525  Training Loss  0.000725113379303366\n",
            "Epoch  26 Batch  100 / 525  Training Loss  0.00036183639895170927\n",
            "Epoch  26 Batch  101 / 525  Training Loss  0.0006347576854750514\n",
            "Epoch  26 Batch  102 / 525  Training Loss  0.0011777457548305392\n",
            "Epoch  26 Batch  103 / 525  Training Loss  0.000325932604027912\n",
            "Epoch  26 Batch  104 / 525  Training Loss  0.0012489536311477423\n",
            "Epoch  26 Batch  105 / 525  Training Loss  0.00018937434651888907\n",
            "Epoch  26 Batch  106 / 525  Training Loss  0.0015001746360212564\n",
            "Epoch  26 Batch  107 / 525  Training Loss  0.00040351226925849915\n",
            "Epoch  26 Batch  108 / 525  Training Loss  0.0006165890372358263\n",
            "Epoch  26 Batch  109 / 525  Training Loss  0.00019107978732790798\n",
            "Epoch  26 Batch  110 / 525  Training Loss  0.0002923646825365722\n",
            "Epoch  26 Batch  111 / 525  Training Loss  0.0001105439368984662\n",
            "Epoch  26 Batch  112 / 525  Training Loss  0.0003722554538398981\n",
            "Epoch  26 Batch  113 / 525  Training Loss  0.00032941525569185615\n",
            "Epoch  26 Batch  114 / 525  Training Loss  0.0007014974253252149\n",
            "Epoch  26 Batch  115 / 525  Training Loss  0.0005155183607712388\n",
            "Epoch  26 Batch  116 / 525  Training Loss  0.0009274639887735248\n",
            "Epoch  26 Batch  117 / 525  Training Loss  0.0002272576093673706\n",
            "Epoch  26 Batch  118 / 525  Training Loss  0.0010942481458187103\n",
            "Epoch  26 Batch  119 / 525  Training Loss  0.0002411947789369151\n",
            "Epoch  26 Batch  120 / 525  Training Loss  0.0006821887800469995\n",
            "Epoch  26 Batch  121 / 525  Training Loss  0.0002709361142478883\n",
            "Epoch  26 Batch  122 / 525  Training Loss  0.0002553728991188109\n",
            "Epoch  26 Batch  123 / 525  Training Loss  0.0006348536699078977\n",
            "Epoch  26 Batch  124 / 525  Training Loss  0.00045634005800820887\n",
            "Epoch  26 Batch  125 / 525  Training Loss  0.00010713907977333292\n",
            "Epoch  26 Batch  126 / 525  Training Loss  0.00027810377650894225\n",
            "Epoch  26 Batch  127 / 525  Training Loss  0.00041526873246766627\n",
            "Epoch  26 Batch  128 / 525  Training Loss  0.00030005554435774684\n",
            "Epoch  26 Batch  129 / 525  Training Loss  0.0004070707072969526\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  26 Batch  130 / 525  Training Loss  0.0001387403317494318\n",
            "Epoch  26 Batch  131 / 525  Training Loss  0.0005459868116304278\n",
            "Epoch  26 Batch  132 / 525  Training Loss  0.0005739987245760858\n",
            "Epoch  26 Batch  133 / 525  Training Loss  0.0005649864906445146\n",
            "Epoch  26 Batch  134 / 525  Training Loss  0.0002700634067878127\n",
            "Epoch  26 Batch  135 / 525  Training Loss  0.0011430898448452353\n",
            "Epoch  26 Batch  136 / 525  Training Loss  0.0006401620339602232\n",
            "Epoch  26 Batch  137 / 525  Training Loss  0.0001848331739893183\n",
            "Epoch  26 Batch  138 / 525  Training Loss  0.0002610447409097105\n",
            "Epoch  26 Batch  139 / 525  Training Loss  0.001096433843486011\n",
            "Epoch  26 Batch  140 / 525  Training Loss  0.00040932526462711394\n",
            "Epoch  26 Batch  141 / 525  Training Loss  0.0005189176881685853\n",
            "Epoch  26 Batch  142 / 525  Training Loss  0.0002554630918893963\n",
            "Epoch  26 Batch  143 / 525  Training Loss  0.0003246865526307374\n",
            "Epoch  26 Batch  144 / 525  Training Loss  0.0003188818518538028\n",
            "Epoch  26 Batch  145 / 525  Training Loss  0.00028164900140836835\n",
            "Epoch  26 Batch  146 / 525  Training Loss  0.00036180022289045155\n",
            "Epoch  26 Batch  147 / 525  Training Loss  0.0002185266639571637\n",
            "Epoch  26 Batch  148 / 525  Training Loss  0.0009046229533851147\n",
            "Epoch  26 Batch  149 / 525  Training Loss  0.00034497957676649094\n",
            "Epoch  26 Batch  150 / 525  Training Loss  0.0005077157402411103\n",
            "Epoch  26 Batch  151 / 525  Training Loss  0.00020860577933490276\n",
            "Epoch  26 Batch  152 / 525  Training Loss  0.00022798404097557068\n",
            "Epoch  26 Batch  153 / 525  Training Loss  0.0002511462371330708\n",
            "Epoch  26 Batch  154 / 525  Training Loss  0.00020645275071728975\n",
            "Epoch  26 Batch  155 / 525  Training Loss  0.00015292914758902043\n",
            "Epoch  26 Batch  156 / 525  Training Loss  0.00045438436791300774\n",
            "Epoch  26 Batch  157 / 525  Training Loss  0.00021259227651171386\n",
            "Epoch  26 Batch  158 / 525  Training Loss  0.0001701835572021082\n",
            "Epoch  26 Batch  159 / 525  Training Loss  0.00032379559706896544\n",
            "Epoch  26 Batch  160 / 525  Training Loss  0.00016621781105641276\n",
            "Epoch  26 Batch  161 / 525  Training Loss  0.00017295819998253137\n",
            "Epoch  26 Batch  162 / 525  Training Loss  0.0001986264978768304\n",
            "Epoch  26 Batch  163 / 525  Training Loss  0.00033690655254758894\n",
            "Epoch  26 Batch  164 / 525  Training Loss  0.00034713378408923745\n",
            "Epoch  26 Batch  165 / 525  Training Loss  0.0008742486243136227\n",
            "Epoch  26 Batch  166 / 525  Training Loss  0.00033765818807296455\n",
            "Epoch  26 Batch  167 / 525  Training Loss  0.0005307890824042261\n",
            "Epoch  26 Batch  168 / 525  Training Loss  0.0005385797703638673\n",
            "Epoch  26 Batch  169 / 525  Training Loss  0.0007790844538249075\n",
            "Epoch  26 Batch  170 / 525  Training Loss  0.001115929102525115\n",
            "Epoch  26 Batch  171 / 525  Training Loss  0.0005283135105855763\n",
            "Epoch  26 Batch  172 / 525  Training Loss  0.0002696575247682631\n",
            "Epoch  26 Batch  173 / 525  Training Loss  0.00033722142688930035\n",
            "Epoch  26 Batch  174 / 525  Training Loss  0.00018076309061143547\n",
            "Epoch  26 Batch  175 / 525  Training Loss  0.00018354057101532817\n",
            "Epoch  26 Batch  176 / 525  Training Loss  0.0006967299268580973\n",
            "Epoch  26 Batch  177 / 525  Training Loss  0.00071867456426844\n",
            "Epoch  26 Batch  178 / 525  Training Loss  0.00016445128130726516\n",
            "Epoch  26 Batch  179 / 525  Training Loss  0.00027374509954825044\n",
            "Epoch  26 Batch  180 / 525  Training Loss  0.00021473996457643807\n",
            "Epoch  26 Batch  181 / 525  Training Loss  0.00043004349572584033\n",
            "Epoch  26 Batch  182 / 525  Training Loss  0.0004914470482617617\n",
            "Epoch  26 Batch  183 / 525  Training Loss  0.00023145193699747324\n",
            "Epoch  26 Batch  184 / 525  Training Loss  0.0004190246108919382\n",
            "Epoch  26 Batch  185 / 525  Training Loss  0.00037107020034454763\n",
            "Epoch  26 Batch  186 / 525  Training Loss  0.0010993733303621411\n",
            "Epoch  26 Batch  187 / 525  Training Loss  0.00017251790268346667\n",
            "Epoch  26 Batch  188 / 525  Training Loss  0.000322599895298481\n",
            "Epoch  26 Batch  189 / 525  Training Loss  0.0003319268871564418\n",
            "Epoch  26 Batch  190 / 525  Training Loss  0.00023241768940351903\n",
            "Epoch  26 Batch  191 / 525  Training Loss  0.0001579401723574847\n",
            "Epoch  26 Batch  192 / 525  Training Loss  0.0009035203838720918\n",
            "Epoch  26 Batch  193 / 525  Training Loss  0.000443616125266999\n",
            "Epoch  26 Batch  194 / 525  Training Loss  0.00018898652342613786\n",
            "Epoch  26 Batch  195 / 525  Training Loss  0.00031532556749880314\n",
            "Epoch  26 Batch  196 / 525  Training Loss  0.00032934537739492953\n",
            "Epoch  26 Batch  197 / 525  Training Loss  0.0003111588302999735\n",
            "Epoch  26 Batch  198 / 525  Training Loss  0.00017879575898405164\n",
            "Epoch  26 Batch  199 / 525  Training Loss  0.00045045014121569693\n",
            "Epoch  26 Batch  200 / 525  Training Loss  0.0011209087679162621\n",
            "Epoch  26 Batch  201 / 525  Training Loss  0.0003034434048458934\n",
            "Epoch  26 Batch  202 / 525  Training Loss  0.0001536001800559461\n",
            "Epoch  26 Batch  203 / 525  Training Loss  0.00022643886040896177\n",
            "Epoch  26 Batch  204 / 525  Training Loss  0.0002677805023267865\n",
            "Epoch  26 Batch  205 / 525  Training Loss  0.00018783859559334815\n",
            "Epoch  26 Batch  206 / 525  Training Loss  0.00018578847812023014\n",
            "Epoch  26 Batch  207 / 525  Training Loss  0.0004561505338642746\n",
            "Epoch  26 Batch  208 / 525  Training Loss  0.00015420239651575685\n",
            "Epoch  26 Batch  209 / 525  Training Loss  0.0002768790000118315\n",
            "Epoch  26 Batch  210 / 525  Training Loss  0.0002293323923368007\n",
            "Epoch  26 Batch  211 / 525  Training Loss  0.0004647705645766109\n",
            "Epoch  26 Batch  212 / 525  Training Loss  0.00011500577966216952\n",
            "Epoch  26 Batch  213 / 525  Training Loss  0.0004291663062758744\n",
            "Epoch  26 Batch  214 / 525  Training Loss  0.00020014014444313943\n",
            "Epoch  26 Batch  215 / 525  Training Loss  0.00017349231347907335\n",
            "Epoch  26 Batch  216 / 525  Training Loss  0.0004239687114022672\n",
            "Epoch  26 Batch  217 / 525  Training Loss  0.00011244880442973226\n",
            "Epoch  26 Batch  218 / 525  Training Loss  0.0004516831540968269\n",
            "Epoch  26 Batch  219 / 525  Training Loss  0.0010669988114386797\n",
            "Epoch  26 Batch  220 / 525  Training Loss  0.00020843562379013747\n",
            "Epoch  26 Batch  221 / 525  Training Loss  0.0004361081519164145\n",
            "Epoch  26 Batch  222 / 525  Training Loss  0.0005616502603515983\n",
            "Epoch  26 Batch  223 / 525  Training Loss  0.00022480885672848672\n",
            "Epoch  26 Batch  224 / 525  Training Loss  0.00017123777070082724\n",
            "Epoch  26 Batch  225 / 525  Training Loss  0.000327052955981344\n",
            "Epoch  26 Batch  226 / 525  Training Loss  0.000614768941886723\n",
            "Epoch  26 Batch  227 / 525  Training Loss  0.00017292483244091272\n",
            "Epoch  26 Batch  228 / 525  Training Loss  0.003061218187212944\n",
            "Epoch  26 Batch  229 / 525  Training Loss  0.00015357122174464166\n",
            "Epoch  26 Batch  230 / 525  Training Loss  0.00017959416436497122\n",
            "Epoch  26 Batch  231 / 525  Training Loss  0.00031523435609415174\n",
            "Epoch  26 Batch  232 / 525  Training Loss  7.282358274096623e-05\n",
            "Epoch  26 Batch  233 / 525  Training Loss  0.000683262653183192\n",
            "Epoch  26 Batch  234 / 525  Training Loss  0.0011185097973793745\n",
            "Epoch  26 Batch  235 / 525  Training Loss  0.0006184429512359202\n",
            "Epoch  26 Batch  236 / 525  Training Loss  0.0004034347366541624\n",
            "Epoch  26 Batch  237 / 525  Training Loss  0.0004387250228319317\n",
            "Epoch  26 Batch  238 / 525  Training Loss  0.00013433638378046453\n",
            "Epoch  26 Batch  239 / 525  Training Loss  0.00043469667434692383\n",
            "Epoch  26 Batch  240 / 525  Training Loss  0.0001566702121635899\n",
            "Epoch  26 Batch  241 / 525  Training Loss  0.0003468081122264266\n",
            "Epoch  26 Batch  242 / 525  Training Loss  0.0001463076041545719\n",
            "Epoch  26 Batch  243 / 525  Training Loss  0.0003370644408278167\n",
            "Epoch  26 Batch  244 / 525  Training Loss  0.0019235281506553292\n",
            "Epoch  26 Batch  245 / 525  Training Loss  0.0010470070410519838\n",
            "Epoch  26 Batch  246 / 525  Training Loss  0.00030117403366602957\n",
            "Epoch  26 Batch  247 / 525  Training Loss  0.00037937008892185986\n",
            "Epoch  26 Batch  248 / 525  Training Loss  0.00017525383736938238\n",
            "Epoch  26 Batch  249 / 525  Training Loss  0.0005967578035779297\n",
            "Epoch  26 Batch  250 / 525  Training Loss  0.00022494906443171203\n",
            "Epoch  26 Batch  251 / 525  Training Loss  0.0007000191253609955\n",
            "Epoch  26 Batch  252 / 525  Training Loss  0.00013360199227463454\n",
            "Epoch  26 Batch  253 / 525  Training Loss  0.0003754822537302971\n",
            "Epoch  26 Batch  254 / 525  Training Loss  0.000393245427403599\n",
            "Epoch  26 Batch  255 / 525  Training Loss  9.182724170386791e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  26 Batch  256 / 525  Training Loss  0.00020098229288123548\n",
            "Epoch  26 Batch  257 / 525  Training Loss  0.00020319342729635537\n",
            "Epoch  26 Batch  258 / 525  Training Loss  0.00031156899058260024\n",
            "Epoch  26 Batch  259 / 525  Training Loss  0.0005342753138393164\n",
            "Epoch  26 Batch  260 / 525  Training Loss  0.00019611825700849295\n",
            "Epoch  26 Batch  261 / 525  Training Loss  0.0004990568268112838\n",
            "Epoch  26 Batch  262 / 525  Training Loss  0.0002549542987253517\n",
            "Epoch  26 Batch  263 / 525  Training Loss  0.0004992145695723593\n",
            "Epoch  26 Batch  264 / 525  Training Loss  0.0001160702231572941\n",
            "Epoch  26 Batch  265 / 525  Training Loss  0.00016291174688376486\n",
            "Epoch  26 Batch  266 / 525  Training Loss  0.00016810547094792128\n",
            "Epoch  26 Batch  267 / 525  Training Loss  0.0001567745057400316\n",
            "Epoch  26 Batch  268 / 525  Training Loss  0.001394624705426395\n",
            "Epoch  26 Batch  269 / 525  Training Loss  0.0003435137332417071\n",
            "Epoch  26 Batch  270 / 525  Training Loss  0.00012673594756051898\n",
            "Epoch  26 Batch  271 / 525  Training Loss  0.00036925729364156723\n",
            "Epoch  26 Batch  272 / 525  Training Loss  0.0002659954479895532\n",
            "Epoch  26 Batch  273 / 525  Training Loss  0.0001775634882505983\n",
            "Epoch  26 Batch  274 / 525  Training Loss  0.00012964304187335074\n",
            "Epoch  26 Batch  275 / 525  Training Loss  0.00011231992539251223\n",
            "Epoch  26 Batch  276 / 525  Training Loss  9.020675497595221e-05\n",
            "Epoch  26 Batch  277 / 525  Training Loss  0.00022591455490328372\n",
            "Epoch  26 Batch  278 / 525  Training Loss  0.0003766245790757239\n",
            "Epoch  26 Batch  279 / 525  Training Loss  0.00010001305781770498\n",
            "Epoch  26 Batch  280 / 525  Training Loss  0.0008721865597181022\n",
            "Epoch  26 Batch  281 / 525  Training Loss  0.00036332730087451637\n",
            "Epoch  26 Batch  282 / 525  Training Loss  0.0004841660556849092\n",
            "Epoch  26 Batch  283 / 525  Training Loss  0.006445467472076416\n",
            "Epoch  26 Batch  284 / 525  Training Loss  0.0005273638525977731\n",
            "Epoch  26 Batch  285 / 525  Training Loss  0.00015738992078695446\n",
            "Epoch  26 Batch  286 / 525  Training Loss  0.00022851931862533092\n",
            "Epoch  26 Batch  287 / 525  Training Loss  0.0003193537995684892\n",
            "Epoch  26 Batch  288 / 525  Training Loss  0.0004621326515916735\n",
            "Epoch  26 Batch  289 / 525  Training Loss  0.00015008503396529704\n",
            "Epoch  26 Batch  290 / 525  Training Loss  0.00034300555125810206\n",
            "Epoch  26 Batch  291 / 525  Training Loss  0.0003864553291350603\n",
            "Epoch  26 Batch  292 / 525  Training Loss  0.00099642900750041\n",
            "Epoch  26 Batch  293 / 525  Training Loss  0.0001910133723868057\n",
            "Epoch  26 Batch  294 / 525  Training Loss  0.00039862003177404404\n",
            "Epoch  26 Batch  295 / 525  Training Loss  0.00019679186516441405\n",
            "Epoch  26 Batch  296 / 525  Training Loss  0.00040298973908647895\n",
            "Epoch  26 Batch  297 / 525  Training Loss  0.00026816560421139\n",
            "Epoch  26 Batch  298 / 525  Training Loss  0.0002940697013400495\n",
            "Epoch  26 Batch  299 / 525  Training Loss  0.00035077641950920224\n",
            "Epoch  26 Batch  300 / 525  Training Loss  0.009009500965476036\n",
            "Epoch  26 Batch  301 / 525  Training Loss  0.017304932698607445\n",
            "Epoch  26 Batch  302 / 525  Training Loss  0.004713250789791346\n",
            "Epoch  26 Batch  303 / 525  Training Loss  0.0028906818479299545\n",
            "Epoch  26 Batch  304 / 525  Training Loss  0.00044367293594405055\n",
            "Epoch  26 Batch  305 / 525  Training Loss  0.0005458443774841726\n",
            "Epoch  26 Batch  306 / 525  Training Loss  0.00032069478766061366\n",
            "Epoch  26 Batch  307 / 525  Training Loss  0.0004169365856796503\n",
            "Epoch  26 Batch  308 / 525  Training Loss  0.0005057296366430819\n",
            "Epoch  26 Batch  309 / 525  Training Loss  0.00033917574910447\n",
            "Epoch  26 Batch  310 / 525  Training Loss  0.001792708644643426\n",
            "Epoch  26 Batch  311 / 525  Training Loss  0.0008239069720730186\n",
            "Epoch  26 Batch  312 / 525  Training Loss  0.00032908309367485344\n",
            "Epoch  26 Batch  313 / 525  Training Loss  0.0003073494299314916\n",
            "Epoch  26 Batch  314 / 525  Training Loss  0.00019346026238054037\n",
            "Epoch  26 Batch  315 / 525  Training Loss  0.0008257734589278698\n",
            "Epoch  26 Batch  316 / 525  Training Loss  0.0004381505714263767\n",
            "Epoch  26 Batch  317 / 525  Training Loss  0.001351867918856442\n",
            "Epoch  26 Batch  318 / 525  Training Loss  0.0005327844992280006\n",
            "Epoch  26 Batch  319 / 525  Training Loss  0.000544507522135973\n",
            "Epoch  26 Batch  320 / 525  Training Loss  0.001110651413910091\n",
            "Epoch  26 Batch  321 / 525  Training Loss  0.001266251434572041\n",
            "Epoch  26 Batch  322 / 525  Training Loss  0.0003320183022879064\n",
            "Epoch  26 Batch  323 / 525  Training Loss  0.0005766107933595777\n",
            "Epoch  26 Batch  324 / 525  Training Loss  0.0005244390922598541\n",
            "Epoch  26 Batch  325 / 525  Training Loss  0.0002977976982947439\n",
            "Epoch  26 Batch  326 / 525  Training Loss  0.00014089918113313615\n",
            "Epoch  26 Batch  327 / 525  Training Loss  0.00017039140220731497\n",
            "Epoch  26 Batch  328 / 525  Training Loss  0.0009729843586683273\n",
            "Epoch  26 Batch  329 / 525  Training Loss  0.00024556455900892615\n",
            "Epoch  26 Batch  330 / 525  Training Loss  0.0012550251558423042\n",
            "Epoch  26 Batch  331 / 525  Training Loss  0.0003296454669907689\n",
            "Epoch  26 Batch  332 / 525  Training Loss  0.0003909890656359494\n",
            "Epoch  26 Batch  333 / 525  Training Loss  0.0004998293588869274\n",
            "Epoch  26 Batch  334 / 525  Training Loss  0.0007153633050620556\n",
            "Epoch  26 Batch  335 / 525  Training Loss  0.0006553605780936778\n",
            "Epoch  26 Batch  336 / 525  Training Loss  0.0004062794614583254\n",
            "Epoch  26 Batch  337 / 525  Training Loss  0.0002627287176437676\n",
            "Epoch  26 Batch  338 / 525  Training Loss  0.00022588021238334477\n",
            "Epoch  26 Batch  339 / 525  Training Loss  0.0002937751996796578\n",
            "Epoch  26 Batch  340 / 525  Training Loss  0.0012103740591555834\n",
            "Epoch  26 Batch  341 / 525  Training Loss  0.000315057928673923\n",
            "Epoch  26 Batch  342 / 525  Training Loss  0.000971303612459451\n",
            "Epoch  26 Batch  343 / 525  Training Loss  0.0009759847307577729\n",
            "Epoch  26 Batch  344 / 525  Training Loss  0.00016247435996774584\n",
            "Epoch  26 Batch  345 / 525  Training Loss  0.00019049279217142612\n",
            "Epoch  26 Batch  346 / 525  Training Loss  0.00015877097030170262\n",
            "Epoch  26 Batch  347 / 525  Training Loss  0.0001741101295920089\n",
            "Epoch  26 Batch  348 / 525  Training Loss  0.0004539288638625294\n",
            "Epoch  26 Batch  349 / 525  Training Loss  0.0003996815939899534\n",
            "Epoch  26 Batch  350 / 525  Training Loss  0.0019106159452348948\n",
            "Epoch  26 Batch  351 / 525  Training Loss  0.00040694489143788815\n",
            "Epoch  26 Batch  352 / 525  Training Loss  0.0006745816208422184\n",
            "Epoch  26 Batch  353 / 525  Training Loss  0.0004889386473223567\n",
            "Epoch  26 Batch  354 / 525  Training Loss  0.000489971658680588\n",
            "Epoch  26 Batch  355 / 525  Training Loss  0.0001740146108204499\n",
            "Epoch  26 Batch  356 / 525  Training Loss  0.00023881839297246188\n",
            "Epoch  26 Batch  357 / 525  Training Loss  0.0016958098858594894\n",
            "Epoch  26 Batch  358 / 525  Training Loss  0.0003141911292914301\n",
            "Epoch  26 Batch  359 / 525  Training Loss  0.0004859186301473528\n",
            "Epoch  26 Batch  360 / 525  Training Loss  0.0016725569730624557\n",
            "Epoch  26 Batch  361 / 525  Training Loss  0.00037033241824246943\n",
            "Epoch  26 Batch  362 / 525  Training Loss  0.00036695162998512387\n",
            "Epoch  26 Batch  363 / 525  Training Loss  0.0006973968120291829\n",
            "Epoch  26 Batch  364 / 525  Training Loss  0.0001626033626962453\n",
            "Epoch  26 Batch  365 / 525  Training Loss  0.0004681810678448528\n",
            "Epoch  26 Batch  366 / 525  Training Loss  0.00023747971863485873\n",
            "Epoch  26 Batch  367 / 525  Training Loss  0.00042925821617245674\n",
            "Epoch  26 Batch  368 / 525  Training Loss  0.0003003380843438208\n",
            "Epoch  26 Batch  369 / 525  Training Loss  0.0002864504640456289\n",
            "Epoch  26 Batch  370 / 525  Training Loss  0.0002561492729000747\n",
            "Epoch  26 Batch  371 / 525  Training Loss  0.00040040217572823167\n",
            "Epoch  26 Batch  372 / 525  Training Loss  0.0002733584842644632\n",
            "Epoch  26 Batch  373 / 525  Training Loss  0.0004844867216888815\n",
            "Epoch  26 Batch  374 / 525  Training Loss  0.0002966373576782644\n",
            "Epoch  26 Batch  375 / 525  Training Loss  0.0009795606601983309\n",
            "Epoch  26 Batch  376 / 525  Training Loss  0.0013356197159737349\n",
            "Epoch  26 Batch  377 / 525  Training Loss  0.0021055927500128746\n",
            "Epoch  26 Batch  378 / 525  Training Loss  0.00019707910541910678\n",
            "Epoch  26 Batch  379 / 525  Training Loss  0.0003099302120972425\n",
            "Epoch  26 Batch  380 / 525  Training Loss  0.0017097952077165246\n",
            "Epoch  26 Batch  381 / 525  Training Loss  0.00035306758945807815\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  26 Batch  382 / 525  Training Loss  0.00020647235214710236\n",
            "Epoch  26 Batch  383 / 525  Training Loss  0.00018347993318457156\n",
            "Epoch  26 Batch  384 / 525  Training Loss  0.0008836771594360471\n",
            "Epoch  26 Batch  385 / 525  Training Loss  0.00025478238239884377\n",
            "Epoch  26 Batch  386 / 525  Training Loss  0.0001834458380471915\n",
            "Epoch  26 Batch  387 / 525  Training Loss  0.00021245997049845755\n",
            "Epoch  26 Batch  388 / 525  Training Loss  0.00015586477820761502\n",
            "Epoch  26 Batch  389 / 525  Training Loss  0.00043028080835938454\n",
            "Epoch  26 Batch  390 / 525  Training Loss  0.0009202848887071013\n",
            "Epoch  26 Batch  391 / 525  Training Loss  0.0002101958671119064\n",
            "Epoch  26 Batch  392 / 525  Training Loss  0.00021817538072355092\n",
            "Epoch  26 Batch  393 / 525  Training Loss  0.0005265375366434455\n",
            "Epoch  26 Batch  394 / 525  Training Loss  0.0005164890317246318\n",
            "Epoch  26 Batch  395 / 525  Training Loss  0.00019004994828719646\n",
            "Epoch  26 Batch  396 / 525  Training Loss  0.00022044862271286547\n",
            "Epoch  26 Batch  397 / 525  Training Loss  0.00021390881738625467\n",
            "Epoch  26 Batch  398 / 525  Training Loss  0.00022279410040937364\n",
            "Epoch  26 Batch  399 / 525  Training Loss  0.0001343433395959437\n",
            "Epoch  26 Batch  400 / 525  Training Loss  0.00019792630337178707\n",
            "Epoch  26 Batch  401 / 525  Training Loss  0.00018104570335708559\n",
            "Epoch  26 Batch  402 / 525  Training Loss  0.00044064573012292385\n",
            "Epoch  26 Batch  403 / 525  Training Loss  0.0011152829974889755\n",
            "Epoch  26 Batch  404 / 525  Training Loss  0.002442652825266123\n",
            "Epoch  26 Batch  405 / 525  Training Loss  0.0003500508319120854\n",
            "Epoch  26 Batch  406 / 525  Training Loss  0.00023676171258557588\n",
            "Epoch  26 Batch  407 / 525  Training Loss  0.001879266113974154\n",
            "Epoch  26 Batch  408 / 525  Training Loss  0.0004936122568324208\n",
            "Epoch  26 Batch  409 / 525  Training Loss  0.0010757020208984613\n",
            "Epoch  26 Batch  410 / 525  Training Loss  0.0005638255970552564\n",
            "Epoch  26 Batch  411 / 525  Training Loss  0.0004350425733719021\n",
            "Epoch  26 Batch  412 / 525  Training Loss  0.0008261663606390357\n",
            "Epoch  26 Batch  413 / 525  Training Loss  0.0005840789526700974\n",
            "Epoch  26 Batch  414 / 525  Training Loss  0.000626930093858391\n",
            "Epoch  26 Batch  415 / 525  Training Loss  0.0005552737275138497\n",
            "Epoch  26 Batch  416 / 525  Training Loss  0.0005816320772282779\n",
            "Epoch  26 Batch  417 / 525  Training Loss  0.0030452334322035313\n",
            "Epoch  26 Batch  418 / 525  Training Loss  0.00040159226045943797\n",
            "Epoch  26 Batch  419 / 525  Training Loss  0.0002763523953035474\n",
            "Epoch  26 Batch  420 / 525  Training Loss  0.0004671941860578954\n",
            "Epoch  26 Batch  421 / 525  Training Loss  0.004405027721077204\n",
            "Epoch  26 Batch  422 / 525  Training Loss  0.0004015761078335345\n",
            "Epoch  26 Batch  423 / 525  Training Loss  0.00016565258556511253\n",
            "Epoch  26 Batch  424 / 525  Training Loss  0.00011397643538657576\n",
            "Epoch  26 Batch  425 / 525  Training Loss  0.0001216075470438227\n",
            "Epoch  26 Batch  426 / 525  Training Loss  0.00038536294596269727\n",
            "Epoch  26 Batch  427 / 525  Training Loss  0.00045590652734972537\n",
            "Epoch  26 Batch  428 / 525  Training Loss  0.00032805645605549216\n",
            "Epoch  26 Batch  429 / 525  Training Loss  0.0006136929732747376\n",
            "Epoch  26 Batch  430 / 525  Training Loss  0.0007015536539256573\n",
            "Epoch  26 Batch  431 / 525  Training Loss  0.004945087246596813\n",
            "Epoch  26 Batch  432 / 525  Training Loss  0.0010645551374182105\n",
            "Epoch  26 Batch  433 / 525  Training Loss  0.0009146635420620441\n",
            "Epoch  26 Batch  434 / 525  Training Loss  0.003633737564086914\n",
            "Epoch  26 Batch  435 / 525  Training Loss  0.002116014249622822\n",
            "Epoch  26 Batch  436 / 525  Training Loss  0.0006450858199968934\n",
            "Epoch  26 Batch  437 / 525  Training Loss  0.00044507120037451386\n",
            "Epoch  26 Batch  438 / 525  Training Loss  0.00028233774355612695\n",
            "Epoch  26 Batch  439 / 525  Training Loss  0.00021201274648774415\n",
            "Epoch  26 Batch  440 / 525  Training Loss  0.0003510435635689646\n",
            "Epoch  26 Batch  441 / 525  Training Loss  0.00020957998640369624\n",
            "Epoch  26 Batch  442 / 525  Training Loss  0.0007791162934154272\n",
            "Epoch  26 Batch  443 / 525  Training Loss  0.00019081275968346745\n",
            "Epoch  26 Batch  444 / 525  Training Loss  0.00043874597758986056\n",
            "Epoch  26 Batch  445 / 525  Training Loss  0.004488777834922075\n",
            "Epoch  26 Batch  446 / 525  Training Loss  0.0013291140785440803\n",
            "Epoch  26 Batch  447 / 525  Training Loss  0.0047379909083247185\n",
            "Epoch  26 Batch  448 / 525  Training Loss  0.0002962963772006333\n",
            "Epoch  26 Batch  449 / 525  Training Loss  0.0012408184120431542\n",
            "Epoch  26 Batch  450 / 525  Training Loss  0.002187524689361453\n",
            "Epoch  26 Batch  451 / 525  Training Loss  0.001145058311522007\n",
            "Epoch  26 Batch  452 / 525  Training Loss  0.0004901401698589325\n",
            "Epoch  26 Batch  453 / 525  Training Loss  0.000852700904943049\n",
            "Epoch  26 Batch  454 / 525  Training Loss  0.0009975254070013762\n",
            "Epoch  26 Batch  455 / 525  Training Loss  0.0005245818756520748\n",
            "Epoch  26 Batch  456 / 525  Training Loss  0.0016200881218537688\n",
            "Epoch  26 Batch  457 / 525  Training Loss  0.007106574717909098\n",
            "Epoch  26 Batch  458 / 525  Training Loss  0.0006246855482459068\n",
            "Epoch  26 Batch  459 / 525  Training Loss  0.009479470551013947\n",
            "Epoch  26 Batch  460 / 525  Training Loss  0.0010277328547090292\n",
            "Epoch  26 Batch  461 / 525  Training Loss  0.005215311888605356\n",
            "Epoch  26 Batch  462 / 525  Training Loss  0.0002571628720033914\n",
            "Epoch  26 Batch  463 / 525  Training Loss  0.0006536824512295425\n",
            "Epoch  26 Batch  464 / 525  Training Loss  0.0003918801376130432\n",
            "Epoch  26 Batch  465 / 525  Training Loss  0.0041496469639241695\n",
            "Epoch  26 Batch  466 / 525  Training Loss  0.002096275333315134\n",
            "Epoch  26 Batch  467 / 525  Training Loss  0.0031796395778656006\n",
            "Epoch  26 Batch  468 / 525  Training Loss  0.0005665150820277631\n",
            "Epoch  26 Batch  469 / 525  Training Loss  0.0028297859244048595\n",
            "Epoch  26 Batch  470 / 525  Training Loss  0.0008467758889310062\n",
            "Epoch  26 Batch  471 / 525  Training Loss  0.00043085627839900553\n",
            "Epoch  26 Batch  472 / 525  Training Loss  0.0010086718248203397\n",
            "Epoch  26 Batch  473 / 525  Training Loss  0.00033583404729142785\n",
            "Epoch  26 Batch  474 / 525  Training Loss  0.0005004177219234407\n",
            "Epoch  26 Batch  475 / 525  Training Loss  0.0007482994114980102\n",
            "Epoch  26 Batch  476 / 525  Training Loss  0.0003600308555178344\n",
            "Epoch  26 Batch  477 / 525  Training Loss  0.002035944489762187\n",
            "Epoch  26 Batch  478 / 525  Training Loss  0.000597412115894258\n",
            "Epoch  26 Batch  479 / 525  Training Loss  0.0003781188279390335\n",
            "Epoch  26 Batch  480 / 525  Training Loss  0.0008586177718825638\n",
            "Epoch  26 Batch  481 / 525  Training Loss  0.0004742697346955538\n",
            "Epoch  26 Batch  482 / 525  Training Loss  0.002069811336696148\n",
            "Epoch  26 Batch  483 / 525  Training Loss  0.0004287173505872488\n",
            "Epoch  26 Batch  484 / 525  Training Loss  0.00024795072386041284\n",
            "Epoch  26 Batch  485 / 525  Training Loss  0.0011841809609904885\n",
            "Epoch  26 Batch  486 / 525  Training Loss  0.0004382621846161783\n",
            "Epoch  26 Batch  487 / 525  Training Loss  0.0008789979619905353\n",
            "Epoch  26 Batch  488 / 525  Training Loss  0.00026307281223125756\n",
            "Epoch  26 Batch  489 / 525  Training Loss  0.00037398881977424026\n",
            "Epoch  26 Batch  490 / 525  Training Loss  0.001292042201384902\n",
            "Epoch  26 Batch  491 / 525  Training Loss  0.005024236626923084\n",
            "Epoch  26 Batch  492 / 525  Training Loss  0.00042537538683973253\n",
            "Epoch  26 Batch  493 / 525  Training Loss  0.0009280240046791732\n",
            "Epoch  26 Batch  494 / 525  Training Loss  0.00035282899625599384\n",
            "Epoch  26 Batch  495 / 525  Training Loss  0.0006823475123383105\n",
            "Epoch  26 Batch  496 / 525  Training Loss  0.0006557362503372133\n",
            "Epoch  26 Batch  497 / 525  Training Loss  0.0017511185724288225\n",
            "Epoch  26 Batch  498 / 525  Training Loss  0.004259125329554081\n",
            "Epoch  26 Batch  499 / 525  Training Loss  0.0011840069200843573\n",
            "Epoch  26 Batch  500 / 525  Training Loss  0.00473012262955308\n",
            "Epoch  26 Batch  501 / 525  Training Loss  0.00048218521988019347\n",
            "Epoch  26 Batch  502 / 525  Training Loss  0.0012326532742008567\n",
            "Epoch  26 Batch  503 / 525  Training Loss  0.003597792237997055\n",
            "Epoch  26 Batch  504 / 525  Training Loss  0.0005655821296386421\n",
            "Epoch  26 Batch  505 / 525  Training Loss  0.0007005721563473344\n",
            "Epoch  26 Batch  506 / 525  Training Loss  0.0014199699508026242\n",
            "Epoch  26 Batch  507 / 525  Training Loss  0.0006777675589546561\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  26 Batch  508 / 525  Training Loss  0.0005399580695666373\n",
            "Epoch  26 Batch  509 / 525  Training Loss  0.0005883304984308779\n",
            "Epoch  26 Batch  510 / 525  Training Loss  0.00048002804396674037\n",
            "Epoch  26 Batch  511 / 525  Training Loss  0.0011397319613024592\n",
            "Epoch  26 Batch  512 / 525  Training Loss  0.00039636302972212434\n",
            "Epoch  26 Batch  513 / 525  Training Loss  0.00036169958184473217\n",
            "Epoch  26 Batch  514 / 525  Training Loss  0.0004107655549887568\n",
            "Epoch  26 Batch  515 / 525  Training Loss  0.0014789651613682508\n",
            "Epoch  26 Batch  516 / 525  Training Loss  0.00039005535654723644\n",
            "Epoch  26 Batch  517 / 525  Training Loss  0.0009940483141690493\n",
            "Epoch  26 Batch  518 / 525  Training Loss  0.00021319571533240378\n",
            "Epoch  26 Batch  519 / 525  Training Loss  0.0004347572976257652\n",
            "Epoch  26 Batch  520 / 525  Training Loss  0.00023122038692235947\n",
            "Epoch  26 Batch  521 / 525  Training Loss  0.0003689986770041287\n",
            "Epoch  26 Batch  522 / 525  Training Loss  0.0007072319276630878\n",
            "Epoch  26 Batch  523 / 525  Training Loss  0.0006010854849591851\n",
            "Epoch  26 Batch  524 / 525  Training Loss  0.001620040275156498\n",
            "  27    |    -    |   0.000707   | 62.208333\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 27\n",
            "Epoch  27 Batch  0 / 525  Training Loss  0.0009673998574726284\n",
            "Epoch  27 Batch  1 / 525  Training Loss  0.00031648995354771614\n",
            "Epoch  27 Batch  2 / 525  Training Loss  0.0059895385056734085\n",
            "Epoch  27 Batch  3 / 525  Training Loss  0.013452487997710705\n",
            "Epoch  27 Batch  4 / 525  Training Loss  0.0023885820992290974\n",
            "Epoch  27 Batch  5 / 525  Training Loss  0.00013334539835341275\n",
            "Epoch  27 Batch  6 / 525  Training Loss  0.0006492370157502592\n",
            "Epoch  27 Batch  7 / 525  Training Loss  0.00031710267649032176\n",
            "Epoch  27 Batch  8 / 525  Training Loss  0.0005274013383314013\n",
            "Epoch  27 Batch  9 / 525  Training Loss  0.0002800285874400288\n",
            "Epoch  27 Batch  10 / 525  Training Loss  0.0003616558969952166\n",
            "Epoch  27 Batch  11 / 525  Training Loss  0.0003350360202603042\n",
            "Epoch  27 Batch  12 / 525  Training Loss  0.0003495973360259086\n",
            "Epoch  27 Batch  13 / 525  Training Loss  0.0002368084096815437\n",
            "Epoch  27 Batch  14 / 525  Training Loss  0.0004609997267834842\n",
            "Epoch  27 Batch  15 / 525  Training Loss  0.0023972077760845423\n",
            "Epoch  27 Batch  16 / 525  Training Loss  0.0017540209228172898\n",
            "Epoch  27 Batch  17 / 525  Training Loss  0.00042020605178549886\n",
            "Epoch  27 Batch  18 / 525  Training Loss  0.0002187663922086358\n",
            "Epoch  27 Batch  19 / 525  Training Loss  0.0005380513030104339\n",
            "Epoch  27 Batch  20 / 525  Training Loss  0.0003569665423128754\n",
            "Epoch  27 Batch  21 / 525  Training Loss  0.00032129415194503963\n",
            "Epoch  27 Batch  22 / 525  Training Loss  0.0005853933398611844\n",
            "Epoch  27 Batch  23 / 525  Training Loss  0.00016679911641404033\n",
            "Epoch  27 Batch  24 / 525  Training Loss  0.0003891310770995915\n",
            "Epoch  27 Batch  25 / 525  Training Loss  0.0002071075577987358\n",
            "Epoch  27 Batch  26 / 525  Training Loss  0.0004899719497188926\n",
            "Epoch  27 Batch  27 / 525  Training Loss  0.00021865955204702914\n",
            "Epoch  27 Batch  28 / 525  Training Loss  0.0006206543184816837\n",
            "Epoch  27 Batch  29 / 525  Training Loss  0.0037541636265814304\n",
            "Epoch  27 Batch  30 / 525  Training Loss  0.012555435299873352\n",
            "Epoch  27 Batch  31 / 525  Training Loss  0.003990317694842815\n",
            "Epoch  27 Batch  32 / 525  Training Loss  0.0007302445010282099\n",
            "Epoch  27 Batch  33 / 525  Training Loss  0.0005197174032218754\n",
            "Epoch  27 Batch  34 / 525  Training Loss  0.0010967199923470616\n",
            "Epoch  27 Batch  35 / 525  Training Loss  0.00044253640226088464\n",
            "Epoch  27 Batch  36 / 525  Training Loss  0.005170885939151049\n",
            "Epoch  27 Batch  37 / 525  Training Loss  0.00042798760114237666\n",
            "Epoch  27 Batch  38 / 525  Training Loss  0.0003010416985489428\n",
            "Epoch  27 Batch  39 / 525  Training Loss  0.0015537895960733294\n",
            "Epoch  27 Batch  40 / 525  Training Loss  0.0006546143558807671\n",
            "Epoch  27 Batch  41 / 525  Training Loss  0.0024803108535706997\n",
            "Epoch  27 Batch  42 / 525  Training Loss  0.0009138694149442017\n",
            "Epoch  27 Batch  43 / 525  Training Loss  0.003943110350519419\n",
            "Epoch  27 Batch  44 / 525  Training Loss  0.00751903373748064\n",
            "Epoch  27 Batch  45 / 525  Training Loss  0.0007617961382493377\n",
            "Epoch  27 Batch  46 / 525  Training Loss  0.0002448917075525969\n",
            "Epoch  27 Batch  47 / 525  Training Loss  0.00038837356260046363\n",
            "Epoch  27 Batch  48 / 525  Training Loss  0.000642080616671592\n",
            "Epoch  27 Batch  49 / 525  Training Loss  0.0002600574807729572\n",
            "Epoch  27 Batch  50 / 525  Training Loss  0.00046983532956801355\n",
            "Epoch  27 Batch  51 / 525  Training Loss  0.000904623419046402\n",
            "Epoch  27 Batch  52 / 525  Training Loss  0.0008550250204280019\n",
            "Epoch  27 Batch  53 / 525  Training Loss  0.00045356148621067405\n",
            "Epoch  27 Batch  54 / 525  Training Loss  0.000523122027516365\n",
            "Epoch  27 Batch  55 / 525  Training Loss  0.0007720751455053687\n",
            "Epoch  27 Batch  56 / 525  Training Loss  0.00032888323767110705\n",
            "Epoch  27 Batch  57 / 525  Training Loss  0.0007478886982426047\n",
            "Epoch  27 Batch  58 / 525  Training Loss  0.00013850709365215153\n",
            "Epoch  27 Batch  59 / 525  Training Loss  0.00043711307807825506\n",
            "Epoch  27 Batch  60 / 525  Training Loss  0.00017034901247825474\n",
            "Epoch  27 Batch  61 / 525  Training Loss  0.0012712504249066114\n",
            "Epoch  27 Batch  62 / 525  Training Loss  0.00026944014825858176\n",
            "Epoch  27 Batch  63 / 525  Training Loss  0.0003075843269471079\n",
            "Epoch  27 Batch  64 / 525  Training Loss  0.0003548080276232213\n",
            "Epoch  27 Batch  65 / 525  Training Loss  0.00045426638098433614\n",
            "Epoch  27 Batch  66 / 525  Training Loss  0.005213867872953415\n",
            "Epoch  27 Batch  67 / 525  Training Loss  0.0026383069343864918\n",
            "Epoch  27 Batch  68 / 525  Training Loss  0.00038598745595663786\n",
            "Epoch  27 Batch  69 / 525  Training Loss  0.02307080291211605\n",
            "Epoch  27 Batch  70 / 525  Training Loss  0.0035505215637385845\n",
            "Epoch  27 Batch  71 / 525  Training Loss  0.0016606980934739113\n",
            "Epoch  27 Batch  72 / 525  Training Loss  0.0003160670748911798\n",
            "Epoch  27 Batch  73 / 525  Training Loss  0.0002027359587373212\n",
            "Epoch  27 Batch  74 / 525  Training Loss  0.00020137280807830393\n",
            "Epoch  27 Batch  75 / 525  Training Loss  0.00027831451734527946\n",
            "Epoch  27 Batch  76 / 525  Training Loss  0.002103053266182542\n",
            "Epoch  27 Batch  77 / 525  Training Loss  0.0004266276955604553\n",
            "Epoch  27 Batch  78 / 525  Training Loss  0.001373225124552846\n",
            "Epoch  27 Batch  79 / 525  Training Loss  0.00038485374534502625\n",
            "Epoch  27 Batch  80 / 525  Training Loss  0.0003403582377359271\n",
            "Epoch  27 Batch  81 / 525  Training Loss  0.0009623781661503017\n",
            "Epoch  27 Batch  82 / 525  Training Loss  0.00039266556268557906\n",
            "Epoch  27 Batch  83 / 525  Training Loss  0.0003950962272938341\n",
            "Epoch  27 Batch  84 / 525  Training Loss  0.0008294354192912579\n",
            "Epoch  27 Batch  85 / 525  Training Loss  0.00012420622806530446\n",
            "Epoch  27 Batch  86 / 525  Training Loss  0.0030232914723455906\n",
            "Epoch  27 Batch  87 / 525  Training Loss  0.0003531551919877529\n",
            "Epoch  27 Batch  88 / 525  Training Loss  0.01286489050835371\n",
            "Epoch  27 Batch  89 / 525  Training Loss  0.0017157274996861815\n",
            "Epoch  27 Batch  90 / 525  Training Loss  0.0006066913483664393\n",
            "Epoch  27 Batch  91 / 525  Training Loss  0.0006971493130549788\n",
            "Epoch  27 Batch  92 / 525  Training Loss  0.0005436219507828355\n",
            "Epoch  27 Batch  93 / 525  Training Loss  0.001801790902391076\n",
            "Epoch  27 Batch  94 / 525  Training Loss  0.0016044499352574348\n",
            "Epoch  27 Batch  95 / 525  Training Loss  0.0003889698418788612\n",
            "Epoch  27 Batch  96 / 525  Training Loss  0.004003255628049374\n",
            "Epoch  27 Batch  97 / 525  Training Loss  0.0005927161546424031\n",
            "Epoch  27 Batch  98 / 525  Training Loss  0.006284862756729126\n",
            "Epoch  27 Batch  99 / 525  Training Loss  0.00201921327970922\n",
            "Epoch  27 Batch  100 / 525  Training Loss  0.00022129714488983154\n",
            "Epoch  27 Batch  101 / 525  Training Loss  0.00030047984910197556\n",
            "Epoch  27 Batch  102 / 525  Training Loss  0.0023760958574712276\n",
            "Epoch  27 Batch  103 / 525  Training Loss  0.0011269372189417481\n",
            "Epoch  27 Batch  104 / 525  Training Loss  0.003765986068174243\n",
            "Epoch  27 Batch  105 / 525  Training Loss  0.002371479058638215\n",
            "Epoch  27 Batch  106 / 525  Training Loss  0.0009214152814820409\n",
            "Epoch  27 Batch  107 / 525  Training Loss  0.0013383649056777358\n",
            "Epoch  27 Batch  108 / 525  Training Loss  0.001163235865533352\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  27 Batch  109 / 525  Training Loss  0.0006837851251475513\n",
            "Epoch  27 Batch  110 / 525  Training Loss  0.0036877181846648455\n",
            "Epoch  27 Batch  111 / 525  Training Loss  0.0009126829681918025\n",
            "Epoch  27 Batch  112 / 525  Training Loss  0.0022118669003248215\n",
            "Epoch  27 Batch  113 / 525  Training Loss  0.0006193327135406435\n",
            "Epoch  27 Batch  114 / 525  Training Loss  0.0004026867391075939\n",
            "Epoch  27 Batch  115 / 525  Training Loss  0.008765707723796368\n",
            "Epoch  27 Batch  116 / 525  Training Loss  0.0022030251566320658\n",
            "Epoch  27 Batch  117 / 525  Training Loss  0.0005496420781128109\n",
            "Epoch  27 Batch  118 / 525  Training Loss  0.000499837682582438\n",
            "Epoch  27 Batch  119 / 525  Training Loss  0.001255250652320683\n",
            "Epoch  27 Batch  120 / 525  Training Loss  0.00034666198189370334\n",
            "Epoch  27 Batch  121 / 525  Training Loss  0.00933626014739275\n",
            "Epoch  27 Batch  122 / 525  Training Loss  0.0005723420763388276\n",
            "Epoch  27 Batch  123 / 525  Training Loss  0.0006178611656650901\n",
            "Epoch  27 Batch  124 / 525  Training Loss  0.0014219169970601797\n",
            "Epoch  27 Batch  125 / 525  Training Loss  0.0039434111677110195\n",
            "Epoch  27 Batch  126 / 525  Training Loss  0.00042199407471343875\n",
            "Epoch  27 Batch  127 / 525  Training Loss  0.0029794678557664156\n",
            "Epoch  27 Batch  128 / 525  Training Loss  0.0008730845293030143\n",
            "Epoch  27 Batch  129 / 525  Training Loss  0.000710983294993639\n",
            "Epoch  27 Batch  130 / 525  Training Loss  0.0004705411847680807\n",
            "Epoch  27 Batch  131 / 525  Training Loss  0.00039268983528018\n",
            "Epoch  27 Batch  132 / 525  Training Loss  0.010522161610424519\n",
            "Epoch  27 Batch  133 / 525  Training Loss  0.00032733165426179767\n",
            "Epoch  27 Batch  134 / 525  Training Loss  0.0011673638364300132\n",
            "Epoch  27 Batch  135 / 525  Training Loss  0.007570971734821796\n",
            "Epoch  27 Batch  136 / 525  Training Loss  0.0012719209771603346\n",
            "Epoch  27 Batch  137 / 525  Training Loss  0.003138655796647072\n",
            "Epoch  27 Batch  138 / 525  Training Loss  0.0011880772653967142\n",
            "Epoch  27 Batch  139 / 525  Training Loss  0.01086939126253128\n",
            "Epoch  27 Batch  140 / 525  Training Loss  0.0007972918683663011\n",
            "Epoch  27 Batch  141 / 525  Training Loss  0.003386346623301506\n",
            "Epoch  27 Batch  142 / 525  Training Loss  0.0008412521565333009\n",
            "Epoch  27 Batch  143 / 525  Training Loss  0.0007481232169084251\n",
            "Epoch  27 Batch  144 / 525  Training Loss  0.0010887965327128768\n",
            "Epoch  27 Batch  145 / 525  Training Loss  0.0009490246884524822\n",
            "Epoch  27 Batch  146 / 525  Training Loss  0.0007774383993819356\n",
            "Epoch  27 Batch  147 / 525  Training Loss  0.000560371670871973\n",
            "Epoch  27 Batch  148 / 525  Training Loss  0.0004489932325668633\n",
            "Epoch  27 Batch  149 / 525  Training Loss  0.00043429690413177013\n",
            "Epoch  27 Batch  150 / 525  Training Loss  0.00025514891603961587\n",
            "Epoch  27 Batch  151 / 525  Training Loss  0.0005491649499163032\n",
            "Epoch  27 Batch  152 / 525  Training Loss  0.0008477185037918389\n",
            "Epoch  27 Batch  153 / 525  Training Loss  0.0021016257815063\n",
            "Epoch  27 Batch  154 / 525  Training Loss  0.001972384052351117\n",
            "Epoch  27 Batch  155 / 525  Training Loss  0.000660796882584691\n",
            "Epoch  27 Batch  156 / 525  Training Loss  0.0003586889652069658\n",
            "Epoch  27 Batch  157 / 525  Training Loss  0.00818767212331295\n",
            "Epoch  27 Batch  158 / 525  Training Loss  0.00566426757723093\n",
            "Epoch  27 Batch  159 / 525  Training Loss  0.0021315261255949736\n",
            "Epoch  27 Batch  160 / 525  Training Loss  0.0006386683089658618\n",
            "Epoch  27 Batch  161 / 525  Training Loss  0.0029975285287946463\n",
            "Epoch  27 Batch  162 / 525  Training Loss  0.0004994496703147888\n",
            "Epoch  27 Batch  163 / 525  Training Loss  0.0010738745331764221\n",
            "Epoch  27 Batch  164 / 525  Training Loss  0.0023689023219048977\n",
            "Epoch  27 Batch  165 / 525  Training Loss  0.0004643829888664186\n",
            "Epoch  27 Batch  166 / 525  Training Loss  0.0006161153432913125\n",
            "Epoch  27 Batch  167 / 525  Training Loss  0.0006706542917527258\n",
            "Epoch  27 Batch  168 / 525  Training Loss  0.00038159472751431167\n",
            "Epoch  27 Batch  169 / 525  Training Loss  0.00018892684602178633\n",
            "Epoch  27 Batch  170 / 525  Training Loss  0.0021024171728640795\n",
            "Epoch  27 Batch  171 / 525  Training Loss  0.005098688416182995\n",
            "Epoch  27 Batch  172 / 525  Training Loss  0.00043993553845211864\n",
            "Epoch  27 Batch  173 / 525  Training Loss  0.0002833185135386884\n",
            "Epoch  27 Batch  174 / 525  Training Loss  0.010554530657827854\n",
            "Epoch  27 Batch  175 / 525  Training Loss  0.000821622263174504\n",
            "Epoch  27 Batch  176 / 525  Training Loss  0.02305077202618122\n",
            "Epoch  27 Batch  177 / 525  Training Loss  0.0030097358394414186\n",
            "Epoch  27 Batch  178 / 525  Training Loss  0.0024185283109545708\n",
            "Epoch  27 Batch  179 / 525  Training Loss  0.0009385304292663932\n",
            "Epoch  27 Batch  180 / 525  Training Loss  0.0010765630286186934\n",
            "Epoch  27 Batch  181 / 525  Training Loss  0.010580317117273808\n",
            "Epoch  27 Batch  182 / 525  Training Loss  0.0016260877018794417\n",
            "Epoch  27 Batch  183 / 525  Training Loss  0.0005600700387731194\n",
            "Epoch  27 Batch  184 / 525  Training Loss  0.0007638629758730531\n",
            "Epoch  27 Batch  185 / 525  Training Loss  0.004569276235997677\n",
            "Epoch  27 Batch  186 / 525  Training Loss  0.0004603669513016939\n",
            "Epoch  27 Batch  187 / 525  Training Loss  0.003880593227222562\n",
            "Epoch  27 Batch  188 / 525  Training Loss  0.002643346320837736\n",
            "Epoch  27 Batch  189 / 525  Training Loss  0.0005812032613903284\n",
            "Epoch  27 Batch  190 / 525  Training Loss  0.0010326514020562172\n",
            "Epoch  27 Batch  191 / 525  Training Loss  0.0011988604674115777\n",
            "Epoch  27 Batch  192 / 525  Training Loss  0.0018095213454216719\n",
            "Epoch  27 Batch  193 / 525  Training Loss  0.0030874419026076794\n",
            "Epoch  27 Batch  194 / 525  Training Loss  0.0003544001665432006\n",
            "Epoch  27 Batch  195 / 525  Training Loss  0.0010109608992934227\n",
            "Epoch  27 Batch  196 / 525  Training Loss  0.0011065215803682804\n",
            "Epoch  27 Batch  197 / 525  Training Loss  0.0004170350730419159\n",
            "Epoch  27 Batch  198 / 525  Training Loss  0.0010790742235258222\n",
            "Epoch  27 Batch  199 / 525  Training Loss  0.00037629911093972623\n",
            "Epoch  27 Batch  200 / 525  Training Loss  0.0002813721657730639\n",
            "Epoch  27 Batch  201 / 525  Training Loss  0.0006268865545280278\n",
            "Epoch  27 Batch  202 / 525  Training Loss  0.0003647604607976973\n",
            "Epoch  27 Batch  203 / 525  Training Loss  0.00019279320258647203\n",
            "Epoch  27 Batch  204 / 525  Training Loss  0.0006815585074946284\n",
            "Epoch  27 Batch  205 / 525  Training Loss  0.0030842425767332315\n",
            "Epoch  27 Batch  206 / 525  Training Loss  0.008344635367393494\n",
            "Epoch  27 Batch  207 / 525  Training Loss  0.0029684938490390778\n",
            "Epoch  27 Batch  208 / 525  Training Loss  0.0012299156514927745\n",
            "Epoch  27 Batch  209 / 525  Training Loss  0.0029062493704259396\n",
            "Epoch  27 Batch  210 / 525  Training Loss  0.0027553862892091274\n",
            "Epoch  27 Batch  211 / 525  Training Loss  0.0013022616039961576\n",
            "Epoch  27 Batch  212 / 525  Training Loss  0.0011016905773431063\n",
            "Epoch  27 Batch  213 / 525  Training Loss  0.005171092692762613\n",
            "Epoch  27 Batch  214 / 525  Training Loss  0.000368951034033671\n",
            "Epoch  27 Batch  215 / 525  Training Loss  0.003680923953652382\n",
            "Epoch  27 Batch  216 / 525  Training Loss  0.0015322418184950948\n",
            "Epoch  27 Batch  217 / 525  Training Loss  0.00046562650823034346\n",
            "Epoch  27 Batch  218 / 525  Training Loss  0.0005718289176002145\n",
            "Epoch  27 Batch  219 / 525  Training Loss  0.001269741915166378\n",
            "Epoch  27 Batch  220 / 525  Training Loss  0.0012206112733110785\n",
            "Epoch  27 Batch  221 / 525  Training Loss  0.001074166502803564\n",
            "Epoch  27 Batch  222 / 525  Training Loss  0.0003917152062058449\n",
            "Epoch  27 Batch  223 / 525  Training Loss  0.0004242142313160002\n",
            "Epoch  27 Batch  224 / 525  Training Loss  0.000805606774520129\n",
            "Epoch  27 Batch  225 / 525  Training Loss  0.0003310336032882333\n",
            "Epoch  27 Batch  226 / 525  Training Loss  0.0005513459909707308\n",
            "Epoch  27 Batch  227 / 525  Training Loss  0.0029534385539591312\n",
            "Epoch  27 Batch  228 / 525  Training Loss  0.0016872957348823547\n",
            "Epoch  27 Batch  229 / 525  Training Loss  0.0005500604165717959\n",
            "Epoch  27 Batch  230 / 525  Training Loss  0.0008258519810624421\n",
            "Epoch  27 Batch  231 / 525  Training Loss  0.0003544155624695122\n",
            "Epoch  27 Batch  232 / 525  Training Loss  0.002228173892945051\n",
            "Epoch  27 Batch  233 / 525  Training Loss  0.00046368519542738795\n",
            "Epoch  27 Batch  234 / 525  Training Loss  0.0006689904257655144\n",
            "Epoch  27 Batch  235 / 525  Training Loss  0.006226127035915852\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  27 Batch  236 / 525  Training Loss  0.006219613831490278\n",
            "Epoch  27 Batch  237 / 525  Training Loss  0.001341992407105863\n",
            "Epoch  27 Batch  238 / 525  Training Loss  0.0005425466224551201\n",
            "Epoch  27 Batch  239 / 525  Training Loss  0.003972461447119713\n",
            "Epoch  27 Batch  240 / 525  Training Loss  0.00027517235139384866\n",
            "Epoch  27 Batch  241 / 525  Training Loss  0.0002597919665277004\n",
            "Epoch  27 Batch  242 / 525  Training Loss  0.0017012578900903463\n",
            "Epoch  27 Batch  243 / 525  Training Loss  0.0004235110245645046\n",
            "Epoch  27 Batch  244 / 525  Training Loss  0.0007338124560192227\n",
            "Epoch  27 Batch  245 / 525  Training Loss  0.003599588992074132\n",
            "Epoch  27 Batch  246 / 525  Training Loss  0.0003856463299598545\n",
            "Epoch  27 Batch  247 / 525  Training Loss  0.00031738606048747897\n",
            "Epoch  27 Batch  248 / 525  Training Loss  0.00034254533238708973\n",
            "Epoch  27 Batch  249 / 525  Training Loss  0.0007724073948338628\n",
            "Epoch  27 Batch  250 / 525  Training Loss  0.0006549217505380511\n",
            "Epoch  27 Batch  251 / 525  Training Loss  0.004448567982763052\n",
            "Epoch  27 Batch  252 / 525  Training Loss  0.0005920346593484282\n",
            "Epoch  27 Batch  253 / 525  Training Loss  0.0008624497568234801\n",
            "Epoch  27 Batch  254 / 525  Training Loss  0.0005806044209748507\n",
            "Epoch  27 Batch  255 / 525  Training Loss  0.0017805105308070779\n",
            "Epoch  27 Batch  256 / 525  Training Loss  0.00038816960295662284\n",
            "Epoch  27 Batch  257 / 525  Training Loss  0.00028261629631742835\n",
            "Epoch  27 Batch  258 / 525  Training Loss  0.0006988992099650204\n",
            "Epoch  27 Batch  259 / 525  Training Loss  0.0005128313205204904\n",
            "Epoch  27 Batch  260 / 525  Training Loss  0.0007268342887982726\n",
            "Epoch  27 Batch  261 / 525  Training Loss  0.0005477470112964511\n",
            "Epoch  27 Batch  262 / 525  Training Loss  0.0010378914885222912\n",
            "Epoch  27 Batch  263 / 525  Training Loss  0.0009425804018974304\n",
            "Epoch  27 Batch  264 / 525  Training Loss  0.0011259516468271613\n",
            "Epoch  27 Batch  265 / 525  Training Loss  0.00018548400839790702\n",
            "Epoch  27 Batch  266 / 525  Training Loss  0.001436450402252376\n",
            "Epoch  27 Batch  267 / 525  Training Loss  0.0010614506900310516\n",
            "Epoch  27 Batch  268 / 525  Training Loss  0.0013407499063760042\n",
            "Epoch  27 Batch  269 / 525  Training Loss  0.0009471004013903439\n",
            "Epoch  27 Batch  270 / 525  Training Loss  0.0011064773425459862\n",
            "Epoch  27 Batch  271 / 525  Training Loss  0.0008880832465365529\n",
            "Epoch  27 Batch  272 / 525  Training Loss  0.0003694085171446204\n",
            "Epoch  27 Batch  273 / 525  Training Loss  0.0008561682770960033\n",
            "Epoch  27 Batch  274 / 525  Training Loss  0.0008071617339737713\n",
            "Epoch  27 Batch  275 / 525  Training Loss  0.003980190027505159\n",
            "Epoch  27 Batch  276 / 525  Training Loss  0.00096089206635952\n",
            "Epoch  27 Batch  277 / 525  Training Loss  0.0003609138075262308\n",
            "Epoch  27 Batch  278 / 525  Training Loss  0.0007145201088860631\n",
            "Epoch  27 Batch  279 / 525  Training Loss  0.0005615156260319054\n",
            "Epoch  27 Batch  280 / 525  Training Loss  0.002339486498385668\n",
            "Epoch  27 Batch  281 / 525  Training Loss  0.001309624407440424\n",
            "Epoch  27 Batch  282 / 525  Training Loss  0.01213998906314373\n",
            "Epoch  27 Batch  283 / 525  Training Loss  0.0007149138255044818\n",
            "Epoch  27 Batch  284 / 525  Training Loss  0.0042557669803500175\n",
            "Epoch  27 Batch  285 / 525  Training Loss  0.001382520655170083\n",
            "Epoch  27 Batch  286 / 525  Training Loss  0.0023110501933842897\n",
            "Epoch  27 Batch  287 / 525  Training Loss  0.0007716717664152384\n",
            "Epoch  27 Batch  288 / 525  Training Loss  0.0007174074416980147\n",
            "Epoch  27 Batch  289 / 525  Training Loss  0.00044567082659341395\n",
            "Epoch  27 Batch  290 / 525  Training Loss  0.001538093201816082\n",
            "Epoch  27 Batch  291 / 525  Training Loss  0.0005862395628355443\n",
            "Epoch  27 Batch  292 / 525  Training Loss  0.001016952213831246\n",
            "Epoch  27 Batch  293 / 525  Training Loss  0.0018691476434469223\n",
            "Epoch  27 Batch  294 / 525  Training Loss  0.0018176657613366842\n",
            "Epoch  27 Batch  295 / 525  Training Loss  0.0004893095465376973\n",
            "Epoch  27 Batch  296 / 525  Training Loss  0.00026998177054338157\n",
            "Epoch  27 Batch  297 / 525  Training Loss  0.0007431822596117854\n",
            "Epoch  27 Batch  298 / 525  Training Loss  0.0071023330092430115\n",
            "Epoch  27 Batch  299 / 525  Training Loss  0.011135880835354328\n",
            "Epoch  27 Batch  300 / 525  Training Loss  0.002687876345589757\n",
            "Epoch  27 Batch  301 / 525  Training Loss  0.0011736584128811955\n",
            "Epoch  27 Batch  302 / 525  Training Loss  0.0009669896098785102\n",
            "Epoch  27 Batch  303 / 525  Training Loss  0.0029127434827387333\n",
            "Epoch  27 Batch  304 / 525  Training Loss  0.0028430018573999405\n",
            "Epoch  27 Batch  305 / 525  Training Loss  0.0029691108502447605\n",
            "Epoch  27 Batch  306 / 525  Training Loss  0.0013579550432041287\n",
            "Epoch  27 Batch  307 / 525  Training Loss  0.0072058276273310184\n",
            "Epoch  27 Batch  308 / 525  Training Loss  0.00036929702037014067\n",
            "Epoch  27 Batch  309 / 525  Training Loss  0.006841265596449375\n",
            "Epoch  27 Batch  310 / 525  Training Loss  0.0018168380483984947\n",
            "Epoch  27 Batch  311 / 525  Training Loss  0.0004794132546521723\n",
            "Epoch  27 Batch  312 / 525  Training Loss  0.0019530376885086298\n",
            "Epoch  27 Batch  313 / 525  Training Loss  0.005303769838064909\n",
            "Epoch  27 Batch  314 / 525  Training Loss  0.0026916363276541233\n",
            "Epoch  27 Batch  315 / 525  Training Loss  0.006232604384422302\n",
            "Epoch  27 Batch  316 / 525  Training Loss  0.015750635415315628\n",
            "Epoch  27 Batch  317 / 525  Training Loss  0.022145714610815048\n",
            "Epoch  27 Batch  318 / 525  Training Loss  0.005861214827746153\n",
            "Epoch  27 Batch  319 / 525  Training Loss  0.0023095128126442432\n",
            "Epoch  27 Batch  320 / 525  Training Loss  0.0017800426576286554\n",
            "Epoch  27 Batch  321 / 525  Training Loss  0.0011309648398309946\n",
            "Epoch  27 Batch  322 / 525  Training Loss  0.002027113689109683\n",
            "Epoch  27 Batch  323 / 525  Training Loss  0.001457607140764594\n",
            "Epoch  27 Batch  324 / 525  Training Loss  0.001877496368251741\n",
            "Epoch  27 Batch  325 / 525  Training Loss  0.0017923889681696892\n",
            "Epoch  27 Batch  326 / 525  Training Loss  0.0020021721720695496\n",
            "Epoch  27 Batch  327 / 525  Training Loss  0.0026677767746150494\n",
            "Epoch  27 Batch  328 / 525  Training Loss  0.0009239022620022297\n",
            "Epoch  27 Batch  329 / 525  Training Loss  0.0005831788294017315\n",
            "Epoch  27 Batch  330 / 525  Training Loss  0.0019290971104055643\n",
            "Epoch  27 Batch  331 / 525  Training Loss  0.004279232583940029\n",
            "Epoch  27 Batch  332 / 525  Training Loss  0.0008131681242957711\n",
            "Epoch  27 Batch  333 / 525  Training Loss  0.0005644595366902649\n",
            "Epoch  27 Batch  334 / 525  Training Loss  0.0012266271514818072\n",
            "Epoch  27 Batch  335 / 525  Training Loss  0.008761193603277206\n",
            "Epoch  27 Batch  336 / 525  Training Loss  0.002113450551405549\n",
            "Epoch  27 Batch  337 / 525  Training Loss  0.0017855858895927668\n",
            "Epoch  27 Batch  338 / 525  Training Loss  0.013855603523552418\n",
            "Epoch  27 Batch  339 / 525  Training Loss  0.018855322152376175\n",
            "Epoch  27 Batch  340 / 525  Training Loss  0.0006310836179181933\n",
            "Epoch  27 Batch  341 / 525  Training Loss  0.005037130322307348\n",
            "Epoch  27 Batch  342 / 525  Training Loss  0.0013488850090652704\n",
            "Epoch  27 Batch  343 / 525  Training Loss  0.00040373377851210535\n",
            "Epoch  27 Batch  344 / 525  Training Loss  0.010239159688353539\n",
            "Epoch  27 Batch  345 / 525  Training Loss  0.0022371183149516582\n",
            "Epoch  27 Batch  346 / 525  Training Loss  0.0028388972859829664\n",
            "Epoch  27 Batch  347 / 525  Training Loss  0.0006342818378470838\n",
            "Epoch  27 Batch  348 / 525  Training Loss  0.001425371621735394\n",
            "Epoch  27 Batch  349 / 525  Training Loss  0.0014867896679788828\n",
            "Epoch  27 Batch  350 / 525  Training Loss  0.00048331095604225993\n",
            "Epoch  27 Batch  351 / 525  Training Loss  0.0013778653228655457\n",
            "Epoch  27 Batch  352 / 525  Training Loss  0.0007180649554356933\n",
            "Epoch  27 Batch  353 / 525  Training Loss  0.0023441691882908344\n",
            "Epoch  27 Batch  354 / 525  Training Loss  0.0007409007521346211\n",
            "Epoch  27 Batch  355 / 525  Training Loss  0.0005006279097869992\n",
            "Epoch  27 Batch  356 / 525  Training Loss  0.008478542789816856\n",
            "Epoch  27 Batch  357 / 525  Training Loss  0.0004204415308777243\n",
            "Epoch  27 Batch  358 / 525  Training Loss  0.0007152886828407645\n",
            "Epoch  27 Batch  359 / 525  Training Loss  0.004987217020243406\n",
            "Epoch  27 Batch  360 / 525  Training Loss  0.0018515590345486999\n",
            "Epoch  27 Batch  361 / 525  Training Loss  0.0015107053332030773\n",
            "Epoch  27 Batch  362 / 525  Training Loss  0.0006290540914051235\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  27 Batch  363 / 525  Training Loss  0.003279276192188263\n",
            "Epoch  27 Batch  364 / 525  Training Loss  0.00023910329036880285\n",
            "Epoch  27 Batch  365 / 525  Training Loss  0.0018922478193417192\n",
            "Epoch  27 Batch  366 / 525  Training Loss  0.0019933287985622883\n",
            "Epoch  27 Batch  367 / 525  Training Loss  0.0009179837070405483\n",
            "Epoch  27 Batch  368 / 525  Training Loss  0.00040906277718022466\n",
            "Epoch  27 Batch  369 / 525  Training Loss  0.0009266125853173435\n",
            "Epoch  27 Batch  370 / 525  Training Loss  0.0037970237899571657\n",
            "Epoch  27 Batch  371 / 525  Training Loss  0.0010406689252704382\n",
            "Epoch  27 Batch  372 / 525  Training Loss  0.0005319989286363125\n",
            "Epoch  27 Batch  373 / 525  Training Loss  0.0005533931544050574\n",
            "Epoch  27 Batch  374 / 525  Training Loss  0.001830566325224936\n",
            "Epoch  27 Batch  375 / 525  Training Loss  0.001152954762801528\n",
            "Epoch  27 Batch  376 / 525  Training Loss  0.0008026751456782222\n",
            "Epoch  27 Batch  377 / 525  Training Loss  0.0002904918510466814\n",
            "Epoch  27 Batch  378 / 525  Training Loss  0.0025706652086228132\n",
            "Epoch  27 Batch  379 / 525  Training Loss  0.00155686866492033\n",
            "Epoch  27 Batch  380 / 525  Training Loss  0.001753716031089425\n",
            "Epoch  27 Batch  381 / 525  Training Loss  0.003475560573861003\n",
            "Epoch  27 Batch  382 / 525  Training Loss  0.001532669528387487\n",
            "Epoch  27 Batch  383 / 525  Training Loss  0.004058010410517454\n",
            "Epoch  27 Batch  384 / 525  Training Loss  0.0004076945479027927\n",
            "Epoch  27 Batch  385 / 525  Training Loss  0.002981901401653886\n",
            "Epoch  27 Batch  386 / 525  Training Loss  0.0022487451788038015\n",
            "Epoch  27 Batch  387 / 525  Training Loss  0.002212332095950842\n",
            "Epoch  27 Batch  388 / 525  Training Loss  0.0024621891789138317\n",
            "Epoch  27 Batch  389 / 525  Training Loss  0.0027782300021499395\n",
            "Epoch  27 Batch  390 / 525  Training Loss  0.0019987255800515413\n",
            "Epoch  27 Batch  391 / 525  Training Loss  0.002281291177496314\n",
            "Epoch  27 Batch  392 / 525  Training Loss  0.00041692875674925745\n",
            "Epoch  27 Batch  393 / 525  Training Loss  0.0016356194391846657\n",
            "Epoch  27 Batch  394 / 525  Training Loss  0.0013197124935686588\n",
            "Epoch  27 Batch  395 / 525  Training Loss  0.0018670459976419806\n",
            "Epoch  27 Batch  396 / 525  Training Loss  0.0009424385498277843\n",
            "Epoch  27 Batch  397 / 525  Training Loss  0.0009767755400389433\n",
            "Epoch  27 Batch  398 / 525  Training Loss  0.00058829749468714\n",
            "Epoch  27 Batch  399 / 525  Training Loss  0.00031098921317607164\n",
            "Epoch  27 Batch  400 / 525  Training Loss  0.0003558035532478243\n",
            "Epoch  27 Batch  401 / 525  Training Loss  0.001095870160497725\n",
            "Epoch  27 Batch  402 / 525  Training Loss  0.0010954415192827582\n",
            "Epoch  27 Batch  403 / 525  Training Loss  0.004580773413181305\n",
            "Epoch  27 Batch  404 / 525  Training Loss  0.0064352331683039665\n",
            "Epoch  27 Batch  405 / 525  Training Loss  0.000570260570384562\n",
            "Epoch  27 Batch  406 / 525  Training Loss  0.0019457768648862839\n",
            "Epoch  27 Batch  407 / 525  Training Loss  0.0007000921759754419\n",
            "Epoch  27 Batch  408 / 525  Training Loss  0.0005393208703026175\n",
            "Epoch  27 Batch  409 / 525  Training Loss  0.001101507106795907\n",
            "Epoch  27 Batch  410 / 525  Training Loss  0.00040589793934486806\n",
            "Epoch  27 Batch  411 / 525  Training Loss  0.001142585650086403\n",
            "Epoch  27 Batch  412 / 525  Training Loss  0.0011935281800106168\n",
            "Epoch  27 Batch  413 / 525  Training Loss  0.0010704065207391977\n",
            "Epoch  27 Batch  414 / 525  Training Loss  0.0016932606231421232\n",
            "Epoch  27 Batch  415 / 525  Training Loss  0.001611157669685781\n",
            "Epoch  27 Batch  416 / 525  Training Loss  0.0031752351205796003\n",
            "Epoch  27 Batch  417 / 525  Training Loss  0.0021208752878010273\n",
            "Epoch  27 Batch  418 / 525  Training Loss  0.0027639276813715696\n",
            "Epoch  27 Batch  419 / 525  Training Loss  0.0007009971886873245\n",
            "Epoch  27 Batch  420 / 525  Training Loss  0.0036056097596883774\n",
            "Epoch  27 Batch  421 / 525  Training Loss  0.0006032637320458889\n",
            "Epoch  27 Batch  422 / 525  Training Loss  0.002890010830014944\n",
            "Epoch  27 Batch  423 / 525  Training Loss  0.0013877691235393286\n",
            "Epoch  27 Batch  424 / 525  Training Loss  0.0008835710468702018\n",
            "Epoch  27 Batch  425 / 525  Training Loss  0.0011121750576421618\n",
            "Epoch  27 Batch  426 / 525  Training Loss  0.0024054476525634527\n",
            "Epoch  27 Batch  427 / 525  Training Loss  0.0014744107611477375\n",
            "Epoch  27 Batch  428 / 525  Training Loss  0.001538052107207477\n",
            "Epoch  27 Batch  429 / 525  Training Loss  0.003442430170252919\n",
            "Epoch  27 Batch  430 / 525  Training Loss  0.0012592925922945142\n",
            "Epoch  27 Batch  431 / 525  Training Loss  0.00019938734476454556\n",
            "Epoch  27 Batch  432 / 525  Training Loss  0.0017044481355696917\n",
            "Epoch  27 Batch  433 / 525  Training Loss  0.0006780653493478894\n",
            "Epoch  27 Batch  434 / 525  Training Loss  0.0016252808272838593\n",
            "Epoch  27 Batch  435 / 525  Training Loss  0.0012877900153398514\n",
            "Epoch  27 Batch  436 / 525  Training Loss  0.0007495279423892498\n",
            "Epoch  27 Batch  437 / 525  Training Loss  0.006139103788882494\n",
            "Epoch  27 Batch  438 / 525  Training Loss  0.0009351487387903035\n",
            "Epoch  27 Batch  439 / 525  Training Loss  0.0011508350726217031\n",
            "Epoch  27 Batch  440 / 525  Training Loss  0.0033227205276489258\n",
            "Epoch  27 Batch  441 / 525  Training Loss  0.0014250855892896652\n",
            "Epoch  27 Batch  442 / 525  Training Loss  0.0030067150946706533\n",
            "Epoch  27 Batch  443 / 525  Training Loss  0.0033420741092413664\n",
            "Epoch  27 Batch  444 / 525  Training Loss  0.0015229900600388646\n",
            "Epoch  27 Batch  445 / 525  Training Loss  0.001219789613969624\n",
            "Epoch  27 Batch  446 / 525  Training Loss  0.00130941323004663\n",
            "Epoch  27 Batch  447 / 525  Training Loss  0.0007118298090063035\n",
            "Epoch  27 Batch  448 / 525  Training Loss  0.0010200623655691743\n",
            "Epoch  27 Batch  449 / 525  Training Loss  0.0002766391262412071\n",
            "Epoch  27 Batch  450 / 525  Training Loss  0.0015440852148458362\n",
            "Epoch  27 Batch  451 / 525  Training Loss  0.00106769148260355\n",
            "Epoch  27 Batch  452 / 525  Training Loss  0.001468984060920775\n",
            "Epoch  27 Batch  453 / 525  Training Loss  0.002163920784369111\n",
            "Epoch  27 Batch  454 / 525  Training Loss  0.001022983924485743\n",
            "Epoch  27 Batch  455 / 525  Training Loss  0.0013620555400848389\n",
            "Epoch  27 Batch  456 / 525  Training Loss  0.000607330643106252\n",
            "Epoch  27 Batch  457 / 525  Training Loss  0.002972203306853771\n",
            "Epoch  27 Batch  458 / 525  Training Loss  0.000754509586840868\n",
            "Epoch  27 Batch  459 / 525  Training Loss  0.00040341069689020514\n",
            "Epoch  27 Batch  460 / 525  Training Loss  0.0015733552863821387\n",
            "Epoch  27 Batch  461 / 525  Training Loss  0.0012930998345836997\n",
            "Epoch  27 Batch  462 / 525  Training Loss  0.0011577691184356809\n",
            "Epoch  27 Batch  463 / 525  Training Loss  0.0017854254692792892\n",
            "Epoch  27 Batch  464 / 525  Training Loss  0.0013026274973526597\n",
            "Epoch  27 Batch  465 / 525  Training Loss  0.0015015027020126581\n",
            "Epoch  27 Batch  466 / 525  Training Loss  0.0007023050566203892\n",
            "Epoch  27 Batch  467 / 525  Training Loss  0.0008517041569575667\n",
            "Epoch  27 Batch  468 / 525  Training Loss  0.0014906462747603655\n",
            "Epoch  27 Batch  469 / 525  Training Loss  0.0006477865972556174\n",
            "Epoch  27 Batch  470 / 525  Training Loss  0.002391990041360259\n",
            "Epoch  27 Batch  471 / 525  Training Loss  0.0015273180324584246\n",
            "Epoch  27 Batch  472 / 525  Training Loss  0.0015649270499125123\n",
            "Epoch  27 Batch  473 / 525  Training Loss  0.0007298402488231659\n",
            "Epoch  27 Batch  474 / 525  Training Loss  0.001649844110943377\n",
            "Epoch  27 Batch  475 / 525  Training Loss  0.0007494139717891812\n",
            "Epoch  27 Batch  476 / 525  Training Loss  0.0004806961806025356\n",
            "Epoch  27 Batch  477 / 525  Training Loss  0.0004960852675139904\n",
            "Epoch  27 Batch  478 / 525  Training Loss  0.0014300600159913301\n",
            "Epoch  27 Batch  479 / 525  Training Loss  0.000966922496445477\n",
            "Epoch  27 Batch  480 / 525  Training Loss  0.000990695902146399\n",
            "Epoch  27 Batch  481 / 525  Training Loss  0.0006843107985332608\n",
            "Epoch  27 Batch  482 / 525  Training Loss  0.0005368400015868247\n",
            "Epoch  27 Batch  483 / 525  Training Loss  0.00252264691516757\n",
            "Epoch  27 Batch  484 / 525  Training Loss  0.0008252835832536221\n",
            "Epoch  27 Batch  485 / 525  Training Loss  0.000524957780726254\n",
            "Epoch  27 Batch  486 / 525  Training Loss  0.0010921843349933624\n",
            "Epoch  27 Batch  487 / 525  Training Loss  0.0005736112361773849\n",
            "Epoch  27 Batch  488 / 525  Training Loss  0.0014413956087082624\n",
            "Epoch  27 Batch  489 / 525  Training Loss  0.002787504345178604\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  27 Batch  490 / 525  Training Loss  0.002303431276232004\n",
            "Epoch  27 Batch  491 / 525  Training Loss  0.003083116840571165\n",
            "Epoch  27 Batch  492 / 525  Training Loss  0.00198673945851624\n",
            "Epoch  27 Batch  493 / 525  Training Loss  0.0004905525711365044\n",
            "Epoch  27 Batch  494 / 525  Training Loss  0.0013618749799206853\n",
            "Epoch  27 Batch  495 / 525  Training Loss  0.0006062713800929487\n",
            "Epoch  27 Batch  496 / 525  Training Loss  0.001106965122744441\n",
            "Epoch  27 Batch  497 / 525  Training Loss  0.00047413926222361624\n",
            "Epoch  27 Batch  498 / 525  Training Loss  0.0009065321646630764\n",
            "Epoch  27 Batch  499 / 525  Training Loss  0.00045943312579765916\n",
            "Epoch  27 Batch  500 / 525  Training Loss  0.0027124008629471064\n",
            "Epoch  27 Batch  501 / 525  Training Loss  0.0009173277067020535\n",
            "Epoch  27 Batch  502 / 525  Training Loss  0.0005505786975845695\n",
            "Epoch  27 Batch  503 / 525  Training Loss  0.0004818277375306934\n",
            "Epoch  27 Batch  504 / 525  Training Loss  0.01802987791597843\n",
            "Epoch  27 Batch  505 / 525  Training Loss  0.001335288630798459\n",
            "Epoch  27 Batch  506 / 525  Training Loss  0.0022604973055422306\n",
            "Epoch  27 Batch  507 / 525  Training Loss  0.0015587249072268605\n",
            "Epoch  27 Batch  508 / 525  Training Loss  0.0003944082709494978\n",
            "Epoch  27 Batch  509 / 525  Training Loss  0.0008001855458132923\n",
            "Epoch  27 Batch  510 / 525  Training Loss  0.003810704220086336\n",
            "Epoch  27 Batch  511 / 525  Training Loss  0.010845265351235867\n",
            "Epoch  27 Batch  512 / 525  Training Loss  0.0005324653466232121\n",
            "Epoch  27 Batch  513 / 525  Training Loss  0.0016912845894694328\n",
            "Epoch  27 Batch  514 / 525  Training Loss  0.0008539397385902703\n",
            "Epoch  27 Batch  515 / 525  Training Loss  0.000490182195790112\n",
            "Epoch  27 Batch  516 / 525  Training Loss  0.0017252679681405425\n",
            "Epoch  27 Batch  517 / 525  Training Loss  0.003885951591655612\n",
            "Epoch  27 Batch  518 / 525  Training Loss  0.002406173385679722\n",
            "Epoch  27 Batch  519 / 525  Training Loss  0.0003929840459022671\n",
            "Epoch  27 Batch  520 / 525  Training Loss  0.011810668744146824\n",
            "Epoch  27 Batch  521 / 525  Training Loss  0.0018056627595797181\n",
            "Epoch  27 Batch  522 / 525  Training Loss  0.0022666200529783964\n",
            "Epoch  27 Batch  523 / 525  Training Loss  0.001686971983872354\n",
            "Epoch  27 Batch  524 / 525  Training Loss  0.0005816395860165358\n",
            "  28    |    -    |   0.002017   | 60.666667\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 28\n",
            "Epoch  28 Batch  0 / 525  Training Loss  0.0009614304872229695\n",
            "Epoch  28 Batch  1 / 525  Training Loss  0.00034868254442699254\n",
            "Epoch  28 Batch  2 / 525  Training Loss  0.00038537790533155203\n",
            "Epoch  28 Batch  3 / 525  Training Loss  0.003852012101560831\n",
            "Epoch  28 Batch  4 / 525  Training Loss  0.0029328835662454367\n",
            "Epoch  28 Batch  5 / 525  Training Loss  0.008743759244680405\n",
            "Epoch  28 Batch  6 / 525  Training Loss  0.0006906394846737385\n",
            "Epoch  28 Batch  7 / 525  Training Loss  0.001322289346717298\n",
            "Epoch  28 Batch  8 / 525  Training Loss  0.001359076937660575\n",
            "Epoch  28 Batch  9 / 525  Training Loss  0.0029794846195727587\n",
            "Epoch  28 Batch  10 / 525  Training Loss  0.0002476081717759371\n",
            "Epoch  28 Batch  11 / 525  Training Loss  0.0007458004984073341\n",
            "Epoch  28 Batch  12 / 525  Training Loss  0.006398049183189869\n",
            "Epoch  28 Batch  13 / 525  Training Loss  0.0026370419654995203\n",
            "Epoch  28 Batch  14 / 525  Training Loss  0.0002927432651631534\n",
            "Epoch  28 Batch  15 / 525  Training Loss  0.0014168054331094027\n",
            "Epoch  28 Batch  16 / 525  Training Loss  0.0013700795825570822\n",
            "Epoch  28 Batch  17 / 525  Training Loss  0.0014370933640748262\n",
            "Epoch  28 Batch  18 / 525  Training Loss  0.00047537978389300406\n",
            "Epoch  28 Batch  19 / 525  Training Loss  0.0009393359650857747\n",
            "Epoch  28 Batch  20 / 525  Training Loss  0.0007781365420669317\n",
            "Epoch  28 Batch  21 / 525  Training Loss  0.004062078893184662\n",
            "Epoch  28 Batch  22 / 525  Training Loss  0.0017137654358521104\n",
            "Epoch  28 Batch  23 / 525  Training Loss  0.0009125106153078377\n",
            "Epoch  28 Batch  24 / 525  Training Loss  0.0009383743745274842\n",
            "Epoch  28 Batch  25 / 525  Training Loss  0.0024583160411566496\n",
            "Epoch  28 Batch  26 / 525  Training Loss  0.0006860021385364234\n",
            "Epoch  28 Batch  27 / 525  Training Loss  0.00035976042272523046\n",
            "Epoch  28 Batch  28 / 525  Training Loss  0.00048241004697047174\n",
            "Epoch  28 Batch  29 / 525  Training Loss  0.00027366512222215533\n",
            "Epoch  28 Batch  30 / 525  Training Loss  0.0014279901515692472\n",
            "Epoch  28 Batch  31 / 525  Training Loss  0.00013418174057733268\n",
            "Epoch  28 Batch  32 / 525  Training Loss  0.002162151038646698\n",
            "Epoch  28 Batch  33 / 525  Training Loss  0.00034419167786836624\n",
            "Epoch  28 Batch  34 / 525  Training Loss  0.00037020305171608925\n",
            "Epoch  28 Batch  35 / 525  Training Loss  0.0008504654979333282\n",
            "Epoch  28 Batch  36 / 525  Training Loss  0.00012906795018352568\n",
            "Epoch  28 Batch  37 / 525  Training Loss  0.00023930026509333402\n",
            "Epoch  28 Batch  38 / 525  Training Loss  0.00103707704693079\n",
            "Epoch  28 Batch  39 / 525  Training Loss  0.004038114100694656\n",
            "Epoch  28 Batch  40 / 525  Training Loss  0.0035780142061412334\n",
            "Epoch  28 Batch  41 / 525  Training Loss  0.0001323097967542708\n",
            "Epoch  28 Batch  42 / 525  Training Loss  0.0005045293946750462\n",
            "Epoch  28 Batch  43 / 525  Training Loss  0.0004063564701937139\n",
            "Epoch  28 Batch  44 / 525  Training Loss  0.0016791265225037932\n",
            "Epoch  28 Batch  45 / 525  Training Loss  0.00034407799830660224\n",
            "Epoch  28 Batch  46 / 525  Training Loss  0.0017208295175805688\n",
            "Epoch  28 Batch  47 / 525  Training Loss  0.0009229386341758072\n",
            "Epoch  28 Batch  48 / 525  Training Loss  0.0008460872923023999\n",
            "Epoch  28 Batch  49 / 525  Training Loss  0.0002686492807697505\n",
            "Epoch  28 Batch  50 / 525  Training Loss  0.0013616039650514722\n",
            "Epoch  28 Batch  51 / 525  Training Loss  0.0008083352586254478\n",
            "Epoch  28 Batch  52 / 525  Training Loss  0.0003641154326032847\n",
            "Epoch  28 Batch  53 / 525  Training Loss  0.0006828530458733439\n",
            "Epoch  28 Batch  54 / 525  Training Loss  0.0005333138979040086\n",
            "Epoch  28 Batch  55 / 525  Training Loss  0.0008197274291887879\n",
            "Epoch  28 Batch  56 / 525  Training Loss  0.00047656154492869973\n",
            "Epoch  28 Batch  57 / 525  Training Loss  0.00028829037910327315\n",
            "Epoch  28 Batch  58 / 525  Training Loss  0.0014199031284078956\n",
            "Epoch  28 Batch  59 / 525  Training Loss  0.00032076079514808953\n",
            "Epoch  28 Batch  60 / 525  Training Loss  0.00022960593923926353\n",
            "Epoch  28 Batch  61 / 525  Training Loss  0.0008026256109587848\n",
            "Epoch  28 Batch  62 / 525  Training Loss  0.006242059171199799\n",
            "Epoch  28 Batch  63 / 525  Training Loss  0.00043500252650119364\n",
            "Epoch  28 Batch  64 / 525  Training Loss  0.0002618589496705681\n",
            "Epoch  28 Batch  65 / 525  Training Loss  0.00021828147873748094\n",
            "Epoch  28 Batch  66 / 525  Training Loss  0.0010505546815693378\n",
            "Epoch  28 Batch  67 / 525  Training Loss  0.0004229192272759974\n",
            "Epoch  28 Batch  68 / 525  Training Loss  0.001111106714233756\n",
            "Epoch  28 Batch  69 / 525  Training Loss  0.000294906843919307\n",
            "Epoch  28 Batch  70 / 525  Training Loss  0.0006634847377426922\n",
            "Epoch  28 Batch  71 / 525  Training Loss  0.0032902006059885025\n",
            "Epoch  28 Batch  72 / 525  Training Loss  0.0010300712892785668\n",
            "Epoch  28 Batch  73 / 525  Training Loss  0.0004920444916933775\n",
            "Epoch  28 Batch  74 / 525  Training Loss  0.0005499246763065457\n",
            "Epoch  28 Batch  75 / 525  Training Loss  4.8261914344038814e-05\n",
            "Epoch  28 Batch  76 / 525  Training Loss  0.0016840845346450806\n",
            "Epoch  28 Batch  77 / 525  Training Loss  0.0009579219040460885\n",
            "Epoch  28 Batch  78 / 525  Training Loss  0.0012036102125421166\n",
            "Epoch  28 Batch  79 / 525  Training Loss  0.001119197579100728\n",
            "Epoch  28 Batch  80 / 525  Training Loss  0.00048203891492448747\n",
            "Epoch  28 Batch  81 / 525  Training Loss  0.0003295389178674668\n",
            "Epoch  28 Batch  82 / 525  Training Loss  0.0008683764608576894\n",
            "Epoch  28 Batch  83 / 525  Training Loss  0.0007913556764833629\n",
            "Epoch  28 Batch  84 / 525  Training Loss  0.0008116433164104819\n",
            "Epoch  28 Batch  85 / 525  Training Loss  0.0015175321605056524\n",
            "Epoch  28 Batch  86 / 525  Training Loss  0.000363416678737849\n",
            "Epoch  28 Batch  87 / 525  Training Loss  0.0003765965229831636\n",
            "Epoch  28 Batch  88 / 525  Training Loss  0.0012619721237570047\n",
            "Epoch  28 Batch  89 / 525  Training Loss  0.0010655883233994246\n",
            "Epoch  28 Batch  90 / 525  Training Loss  0.0005123639712110162\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  28 Batch  91 / 525  Training Loss  0.0001109516597352922\n",
            "Epoch  28 Batch  92 / 525  Training Loss  0.0008002513786777854\n",
            "Epoch  28 Batch  93 / 525  Training Loss  0.006206339690834284\n",
            "Epoch  28 Batch  94 / 525  Training Loss  0.0006907825008966029\n",
            "Epoch  28 Batch  95 / 525  Training Loss  0.0005693173734471202\n",
            "Epoch  28 Batch  96 / 525  Training Loss  0.0007293397793546319\n",
            "Epoch  28 Batch  97 / 525  Training Loss  0.004688127897679806\n",
            "Epoch  28 Batch  98 / 525  Training Loss  0.0005467558512464166\n",
            "Epoch  28 Batch  99 / 525  Training Loss  0.0003205096290912479\n",
            "Epoch  28 Batch  100 / 525  Training Loss  0.0010917139006778598\n",
            "Epoch  28 Batch  101 / 525  Training Loss  0.00042116836993955076\n",
            "Epoch  28 Batch  102 / 525  Training Loss  0.00013719996786676347\n",
            "Epoch  28 Batch  103 / 525  Training Loss  0.0021854524966329336\n",
            "Epoch  28 Batch  104 / 525  Training Loss  0.00024275193572975695\n",
            "Epoch  28 Batch  105 / 525  Training Loss  0.0003853075613733381\n",
            "Epoch  28 Batch  106 / 525  Training Loss  0.0006106538930907845\n",
            "Epoch  28 Batch  107 / 525  Training Loss  0.0008549115736968815\n",
            "Epoch  28 Batch  108 / 525  Training Loss  0.000345104665029794\n",
            "Epoch  28 Batch  109 / 525  Training Loss  0.00017531322373542935\n",
            "Epoch  28 Batch  110 / 525  Training Loss  0.00031048391247168183\n",
            "Epoch  28 Batch  111 / 525  Training Loss  0.0016430951654911041\n",
            "Epoch  28 Batch  112 / 525  Training Loss  0.0005268128006719053\n",
            "Epoch  28 Batch  113 / 525  Training Loss  0.00043681106762960553\n",
            "Epoch  28 Batch  114 / 525  Training Loss  0.000168015350936912\n",
            "Epoch  28 Batch  115 / 525  Training Loss  0.0013379581505432725\n",
            "Epoch  28 Batch  116 / 525  Training Loss  0.0006327559240162373\n",
            "Epoch  28 Batch  117 / 525  Training Loss  0.00027267486439086497\n",
            "Epoch  28 Batch  118 / 525  Training Loss  0.0004903700319118798\n",
            "Epoch  28 Batch  119 / 525  Training Loss  0.004361807834357023\n",
            "Epoch  28 Batch  120 / 525  Training Loss  0.00030370752210728824\n",
            "Epoch  28 Batch  121 / 525  Training Loss  0.001842426834627986\n",
            "Epoch  28 Batch  122 / 525  Training Loss  0.0003869813517667353\n",
            "Epoch  28 Batch  123 / 525  Training Loss  0.00047389199608005583\n",
            "Epoch  28 Batch  124 / 525  Training Loss  0.000621877028606832\n",
            "Epoch  28 Batch  125 / 525  Training Loss  0.0004835302534047514\n",
            "Epoch  28 Batch  126 / 525  Training Loss  0.0007966547273099422\n",
            "Epoch  28 Batch  127 / 525  Training Loss  0.000813288614153862\n",
            "Epoch  28 Batch  128 / 525  Training Loss  0.0007472028955817223\n",
            "Epoch  28 Batch  129 / 525  Training Loss  0.0006337683880701661\n",
            "Epoch  28 Batch  130 / 525  Training Loss  0.0009723788825795054\n",
            "Epoch  28 Batch  131 / 525  Training Loss  0.0006371148629114032\n",
            "Epoch  28 Batch  132 / 525  Training Loss  0.0006049207877367735\n",
            "Epoch  28 Batch  133 / 525  Training Loss  0.0011314254952594638\n",
            "Epoch  28 Batch  134 / 525  Training Loss  0.0006555465515702963\n",
            "Epoch  28 Batch  135 / 525  Training Loss  0.0005441549583338201\n",
            "Epoch  28 Batch  136 / 525  Training Loss  0.007508937269449234\n",
            "Epoch  28 Batch  137 / 525  Training Loss  0.0005125229363329709\n",
            "Epoch  28 Batch  138 / 525  Training Loss  0.00047560810344293714\n",
            "Epoch  28 Batch  139 / 525  Training Loss  0.0007234143558889627\n",
            "Epoch  28 Batch  140 / 525  Training Loss  0.0017455434426665306\n",
            "Epoch  28 Batch  141 / 525  Training Loss  0.0006347015732899308\n",
            "Epoch  28 Batch  142 / 525  Training Loss  0.0006633441662415862\n",
            "Epoch  28 Batch  143 / 525  Training Loss  0.0005142091540619731\n",
            "Epoch  28 Batch  144 / 525  Training Loss  0.0006546486401930451\n",
            "Epoch  28 Batch  145 / 525  Training Loss  0.0004372337425593287\n",
            "Epoch  28 Batch  146 / 525  Training Loss  0.0004117981006857008\n",
            "Epoch  28 Batch  147 / 525  Training Loss  0.0003227134293410927\n",
            "Epoch  28 Batch  148 / 525  Training Loss  0.0006941828760318458\n",
            "Epoch  28 Batch  149 / 525  Training Loss  0.0008315758896060288\n",
            "Epoch  28 Batch  150 / 525  Training Loss  0.00014466297579929233\n",
            "Epoch  28 Batch  151 / 525  Training Loss  0.0004317652783356607\n",
            "Epoch  28 Batch  152 / 525  Training Loss  0.00021655177988577634\n",
            "Epoch  28 Batch  153 / 525  Training Loss  0.000767610443290323\n",
            "Epoch  28 Batch  154 / 525  Training Loss  0.0005172968958504498\n",
            "Epoch  28 Batch  155 / 525  Training Loss  0.002063447143882513\n",
            "Epoch  28 Batch  156 / 525  Training Loss  7.110455044312403e-05\n",
            "Epoch  28 Batch  157 / 525  Training Loss  0.004312008619308472\n",
            "Epoch  28 Batch  158 / 525  Training Loss  0.000688049360178411\n",
            "Epoch  28 Batch  159 / 525  Training Loss  0.002052547875791788\n",
            "Epoch  28 Batch  160 / 525  Training Loss  0.0007923035882413387\n",
            "Epoch  28 Batch  161 / 525  Training Loss  0.004855792969465256\n",
            "Epoch  28 Batch  162 / 525  Training Loss  0.0013407800579443574\n",
            "Epoch  28 Batch  163 / 525  Training Loss  0.00048326406977139413\n",
            "Epoch  28 Batch  164 / 525  Training Loss  0.0003556548326741904\n",
            "Epoch  28 Batch  165 / 525  Training Loss  0.00015094569243956357\n",
            "Epoch  28 Batch  166 / 525  Training Loss  0.002771873027086258\n",
            "Epoch  28 Batch  167 / 525  Training Loss  0.006682754494249821\n",
            "Epoch  28 Batch  168 / 525  Training Loss  0.0028569353744387627\n",
            "Epoch  28 Batch  169 / 525  Training Loss  0.0019109087297692895\n",
            "Epoch  28 Batch  170 / 525  Training Loss  0.0011218294966965914\n",
            "Epoch  28 Batch  171 / 525  Training Loss  0.0020436968188732862\n",
            "Epoch  28 Batch  172 / 525  Training Loss  0.0006353865610435605\n",
            "Epoch  28 Batch  173 / 525  Training Loss  0.0004487958212848753\n",
            "Epoch  28 Batch  174 / 525  Training Loss  0.0006442222511395812\n",
            "Epoch  28 Batch  175 / 525  Training Loss  0.0021240804344415665\n",
            "Epoch  28 Batch  176 / 525  Training Loss  0.0001708872296148911\n",
            "Epoch  28 Batch  177 / 525  Training Loss  0.0002163117314921692\n",
            "Epoch  28 Batch  178 / 525  Training Loss  0.00048286194214597344\n",
            "Epoch  28 Batch  179 / 525  Training Loss  0.0006697199423797429\n",
            "Epoch  28 Batch  180 / 525  Training Loss  0.0007163485279306769\n",
            "Epoch  28 Batch  181 / 525  Training Loss  0.0012253232998773456\n",
            "Epoch  28 Batch  182 / 525  Training Loss  0.000346908054780215\n",
            "Epoch  28 Batch  183 / 525  Training Loss  0.00038087135180830956\n",
            "Epoch  28 Batch  184 / 525  Training Loss  0.0004935608594678342\n",
            "Epoch  28 Batch  185 / 525  Training Loss  0.0010623885318636894\n",
            "Epoch  28 Batch  186 / 525  Training Loss  0.0006224128883332014\n",
            "Epoch  28 Batch  187 / 525  Training Loss  0.0005218586302362382\n",
            "Epoch  28 Batch  188 / 525  Training Loss  0.00022897578310221434\n",
            "Epoch  28 Batch  189 / 525  Training Loss  0.0008268191595561802\n",
            "Epoch  28 Batch  190 / 525  Training Loss  0.000614920980297029\n",
            "Epoch  28 Batch  191 / 525  Training Loss  0.0002079199330182746\n",
            "Epoch  28 Batch  192 / 525  Training Loss  0.0050747510977089405\n",
            "Epoch  28 Batch  193 / 525  Training Loss  0.002973811235278845\n",
            "Epoch  28 Batch  194 / 525  Training Loss  0.0002211723185610026\n",
            "Epoch  28 Batch  195 / 525  Training Loss  0.0020508687011897564\n",
            "Epoch  28 Batch  196 / 525  Training Loss  0.0013411151012405753\n",
            "Epoch  28 Batch  197 / 525  Training Loss  0.00037076155422255397\n",
            "Epoch  28 Batch  198 / 525  Training Loss  0.00022640559473074973\n",
            "Epoch  28 Batch  199 / 525  Training Loss  0.0003122474008705467\n",
            "Epoch  28 Batch  200 / 525  Training Loss  0.0019082811195403337\n",
            "Epoch  28 Batch  201 / 525  Training Loss  0.0006744180573150516\n",
            "Epoch  28 Batch  202 / 525  Training Loss  0.0007849146495573223\n",
            "Epoch  28 Batch  203 / 525  Training Loss  0.0006738790543749928\n",
            "Epoch  28 Batch  204 / 525  Training Loss  0.0023150616325438023\n",
            "Epoch  28 Batch  205 / 525  Training Loss  0.0010555207263678312\n",
            "Epoch  28 Batch  206 / 525  Training Loss  0.0003675783227663487\n",
            "Epoch  28 Batch  207 / 525  Training Loss  0.0048817857168614864\n",
            "Epoch  28 Batch  208 / 525  Training Loss  0.0011513071367517114\n",
            "Epoch  28 Batch  209 / 525  Training Loss  0.0003160491760354489\n",
            "Epoch  28 Batch  210 / 525  Training Loss  0.00021790228493046016\n",
            "Epoch  28 Batch  211 / 525  Training Loss  0.0009692596504464746\n",
            "Epoch  28 Batch  212 / 525  Training Loss  0.0007027353276498616\n",
            "Epoch  28 Batch  213 / 525  Training Loss  0.012118342332541943\n",
            "Epoch  28 Batch  214 / 525  Training Loss  0.010924947448074818\n",
            "Epoch  28 Batch  215 / 525  Training Loss  0.004935147240757942\n",
            "Epoch  28 Batch  216 / 525  Training Loss  0.00029827095568180084\n",
            "Epoch  28 Batch  217 / 525  Training Loss  0.0011810254072770476\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  28 Batch  218 / 525  Training Loss  0.0008873088518157601\n",
            "Epoch  28 Batch  219 / 525  Training Loss  0.0012068971991539001\n",
            "Epoch  28 Batch  220 / 525  Training Loss  0.00034179037902504206\n",
            "Epoch  28 Batch  221 / 525  Training Loss  0.00029919558437541127\n",
            "Epoch  28 Batch  222 / 525  Training Loss  0.00024391195620410144\n",
            "Epoch  28 Batch  223 / 525  Training Loss  0.0003665878903120756\n",
            "Epoch  28 Batch  224 / 525  Training Loss  0.004156153183430433\n",
            "Epoch  28 Batch  225 / 525  Training Loss  0.0003113073471467942\n",
            "Epoch  28 Batch  226 / 525  Training Loss  0.0005278330063447356\n",
            "Epoch  28 Batch  227 / 525  Training Loss  0.008352356031537056\n",
            "Epoch  28 Batch  228 / 525  Training Loss  0.0008331408025696874\n",
            "Epoch  28 Batch  229 / 525  Training Loss  0.0030171989928931\n",
            "Epoch  28 Batch  230 / 525  Training Loss  0.0009540800238028169\n",
            "Epoch  28 Batch  231 / 525  Training Loss  0.0004963589599356055\n",
            "Epoch  28 Batch  232 / 525  Training Loss  0.00041301417513750494\n",
            "Epoch  28 Batch  233 / 525  Training Loss  0.0004810805548913777\n",
            "Epoch  28 Batch  234 / 525  Training Loss  0.0012216120958328247\n",
            "Epoch  28 Batch  235 / 525  Training Loss  0.0006132376729510725\n",
            "Epoch  28 Batch  236 / 525  Training Loss  0.001144444802775979\n",
            "Epoch  28 Batch  237 / 525  Training Loss  0.0008568072807975113\n",
            "Epoch  28 Batch  238 / 525  Training Loss  9.331164619652554e-05\n",
            "Epoch  28 Batch  239 / 525  Training Loss  0.0008215253474190831\n",
            "Epoch  28 Batch  240 / 525  Training Loss  0.00025837033172138035\n",
            "Epoch  28 Batch  241 / 525  Training Loss  0.001757748075760901\n",
            "Epoch  28 Batch  242 / 525  Training Loss  0.0007233956712298095\n",
            "Epoch  28 Batch  243 / 525  Training Loss  0.001805573352612555\n",
            "Epoch  28 Batch  244 / 525  Training Loss  0.0008401767117902637\n",
            "Epoch  28 Batch  245 / 525  Training Loss  0.0021238706540316343\n",
            "Epoch  28 Batch  246 / 525  Training Loss  0.0005559945129789412\n",
            "Epoch  28 Batch  247 / 525  Training Loss  0.0029308770317584276\n",
            "Epoch  28 Batch  248 / 525  Training Loss  0.0011747798416763544\n",
            "Epoch  28 Batch  249 / 525  Training Loss  0.0014140218263491988\n",
            "Epoch  28 Batch  250 / 525  Training Loss  0.00018321959942113608\n",
            "Epoch  28 Batch  251 / 525  Training Loss  0.0002861307584680617\n",
            "Epoch  28 Batch  252 / 525  Training Loss  0.0008023487171158195\n",
            "Epoch  28 Batch  253 / 525  Training Loss  0.0004857744206674397\n",
            "Epoch  28 Batch  254 / 525  Training Loss  0.0007476365426555276\n",
            "Epoch  28 Batch  255 / 525  Training Loss  0.0010220477124676108\n",
            "Epoch  28 Batch  256 / 525  Training Loss  0.0007703721639700234\n",
            "Epoch  28 Batch  257 / 525  Training Loss  0.001459894934669137\n",
            "Epoch  28 Batch  258 / 525  Training Loss  0.0003370339691173285\n",
            "Epoch  28 Batch  259 / 525  Training Loss  0.0008483176934532821\n",
            "Epoch  28 Batch  260 / 525  Training Loss  0.00037966424133628607\n",
            "Epoch  28 Batch  261 / 525  Training Loss  0.0010699860285967588\n",
            "Epoch  28 Batch  262 / 525  Training Loss  0.0023099780082702637\n",
            "Epoch  28 Batch  263 / 525  Training Loss  0.0006334333447739482\n",
            "Epoch  28 Batch  264 / 525  Training Loss  0.0022528418339788914\n",
            "Epoch  28 Batch  265 / 525  Training Loss  0.014170950278639793\n",
            "Epoch  28 Batch  266 / 525  Training Loss  0.000712714740075171\n",
            "Epoch  28 Batch  267 / 525  Training Loss  0.004285688046365976\n",
            "Epoch  28 Batch  268 / 525  Training Loss  0.0005647367797791958\n",
            "Epoch  28 Batch  269 / 525  Training Loss  0.002936410252004862\n",
            "Epoch  28 Batch  270 / 525  Training Loss  0.0005405122064985335\n",
            "Epoch  28 Batch  271 / 525  Training Loss  0.00038718944415450096\n",
            "Epoch  28 Batch  272 / 525  Training Loss  0.0003050101804547012\n",
            "Epoch  28 Batch  273 / 525  Training Loss  0.00258924113586545\n",
            "Epoch  28 Batch  274 / 525  Training Loss  0.001530424109660089\n",
            "Epoch  28 Batch  275 / 525  Training Loss  0.00038656656397506595\n",
            "Epoch  28 Batch  276 / 525  Training Loss  0.00032038771314546466\n",
            "Epoch  28 Batch  277 / 525  Training Loss  0.004319401457905769\n",
            "Epoch  28 Batch  278 / 525  Training Loss  0.000374168943380937\n",
            "Epoch  28 Batch  279 / 525  Training Loss  0.0008932821219787002\n",
            "Epoch  28 Batch  280 / 525  Training Loss  0.004900881554931402\n",
            "Epoch  28 Batch  281 / 525  Training Loss  0.0004305308684706688\n",
            "Epoch  28 Batch  282 / 525  Training Loss  0.0008882149122655392\n",
            "Epoch  28 Batch  283 / 525  Training Loss  0.0006715563940815628\n",
            "Epoch  28 Batch  284 / 525  Training Loss  0.0009038623538799584\n",
            "Epoch  28 Batch  285 / 525  Training Loss  0.0032518822699785233\n",
            "Epoch  28 Batch  286 / 525  Training Loss  0.0005218824953772128\n",
            "Epoch  28 Batch  287 / 525  Training Loss  0.0007168461452238262\n",
            "Epoch  28 Batch  288 / 525  Training Loss  0.00035347515949979424\n",
            "Epoch  28 Batch  289 / 525  Training Loss  0.00040805726894177496\n",
            "Epoch  28 Batch  290 / 525  Training Loss  0.001054365886375308\n",
            "Epoch  28 Batch  291 / 525  Training Loss  0.00043594217277131975\n",
            "Epoch  28 Batch  292 / 525  Training Loss  0.0005003690021112561\n",
            "Epoch  28 Batch  293 / 525  Training Loss  0.0030568134970963\n",
            "Epoch  28 Batch  294 / 525  Training Loss  0.0012202683137729764\n",
            "Epoch  28 Batch  295 / 525  Training Loss  0.0007590617751702666\n",
            "Epoch  28 Batch  296 / 525  Training Loss  0.0008533017826266587\n",
            "Epoch  28 Batch  297 / 525  Training Loss  0.000505009142216295\n",
            "Epoch  28 Batch  298 / 525  Training Loss  0.002929318230599165\n",
            "Epoch  28 Batch  299 / 525  Training Loss  0.002846310380846262\n",
            "Epoch  28 Batch  300 / 525  Training Loss  0.0002622134634293616\n",
            "Epoch  28 Batch  301 / 525  Training Loss  0.00015047192573547363\n",
            "Epoch  28 Batch  302 / 525  Training Loss  0.006696105003356934\n",
            "Epoch  28 Batch  303 / 525  Training Loss  0.0009222500957548618\n",
            "Epoch  28 Batch  304 / 525  Training Loss  0.001976673025637865\n",
            "Epoch  28 Batch  305 / 525  Training Loss  0.00033003746648319066\n",
            "Epoch  28 Batch  306 / 525  Training Loss  0.0009753878111951053\n",
            "Epoch  28 Batch  307 / 525  Training Loss  0.004321856424212456\n",
            "Epoch  28 Batch  308 / 525  Training Loss  0.0027731754817068577\n",
            "Epoch  28 Batch  309 / 525  Training Loss  0.0011014218907803297\n",
            "Epoch  28 Batch  310 / 525  Training Loss  0.0017385348910465837\n",
            "Epoch  28 Batch  311 / 525  Training Loss  0.0022965320385992527\n",
            "Epoch  28 Batch  312 / 525  Training Loss  0.001109359203837812\n",
            "Epoch  28 Batch  313 / 525  Training Loss  0.001879317918792367\n",
            "Epoch  28 Batch  314 / 525  Training Loss  0.0017736289883032441\n",
            "Epoch  28 Batch  315 / 525  Training Loss  0.00028054669382981956\n",
            "Epoch  28 Batch  316 / 525  Training Loss  0.000547275529243052\n",
            "Epoch  28 Batch  317 / 525  Training Loss  0.004898683167994022\n",
            "Epoch  28 Batch  318 / 525  Training Loss  0.0002464965800754726\n",
            "Epoch  28 Batch  319 / 525  Training Loss  0.0009169926051981747\n",
            "Epoch  28 Batch  320 / 525  Training Loss  0.007760754786431789\n",
            "Epoch  28 Batch  321 / 525  Training Loss  0.011186101473867893\n",
            "Epoch  28 Batch  322 / 525  Training Loss  0.0012490787776187062\n",
            "Epoch  28 Batch  323 / 525  Training Loss  0.002887009410187602\n",
            "Epoch  28 Batch  324 / 525  Training Loss  0.0005940920673310757\n",
            "Epoch  28 Batch  325 / 525  Training Loss  0.0004373053670860827\n",
            "Epoch  28 Batch  326 / 525  Training Loss  0.0020596948452293873\n",
            "Epoch  28 Batch  327 / 525  Training Loss  0.002649901667609811\n",
            "Epoch  28 Batch  328 / 525  Training Loss  0.0008945867302827537\n",
            "Epoch  28 Batch  329 / 525  Training Loss  0.005000975448638201\n",
            "Epoch  28 Batch  330 / 525  Training Loss  0.0006466934573836625\n",
            "Epoch  28 Batch  331 / 525  Training Loss  0.0017897107172757387\n",
            "Epoch  28 Batch  332 / 525  Training Loss  0.001993119250983\n",
            "Epoch  28 Batch  333 / 525  Training Loss  0.0029426035471260548\n",
            "Epoch  28 Batch  334 / 525  Training Loss  0.00332977925427258\n",
            "Epoch  28 Batch  335 / 525  Training Loss  0.000649198773317039\n",
            "Epoch  28 Batch  336 / 525  Training Loss  0.0003708043077494949\n",
            "Epoch  28 Batch  337 / 525  Training Loss  0.005906874779611826\n",
            "Epoch  28 Batch  338 / 525  Training Loss  0.0018460830906406045\n",
            "Epoch  28 Batch  339 / 525  Training Loss  0.0009692857856862247\n",
            "Epoch  28 Batch  340 / 525  Training Loss  0.0012427011970430613\n",
            "Epoch  28 Batch  341 / 525  Training Loss  0.005161548964679241\n",
            "Epoch  28 Batch  342 / 525  Training Loss  0.0005164345493540168\n",
            "Epoch  28 Batch  343 / 525  Training Loss  0.003406139789149165\n",
            "Epoch  28 Batch  344 / 525  Training Loss  0.0010649256873875856\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  28 Batch  345 / 525  Training Loss  0.0007615112699568272\n",
            "Epoch  28 Batch  346 / 525  Training Loss  0.001146039692685008\n",
            "Epoch  28 Batch  347 / 525  Training Loss  0.0001801301259547472\n",
            "Epoch  28 Batch  348 / 525  Training Loss  0.0003859858261421323\n",
            "Epoch  28 Batch  349 / 525  Training Loss  0.0028141001239418983\n",
            "Epoch  28 Batch  350 / 525  Training Loss  0.003559274133294821\n",
            "Epoch  28 Batch  351 / 525  Training Loss  0.00021039016428403556\n",
            "Epoch  28 Batch  352 / 525  Training Loss  0.0026219512801617384\n",
            "Epoch  28 Batch  353 / 525  Training Loss  0.0003511506656650454\n",
            "Epoch  28 Batch  354 / 525  Training Loss  0.0007009577238932252\n",
            "Epoch  28 Batch  355 / 525  Training Loss  0.000408475985750556\n",
            "Epoch  28 Batch  356 / 525  Training Loss  0.0006862028385512531\n",
            "Epoch  28 Batch  357 / 525  Training Loss  0.00043719340465031564\n",
            "Epoch  28 Batch  358 / 525  Training Loss  0.0008573203231208026\n",
            "Epoch  28 Batch  359 / 525  Training Loss  0.0011392600135877728\n",
            "Epoch  28 Batch  360 / 525  Training Loss  0.0012912102974951267\n",
            "Epoch  28 Batch  361 / 525  Training Loss  0.0005978106637485325\n",
            "Epoch  28 Batch  362 / 525  Training Loss  0.00046846611076034606\n",
            "Epoch  28 Batch  363 / 525  Training Loss  0.0009796514641493559\n",
            "Epoch  28 Batch  364 / 525  Training Loss  0.0006487321224994957\n",
            "Epoch  28 Batch  365 / 525  Training Loss  0.0008738672477193177\n",
            "Epoch  28 Batch  366 / 525  Training Loss  0.0006215596804395318\n",
            "Epoch  28 Batch  367 / 525  Training Loss  0.00035925148404203355\n",
            "Epoch  28 Batch  368 / 525  Training Loss  0.0007263389998115599\n",
            "Epoch  28 Batch  369 / 525  Training Loss  0.0012597200693562627\n",
            "Epoch  28 Batch  370 / 525  Training Loss  0.0009342326084151864\n",
            "Epoch  28 Batch  371 / 525  Training Loss  0.000294585304800421\n",
            "Epoch  28 Batch  372 / 525  Training Loss  0.00043111262493766844\n",
            "Epoch  28 Batch  373 / 525  Training Loss  0.004813390783965588\n",
            "Epoch  28 Batch  374 / 525  Training Loss  0.0006900610169395804\n",
            "Epoch  28 Batch  375 / 525  Training Loss  0.0006101873004809022\n",
            "Epoch  28 Batch  376 / 525  Training Loss  0.00034444269840605557\n",
            "Epoch  28 Batch  377 / 525  Training Loss  0.0009440623107366264\n",
            "Epoch  28 Batch  378 / 525  Training Loss  0.00039842756814323366\n",
            "Epoch  28 Batch  379 / 525  Training Loss  0.0009123504278250039\n",
            "Epoch  28 Batch  380 / 525  Training Loss  0.0012179628247395158\n",
            "Epoch  28 Batch  381 / 525  Training Loss  0.0012909234501421452\n",
            "Epoch  28 Batch  382 / 525  Training Loss  0.0014932246413081884\n",
            "Epoch  28 Batch  383 / 525  Training Loss  0.002222407143563032\n",
            "Epoch  28 Batch  384 / 525  Training Loss  0.00048595824046060443\n",
            "Epoch  28 Batch  385 / 525  Training Loss  0.0002174200490117073\n",
            "Epoch  28 Batch  386 / 525  Training Loss  0.0003327989252284169\n",
            "Epoch  28 Batch  387 / 525  Training Loss  0.0016708371695131063\n",
            "Epoch  28 Batch  388 / 525  Training Loss  0.00036126020131632686\n",
            "Epoch  28 Batch  389 / 525  Training Loss  0.0021463828161358833\n",
            "Epoch  28 Batch  390 / 525  Training Loss  0.0005666612414643168\n",
            "Epoch  28 Batch  391 / 525  Training Loss  0.0032253817189484835\n",
            "Epoch  28 Batch  392 / 525  Training Loss  0.00025184141122736037\n",
            "Epoch  28 Batch  393 / 525  Training Loss  0.0024332418106496334\n",
            "Epoch  28 Batch  394 / 525  Training Loss  0.003938555717468262\n",
            "Epoch  28 Batch  395 / 525  Training Loss  0.0011852409224957228\n",
            "Epoch  28 Batch  396 / 525  Training Loss  0.0005049056489951909\n",
            "Epoch  28 Batch  397 / 525  Training Loss  0.0232708603143692\n",
            "Epoch  28 Batch  398 / 525  Training Loss  0.0007765738992020488\n",
            "Epoch  28 Batch  399 / 525  Training Loss  0.0004396279400680214\n",
            "Epoch  28 Batch  400 / 525  Training Loss  0.0007147224387153983\n",
            "Epoch  28 Batch  401 / 525  Training Loss  0.00260731833986938\n",
            "Epoch  28 Batch  402 / 525  Training Loss  0.00048668068484403193\n",
            "Epoch  28 Batch  403 / 525  Training Loss  0.009033126756548882\n",
            "Epoch  28 Batch  404 / 525  Training Loss  0.0005244943895377219\n",
            "Epoch  28 Batch  405 / 525  Training Loss  0.000734081317204982\n",
            "Epoch  28 Batch  406 / 525  Training Loss  0.004092865623533726\n",
            "Epoch  28 Batch  407 / 525  Training Loss  0.0006176819442771375\n",
            "Epoch  28 Batch  408 / 525  Training Loss  0.002636798657476902\n",
            "Epoch  28 Batch  409 / 525  Training Loss  0.0013841179898008704\n",
            "Epoch  28 Batch  410 / 525  Training Loss  0.0005248725647106767\n",
            "Epoch  28 Batch  411 / 525  Training Loss  0.0006315921782515943\n",
            "Epoch  28 Batch  412 / 525  Training Loss  0.001368852099403739\n",
            "Epoch  28 Batch  413 / 525  Training Loss  0.0005629462539218366\n",
            "Epoch  28 Batch  414 / 525  Training Loss  0.0045735519379377365\n",
            "Epoch  28 Batch  415 / 525  Training Loss  0.0006508614169433713\n",
            "Epoch  28 Batch  416 / 525  Training Loss  0.00026551017072051764\n",
            "Epoch  28 Batch  417 / 525  Training Loss  0.0004932174924761057\n",
            "Epoch  28 Batch  418 / 525  Training Loss  0.0038814146537333727\n",
            "Epoch  28 Batch  419 / 525  Training Loss  0.0002071151975542307\n",
            "Epoch  28 Batch  420 / 525  Training Loss  0.00041078325011767447\n",
            "Epoch  28 Batch  421 / 525  Training Loss  0.003093777224421501\n",
            "Epoch  28 Batch  422 / 525  Training Loss  0.007918045856058598\n",
            "Epoch  28 Batch  423 / 525  Training Loss  0.0018450722564011812\n",
            "Epoch  28 Batch  424 / 525  Training Loss  0.0009538716403767467\n",
            "Epoch  28 Batch  425 / 525  Training Loss  0.001959868473932147\n",
            "Epoch  28 Batch  426 / 525  Training Loss  0.0043498617596924305\n",
            "Epoch  28 Batch  427 / 525  Training Loss  0.0012853079242631793\n",
            "Epoch  28 Batch  428 / 525  Training Loss  0.002779425121843815\n",
            "Epoch  28 Batch  429 / 525  Training Loss  0.0009574828436598182\n",
            "Epoch  28 Batch  430 / 525  Training Loss  0.000585982867050916\n",
            "Epoch  28 Batch  431 / 525  Training Loss  0.002009171759709716\n",
            "Epoch  28 Batch  432 / 525  Training Loss  0.0008037342922762036\n",
            "Epoch  28 Batch  433 / 525  Training Loss  0.00046928258961997926\n",
            "Epoch  28 Batch  434 / 525  Training Loss  0.0032271835952997208\n",
            "Epoch  28 Batch  435 / 525  Training Loss  0.0005260869511403143\n",
            "Epoch  28 Batch  436 / 525  Training Loss  0.001834212220273912\n",
            "Epoch  28 Batch  437 / 525  Training Loss  0.0014065771829336882\n",
            "Epoch  28 Batch  438 / 525  Training Loss  0.0021718419156968594\n",
            "Epoch  28 Batch  439 / 525  Training Loss  0.0006330505711957812\n",
            "Epoch  28 Batch  440 / 525  Training Loss  0.000624768203124404\n",
            "Epoch  28 Batch  441 / 525  Training Loss  0.0004591709584929049\n",
            "Epoch  28 Batch  442 / 525  Training Loss  0.00047757886932231486\n",
            "Epoch  28 Batch  443 / 525  Training Loss  0.0011317512253299356\n",
            "Epoch  28 Batch  444 / 525  Training Loss  0.0011193484533578157\n",
            "Epoch  28 Batch  445 / 525  Training Loss  0.001292091910727322\n",
            "Epoch  28 Batch  446 / 525  Training Loss  0.0005299457116052508\n",
            "Epoch  28 Batch  447 / 525  Training Loss  0.0031438469886779785\n",
            "Epoch  28 Batch  448 / 525  Training Loss  0.0008022463880479336\n",
            "Epoch  28 Batch  449 / 525  Training Loss  0.0017554893856868148\n",
            "Epoch  28 Batch  450 / 525  Training Loss  0.0003838859556708485\n",
            "Epoch  28 Batch  451 / 525  Training Loss  0.0007900469936430454\n",
            "Epoch  28 Batch  452 / 525  Training Loss  0.0011214327532798052\n",
            "Epoch  28 Batch  453 / 525  Training Loss  0.0010439688339829445\n",
            "Epoch  28 Batch  454 / 525  Training Loss  0.0007537344936281443\n",
            "Epoch  28 Batch  455 / 525  Training Loss  0.0027364762499928474\n",
            "Epoch  28 Batch  456 / 525  Training Loss  0.00045457793748937547\n",
            "Epoch  28 Batch  457 / 525  Training Loss  0.0006236693589016795\n",
            "Epoch  28 Batch  458 / 525  Training Loss  0.0014969046460464597\n",
            "Epoch  28 Batch  459 / 525  Training Loss  0.0005440744571387768\n",
            "Epoch  28 Batch  460 / 525  Training Loss  0.00044086118577979505\n",
            "Epoch  28 Batch  461 / 525  Training Loss  0.0014075606595724821\n",
            "Epoch  28 Batch  462 / 525  Training Loss  0.0005206361529417336\n",
            "Epoch  28 Batch  463 / 525  Training Loss  0.0005712698912248015\n",
            "Epoch  28 Batch  464 / 525  Training Loss  0.0005269552348181605\n",
            "Epoch  28 Batch  465 / 525  Training Loss  0.004616756923496723\n",
            "Epoch  28 Batch  466 / 525  Training Loss  0.0005653206026181579\n",
            "Epoch  28 Batch  467 / 525  Training Loss  0.0005784924724139273\n",
            "Epoch  28 Batch  468 / 525  Training Loss  0.0015452904626727104\n",
            "Epoch  28 Batch  469 / 525  Training Loss  0.0008846120908856392\n",
            "Epoch  28 Batch  470 / 525  Training Loss  0.0014677327126264572\n",
            "Epoch  28 Batch  471 / 525  Training Loss  0.004977698903530836\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  28 Batch  472 / 525  Training Loss  0.0009155956795439124\n",
            "Epoch  28 Batch  473 / 525  Training Loss  0.00034180746297352016\n",
            "Epoch  28 Batch  474 / 525  Training Loss  0.001980372006073594\n",
            "Epoch  28 Batch  475 / 525  Training Loss  0.001135193626396358\n",
            "Epoch  28 Batch  476 / 525  Training Loss  0.000354734540451318\n",
            "Epoch  28 Batch  477 / 525  Training Loss  0.0014423191314563155\n",
            "Epoch  28 Batch  478 / 525  Training Loss  0.0005690566031262279\n",
            "Epoch  28 Batch  479 / 525  Training Loss  0.003589655738323927\n",
            "Epoch  28 Batch  480 / 525  Training Loss  0.0005224662600085139\n",
            "Epoch  28 Batch  481 / 525  Training Loss  0.006642959080636501\n",
            "Epoch  28 Batch  482 / 525  Training Loss  0.0009334151400253177\n",
            "Epoch  28 Batch  483 / 525  Training Loss  0.0004954535979777575\n",
            "Epoch  28 Batch  484 / 525  Training Loss  0.0023536942899227142\n",
            "Epoch  28 Batch  485 / 525  Training Loss  0.001663748174905777\n",
            "Epoch  28 Batch  486 / 525  Training Loss  0.0013259955449029803\n",
            "Epoch  28 Batch  487 / 525  Training Loss  0.00043749817996285856\n",
            "Epoch  28 Batch  488 / 525  Training Loss  0.0010684732114896178\n",
            "Epoch  28 Batch  489 / 525  Training Loss  0.0006575871375389397\n",
            "Epoch  28 Batch  490 / 525  Training Loss  0.0006998867029324174\n",
            "Epoch  28 Batch  491 / 525  Training Loss  0.0028526003006845713\n",
            "Epoch  28 Batch  492 / 525  Training Loss  0.0017909035086631775\n",
            "Epoch  28 Batch  493 / 525  Training Loss  0.001654375926591456\n",
            "Epoch  28 Batch  494 / 525  Training Loss  0.0007874613511376083\n",
            "Epoch  28 Batch  495 / 525  Training Loss  0.0006006404873915017\n",
            "Epoch  28 Batch  496 / 525  Training Loss  0.0011290357215330005\n",
            "Epoch  28 Batch  497 / 525  Training Loss  0.000870490912348032\n",
            "Epoch  28 Batch  498 / 525  Training Loss  0.0005488785682246089\n",
            "Epoch  28 Batch  499 / 525  Training Loss  0.0005360945942811668\n",
            "Epoch  28 Batch  500 / 525  Training Loss  0.0013379320735111833\n",
            "Epoch  28 Batch  501 / 525  Training Loss  0.0005942042917013168\n",
            "Epoch  28 Batch  502 / 525  Training Loss  0.0023018515203148127\n",
            "Epoch  28 Batch  503 / 525  Training Loss  0.0014636447886005044\n",
            "Epoch  28 Batch  504 / 525  Training Loss  0.00039207120425999165\n",
            "Epoch  28 Batch  505 / 525  Training Loss  0.0005948895122855902\n",
            "Epoch  28 Batch  506 / 525  Training Loss  0.0012519151205196977\n",
            "Epoch  28 Batch  507 / 525  Training Loss  0.00014615604595746845\n",
            "Epoch  28 Batch  508 / 525  Training Loss  0.0009031520457938313\n",
            "Epoch  28 Batch  509 / 525  Training Loss  0.0008764553931541741\n",
            "Epoch  28 Batch  510 / 525  Training Loss  0.0010090216528624296\n",
            "Epoch  28 Batch  511 / 525  Training Loss  0.0003136496525257826\n",
            "Epoch  28 Batch  512 / 525  Training Loss  0.00028696010122075677\n",
            "Epoch  28 Batch  513 / 525  Training Loss  0.00079027732135728\n",
            "Epoch  28 Batch  514 / 525  Training Loss  0.0015061674639582634\n",
            "Epoch  28 Batch  515 / 525  Training Loss  0.007632082793861628\n",
            "Epoch  28 Batch  516 / 525  Training Loss  0.00026158150285482407\n",
            "Epoch  28 Batch  517 / 525  Training Loss  0.005406551994383335\n",
            "Epoch  28 Batch  518 / 525  Training Loss  0.0009322576224803925\n",
            "Epoch  28 Batch  519 / 525  Training Loss  0.0009952059481292963\n",
            "Epoch  28 Batch  520 / 525  Training Loss  0.0008531213970854878\n",
            "Epoch  28 Batch  521 / 525  Training Loss  0.00030912860529497266\n",
            "Epoch  28 Batch  522 / 525  Training Loss  0.0009129426325671375\n",
            "Epoch  28 Batch  523 / 525  Training Loss  0.0008826808189041913\n",
            "Epoch  28 Batch  524 / 525  Training Loss  0.00036939565325155854\n",
            "  29    |    -    |   0.001453   | 60.858333\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 29\n",
            "Epoch  29 Batch  0 / 525  Training Loss  0.0005588038475252688\n",
            "Epoch  29 Batch  1 / 525  Training Loss  0.00026639964198693633\n",
            "Epoch  29 Batch  2 / 525  Training Loss  0.0008265491342172027\n",
            "Epoch  29 Batch  3 / 525  Training Loss  0.002990593435242772\n",
            "Epoch  29 Batch  4 / 525  Training Loss  0.0021715909242630005\n",
            "Epoch  29 Batch  5 / 525  Training Loss  0.0002390881272731349\n",
            "Epoch  29 Batch  6 / 525  Training Loss  0.0007136637577787042\n",
            "Epoch  29 Batch  7 / 525  Training Loss  0.0022187349386513233\n",
            "Epoch  29 Batch  8 / 525  Training Loss  0.0001661841233726591\n",
            "Epoch  29 Batch  9 / 525  Training Loss  0.002108293818309903\n",
            "Epoch  29 Batch  10 / 525  Training Loss  0.0004695256648119539\n",
            "Epoch  29 Batch  11 / 525  Training Loss  0.0020752071868628263\n",
            "Epoch  29 Batch  12 / 525  Training Loss  0.0004382869810797274\n",
            "Epoch  29 Batch  13 / 525  Training Loss  0.000515305029693991\n",
            "Epoch  29 Batch  14 / 525  Training Loss  0.00023217608395498246\n",
            "Epoch  29 Batch  15 / 525  Training Loss  0.0009092002874240279\n",
            "Epoch  29 Batch  16 / 525  Training Loss  0.011020289734005928\n",
            "Epoch  29 Batch  17 / 525  Training Loss  0.0011877905344590545\n",
            "Epoch  29 Batch  18 / 525  Training Loss  0.00027508437051437795\n",
            "Epoch  29 Batch  19 / 525  Training Loss  0.001666031195782125\n",
            "Epoch  29 Batch  20 / 525  Training Loss  0.00045299678458832204\n",
            "Epoch  29 Batch  21 / 525  Training Loss  0.0004704507882706821\n",
            "Epoch  29 Batch  22 / 525  Training Loss  0.0035843285731971264\n",
            "Epoch  29 Batch  23 / 525  Training Loss  0.012989687733352184\n",
            "Epoch  29 Batch  24 / 525  Training Loss  0.0031300578266382217\n",
            "Epoch  29 Batch  25 / 525  Training Loss  0.00042897858656942844\n",
            "Epoch  29 Batch  26 / 525  Training Loss  0.00033276370959356427\n",
            "Epoch  29 Batch  27 / 525  Training Loss  0.0019411254907026887\n",
            "Epoch  29 Batch  28 / 525  Training Loss  0.0009380261180922389\n",
            "Epoch  29 Batch  29 / 525  Training Loss  0.0008603981696069241\n",
            "Epoch  29 Batch  30 / 525  Training Loss  0.00045885349391028285\n",
            "Epoch  29 Batch  31 / 525  Training Loss  0.0001697328989394009\n",
            "Epoch  29 Batch  32 / 525  Training Loss  0.00028468650998547673\n",
            "Epoch  29 Batch  33 / 525  Training Loss  0.00017713182023726404\n",
            "Epoch  29 Batch  34 / 525  Training Loss  0.00037771626375615597\n",
            "Epoch  29 Batch  35 / 525  Training Loss  0.00014195182302501053\n",
            "Epoch  29 Batch  36 / 525  Training Loss  0.0003343000716995448\n",
            "Epoch  29 Batch  37 / 525  Training Loss  0.0003787647292483598\n",
            "Epoch  29 Batch  38 / 525  Training Loss  0.0006259774672798812\n",
            "Epoch  29 Batch  39 / 525  Training Loss  0.0009698208305053413\n",
            "Epoch  29 Batch  40 / 525  Training Loss  0.0010401257313787937\n",
            "Epoch  29 Batch  41 / 525  Training Loss  0.0005024216952733696\n",
            "Epoch  29 Batch  42 / 525  Training Loss  0.0002862295077648014\n",
            "Epoch  29 Batch  43 / 525  Training Loss  0.0005875056376680732\n",
            "Epoch  29 Batch  44 / 525  Training Loss  0.001370065612718463\n",
            "Epoch  29 Batch  45 / 525  Training Loss  0.00032921868842095137\n",
            "Epoch  29 Batch  46 / 525  Training Loss  0.0003325760771986097\n",
            "Epoch  29 Batch  47 / 525  Training Loss  0.007328130304813385\n",
            "Epoch  29 Batch  48 / 525  Training Loss  0.0014755758456885815\n",
            "Epoch  29 Batch  49 / 525  Training Loss  0.00043521038605831563\n",
            "Epoch  29 Batch  50 / 525  Training Loss  0.0004341411404311657\n",
            "Epoch  29 Batch  51 / 525  Training Loss  0.00033068511402234435\n",
            "Epoch  29 Batch  52 / 525  Training Loss  0.001275616348721087\n",
            "Epoch  29 Batch  53 / 525  Training Loss  0.00028395498520694673\n",
            "Epoch  29 Batch  54 / 525  Training Loss  0.00016648737073410302\n",
            "Epoch  29 Batch  55 / 525  Training Loss  0.001982045592740178\n",
            "Epoch  29 Batch  56 / 525  Training Loss  0.0003847845073323697\n",
            "Epoch  29 Batch  57 / 525  Training Loss  0.0007656662492081523\n",
            "Epoch  29 Batch  58 / 525  Training Loss  0.003755322191864252\n",
            "Epoch  29 Batch  59 / 525  Training Loss  0.00247757276520133\n",
            "Epoch  29 Batch  60 / 525  Training Loss  0.0021266781259328127\n",
            "Epoch  29 Batch  61 / 525  Training Loss  0.0019388571381568909\n",
            "Epoch  29 Batch  62 / 525  Training Loss  0.0015132842818275094\n",
            "Epoch  29 Batch  63 / 525  Training Loss  0.00037636380875483155\n",
            "Epoch  29 Batch  64 / 525  Training Loss  0.007533550262451172\n",
            "Epoch  29 Batch  65 / 525  Training Loss  0.0012477526906877756\n",
            "Epoch  29 Batch  66 / 525  Training Loss  0.0012676495825871825\n",
            "Epoch  29 Batch  67 / 525  Training Loss  0.00023860411602072418\n",
            "Epoch  29 Batch  68 / 525  Training Loss  0.002446959726512432\n",
            "Epoch  29 Batch  69 / 525  Training Loss  0.00039868010208010674\n",
            "Epoch  29 Batch  70 / 525  Training Loss  0.0010142409009858966\n",
            "Epoch  29 Batch  71 / 525  Training Loss  0.0003500908496789634\n",
            "Epoch  29 Batch  72 / 525  Training Loss  0.0012784762075170875\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  29 Batch  73 / 525  Training Loss  0.0009636399336159229\n",
            "Epoch  29 Batch  74 / 525  Training Loss  0.00029139971593394876\n",
            "Epoch  29 Batch  75 / 525  Training Loss  0.0007419627509079874\n",
            "Epoch  29 Batch  76 / 525  Training Loss  0.0002481082337908447\n",
            "Epoch  29 Batch  77 / 525  Training Loss  0.0006611815188080072\n",
            "Epoch  29 Batch  78 / 525  Training Loss  0.0006102289771661162\n",
            "Epoch  29 Batch  79 / 525  Training Loss  0.0009318210068158805\n",
            "Epoch  29 Batch  80 / 525  Training Loss  0.00031188142020255327\n",
            "Epoch  29 Batch  81 / 525  Training Loss  0.002939482918009162\n",
            "Epoch  29 Batch  82 / 525  Training Loss  0.0005480199470184743\n",
            "Epoch  29 Batch  83 / 525  Training Loss  0.005658949259668589\n",
            "Epoch  29 Batch  84 / 525  Training Loss  0.0013080134522169828\n",
            "Epoch  29 Batch  85 / 525  Training Loss  0.00027605428476817906\n",
            "Epoch  29 Batch  86 / 525  Training Loss  0.004980742000043392\n",
            "Epoch  29 Batch  87 / 525  Training Loss  0.006793855223804712\n",
            "Epoch  29 Batch  88 / 525  Training Loss  0.0006714907940477133\n",
            "Epoch  29 Batch  89 / 525  Training Loss  0.0005846293643116951\n",
            "Epoch  29 Batch  90 / 525  Training Loss  0.006489561405032873\n",
            "Epoch  29 Batch  91 / 525  Training Loss  0.0012390469200909138\n",
            "Epoch  29 Batch  92 / 525  Training Loss  0.0051331352442502975\n",
            "Epoch  29 Batch  93 / 525  Training Loss  0.00048555940156802535\n",
            "Epoch  29 Batch  94 / 525  Training Loss  0.0006318240193650126\n",
            "Epoch  29 Batch  95 / 525  Training Loss  0.00035094207851216197\n",
            "Epoch  29 Batch  96 / 525  Training Loss  0.0012244636891409755\n",
            "Epoch  29 Batch  97 / 525  Training Loss  0.0029411811847239733\n",
            "Epoch  29 Batch  98 / 525  Training Loss  0.0012967975344508886\n",
            "Epoch  29 Batch  99 / 525  Training Loss  0.0011040593963116407\n",
            "Epoch  29 Batch  100 / 525  Training Loss  0.00044128522858954966\n",
            "Epoch  29 Batch  101 / 525  Training Loss  0.0028322588186711073\n",
            "Epoch  29 Batch  102 / 525  Training Loss  0.0009116143919527531\n",
            "Epoch  29 Batch  103 / 525  Training Loss  0.0006358997779898345\n",
            "Epoch  29 Batch  104 / 525  Training Loss  0.00037788302870467305\n",
            "Epoch  29 Batch  105 / 525  Training Loss  0.0002322435175301507\n",
            "Epoch  29 Batch  106 / 525  Training Loss  0.00048029967001639307\n",
            "Epoch  29 Batch  107 / 525  Training Loss  0.008279848843812943\n",
            "Epoch  29 Batch  108 / 525  Training Loss  0.008035186678171158\n",
            "Epoch  29 Batch  109 / 525  Training Loss  0.009194426238536835\n",
            "Epoch  29 Batch  110 / 525  Training Loss  0.0017659323057159781\n",
            "Epoch  29 Batch  111 / 525  Training Loss  0.002199317328631878\n",
            "Epoch  29 Batch  112 / 525  Training Loss  0.0015545559581369162\n",
            "Epoch  29 Batch  113 / 525  Training Loss  0.0006339055253192782\n",
            "Epoch  29 Batch  114 / 525  Training Loss  0.0014292923733592033\n",
            "Epoch  29 Batch  115 / 525  Training Loss  0.005474405828863382\n",
            "Epoch  29 Batch  116 / 525  Training Loss  0.00245117605663836\n",
            "Epoch  29 Batch  117 / 525  Training Loss  0.00933611299842596\n",
            "Epoch  29 Batch  118 / 525  Training Loss  0.004490300081670284\n",
            "Epoch  29 Batch  119 / 525  Training Loss  0.0005744500667788088\n",
            "Epoch  29 Batch  120 / 525  Training Loss  0.0004613927158061415\n",
            "Epoch  29 Batch  121 / 525  Training Loss  0.0025457055307924747\n",
            "Epoch  29 Batch  122 / 525  Training Loss  0.000595338293351233\n",
            "Epoch  29 Batch  123 / 525  Training Loss  0.0011771179269999266\n",
            "Epoch  29 Batch  124 / 525  Training Loss  0.0048745195381343365\n",
            "Epoch  29 Batch  125 / 525  Training Loss  0.001156518468633294\n",
            "Epoch  29 Batch  126 / 525  Training Loss  0.006949731148779392\n",
            "Epoch  29 Batch  127 / 525  Training Loss  0.0021268411073833704\n",
            "Epoch  29 Batch  128 / 525  Training Loss  0.0005911936168558896\n",
            "Epoch  29 Batch  129 / 525  Training Loss  0.015507636591792107\n",
            "Epoch  29 Batch  130 / 525  Training Loss  0.0018123263726010919\n",
            "Epoch  29 Batch  131 / 525  Training Loss  0.0017770782578736544\n",
            "Epoch  29 Batch  132 / 525  Training Loss  0.014643052592873573\n",
            "Epoch  29 Batch  133 / 525  Training Loss  0.0032902255188673735\n",
            "Epoch  29 Batch  134 / 525  Training Loss  0.002641975414007902\n",
            "Epoch  29 Batch  135 / 525  Training Loss  0.0005393657484091818\n",
            "Epoch  29 Batch  136 / 525  Training Loss  0.0012703106040135026\n",
            "Epoch  29 Batch  137 / 525  Training Loss  0.002252843463793397\n",
            "Epoch  29 Batch  138 / 525  Training Loss  0.002449944382533431\n",
            "Epoch  29 Batch  139 / 525  Training Loss  0.0008550508064217865\n",
            "Epoch  29 Batch  140 / 525  Training Loss  0.009815695695579052\n",
            "Epoch  29 Batch  141 / 525  Training Loss  0.00515181478112936\n",
            "Epoch  29 Batch  142 / 525  Training Loss  0.0006473307148553431\n",
            "Epoch  29 Batch  143 / 525  Training Loss  0.0015901315491646528\n",
            "Epoch  29 Batch  144 / 525  Training Loss  0.008561047725379467\n",
            "Epoch  29 Batch  145 / 525  Training Loss  0.0006416874821297824\n",
            "Epoch  29 Batch  146 / 525  Training Loss  0.012288099154829979\n",
            "Epoch  29 Batch  147 / 525  Training Loss  0.002885838272050023\n",
            "Epoch  29 Batch  148 / 525  Training Loss  0.0007539689540863037\n",
            "Epoch  29 Batch  149 / 525  Training Loss  0.002068435773253441\n",
            "Epoch  29 Batch  150 / 525  Training Loss  0.002197729889303446\n",
            "Epoch  29 Batch  151 / 525  Training Loss  0.0014492608606815338\n",
            "Epoch  29 Batch  152 / 525  Training Loss  0.0010977665660902858\n",
            "Epoch  29 Batch  153 / 525  Training Loss  0.0010225973092019558\n",
            "Epoch  29 Batch  154 / 525  Training Loss  0.0007723972084932029\n",
            "Epoch  29 Batch  155 / 525  Training Loss  0.0015442360891029239\n",
            "Epoch  29 Batch  156 / 525  Training Loss  0.0037401604931801558\n",
            "Epoch  29 Batch  157 / 525  Training Loss  0.0008967301691882312\n",
            "Epoch  29 Batch  158 / 525  Training Loss  0.0014050217578187585\n",
            "Epoch  29 Batch  159 / 525  Training Loss  0.0019982480444014072\n",
            "Epoch  29 Batch  160 / 525  Training Loss  0.0037001133896410465\n",
            "Epoch  29 Batch  161 / 525  Training Loss  0.004636866971850395\n",
            "Epoch  29 Batch  162 / 525  Training Loss  0.006818812340497971\n",
            "Epoch  29 Batch  163 / 525  Training Loss  0.0008931817719712853\n",
            "Epoch  29 Batch  164 / 525  Training Loss  0.0017382527003064752\n",
            "Epoch  29 Batch  165 / 525  Training Loss  0.0011972866486757994\n",
            "Epoch  29 Batch  166 / 525  Training Loss  0.0021686661057174206\n",
            "Epoch  29 Batch  167 / 525  Training Loss  0.0006297514773905277\n",
            "Epoch  29 Batch  168 / 525  Training Loss  0.0021491842344403267\n",
            "Epoch  29 Batch  169 / 525  Training Loss  0.00962738785892725\n",
            "Epoch  29 Batch  170 / 525  Training Loss  0.00020310615946073085\n",
            "Epoch  29 Batch  171 / 525  Training Loss  0.0014665174530819058\n",
            "Epoch  29 Batch  172 / 525  Training Loss  0.009304877370595932\n",
            "Epoch  29 Batch  173 / 525  Training Loss  0.0018814385402947664\n",
            "Epoch  29 Batch  174 / 525  Training Loss  0.0014430880546569824\n",
            "Epoch  29 Batch  175 / 525  Training Loss  0.005514347460120916\n",
            "Epoch  29 Batch  176 / 525  Training Loss  0.0037617734633386135\n",
            "Epoch  29 Batch  177 / 525  Training Loss  0.0017074259230867028\n",
            "Epoch  29 Batch  178 / 525  Training Loss  0.0011150215286761522\n",
            "Epoch  29 Batch  179 / 525  Training Loss  0.002858641091734171\n",
            "Epoch  29 Batch  180 / 525  Training Loss  0.001004810445010662\n",
            "Epoch  29 Batch  181 / 525  Training Loss  0.0009012968512251973\n",
            "Epoch  29 Batch  182 / 525  Training Loss  0.002689756453037262\n",
            "Epoch  29 Batch  183 / 525  Training Loss  0.0018109467346221209\n",
            "Epoch  29 Batch  184 / 525  Training Loss  0.0007542540552094579\n",
            "Epoch  29 Batch  185 / 525  Training Loss  0.0066115232184529305\n",
            "Epoch  29 Batch  186 / 525  Training Loss  0.0006788569735363126\n",
            "Epoch  29 Batch  187 / 525  Training Loss  0.0012637438485398889\n",
            "Epoch  29 Batch  188 / 525  Training Loss  0.005564928520470858\n",
            "Epoch  29 Batch  189 / 525  Training Loss  0.000676471390761435\n",
            "Epoch  29 Batch  190 / 525  Training Loss  0.0009259528596885502\n",
            "Epoch  29 Batch  191 / 525  Training Loss  0.0032855358440428972\n",
            "Epoch  29 Batch  192 / 525  Training Loss  0.004717312287539244\n",
            "Epoch  29 Batch  193 / 525  Training Loss  0.0006628353730775416\n",
            "Epoch  29 Batch  194 / 525  Training Loss  0.0004994364571757615\n",
            "Epoch  29 Batch  195 / 525  Training Loss  0.0011248288210481405\n",
            "Epoch  29 Batch  196 / 525  Training Loss  0.0070131756365299225\n",
            "Epoch  29 Batch  197 / 525  Training Loss  0.0027210672851651907\n",
            "Epoch  29 Batch  198 / 525  Training Loss  0.002727469429373741\n",
            "Epoch  29 Batch  199 / 525  Training Loss  0.0024845940060913563\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  29 Batch  200 / 525  Training Loss  0.0047047254629433155\n",
            "Epoch  29 Batch  201 / 525  Training Loss  0.011604956351220608\n",
            "Epoch  29 Batch  202 / 525  Training Loss  0.00947757251560688\n",
            "Epoch  29 Batch  203 / 525  Training Loss  0.0009435241809114814\n",
            "Epoch  29 Batch  204 / 525  Training Loss  0.002169900108128786\n",
            "Epoch  29 Batch  205 / 525  Training Loss  0.00234012259170413\n",
            "Epoch  29 Batch  206 / 525  Training Loss  0.0017726446967571974\n",
            "Epoch  29 Batch  207 / 525  Training Loss  0.000699180003721267\n",
            "Epoch  29 Batch  208 / 525  Training Loss  0.0021430416963994503\n",
            "Epoch  29 Batch  209 / 525  Training Loss  0.001255278242751956\n",
            "Epoch  29 Batch  210 / 525  Training Loss  0.003912127576768398\n",
            "Epoch  29 Batch  211 / 525  Training Loss  0.003757199738174677\n",
            "Epoch  29 Batch  212 / 525  Training Loss  0.0032525514252483845\n",
            "Epoch  29 Batch  213 / 525  Training Loss  0.0019632813055068254\n",
            "Epoch  29 Batch  214 / 525  Training Loss  0.0023658836726099253\n",
            "Epoch  29 Batch  215 / 525  Training Loss  0.014939764514565468\n",
            "Epoch  29 Batch  216 / 525  Training Loss  0.0005030925385653973\n",
            "Epoch  29 Batch  217 / 525  Training Loss  0.020950976759195328\n",
            "Epoch  29 Batch  218 / 525  Training Loss  0.001975869992747903\n",
            "Epoch  29 Batch  219 / 525  Training Loss  0.0004627757298294455\n",
            "Epoch  29 Batch  220 / 525  Training Loss  0.0045004417188465595\n",
            "Epoch  29 Batch  221 / 525  Training Loss  0.003318166360259056\n",
            "Epoch  29 Batch  222 / 525  Training Loss  0.0002813999599311501\n",
            "Epoch  29 Batch  223 / 525  Training Loss  0.00029801830532960594\n",
            "Epoch  29 Batch  224 / 525  Training Loss  0.005046602338552475\n",
            "Epoch  29 Batch  225 / 525  Training Loss  0.0010250350460410118\n",
            "Epoch  29 Batch  226 / 525  Training Loss  0.00047951447777450085\n",
            "Epoch  29 Batch  227 / 525  Training Loss  0.0008870015735737979\n",
            "Epoch  29 Batch  228 / 525  Training Loss  0.00122173223644495\n",
            "Epoch  29 Batch  229 / 525  Training Loss  0.0024854899384081364\n",
            "Epoch  29 Batch  230 / 525  Training Loss  0.007172033190727234\n",
            "Epoch  29 Batch  231 / 525  Training Loss  0.01983550190925598\n",
            "Epoch  29 Batch  232 / 525  Training Loss  0.0012218692572787404\n",
            "Epoch  29 Batch  233 / 525  Training Loss  0.041099756956100464\n",
            "Epoch  29 Batch  234 / 525  Training Loss  0.011589817702770233\n",
            "Epoch  29 Batch  235 / 525  Training Loss  0.0028267635498195887\n",
            "Epoch  29 Batch  236 / 525  Training Loss  0.004967874847352505\n",
            "Epoch  29 Batch  237 / 525  Training Loss  0.003977936692535877\n",
            "Epoch  29 Batch  238 / 525  Training Loss  0.003077706089243293\n",
            "Epoch  29 Batch  239 / 525  Training Loss  0.0032408013939857483\n",
            "Epoch  29 Batch  240 / 525  Training Loss  0.010391843505203724\n",
            "Epoch  29 Batch  241 / 525  Training Loss  0.0006644686218351126\n",
            "Epoch  29 Batch  242 / 525  Training Loss  0.006122380495071411\n",
            "Epoch  29 Batch  243 / 525  Training Loss  0.0015613727737218142\n",
            "Epoch  29 Batch  244 / 525  Training Loss  0.0062144724652171135\n",
            "Epoch  29 Batch  245 / 525  Training Loss  0.006671842187643051\n",
            "Epoch  29 Batch  246 / 525  Training Loss  0.003117116866633296\n",
            "Epoch  29 Batch  247 / 525  Training Loss  0.003450628835707903\n",
            "Epoch  29 Batch  248 / 525  Training Loss  0.004522825591266155\n",
            "Epoch  29 Batch  249 / 525  Training Loss  0.010847162455320358\n",
            "Epoch  29 Batch  250 / 525  Training Loss  0.01771249622106552\n",
            "Epoch  29 Batch  251 / 525  Training Loss  0.014528085477650166\n",
            "Epoch  29 Batch  252 / 525  Training Loss  0.0007744897739030421\n",
            "Epoch  29 Batch  253 / 525  Training Loss  0.007387892808765173\n",
            "Epoch  29 Batch  254 / 525  Training Loss  0.007581514772027731\n",
            "Epoch  29 Batch  255 / 525  Training Loss  0.001441537169739604\n",
            "Epoch  29 Batch  256 / 525  Training Loss  0.004485278856009245\n",
            "Epoch  29 Batch  257 / 525  Training Loss  0.008263250812888145\n",
            "Epoch  29 Batch  258 / 525  Training Loss  0.0011483978014439344\n",
            "Epoch  29 Batch  259 / 525  Training Loss  0.0092181246727705\n",
            "Epoch  29 Batch  260 / 525  Training Loss  0.000712521024979651\n",
            "Epoch  29 Batch  261 / 525  Training Loss  0.00151930283755064\n",
            "Epoch  29 Batch  262 / 525  Training Loss  0.010015754029154778\n",
            "Epoch  29 Batch  263 / 525  Training Loss  0.0011250770185142756\n",
            "Epoch  29 Batch  264 / 525  Training Loss  0.008695811033248901\n",
            "Epoch  29 Batch  265 / 525  Training Loss  0.012755272909998894\n",
            "Epoch  29 Batch  266 / 525  Training Loss  0.009210152551531792\n",
            "Epoch  29 Batch  267 / 525  Training Loss  0.001075959880836308\n",
            "Epoch  29 Batch  268 / 525  Training Loss  0.0015664302045479417\n",
            "Epoch  29 Batch  269 / 525  Training Loss  0.004325101152062416\n",
            "Epoch  29 Batch  270 / 525  Training Loss  0.0038743619807064533\n",
            "Epoch  29 Batch  271 / 525  Training Loss  0.0013160060625523329\n",
            "Epoch  29 Batch  272 / 525  Training Loss  0.00524787325412035\n",
            "Epoch  29 Batch  273 / 525  Training Loss  0.0038924776017665863\n",
            "Epoch  29 Batch  274 / 525  Training Loss  0.002016779500991106\n",
            "Epoch  29 Batch  275 / 525  Training Loss  0.001116895698942244\n",
            "Epoch  29 Batch  276 / 525  Training Loss  0.0017168314661830664\n",
            "Epoch  29 Batch  277 / 525  Training Loss  0.003180988132953644\n",
            "Epoch  29 Batch  278 / 525  Training Loss  0.0033275976311415434\n",
            "Epoch  29 Batch  279 / 525  Training Loss  0.0010819369927048683\n",
            "Epoch  29 Batch  280 / 525  Training Loss  0.0004832178819924593\n",
            "Epoch  29 Batch  281 / 525  Training Loss  0.002225442323833704\n",
            "Epoch  29 Batch  282 / 525  Training Loss  0.0013013662537559867\n",
            "Epoch  29 Batch  283 / 525  Training Loss  0.0034750383347272873\n",
            "Epoch  29 Batch  284 / 525  Training Loss  0.0010821577161550522\n",
            "Epoch  29 Batch  285 / 525  Training Loss  0.0010158185614272952\n",
            "Epoch  29 Batch  286 / 525  Training Loss  0.002222352661192417\n",
            "Epoch  29 Batch  287 / 525  Training Loss  0.0004975854535587132\n",
            "Epoch  29 Batch  288 / 525  Training Loss  0.0012433092342689633\n",
            "Epoch  29 Batch  289 / 525  Training Loss  0.0010810722596943378\n",
            "Epoch  29 Batch  290 / 525  Training Loss  0.0018822072306647897\n",
            "Epoch  29 Batch  291 / 525  Training Loss  0.0019748869817703962\n",
            "Epoch  29 Batch  292 / 525  Training Loss  0.0012798771494999528\n",
            "Epoch  29 Batch  293 / 525  Training Loss  0.0005013247719034553\n",
            "Epoch  29 Batch  294 / 525  Training Loss  0.001999804051592946\n",
            "Epoch  29 Batch  295 / 525  Training Loss  0.0004040020576212555\n",
            "Epoch  29 Batch  296 / 525  Training Loss  0.001268470659852028\n",
            "Epoch  29 Batch  297 / 525  Training Loss  0.0035611968487501144\n",
            "Epoch  29 Batch  298 / 525  Training Loss  0.0026859561912715435\n",
            "Epoch  29 Batch  299 / 525  Training Loss  0.000806501426268369\n",
            "Epoch  29 Batch  300 / 525  Training Loss  0.0020219446159899235\n",
            "Epoch  29 Batch  301 / 525  Training Loss  0.011809831485152245\n",
            "Epoch  29 Batch  302 / 525  Training Loss  0.002922218758612871\n",
            "Epoch  29 Batch  303 / 525  Training Loss  0.002242419868707657\n",
            "Epoch  29 Batch  304 / 525  Training Loss  0.004237411078065634\n",
            "Epoch  29 Batch  305 / 525  Training Loss  0.0016415916616097093\n",
            "Epoch  29 Batch  306 / 525  Training Loss  0.0005468547460623085\n",
            "Epoch  29 Batch  307 / 525  Training Loss  0.0024475150275975466\n",
            "Epoch  29 Batch  308 / 525  Training Loss  0.001922196475788951\n",
            "Epoch  29 Batch  309 / 525  Training Loss  0.0010222176788374782\n",
            "Epoch  29 Batch  310 / 525  Training Loss  0.0006133798160590231\n",
            "Epoch  29 Batch  311 / 525  Training Loss  0.011390138417482376\n",
            "Epoch  29 Batch  312 / 525  Training Loss  0.0013083426747471094\n",
            "Epoch  29 Batch  313 / 525  Training Loss  0.007289335131645203\n",
            "Epoch  29 Batch  314 / 525  Training Loss  0.002500031143426895\n",
            "Epoch  29 Batch  315 / 525  Training Loss  0.002814843552187085\n",
            "Epoch  29 Batch  316 / 525  Training Loss  0.0004870593547821045\n",
            "Epoch  29 Batch  317 / 525  Training Loss  0.0007479247869923711\n",
            "Epoch  29 Batch  318 / 525  Training Loss  0.0015249334974214435\n",
            "Epoch  29 Batch  319 / 525  Training Loss  0.0008079372346401215\n",
            "Epoch  29 Batch  320 / 525  Training Loss  0.001327283913269639\n",
            "Epoch  29 Batch  321 / 525  Training Loss  0.0015906805638223886\n",
            "Epoch  29 Batch  322 / 525  Training Loss  0.0002759263152256608\n",
            "Epoch  29 Batch  323 / 525  Training Loss  0.0012536246795207262\n",
            "Epoch  29 Batch  324 / 525  Training Loss  0.007686441298574209\n",
            "Epoch  29 Batch  325 / 525  Training Loss  0.005551321431994438\n",
            "Epoch  29 Batch  326 / 525  Training Loss  0.0020920538809150457\n",
            "Epoch  29 Batch  327 / 525  Training Loss  0.01730504259467125\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  29 Batch  328 / 525  Training Loss  0.00030917394906282425\n",
            "Epoch  29 Batch  329 / 525  Training Loss  0.0014732214622199535\n",
            "Epoch  29 Batch  330 / 525  Training Loss  0.0031709703616797924\n",
            "Epoch  29 Batch  331 / 525  Training Loss  0.021723538637161255\n",
            "Epoch  29 Batch  332 / 525  Training Loss  0.003327262355014682\n",
            "Epoch  29 Batch  333 / 525  Training Loss  0.0018915238324552774\n",
            "Epoch  29 Batch  334 / 525  Training Loss  0.003994624130427837\n",
            "Epoch  29 Batch  335 / 525  Training Loss  0.005261842627078295\n",
            "Epoch  29 Batch  336 / 525  Training Loss  0.0016324480529874563\n",
            "Epoch  29 Batch  337 / 525  Training Loss  0.0006747193401679397\n",
            "Epoch  29 Batch  338 / 525  Training Loss  0.0006164569640532136\n",
            "Epoch  29 Batch  339 / 525  Training Loss  0.0011176049010828137\n",
            "Epoch  29 Batch  340 / 525  Training Loss  0.0006962838233448565\n",
            "Epoch  29 Batch  341 / 525  Training Loss  0.006030063144862652\n",
            "Epoch  29 Batch  342 / 525  Training Loss  0.010083835572004318\n",
            "Epoch  29 Batch  343 / 525  Training Loss  0.0014469021698459983\n",
            "Epoch  29 Batch  344 / 525  Training Loss  0.0017357368487864733\n",
            "Epoch  29 Batch  345 / 525  Training Loss  0.0046793133951723576\n",
            "Epoch  29 Batch  346 / 525  Training Loss  0.0008703258936293423\n",
            "Epoch  29 Batch  347 / 525  Training Loss  0.00038350289105437696\n",
            "Epoch  29 Batch  348 / 525  Training Loss  0.0015545748174190521\n",
            "Epoch  29 Batch  349 / 525  Training Loss  0.0037452932447195053\n",
            "Epoch  29 Batch  350 / 525  Training Loss  0.003906595055013895\n",
            "Epoch  29 Batch  351 / 525  Training Loss  0.0007801916217431426\n",
            "Epoch  29 Batch  352 / 525  Training Loss  0.0030405307188630104\n",
            "Epoch  29 Batch  353 / 525  Training Loss  0.00069547223392874\n",
            "Epoch  29 Batch  354 / 525  Training Loss  0.00588867487385869\n",
            "Epoch  29 Batch  355 / 525  Training Loss  0.0021869363263249397\n",
            "Epoch  29 Batch  356 / 525  Training Loss  0.0005741780623793602\n",
            "Epoch  29 Batch  357 / 525  Training Loss  0.0015441671712324023\n",
            "Epoch  29 Batch  358 / 525  Training Loss  0.004300042521208525\n",
            "Epoch  29 Batch  359 / 525  Training Loss  0.0006354149663820863\n",
            "Epoch  29 Batch  360 / 525  Training Loss  0.005024045705795288\n",
            "Epoch  29 Batch  361 / 525  Training Loss  0.0015295096673071384\n",
            "Epoch  29 Batch  362 / 525  Training Loss  0.0010455779265612364\n",
            "Epoch  29 Batch  363 / 525  Training Loss  0.002086922060698271\n",
            "Epoch  29 Batch  364 / 525  Training Loss  0.001837050192989409\n",
            "Epoch  29 Batch  365 / 525  Training Loss  0.0006231412407942116\n",
            "Epoch  29 Batch  366 / 525  Training Loss  0.005389066878706217\n",
            "Epoch  29 Batch  367 / 525  Training Loss  0.014333544299006462\n",
            "Epoch  29 Batch  368 / 525  Training Loss  0.0025833328254520893\n",
            "Epoch  29 Batch  369 / 525  Training Loss  0.0016318000853061676\n",
            "Epoch  29 Batch  370 / 525  Training Loss  0.0015489915385842323\n",
            "Epoch  29 Batch  371 / 525  Training Loss  0.0029851526487618685\n",
            "Epoch  29 Batch  372 / 525  Training Loss  0.0007974550244398415\n",
            "Epoch  29 Batch  373 / 525  Training Loss  0.00842510350048542\n",
            "Epoch  29 Batch  374 / 525  Training Loss  0.005524227395653725\n",
            "Epoch  29 Batch  375 / 525  Training Loss  0.006368743721395731\n",
            "Epoch  29 Batch  376 / 525  Training Loss  0.0006393225630745292\n",
            "Epoch  29 Batch  377 / 525  Training Loss  0.0004801078175660223\n",
            "Epoch  29 Batch  378 / 525  Training Loss  0.006016437895596027\n",
            "Epoch  29 Batch  379 / 525  Training Loss  0.00037966336822137237\n",
            "Epoch  29 Batch  380 / 525  Training Loss  0.0007186859147623181\n",
            "Epoch  29 Batch  381 / 525  Training Loss  0.004595461767166853\n",
            "Epoch  29 Batch  382 / 525  Training Loss  0.004610829520970583\n",
            "Epoch  29 Batch  383 / 525  Training Loss  0.0053301588632166386\n",
            "Epoch  29 Batch  384 / 525  Training Loss  0.0035452365409582853\n",
            "Epoch  29 Batch  385 / 525  Training Loss  0.0009352938504889607\n",
            "Epoch  29 Batch  386 / 525  Training Loss  0.0042553371749818325\n",
            "Epoch  29 Batch  387 / 525  Training Loss  0.001005649915896356\n",
            "Epoch  29 Batch  388 / 525  Training Loss  0.00511901592835784\n",
            "Epoch  29 Batch  389 / 525  Training Loss  0.0011019574012607336\n",
            "Epoch  29 Batch  390 / 525  Training Loss  0.016660990193486214\n",
            "Epoch  29 Batch  391 / 525  Training Loss  0.00517947506159544\n",
            "Epoch  29 Batch  392 / 525  Training Loss  0.0010031805140897632\n",
            "Epoch  29 Batch  393 / 525  Training Loss  0.0006998383323661983\n",
            "Epoch  29 Batch  394 / 525  Training Loss  0.011400988325476646\n",
            "Epoch  29 Batch  395 / 525  Training Loss  0.0017338187899440527\n",
            "Epoch  29 Batch  396 / 525  Training Loss  0.0018187228124588728\n",
            "Epoch  29 Batch  397 / 525  Training Loss  0.0011602102313190699\n",
            "Epoch  29 Batch  398 / 525  Training Loss  0.0017016830388456583\n",
            "Epoch  29 Batch  399 / 525  Training Loss  0.0009740883251652122\n",
            "Epoch  29 Batch  400 / 525  Training Loss  0.004035580903291702\n",
            "Epoch  29 Batch  401 / 525  Training Loss  0.00329226884059608\n",
            "Epoch  29 Batch  402 / 525  Training Loss  0.001209184993058443\n",
            "Epoch  29 Batch  403 / 525  Training Loss  0.002715149661526084\n",
            "Epoch  29 Batch  404 / 525  Training Loss  0.003316310001537204\n",
            "Epoch  29 Batch  405 / 525  Training Loss  0.0006938759470358491\n",
            "Epoch  29 Batch  406 / 525  Training Loss  0.0023887597490102053\n",
            "Epoch  29 Batch  407 / 525  Training Loss  0.004452208522707224\n",
            "Epoch  29 Batch  408 / 525  Training Loss  0.0051424261182546616\n",
            "Epoch  29 Batch  409 / 525  Training Loss  0.002964557148516178\n",
            "Epoch  29 Batch  410 / 525  Training Loss  0.004399433732032776\n",
            "Epoch  29 Batch  411 / 525  Training Loss  0.0030788592994213104\n",
            "Epoch  29 Batch  412 / 525  Training Loss  0.0006329972529783845\n",
            "Epoch  29 Batch  413 / 525  Training Loss  0.0020518172532320023\n",
            "Epoch  29 Batch  414 / 525  Training Loss  0.0018582846969366074\n",
            "Epoch  29 Batch  415 / 525  Training Loss  0.0011773246806114912\n",
            "Epoch  29 Batch  416 / 525  Training Loss  0.0009258603677153587\n",
            "Epoch  29 Batch  417 / 525  Training Loss  0.004854243248701096\n",
            "Epoch  29 Batch  418 / 525  Training Loss  0.0008343554800376296\n",
            "Epoch  29 Batch  419 / 525  Training Loss  0.0036637415178120136\n",
            "Epoch  29 Batch  420 / 525  Training Loss  0.001186821493320167\n",
            "Epoch  29 Batch  421 / 525  Training Loss  0.0014337945031002164\n",
            "Epoch  29 Batch  422 / 525  Training Loss  0.0009309861925430596\n",
            "Epoch  29 Batch  423 / 525  Training Loss  0.0006211017025634646\n",
            "Epoch  29 Batch  424 / 525  Training Loss  0.0019184255506843328\n",
            "Epoch  29 Batch  425 / 525  Training Loss  0.0008145022438839078\n",
            "Epoch  29 Batch  426 / 525  Training Loss  0.001377289416268468\n",
            "Epoch  29 Batch  427 / 525  Training Loss  0.002247473457828164\n",
            "Epoch  29 Batch  428 / 525  Training Loss  0.0013355739647522569\n",
            "Epoch  29 Batch  429 / 525  Training Loss  0.00033570753294043243\n",
            "Epoch  29 Batch  430 / 525  Training Loss  0.0015709176659584045\n",
            "Epoch  29 Batch  431 / 525  Training Loss  0.0024974504485726357\n",
            "Epoch  29 Batch  432 / 525  Training Loss  0.001199355348944664\n",
            "Epoch  29 Batch  433 / 525  Training Loss  0.0009114829590544105\n",
            "Epoch  29 Batch  434 / 525  Training Loss  0.0024452777579426765\n",
            "Epoch  29 Batch  435 / 525  Training Loss  0.0017355792224407196\n",
            "Epoch  29 Batch  436 / 525  Training Loss  0.0011090473271906376\n",
            "Epoch  29 Batch  437 / 525  Training Loss  0.001128370058722794\n",
            "Epoch  29 Batch  438 / 525  Training Loss  0.0004179876414127648\n",
            "Epoch  29 Batch  439 / 525  Training Loss  0.0057154251262545586\n",
            "Epoch  29 Batch  440 / 525  Training Loss  0.0009059116127900779\n",
            "Epoch  29 Batch  441 / 525  Training Loss  0.00278283329680562\n",
            "Epoch  29 Batch  442 / 525  Training Loss  0.0022320756688714027\n",
            "Epoch  29 Batch  443 / 525  Training Loss  0.0031572163570672274\n",
            "Epoch  29 Batch  444 / 525  Training Loss  0.0005402895621955395\n",
            "Epoch  29 Batch  445 / 525  Training Loss  0.0027469417545944452\n",
            "Epoch  29 Batch  446 / 525  Training Loss  0.0031712118070572615\n",
            "Epoch  29 Batch  447 / 525  Training Loss  0.0010131607996299863\n",
            "Epoch  29 Batch  448 / 525  Training Loss  0.0009817542741075158\n",
            "Epoch  29 Batch  449 / 525  Training Loss  0.0003139498003292829\n",
            "Epoch  29 Batch  450 / 525  Training Loss  0.0005299245822243392\n",
            "Epoch  29 Batch  451 / 525  Training Loss  0.0004071354051120579\n",
            "Epoch  29 Batch  452 / 525  Training Loss  0.0008381626685149968\n",
            "Epoch  29 Batch  453 / 525  Training Loss  0.0014833194436505437\n",
            "Epoch  29 Batch  454 / 525  Training Loss  0.0014955943915992975\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  29 Batch  455 / 525  Training Loss  0.0004558594955597073\n",
            "Epoch  29 Batch  456 / 525  Training Loss  0.0007955768960528076\n",
            "Epoch  29 Batch  457 / 525  Training Loss  0.0008865506388247013\n",
            "Epoch  29 Batch  458 / 525  Training Loss  0.0009611574932932854\n",
            "Epoch  29 Batch  459 / 525  Training Loss  0.009923504665493965\n",
            "Epoch  29 Batch  460 / 525  Training Loss  0.0011563829611986876\n",
            "Epoch  29 Batch  461 / 525  Training Loss  0.00037453832919709384\n",
            "Epoch  29 Batch  462 / 525  Training Loss  0.0007057069451548159\n",
            "Epoch  29 Batch  463 / 525  Training Loss  0.0024572215043008327\n",
            "Epoch  29 Batch  464 / 525  Training Loss  0.0014715874567627907\n",
            "Epoch  29 Batch  465 / 525  Training Loss  0.001239144359715283\n",
            "Epoch  29 Batch  466 / 525  Training Loss  0.00034409170621074736\n",
            "Epoch  29 Batch  467 / 525  Training Loss  0.00018025939061772078\n",
            "Epoch  29 Batch  468 / 525  Training Loss  0.0017423670506104827\n",
            "Epoch  29 Batch  469 / 525  Training Loss  0.000836826569866389\n",
            "Epoch  29 Batch  470 / 525  Training Loss  0.0026593971997499466\n",
            "Epoch  29 Batch  471 / 525  Training Loss  0.0006324200658127666\n",
            "Epoch  29 Batch  472 / 525  Training Loss  0.003597468137741089\n",
            "Epoch  29 Batch  473 / 525  Training Loss  0.002839761320501566\n",
            "Epoch  29 Batch  474 / 525  Training Loss  0.004973195027559996\n",
            "Epoch  29 Batch  475 / 525  Training Loss  0.0013136878842487931\n",
            "Epoch  29 Batch  476 / 525  Training Loss  0.001291341264732182\n",
            "Epoch  29 Batch  477 / 525  Training Loss  0.0025445870123803616\n",
            "Epoch  29 Batch  478 / 525  Training Loss  0.0033551498781889677\n",
            "Epoch  29 Batch  479 / 525  Training Loss  0.007956443354487419\n",
            "Epoch  29 Batch  480 / 525  Training Loss  0.006420357618480921\n",
            "Epoch  29 Batch  481 / 525  Training Loss  0.0034882850013673306\n",
            "Epoch  29 Batch  482 / 525  Training Loss  0.0020068942103534937\n",
            "Epoch  29 Batch  483 / 525  Training Loss  0.00024053808010648936\n",
            "Epoch  29 Batch  484 / 525  Training Loss  0.00039363832911476493\n",
            "Epoch  29 Batch  485 / 525  Training Loss  0.0023759815376251936\n",
            "Epoch  29 Batch  486 / 525  Training Loss  0.0011628367938101292\n",
            "Epoch  29 Batch  487 / 525  Training Loss  0.0017946906154975295\n",
            "Epoch  29 Batch  488 / 525  Training Loss  0.004509656224399805\n",
            "Epoch  29 Batch  489 / 525  Training Loss  0.0020358222536742687\n",
            "Epoch  29 Batch  490 / 525  Training Loss  0.0009071471868082881\n",
            "Epoch  29 Batch  491 / 525  Training Loss  0.0018988288938999176\n",
            "Epoch  29 Batch  492 / 525  Training Loss  0.001977254170924425\n",
            "Epoch  29 Batch  493 / 525  Training Loss  0.0017045199638232589\n",
            "Epoch  29 Batch  494 / 525  Training Loss  0.001063178526237607\n",
            "Epoch  29 Batch  495 / 525  Training Loss  0.004522680304944515\n",
            "Epoch  29 Batch  496 / 525  Training Loss  0.002532272133976221\n",
            "Epoch  29 Batch  497 / 525  Training Loss  0.00264722784049809\n",
            "Epoch  29 Batch  498 / 525  Training Loss  0.0011434765765443444\n",
            "Epoch  29 Batch  499 / 525  Training Loss  0.0012257647467777133\n",
            "Epoch  29 Batch  500 / 525  Training Loss  0.003192910458892584\n",
            "Epoch  29 Batch  501 / 525  Training Loss  0.00046722954721190035\n",
            "Epoch  29 Batch  502 / 525  Training Loss  0.0008512021158821881\n",
            "Epoch  29 Batch  503 / 525  Training Loss  0.0017744142096489668\n",
            "Epoch  29 Batch  504 / 525  Training Loss  0.000567861192394048\n",
            "Epoch  29 Batch  505 / 525  Training Loss  0.0004711711371783167\n",
            "Epoch  29 Batch  506 / 525  Training Loss  0.0010364956688135862\n",
            "Epoch  29 Batch  507 / 525  Training Loss  0.006805458106100559\n",
            "Epoch  29 Batch  508 / 525  Training Loss  0.000587871007155627\n",
            "Epoch  29 Batch  509 / 525  Training Loss  0.003134454134851694\n",
            "Epoch  29 Batch  510 / 525  Training Loss  0.00106701604090631\n",
            "Epoch  29 Batch  511 / 525  Training Loss  0.0018732817843556404\n",
            "Epoch  29 Batch  512 / 525  Training Loss  0.0006273463950492442\n",
            "Epoch  29 Batch  513 / 525  Training Loss  0.0006756257498636842\n",
            "Epoch  29 Batch  514 / 525  Training Loss  0.0023945400025695562\n",
            "Epoch  29 Batch  515 / 525  Training Loss  0.0021709396969527006\n",
            "Epoch  29 Batch  516 / 525  Training Loss  0.003908876329660416\n",
            "Epoch  29 Batch  517 / 525  Training Loss  0.0016080125933513045\n",
            "Epoch  29 Batch  518 / 525  Training Loss  0.0016398841980844736\n",
            "Epoch  29 Batch  519 / 525  Training Loss  0.0011824864195659757\n",
            "Epoch  29 Batch  520 / 525  Training Loss  0.012849666178226471\n",
            "Epoch  29 Batch  521 / 525  Training Loss  0.001941798604093492\n",
            "Epoch  29 Batch  522 / 525  Training Loss  0.0017565276939421892\n",
            "Epoch  29 Batch  523 / 525  Training Loss  0.0036723469384014606\n",
            "Epoch  29 Batch  524 / 525  Training Loss  0.008375907316803932\n",
            "  30    |    -    |   0.002881   | 60.658333\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 30\n",
            "Epoch  30 Batch  0 / 525  Training Loss  0.0004978612414561212\n",
            "Epoch  30 Batch  1 / 525  Training Loss  0.0021322406828403473\n",
            "Epoch  30 Batch  2 / 525  Training Loss  0.0011344390222802758\n",
            "Epoch  30 Batch  3 / 525  Training Loss  0.0006089297239668667\n",
            "Epoch  30 Batch  4 / 525  Training Loss  0.0016174374613910913\n",
            "Epoch  30 Batch  5 / 525  Training Loss  0.0007489918498322368\n",
            "Epoch  30 Batch  6 / 525  Training Loss  0.0006225726683624089\n",
            "Epoch  30 Batch  7 / 525  Training Loss  0.0003317285736557096\n",
            "Epoch  30 Batch  8 / 525  Training Loss  0.00042095076059922576\n",
            "Epoch  30 Batch  9 / 525  Training Loss  0.0013129652943462133\n",
            "Epoch  30 Batch  10 / 525  Training Loss  0.0007227205205708742\n",
            "Epoch  30 Batch  11 / 525  Training Loss  0.0007193448254838586\n",
            "Epoch  30 Batch  12 / 525  Training Loss  0.0035423175431787968\n",
            "Epoch  30 Batch  13 / 525  Training Loss  0.0011302984785288572\n",
            "Epoch  30 Batch  14 / 525  Training Loss  0.0005567438201978803\n",
            "Epoch  30 Batch  15 / 525  Training Loss  0.0006977708544582129\n",
            "Epoch  30 Batch  16 / 525  Training Loss  0.0005934784421697259\n",
            "Epoch  30 Batch  17 / 525  Training Loss  0.0003663049719762057\n",
            "Epoch  30 Batch  18 / 525  Training Loss  0.0023311374243348837\n",
            "Epoch  30 Batch  19 / 525  Training Loss  0.0010257146786898375\n",
            "Epoch  30 Batch  20 / 525  Training Loss  0.003267662599682808\n",
            "Epoch  30 Batch  21 / 525  Training Loss  0.0007077313493937254\n",
            "Epoch  30 Batch  22 / 525  Training Loss  0.0001273804227821529\n",
            "Epoch  30 Batch  23 / 525  Training Loss  0.0016461843624711037\n",
            "Epoch  30 Batch  24 / 525  Training Loss  0.0016582002863287926\n",
            "Epoch  30 Batch  25 / 525  Training Loss  0.002247877884656191\n",
            "Epoch  30 Batch  26 / 525  Training Loss  0.0017967451130971313\n",
            "Epoch  30 Batch  27 / 525  Training Loss  0.0004790620878338814\n",
            "Epoch  30 Batch  28 / 525  Training Loss  0.001370400539599359\n",
            "Epoch  30 Batch  29 / 525  Training Loss  0.001234622672200203\n",
            "Epoch  30 Batch  30 / 525  Training Loss  0.001484134467318654\n",
            "Epoch  30 Batch  31 / 525  Training Loss  0.0031878422014415264\n",
            "Epoch  30 Batch  32 / 525  Training Loss  0.0018169510876759887\n",
            "Epoch  30 Batch  33 / 525  Training Loss  0.0008584485622122884\n",
            "Epoch  30 Batch  34 / 525  Training Loss  0.0012583096977323294\n",
            "Epoch  30 Batch  35 / 525  Training Loss  0.001311778323724866\n",
            "Epoch  30 Batch  36 / 525  Training Loss  0.0009216500329785049\n",
            "Epoch  30 Batch  37 / 525  Training Loss  0.0009694177424535155\n",
            "Epoch  30 Batch  38 / 525  Training Loss  0.0004563715192489326\n",
            "Epoch  30 Batch  39 / 525  Training Loss  0.00094180169980973\n",
            "Epoch  30 Batch  40 / 525  Training Loss  0.000468299665953964\n",
            "Epoch  30 Batch  41 / 525  Training Loss  0.00036325090331956744\n",
            "Epoch  30 Batch  42 / 525  Training Loss  0.0002234505518572405\n",
            "Epoch  30 Batch  43 / 525  Training Loss  0.0001982544781640172\n",
            "Epoch  30 Batch  44 / 525  Training Loss  0.0001870267151389271\n",
            "Epoch  30 Batch  45 / 525  Training Loss  0.000433669047197327\n",
            "Epoch  30 Batch  46 / 525  Training Loss  0.0004619565443135798\n",
            "Epoch  30 Batch  47 / 525  Training Loss  0.0021451252978295088\n",
            "Epoch  30 Batch  48 / 525  Training Loss  0.001027659047394991\n",
            "Epoch  30 Batch  49 / 525  Training Loss  0.002210223115980625\n",
            "Epoch  30 Batch  50 / 525  Training Loss  0.00023932366457302123\n",
            "Epoch  30 Batch  51 / 525  Training Loss  0.0012599568581208587\n",
            "Epoch  30 Batch  52 / 525  Training Loss  0.005774240009486675\n",
            "Epoch  30 Batch  53 / 525  Training Loss  0.0011989036574959755\n",
            "Epoch  30 Batch  54 / 525  Training Loss  0.0014363580849021673\n",
            "Epoch  30 Batch  55 / 525  Training Loss  0.0006345574511215091\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  30 Batch  56 / 525  Training Loss  0.0008845880511216819\n",
            "Epoch  30 Batch  57 / 525  Training Loss  0.0007805619388818741\n",
            "Epoch  30 Batch  58 / 525  Training Loss  0.0007754226098768413\n",
            "Epoch  30 Batch  59 / 525  Training Loss  0.0007151378085836768\n",
            "Epoch  30 Batch  60 / 525  Training Loss  0.0005195960984565318\n",
            "Epoch  30 Batch  61 / 525  Training Loss  0.0008908546296879649\n",
            "Epoch  30 Batch  62 / 525  Training Loss  0.0009711119346320629\n",
            "Epoch  30 Batch  63 / 525  Training Loss  0.0015820810804143548\n",
            "Epoch  30 Batch  64 / 525  Training Loss  0.0007843592902645469\n",
            "Epoch  30 Batch  65 / 525  Training Loss  0.0011835445184260607\n",
            "Epoch  30 Batch  66 / 525  Training Loss  0.0006247240817174315\n",
            "Epoch  30 Batch  67 / 525  Training Loss  0.0011764793889597058\n",
            "Epoch  30 Batch  68 / 525  Training Loss  0.000321677653118968\n",
            "Epoch  30 Batch  69 / 525  Training Loss  0.0033696580212563276\n",
            "Epoch  30 Batch  70 / 525  Training Loss  0.0006115046562626958\n",
            "Epoch  30 Batch  71 / 525  Training Loss  0.00021495029795914888\n",
            "Epoch  30 Batch  72 / 525  Training Loss  0.0008749679545871913\n",
            "Epoch  30 Batch  73 / 525  Training Loss  0.0003062825999222696\n",
            "Epoch  30 Batch  74 / 525  Training Loss  0.00043540154001675546\n",
            "Epoch  30 Batch  75 / 525  Training Loss  0.0013262111460790038\n",
            "Epoch  30 Batch  76 / 525  Training Loss  0.005741181783378124\n",
            "Epoch  30 Batch  77 / 525  Training Loss  0.004403723869472742\n",
            "Epoch  30 Batch  78 / 525  Training Loss  0.0008616310660727322\n",
            "Epoch  30 Batch  79 / 525  Training Loss  0.0009502125903964043\n",
            "Epoch  30 Batch  80 / 525  Training Loss  0.0002688343229237944\n",
            "Epoch  30 Batch  81 / 525  Training Loss  0.0002953306247945875\n",
            "Epoch  30 Batch  82 / 525  Training Loss  0.000517105043400079\n",
            "Epoch  30 Batch  83 / 525  Training Loss  0.00023609113122802228\n",
            "Epoch  30 Batch  84 / 525  Training Loss  0.0005316106835380197\n",
            "Epoch  30 Batch  85 / 525  Training Loss  0.0005344421369954944\n",
            "Epoch  30 Batch  86 / 525  Training Loss  0.003644109470769763\n",
            "Epoch  30 Batch  87 / 525  Training Loss  0.001849376829341054\n",
            "Epoch  30 Batch  88 / 525  Training Loss  0.0011953990906476974\n",
            "Epoch  30 Batch  89 / 525  Training Loss  0.00025385821936652064\n",
            "Epoch  30 Batch  90 / 525  Training Loss  0.0009869179921224713\n",
            "Epoch  30 Batch  91 / 525  Training Loss  0.015655646100640297\n",
            "Epoch  30 Batch  92 / 525  Training Loss  0.00041556995711289346\n",
            "Epoch  30 Batch  93 / 525  Training Loss  0.00016592931933701038\n",
            "Epoch  30 Batch  94 / 525  Training Loss  0.0014891421888023615\n",
            "Epoch  30 Batch  95 / 525  Training Loss  0.004874409642070532\n",
            "Epoch  30 Batch  96 / 525  Training Loss  0.004988412838429213\n",
            "Epoch  30 Batch  97 / 525  Training Loss  0.00043338589603081346\n",
            "Epoch  30 Batch  98 / 525  Training Loss  0.0008323475485667586\n",
            "Epoch  30 Batch  99 / 525  Training Loss  0.0024219530168920755\n",
            "Epoch  30 Batch  100 / 525  Training Loss  0.00013966020196676254\n",
            "Epoch  30 Batch  101 / 525  Training Loss  0.0008362127700820565\n",
            "Epoch  30 Batch  102 / 525  Training Loss  0.001141563756391406\n",
            "Epoch  30 Batch  103 / 525  Training Loss  0.00044331984827294946\n",
            "Epoch  30 Batch  104 / 525  Training Loss  0.0011546009918674827\n",
            "Epoch  30 Batch  105 / 525  Training Loss  0.006259258836507797\n",
            "Epoch  30 Batch  106 / 525  Training Loss  0.004387001972645521\n",
            "Epoch  30 Batch  107 / 525  Training Loss  0.001520879683084786\n",
            "Epoch  30 Batch  108 / 525  Training Loss  0.0027639034669846296\n",
            "Epoch  30 Batch  109 / 525  Training Loss  0.0006187160615809262\n",
            "Epoch  30 Batch  110 / 525  Training Loss  0.0008212264510802925\n",
            "Epoch  30 Batch  111 / 525  Training Loss  0.000752854801248759\n",
            "Epoch  30 Batch  112 / 525  Training Loss  0.0050706397742033005\n",
            "Epoch  30 Batch  113 / 525  Training Loss  0.0012580219190567732\n",
            "Epoch  30 Batch  114 / 525  Training Loss  0.0014500515535473824\n",
            "Epoch  30 Batch  115 / 525  Training Loss  0.00044994158088229597\n",
            "Epoch  30 Batch  116 / 525  Training Loss  0.0005760182393714786\n",
            "Epoch  30 Batch  117 / 525  Training Loss  0.00026062194956466556\n",
            "Epoch  30 Batch  118 / 525  Training Loss  0.0016068331897258759\n",
            "Epoch  30 Batch  119 / 525  Training Loss  0.0004342006577644497\n",
            "Epoch  30 Batch  120 / 525  Training Loss  0.0010406406363472342\n",
            "Epoch  30 Batch  121 / 525  Training Loss  0.0002868819865398109\n",
            "Epoch  30 Batch  122 / 525  Training Loss  0.0004030719865113497\n",
            "Epoch  30 Batch  123 / 525  Training Loss  0.0016534235328435898\n",
            "Epoch  30 Batch  124 / 525  Training Loss  0.015286630019545555\n",
            "Epoch  30 Batch  125 / 525  Training Loss  0.004610070027410984\n",
            "Epoch  30 Batch  126 / 525  Training Loss  0.001381142414174974\n",
            "Epoch  30 Batch  127 / 525  Training Loss  0.0005059635732322931\n",
            "Epoch  30 Batch  128 / 525  Training Loss  0.003968226257711649\n",
            "Epoch  30 Batch  129 / 525  Training Loss  0.0013412714470177889\n",
            "Epoch  30 Batch  130 / 525  Training Loss  0.004105974920094013\n",
            "Epoch  30 Batch  131 / 525  Training Loss  0.002670691814273596\n",
            "Epoch  30 Batch  132 / 525  Training Loss  0.0003026131307706237\n",
            "Epoch  30 Batch  133 / 525  Training Loss  0.008575383573770523\n",
            "Epoch  30 Batch  134 / 525  Training Loss  0.0010026289382949471\n",
            "Epoch  30 Batch  135 / 525  Training Loss  0.002059781225398183\n",
            "Epoch  30 Batch  136 / 525  Training Loss  0.0007856754818931222\n",
            "Epoch  30 Batch  137 / 525  Training Loss  0.00041922443779185414\n",
            "Epoch  30 Batch  138 / 525  Training Loss  0.000675278773996979\n",
            "Epoch  30 Batch  139 / 525  Training Loss  0.00044682310544885695\n",
            "Epoch  30 Batch  140 / 525  Training Loss  0.000366530439350754\n",
            "Epoch  30 Batch  141 / 525  Training Loss  0.002382370876148343\n",
            "Epoch  30 Batch  142 / 525  Training Loss  0.0018061179434880614\n",
            "Epoch  30 Batch  143 / 525  Training Loss  0.0015724448021501303\n",
            "Epoch  30 Batch  144 / 525  Training Loss  0.0005820034421049058\n",
            "Epoch  30 Batch  145 / 525  Training Loss  0.0006928417133167386\n",
            "Epoch  30 Batch  146 / 525  Training Loss  0.0076080067083239555\n",
            "Epoch  30 Batch  147 / 525  Training Loss  0.0030286156106740236\n",
            "Epoch  30 Batch  148 / 525  Training Loss  0.00033051142236217856\n",
            "Epoch  30 Batch  149 / 525  Training Loss  0.0011651921086013317\n",
            "Epoch  30 Batch  150 / 525  Training Loss  0.000986385392025113\n",
            "Epoch  30 Batch  151 / 525  Training Loss  0.0006111797993071377\n",
            "Epoch  30 Batch  152 / 525  Training Loss  0.002844660310074687\n",
            "Epoch  30 Batch  153 / 525  Training Loss  0.00024106944329105318\n",
            "Epoch  30 Batch  154 / 525  Training Loss  0.0017608763882890344\n",
            "Epoch  30 Batch  155 / 525  Training Loss  0.0029970004688948393\n",
            "Epoch  30 Batch  156 / 525  Training Loss  0.006660121493041515\n",
            "Epoch  30 Batch  157 / 525  Training Loss  0.00022124095994513482\n",
            "Epoch  30 Batch  158 / 525  Training Loss  0.0016230558976531029\n",
            "Epoch  30 Batch  159 / 525  Training Loss  0.0011572323273867369\n",
            "Epoch  30 Batch  160 / 525  Training Loss  0.0004890966811217368\n",
            "Epoch  30 Batch  161 / 525  Training Loss  0.00042087826295755804\n",
            "Epoch  30 Batch  162 / 525  Training Loss  0.004337417893111706\n",
            "Epoch  30 Batch  163 / 525  Training Loss  0.0015368901658803225\n",
            "Epoch  30 Batch  164 / 525  Training Loss  0.0008483959245495498\n",
            "Epoch  30 Batch  165 / 525  Training Loss  0.00047505428665317595\n",
            "Epoch  30 Batch  166 / 525  Training Loss  0.003906780853867531\n",
            "Epoch  30 Batch  167 / 525  Training Loss  0.012068429961800575\n",
            "Epoch  30 Batch  168 / 525  Training Loss  0.0015295346966013312\n",
            "Epoch  30 Batch  169 / 525  Training Loss  0.000791826460044831\n",
            "Epoch  30 Batch  170 / 525  Training Loss  0.0006620910717174411\n",
            "Epoch  30 Batch  171 / 525  Training Loss  0.000539866101462394\n",
            "Epoch  30 Batch  172 / 525  Training Loss  0.0007626411388628185\n",
            "Epoch  30 Batch  173 / 525  Training Loss  0.0007862914353609085\n",
            "Epoch  30 Batch  174 / 525  Training Loss  0.006432197988033295\n",
            "Epoch  30 Batch  175 / 525  Training Loss  0.0016920730704441667\n",
            "Epoch  30 Batch  176 / 525  Training Loss  0.00039405463030561805\n",
            "Epoch  30 Batch  177 / 525  Training Loss  0.0011012934846803546\n",
            "Epoch  30 Batch  178 / 525  Training Loss  0.0009236264158971608\n",
            "Epoch  30 Batch  179 / 525  Training Loss  0.0005832619499415159\n",
            "Epoch  30 Batch  180 / 525  Training Loss  0.0007308116182684898\n",
            "Epoch  30 Batch  181 / 525  Training Loss  0.0004452834837138653\n",
            "Epoch  30 Batch  182 / 525  Training Loss  0.0019199189264327288\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  30 Batch  183 / 525  Training Loss  0.0020903691183775663\n",
            "Epoch  30 Batch  184 / 525  Training Loss  0.0006436628173105419\n",
            "Epoch  30 Batch  185 / 525  Training Loss  0.005525769200176001\n",
            "Epoch  30 Batch  186 / 525  Training Loss  0.0005705446819774806\n",
            "Epoch  30 Batch  187 / 525  Training Loss  0.0011481218971312046\n",
            "Epoch  30 Batch  188 / 525  Training Loss  0.002374271396547556\n",
            "Epoch  30 Batch  189 / 525  Training Loss  0.002251980360597372\n",
            "Epoch  30 Batch  190 / 525  Training Loss  0.000674100243486464\n",
            "Epoch  30 Batch  191 / 525  Training Loss  0.0037917904555797577\n",
            "Epoch  30 Batch  192 / 525  Training Loss  0.0030389553867280483\n",
            "Epoch  30 Batch  193 / 525  Training Loss  0.004329481162130833\n",
            "Epoch  30 Batch  194 / 525  Training Loss  0.0010135590564459562\n",
            "Epoch  30 Batch  195 / 525  Training Loss  0.0003982087946496904\n",
            "Epoch  30 Batch  196 / 525  Training Loss  0.001798286335542798\n",
            "Epoch  30 Batch  197 / 525  Training Loss  0.004477612674236298\n",
            "Epoch  30 Batch  198 / 525  Training Loss  0.0005545175517909229\n",
            "Epoch  30 Batch  199 / 525  Training Loss  0.00027254471206106246\n",
            "Epoch  30 Batch  200 / 525  Training Loss  0.0025687445886433125\n",
            "Epoch  30 Batch  201 / 525  Training Loss  0.0015020661521703005\n",
            "Epoch  30 Batch  202 / 525  Training Loss  0.00034640973899513483\n",
            "Epoch  30 Batch  203 / 525  Training Loss  0.0004269331111572683\n",
            "Epoch  30 Batch  204 / 525  Training Loss  0.003564470214769244\n",
            "Epoch  30 Batch  205 / 525  Training Loss  0.0027872209902852774\n",
            "Epoch  30 Batch  206 / 525  Training Loss  0.0006200777716003358\n",
            "Epoch  30 Batch  207 / 525  Training Loss  0.0006099210004322231\n",
            "Epoch  30 Batch  208 / 525  Training Loss  0.002517650369554758\n",
            "Epoch  30 Batch  209 / 525  Training Loss  0.0034797198604792356\n",
            "Epoch  30 Batch  210 / 525  Training Loss  0.004662768449634314\n",
            "Epoch  30 Batch  211 / 525  Training Loss  0.0023635919205844402\n",
            "Epoch  30 Batch  212 / 525  Training Loss  0.000582017470151186\n",
            "Epoch  30 Batch  213 / 525  Training Loss  0.0011750258272513747\n",
            "Epoch  30 Batch  214 / 525  Training Loss  0.0016154611948877573\n",
            "Epoch  30 Batch  215 / 525  Training Loss  0.00130584673024714\n",
            "Epoch  30 Batch  216 / 525  Training Loss  0.0006951259565539658\n",
            "Epoch  30 Batch  217 / 525  Training Loss  0.001872271648608148\n",
            "Epoch  30 Batch  218 / 525  Training Loss  0.000608803064096719\n",
            "Epoch  30 Batch  219 / 525  Training Loss  0.0009383646538481116\n",
            "Epoch  30 Batch  220 / 525  Training Loss  0.003944650758057833\n",
            "Epoch  30 Batch  221 / 525  Training Loss  0.0006656333571299911\n",
            "Epoch  30 Batch  222 / 525  Training Loss  0.001955429557710886\n",
            "Epoch  30 Batch  223 / 525  Training Loss  0.00028525758534669876\n",
            "Epoch  30 Batch  224 / 525  Training Loss  0.002039390616118908\n",
            "Epoch  30 Batch  225 / 525  Training Loss  0.0014854985056445003\n",
            "Epoch  30 Batch  226 / 525  Training Loss  0.0008174522663466632\n",
            "Epoch  30 Batch  227 / 525  Training Loss  0.0005832918686792254\n",
            "Epoch  30 Batch  228 / 525  Training Loss  0.0007277327822521329\n",
            "Epoch  30 Batch  229 / 525  Training Loss  0.0003896342823281884\n",
            "Epoch  30 Batch  230 / 525  Training Loss  0.00043933960841968656\n",
            "Epoch  30 Batch  231 / 525  Training Loss  0.0002562537556514144\n",
            "Epoch  30 Batch  232 / 525  Training Loss  0.0015390269691124558\n",
            "Epoch  30 Batch  233 / 525  Training Loss  0.0007936998154036701\n",
            "Epoch  30 Batch  234 / 525  Training Loss  0.0039518484845757484\n",
            "Epoch  30 Batch  235 / 525  Training Loss  0.0009648146806284785\n",
            "Epoch  30 Batch  236 / 525  Training Loss  0.00039837928488850594\n",
            "Epoch  30 Batch  237 / 525  Training Loss  0.0004817050357814878\n",
            "Epoch  30 Batch  238 / 525  Training Loss  0.00037494770367629826\n",
            "Epoch  30 Batch  239 / 525  Training Loss  0.000667397805955261\n",
            "Epoch  30 Batch  240 / 525  Training Loss  0.0006055353442206979\n",
            "Epoch  30 Batch  241 / 525  Training Loss  0.00064789445605129\n",
            "Epoch  30 Batch  242 / 525  Training Loss  0.002889450406655669\n",
            "Epoch  30 Batch  243 / 525  Training Loss  0.0012503124307841063\n",
            "Epoch  30 Batch  244 / 525  Training Loss  0.000344438711181283\n",
            "Epoch  30 Batch  245 / 525  Training Loss  0.0002935473166871816\n",
            "Epoch  30 Batch  246 / 525  Training Loss  0.0004397198208607733\n",
            "Epoch  30 Batch  247 / 525  Training Loss  0.00036449561594054103\n",
            "Epoch  30 Batch  248 / 525  Training Loss  0.006783881224691868\n",
            "Epoch  30 Batch  249 / 525  Training Loss  0.00976224523037672\n",
            "Epoch  30 Batch  250 / 525  Training Loss  0.0059256600216031075\n",
            "Epoch  30 Batch  251 / 525  Training Loss  0.0005740531487390399\n",
            "Epoch  30 Batch  252 / 525  Training Loss  0.0004333210235927254\n",
            "Epoch  30 Batch  253 / 525  Training Loss  0.0003885035985149443\n",
            "Epoch  30 Batch  254 / 525  Training Loss  0.0020068464800715446\n",
            "Epoch  30 Batch  255 / 525  Training Loss  0.0018259665230289102\n",
            "Epoch  30 Batch  256 / 525  Training Loss  0.0015787973534315825\n",
            "Epoch  30 Batch  257 / 525  Training Loss  0.0009879230055958033\n",
            "Epoch  30 Batch  258 / 525  Training Loss  0.004467979539185762\n",
            "Epoch  30 Batch  259 / 525  Training Loss  0.0012911423109471798\n",
            "Epoch  30 Batch  260 / 525  Training Loss  0.0011159442365169525\n",
            "Epoch  30 Batch  261 / 525  Training Loss  0.0008061282569542527\n",
            "Epoch  30 Batch  262 / 525  Training Loss  0.0014298380119726062\n",
            "Epoch  30 Batch  263 / 525  Training Loss  0.0006580267217941582\n",
            "Epoch  30 Batch  264 / 525  Training Loss  0.0008671687683090568\n",
            "Epoch  30 Batch  265 / 525  Training Loss  0.0005800729268230498\n",
            "Epoch  30 Batch  266 / 525  Training Loss  0.001134598278440535\n",
            "Epoch  30 Batch  267 / 525  Training Loss  0.0005480097606778145\n",
            "Epoch  30 Batch  268 / 525  Training Loss  0.00039545135223306715\n",
            "Epoch  30 Batch  269 / 525  Training Loss  0.0026396929752081633\n",
            "Epoch  30 Batch  270 / 525  Training Loss  0.0010475661838427186\n",
            "Epoch  30 Batch  271 / 525  Training Loss  0.0008730387198738754\n",
            "Epoch  30 Batch  272 / 525  Training Loss  0.005997635889798403\n",
            "Epoch  30 Batch  273 / 525  Training Loss  0.0009707499411888421\n",
            "Epoch  30 Batch  274 / 525  Training Loss  0.002835267223417759\n",
            "Epoch  30 Batch  275 / 525  Training Loss  0.0011806932743638754\n",
            "Epoch  30 Batch  276 / 525  Training Loss  0.0012743965489789844\n",
            "Epoch  30 Batch  277 / 525  Training Loss  0.0008783843368291855\n",
            "Epoch  30 Batch  278 / 525  Training Loss  0.0012740028323605657\n",
            "Epoch  30 Batch  279 / 525  Training Loss  0.0004974553594365716\n",
            "Epoch  30 Batch  280 / 525  Training Loss  0.0017701834440231323\n",
            "Epoch  30 Batch  281 / 525  Training Loss  0.0034086699597537518\n",
            "Epoch  30 Batch  282 / 525  Training Loss  0.0006040757289156318\n",
            "Epoch  30 Batch  283 / 525  Training Loss  0.0009207301773130894\n",
            "Epoch  30 Batch  284 / 525  Training Loss  0.001776665449142456\n",
            "Epoch  30 Batch  285 / 525  Training Loss  0.0004668661276809871\n",
            "Epoch  30 Batch  286 / 525  Training Loss  0.008958973921835423\n",
            "Epoch  30 Batch  287 / 525  Training Loss  0.004543331917375326\n",
            "Epoch  30 Batch  288 / 525  Training Loss  0.0005678621819242835\n",
            "Epoch  30 Batch  289 / 525  Training Loss  0.0032970462925732136\n",
            "Epoch  30 Batch  290 / 525  Training Loss  0.000569579133298248\n",
            "Epoch  30 Batch  291 / 525  Training Loss  0.0013863106723874807\n",
            "Epoch  30 Batch  292 / 525  Training Loss  0.001091271871700883\n",
            "Epoch  30 Batch  293 / 525  Training Loss  0.00046954414574429393\n",
            "Epoch  30 Batch  294 / 525  Training Loss  0.0031030471436679363\n",
            "Epoch  30 Batch  295 / 525  Training Loss  0.001847169129177928\n",
            "Epoch  30 Batch  296 / 525  Training Loss  0.004613278899341822\n",
            "Epoch  30 Batch  297 / 525  Training Loss  0.0044000111520290375\n",
            "Epoch  30 Batch  298 / 525  Training Loss  0.0028706523589789867\n",
            "Epoch  30 Batch  299 / 525  Training Loss  0.0012252232991158962\n",
            "Epoch  30 Batch  300 / 525  Training Loss  0.0008628169307485223\n",
            "Epoch  30 Batch  301 / 525  Training Loss  0.0007281957077793777\n",
            "Epoch  30 Batch  302 / 525  Training Loss  0.0004833280108869076\n",
            "Epoch  30 Batch  303 / 525  Training Loss  0.0034713614732027054\n",
            "Epoch  30 Batch  304 / 525  Training Loss  0.006148066371679306\n",
            "Epoch  30 Batch  305 / 525  Training Loss  0.0016809608787298203\n",
            "Epoch  30 Batch  306 / 525  Training Loss  0.008396323770284653\n",
            "Epoch  30 Batch  307 / 525  Training Loss  0.005542648024857044\n",
            "Epoch  30 Batch  308 / 525  Training Loss  0.006165329832583666\n",
            "Epoch  30 Batch  309 / 525  Training Loss  0.0022302749566733837\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  30 Batch  310 / 525  Training Loss  0.0004624124267138541\n",
            "Epoch  30 Batch  311 / 525  Training Loss  0.0004881170461885631\n",
            "Epoch  30 Batch  312 / 525  Training Loss  0.0042442334815859795\n",
            "Epoch  30 Batch  313 / 525  Training Loss  0.020452668890357018\n",
            "Epoch  30 Batch  314 / 525  Training Loss  0.01120167225599289\n",
            "Epoch  30 Batch  315 / 525  Training Loss  0.003893537912517786\n",
            "Epoch  30 Batch  316 / 525  Training Loss  0.0013292604126036167\n",
            "Epoch  30 Batch  317 / 525  Training Loss  0.0016942915972322226\n",
            "Epoch  30 Batch  318 / 525  Training Loss  0.0028057601302862167\n",
            "Epoch  30 Batch  319 / 525  Training Loss  0.0011253616539761424\n",
            "Epoch  30 Batch  320 / 525  Training Loss  0.005091952625662088\n",
            "Epoch  30 Batch  321 / 525  Training Loss  0.0036227605305612087\n",
            "Epoch  30 Batch  322 / 525  Training Loss  0.0012549541424959898\n",
            "Epoch  30 Batch  323 / 525  Training Loss  0.0036706323735415936\n",
            "Epoch  30 Batch  324 / 525  Training Loss  0.0008395340410061181\n",
            "Epoch  30 Batch  325 / 525  Training Loss  0.0029296893626451492\n",
            "Epoch  30 Batch  326 / 525  Training Loss  0.010728448629379272\n",
            "Epoch  30 Batch  327 / 525  Training Loss  0.002361173741519451\n",
            "Epoch  30 Batch  328 / 525  Training Loss  0.00207704259082675\n",
            "Epoch  30 Batch  329 / 525  Training Loss  0.002894760575145483\n",
            "Epoch  30 Batch  330 / 525  Training Loss  0.011594267562031746\n",
            "Epoch  30 Batch  331 / 525  Training Loss  0.0018695773323997855\n",
            "Epoch  30 Batch  332 / 525  Training Loss  0.0028519730549305677\n",
            "Epoch  30 Batch  333 / 525  Training Loss  0.0027330999728292227\n",
            "Epoch  30 Batch  334 / 525  Training Loss  0.0028714032378047705\n",
            "Epoch  30 Batch  335 / 525  Training Loss  0.003167607355862856\n",
            "Epoch  30 Batch  336 / 525  Training Loss  0.004486939869821072\n",
            "Epoch  30 Batch  337 / 525  Training Loss  0.0006256805500015616\n",
            "Epoch  30 Batch  338 / 525  Training Loss  0.004468727391213179\n",
            "Epoch  30 Batch  339 / 525  Training Loss  0.0009008579072542489\n",
            "Epoch  30 Batch  340 / 525  Training Loss  0.017615098506212234\n",
            "Epoch  30 Batch  341 / 525  Training Loss  0.002492145635187626\n",
            "Epoch  30 Batch  342 / 525  Training Loss  0.016315005719661713\n",
            "Epoch  30 Batch  343 / 525  Training Loss  0.020205361768603325\n",
            "Epoch  30 Batch  344 / 525  Training Loss  0.04232250899076462\n",
            "Epoch  30 Batch  345 / 525  Training Loss  0.004457223229110241\n",
            "Epoch  30 Batch  346 / 525  Training Loss  0.033668823540210724\n",
            "Epoch  30 Batch  347 / 525  Training Loss  0.0015970144886523485\n",
            "Epoch  30 Batch  348 / 525  Training Loss  0.010307359509170055\n",
            "Epoch  30 Batch  349 / 525  Training Loss  0.007597996853291988\n",
            "Epoch  30 Batch  350 / 525  Training Loss  0.004586145281791687\n",
            "Epoch  30 Batch  351 / 525  Training Loss  0.005352037493139505\n",
            "Epoch  30 Batch  352 / 525  Training Loss  0.0021449076011776924\n",
            "Epoch  30 Batch  353 / 525  Training Loss  0.002250825520604849\n",
            "Epoch  30 Batch  354 / 525  Training Loss  0.01974709890782833\n",
            "Epoch  30 Batch  355 / 525  Training Loss  0.002675487892702222\n",
            "Epoch  30 Batch  356 / 525  Training Loss  0.001980480272322893\n",
            "Epoch  30 Batch  357 / 525  Training Loss  0.0009565882501192391\n",
            "Epoch  30 Batch  358 / 525  Training Loss  0.0012369781034067273\n",
            "Epoch  30 Batch  359 / 525  Training Loss  0.00657821586355567\n",
            "Epoch  30 Batch  360 / 525  Training Loss  0.0011120851850137115\n",
            "Epoch  30 Batch  361 / 525  Training Loss  0.002188321901485324\n",
            "Epoch  30 Batch  362 / 525  Training Loss  0.0031981258653104305\n",
            "Epoch  30 Batch  363 / 525  Training Loss  0.017608527094125748\n",
            "Epoch  30 Batch  364 / 525  Training Loss  0.0014150881906971335\n",
            "Epoch  30 Batch  365 / 525  Training Loss  0.003957225941121578\n",
            "Epoch  30 Batch  366 / 525  Training Loss  0.01227705180644989\n",
            "Epoch  30 Batch  367 / 525  Training Loss  0.0016083146911114454\n",
            "Epoch  30 Batch  368 / 525  Training Loss  0.002084048930555582\n",
            "Epoch  30 Batch  369 / 525  Training Loss  0.0017116082599386573\n",
            "Epoch  30 Batch  370 / 525  Training Loss  0.00298164295963943\n",
            "Epoch  30 Batch  371 / 525  Training Loss  0.0011738204630091786\n",
            "Epoch  30 Batch  372 / 525  Training Loss  0.0028043012134730816\n",
            "Epoch  30 Batch  373 / 525  Training Loss  0.0004298777785152197\n",
            "Epoch  30 Batch  374 / 525  Training Loss  0.0026689122896641493\n",
            "Epoch  30 Batch  375 / 525  Training Loss  0.001176481950096786\n",
            "Epoch  30 Batch  376 / 525  Training Loss  0.001986317103728652\n",
            "Epoch  30 Batch  377 / 525  Training Loss  0.0018698328640311956\n",
            "Epoch  30 Batch  378 / 525  Training Loss  0.0014726866502314806\n",
            "Epoch  30 Batch  379 / 525  Training Loss  0.005790422670543194\n",
            "Epoch  30 Batch  380 / 525  Training Loss  0.0009503133478574455\n",
            "Epoch  30 Batch  381 / 525  Training Loss  0.0005439795204438269\n",
            "Epoch  30 Batch  382 / 525  Training Loss  0.0009838576661422849\n",
            "Epoch  30 Batch  383 / 525  Training Loss  0.0008716112934052944\n",
            "Epoch  30 Batch  384 / 525  Training Loss  0.0012288842117413878\n",
            "Epoch  30 Batch  385 / 525  Training Loss  0.002194229979068041\n",
            "Epoch  30 Batch  386 / 525  Training Loss  0.001308040926232934\n",
            "Epoch  30 Batch  387 / 525  Training Loss  0.0017214991385117173\n",
            "Epoch  30 Batch  388 / 525  Training Loss  0.002168983919546008\n",
            "Epoch  30 Batch  389 / 525  Training Loss  0.0017253573751077056\n",
            "Epoch  30 Batch  390 / 525  Training Loss  0.001597756170667708\n",
            "Epoch  30 Batch  391 / 525  Training Loss  0.0030236076563596725\n",
            "Epoch  30 Batch  392 / 525  Training Loss  0.0008887171861715615\n",
            "Epoch  30 Batch  393 / 525  Training Loss  0.0005065907025709748\n",
            "Epoch  30 Batch  394 / 525  Training Loss  0.013088884763419628\n",
            "Epoch  30 Batch  395 / 525  Training Loss  0.0017314443830400705\n",
            "Epoch  30 Batch  396 / 525  Training Loss  0.010061592794954777\n",
            "Epoch  30 Batch  397 / 525  Training Loss  0.0034804376773536205\n",
            "Epoch  30 Batch  398 / 525  Training Loss  0.004667047411203384\n",
            "Epoch  30 Batch  399 / 525  Training Loss  0.0009521353058516979\n",
            "Epoch  30 Batch  400 / 525  Training Loss  0.0020636236295104027\n",
            "Epoch  30 Batch  401 / 525  Training Loss  0.0038785517681390047\n",
            "Epoch  30 Batch  402 / 525  Training Loss  0.0014732612762600183\n",
            "Epoch  30 Batch  403 / 525  Training Loss  0.004103121347725391\n",
            "Epoch  30 Batch  404 / 525  Training Loss  0.007335178554058075\n",
            "Epoch  30 Batch  405 / 525  Training Loss  0.009389491751790047\n",
            "Epoch  30 Batch  406 / 525  Training Loss  0.003479401348158717\n",
            "Epoch  30 Batch  407 / 525  Training Loss  0.00688042351976037\n",
            "Epoch  30 Batch  408 / 525  Training Loss  0.0019151077140122652\n",
            "Epoch  30 Batch  409 / 525  Training Loss  0.002356034703552723\n",
            "Epoch  30 Batch  410 / 525  Training Loss  0.0020876978524029255\n",
            "Epoch  30 Batch  411 / 525  Training Loss  0.005595291964709759\n",
            "Epoch  30 Batch  412 / 525  Training Loss  0.012204371392726898\n",
            "Epoch  30 Batch  413 / 525  Training Loss  0.004531995393335819\n",
            "Epoch  30 Batch  414 / 525  Training Loss  0.003580703167244792\n",
            "Epoch  30 Batch  415 / 525  Training Loss  0.005203585140407085\n",
            "Epoch  30 Batch  416 / 525  Training Loss  0.0030019786208868027\n",
            "Epoch  30 Batch  417 / 525  Training Loss  0.00039726626710034907\n",
            "Epoch  30 Batch  418 / 525  Training Loss  0.0008584517054259777\n",
            "Epoch  30 Batch  419 / 525  Training Loss  0.012658141553401947\n",
            "Epoch  30 Batch  420 / 525  Training Loss  0.003699344350025058\n",
            "Epoch  30 Batch  421 / 525  Training Loss  0.013615302741527557\n",
            "Epoch  30 Batch  422 / 525  Training Loss  0.026990016922354698\n",
            "Epoch  30 Batch  423 / 525  Training Loss  0.019585493952035904\n",
            "Epoch  30 Batch  424 / 525  Training Loss  0.00037171205622144043\n",
            "Epoch  30 Batch  425 / 525  Training Loss  0.006100246217101812\n",
            "Epoch  30 Batch  426 / 525  Training Loss  0.005883273668587208\n",
            "Epoch  30 Batch  427 / 525  Training Loss  0.006522215902805328\n",
            "Epoch  30 Batch  428 / 525  Training Loss  0.005398594308644533\n",
            "Epoch  30 Batch  429 / 525  Training Loss  0.008801175281405449\n",
            "Epoch  30 Batch  430 / 525  Training Loss  0.0034614361356943846\n",
            "Epoch  30 Batch  431 / 525  Training Loss  0.002587252762168646\n",
            "Epoch  30 Batch  432 / 525  Training Loss  0.0028185455594211817\n",
            "Epoch  30 Batch  433 / 525  Training Loss  0.012909342534840107\n",
            "Epoch  30 Batch  434 / 525  Training Loss  0.0021603405475616455\n",
            "Epoch  30 Batch  435 / 525  Training Loss  0.0009273812174797058\n",
            "Epoch  30 Batch  436 / 525  Training Loss  0.0022113046143203974\n",
            "Epoch  30 Batch  437 / 525  Training Loss  0.015098442323505878\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  30 Batch  438 / 525  Training Loss  0.005063576158136129\n",
            "Epoch  30 Batch  439 / 525  Training Loss  0.011339881457388401\n",
            "Epoch  30 Batch  440 / 525  Training Loss  0.008241801522672176\n",
            "Epoch  30 Batch  441 / 525  Training Loss  0.0022679525427520275\n",
            "Epoch  30 Batch  442 / 525  Training Loss  0.002316256519407034\n",
            "Epoch  30 Batch  443 / 525  Training Loss  0.005601619370281696\n",
            "Epoch  30 Batch  444 / 525  Training Loss  0.0053986795246601105\n",
            "Epoch  30 Batch  445 / 525  Training Loss  0.008751241490244865\n",
            "Epoch  30 Batch  446 / 525  Training Loss  0.0009520185412839055\n",
            "Epoch  30 Batch  447 / 525  Training Loss  0.004732336848974228\n",
            "Epoch  30 Batch  448 / 525  Training Loss  0.003349456237629056\n",
            "Epoch  30 Batch  449 / 525  Training Loss  0.01129180658608675\n",
            "Epoch  30 Batch  450 / 525  Training Loss  0.00444411626085639\n",
            "Epoch  30 Batch  451 / 525  Training Loss  0.00215735356323421\n",
            "Epoch  30 Batch  452 / 525  Training Loss  0.0013720592251047492\n",
            "Epoch  30 Batch  453 / 525  Training Loss  0.006193964742124081\n",
            "Epoch  30 Batch  454 / 525  Training Loss  0.007030308246612549\n",
            "Epoch  30 Batch  455 / 525  Training Loss  0.0026936279609799385\n",
            "Epoch  30 Batch  456 / 525  Training Loss  0.0034802008885890245\n",
            "Epoch  30 Batch  457 / 525  Training Loss  0.0023539885878562927\n",
            "Epoch  30 Batch  458 / 525  Training Loss  0.0008905714494176209\n",
            "Epoch  30 Batch  459 / 525  Training Loss  0.0008420714293606579\n",
            "Epoch  30 Batch  460 / 525  Training Loss  0.002762145595625043\n",
            "Epoch  30 Batch  461 / 525  Training Loss  0.0028337924741208553\n",
            "Epoch  30 Batch  462 / 525  Training Loss  0.00566343916580081\n",
            "Epoch  30 Batch  463 / 525  Training Loss  0.002646795706823468\n",
            "Epoch  30 Batch  464 / 525  Training Loss  0.0064932540990412235\n",
            "Epoch  30 Batch  465 / 525  Training Loss  0.003497079713270068\n",
            "Epoch  30 Batch  466 / 525  Training Loss  0.005384636111557484\n",
            "Epoch  30 Batch  467 / 525  Training Loss  0.005945923272520304\n",
            "Epoch  30 Batch  468 / 525  Training Loss  0.0024044590536504984\n",
            "Epoch  30 Batch  469 / 525  Training Loss  0.0030284104868769646\n",
            "Epoch  30 Batch  470 / 525  Training Loss  0.0015621219063177705\n",
            "Epoch  30 Batch  471 / 525  Training Loss  0.0014083980349823833\n",
            "Epoch  30 Batch  472 / 525  Training Loss  0.016074979677796364\n",
            "Epoch  30 Batch  473 / 525  Training Loss  0.006935738958418369\n",
            "Epoch  30 Batch  474 / 525  Training Loss  0.002146372338756919\n",
            "Epoch  30 Batch  475 / 525  Training Loss  0.004313020035624504\n",
            "Epoch  30 Batch  476 / 525  Training Loss  0.0015687408158555627\n",
            "Epoch  30 Batch  477 / 525  Training Loss  0.0010538666974753141\n",
            "Epoch  30 Batch  478 / 525  Training Loss  0.002042687265202403\n",
            "Epoch  30 Batch  479 / 525  Training Loss  0.002461876953020692\n",
            "Epoch  30 Batch  480 / 525  Training Loss  0.005866548046469688\n",
            "Epoch  30 Batch  481 / 525  Training Loss  0.0005297898896969855\n",
            "Epoch  30 Batch  482 / 525  Training Loss  0.0007733367965556681\n",
            "Epoch  30 Batch  483 / 525  Training Loss  0.0028502054046839476\n",
            "Epoch  30 Batch  484 / 525  Training Loss  0.0013371612876653671\n",
            "Epoch  30 Batch  485 / 525  Training Loss  0.0005581154837273061\n",
            "Epoch  30 Batch  486 / 525  Training Loss  0.007941319607198238\n",
            "Epoch  30 Batch  487 / 525  Training Loss  0.008409058675169945\n",
            "Epoch  30 Batch  488 / 525  Training Loss  0.006601334549486637\n",
            "Epoch  30 Batch  489 / 525  Training Loss  0.004296426195651293\n",
            "Epoch  30 Batch  490 / 525  Training Loss  0.006069095339626074\n",
            "Epoch  30 Batch  491 / 525  Training Loss  0.0050964062102139\n",
            "Epoch  30 Batch  492 / 525  Training Loss  0.0019029084360226989\n",
            "Epoch  30 Batch  493 / 525  Training Loss  0.003521367907524109\n",
            "Epoch  30 Batch  494 / 525  Training Loss  0.0018399336840957403\n",
            "Epoch  30 Batch  495 / 525  Training Loss  0.004079611040651798\n",
            "Epoch  30 Batch  496 / 525  Training Loss  0.0012379289837554097\n",
            "Epoch  30 Batch  497 / 525  Training Loss  0.004489760845899582\n",
            "Epoch  30 Batch  498 / 525  Training Loss  0.00205189804546535\n",
            "Epoch  30 Batch  499 / 525  Training Loss  0.006236325949430466\n",
            "Epoch  30 Batch  500 / 525  Training Loss  0.006205582991242409\n",
            "Epoch  30 Batch  501 / 525  Training Loss  0.002837864216417074\n",
            "Epoch  30 Batch  502 / 525  Training Loss  0.0025658663362264633\n",
            "Epoch  30 Batch  503 / 525  Training Loss  0.0016201278194785118\n",
            "Epoch  30 Batch  504 / 525  Training Loss  0.00099933089222759\n",
            "Epoch  30 Batch  505 / 525  Training Loss  0.001956214662641287\n",
            "Epoch  30 Batch  506 / 525  Training Loss  0.015581334009766579\n",
            "Epoch  30 Batch  507 / 525  Training Loss  0.0008836900815367699\n",
            "Epoch  30 Batch  508 / 525  Training Loss  0.012786127626895905\n",
            "Epoch  30 Batch  509 / 525  Training Loss  0.006147121079266071\n",
            "Epoch  30 Batch  510 / 525  Training Loss  0.005253244657069445\n",
            "Epoch  30 Batch  511 / 525  Training Loss  0.0034601811785250902\n",
            "Epoch  30 Batch  512 / 525  Training Loss  0.01584610901772976\n",
            "Epoch  30 Batch  513 / 525  Training Loss  0.006418672855943441\n",
            "Epoch  30 Batch  514 / 525  Training Loss  0.006672061048448086\n",
            "Epoch  30 Batch  515 / 525  Training Loss  0.003649328602477908\n",
            "Epoch  30 Batch  516 / 525  Training Loss  0.003329942701384425\n",
            "Epoch  30 Batch  517 / 525  Training Loss  0.001496287528425455\n",
            "Epoch  30 Batch  518 / 525  Training Loss  0.0007502330699935555\n",
            "Epoch  30 Batch  519 / 525  Training Loss  0.004101998172700405\n",
            "Epoch  30 Batch  520 / 525  Training Loss  0.0019245629664510489\n",
            "Epoch  30 Batch  521 / 525  Training Loss  0.004318657331168652\n",
            "Epoch  30 Batch  522 / 525  Training Loss  0.0073747048154473305\n",
            "Epoch  30 Batch  523 / 525  Training Loss  0.005479025654494762\n",
            "Epoch  30 Batch  524 / 525  Training Loss  0.0024377123918384314\n",
            "  31    |    -    |   0.003065   | 60.900000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 31\n",
            "Epoch  31 Batch  0 / 525  Training Loss  0.0005858700023964047\n",
            "Epoch  31 Batch  1 / 525  Training Loss  0.0016415786230936646\n",
            "Epoch  31 Batch  2 / 525  Training Loss  0.0012235449394211173\n",
            "Epoch  31 Batch  3 / 525  Training Loss  0.004152788780629635\n",
            "Epoch  31 Batch  4 / 525  Training Loss  0.00047517428174614906\n",
            "Epoch  31 Batch  5 / 525  Training Loss  0.0007857586024329066\n",
            "Epoch  31 Batch  6 / 525  Training Loss  0.0009357926319353282\n",
            "Epoch  31 Batch  7 / 525  Training Loss  0.0003489385708235204\n",
            "Epoch  31 Batch  8 / 525  Training Loss  0.011867543682456017\n",
            "Epoch  31 Batch  9 / 525  Training Loss  0.0046160416677594185\n",
            "Epoch  31 Batch  10 / 525  Training Loss  0.0011276027653366327\n",
            "Epoch  31 Batch  11 / 525  Training Loss  0.0030186024960130453\n",
            "Epoch  31 Batch  12 / 525  Training Loss  0.0054633645340800285\n",
            "Epoch  31 Batch  13 / 525  Training Loss  0.0008775506285019219\n",
            "Epoch  31 Batch  14 / 525  Training Loss  0.00028997129993513227\n",
            "Epoch  31 Batch  15 / 525  Training Loss  0.003496031044051051\n",
            "Epoch  31 Batch  16 / 525  Training Loss  0.001043743104673922\n",
            "Epoch  31 Batch  17 / 525  Training Loss  0.0004828750097658485\n",
            "Epoch  31 Batch  18 / 525  Training Loss  0.0011384331155568361\n",
            "Epoch  31 Batch  19 / 525  Training Loss  0.0028869437519460917\n",
            "Epoch  31 Batch  20 / 525  Training Loss  0.003972874954342842\n",
            "Epoch  31 Batch  21 / 525  Training Loss  0.0005644856719300151\n",
            "Epoch  31 Batch  22 / 525  Training Loss  0.0003068599908147007\n",
            "Epoch  31 Batch  23 / 525  Training Loss  0.0005284965736791492\n",
            "Epoch  31 Batch  24 / 525  Training Loss  0.006550382822751999\n",
            "Epoch  31 Batch  25 / 525  Training Loss  0.004040009342133999\n",
            "Epoch  31 Batch  26 / 525  Training Loss  0.00108053139410913\n",
            "Epoch  31 Batch  27 / 525  Training Loss  0.0005428456934168935\n",
            "Epoch  31 Batch  28 / 525  Training Loss  0.004296313039958477\n",
            "Epoch  31 Batch  29 / 525  Training Loss  0.0034070126712322235\n",
            "Epoch  31 Batch  30 / 525  Training Loss  0.001207395689561963\n",
            "Epoch  31 Batch  31 / 525  Training Loss  0.005905465222895145\n",
            "Epoch  31 Batch  32 / 525  Training Loss  0.0029004293028265238\n",
            "Epoch  31 Batch  33 / 525  Training Loss  0.0023358003236353397\n",
            "Epoch  31 Batch  34 / 525  Training Loss  0.002494607586413622\n",
            "Epoch  31 Batch  35 / 525  Training Loss  0.0015516416169703007\n",
            "Epoch  31 Batch  36 / 525  Training Loss  0.0003362972056493163\n",
            "Epoch  31 Batch  37 / 525  Training Loss  0.0041050962172448635\n",
            "Epoch  31 Batch  38 / 525  Training Loss  0.0013230403419584036\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  31 Batch  39 / 525  Training Loss  0.0019949947018176317\n",
            "Epoch  31 Batch  40 / 525  Training Loss  0.0014537818497046828\n",
            "Epoch  31 Batch  41 / 525  Training Loss  0.0013458924368023872\n",
            "Epoch  31 Batch  42 / 525  Training Loss  0.0019295451929792762\n",
            "Epoch  31 Batch  43 / 525  Training Loss  0.001635033986531198\n",
            "Epoch  31 Batch  44 / 525  Training Loss  0.0007720763678662479\n",
            "Epoch  31 Batch  45 / 525  Training Loss  0.004245369229465723\n",
            "Epoch  31 Batch  46 / 525  Training Loss  0.002743940334767103\n",
            "Epoch  31 Batch  47 / 525  Training Loss  0.0029347764793783426\n",
            "Epoch  31 Batch  48 / 525  Training Loss  0.014349767938256264\n",
            "Epoch  31 Batch  49 / 525  Training Loss  0.004444862250238657\n",
            "Epoch  31 Batch  50 / 525  Training Loss  0.0014235356356948614\n",
            "Epoch  31 Batch  51 / 525  Training Loss  0.0010329841170459986\n",
            "Epoch  31 Batch  52 / 525  Training Loss  0.0005421788082458079\n",
            "Epoch  31 Batch  53 / 525  Training Loss  0.003229959402233362\n",
            "Epoch  31 Batch  54 / 525  Training Loss  0.004085813648998737\n",
            "Epoch  31 Batch  55 / 525  Training Loss  0.0034188111312687397\n",
            "Epoch  31 Batch  56 / 525  Training Loss  0.0023041022941470146\n",
            "Epoch  31 Batch  57 / 525  Training Loss  0.0012077230494469404\n",
            "Epoch  31 Batch  58 / 525  Training Loss  0.0015177266905084252\n",
            "Epoch  31 Batch  59 / 525  Training Loss  0.001137854764238\n",
            "Epoch  31 Batch  60 / 525  Training Loss  0.008801294490695\n",
            "Epoch  31 Batch  61 / 525  Training Loss  0.004801989067345858\n",
            "Epoch  31 Batch  62 / 525  Training Loss  0.0019210234750062227\n",
            "Epoch  31 Batch  63 / 525  Training Loss  0.016437258571386337\n",
            "Epoch  31 Batch  64 / 525  Training Loss  0.003128218697384\n",
            "Epoch  31 Batch  65 / 525  Training Loss  0.0016370853409171104\n",
            "Epoch  31 Batch  66 / 525  Training Loss  0.001050243736244738\n",
            "Epoch  31 Batch  67 / 525  Training Loss  0.006637522019445896\n",
            "Epoch  31 Batch  68 / 525  Training Loss  0.001006006496027112\n",
            "Epoch  31 Batch  69 / 525  Training Loss  0.0025556180626153946\n",
            "Epoch  31 Batch  70 / 525  Training Loss  0.0015331999165937304\n",
            "Epoch  31 Batch  71 / 525  Training Loss  0.0002264811919303611\n",
            "Epoch  31 Batch  72 / 525  Training Loss  0.001172534073702991\n",
            "Epoch  31 Batch  73 / 525  Training Loss  0.0024608722887933254\n",
            "Epoch  31 Batch  74 / 525  Training Loss  0.002635769546031952\n",
            "Epoch  31 Batch  75 / 525  Training Loss  0.0011704092612490058\n",
            "Epoch  31 Batch  76 / 525  Training Loss  0.007293106522411108\n",
            "Epoch  31 Batch  77 / 525  Training Loss  0.006376561708748341\n",
            "Epoch  31 Batch  78 / 525  Training Loss  0.001526632229797542\n",
            "Epoch  31 Batch  79 / 525  Training Loss  0.009623290039598942\n",
            "Epoch  31 Batch  80 / 525  Training Loss  0.0010017178719863296\n",
            "Epoch  31 Batch  81 / 525  Training Loss  0.003280112985521555\n",
            "Epoch  31 Batch  82 / 525  Training Loss  0.0028541102074086666\n",
            "Epoch  31 Batch  83 / 525  Training Loss  0.005178037565201521\n",
            "Epoch  31 Batch  84 / 525  Training Loss  0.0006699818186461926\n",
            "Epoch  31 Batch  85 / 525  Training Loss  0.005460967309772968\n",
            "Epoch  31 Batch  86 / 525  Training Loss  0.000720566778909415\n",
            "Epoch  31 Batch  87 / 525  Training Loss  0.0019262119894847274\n",
            "Epoch  31 Batch  88 / 525  Training Loss  0.0002937101817224175\n",
            "Epoch  31 Batch  89 / 525  Training Loss  0.004398397170007229\n",
            "Epoch  31 Batch  90 / 525  Training Loss  0.01410455722361803\n",
            "Epoch  31 Batch  91 / 525  Training Loss  0.00023467717983294278\n",
            "Epoch  31 Batch  92 / 525  Training Loss  0.0012987445807084441\n",
            "Epoch  31 Batch  93 / 525  Training Loss  0.00411387300118804\n",
            "Epoch  31 Batch  94 / 525  Training Loss  0.007370986044406891\n",
            "Epoch  31 Batch  95 / 525  Training Loss  0.0017701589968055487\n",
            "Epoch  31 Batch  96 / 525  Training Loss  0.002325973240658641\n",
            "Epoch  31 Batch  97 / 525  Training Loss  0.0034810262732207775\n",
            "Epoch  31 Batch  98 / 525  Training Loss  0.00428369827568531\n",
            "Epoch  31 Batch  99 / 525  Training Loss  0.0006743029807694256\n",
            "Epoch  31 Batch  100 / 525  Training Loss  0.0010137801291421056\n",
            "Epoch  31 Batch  101 / 525  Training Loss  0.0029694396071135998\n",
            "Epoch  31 Batch  102 / 525  Training Loss  0.0006011552177369595\n",
            "Epoch  31 Batch  103 / 525  Training Loss  0.0016752484953030944\n",
            "Epoch  31 Batch  104 / 525  Training Loss  0.003957221284508705\n",
            "Epoch  31 Batch  105 / 525  Training Loss  0.0011607385240495205\n",
            "Epoch  31 Batch  106 / 525  Training Loss  0.0015030116774141788\n",
            "Epoch  31 Batch  107 / 525  Training Loss  0.0011551333591341972\n",
            "Epoch  31 Batch  108 / 525  Training Loss  0.0018086576601490378\n",
            "Epoch  31 Batch  109 / 525  Training Loss  0.003976526670157909\n",
            "Epoch  31 Batch  110 / 525  Training Loss  0.0079152537509799\n",
            "Epoch  31 Batch  111 / 525  Training Loss  0.0020708017982542515\n",
            "Epoch  31 Batch  112 / 525  Training Loss  0.001185581786558032\n",
            "Epoch  31 Batch  113 / 525  Training Loss  0.0038663637824356556\n",
            "Epoch  31 Batch  114 / 525  Training Loss  0.0005034650093875825\n",
            "Epoch  31 Batch  115 / 525  Training Loss  0.003111753147095442\n",
            "Epoch  31 Batch  116 / 525  Training Loss  0.006661493331193924\n",
            "Epoch  31 Batch  117 / 525  Training Loss  0.0036156834103167057\n",
            "Epoch  31 Batch  118 / 525  Training Loss  0.0006928649381734431\n",
            "Epoch  31 Batch  119 / 525  Training Loss  0.002264229813590646\n",
            "Epoch  31 Batch  120 / 525  Training Loss  0.003241421189159155\n",
            "Epoch  31 Batch  121 / 525  Training Loss  0.0016113746678456664\n",
            "Epoch  31 Batch  122 / 525  Training Loss  0.000634346972219646\n",
            "Epoch  31 Batch  123 / 525  Training Loss  0.0024080495350062847\n",
            "Epoch  31 Batch  124 / 525  Training Loss  0.002171278465539217\n",
            "Epoch  31 Batch  125 / 525  Training Loss  0.0031336478423327208\n",
            "Epoch  31 Batch  126 / 525  Training Loss  0.0003543556376826018\n",
            "Epoch  31 Batch  127 / 525  Training Loss  0.0022506467066705227\n",
            "Epoch  31 Batch  128 / 525  Training Loss  0.0013140372466295958\n",
            "Epoch  31 Batch  129 / 525  Training Loss  0.0013324536848813295\n",
            "Epoch  31 Batch  130 / 525  Training Loss  0.0006638417253270745\n",
            "Epoch  31 Batch  131 / 525  Training Loss  0.0028772640507668257\n",
            "Epoch  31 Batch  132 / 525  Training Loss  0.0014031907776370645\n",
            "Epoch  31 Batch  133 / 525  Training Loss  0.003465397283434868\n",
            "Epoch  31 Batch  134 / 525  Training Loss  0.00220084423199296\n",
            "Epoch  31 Batch  135 / 525  Training Loss  0.0010080649517476559\n",
            "Epoch  31 Batch  136 / 525  Training Loss  0.0035993834026157856\n",
            "Epoch  31 Batch  137 / 525  Training Loss  0.004385675769299269\n",
            "Epoch  31 Batch  138 / 525  Training Loss  0.0015138215385377407\n",
            "Epoch  31 Batch  139 / 525  Training Loss  0.005072267726063728\n",
            "Epoch  31 Batch  140 / 525  Training Loss  0.0027096965350210667\n",
            "Epoch  31 Batch  141 / 525  Training Loss  0.0019515871535986662\n",
            "Epoch  31 Batch  142 / 525  Training Loss  0.0005616875132545829\n",
            "Epoch  31 Batch  143 / 525  Training Loss  0.0013537590857595205\n",
            "Epoch  31 Batch  144 / 525  Training Loss  0.0018065476324409246\n",
            "Epoch  31 Batch  145 / 525  Training Loss  0.0009621932986192405\n",
            "Epoch  31 Batch  146 / 525  Training Loss  0.0040636747144162655\n",
            "Epoch  31 Batch  147 / 525  Training Loss  0.0028359938878566027\n",
            "Epoch  31 Batch  148 / 525  Training Loss  0.001952165155671537\n",
            "Epoch  31 Batch  149 / 525  Training Loss  0.004280526656657457\n",
            "Epoch  31 Batch  150 / 525  Training Loss  0.006963500287383795\n",
            "Epoch  31 Batch  151 / 525  Training Loss  0.008272161707282066\n",
            "Epoch  31 Batch  152 / 525  Training Loss  0.0021173343993723392\n",
            "Epoch  31 Batch  153 / 525  Training Loss  0.00040305667789652944\n",
            "Epoch  31 Batch  154 / 525  Training Loss  0.009803982451558113\n",
            "Epoch  31 Batch  155 / 525  Training Loss  0.0022914891596883535\n",
            "Epoch  31 Batch  156 / 525  Training Loss  0.0019064366351813078\n",
            "Epoch  31 Batch  157 / 525  Training Loss  0.0035248983185738325\n",
            "Epoch  31 Batch  158 / 525  Training Loss  0.0023528658784925938\n",
            "Epoch  31 Batch  159 / 525  Training Loss  0.00047050672583281994\n",
            "Epoch  31 Batch  160 / 525  Training Loss  0.0012414166703820229\n",
            "Epoch  31 Batch  161 / 525  Training Loss  0.001009729108773172\n",
            "Epoch  31 Batch  162 / 525  Training Loss  0.0006397232064045966\n",
            "Epoch  31 Batch  163 / 525  Training Loss  0.0003069208178203553\n",
            "Epoch  31 Batch  164 / 525  Training Loss  0.001803844003006816\n",
            "Epoch  31 Batch  165 / 525  Training Loss  0.0006123476778157055\n",
            "Epoch  31 Batch  166 / 525  Training Loss  0.0019364937907084823\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  31 Batch  167 / 525  Training Loss  0.0014695979189127684\n",
            "Epoch  31 Batch  168 / 525  Training Loss  0.006218305788934231\n",
            "Epoch  31 Batch  169 / 525  Training Loss  0.0037839836440980434\n",
            "Epoch  31 Batch  170 / 525  Training Loss  0.0011607494670897722\n",
            "Epoch  31 Batch  171 / 525  Training Loss  0.0021907459013164043\n",
            "Epoch  31 Batch  172 / 525  Training Loss  0.005050963722169399\n",
            "Epoch  31 Batch  173 / 525  Training Loss  0.009091738611459732\n",
            "Epoch  31 Batch  174 / 525  Training Loss  0.002811300102621317\n",
            "Epoch  31 Batch  175 / 525  Training Loss  0.0025966346729546785\n",
            "Epoch  31 Batch  176 / 525  Training Loss  0.0019044062355533242\n",
            "Epoch  31 Batch  177 / 525  Training Loss  0.005833999253809452\n",
            "Epoch  31 Batch  178 / 525  Training Loss  0.0020023020915687084\n",
            "Epoch  31 Batch  179 / 525  Training Loss  0.005002269521355629\n",
            "Epoch  31 Batch  180 / 525  Training Loss  0.007428785320371389\n",
            "Epoch  31 Batch  181 / 525  Training Loss  0.0006407975452020764\n",
            "Epoch  31 Batch  182 / 525  Training Loss  0.006673810072243214\n",
            "Epoch  31 Batch  183 / 525  Training Loss  0.0005211532115936279\n",
            "Epoch  31 Batch  184 / 525  Training Loss  0.0013791201636195183\n",
            "Epoch  31 Batch  185 / 525  Training Loss  0.009968234226107597\n",
            "Epoch  31 Batch  186 / 525  Training Loss  0.004274759907275438\n",
            "Epoch  31 Batch  187 / 525  Training Loss  0.01356938760727644\n",
            "Epoch  31 Batch  188 / 525  Training Loss  0.003959725610911846\n",
            "Epoch  31 Batch  189 / 525  Training Loss  0.006420822348445654\n",
            "Epoch  31 Batch  190 / 525  Training Loss  0.00874349381774664\n",
            "Epoch  31 Batch  191 / 525  Training Loss  0.00433474313467741\n",
            "Epoch  31 Batch  192 / 525  Training Loss  0.0012090984964743257\n",
            "Epoch  31 Batch  193 / 525  Training Loss  0.009595107287168503\n",
            "Epoch  31 Batch  194 / 525  Training Loss  0.015212292782962322\n",
            "Epoch  31 Batch  195 / 525  Training Loss  0.005493325646966696\n",
            "Epoch  31 Batch  196 / 525  Training Loss  0.006292576901614666\n",
            "Epoch  31 Batch  197 / 525  Training Loss  0.005436849780380726\n",
            "Epoch  31 Batch  198 / 525  Training Loss  0.005231105722486973\n",
            "Epoch  31 Batch  199 / 525  Training Loss  0.003325529396533966\n",
            "Epoch  31 Batch  200 / 525  Training Loss  0.0024150931276381016\n",
            "Epoch  31 Batch  201 / 525  Training Loss  0.0008977122488431633\n",
            "Epoch  31 Batch  202 / 525  Training Loss  0.0005774791934527457\n",
            "Epoch  31 Batch  203 / 525  Training Loss  0.003138468135148287\n",
            "Epoch  31 Batch  204 / 525  Training Loss  0.00927713792771101\n",
            "Epoch  31 Batch  205 / 525  Training Loss  0.01124003529548645\n",
            "Epoch  31 Batch  206 / 525  Training Loss  0.002086868742480874\n",
            "Epoch  31 Batch  207 / 525  Training Loss  0.0007541578961536288\n",
            "Epoch  31 Batch  208 / 525  Training Loss  0.004479802679270506\n",
            "Epoch  31 Batch  209 / 525  Training Loss  0.002239780966192484\n",
            "Epoch  31 Batch  210 / 525  Training Loss  0.0007013198919594288\n",
            "Epoch  31 Batch  211 / 525  Training Loss  0.0037080273032188416\n",
            "Epoch  31 Batch  212 / 525  Training Loss  0.00402840506285429\n",
            "Epoch  31 Batch  213 / 525  Training Loss  0.009755785576999187\n",
            "Epoch  31 Batch  214 / 525  Training Loss  0.013444979675114155\n",
            "Epoch  31 Batch  215 / 525  Training Loss  0.0021577717270702124\n",
            "Epoch  31 Batch  216 / 525  Training Loss  0.0026091360487043858\n",
            "Epoch  31 Batch  217 / 525  Training Loss  0.00893137976527214\n",
            "Epoch  31 Batch  218 / 525  Training Loss  0.0003004527243319899\n",
            "Epoch  31 Batch  219 / 525  Training Loss  0.005972085054963827\n",
            "Epoch  31 Batch  220 / 525  Training Loss  0.004927932284772396\n",
            "Epoch  31 Batch  221 / 525  Training Loss  0.0016729955095797777\n",
            "Epoch  31 Batch  222 / 525  Training Loss  0.009046402759850025\n",
            "Epoch  31 Batch  223 / 525  Training Loss  0.011532848700881004\n",
            "Epoch  31 Batch  224 / 525  Training Loss  0.0028483436908572912\n",
            "Epoch  31 Batch  225 / 525  Training Loss  0.007161744870245457\n",
            "Epoch  31 Batch  226 / 525  Training Loss  0.0021846252493560314\n",
            "Epoch  31 Batch  227 / 525  Training Loss  0.003210844937711954\n",
            "Epoch  31 Batch  228 / 525  Training Loss  0.0004428671672940254\n",
            "Epoch  31 Batch  229 / 525  Training Loss  0.004335637204349041\n",
            "Epoch  31 Batch  230 / 525  Training Loss  0.0016275178641080856\n",
            "Epoch  31 Batch  231 / 525  Training Loss  0.0018467974150553346\n",
            "Epoch  31 Batch  232 / 525  Training Loss  0.0011063499841839075\n",
            "Epoch  31 Batch  233 / 525  Training Loss  0.011849009431898594\n",
            "Epoch  31 Batch  234 / 525  Training Loss  0.012897538021206856\n",
            "Epoch  31 Batch  235 / 525  Training Loss  0.009909592568874359\n",
            "Epoch  31 Batch  236 / 525  Training Loss  0.0024317209608852863\n",
            "Epoch  31 Batch  237 / 525  Training Loss  0.0022866115905344486\n",
            "Epoch  31 Batch  238 / 525  Training Loss  0.001773683587089181\n",
            "Epoch  31 Batch  239 / 525  Training Loss  0.005259142257273197\n",
            "Epoch  31 Batch  240 / 525  Training Loss  0.006548660341650248\n",
            "Epoch  31 Batch  241 / 525  Training Loss  0.004779161885380745\n",
            "Epoch  31 Batch  242 / 525  Training Loss  0.006827539764344692\n",
            "Epoch  31 Batch  243 / 525  Training Loss  0.007702380418777466\n",
            "Epoch  31 Batch  244 / 525  Training Loss  0.00996070634573698\n",
            "Epoch  31 Batch  245 / 525  Training Loss  0.0016339861322194338\n",
            "Epoch  31 Batch  246 / 525  Training Loss  0.009690185077488422\n",
            "Epoch  31 Batch  247 / 525  Training Loss  0.0030104040633887053\n",
            "Epoch  31 Batch  248 / 525  Training Loss  0.003241428406909108\n",
            "Epoch  31 Batch  249 / 525  Training Loss  0.007486454211175442\n",
            "Epoch  31 Batch  250 / 525  Training Loss  0.004199543036520481\n",
            "Epoch  31 Batch  251 / 525  Training Loss  0.0038272179663181305\n",
            "Epoch  31 Batch  252 / 525  Training Loss  0.0009789445903152227\n",
            "Epoch  31 Batch  253 / 525  Training Loss  0.0018152392003685236\n",
            "Epoch  31 Batch  254 / 525  Training Loss  0.00040621132939122617\n",
            "Epoch  31 Batch  255 / 525  Training Loss  0.006727276835590601\n",
            "Epoch  31 Batch  256 / 525  Training Loss  0.0005179614527150989\n",
            "Epoch  31 Batch  257 / 525  Training Loss  0.0136442631483078\n",
            "Epoch  31 Batch  258 / 525  Training Loss  0.00953588169068098\n",
            "Epoch  31 Batch  259 / 525  Training Loss  0.0045706285163760185\n",
            "Epoch  31 Batch  260 / 525  Training Loss  0.0020951651968061924\n",
            "Epoch  31 Batch  261 / 525  Training Loss  0.0013490613782778382\n",
            "Epoch  31 Batch  262 / 525  Training Loss  0.003329330589622259\n",
            "Epoch  31 Batch  263 / 525  Training Loss  0.0015611561248078942\n",
            "Epoch  31 Batch  264 / 525  Training Loss  0.0007971146842464805\n",
            "Epoch  31 Batch  265 / 525  Training Loss  0.003520683152601123\n",
            "Epoch  31 Batch  266 / 525  Training Loss  0.0133495032787323\n",
            "Epoch  31 Batch  267 / 525  Training Loss  0.011077805422246456\n",
            "Epoch  31 Batch  268 / 525  Training Loss  0.007445938885211945\n",
            "Epoch  31 Batch  269 / 525  Training Loss  0.007624391466379166\n",
            "Epoch  31 Batch  270 / 525  Training Loss  0.008458840660750866\n",
            "Epoch  31 Batch  271 / 525  Training Loss  0.0038297921419143677\n",
            "Epoch  31 Batch  272 / 525  Training Loss  0.0022985562682151794\n",
            "Epoch  31 Batch  273 / 525  Training Loss  0.006691038608551025\n",
            "Epoch  31 Batch  274 / 525  Training Loss  0.003432253375649452\n",
            "Epoch  31 Batch  275 / 525  Training Loss  0.0035758267622441053\n",
            "Epoch  31 Batch  276 / 525  Training Loss  0.004360746592283249\n",
            "Epoch  31 Batch  277 / 525  Training Loss  0.007152125239372253\n",
            "Epoch  31 Batch  278 / 525  Training Loss  0.003690289333462715\n",
            "Epoch  31 Batch  279 / 525  Training Loss  0.004599802196025848\n",
            "Epoch  31 Batch  280 / 525  Training Loss  0.0021954274270683527\n",
            "Epoch  31 Batch  281 / 525  Training Loss  0.0035141624975949526\n",
            "Epoch  31 Batch  282 / 525  Training Loss  0.005604338366538286\n",
            "Epoch  31 Batch  283 / 525  Training Loss  0.010549947619438171\n",
            "Epoch  31 Batch  284 / 525  Training Loss  0.005063601769506931\n",
            "Epoch  31 Batch  285 / 525  Training Loss  0.002585314679890871\n",
            "Epoch  31 Batch  286 / 525  Training Loss  0.0025602434761822224\n",
            "Epoch  31 Batch  287 / 525  Training Loss  0.018658224493265152\n",
            "Epoch  31 Batch  288 / 525  Training Loss  0.013809716328978539\n",
            "Epoch  31 Batch  289 / 525  Training Loss  0.009350535459816456\n",
            "Epoch  31 Batch  290 / 525  Training Loss  0.02358303777873516\n",
            "Epoch  31 Batch  291 / 525  Training Loss  0.012465263716876507\n",
            "Epoch  31 Batch  292 / 525  Training Loss  0.004956413991749287\n",
            "Epoch  31 Batch  293 / 525  Training Loss  0.009094400331377983\n",
            "Epoch  31 Batch  294 / 525  Training Loss  0.008643354289233685\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  31 Batch  295 / 525  Training Loss  0.0045284307561814785\n",
            "Epoch  31 Batch  296 / 525  Training Loss  0.0020827611442655325\n",
            "Epoch  31 Batch  297 / 525  Training Loss  0.0226531233638525\n",
            "Epoch  31 Batch  298 / 525  Training Loss  0.0016892548883333802\n",
            "Epoch  31 Batch  299 / 525  Training Loss  0.003241580678150058\n",
            "Epoch  31 Batch  300 / 525  Training Loss  0.005557631608098745\n",
            "Epoch  31 Batch  301 / 525  Training Loss  0.011410048231482506\n",
            "Epoch  31 Batch  302 / 525  Training Loss  0.004341319669038057\n",
            "Epoch  31 Batch  303 / 525  Training Loss  0.0018051393562927842\n",
            "Epoch  31 Batch  304 / 525  Training Loss  0.0016073656734079123\n",
            "Epoch  31 Batch  305 / 525  Training Loss  0.007015323732048273\n",
            "Epoch  31 Batch  306 / 525  Training Loss  0.0022525512613356113\n",
            "Epoch  31 Batch  307 / 525  Training Loss  0.0069149574264883995\n",
            "Epoch  31 Batch  308 / 525  Training Loss  0.02142038755118847\n",
            "Epoch  31 Batch  309 / 525  Training Loss  0.008136658929288387\n",
            "Epoch  31 Batch  310 / 525  Training Loss  0.0028952632565051317\n",
            "Epoch  31 Batch  311 / 525  Training Loss  0.004717011470347643\n",
            "Epoch  31 Batch  312 / 525  Training Loss  0.006350268609821796\n",
            "Epoch  31 Batch  313 / 525  Training Loss  0.0014377234037965536\n",
            "Epoch  31 Batch  314 / 525  Training Loss  0.0025832669343799353\n",
            "Epoch  31 Batch  315 / 525  Training Loss  0.0034187007695436478\n",
            "Epoch  31 Batch  316 / 525  Training Loss  0.004524370189756155\n",
            "Epoch  31 Batch  317 / 525  Training Loss  0.008776710368692875\n",
            "Epoch  31 Batch  318 / 525  Training Loss  0.003921906929463148\n",
            "Epoch  31 Batch  319 / 525  Training Loss  0.0035700243897736073\n",
            "Epoch  31 Batch  320 / 525  Training Loss  0.006514504551887512\n",
            "Epoch  31 Batch  321 / 525  Training Loss  0.0016631822800263762\n",
            "Epoch  31 Batch  322 / 525  Training Loss  0.0018007040489464998\n",
            "Epoch  31 Batch  323 / 525  Training Loss  0.0029398687183856964\n",
            "Epoch  31 Batch  324 / 525  Training Loss  0.0039035796653479338\n",
            "Epoch  31 Batch  325 / 525  Training Loss  0.002852406119927764\n",
            "Epoch  31 Batch  326 / 525  Training Loss  0.0037293985951691866\n",
            "Epoch  31 Batch  327 / 525  Training Loss  0.0008202766766771674\n",
            "Epoch  31 Batch  328 / 525  Training Loss  0.006037799175828695\n",
            "Epoch  31 Batch  329 / 525  Training Loss  0.004951410461217165\n",
            "Epoch  31 Batch  330 / 525  Training Loss  0.004922806750983\n",
            "Epoch  31 Batch  331 / 525  Training Loss  0.001415712176822126\n",
            "Epoch  31 Batch  332 / 525  Training Loss  0.0010917233303189278\n",
            "Epoch  31 Batch  333 / 525  Training Loss  0.0013250487390905619\n",
            "Epoch  31 Batch  334 / 525  Training Loss  0.004213309846818447\n",
            "Epoch  31 Batch  335 / 525  Training Loss  0.002666436368599534\n",
            "Epoch  31 Batch  336 / 525  Training Loss  0.011379758827388287\n",
            "Epoch  31 Batch  337 / 525  Training Loss  0.005987687036395073\n",
            "Epoch  31 Batch  338 / 525  Training Loss  0.007750608958303928\n",
            "Epoch  31 Batch  339 / 525  Training Loss  0.0041005900129675865\n",
            "Epoch  31 Batch  340 / 525  Training Loss  0.0049910652451217175\n",
            "Epoch  31 Batch  341 / 525  Training Loss  0.0021301796659827232\n",
            "Epoch  31 Batch  342 / 525  Training Loss  0.002096774522215128\n",
            "Epoch  31 Batch  343 / 525  Training Loss  0.006120393518358469\n",
            "Epoch  31 Batch  344 / 525  Training Loss  0.00111852225381881\n",
            "Epoch  31 Batch  345 / 525  Training Loss  0.0031023509800434113\n",
            "Epoch  31 Batch  346 / 525  Training Loss  0.007370332721620798\n",
            "Epoch  31 Batch  347 / 525  Training Loss  0.0011360831558704376\n",
            "Epoch  31 Batch  348 / 525  Training Loss  0.0033936884719878435\n",
            "Epoch  31 Batch  349 / 525  Training Loss  0.002807751065120101\n",
            "Epoch  31 Batch  350 / 525  Training Loss  0.006165388040244579\n",
            "Epoch  31 Batch  351 / 525  Training Loss  0.005226730834692717\n",
            "Epoch  31 Batch  352 / 525  Training Loss  0.020381521433591843\n",
            "Epoch  31 Batch  353 / 525  Training Loss  0.0013724509626626968\n",
            "Epoch  31 Batch  354 / 525  Training Loss  0.0015441954601556063\n",
            "Epoch  31 Batch  355 / 525  Training Loss  0.0025144435930997133\n",
            "Epoch  31 Batch  356 / 525  Training Loss  0.004868900403380394\n",
            "Epoch  31 Batch  357 / 525  Training Loss  0.004673861898481846\n",
            "Epoch  31 Batch  358 / 525  Training Loss  0.0012927802745252848\n",
            "Epoch  31 Batch  359 / 525  Training Loss  0.005125259980559349\n",
            "Epoch  31 Batch  360 / 525  Training Loss  0.0012777869123965502\n",
            "Epoch  31 Batch  361 / 525  Training Loss  0.001525169936940074\n",
            "Epoch  31 Batch  362 / 525  Training Loss  0.00401793560013175\n",
            "Epoch  31 Batch  363 / 525  Training Loss  0.0019270461052656174\n",
            "Epoch  31 Batch  364 / 525  Training Loss  0.011760787107050419\n",
            "Epoch  31 Batch  365 / 525  Training Loss  0.002004602225497365\n",
            "Epoch  31 Batch  366 / 525  Training Loss  0.0029959387611597776\n",
            "Epoch  31 Batch  367 / 525  Training Loss  0.0014626884367316961\n",
            "Epoch  31 Batch  368 / 525  Training Loss  0.004058618098497391\n",
            "Epoch  31 Batch  369 / 525  Training Loss  0.008328894153237343\n",
            "Epoch  31 Batch  370 / 525  Training Loss  0.0047464026138186455\n",
            "Epoch  31 Batch  371 / 525  Training Loss  0.005185909103602171\n",
            "Epoch  31 Batch  372 / 525  Training Loss  0.0023517452646046877\n",
            "Epoch  31 Batch  373 / 525  Training Loss  0.0032566904556006193\n",
            "Epoch  31 Batch  374 / 525  Training Loss  0.006048263516277075\n",
            "Epoch  31 Batch  375 / 525  Training Loss  0.004805822856724262\n",
            "Epoch  31 Batch  376 / 525  Training Loss  0.0038057267665863037\n",
            "Epoch  31 Batch  377 / 525  Training Loss  0.008913032710552216\n",
            "Epoch  31 Batch  378 / 525  Training Loss  0.009086674079298973\n",
            "Epoch  31 Batch  379 / 525  Training Loss  0.0072894953191280365\n",
            "Epoch  31 Batch  380 / 525  Training Loss  0.0009354283101856709\n",
            "Epoch  31 Batch  381 / 525  Training Loss  0.005376871209591627\n",
            "Epoch  31 Batch  382 / 525  Training Loss  0.008789416402578354\n",
            "Epoch  31 Batch  383 / 525  Training Loss  0.0065011875703930855\n",
            "Epoch  31 Batch  384 / 525  Training Loss  0.0016633536433801055\n",
            "Epoch  31 Batch  385 / 525  Training Loss  0.003197514219209552\n",
            "Epoch  31 Batch  386 / 525  Training Loss  0.004053519107401371\n",
            "Epoch  31 Batch  387 / 525  Training Loss  0.0031218468211591244\n",
            "Epoch  31 Batch  388 / 525  Training Loss  0.0018875685054808855\n",
            "Epoch  31 Batch  389 / 525  Training Loss  0.005388376768678427\n",
            "Epoch  31 Batch  390 / 525  Training Loss  0.0061462945304811\n",
            "Epoch  31 Batch  391 / 525  Training Loss  0.005019852891564369\n",
            "Epoch  31 Batch  392 / 525  Training Loss  0.0003897889400832355\n",
            "Epoch  31 Batch  393 / 525  Training Loss  0.0016081469366326928\n",
            "Epoch  31 Batch  394 / 525  Training Loss  0.0009021792793646455\n",
            "Epoch  31 Batch  395 / 525  Training Loss  0.003353214357048273\n",
            "Epoch  31 Batch  396 / 525  Training Loss  0.0028587556444108486\n",
            "Epoch  31 Batch  397 / 525  Training Loss  0.0013263941509649158\n",
            "Epoch  31 Batch  398 / 525  Training Loss  0.008136908523738384\n",
            "Epoch  31 Batch  399 / 525  Training Loss  0.002817952772602439\n",
            "Epoch  31 Batch  400 / 525  Training Loss  0.001678337575867772\n",
            "Epoch  31 Batch  401 / 525  Training Loss  0.011363105848431587\n",
            "Epoch  31 Batch  402 / 525  Training Loss  0.008625440299510956\n",
            "Epoch  31 Batch  403 / 525  Training Loss  0.0090477978810668\n",
            "Epoch  31 Batch  404 / 525  Training Loss  0.013158610090613365\n",
            "Epoch  31 Batch  405 / 525  Training Loss  0.00168651866260916\n",
            "Epoch  31 Batch  406 / 525  Training Loss  0.0019689525943249464\n",
            "Epoch  31 Batch  407 / 525  Training Loss  0.016352258622646332\n",
            "Epoch  31 Batch  408 / 525  Training Loss  0.0022613988257944584\n",
            "Epoch  31 Batch  409 / 525  Training Loss  0.0011410138104110956\n",
            "Epoch  31 Batch  410 / 525  Training Loss  0.0013280867133289576\n",
            "Epoch  31 Batch  411 / 525  Training Loss  0.0031109252013266087\n",
            "Epoch  31 Batch  412 / 525  Training Loss  0.019953390583395958\n",
            "Epoch  31 Batch  413 / 525  Training Loss  0.0025490138214081526\n",
            "Epoch  31 Batch  414 / 525  Training Loss  0.002501377137377858\n",
            "Epoch  31 Batch  415 / 525  Training Loss  0.004878920502960682\n",
            "Epoch  31 Batch  416 / 525  Training Loss  0.004263483453541994\n",
            "Epoch  31 Batch  417 / 525  Training Loss  0.004198002628982067\n",
            "Epoch  31 Batch  418 / 525  Training Loss  0.0015792722115293145\n",
            "Epoch  31 Batch  419 / 525  Training Loss  0.0030011353082954884\n",
            "Epoch  31 Batch  420 / 525  Training Loss  0.0011572081129997969\n",
            "Epoch  31 Batch  421 / 525  Training Loss  0.008654413744807243\n",
            "Epoch  31 Batch  422 / 525  Training Loss  0.0026620265562087297\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  31 Batch  423 / 525  Training Loss  0.002373004797846079\n",
            "Epoch  31 Batch  424 / 525  Training Loss  0.007585365325212479\n",
            "Epoch  31 Batch  425 / 525  Training Loss  0.0028791087679564953\n",
            "Epoch  31 Batch  426 / 525  Training Loss  0.0020530717447400093\n",
            "Epoch  31 Batch  427 / 525  Training Loss  0.0020075952634215355\n",
            "Epoch  31 Batch  428 / 525  Training Loss  0.0006195729365572333\n",
            "Epoch  31 Batch  429 / 525  Training Loss  0.003864215686917305\n",
            "Epoch  31 Batch  430 / 525  Training Loss  0.0021784058772027493\n",
            "Epoch  31 Batch  431 / 525  Training Loss  0.0022593417670577765\n",
            "Epoch  31 Batch  432 / 525  Training Loss  0.0023649984505027533\n",
            "Epoch  31 Batch  433 / 525  Training Loss  0.0032457802444696426\n",
            "Epoch  31 Batch  434 / 525  Training Loss  0.000605422246735543\n",
            "Epoch  31 Batch  435 / 525  Training Loss  0.011871729046106339\n",
            "Epoch  31 Batch  436 / 525  Training Loss  0.007254404481500387\n",
            "Epoch  31 Batch  437 / 525  Training Loss  0.02501734159886837\n",
            "Epoch  31 Batch  438 / 525  Training Loss  0.011552176438272\n",
            "Epoch  31 Batch  439 / 525  Training Loss  0.013361068442463875\n",
            "Epoch  31 Batch  440 / 525  Training Loss  0.0035965696442872286\n",
            "Epoch  31 Batch  441 / 525  Training Loss  0.004953368101269007\n",
            "Epoch  31 Batch  442 / 525  Training Loss  0.01623767614364624\n",
            "Epoch  31 Batch  443 / 525  Training Loss  0.0027564691845327616\n",
            "Epoch  31 Batch  444 / 525  Training Loss  0.006464027799665928\n",
            "Epoch  31 Batch  445 / 525  Training Loss  0.00874629057943821\n",
            "Epoch  31 Batch  446 / 525  Training Loss  0.0028790379874408245\n",
            "Epoch  31 Batch  447 / 525  Training Loss  0.019128652289509773\n",
            "Epoch  31 Batch  448 / 525  Training Loss  0.005937384907156229\n",
            "Epoch  31 Batch  449 / 525  Training Loss  0.00991745013743639\n",
            "Epoch  31 Batch  450 / 525  Training Loss  0.007330670952796936\n",
            "Epoch  31 Batch  451 / 525  Training Loss  0.00642122607678175\n",
            "Epoch  31 Batch  452 / 525  Training Loss  0.008526181802153587\n",
            "Epoch  31 Batch  453 / 525  Training Loss  0.00352758658118546\n",
            "Epoch  31 Batch  454 / 525  Training Loss  0.002451712731271982\n",
            "Epoch  31 Batch  455 / 525  Training Loss  0.005445054732263088\n",
            "Epoch  31 Batch  456 / 525  Training Loss  0.008172715082764626\n",
            "Epoch  31 Batch  457 / 525  Training Loss  0.006453470326960087\n",
            "Epoch  31 Batch  458 / 525  Training Loss  0.005531308241188526\n",
            "Epoch  31 Batch  459 / 525  Training Loss  0.004874837584793568\n",
            "Epoch  31 Batch  460 / 525  Training Loss  0.006703080143779516\n",
            "Epoch  31 Batch  461 / 525  Training Loss  0.0012437549885362387\n",
            "Epoch  31 Batch  462 / 525  Training Loss  0.0027723878156393766\n",
            "Epoch  31 Batch  463 / 525  Training Loss  0.006208495236933231\n",
            "Epoch  31 Batch  464 / 525  Training Loss  0.017757577821612358\n",
            "Epoch  31 Batch  465 / 525  Training Loss  0.0012583885109052062\n",
            "Epoch  31 Batch  466 / 525  Training Loss  0.004230169113725424\n",
            "Epoch  31 Batch  467 / 525  Training Loss  0.00256697298027575\n",
            "Epoch  31 Batch  468 / 525  Training Loss  0.002683298196643591\n",
            "Epoch  31 Batch  469 / 525  Training Loss  0.007003604434430599\n",
            "Epoch  31 Batch  470 / 525  Training Loss  0.013602624647319317\n",
            "Epoch  31 Batch  471 / 525  Training Loss  0.010172924026846886\n",
            "Epoch  31 Batch  472 / 525  Training Loss  0.001854144735261798\n",
            "Epoch  31 Batch  473 / 525  Training Loss  0.004559655673801899\n",
            "Epoch  31 Batch  474 / 525  Training Loss  0.004053441341966391\n",
            "Epoch  31 Batch  475 / 525  Training Loss  0.0016515299212187529\n",
            "Epoch  31 Batch  476 / 525  Training Loss  0.006320020649582148\n",
            "Epoch  31 Batch  477 / 525  Training Loss  0.00189892016351223\n",
            "Epoch  31 Batch  478 / 525  Training Loss  0.004731437191367149\n",
            "Epoch  31 Batch  479 / 525  Training Loss  0.0036979210563004017\n",
            "Epoch  31 Batch  480 / 525  Training Loss  0.0024194568395614624\n",
            "Epoch  31 Batch  481 / 525  Training Loss  0.0010041362838819623\n",
            "Epoch  31 Batch  482 / 525  Training Loss  0.0010912412544712424\n",
            "Epoch  31 Batch  483 / 525  Training Loss  0.01107683777809143\n",
            "Epoch  31 Batch  484 / 525  Training Loss  0.0025660754181444645\n",
            "Epoch  31 Batch  485 / 525  Training Loss  0.008097758516669273\n",
            "Epoch  31 Batch  486 / 525  Training Loss  0.008749489672482014\n",
            "Epoch  31 Batch  487 / 525  Training Loss  0.005071300081908703\n",
            "Epoch  31 Batch  488 / 525  Training Loss  0.0025936481542885303\n",
            "Epoch  31 Batch  489 / 525  Training Loss  0.013672676868736744\n",
            "Epoch  31 Batch  490 / 525  Training Loss  0.004118802957236767\n",
            "Epoch  31 Batch  491 / 525  Training Loss  0.004053913988173008\n",
            "Epoch  31 Batch  492 / 525  Training Loss  0.009571717120707035\n",
            "Epoch  31 Batch  493 / 525  Training Loss  0.005730991251766682\n",
            "Epoch  31 Batch  494 / 525  Training Loss  0.007914232090115547\n",
            "Epoch  31 Batch  495 / 525  Training Loss  0.010499645955860615\n",
            "Epoch  31 Batch  496 / 525  Training Loss  0.00853670947253704\n",
            "Epoch  31 Batch  497 / 525  Training Loss  0.006052801385521889\n",
            "Epoch  31 Batch  498 / 525  Training Loss  0.0046410574577748775\n",
            "Epoch  31 Batch  499 / 525  Training Loss  0.009504917077720165\n",
            "Epoch  31 Batch  500 / 525  Training Loss  0.010376157239079475\n",
            "Epoch  31 Batch  501 / 525  Training Loss  0.011763809248805046\n",
            "Epoch  31 Batch  502 / 525  Training Loss  0.0031731289345771074\n",
            "Epoch  31 Batch  503 / 525  Training Loss  0.012575077824294567\n",
            "Epoch  31 Batch  504 / 525  Training Loss  0.0019098601769655943\n",
            "Epoch  31 Batch  505 / 525  Training Loss  0.001497762044891715\n",
            "Epoch  31 Batch  506 / 525  Training Loss  0.0009087423095479608\n",
            "Epoch  31 Batch  507 / 525  Training Loss  0.0017540242988616228\n",
            "Epoch  31 Batch  508 / 525  Training Loss  0.01206330768764019\n",
            "Epoch  31 Batch  509 / 525  Training Loss  0.005538051947951317\n",
            "Epoch  31 Batch  510 / 525  Training Loss  0.004022374749183655\n",
            "Epoch  31 Batch  511 / 525  Training Loss  0.00337473233230412\n",
            "Epoch  31 Batch  512 / 525  Training Loss  0.009447081945836544\n",
            "Epoch  31 Batch  513 / 525  Training Loss  0.008363494649529457\n",
            "Epoch  31 Batch  514 / 525  Training Loss  0.01096146646887064\n",
            "Epoch  31 Batch  515 / 525  Training Loss  0.004159504547715187\n",
            "Epoch  31 Batch  516 / 525  Training Loss  0.0020674464758485556\n",
            "Epoch  31 Batch  517 / 525  Training Loss  0.0006204505916684866\n",
            "Epoch  31 Batch  518 / 525  Training Loss  0.0011336910538375378\n",
            "Epoch  31 Batch  519 / 525  Training Loss  0.00478306645527482\n",
            "Epoch  31 Batch  520 / 525  Training Loss  0.0007953173480927944\n",
            "Epoch  31 Batch  521 / 525  Training Loss  0.00395912304520607\n",
            "Epoch  31 Batch  522 / 525  Training Loss  0.004197121597826481\n",
            "Epoch  31 Batch  523 / 525  Training Loss  0.0022999730426818132\n",
            "Epoch  31 Batch  524 / 525  Training Loss  0.008481981232762337\n",
            "  32    |    -    |   0.004518   | 60.400000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 32\n",
            "Epoch  32 Batch  0 / 525  Training Loss  0.0061851306818425655\n",
            "Epoch  32 Batch  1 / 525  Training Loss  0.0032575377263128757\n",
            "Epoch  32 Batch  2 / 525  Training Loss  0.002668960951268673\n",
            "Epoch  32 Batch  3 / 525  Training Loss  0.0016113684978336096\n",
            "Epoch  32 Batch  4 / 525  Training Loss  0.00384999206289649\n",
            "Epoch  32 Batch  5 / 525  Training Loss  0.006180946249514818\n",
            "Epoch  32 Batch  6 / 525  Training Loss  0.009510789066553116\n",
            "Epoch  32 Batch  7 / 525  Training Loss  0.0029286930803209543\n",
            "Epoch  32 Batch  8 / 525  Training Loss  0.0017631793161854148\n",
            "Epoch  32 Batch  9 / 525  Training Loss  0.001087492797523737\n",
            "Epoch  32 Batch  10 / 525  Training Loss  0.004033603705465794\n",
            "Epoch  32 Batch  11 / 525  Training Loss  0.0023509827442467213\n",
            "Epoch  32 Batch  12 / 525  Training Loss  0.005793169140815735\n",
            "Epoch  32 Batch  13 / 525  Training Loss  0.012298906221985817\n",
            "Epoch  32 Batch  14 / 525  Training Loss  0.009918262250721455\n",
            "Epoch  32 Batch  15 / 525  Training Loss  0.001928777783177793\n",
            "Epoch  32 Batch  16 / 525  Training Loss  0.014534013345837593\n",
            "Epoch  32 Batch  17 / 525  Training Loss  0.01405585277825594\n",
            "Epoch  32 Batch  18 / 525  Training Loss  0.00862230733036995\n",
            "Epoch  32 Batch  19 / 525  Training Loss  0.0022439141757786274\n",
            "Epoch  32 Batch  20 / 525  Training Loss  0.01711832731962204\n",
            "Epoch  32 Batch  21 / 525  Training Loss  0.012099793180823326\n",
            "Epoch  32 Batch  22 / 525  Training Loss  0.0035475152544677258\n",
            "Epoch  32 Batch  23 / 525  Training Loss  0.0011262530460953712\n",
            "Epoch  32 Batch  24 / 525  Training Loss  0.011320939287543297\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  32 Batch  25 / 525  Training Loss  0.0033330097794532776\n",
            "Epoch  32 Batch  26 / 525  Training Loss  0.0016095697646960616\n",
            "Epoch  32 Batch  27 / 525  Training Loss  0.007770200725644827\n",
            "Epoch  32 Batch  28 / 525  Training Loss  0.0042971838265657425\n",
            "Epoch  32 Batch  29 / 525  Training Loss  0.004066403955221176\n",
            "Epoch  32 Batch  30 / 525  Training Loss  0.01211685873568058\n",
            "Epoch  32 Batch  31 / 525  Training Loss  0.006601898465305567\n",
            "Epoch  32 Batch  32 / 525  Training Loss  0.00947655364871025\n",
            "Epoch  32 Batch  33 / 525  Training Loss  0.002578376792371273\n",
            "Epoch  32 Batch  34 / 525  Training Loss  0.0052966284565627575\n",
            "Epoch  32 Batch  35 / 525  Training Loss  0.002022217260673642\n",
            "Epoch  32 Batch  36 / 525  Training Loss  0.0037988622207194567\n",
            "Epoch  32 Batch  37 / 525  Training Loss  0.003270822111517191\n",
            "Epoch  32 Batch  38 / 525  Training Loss  0.007964057847857475\n",
            "Epoch  32 Batch  39 / 525  Training Loss  0.002314331941306591\n",
            "Epoch  32 Batch  40 / 525  Training Loss  0.0018865263555198908\n",
            "Epoch  32 Batch  41 / 525  Training Loss  0.0037342540454119444\n",
            "Epoch  32 Batch  42 / 525  Training Loss  0.0034969814587384462\n",
            "Epoch  32 Batch  43 / 525  Training Loss  0.01269945316016674\n",
            "Epoch  32 Batch  44 / 525  Training Loss  0.0043001435697078705\n",
            "Epoch  32 Batch  45 / 525  Training Loss  0.010322319343686104\n",
            "Epoch  32 Batch  46 / 525  Training Loss  0.006109684240072966\n",
            "Epoch  32 Batch  47 / 525  Training Loss  0.007855275645852089\n",
            "Epoch  32 Batch  48 / 525  Training Loss  0.0034862346947193146\n",
            "Epoch  32 Batch  49 / 525  Training Loss  0.002940719248726964\n",
            "Epoch  32 Batch  50 / 525  Training Loss  0.0010879567125812173\n",
            "Epoch  32 Batch  51 / 525  Training Loss  0.009474923834204674\n",
            "Epoch  32 Batch  52 / 525  Training Loss  0.0016545029357075691\n",
            "Epoch  32 Batch  53 / 525  Training Loss  0.0049031442031264305\n",
            "Epoch  32 Batch  54 / 525  Training Loss  0.0013536670012399554\n",
            "Epoch  32 Batch  55 / 525  Training Loss  0.0003673156606964767\n",
            "Epoch  32 Batch  56 / 525  Training Loss  0.00929329451173544\n",
            "Epoch  32 Batch  57 / 525  Training Loss  0.0006055921548977494\n",
            "Epoch  32 Batch  58 / 525  Training Loss  0.012923155911266804\n",
            "Epoch  32 Batch  59 / 525  Training Loss  0.005654721986502409\n",
            "Epoch  32 Batch  60 / 525  Training Loss  0.000811890116892755\n",
            "Epoch  32 Batch  61 / 525  Training Loss  0.011838773265480995\n",
            "Epoch  32 Batch  62 / 525  Training Loss  0.0028739331755787134\n",
            "Epoch  32 Batch  63 / 525  Training Loss  0.004505916498601437\n",
            "Epoch  32 Batch  64 / 525  Training Loss  0.007604809943586588\n",
            "Epoch  32 Batch  65 / 525  Training Loss  0.0006723344558849931\n",
            "Epoch  32 Batch  66 / 525  Training Loss  0.007916862145066261\n",
            "Epoch  32 Batch  67 / 525  Training Loss  0.007056745700538158\n",
            "Epoch  32 Batch  68 / 525  Training Loss  0.004562528803944588\n",
            "Epoch  32 Batch  69 / 525  Training Loss  0.011708922684192657\n",
            "Epoch  32 Batch  70 / 525  Training Loss  0.00914401188492775\n",
            "Epoch  32 Batch  71 / 525  Training Loss  0.00419812323525548\n",
            "Epoch  32 Batch  72 / 525  Training Loss  0.007020742632448673\n",
            "Epoch  32 Batch  73 / 525  Training Loss  0.018567349761724472\n",
            "Epoch  32 Batch  74 / 525  Training Loss  0.0038603530265390873\n",
            "Epoch  32 Batch  75 / 525  Training Loss  0.00970374047756195\n",
            "Epoch  32 Batch  76 / 525  Training Loss  0.009559457190334797\n",
            "Epoch  32 Batch  77 / 525  Training Loss  0.0025260234251618385\n",
            "Epoch  32 Batch  78 / 525  Training Loss  0.007681276649236679\n",
            "Epoch  32 Batch  79 / 525  Training Loss  0.022005030885338783\n",
            "Epoch  32 Batch  80 / 525  Training Loss  0.010275508277118206\n",
            "Epoch  32 Batch  81 / 525  Training Loss  0.008994592353701591\n",
            "Epoch  32 Batch  82 / 525  Training Loss  0.006605935283005238\n",
            "Epoch  32 Batch  83 / 525  Training Loss  0.006238252855837345\n",
            "Epoch  32 Batch  84 / 525  Training Loss  0.002246833872050047\n",
            "Epoch  32 Batch  85 / 525  Training Loss  0.005938214715570211\n",
            "Epoch  32 Batch  86 / 525  Training Loss  0.0024256191682070494\n",
            "Epoch  32 Batch  87 / 525  Training Loss  0.0029307971708476543\n",
            "Epoch  32 Batch  88 / 525  Training Loss  0.0026812341529875994\n",
            "Epoch  32 Batch  89 / 525  Training Loss  0.003000499214977026\n",
            "Epoch  32 Batch  90 / 525  Training Loss  0.0024579642340540886\n",
            "Epoch  32 Batch  91 / 525  Training Loss  0.009585787542164326\n",
            "Epoch  32 Batch  92 / 525  Training Loss  0.004660584963858128\n",
            "Epoch  32 Batch  93 / 525  Training Loss  0.001052262494340539\n",
            "Epoch  32 Batch  94 / 525  Training Loss  0.0005407350836321712\n",
            "Epoch  32 Batch  95 / 525  Training Loss  0.007906349375844002\n",
            "Epoch  32 Batch  96 / 525  Training Loss  0.006239742506295443\n",
            "Epoch  32 Batch  97 / 525  Training Loss  0.009805754758417606\n",
            "Epoch  32 Batch  98 / 525  Training Loss  0.004359554499387741\n",
            "Epoch  32 Batch  99 / 525  Training Loss  0.0020102509297430515\n",
            "Epoch  32 Batch  100 / 525  Training Loss  0.0019441725453361869\n",
            "Epoch  32 Batch  101 / 525  Training Loss  0.0033926465548574924\n",
            "Epoch  32 Batch  102 / 525  Training Loss  0.002143243094906211\n",
            "Epoch  32 Batch  103 / 525  Training Loss  0.007039403077214956\n",
            "Epoch  32 Batch  104 / 525  Training Loss  0.0024062583688646555\n",
            "Epoch  32 Batch  105 / 525  Training Loss  0.00616240780800581\n",
            "Epoch  32 Batch  106 / 525  Training Loss  0.004908392671495676\n",
            "Epoch  32 Batch  107 / 525  Training Loss  0.011304501444101334\n",
            "Epoch  32 Batch  108 / 525  Training Loss  0.003949644975364208\n",
            "Epoch  32 Batch  109 / 525  Training Loss  0.0061845434829592705\n",
            "Epoch  32 Batch  110 / 525  Training Loss  0.0018559275195002556\n",
            "Epoch  32 Batch  111 / 525  Training Loss  0.003485304769128561\n",
            "Epoch  32 Batch  112 / 525  Training Loss  0.002535804407671094\n",
            "Epoch  32 Batch  113 / 525  Training Loss  0.00264861062169075\n",
            "Epoch  32 Batch  114 / 525  Training Loss  0.009548786096274853\n",
            "Epoch  32 Batch  115 / 525  Training Loss  0.004102093167603016\n",
            "Epoch  32 Batch  116 / 525  Training Loss  0.010511314496397972\n",
            "Epoch  32 Batch  117 / 525  Training Loss  0.007607348263263702\n",
            "Epoch  32 Batch  118 / 525  Training Loss  0.001418414176441729\n",
            "Epoch  32 Batch  119 / 525  Training Loss  0.0065704346634447575\n",
            "Epoch  32 Batch  120 / 525  Training Loss  0.0009535690769553185\n",
            "Epoch  32 Batch  121 / 525  Training Loss  0.004924962297081947\n",
            "Epoch  32 Batch  122 / 525  Training Loss  0.0019713151268661022\n",
            "Epoch  32 Batch  123 / 525  Training Loss  0.00959003996104002\n",
            "Epoch  32 Batch  124 / 525  Training Loss  0.007419494446367025\n",
            "Epoch  32 Batch  125 / 525  Training Loss  0.0027318927459418774\n",
            "Epoch  32 Batch  126 / 525  Training Loss  0.004661683924496174\n",
            "Epoch  32 Batch  127 / 525  Training Loss  0.0005050227628089488\n",
            "Epoch  32 Batch  128 / 525  Training Loss  0.007564237806946039\n",
            "Epoch  32 Batch  129 / 525  Training Loss  0.006049659103155136\n",
            "Epoch  32 Batch  130 / 525  Training Loss  0.003651364240795374\n",
            "Epoch  32 Batch  131 / 525  Training Loss  0.002779111498966813\n",
            "Epoch  32 Batch  132 / 525  Training Loss  0.001027823775075376\n",
            "Epoch  32 Batch  133 / 525  Training Loss  0.0016285751480609179\n",
            "Epoch  32 Batch  134 / 525  Training Loss  0.0026406566612422466\n",
            "Epoch  32 Batch  135 / 525  Training Loss  0.002299562096595764\n",
            "Epoch  32 Batch  136 / 525  Training Loss  0.0023645414039492607\n",
            "Epoch  32 Batch  137 / 525  Training Loss  0.0038149666506797075\n",
            "Epoch  32 Batch  138 / 525  Training Loss  0.004361394327133894\n",
            "Epoch  32 Batch  139 / 525  Training Loss  0.0039644623175263405\n",
            "Epoch  32 Batch  140 / 525  Training Loss  0.0009548381203785539\n",
            "Epoch  32 Batch  141 / 525  Training Loss  0.005916436668485403\n",
            "Epoch  32 Batch  142 / 525  Training Loss  0.0011673560366034508\n",
            "Epoch  32 Batch  143 / 525  Training Loss  0.005387770943343639\n",
            "Epoch  32 Batch  144 / 525  Training Loss  0.006770063191652298\n",
            "Epoch  32 Batch  145 / 525  Training Loss  0.0027767110150307417\n",
            "Epoch  32 Batch  146 / 525  Training Loss  0.002527473494410515\n",
            "Epoch  32 Batch  147 / 525  Training Loss  0.004290965851396322\n",
            "Epoch  32 Batch  148 / 525  Training Loss  0.0028965845704078674\n",
            "Epoch  32 Batch  149 / 525  Training Loss  0.008247939869761467\n",
            "Epoch  32 Batch  150 / 525  Training Loss  0.004478607326745987\n",
            "Epoch  32 Batch  151 / 525  Training Loss  0.00869251973927021\n",
            "Epoch  32 Batch  152 / 525  Training Loss  0.0031189550645649433\n",
            "Epoch  32 Batch  153 / 525  Training Loss  0.005226641893386841\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  32 Batch  154 / 525  Training Loss  0.005258369259536266\n",
            "Epoch  32 Batch  155 / 525  Training Loss  0.005432239267975092\n",
            "Epoch  32 Batch  156 / 525  Training Loss  0.00628627697005868\n",
            "Epoch  32 Batch  157 / 525  Training Loss  0.0046202377416193485\n",
            "Epoch  32 Batch  158 / 525  Training Loss  0.0016061539063230157\n",
            "Epoch  32 Batch  159 / 525  Training Loss  0.005846975836902857\n",
            "Epoch  32 Batch  160 / 525  Training Loss  0.012489279732108116\n",
            "Epoch  32 Batch  161 / 525  Training Loss  0.0064643076620996\n",
            "Epoch  32 Batch  162 / 525  Training Loss  0.01578638330101967\n",
            "Epoch  32 Batch  163 / 525  Training Loss  0.001761366263963282\n",
            "Epoch  32 Batch  164 / 525  Training Loss  0.010376841761171818\n",
            "Epoch  32 Batch  165 / 525  Training Loss  0.006991632282733917\n",
            "Epoch  32 Batch  166 / 525  Training Loss  0.005911434534937143\n",
            "Epoch  32 Batch  167 / 525  Training Loss  0.0007632157648913562\n",
            "Epoch  32 Batch  168 / 525  Training Loss  0.007418314926326275\n",
            "Epoch  32 Batch  169 / 525  Training Loss  0.004879659973084927\n",
            "Epoch  32 Batch  170 / 525  Training Loss  0.019930003210902214\n",
            "Epoch  32 Batch  171 / 525  Training Loss  0.004736681934446096\n",
            "Epoch  32 Batch  172 / 525  Training Loss  0.00406644307076931\n",
            "Epoch  32 Batch  173 / 525  Training Loss  0.006095302756875753\n",
            "Epoch  32 Batch  174 / 525  Training Loss  0.004186922684311867\n",
            "Epoch  32 Batch  175 / 525  Training Loss  0.007638528011739254\n",
            "Epoch  32 Batch  176 / 525  Training Loss  0.002265583723783493\n",
            "Epoch  32 Batch  177 / 525  Training Loss  0.003476234618574381\n",
            "Epoch  32 Batch  178 / 525  Training Loss  0.0038001753855496645\n",
            "Epoch  32 Batch  179 / 525  Training Loss  0.007695876993238926\n",
            "Epoch  32 Batch  180 / 525  Training Loss  0.0017891715979203582\n",
            "Epoch  32 Batch  181 / 525  Training Loss  0.007163570262491703\n",
            "Epoch  32 Batch  182 / 525  Training Loss  0.0065134866163134575\n",
            "Epoch  32 Batch  183 / 525  Training Loss  0.005640671122819185\n",
            "Epoch  32 Batch  184 / 525  Training Loss  0.0022293145302683115\n",
            "Epoch  32 Batch  185 / 525  Training Loss  0.0031518605537712574\n",
            "Epoch  32 Batch  186 / 525  Training Loss  0.002612608950585127\n",
            "Epoch  32 Batch  187 / 525  Training Loss  0.0019630310125648975\n",
            "Epoch  32 Batch  188 / 525  Training Loss  0.002263881964609027\n",
            "Epoch  32 Batch  189 / 525  Training Loss  0.0005582870217040181\n",
            "Epoch  32 Batch  190 / 525  Training Loss  0.0042651137337088585\n",
            "Epoch  32 Batch  191 / 525  Training Loss  0.003481884254142642\n",
            "Epoch  32 Batch  192 / 525  Training Loss  0.008592543192207813\n",
            "Epoch  32 Batch  193 / 525  Training Loss  0.005303146783262491\n",
            "Epoch  32 Batch  194 / 525  Training Loss  0.0040351031348109245\n",
            "Epoch  32 Batch  195 / 525  Training Loss  0.0005649134982377291\n",
            "Epoch  32 Batch  196 / 525  Training Loss  0.0008077998645603657\n",
            "Epoch  32 Batch  197 / 525  Training Loss  0.013833107426762581\n",
            "Epoch  32 Batch  198 / 525  Training Loss  0.006156712770462036\n",
            "Epoch  32 Batch  199 / 525  Training Loss  0.00420360267162323\n",
            "Epoch  32 Batch  200 / 525  Training Loss  0.0011858291691169143\n",
            "Epoch  32 Batch  201 / 525  Training Loss  0.014126213267445564\n",
            "Epoch  32 Batch  202 / 525  Training Loss  0.0030654475558549166\n",
            "Epoch  32 Batch  203 / 525  Training Loss  0.005752545781433582\n",
            "Epoch  32 Batch  204 / 525  Training Loss  0.007186408154666424\n",
            "Epoch  32 Batch  205 / 525  Training Loss  0.010182189755141735\n",
            "Epoch  32 Batch  206 / 525  Training Loss  0.005129969213157892\n",
            "Epoch  32 Batch  207 / 525  Training Loss  0.01057647354900837\n",
            "Epoch  32 Batch  208 / 525  Training Loss  0.018480993807315826\n",
            "Epoch  32 Batch  209 / 525  Training Loss  0.0070956372655928135\n",
            "Epoch  32 Batch  210 / 525  Training Loss  0.008140910416841507\n",
            "Epoch  32 Batch  211 / 525  Training Loss  0.012349833734333515\n",
            "Epoch  32 Batch  212 / 525  Training Loss  0.006843694485723972\n",
            "Epoch  32 Batch  213 / 525  Training Loss  0.008783881552517414\n",
            "Epoch  32 Batch  214 / 525  Training Loss  0.0035202372819185257\n",
            "Epoch  32 Batch  215 / 525  Training Loss  0.022035885602235794\n",
            "Epoch  32 Batch  216 / 525  Training Loss  0.0056137326173484325\n",
            "Epoch  32 Batch  217 / 525  Training Loss  0.0019239854300394654\n",
            "Epoch  32 Batch  218 / 525  Training Loss  0.005694958381354809\n",
            "Epoch  32 Batch  219 / 525  Training Loss  0.0018504641484469175\n",
            "Epoch  32 Batch  220 / 525  Training Loss  0.005383315496146679\n",
            "Epoch  32 Batch  221 / 525  Training Loss  0.0013353656977415085\n",
            "Epoch  32 Batch  222 / 525  Training Loss  0.0032225705217570066\n",
            "Epoch  32 Batch  223 / 525  Training Loss  0.004055607598274946\n",
            "Epoch  32 Batch  224 / 525  Training Loss  0.004098844714462757\n",
            "Epoch  32 Batch  225 / 525  Training Loss  0.010211586020886898\n",
            "Epoch  32 Batch  226 / 525  Training Loss  0.004727861378341913\n",
            "Epoch  32 Batch  227 / 525  Training Loss  0.003467906266450882\n",
            "Epoch  32 Batch  228 / 525  Training Loss  0.005663686897605658\n",
            "Epoch  32 Batch  229 / 525  Training Loss  0.020489880815148354\n",
            "Epoch  32 Batch  230 / 525  Training Loss  0.0036017377860844135\n",
            "Epoch  32 Batch  231 / 525  Training Loss  0.0024981857277452946\n",
            "Epoch  32 Batch  232 / 525  Training Loss  0.004933892749249935\n",
            "Epoch  32 Batch  233 / 525  Training Loss  0.007195126265287399\n",
            "Epoch  32 Batch  234 / 525  Training Loss  0.008356650359928608\n",
            "Epoch  32 Batch  235 / 525  Training Loss  0.0035385428927838802\n",
            "Epoch  32 Batch  236 / 525  Training Loss  0.007294746581465006\n",
            "Epoch  32 Batch  237 / 525  Training Loss  0.008973420597612858\n",
            "Epoch  32 Batch  238 / 525  Training Loss  0.004282676614820957\n",
            "Epoch  32 Batch  239 / 525  Training Loss  0.02261361852288246\n",
            "Epoch  32 Batch  240 / 525  Training Loss  0.012401558458805084\n",
            "Epoch  32 Batch  241 / 525  Training Loss  0.001316039590165019\n",
            "Epoch  32 Batch  242 / 525  Training Loss  0.0016933940351009369\n",
            "Epoch  32 Batch  243 / 525  Training Loss  0.0051977792754769325\n",
            "Epoch  32 Batch  244 / 525  Training Loss  0.00722276559099555\n",
            "Epoch  32 Batch  245 / 525  Training Loss  0.0018185216467827559\n",
            "Epoch  32 Batch  246 / 525  Training Loss  0.003135723527520895\n",
            "Epoch  32 Batch  247 / 525  Training Loss  0.0008865393465384841\n",
            "Epoch  32 Batch  248 / 525  Training Loss  0.005253014154732227\n",
            "Epoch  32 Batch  249 / 525  Training Loss  0.011030744761228561\n",
            "Epoch  32 Batch  250 / 525  Training Loss  0.0006977545563131571\n",
            "Epoch  32 Batch  251 / 525  Training Loss  0.006695991847664118\n",
            "Epoch  32 Batch  252 / 525  Training Loss  0.0017939588287845254\n",
            "Epoch  32 Batch  253 / 525  Training Loss  0.005952778737992048\n",
            "Epoch  32 Batch  254 / 525  Training Loss  0.0032162300776690245\n",
            "Epoch  32 Batch  255 / 525  Training Loss  0.0016314077656716108\n",
            "Epoch  32 Batch  256 / 525  Training Loss  0.002271949779242277\n",
            "Epoch  32 Batch  257 / 525  Training Loss  0.004591950215399265\n",
            "Epoch  32 Batch  258 / 525  Training Loss  0.0027063731104135513\n",
            "Epoch  32 Batch  259 / 525  Training Loss  0.0055957273580133915\n",
            "Epoch  32 Batch  260 / 525  Training Loss  0.0037199375219643116\n",
            "Epoch  32 Batch  261 / 525  Training Loss  0.006373799405992031\n",
            "Epoch  32 Batch  262 / 525  Training Loss  0.00660574808716774\n",
            "Epoch  32 Batch  263 / 525  Training Loss  0.0034720406401902437\n",
            "Epoch  32 Batch  264 / 525  Training Loss  0.0023629709612578154\n",
            "Epoch  32 Batch  265 / 525  Training Loss  0.0008501804550178349\n",
            "Epoch  32 Batch  266 / 525  Training Loss  0.008552326820790768\n",
            "Epoch  32 Batch  267 / 525  Training Loss  0.004004067741334438\n",
            "Epoch  32 Batch  268 / 525  Training Loss  0.005577733740210533\n",
            "Epoch  32 Batch  269 / 525  Training Loss  0.001761685125529766\n",
            "Epoch  32 Batch  270 / 525  Training Loss  0.0020278948359191418\n",
            "Epoch  32 Batch  271 / 525  Training Loss  0.005529133137315512\n",
            "Epoch  32 Batch  272 / 525  Training Loss  0.0028056877199560404\n",
            "Epoch  32 Batch  273 / 525  Training Loss  0.0024947687052190304\n",
            "Epoch  32 Batch  274 / 525  Training Loss  0.0066042738035321236\n",
            "Epoch  32 Batch  275 / 525  Training Loss  0.0019483078503981233\n",
            "Epoch  32 Batch  276 / 525  Training Loss  0.009916660375893116\n",
            "Epoch  32 Batch  277 / 525  Training Loss  0.0047828322276473045\n",
            "Epoch  32 Batch  278 / 525  Training Loss  0.007936414331197739\n",
            "Epoch  32 Batch  279 / 525  Training Loss  0.0025743446312844753\n",
            "Epoch  32 Batch  280 / 525  Training Loss  0.001210944028571248\n",
            "Epoch  32 Batch  281 / 525  Training Loss  0.0007198971579782665\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  32 Batch  282 / 525  Training Loss  0.0016807252541184425\n",
            "Epoch  32 Batch  283 / 525  Training Loss  0.0006551524857059121\n",
            "Epoch  32 Batch  284 / 525  Training Loss  0.0015835228841751814\n",
            "Epoch  32 Batch  285 / 525  Training Loss  0.0026246560737490654\n",
            "Epoch  32 Batch  286 / 525  Training Loss  0.009456693194806576\n",
            "Epoch  32 Batch  287 / 525  Training Loss  0.003040675073862076\n",
            "Epoch  32 Batch  288 / 525  Training Loss  0.001884137629531324\n",
            "Epoch  32 Batch  289 / 525  Training Loss  0.0030560134910047054\n",
            "Epoch  32 Batch  290 / 525  Training Loss  0.015513377264142036\n",
            "Epoch  32 Batch  291 / 525  Training Loss  0.005909493193030357\n",
            "Epoch  32 Batch  292 / 525  Training Loss  0.0017597073456272483\n",
            "Epoch  32 Batch  293 / 525  Training Loss  0.0042383624240756035\n",
            "Epoch  32 Batch  294 / 525  Training Loss  0.004652767442166805\n",
            "Epoch  32 Batch  295 / 525  Training Loss  0.003336660098284483\n",
            "Epoch  32 Batch  296 / 525  Training Loss  0.004993712529540062\n",
            "Epoch  32 Batch  297 / 525  Training Loss  0.005023744888603687\n",
            "Epoch  32 Batch  298 / 525  Training Loss  0.006577306892722845\n",
            "Epoch  32 Batch  299 / 525  Training Loss  0.001446256646886468\n",
            "Epoch  32 Batch  300 / 525  Training Loss  0.0013895876472815871\n",
            "Epoch  32 Batch  301 / 525  Training Loss  0.006630535237491131\n",
            "Epoch  32 Batch  302 / 525  Training Loss  0.004612886346876621\n",
            "Epoch  32 Batch  303 / 525  Training Loss  0.002481370698660612\n",
            "Epoch  32 Batch  304 / 525  Training Loss  0.006528381258249283\n",
            "Epoch  32 Batch  305 / 525  Training Loss  0.004186300560832024\n",
            "Epoch  32 Batch  306 / 525  Training Loss  0.003552861511707306\n",
            "Epoch  32 Batch  307 / 525  Training Loss  0.008618518710136414\n",
            "Epoch  32 Batch  308 / 525  Training Loss  0.0057901195250451565\n",
            "Epoch  32 Batch  309 / 525  Training Loss  0.0031853565014898777\n",
            "Epoch  32 Batch  310 / 525  Training Loss  0.006500615738332272\n",
            "Epoch  32 Batch  311 / 525  Training Loss  0.014433948323130608\n",
            "Epoch  32 Batch  312 / 525  Training Loss  0.0016271620988845825\n",
            "Epoch  32 Batch  313 / 525  Training Loss  0.007514918688684702\n",
            "Epoch  32 Batch  314 / 525  Training Loss  0.009875509887933731\n",
            "Epoch  32 Batch  315 / 525  Training Loss  0.006002127192914486\n",
            "Epoch  32 Batch  316 / 525  Training Loss  0.0016539301723241806\n",
            "Epoch  32 Batch  317 / 525  Training Loss  0.0026368543040007353\n",
            "Epoch  32 Batch  318 / 525  Training Loss  0.002357888501137495\n",
            "Epoch  32 Batch  319 / 525  Training Loss  0.0023824127856642008\n",
            "Epoch  32 Batch  320 / 525  Training Loss  0.006353083066642284\n",
            "Epoch  32 Batch  321 / 525  Training Loss  0.0026662214659154415\n",
            "Epoch  32 Batch  322 / 525  Training Loss  0.0034020249731838703\n",
            "Epoch  32 Batch  323 / 525  Training Loss  0.003432330209761858\n",
            "Epoch  32 Batch  324 / 525  Training Loss  0.009461174719035625\n",
            "Epoch  32 Batch  325 / 525  Training Loss  0.00203746953047812\n",
            "Epoch  32 Batch  326 / 525  Training Loss  0.01199646107852459\n",
            "Epoch  32 Batch  327 / 525  Training Loss  0.004309342708438635\n",
            "Epoch  32 Batch  328 / 525  Training Loss  0.0016042732167989016\n",
            "Epoch  32 Batch  329 / 525  Training Loss  0.0018280692165717483\n",
            "Epoch  32 Batch  330 / 525  Training Loss  0.00541871041059494\n",
            "Epoch  32 Batch  331 / 525  Training Loss  0.008838267996907234\n",
            "Epoch  32 Batch  332 / 525  Training Loss  0.014712977223098278\n",
            "Epoch  32 Batch  333 / 525  Training Loss  0.003368498757481575\n",
            "Epoch  32 Batch  334 / 525  Training Loss  0.002249270211905241\n",
            "Epoch  32 Batch  335 / 525  Training Loss  0.002240143483504653\n",
            "Epoch  32 Batch  336 / 525  Training Loss  0.020776236429810524\n",
            "Epoch  32 Batch  337 / 525  Training Loss  0.008120547980070114\n",
            "Epoch  32 Batch  338 / 525  Training Loss  0.003243881743401289\n",
            "Epoch  32 Batch  339 / 525  Training Loss  0.01087083201855421\n",
            "Epoch  32 Batch  340 / 525  Training Loss  0.007096294313669205\n",
            "Epoch  32 Batch  341 / 525  Training Loss  0.0015346028376370668\n",
            "Epoch  32 Batch  342 / 525  Training Loss  0.008388372138142586\n",
            "Epoch  32 Batch  343 / 525  Training Loss  0.020307093858718872\n",
            "Epoch  32 Batch  344 / 525  Training Loss  0.005174257326871157\n",
            "Epoch  32 Batch  345 / 525  Training Loss  0.007374552544206381\n",
            "Epoch  32 Batch  346 / 525  Training Loss  0.001273626578040421\n",
            "Epoch  32 Batch  347 / 525  Training Loss  0.004033136647194624\n",
            "Epoch  32 Batch  348 / 525  Training Loss  0.002018302446231246\n",
            "Epoch  32 Batch  349 / 525  Training Loss  0.0030532157979905605\n",
            "Epoch  32 Batch  350 / 525  Training Loss  0.0012982130283489823\n",
            "Epoch  32 Batch  351 / 525  Training Loss  0.0014768418623134494\n",
            "Epoch  32 Batch  352 / 525  Training Loss  0.003167795017361641\n",
            "Epoch  32 Batch  353 / 525  Training Loss  0.003102264367043972\n",
            "Epoch  32 Batch  354 / 525  Training Loss  0.002163338242098689\n",
            "Epoch  32 Batch  355 / 525  Training Loss  0.0013524626847356558\n",
            "Epoch  32 Batch  356 / 525  Training Loss  0.003888919483870268\n",
            "Epoch  32 Batch  357 / 525  Training Loss  0.009777890518307686\n",
            "Epoch  32 Batch  358 / 525  Training Loss  0.0017339866608381271\n",
            "Epoch  32 Batch  359 / 525  Training Loss  0.0028468866366893053\n",
            "Epoch  32 Batch  360 / 525  Training Loss  0.0025160557124763727\n",
            "Epoch  32 Batch  361 / 525  Training Loss  0.012096617370843887\n",
            "Epoch  32 Batch  362 / 525  Training Loss  0.006553805433213711\n",
            "Epoch  32 Batch  363 / 525  Training Loss  0.01129878032952547\n",
            "Epoch  32 Batch  364 / 525  Training Loss  0.009157771244645119\n",
            "Epoch  32 Batch  365 / 525  Training Loss  0.0005571599467657506\n",
            "Epoch  32 Batch  366 / 525  Training Loss  0.002523904200643301\n",
            "Epoch  32 Batch  367 / 525  Training Loss  0.0012173220748081803\n",
            "Epoch  32 Batch  368 / 525  Training Loss  0.001911157974973321\n",
            "Epoch  32 Batch  369 / 525  Training Loss  0.002164894714951515\n",
            "Epoch  32 Batch  370 / 525  Training Loss  0.0012921371962875128\n",
            "Epoch  32 Batch  371 / 525  Training Loss  0.00782833993434906\n",
            "Epoch  32 Batch  372 / 525  Training Loss  0.021245528012514114\n",
            "Epoch  32 Batch  373 / 525  Training Loss  0.0055469186045229435\n",
            "Epoch  32 Batch  374 / 525  Training Loss  0.004482727497816086\n",
            "Epoch  32 Batch  375 / 525  Training Loss  0.003791187424212694\n",
            "Epoch  32 Batch  376 / 525  Training Loss  0.01518381666392088\n",
            "Epoch  32 Batch  377 / 525  Training Loss  0.007749135605990887\n",
            "Epoch  32 Batch  378 / 525  Training Loss  0.008776136673986912\n",
            "Epoch  32 Batch  379 / 525  Training Loss  0.0013861472252756357\n",
            "Epoch  32 Batch  380 / 525  Training Loss  0.008682964369654655\n",
            "Epoch  32 Batch  381 / 525  Training Loss  0.007368835620582104\n",
            "Epoch  32 Batch  382 / 525  Training Loss  0.004745130427181721\n",
            "Epoch  32 Batch  383 / 525  Training Loss  0.0025990912690758705\n",
            "Epoch  32 Batch  384 / 525  Training Loss  0.001694403006695211\n",
            "Epoch  32 Batch  385 / 525  Training Loss  0.0010226068552583456\n",
            "Epoch  32 Batch  386 / 525  Training Loss  0.004829044919461012\n",
            "Epoch  32 Batch  387 / 525  Training Loss  0.012002464383840561\n",
            "Epoch  32 Batch  388 / 525  Training Loss  0.00895204208791256\n",
            "Epoch  32 Batch  389 / 525  Training Loss  0.010675868019461632\n",
            "Epoch  32 Batch  390 / 525  Training Loss  0.006034368183463812\n",
            "Epoch  32 Batch  391 / 525  Training Loss  0.014633366838097572\n",
            "Epoch  32 Batch  392 / 525  Training Loss  0.009339792653918266\n",
            "Epoch  32 Batch  393 / 525  Training Loss  0.009115489199757576\n",
            "Epoch  32 Batch  394 / 525  Training Loss  0.003620927454903722\n",
            "Epoch  32 Batch  395 / 525  Training Loss  0.0023352811112999916\n",
            "Epoch  32 Batch  396 / 525  Training Loss  0.010905359871685505\n",
            "Epoch  32 Batch  397 / 525  Training Loss  0.005937530659139156\n",
            "Epoch  32 Batch  398 / 525  Training Loss  0.002512488281354308\n",
            "Epoch  32 Batch  399 / 525  Training Loss  0.00222543952986598\n",
            "Epoch  32 Batch  400 / 525  Training Loss  0.009243104606866837\n",
            "Epoch  32 Batch  401 / 525  Training Loss  0.0039302813820540905\n",
            "Epoch  32 Batch  402 / 525  Training Loss  0.012311773374676704\n",
            "Epoch  32 Batch  403 / 525  Training Loss  0.012624231167137623\n",
            "Epoch  32 Batch  404 / 525  Training Loss  0.007570435293018818\n",
            "Epoch  32 Batch  405 / 525  Training Loss  0.004606780596077442\n",
            "Epoch  32 Batch  406 / 525  Training Loss  0.006106535904109478\n",
            "Epoch  32 Batch  407 / 525  Training Loss  0.002329998416826129\n",
            "Epoch  32 Batch  408 / 525  Training Loss  0.0031302603892982006\n",
            "Epoch  32 Batch  409 / 525  Training Loss  0.0038632391951978207\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  32 Batch  410 / 525  Training Loss  0.0024042068980634212\n",
            "Epoch  32 Batch  411 / 525  Training Loss  0.0006002679583616555\n",
            "Epoch  32 Batch  412 / 525  Training Loss  0.007961996830999851\n",
            "Epoch  32 Batch  413 / 525  Training Loss  0.007079645060002804\n",
            "Epoch  32 Batch  414 / 525  Training Loss  0.003480656770989299\n",
            "Epoch  32 Batch  415 / 525  Training Loss  0.004936581011861563\n",
            "Epoch  32 Batch  416 / 525  Training Loss  0.004855439532548189\n",
            "Epoch  32 Batch  417 / 525  Training Loss  0.001414519501850009\n",
            "Epoch  32 Batch  418 / 525  Training Loss  0.003483186010271311\n",
            "Epoch  32 Batch  419 / 525  Training Loss  0.0015780131798237562\n",
            "Epoch  32 Batch  420 / 525  Training Loss  0.00394967757165432\n",
            "Epoch  32 Batch  421 / 525  Training Loss  0.002788177691400051\n",
            "Epoch  32 Batch  422 / 525  Training Loss  0.004499164409935474\n",
            "Epoch  32 Batch  423 / 525  Training Loss  0.004262332804501057\n",
            "Epoch  32 Batch  424 / 525  Training Loss  0.005374975502490997\n",
            "Epoch  32 Batch  425 / 525  Training Loss  0.013105681166052818\n",
            "Epoch  32 Batch  426 / 525  Training Loss  0.005675249267369509\n",
            "Epoch  32 Batch  427 / 525  Training Loss  0.016990739852190018\n",
            "Epoch  32 Batch  428 / 525  Training Loss  0.003291021566838026\n",
            "Epoch  32 Batch  429 / 525  Training Loss  0.007812881842255592\n",
            "Epoch  32 Batch  430 / 525  Training Loss  0.0030723458621650934\n",
            "Epoch  32 Batch  431 / 525  Training Loss  0.0034332729410380125\n",
            "Epoch  32 Batch  432 / 525  Training Loss  0.0019483508076518774\n",
            "Epoch  32 Batch  433 / 525  Training Loss  0.0030608067754656076\n",
            "Epoch  32 Batch  434 / 525  Training Loss  0.0028795511461794376\n",
            "Epoch  32 Batch  435 / 525  Training Loss  0.0020956420339643955\n",
            "Epoch  32 Batch  436 / 525  Training Loss  0.005185498856008053\n",
            "Epoch  32 Batch  437 / 525  Training Loss  0.002106702420860529\n",
            "Epoch  32 Batch  438 / 525  Training Loss  0.00439167395234108\n",
            "Epoch  32 Batch  439 / 525  Training Loss  0.006968715228140354\n",
            "Epoch  32 Batch  440 / 525  Training Loss  0.002702120691537857\n",
            "Epoch  32 Batch  441 / 525  Training Loss  0.002648914698511362\n",
            "Epoch  32 Batch  442 / 525  Training Loss  0.003363894298672676\n",
            "Epoch  32 Batch  443 / 525  Training Loss  0.0005870839231647551\n",
            "Epoch  32 Batch  444 / 525  Training Loss  0.004134214948862791\n",
            "Epoch  32 Batch  445 / 525  Training Loss  0.0032634087838232517\n",
            "Epoch  32 Batch  446 / 525  Training Loss  0.002270266180858016\n",
            "Epoch  32 Batch  447 / 525  Training Loss  0.003694822546094656\n",
            "Epoch  32 Batch  448 / 525  Training Loss  0.0018357349326834083\n",
            "Epoch  32 Batch  449 / 525  Training Loss  0.012335892766714096\n",
            "Epoch  32 Batch  450 / 525  Training Loss  0.002412437228485942\n",
            "Epoch  32 Batch  451 / 525  Training Loss  0.009703487157821655\n",
            "Epoch  32 Batch  452 / 525  Training Loss  0.0080708097666502\n",
            "Epoch  32 Batch  453 / 525  Training Loss  0.003911209758371115\n",
            "Epoch  32 Batch  454 / 525  Training Loss  0.010498839430510998\n",
            "Epoch  32 Batch  455 / 525  Training Loss  0.006331463344395161\n",
            "Epoch  32 Batch  456 / 525  Training Loss  0.0013894506264477968\n",
            "Epoch  32 Batch  457 / 525  Training Loss  0.005212568677961826\n",
            "Epoch  32 Batch  458 / 525  Training Loss  0.006073116324841976\n",
            "Epoch  32 Batch  459 / 525  Training Loss  0.0019920547492802143\n",
            "Epoch  32 Batch  460 / 525  Training Loss  0.0036243549548089504\n",
            "Epoch  32 Batch  461 / 525  Training Loss  0.0016841317992657423\n",
            "Epoch  32 Batch  462 / 525  Training Loss  0.013713431544601917\n",
            "Epoch  32 Batch  463 / 525  Training Loss  0.010196574963629246\n",
            "Epoch  32 Batch  464 / 525  Training Loss  0.005567704793065786\n",
            "Epoch  32 Batch  465 / 525  Training Loss  0.0017652955139055848\n",
            "Epoch  32 Batch  466 / 525  Training Loss  0.020732492208480835\n",
            "Epoch  32 Batch  467 / 525  Training Loss  0.0016163745895028114\n",
            "Epoch  32 Batch  468 / 525  Training Loss  0.002206783276051283\n",
            "Epoch  32 Batch  469 / 525  Training Loss  0.004332885146141052\n",
            "Epoch  32 Batch  470 / 525  Training Loss  0.004891890566796064\n",
            "Epoch  32 Batch  471 / 525  Training Loss  0.0028820973820984364\n",
            "Epoch  32 Batch  472 / 525  Training Loss  0.0013585833366960287\n",
            "Epoch  32 Batch  473 / 525  Training Loss  0.0008153300732374191\n",
            "Epoch  32 Batch  474 / 525  Training Loss  0.005621187388896942\n",
            "Epoch  32 Batch  475 / 525  Training Loss  0.0043072751723229885\n",
            "Epoch  32 Batch  476 / 525  Training Loss  0.0020119580440223217\n",
            "Epoch  32 Batch  477 / 525  Training Loss  0.007063365541398525\n",
            "Epoch  32 Batch  478 / 525  Training Loss  0.001049579819664359\n",
            "Epoch  32 Batch  479 / 525  Training Loss  0.0065373145043849945\n",
            "Epoch  32 Batch  480 / 525  Training Loss  0.003412998514249921\n",
            "Epoch  32 Batch  481 / 525  Training Loss  0.003885895712301135\n",
            "Epoch  32 Batch  482 / 525  Training Loss  0.006636796053498983\n",
            "Epoch  32 Batch  483 / 525  Training Loss  0.0019106768304482102\n",
            "Epoch  32 Batch  484 / 525  Training Loss  0.0020470316521823406\n",
            "Epoch  32 Batch  485 / 525  Training Loss  0.010557344183325768\n",
            "Epoch  32 Batch  486 / 525  Training Loss  0.00484669441357255\n",
            "Epoch  32 Batch  487 / 525  Training Loss  0.0046864524483680725\n",
            "Epoch  32 Batch  488 / 525  Training Loss  0.021313542500138283\n",
            "Epoch  32 Batch  489 / 525  Training Loss  0.015306380577385426\n",
            "Epoch  32 Batch  490 / 525  Training Loss  0.021522754803299904\n",
            "Epoch  32 Batch  491 / 525  Training Loss  0.006388416979461908\n",
            "Epoch  32 Batch  492 / 525  Training Loss  0.016504211351275444\n",
            "Epoch  32 Batch  493 / 525  Training Loss  0.005881468765437603\n",
            "Epoch  32 Batch  494 / 525  Training Loss  0.004721333272755146\n",
            "Epoch  32 Batch  495 / 525  Training Loss  0.0013890002155676484\n",
            "Epoch  32 Batch  496 / 525  Training Loss  0.00554612698033452\n",
            "Epoch  32 Batch  497 / 525  Training Loss  0.006191685330122709\n",
            "Epoch  32 Batch  498 / 525  Training Loss  0.0018258275231346488\n",
            "Epoch  32 Batch  499 / 525  Training Loss  0.0038783394265919924\n",
            "Epoch  32 Batch  500 / 525  Training Loss  0.013003694824874401\n",
            "Epoch  32 Batch  501 / 525  Training Loss  0.01224457286298275\n",
            "Epoch  32 Batch  502 / 525  Training Loss  0.0041852425783872604\n",
            "Epoch  32 Batch  503 / 525  Training Loss  0.010313272476196289\n",
            "Epoch  32 Batch  504 / 525  Training Loss  0.0023540728725492954\n",
            "Epoch  32 Batch  505 / 525  Training Loss  0.011587273329496384\n",
            "Epoch  32 Batch  506 / 525  Training Loss  0.002415220020338893\n",
            "Epoch  32 Batch  507 / 525  Training Loss  0.0058816224336624146\n",
            "Epoch  32 Batch  508 / 525  Training Loss  0.0034915157593786716\n",
            "Epoch  32 Batch  509 / 525  Training Loss  0.0023619229905307293\n",
            "Epoch  32 Batch  510 / 525  Training Loss  0.003832319052889943\n",
            "Epoch  32 Batch  511 / 525  Training Loss  0.001317259157076478\n",
            "Epoch  32 Batch  512 / 525  Training Loss  0.010377725586295128\n",
            "Epoch  32 Batch  513 / 525  Training Loss  0.006434851326048374\n",
            "Epoch  32 Batch  514 / 525  Training Loss  0.016254115849733353\n",
            "Epoch  32 Batch  515 / 525  Training Loss  0.005616080015897751\n",
            "Epoch  32 Batch  516 / 525  Training Loss  0.010942434892058372\n",
            "Epoch  32 Batch  517 / 525  Training Loss  0.00610833615064621\n",
            "Epoch  32 Batch  518 / 525  Training Loss  0.0032629012130200863\n",
            "Epoch  32 Batch  519 / 525  Training Loss  0.009824338369071484\n",
            "Epoch  32 Batch  520 / 525  Training Loss  0.008974434807896614\n",
            "Epoch  32 Batch  521 / 525  Training Loss  0.01126046385616064\n",
            "Epoch  32 Batch  522 / 525  Training Loss  0.010862240567803383\n",
            "Epoch  32 Batch  523 / 525  Training Loss  0.0037576346658170223\n",
            "Epoch  32 Batch  524 / 525  Training Loss  0.005852086469531059\n",
            "  33    |    -    |   0.005601   | 59.358333\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 33\n",
            "Epoch  33 Batch  0 / 525  Training Loss  0.0053727999329566956\n",
            "Epoch  33 Batch  1 / 525  Training Loss  0.00258150277659297\n",
            "Epoch  33 Batch  2 / 525  Training Loss  0.004478434566408396\n",
            "Epoch  33 Batch  3 / 525  Training Loss  0.000656656629871577\n",
            "Epoch  33 Batch  4 / 525  Training Loss  0.00028389872750267386\n",
            "Epoch  33 Batch  5 / 525  Training Loss  0.0043786498717963696\n",
            "Epoch  33 Batch  6 / 525  Training Loss  0.0008362190565094352\n",
            "Epoch  33 Batch  7 / 525  Training Loss  0.0019416576251387596\n",
            "Epoch  33 Batch  8 / 525  Training Loss  0.0025506112724542618\n",
            "Epoch  33 Batch  9 / 525  Training Loss  0.0032796342857182026\n",
            "Epoch  33 Batch  10 / 525  Training Loss  0.002786253811791539\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  33 Batch  11 / 525  Training Loss  0.0024292324669659138\n",
            "Epoch  33 Batch  12 / 525  Training Loss  0.013603699393570423\n",
            "Epoch  33 Batch  13 / 525  Training Loss  0.005719445180147886\n",
            "Epoch  33 Batch  14 / 525  Training Loss  0.004542311653494835\n",
            "Epoch  33 Batch  15 / 525  Training Loss  0.005556684453040361\n",
            "Epoch  33 Batch  16 / 525  Training Loss  0.009114494547247887\n",
            "Epoch  33 Batch  17 / 525  Training Loss  0.003432161407545209\n",
            "Epoch  33 Batch  18 / 525  Training Loss  0.008373327553272247\n",
            "Epoch  33 Batch  19 / 525  Training Loss  0.0022019310854375362\n",
            "Epoch  33 Batch  20 / 525  Training Loss  0.0020855064503848553\n",
            "Epoch  33 Batch  21 / 525  Training Loss  0.0010714309755712748\n",
            "Epoch  33 Batch  22 / 525  Training Loss  0.006261770613491535\n",
            "Epoch  33 Batch  23 / 525  Training Loss  0.0018462067237123847\n",
            "Epoch  33 Batch  24 / 525  Training Loss  0.0069732824340462685\n",
            "Epoch  33 Batch  25 / 525  Training Loss  0.0015859405975788832\n",
            "Epoch  33 Batch  26 / 525  Training Loss  0.0030205014627426863\n",
            "Epoch  33 Batch  27 / 525  Training Loss  0.002669400069862604\n",
            "Epoch  33 Batch  28 / 525  Training Loss  0.008157511241734028\n",
            "Epoch  33 Batch  29 / 525  Training Loss  0.001426412258297205\n",
            "Epoch  33 Batch  30 / 525  Training Loss  0.001348567777313292\n",
            "Epoch  33 Batch  31 / 525  Training Loss  0.00120869604870677\n",
            "Epoch  33 Batch  32 / 525  Training Loss  0.006439083721488714\n",
            "Epoch  33 Batch  33 / 525  Training Loss  0.0011032868642359972\n",
            "Epoch  33 Batch  34 / 525  Training Loss  0.010133937001228333\n",
            "Epoch  33 Batch  35 / 525  Training Loss  0.0039504533633589745\n",
            "Epoch  33 Batch  36 / 525  Training Loss  0.005944614764302969\n",
            "Epoch  33 Batch  37 / 525  Training Loss  0.009565440937876701\n",
            "Epoch  33 Batch  38 / 525  Training Loss  0.01348645705729723\n",
            "Epoch  33 Batch  39 / 525  Training Loss  0.0021307722199708223\n",
            "Epoch  33 Batch  40 / 525  Training Loss  0.0031396597623825073\n",
            "Epoch  33 Batch  41 / 525  Training Loss  0.0031019817106425762\n",
            "Epoch  33 Batch  42 / 525  Training Loss  0.013446817174553871\n",
            "Epoch  33 Batch  43 / 525  Training Loss  0.0024674502201378345\n",
            "Epoch  33 Batch  44 / 525  Training Loss  0.0030581026803702116\n",
            "Epoch  33 Batch  45 / 525  Training Loss  0.004079116508364677\n",
            "Epoch  33 Batch  46 / 525  Training Loss  0.005253577139228582\n",
            "Epoch  33 Batch  47 / 525  Training Loss  0.0018532760441303253\n",
            "Epoch  33 Batch  48 / 525  Training Loss  0.00358995096758008\n",
            "Epoch  33 Batch  49 / 525  Training Loss  0.0023598987609148026\n",
            "Epoch  33 Batch  50 / 525  Training Loss  0.009162668138742447\n",
            "Epoch  33 Batch  51 / 525  Training Loss  0.018800582736730576\n",
            "Epoch  33 Batch  52 / 525  Training Loss  0.006617228500545025\n",
            "Epoch  33 Batch  53 / 525  Training Loss  0.016340084373950958\n",
            "Epoch  33 Batch  54 / 525  Training Loss  0.007582931313663721\n",
            "Epoch  33 Batch  55 / 525  Training Loss  0.0014059749664738774\n",
            "Epoch  33 Batch  56 / 525  Training Loss  0.0005900413380004466\n",
            "Epoch  33 Batch  57 / 525  Training Loss  0.004452715627849102\n",
            "Epoch  33 Batch  58 / 525  Training Loss  0.014565947465598583\n",
            "Epoch  33 Batch  59 / 525  Training Loss  0.007931359112262726\n",
            "Epoch  33 Batch  60 / 525  Training Loss  0.016917690634727478\n",
            "Epoch  33 Batch  61 / 525  Training Loss  0.003957211039960384\n",
            "Epoch  33 Batch  62 / 525  Training Loss  0.014387264847755432\n",
            "Epoch  33 Batch  63 / 525  Training Loss  0.001394464517943561\n",
            "Epoch  33 Batch  64 / 525  Training Loss  0.008069994859397411\n",
            "Epoch  33 Batch  65 / 525  Training Loss  0.008003909140825272\n",
            "Epoch  33 Batch  66 / 525  Training Loss  0.0041037858463823795\n",
            "Epoch  33 Batch  67 / 525  Training Loss  0.004189956467598677\n",
            "Epoch  33 Batch  68 / 525  Training Loss  0.00899745337665081\n",
            "Epoch  33 Batch  69 / 525  Training Loss  0.0062965890392661095\n",
            "Epoch  33 Batch  70 / 525  Training Loss  0.0021225062664598227\n",
            "Epoch  33 Batch  71 / 525  Training Loss  0.002051243092864752\n",
            "Epoch  33 Batch  72 / 525  Training Loss  0.0008587446063756943\n",
            "Epoch  33 Batch  73 / 525  Training Loss  0.001255556708201766\n",
            "Epoch  33 Batch  74 / 525  Training Loss  0.001083647832274437\n",
            "Epoch  33 Batch  75 / 525  Training Loss  0.0010210203472524881\n",
            "Epoch  33 Batch  76 / 525  Training Loss  0.0018881747964769602\n",
            "Epoch  33 Batch  77 / 525  Training Loss  0.004702502395957708\n",
            "Epoch  33 Batch  78 / 525  Training Loss  0.001558849005959928\n",
            "Epoch  33 Batch  79 / 525  Training Loss  0.009360821917653084\n",
            "Epoch  33 Batch  80 / 525  Training Loss  0.003857240080833435\n",
            "Epoch  33 Batch  81 / 525  Training Loss  0.004878047853708267\n",
            "Epoch  33 Batch  82 / 525  Training Loss  0.0031829546205699444\n",
            "Epoch  33 Batch  83 / 525  Training Loss  0.00617983378469944\n",
            "Epoch  33 Batch  84 / 525  Training Loss  0.0011208753567188978\n",
            "Epoch  33 Batch  85 / 525  Training Loss  0.0037138243205845356\n",
            "Epoch  33 Batch  86 / 525  Training Loss  0.0005233693518675864\n",
            "Epoch  33 Batch  87 / 525  Training Loss  0.004849670920521021\n",
            "Epoch  33 Batch  88 / 525  Training Loss  0.003141655819490552\n",
            "Epoch  33 Batch  89 / 525  Training Loss  0.005340521689504385\n",
            "Epoch  33 Batch  90 / 525  Training Loss  0.004700430668890476\n",
            "Epoch  33 Batch  91 / 525  Training Loss  0.0048036412335932255\n",
            "Epoch  33 Batch  92 / 525  Training Loss  0.012559713795781136\n",
            "Epoch  33 Batch  93 / 525  Training Loss  0.0022573929745703936\n",
            "Epoch  33 Batch  94 / 525  Training Loss  0.0018882944714277983\n",
            "Epoch  33 Batch  95 / 525  Training Loss  0.004325895570218563\n",
            "Epoch  33 Batch  96 / 525  Training Loss  0.003985006362199783\n",
            "Epoch  33 Batch  97 / 525  Training Loss  0.002115183975547552\n",
            "Epoch  33 Batch  98 / 525  Training Loss  0.0014316401211544871\n",
            "Epoch  33 Batch  99 / 525  Training Loss  0.0008799827774055302\n",
            "Epoch  33 Batch  100 / 525  Training Loss  0.005856641568243504\n",
            "Epoch  33 Batch  101 / 525  Training Loss  0.002559649059548974\n",
            "Epoch  33 Batch  102 / 525  Training Loss  0.0026171039789915085\n",
            "Epoch  33 Batch  103 / 525  Training Loss  0.0043899365700781345\n",
            "Epoch  33 Batch  104 / 525  Training Loss  0.0006973077543079853\n",
            "Epoch  33 Batch  105 / 525  Training Loss  0.00300807342864573\n",
            "Epoch  33 Batch  106 / 525  Training Loss  0.0036535151302814484\n",
            "Epoch  33 Batch  107 / 525  Training Loss  0.0025073830038309097\n",
            "Epoch  33 Batch  108 / 525  Training Loss  0.013529479503631592\n",
            "Epoch  33 Batch  109 / 525  Training Loss  0.0007379862945526838\n",
            "Epoch  33 Batch  110 / 525  Training Loss  0.002789177233353257\n",
            "Epoch  33 Batch  111 / 525  Training Loss  0.012562084011733532\n",
            "Epoch  33 Batch  112 / 525  Training Loss  0.0023169396445155144\n",
            "Epoch  33 Batch  113 / 525  Training Loss  0.003950431011617184\n",
            "Epoch  33 Batch  114 / 525  Training Loss  0.002028451068326831\n",
            "Epoch  33 Batch  115 / 525  Training Loss  0.0010766464984044433\n",
            "Epoch  33 Batch  116 / 525  Training Loss  0.0009813758078962564\n",
            "Epoch  33 Batch  117 / 525  Training Loss  0.013175423257052898\n",
            "Epoch  33 Batch  118 / 525  Training Loss  0.01032246369868517\n",
            "Epoch  33 Batch  119 / 525  Training Loss  0.0023453671019524336\n",
            "Epoch  33 Batch  120 / 525  Training Loss  0.0014442524407058954\n",
            "Epoch  33 Batch  121 / 525  Training Loss  0.003501755418255925\n",
            "Epoch  33 Batch  122 / 525  Training Loss  0.0006255617481656373\n",
            "Epoch  33 Batch  123 / 525  Training Loss  0.001868189312517643\n",
            "Epoch  33 Batch  124 / 525  Training Loss  0.002299082465469837\n",
            "Epoch  33 Batch  125 / 525  Training Loss  0.0008450282039120793\n",
            "Epoch  33 Batch  126 / 525  Training Loss  0.004069113172590733\n",
            "Epoch  33 Batch  127 / 525  Training Loss  0.00033060339046642184\n",
            "Epoch  33 Batch  128 / 525  Training Loss  0.003441192675381899\n",
            "Epoch  33 Batch  129 / 525  Training Loss  0.007805638946592808\n",
            "Epoch  33 Batch  130 / 525  Training Loss  0.0023256600834429264\n",
            "Epoch  33 Batch  131 / 525  Training Loss  0.0010991457384079695\n",
            "Epoch  33 Batch  132 / 525  Training Loss  0.0008952999487519264\n",
            "Epoch  33 Batch  133 / 525  Training Loss  0.003842706326395273\n",
            "Epoch  33 Batch  134 / 525  Training Loss  0.007215484045445919\n",
            "Epoch  33 Batch  135 / 525  Training Loss  0.004059571772813797\n",
            "Epoch  33 Batch  136 / 525  Training Loss  0.0023476530332118273\n",
            "Epoch  33 Batch  137 / 525  Training Loss  0.0003346711746416986\n",
            "Epoch  33 Batch  138 / 525  Training Loss  0.001863595680333674\n",
            "Epoch  33 Batch  139 / 525  Training Loss  0.0015406335005536675\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  33 Batch  140 / 525  Training Loss  0.002007310278713703\n",
            "Epoch  33 Batch  141 / 525  Training Loss  0.0022116019390523434\n",
            "Epoch  33 Batch  142 / 525  Training Loss  0.004947733134031296\n",
            "Epoch  33 Batch  143 / 525  Training Loss  0.003496034536510706\n",
            "Epoch  33 Batch  144 / 525  Training Loss  0.0016740768915042281\n",
            "Epoch  33 Batch  145 / 525  Training Loss  0.0005923192948102951\n",
            "Epoch  33 Batch  146 / 525  Training Loss  0.006757584400475025\n",
            "Epoch  33 Batch  147 / 525  Training Loss  0.004499724600464106\n",
            "Epoch  33 Batch  148 / 525  Training Loss  0.0007915536989457905\n",
            "Epoch  33 Batch  149 / 525  Training Loss  0.0004994955379515886\n",
            "Epoch  33 Batch  150 / 525  Training Loss  0.003362697083503008\n",
            "Epoch  33 Batch  151 / 525  Training Loss  0.00030135782435536385\n",
            "Epoch  33 Batch  152 / 525  Training Loss  0.004041870590299368\n",
            "Epoch  33 Batch  153 / 525  Training Loss  0.00817692931741476\n",
            "Epoch  33 Batch  154 / 525  Training Loss  0.0014218699652701616\n",
            "Epoch  33 Batch  155 / 525  Training Loss  0.0004567690775729716\n",
            "Epoch  33 Batch  156 / 525  Training Loss  0.009704306721687317\n",
            "Epoch  33 Batch  157 / 525  Training Loss  0.005321580450981855\n",
            "Epoch  33 Batch  158 / 525  Training Loss  0.001415220438502729\n",
            "Epoch  33 Batch  159 / 525  Training Loss  0.0040124948136508465\n",
            "Epoch  33 Batch  160 / 525  Training Loss  0.004527929238975048\n",
            "Epoch  33 Batch  161 / 525  Training Loss  0.004994703456759453\n",
            "Epoch  33 Batch  162 / 525  Training Loss  0.01284070499241352\n",
            "Epoch  33 Batch  163 / 525  Training Loss  0.0011996817775070667\n",
            "Epoch  33 Batch  164 / 525  Training Loss  0.004736042115837336\n",
            "Epoch  33 Batch  165 / 525  Training Loss  0.00404129084199667\n",
            "Epoch  33 Batch  166 / 525  Training Loss  0.0006348652532324195\n",
            "Epoch  33 Batch  167 / 525  Training Loss  0.006426720879971981\n",
            "Epoch  33 Batch  168 / 525  Training Loss  0.0028510023839771748\n",
            "Epoch  33 Batch  169 / 525  Training Loss  0.009170163422822952\n",
            "Epoch  33 Batch  170 / 525  Training Loss  0.005394565407186747\n",
            "Epoch  33 Batch  171 / 525  Training Loss  0.0015886879991739988\n",
            "Epoch  33 Batch  172 / 525  Training Loss  0.002834040205925703\n",
            "Epoch  33 Batch  173 / 525  Training Loss  0.002453770022839308\n",
            "Epoch  33 Batch  174 / 525  Training Loss  0.013534474186599255\n",
            "Epoch  33 Batch  175 / 525  Training Loss  0.004500550217926502\n",
            "Epoch  33 Batch  176 / 525  Training Loss  0.0011827361304312944\n",
            "Epoch  33 Batch  177 / 525  Training Loss  0.00541298184543848\n",
            "Epoch  33 Batch  178 / 525  Training Loss  0.0035344441421329975\n",
            "Epoch  33 Batch  179 / 525  Training Loss  0.001974367303773761\n",
            "Epoch  33 Batch  180 / 525  Training Loss  0.002079090801998973\n",
            "Epoch  33 Batch  181 / 525  Training Loss  0.02318616583943367\n",
            "Epoch  33 Batch  182 / 525  Training Loss  0.0019443898927420378\n",
            "Epoch  33 Batch  183 / 525  Training Loss  0.0070867715403437614\n",
            "Epoch  33 Batch  184 / 525  Training Loss  0.00214725942350924\n",
            "Epoch  33 Batch  185 / 525  Training Loss  0.0017352153081446886\n",
            "Epoch  33 Batch  186 / 525  Training Loss  0.0011100901756435633\n",
            "Epoch  33 Batch  187 / 525  Training Loss  0.005675979889929295\n",
            "Epoch  33 Batch  188 / 525  Training Loss  0.0007554072071798146\n",
            "Epoch  33 Batch  189 / 525  Training Loss  0.0011300180340185761\n",
            "Epoch  33 Batch  190 / 525  Training Loss  0.0032103187404572964\n",
            "Epoch  33 Batch  191 / 525  Training Loss  0.001657586544752121\n",
            "Epoch  33 Batch  192 / 525  Training Loss  0.0008607182535342872\n",
            "Epoch  33 Batch  193 / 525  Training Loss  0.001493864576332271\n",
            "Epoch  33 Batch  194 / 525  Training Loss  0.002084363019093871\n",
            "Epoch  33 Batch  195 / 525  Training Loss  0.0011720595648512244\n",
            "Epoch  33 Batch  196 / 525  Training Loss  0.0020496102515608072\n",
            "Epoch  33 Batch  197 / 525  Training Loss  0.00314195710234344\n",
            "Epoch  33 Batch  198 / 525  Training Loss  0.002499229507520795\n",
            "Epoch  33 Batch  199 / 525  Training Loss  0.0014695286517962813\n",
            "Epoch  33 Batch  200 / 525  Training Loss  0.0032705902121961117\n",
            "Epoch  33 Batch  201 / 525  Training Loss  0.0017547293100506067\n",
            "Epoch  33 Batch  202 / 525  Training Loss  0.0021044081076979637\n",
            "Epoch  33 Batch  203 / 525  Training Loss  0.00654240045696497\n",
            "Epoch  33 Batch  204 / 525  Training Loss  0.0015946633648127317\n",
            "Epoch  33 Batch  205 / 525  Training Loss  0.005193118005990982\n",
            "Epoch  33 Batch  206 / 525  Training Loss  0.0007970480364747345\n",
            "Epoch  33 Batch  207 / 525  Training Loss  0.0063125258311629295\n",
            "Epoch  33 Batch  208 / 525  Training Loss  0.0003120101464446634\n",
            "Epoch  33 Batch  209 / 525  Training Loss  0.0018944637849926949\n",
            "Epoch  33 Batch  210 / 525  Training Loss  0.0022350132931023836\n",
            "Epoch  33 Batch  211 / 525  Training Loss  0.0032603994477540255\n",
            "Epoch  33 Batch  212 / 525  Training Loss  0.001580329379066825\n",
            "Epoch  33 Batch  213 / 525  Training Loss  0.003811051370576024\n",
            "Epoch  33 Batch  214 / 525  Training Loss  0.010285143740475178\n",
            "Epoch  33 Batch  215 / 525  Training Loss  0.0022167519200593233\n",
            "Epoch  33 Batch  216 / 525  Training Loss  0.002664647065103054\n",
            "Epoch  33 Batch  217 / 525  Training Loss  0.0024701254442334175\n",
            "Epoch  33 Batch  218 / 525  Training Loss  0.0020459797233343124\n",
            "Epoch  33 Batch  219 / 525  Training Loss  0.0025359729770570993\n",
            "Epoch  33 Batch  220 / 525  Training Loss  0.003841811791062355\n",
            "Epoch  33 Batch  221 / 525  Training Loss  0.00801088847219944\n",
            "Epoch  33 Batch  222 / 525  Training Loss  0.00594351114705205\n",
            "Epoch  33 Batch  223 / 525  Training Loss  0.0010779289295896888\n",
            "Epoch  33 Batch  224 / 525  Training Loss  0.003117551328614354\n",
            "Epoch  33 Batch  225 / 525  Training Loss  0.0029064747504889965\n",
            "Epoch  33 Batch  226 / 525  Training Loss  0.006402873434126377\n",
            "Epoch  33 Batch  227 / 525  Training Loss  0.013697465881705284\n",
            "Epoch  33 Batch  228 / 525  Training Loss  0.008080457337200642\n",
            "Epoch  33 Batch  229 / 525  Training Loss  0.005097587127238512\n",
            "Epoch  33 Batch  230 / 525  Training Loss  0.009204845875501633\n",
            "Epoch  33 Batch  231 / 525  Training Loss  0.0038228787016123533\n",
            "Epoch  33 Batch  232 / 525  Training Loss  0.007600085344165564\n",
            "Epoch  33 Batch  233 / 525  Training Loss  0.00940027181059122\n",
            "Epoch  33 Batch  234 / 525  Training Loss  0.003133585676550865\n",
            "Epoch  33 Batch  235 / 525  Training Loss  0.003483142703771591\n",
            "Epoch  33 Batch  236 / 525  Training Loss  0.002811301499605179\n",
            "Epoch  33 Batch  237 / 525  Training Loss  0.00041701231384649873\n",
            "Epoch  33 Batch  238 / 525  Training Loss  0.0025086526293307543\n",
            "Epoch  33 Batch  239 / 525  Training Loss  0.006388106849044561\n",
            "Epoch  33 Batch  240 / 525  Training Loss  0.00086630490841344\n",
            "Epoch  33 Batch  241 / 525  Training Loss  0.001049196464009583\n",
            "Epoch  33 Batch  242 / 525  Training Loss  0.009881462901830673\n",
            "Epoch  33 Batch  243 / 525  Training Loss  0.0023100553080439568\n",
            "Epoch  33 Batch  244 / 525  Training Loss  0.002059324411675334\n",
            "Epoch  33 Batch  245 / 525  Training Loss  0.0039466009475290775\n",
            "Epoch  33 Batch  246 / 525  Training Loss  0.012149861082434654\n",
            "Epoch  33 Batch  247 / 525  Training Loss  0.011638427153229713\n",
            "Epoch  33 Batch  248 / 525  Training Loss  0.0029803216457366943\n",
            "Epoch  33 Batch  249 / 525  Training Loss  0.001994770485907793\n",
            "Epoch  33 Batch  250 / 525  Training Loss  0.001808226341381669\n",
            "Epoch  33 Batch  251 / 525  Training Loss  0.003481165738776326\n",
            "Epoch  33 Batch  252 / 525  Training Loss  0.0017679702723398805\n",
            "Epoch  33 Batch  253 / 525  Training Loss  0.009361529722809792\n",
            "Epoch  33 Batch  254 / 525  Training Loss  0.006774939596652985\n",
            "Epoch  33 Batch  255 / 525  Training Loss  0.0016110262367874384\n",
            "Epoch  33 Batch  256 / 525  Training Loss  0.009511309675872326\n",
            "Epoch  33 Batch  257 / 525  Training Loss  0.0016191008035093546\n",
            "Epoch  33 Batch  258 / 525  Training Loss  0.004081401042640209\n",
            "Epoch  33 Batch  259 / 525  Training Loss  0.0015189375262707472\n",
            "Epoch  33 Batch  260 / 525  Training Loss  0.004382915794849396\n",
            "Epoch  33 Batch  261 / 525  Training Loss  0.00447959965094924\n",
            "Epoch  33 Batch  262 / 525  Training Loss  0.0009149712277576327\n",
            "Epoch  33 Batch  263 / 525  Training Loss  0.003732499433681369\n",
            "Epoch  33 Batch  264 / 525  Training Loss  0.002791165839880705\n",
            "Epoch  33 Batch  265 / 525  Training Loss  0.007909298874437809\n",
            "Epoch  33 Batch  266 / 525  Training Loss  0.0025225565768778324\n",
            "Epoch  33 Batch  267 / 525  Training Loss  0.0031267311424016953\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  33 Batch  268 / 525  Training Loss  0.0033821507822722197\n",
            "Epoch  33 Batch  269 / 525  Training Loss  0.001910002320073545\n",
            "Epoch  33 Batch  270 / 525  Training Loss  0.0018069753423333168\n",
            "Epoch  33 Batch  271 / 525  Training Loss  0.010147593915462494\n",
            "Epoch  33 Batch  272 / 525  Training Loss  0.0016363827744498849\n",
            "Epoch  33 Batch  273 / 525  Training Loss  0.004473273642361164\n",
            "Epoch  33 Batch  274 / 525  Training Loss  0.002843534341081977\n",
            "Epoch  33 Batch  275 / 525  Training Loss  0.005854986608028412\n",
            "Epoch  33 Batch  276 / 525  Training Loss  0.0027984105981886387\n",
            "Epoch  33 Batch  277 / 525  Training Loss  0.0040080915205180645\n",
            "Epoch  33 Batch  278 / 525  Training Loss  0.0038423941005021334\n",
            "Epoch  33 Batch  279 / 525  Training Loss  0.0063126832246780396\n",
            "Epoch  33 Batch  280 / 525  Training Loss  0.010260333307087421\n",
            "Epoch  33 Batch  281 / 525  Training Loss  0.004769396036863327\n",
            "Epoch  33 Batch  282 / 525  Training Loss  0.002403340535238385\n",
            "Epoch  33 Batch  283 / 525  Training Loss  0.0025927890092134476\n",
            "Epoch  33 Batch  284 / 525  Training Loss  0.0015382521087303758\n",
            "Epoch  33 Batch  285 / 525  Training Loss  0.001294124173000455\n",
            "Epoch  33 Batch  286 / 525  Training Loss  0.0013986750273033977\n",
            "Epoch  33 Batch  287 / 525  Training Loss  0.0019776499830186367\n",
            "Epoch  33 Batch  288 / 525  Training Loss  0.0013341873418539762\n",
            "Epoch  33 Batch  289 / 525  Training Loss  0.004351283423602581\n",
            "Epoch  33 Batch  290 / 525  Training Loss  0.003081938251852989\n",
            "Epoch  33 Batch  291 / 525  Training Loss  0.0023905278649181128\n",
            "Epoch  33 Batch  292 / 525  Training Loss  0.0015074419789016247\n",
            "Epoch  33 Batch  293 / 525  Training Loss  0.004777354653924704\n",
            "Epoch  33 Batch  294 / 525  Training Loss  0.009871614165604115\n",
            "Epoch  33 Batch  295 / 525  Training Loss  0.0007568785222247243\n",
            "Epoch  33 Batch  296 / 525  Training Loss  0.0019716594833880663\n",
            "Epoch  33 Batch  297 / 525  Training Loss  0.00379803660325706\n",
            "Epoch  33 Batch  298 / 525  Training Loss  0.0014095037477090955\n",
            "Epoch  33 Batch  299 / 525  Training Loss  0.003569437889382243\n",
            "Epoch  33 Batch  300 / 525  Training Loss  0.0028157790657132864\n",
            "Epoch  33 Batch  301 / 525  Training Loss  0.000638628494925797\n",
            "Epoch  33 Batch  302 / 525  Training Loss  0.001993810757994652\n",
            "Epoch  33 Batch  303 / 525  Training Loss  0.00040726721636019647\n",
            "Epoch  33 Batch  304 / 525  Training Loss  0.00711596617475152\n",
            "Epoch  33 Batch  305 / 525  Training Loss  0.003740305081009865\n",
            "Epoch  33 Batch  306 / 525  Training Loss  0.0035328760277479887\n",
            "Epoch  33 Batch  307 / 525  Training Loss  0.005554940551519394\n",
            "Epoch  33 Batch  308 / 525  Training Loss  0.003826940432190895\n",
            "Epoch  33 Batch  309 / 525  Training Loss  0.006268116179853678\n",
            "Epoch  33 Batch  310 / 525  Training Loss  0.004769313149154186\n",
            "Epoch  33 Batch  311 / 525  Training Loss  0.005916760303080082\n",
            "Epoch  33 Batch  312 / 525  Training Loss  0.009342655539512634\n",
            "Epoch  33 Batch  313 / 525  Training Loss  0.0032515767961740494\n",
            "Epoch  33 Batch  314 / 525  Training Loss  0.006592039950191975\n",
            "Epoch  33 Batch  315 / 525  Training Loss  0.0012894317042082548\n",
            "Epoch  33 Batch  316 / 525  Training Loss  0.0023292831610888243\n",
            "Epoch  33 Batch  317 / 525  Training Loss  0.0016180509701371193\n",
            "Epoch  33 Batch  318 / 525  Training Loss  0.002562892157584429\n",
            "Epoch  33 Batch  319 / 525  Training Loss  0.0019135477486997843\n",
            "Epoch  33 Batch  320 / 525  Training Loss  0.002998531563207507\n",
            "Epoch  33 Batch  321 / 525  Training Loss  0.005697581451386213\n",
            "Epoch  33 Batch  322 / 525  Training Loss  0.004598272033035755\n",
            "Epoch  33 Batch  323 / 525  Training Loss  0.002417489420622587\n",
            "Epoch  33 Batch  324 / 525  Training Loss  0.007040383759886026\n",
            "Epoch  33 Batch  325 / 525  Training Loss  0.0015866417670622468\n",
            "Epoch  33 Batch  326 / 525  Training Loss  0.0015471592778339982\n",
            "Epoch  33 Batch  327 / 525  Training Loss  0.0017167404294013977\n",
            "Epoch  33 Batch  328 / 525  Training Loss  0.007188624236732721\n",
            "Epoch  33 Batch  329 / 525  Training Loss  0.0007441441412083805\n",
            "Epoch  33 Batch  330 / 525  Training Loss  0.004777485970407724\n",
            "Epoch  33 Batch  331 / 525  Training Loss  0.003039267612621188\n",
            "Epoch  33 Batch  332 / 525  Training Loss  0.0027319504879415035\n",
            "Epoch  33 Batch  333 / 525  Training Loss  0.002971018198877573\n",
            "Epoch  33 Batch  334 / 525  Training Loss  0.0008124042069539428\n",
            "Epoch  33 Batch  335 / 525  Training Loss  0.003852288704365492\n",
            "Epoch  33 Batch  336 / 525  Training Loss  0.005382333416491747\n",
            "Epoch  33 Batch  337 / 525  Training Loss  0.0034585059620440006\n",
            "Epoch  33 Batch  338 / 525  Training Loss  0.0023853082675486803\n",
            "Epoch  33 Batch  339 / 525  Training Loss  0.004680813290178776\n",
            "Epoch  33 Batch  340 / 525  Training Loss  0.004174164030700922\n",
            "Epoch  33 Batch  341 / 525  Training Loss  0.006362687796354294\n",
            "Epoch  33 Batch  342 / 525  Training Loss  0.001430798671208322\n",
            "Epoch  33 Batch  343 / 525  Training Loss  0.0012293492909520864\n",
            "Epoch  33 Batch  344 / 525  Training Loss  0.0005558579578064382\n",
            "Epoch  33 Batch  345 / 525  Training Loss  0.0006782283307984471\n",
            "Epoch  33 Batch  346 / 525  Training Loss  0.0005467738956212997\n",
            "Epoch  33 Batch  347 / 525  Training Loss  0.0006667572888545692\n",
            "Epoch  33 Batch  348 / 525  Training Loss  0.005258833058178425\n",
            "Epoch  33 Batch  349 / 525  Training Loss  0.0024524794425815344\n",
            "Epoch  33 Batch  350 / 525  Training Loss  0.009791308082640171\n",
            "Epoch  33 Batch  351 / 525  Training Loss  0.0044923992827534676\n",
            "Epoch  33 Batch  352 / 525  Training Loss  0.0017096095252782106\n",
            "Epoch  33 Batch  353 / 525  Training Loss  0.0007938087801449001\n",
            "Epoch  33 Batch  354 / 525  Training Loss  0.005147294607013464\n",
            "Epoch  33 Batch  355 / 525  Training Loss  0.005181735381484032\n",
            "Epoch  33 Batch  356 / 525  Training Loss  0.002493404783308506\n",
            "Epoch  33 Batch  357 / 525  Training Loss  0.002741115866228938\n",
            "Epoch  33 Batch  358 / 525  Training Loss  0.0012127377558499575\n",
            "Epoch  33 Batch  359 / 525  Training Loss  0.006891666911542416\n",
            "Epoch  33 Batch  360 / 525  Training Loss  0.003740557935088873\n",
            "Epoch  33 Batch  361 / 525  Training Loss  0.0009923703037202358\n",
            "Epoch  33 Batch  362 / 525  Training Loss  0.001239819684997201\n",
            "Epoch  33 Batch  363 / 525  Training Loss  0.00609330739825964\n",
            "Epoch  33 Batch  364 / 525  Training Loss  0.01252853311598301\n",
            "Epoch  33 Batch  365 / 525  Training Loss  0.004719599150121212\n",
            "Epoch  33 Batch  366 / 525  Training Loss  0.004850042052567005\n",
            "Epoch  33 Batch  367 / 525  Training Loss  0.0028350597713142633\n",
            "Epoch  33 Batch  368 / 525  Training Loss  0.0014443619875237346\n",
            "Epoch  33 Batch  369 / 525  Training Loss  0.016307491809129715\n",
            "Epoch  33 Batch  370 / 525  Training Loss  0.003922606818377972\n",
            "Epoch  33 Batch  371 / 525  Training Loss  0.0023639313876628876\n",
            "Epoch  33 Batch  372 / 525  Training Loss  0.004958554171025753\n",
            "Epoch  33 Batch  373 / 525  Training Loss  0.017396021634340286\n",
            "Epoch  33 Batch  374 / 525  Training Loss  0.007337523158639669\n",
            "Epoch  33 Batch  375 / 525  Training Loss  0.0006145758088678122\n",
            "Epoch  33 Batch  376 / 525  Training Loss  0.0016903672367334366\n",
            "Epoch  33 Batch  377 / 525  Training Loss  0.0009319002856500447\n",
            "Epoch  33 Batch  378 / 525  Training Loss  0.005612638778984547\n",
            "Epoch  33 Batch  379 / 525  Training Loss  0.007311451248824596\n",
            "Epoch  33 Batch  380 / 525  Training Loss  0.0010780039010569453\n",
            "Epoch  33 Batch  381 / 525  Training Loss  0.007701175753027201\n",
            "Epoch  33 Batch  382 / 525  Training Loss  0.002291168551892042\n",
            "Epoch  33 Batch  383 / 525  Training Loss  0.010817011818289757\n",
            "Epoch  33 Batch  384 / 525  Training Loss  0.009086837992072105\n",
            "Epoch  33 Batch  385 / 525  Training Loss  0.005945171695202589\n",
            "Epoch  33 Batch  386 / 525  Training Loss  0.0013372267130762339\n",
            "Epoch  33 Batch  387 / 525  Training Loss  0.003982690162956715\n",
            "Epoch  33 Batch  388 / 525  Training Loss  0.0032229837961494923\n",
            "Epoch  33 Batch  389 / 525  Training Loss  0.000835852581076324\n",
            "Epoch  33 Batch  390 / 525  Training Loss  0.0018287382554262877\n",
            "Epoch  33 Batch  391 / 525  Training Loss  0.002000811044126749\n",
            "Epoch  33 Batch  392 / 525  Training Loss  0.0005863654660061002\n",
            "Epoch  33 Batch  393 / 525  Training Loss  0.0007902812212705612\n",
            "Epoch  33 Batch  394 / 525  Training Loss  0.006812372710555792\n",
            "Epoch  33 Batch  395 / 525  Training Loss  0.010304833762347698\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  33 Batch  396 / 525  Training Loss  0.0033765758853405714\n",
            "Epoch  33 Batch  397 / 525  Training Loss  0.0025183747056871653\n",
            "Epoch  33 Batch  398 / 525  Training Loss  0.008332076482474804\n",
            "Epoch  33 Batch  399 / 525  Training Loss  0.005962491501122713\n",
            "Epoch  33 Batch  400 / 525  Training Loss  0.0011888903100043535\n",
            "Epoch  33 Batch  401 / 525  Training Loss  0.001203406834974885\n",
            "Epoch  33 Batch  402 / 525  Training Loss  0.006222518160939217\n",
            "Epoch  33 Batch  403 / 525  Training Loss  0.007829500362277031\n",
            "Epoch  33 Batch  404 / 525  Training Loss  0.002857849933207035\n",
            "Epoch  33 Batch  405 / 525  Training Loss  0.00269204075448215\n",
            "Epoch  33 Batch  406 / 525  Training Loss  0.0026270058006048203\n",
            "Epoch  33 Batch  407 / 525  Training Loss  0.0012492270907387137\n",
            "Epoch  33 Batch  408 / 525  Training Loss  0.004141629673540592\n",
            "Epoch  33 Batch  409 / 525  Training Loss  0.0011291041737422347\n",
            "Epoch  33 Batch  410 / 525  Training Loss  0.007636168040335178\n",
            "Epoch  33 Batch  411 / 525  Training Loss  0.008715955540537834\n",
            "Epoch  33 Batch  412 / 525  Training Loss  0.0024436158128082752\n",
            "Epoch  33 Batch  413 / 525  Training Loss  0.0018237910699099302\n",
            "Epoch  33 Batch  414 / 525  Training Loss  0.0034991088323295116\n",
            "Epoch  33 Batch  415 / 525  Training Loss  0.014142709784209728\n",
            "Epoch  33 Batch  416 / 525  Training Loss  0.0004950898000970483\n",
            "Epoch  33 Batch  417 / 525  Training Loss  0.001181535073556006\n",
            "Epoch  33 Batch  418 / 525  Training Loss  0.002645390573889017\n",
            "Epoch  33 Batch  419 / 525  Training Loss  0.0018520141020417213\n",
            "Epoch  33 Batch  420 / 525  Training Loss  0.002292089629918337\n",
            "Epoch  33 Batch  421 / 525  Training Loss  0.004586636088788509\n",
            "Epoch  33 Batch  422 / 525  Training Loss  0.0014226976782083511\n",
            "Epoch  33 Batch  423 / 525  Training Loss  0.0004888174589723349\n",
            "Epoch  33 Batch  424 / 525  Training Loss  0.008893029764294624\n",
            "Epoch  33 Batch  425 / 525  Training Loss  0.0012688260758295655\n",
            "Epoch  33 Batch  426 / 525  Training Loss  0.0023282640613615513\n",
            "Epoch  33 Batch  427 / 525  Training Loss  0.003285670652985573\n",
            "Epoch  33 Batch  428 / 525  Training Loss  0.005596349481493235\n",
            "Epoch  33 Batch  429 / 525  Training Loss  0.011281125247478485\n",
            "Epoch  33 Batch  430 / 525  Training Loss  0.002362234052270651\n",
            "Epoch  33 Batch  431 / 525  Training Loss  0.0034069507382810116\n",
            "Epoch  33 Batch  432 / 525  Training Loss  0.006772727705538273\n",
            "Epoch  33 Batch  433 / 525  Training Loss  0.0022168559953570366\n",
            "Epoch  33 Batch  434 / 525  Training Loss  0.008038456551730633\n",
            "Epoch  33 Batch  435 / 525  Training Loss  0.0011398247443139553\n",
            "Epoch  33 Batch  436 / 525  Training Loss  0.0021514163818210363\n",
            "Epoch  33 Batch  437 / 525  Training Loss  0.002077557612210512\n",
            "Epoch  33 Batch  438 / 525  Training Loss  0.0012134492862969637\n",
            "Epoch  33 Batch  439 / 525  Training Loss  0.001508977496996522\n",
            "Epoch  33 Batch  440 / 525  Training Loss  0.0035094295162707567\n",
            "Epoch  33 Batch  441 / 525  Training Loss  0.0009876296389847994\n",
            "Epoch  33 Batch  442 / 525  Training Loss  0.0017557491082698107\n",
            "Epoch  33 Batch  443 / 525  Training Loss  0.0027679402846843004\n",
            "Epoch  33 Batch  444 / 525  Training Loss  0.0018900120630860329\n",
            "Epoch  33 Batch  445 / 525  Training Loss  0.0022687860764563084\n",
            "Epoch  33 Batch  446 / 525  Training Loss  0.0007436475716531277\n",
            "Epoch  33 Batch  447 / 525  Training Loss  0.0034598030615597963\n",
            "Epoch  33 Batch  448 / 525  Training Loss  0.0013890834525227547\n",
            "Epoch  33 Batch  449 / 525  Training Loss  0.010732959024608135\n",
            "Epoch  33 Batch  450 / 525  Training Loss  0.008361286483705044\n",
            "Epoch  33 Batch  451 / 525  Training Loss  0.006933902855962515\n",
            "Epoch  33 Batch  452 / 525  Training Loss  0.0012839753180742264\n",
            "Epoch  33 Batch  453 / 525  Training Loss  0.004132239148020744\n",
            "Epoch  33 Batch  454 / 525  Training Loss  0.023941010236740112\n",
            "Epoch  33 Batch  455 / 525  Training Loss  0.006370211951434612\n",
            "Epoch  33 Batch  456 / 525  Training Loss  0.006287419702857733\n",
            "Epoch  33 Batch  457 / 525  Training Loss  0.016801241785287857\n",
            "Epoch  33 Batch  458 / 525  Training Loss  0.0030245333909988403\n",
            "Epoch  33 Batch  459 / 525  Training Loss  0.005180385895073414\n",
            "Epoch  33 Batch  460 / 525  Training Loss  0.00474956352263689\n",
            "Epoch  33 Batch  461 / 525  Training Loss  0.0016239037504419684\n",
            "Epoch  33 Batch  462 / 525  Training Loss  0.0021515539847314358\n",
            "Epoch  33 Batch  463 / 525  Training Loss  0.007013289723545313\n",
            "Epoch  33 Batch  464 / 525  Training Loss  0.0010208376916125417\n",
            "Epoch  33 Batch  465 / 525  Training Loss  0.005359105300158262\n",
            "Epoch  33 Batch  466 / 525  Training Loss  0.0011055024806410074\n",
            "Epoch  33 Batch  467 / 525  Training Loss  0.0034060690086334944\n",
            "Epoch  33 Batch  468 / 525  Training Loss  0.012820633128285408\n",
            "Epoch  33 Batch  469 / 525  Training Loss  0.007052108645439148\n",
            "Epoch  33 Batch  470 / 525  Training Loss  0.011907096020877361\n",
            "Epoch  33 Batch  471 / 525  Training Loss  0.0015750329475849867\n",
            "Epoch  33 Batch  472 / 525  Training Loss  0.0011193366954103112\n",
            "Epoch  33 Batch  473 / 525  Training Loss  0.002598168794065714\n",
            "Epoch  33 Batch  474 / 525  Training Loss  0.001098759239539504\n",
            "Epoch  33 Batch  475 / 525  Training Loss  0.0022792017553001642\n",
            "Epoch  33 Batch  476 / 525  Training Loss  0.0021737483330070972\n",
            "Epoch  33 Batch  477 / 525  Training Loss  0.005601500626653433\n",
            "Epoch  33 Batch  478 / 525  Training Loss  0.004668714012950659\n",
            "Epoch  33 Batch  479 / 525  Training Loss  0.0033760815858840942\n",
            "Epoch  33 Batch  480 / 525  Training Loss  0.004342349711805582\n",
            "Epoch  33 Batch  481 / 525  Training Loss  0.003796397941187024\n",
            "Epoch  33 Batch  482 / 525  Training Loss  0.0009808469330891967\n",
            "Epoch  33 Batch  483 / 525  Training Loss  0.010646602138876915\n",
            "Epoch  33 Batch  484 / 525  Training Loss  0.01092772837728262\n",
            "Epoch  33 Batch  485 / 525  Training Loss  0.0034957125317305326\n",
            "Epoch  33 Batch  486 / 525  Training Loss  0.007128714583814144\n",
            "Epoch  33 Batch  487 / 525  Training Loss  0.006722102407366037\n",
            "Epoch  33 Batch  488 / 525  Training Loss  0.003935935441404581\n",
            "Epoch  33 Batch  489 / 525  Training Loss  0.0038995903450995684\n",
            "Epoch  33 Batch  490 / 525  Training Loss  0.008024509996175766\n",
            "Epoch  33 Batch  491 / 525  Training Loss  0.005827759392559528\n",
            "Epoch  33 Batch  492 / 525  Training Loss  0.002638900652527809\n",
            "Epoch  33 Batch  493 / 525  Training Loss  0.002249529119580984\n",
            "Epoch  33 Batch  494 / 525  Training Loss  0.0041201068088412285\n",
            "Epoch  33 Batch  495 / 525  Training Loss  0.004543763119727373\n",
            "Epoch  33 Batch  496 / 525  Training Loss  0.0017275052377954125\n",
            "Epoch  33 Batch  497 / 525  Training Loss  0.0010546190897002816\n",
            "Epoch  33 Batch  498 / 525  Training Loss  0.0030350361485034227\n",
            "Epoch  33 Batch  499 / 525  Training Loss  0.01607930287718773\n",
            "Epoch  33 Batch  500 / 525  Training Loss  0.002869094256311655\n",
            "Epoch  33 Batch  501 / 525  Training Loss  0.0011798792984336615\n",
            "Epoch  33 Batch  502 / 525  Training Loss  0.0019849317613989115\n",
            "Epoch  33 Batch  503 / 525  Training Loss  0.003051363630220294\n",
            "Epoch  33 Batch  504 / 525  Training Loss  0.009151999838650227\n",
            "Epoch  33 Batch  505 / 525  Training Loss  0.0031272023916244507\n",
            "Epoch  33 Batch  506 / 525  Training Loss  0.008735266514122486\n",
            "Epoch  33 Batch  507 / 525  Training Loss  0.0018113130936399102\n",
            "Epoch  33 Batch  508 / 525  Training Loss  0.005014162976294756\n",
            "Epoch  33 Batch  509 / 525  Training Loss  0.008685416541993618\n",
            "Epoch  33 Batch  510 / 525  Training Loss  0.0028044888749718666\n",
            "Epoch  33 Batch  511 / 525  Training Loss  0.009051303379237652\n",
            "Epoch  33 Batch  512 / 525  Training Loss  0.006972864270210266\n",
            "Epoch  33 Batch  513 / 525  Training Loss  0.0022762690205127\n",
            "Epoch  33 Batch  514 / 525  Training Loss  0.006514924578368664\n",
            "Epoch  33 Batch  515 / 525  Training Loss  0.005344292148947716\n",
            "Epoch  33 Batch  516 / 525  Training Loss  0.0028964965604245663\n",
            "Epoch  33 Batch  517 / 525  Training Loss  0.007151213940232992\n",
            "Epoch  33 Batch  518 / 525  Training Loss  0.004891847260296345\n",
            "Epoch  33 Batch  519 / 525  Training Loss  0.007736422121524811\n",
            "Epoch  33 Batch  520 / 525  Training Loss  0.004996961914002895\n",
            "Epoch  33 Batch  521 / 525  Training Loss  0.004713766276836395\n",
            "Epoch  33 Batch  522 / 525  Training Loss  0.004488452337682247\n",
            "Epoch  33 Batch  523 / 525  Training Loss  0.0018580702599138021\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  33 Batch  524 / 525  Training Loss  0.001880811178125441\n",
            "  34    |    -    |   0.004241   | 60.700000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 34\n",
            "Epoch  34 Batch  0 / 525  Training Loss  0.001296250382438302\n",
            "Epoch  34 Batch  1 / 525  Training Loss  0.0021254958119243383\n",
            "Epoch  34 Batch  2 / 525  Training Loss  0.00029947032453492284\n",
            "Epoch  34 Batch  3 / 525  Training Loss  0.001150009804405272\n",
            "Epoch  34 Batch  4 / 525  Training Loss  0.003996212501078844\n",
            "Epoch  34 Batch  5 / 525  Training Loss  0.0013322781305760145\n",
            "Epoch  34 Batch  6 / 525  Training Loss  0.001220343285240233\n",
            "Epoch  34 Batch  7 / 525  Training Loss  0.0007702976581640542\n",
            "Epoch  34 Batch  8 / 525  Training Loss  0.0041441828943789005\n",
            "Epoch  34 Batch  9 / 525  Training Loss  0.0005481283878907561\n",
            "Epoch  34 Batch  10 / 525  Training Loss  0.0012271911837160587\n",
            "Epoch  34 Batch  11 / 525  Training Loss  0.00144228246062994\n",
            "Epoch  34 Batch  12 / 525  Training Loss  0.0011137811234220862\n",
            "Epoch  34 Batch  13 / 525  Training Loss  0.0012337823864072561\n",
            "Epoch  34 Batch  14 / 525  Training Loss  0.00460758525878191\n",
            "Epoch  34 Batch  15 / 525  Training Loss  0.0018720673397183418\n",
            "Epoch  34 Batch  16 / 525  Training Loss  0.0005549090565182269\n",
            "Epoch  34 Batch  17 / 525  Training Loss  0.0003666053235065192\n",
            "Epoch  34 Batch  18 / 525  Training Loss  0.0004636673256754875\n",
            "Epoch  34 Batch  19 / 525  Training Loss  0.005788459442555904\n",
            "Epoch  34 Batch  20 / 525  Training Loss  0.003192255739122629\n",
            "Epoch  34 Batch  21 / 525  Training Loss  0.01896558329463005\n",
            "Epoch  34 Batch  22 / 525  Training Loss  0.0003510849492158741\n",
            "Epoch  34 Batch  23 / 525  Training Loss  0.0014658685540780425\n",
            "Epoch  34 Batch  24 / 525  Training Loss  0.0015733835753053427\n",
            "Epoch  34 Batch  25 / 525  Training Loss  0.0012731894385069609\n",
            "Epoch  34 Batch  26 / 525  Training Loss  0.003190418239682913\n",
            "Epoch  34 Batch  27 / 525  Training Loss  0.0035767920780926943\n",
            "Epoch  34 Batch  28 / 525  Training Loss  0.0017710344400256872\n",
            "Epoch  34 Batch  29 / 525  Training Loss  0.0005475825746543705\n",
            "Epoch  34 Batch  30 / 525  Training Loss  0.0009404387092217803\n",
            "Epoch  34 Batch  31 / 525  Training Loss  0.0027918179985135794\n",
            "Epoch  34 Batch  32 / 525  Training Loss  0.0015700956573709846\n",
            "Epoch  34 Batch  33 / 525  Training Loss  0.0009325697319582105\n",
            "Epoch  34 Batch  34 / 525  Training Loss  0.0016352769453078508\n",
            "Epoch  34 Batch  35 / 525  Training Loss  0.0008025599527172744\n",
            "Epoch  34 Batch  36 / 525  Training Loss  0.0012816195376217365\n",
            "Epoch  34 Batch  37 / 525  Training Loss  0.001766514265909791\n",
            "Epoch  34 Batch  38 / 525  Training Loss  0.0068707214668393135\n",
            "Epoch  34 Batch  39 / 525  Training Loss  0.004708948545157909\n",
            "Epoch  34 Batch  40 / 525  Training Loss  0.0006657508783973753\n",
            "Epoch  34 Batch  41 / 525  Training Loss  0.003093373030424118\n",
            "Epoch  34 Batch  42 / 525  Training Loss  0.00100583431776613\n",
            "Epoch  34 Batch  43 / 525  Training Loss  0.0036899775732308626\n",
            "Epoch  34 Batch  44 / 525  Training Loss  0.0012429860653355718\n",
            "Epoch  34 Batch  45 / 525  Training Loss  0.0017410482978448272\n",
            "Epoch  34 Batch  46 / 525  Training Loss  0.00668681925162673\n",
            "Epoch  34 Batch  47 / 525  Training Loss  0.0016036384040489793\n",
            "Epoch  34 Batch  48 / 525  Training Loss  0.0009420294081792235\n",
            "Epoch  34 Batch  49 / 525  Training Loss  0.0014714017743244767\n",
            "Epoch  34 Batch  50 / 525  Training Loss  0.00023399856581818312\n",
            "Epoch  34 Batch  51 / 525  Training Loss  0.004512551706284285\n",
            "Epoch  34 Batch  52 / 525  Training Loss  0.0037944247014820576\n",
            "Epoch  34 Batch  53 / 525  Training Loss  0.003416957799345255\n",
            "Epoch  34 Batch  54 / 525  Training Loss  0.00133398303296417\n",
            "Epoch  34 Batch  55 / 525  Training Loss  0.0025542383082211018\n",
            "Epoch  34 Batch  56 / 525  Training Loss  0.0012815811205655336\n",
            "Epoch  34 Batch  57 / 525  Training Loss  0.002256899606436491\n",
            "Epoch  34 Batch  58 / 525  Training Loss  0.0028459406457841396\n",
            "Epoch  34 Batch  59 / 525  Training Loss  0.001300488947890699\n",
            "Epoch  34 Batch  60 / 525  Training Loss  0.00531385513022542\n",
            "Epoch  34 Batch  61 / 525  Training Loss  0.008836453780531883\n",
            "Epoch  34 Batch  62 / 525  Training Loss  0.0003842816222459078\n",
            "Epoch  34 Batch  63 / 525  Training Loss  0.0019287079339846969\n",
            "Epoch  34 Batch  64 / 525  Training Loss  0.0015869361814111471\n",
            "Epoch  34 Batch  65 / 525  Training Loss  0.0020654997788369656\n",
            "Epoch  34 Batch  66 / 525  Training Loss  0.0031615369953215122\n",
            "Epoch  34 Batch  67 / 525  Training Loss  0.0009114535641856492\n",
            "Epoch  34 Batch  68 / 525  Training Loss  0.002125877421349287\n",
            "Epoch  34 Batch  69 / 525  Training Loss  0.0009126606164500117\n",
            "Epoch  34 Batch  70 / 525  Training Loss  0.0007906230166554451\n",
            "Epoch  34 Batch  71 / 525  Training Loss  0.0007190959295257926\n",
            "Epoch  34 Batch  72 / 525  Training Loss  0.004817029926925898\n",
            "Epoch  34 Batch  73 / 525  Training Loss  0.002068428322672844\n",
            "Epoch  34 Batch  74 / 525  Training Loss  0.0010475958697497845\n",
            "Epoch  34 Batch  75 / 525  Training Loss  0.0008112718351185322\n",
            "Epoch  34 Batch  76 / 525  Training Loss  0.000818801112473011\n",
            "Epoch  34 Batch  77 / 525  Training Loss  0.0003033589746337384\n",
            "Epoch  34 Batch  78 / 525  Training Loss  0.0010460137855261564\n",
            "Epoch  34 Batch  79 / 525  Training Loss  0.003318963572382927\n",
            "Epoch  34 Batch  80 / 525  Training Loss  0.0015807731542736292\n",
            "Epoch  34 Batch  81 / 525  Training Loss  0.003561685560271144\n",
            "Epoch  34 Batch  82 / 525  Training Loss  0.006414291448891163\n",
            "Epoch  34 Batch  83 / 525  Training Loss  0.0014895298518240452\n",
            "Epoch  34 Batch  84 / 525  Training Loss  0.0011057088850066066\n",
            "Epoch  34 Batch  85 / 525  Training Loss  0.000533735437784344\n",
            "Epoch  34 Batch  86 / 525  Training Loss  0.002056204481050372\n",
            "Epoch  34 Batch  87 / 525  Training Loss  0.0012755877105519176\n",
            "Epoch  34 Batch  88 / 525  Training Loss  0.0013226649025455117\n",
            "Epoch  34 Batch  89 / 525  Training Loss  0.002250183839350939\n",
            "Epoch  34 Batch  90 / 525  Training Loss  0.002417685929685831\n",
            "Epoch  34 Batch  91 / 525  Training Loss  0.0010144728003069758\n",
            "Epoch  34 Batch  92 / 525  Training Loss  0.0012472209054976702\n",
            "Epoch  34 Batch  93 / 525  Training Loss  0.00947632361203432\n",
            "Epoch  34 Batch  94 / 525  Training Loss  0.00044768815860152245\n",
            "Epoch  34 Batch  95 / 525  Training Loss  0.002782754832878709\n",
            "Epoch  34 Batch  96 / 525  Training Loss  0.0023658638820052147\n",
            "Epoch  34 Batch  97 / 525  Training Loss  0.0006482552271336317\n",
            "Epoch  34 Batch  98 / 525  Training Loss  0.004947949666529894\n",
            "Epoch  34 Batch  99 / 525  Training Loss  0.0012486388441175222\n",
            "Epoch  34 Batch  100 / 525  Training Loss  0.00485735759139061\n",
            "Epoch  34 Batch  101 / 525  Training Loss  0.0012690846342593431\n",
            "Epoch  34 Batch  102 / 525  Training Loss  0.0020338576287031174\n",
            "Epoch  34 Batch  103 / 525  Training Loss  0.003446331713348627\n",
            "Epoch  34 Batch  104 / 525  Training Loss  0.00217458326369524\n",
            "Epoch  34 Batch  105 / 525  Training Loss  0.0022925063967704773\n",
            "Epoch  34 Batch  106 / 525  Training Loss  0.0009996709413826466\n",
            "Epoch  34 Batch  107 / 525  Training Loss  0.004079666920006275\n",
            "Epoch  34 Batch  108 / 525  Training Loss  0.007939002476632595\n",
            "Epoch  34 Batch  109 / 525  Training Loss  0.0006090848473832011\n",
            "Epoch  34 Batch  110 / 525  Training Loss  0.001997541170567274\n",
            "Epoch  34 Batch  111 / 525  Training Loss  0.0015848185867071152\n",
            "Epoch  34 Batch  112 / 525  Training Loss  0.01018886175006628\n",
            "Epoch  34 Batch  113 / 525  Training Loss  0.0005373957683332264\n",
            "Epoch  34 Batch  114 / 525  Training Loss  0.0012846067547798157\n",
            "Epoch  34 Batch  115 / 525  Training Loss  0.013127021491527557\n",
            "Epoch  34 Batch  116 / 525  Training Loss  0.009598730131983757\n",
            "Epoch  34 Batch  117 / 525  Training Loss  0.0012451681541278958\n",
            "Epoch  34 Batch  118 / 525  Training Loss  0.005261913873255253\n",
            "Epoch  34 Batch  119 / 525  Training Loss  0.0013946688268333673\n",
            "Epoch  34 Batch  120 / 525  Training Loss  0.0007832532864995301\n",
            "Epoch  34 Batch  121 / 525  Training Loss  0.0026559694670140743\n",
            "Epoch  34 Batch  122 / 525  Training Loss  0.00213463231921196\n",
            "Epoch  34 Batch  123 / 525  Training Loss  0.009317840449512005\n",
            "Epoch  34 Batch  124 / 525  Training Loss  0.0014582887524738908\n",
            "Epoch  34 Batch  125 / 525  Training Loss  0.001156388781964779\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  34 Batch  126 / 525  Training Loss  0.0012099710293114185\n",
            "Epoch  34 Batch  127 / 525  Training Loss  0.003961953334510326\n",
            "Epoch  34 Batch  128 / 525  Training Loss  0.0026839557103812695\n",
            "Epoch  34 Batch  129 / 525  Training Loss  0.002413484500721097\n",
            "Epoch  34 Batch  130 / 525  Training Loss  0.0003947465738747269\n",
            "Epoch  34 Batch  131 / 525  Training Loss  0.0010865775402635336\n",
            "Epoch  34 Batch  132 / 525  Training Loss  0.0012805755250155926\n",
            "Epoch  34 Batch  133 / 525  Training Loss  0.000985757797025144\n",
            "Epoch  34 Batch  134 / 525  Training Loss  0.005838883109390736\n",
            "Epoch  34 Batch  135 / 525  Training Loss  0.012535857036709785\n",
            "Epoch  34 Batch  136 / 525  Training Loss  0.0011274119606241584\n",
            "Epoch  34 Batch  137 / 525  Training Loss  0.0035770852118730545\n",
            "Epoch  34 Batch  138 / 525  Training Loss  0.0006486971979029477\n",
            "Epoch  34 Batch  139 / 525  Training Loss  0.002504381351172924\n",
            "Epoch  34 Batch  140 / 525  Training Loss  0.006580493412911892\n",
            "Epoch  34 Batch  141 / 525  Training Loss  0.0005388660938479006\n",
            "Epoch  34 Batch  142 / 525  Training Loss  0.0003851069777738303\n",
            "Epoch  34 Batch  143 / 525  Training Loss  0.005883796606212854\n",
            "Epoch  34 Batch  144 / 525  Training Loss  0.0027067246846854687\n",
            "Epoch  34 Batch  145 / 525  Training Loss  0.0012363085988909006\n",
            "Epoch  34 Batch  146 / 525  Training Loss  0.005598103627562523\n",
            "Epoch  34 Batch  147 / 525  Training Loss  0.0040850769728422165\n",
            "Epoch  34 Batch  148 / 525  Training Loss  0.002562594600021839\n",
            "Epoch  34 Batch  149 / 525  Training Loss  0.001966890413314104\n",
            "Epoch  34 Batch  150 / 525  Training Loss  0.0020906804129481316\n",
            "Epoch  34 Batch  151 / 525  Training Loss  0.001410166616551578\n",
            "Epoch  34 Batch  152 / 525  Training Loss  0.001078817993402481\n",
            "Epoch  34 Batch  153 / 525  Training Loss  0.0008143866434693336\n",
            "Epoch  34 Batch  154 / 525  Training Loss  0.002597121288999915\n",
            "Epoch  34 Batch  155 / 525  Training Loss  0.0006583994836546481\n",
            "Epoch  34 Batch  156 / 525  Training Loss  0.0030416594818234444\n",
            "Epoch  34 Batch  157 / 525  Training Loss  0.003868111176416278\n",
            "Epoch  34 Batch  158 / 525  Training Loss  0.0020632254891097546\n",
            "Epoch  34 Batch  159 / 525  Training Loss  0.0007467572577297688\n",
            "Epoch  34 Batch  160 / 525  Training Loss  0.004127734340727329\n",
            "Epoch  34 Batch  161 / 525  Training Loss  0.005950007122009993\n",
            "Epoch  34 Batch  162 / 525  Training Loss  0.0006738725351169705\n",
            "Epoch  34 Batch  163 / 525  Training Loss  0.0016618743538856506\n",
            "Epoch  34 Batch  164 / 525  Training Loss  0.0028459688182920218\n",
            "Epoch  34 Batch  165 / 525  Training Loss  0.00021126768842805177\n",
            "Epoch  34 Batch  166 / 525  Training Loss  0.005794106982648373\n",
            "Epoch  34 Batch  167 / 525  Training Loss  0.0004537876811809838\n",
            "Epoch  34 Batch  168 / 525  Training Loss  0.0007586142746731639\n",
            "Epoch  34 Batch  169 / 525  Training Loss  0.003544693812727928\n",
            "Epoch  34 Batch  170 / 525  Training Loss  0.0050676362589001656\n",
            "Epoch  34 Batch  171 / 525  Training Loss  0.0027549201622605324\n",
            "Epoch  34 Batch  172 / 525  Training Loss  0.0036921785213053226\n",
            "Epoch  34 Batch  173 / 525  Training Loss  0.005332618951797485\n",
            "Epoch  34 Batch  174 / 525  Training Loss  0.005607247818261385\n",
            "Epoch  34 Batch  175 / 525  Training Loss  0.005084474571049213\n",
            "Epoch  34 Batch  176 / 525  Training Loss  0.0006623705849051476\n",
            "Epoch  34 Batch  177 / 525  Training Loss  0.0011810316937044263\n",
            "Epoch  34 Batch  178 / 525  Training Loss  0.0008164061000570655\n",
            "Epoch  34 Batch  179 / 525  Training Loss  0.0023173547815531492\n",
            "Epoch  34 Batch  180 / 525  Training Loss  0.00603750254958868\n",
            "Epoch  34 Batch  181 / 525  Training Loss  0.0015421806601807475\n",
            "Epoch  34 Batch  182 / 525  Training Loss  0.0029602323193103075\n",
            "Epoch  34 Batch  183 / 525  Training Loss  0.003560699988156557\n",
            "Epoch  34 Batch  184 / 525  Training Loss  0.0013903030194342136\n",
            "Epoch  34 Batch  185 / 525  Training Loss  0.0038344361819326878\n",
            "Epoch  34 Batch  186 / 525  Training Loss  0.001966832671314478\n",
            "Epoch  34 Batch  187 / 525  Training Loss  0.0009804567089304328\n",
            "Epoch  34 Batch  188 / 525  Training Loss  0.0009753193589858711\n",
            "Epoch  34 Batch  189 / 525  Training Loss  0.007184501737356186\n",
            "Epoch  34 Batch  190 / 525  Training Loss  0.016655059531331062\n",
            "Epoch  34 Batch  191 / 525  Training Loss  0.003932016436010599\n",
            "Epoch  34 Batch  192 / 525  Training Loss  0.004418902564793825\n",
            "Epoch  34 Batch  193 / 525  Training Loss  0.008039308711886406\n",
            "Epoch  34 Batch  194 / 525  Training Loss  0.011541858315467834\n",
            "Epoch  34 Batch  195 / 525  Training Loss  0.002263734582811594\n",
            "Epoch  34 Batch  196 / 525  Training Loss  0.0033230497501790524\n",
            "Epoch  34 Batch  197 / 525  Training Loss  0.0014777823816984892\n",
            "Epoch  34 Batch  198 / 525  Training Loss  0.0014754809672012925\n",
            "Epoch  34 Batch  199 / 525  Training Loss  0.004985026549547911\n",
            "Epoch  34 Batch  200 / 525  Training Loss  0.0013574191834777594\n",
            "Epoch  34 Batch  201 / 525  Training Loss  0.002228406723588705\n",
            "Epoch  34 Batch  202 / 525  Training Loss  0.004790282342582941\n",
            "Epoch  34 Batch  203 / 525  Training Loss  0.006892384495586157\n",
            "Epoch  34 Batch  204 / 525  Training Loss  0.0035027593839913607\n",
            "Epoch  34 Batch  205 / 525  Training Loss  0.0013618380762636662\n",
            "Epoch  34 Batch  206 / 525  Training Loss  0.0011093330103904009\n",
            "Epoch  34 Batch  207 / 525  Training Loss  0.0011848285794258118\n",
            "Epoch  34 Batch  208 / 525  Training Loss  0.0031755634117871523\n",
            "Epoch  34 Batch  209 / 525  Training Loss  0.008752163499593735\n",
            "Epoch  34 Batch  210 / 525  Training Loss  0.0007673248765058815\n",
            "Epoch  34 Batch  211 / 525  Training Loss  0.004707798361778259\n",
            "Epoch  34 Batch  212 / 525  Training Loss  0.00293719326145947\n",
            "Epoch  34 Batch  213 / 525  Training Loss  0.003170877695083618\n",
            "Epoch  34 Batch  214 / 525  Training Loss  0.009668583981692791\n",
            "Epoch  34 Batch  215 / 525  Training Loss  0.0006848384509794414\n",
            "Epoch  34 Batch  216 / 525  Training Loss  0.0014368582051247358\n",
            "Epoch  34 Batch  217 / 525  Training Loss  0.0020009714644402266\n",
            "Epoch  34 Batch  218 / 525  Training Loss  0.00234567211009562\n",
            "Epoch  34 Batch  219 / 525  Training Loss  0.005200020968914032\n",
            "Epoch  34 Batch  220 / 525  Training Loss  0.007882134057581425\n",
            "Epoch  34 Batch  221 / 525  Training Loss  0.00551619054749608\n",
            "Epoch  34 Batch  222 / 525  Training Loss  0.002730648498982191\n",
            "Epoch  34 Batch  223 / 525  Training Loss  0.003388386918231845\n",
            "Epoch  34 Batch  224 / 525  Training Loss  0.0004661936254706234\n",
            "Epoch  34 Batch  225 / 525  Training Loss  0.001542445970699191\n",
            "Epoch  34 Batch  226 / 525  Training Loss  0.010203031823039055\n",
            "Epoch  34 Batch  227 / 525  Training Loss  0.002073928015306592\n",
            "Epoch  34 Batch  228 / 525  Training Loss  0.004842041991651058\n",
            "Epoch  34 Batch  229 / 525  Training Loss  0.0017301026964560151\n",
            "Epoch  34 Batch  230 / 525  Training Loss  0.0006981791811995208\n",
            "Epoch  34 Batch  231 / 525  Training Loss  0.0029048623982816935\n",
            "Epoch  34 Batch  232 / 525  Training Loss  0.0015690283617004752\n",
            "Epoch  34 Batch  233 / 525  Training Loss  0.008010203018784523\n",
            "Epoch  34 Batch  234 / 525  Training Loss  0.0012623288203030825\n",
            "Epoch  34 Batch  235 / 525  Training Loss  0.0031142274383455515\n",
            "Epoch  34 Batch  236 / 525  Training Loss  0.0007767794886603951\n",
            "Epoch  34 Batch  237 / 525  Training Loss  0.000342333602020517\n",
            "Epoch  34 Batch  238 / 525  Training Loss  0.0007519511273130774\n",
            "Epoch  34 Batch  239 / 525  Training Loss  0.0008814729517325759\n",
            "Epoch  34 Batch  240 / 525  Training Loss  0.002238623332232237\n",
            "Epoch  34 Batch  241 / 525  Training Loss  0.0012384377187117934\n",
            "Epoch  34 Batch  242 / 525  Training Loss  0.002663717605173588\n",
            "Epoch  34 Batch  243 / 525  Training Loss  0.0013452342245727777\n",
            "Epoch  34 Batch  244 / 525  Training Loss  0.002640092046931386\n",
            "Epoch  34 Batch  245 / 525  Training Loss  0.0017746262019500136\n",
            "Epoch  34 Batch  246 / 525  Training Loss  0.0005063207936473191\n",
            "Epoch  34 Batch  247 / 525  Training Loss  0.002855705562978983\n",
            "Epoch  34 Batch  248 / 525  Training Loss  0.0016550874570384622\n",
            "Epoch  34 Batch  249 / 525  Training Loss  0.0017146203899756074\n",
            "Epoch  34 Batch  250 / 525  Training Loss  0.003683301853016019\n",
            "Epoch  34 Batch  251 / 525  Training Loss  0.0013466639211401343\n",
            "Epoch  34 Batch  252 / 525  Training Loss  0.002272040816023946\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  34 Batch  253 / 525  Training Loss  0.002454024041071534\n",
            "Epoch  34 Batch  254 / 525  Training Loss  0.001453540986403823\n",
            "Epoch  34 Batch  255 / 525  Training Loss  0.005363505333662033\n",
            "Epoch  34 Batch  256 / 525  Training Loss  0.000376960844732821\n",
            "Epoch  34 Batch  257 / 525  Training Loss  0.00034959142794832587\n",
            "Epoch  34 Batch  258 / 525  Training Loss  0.001024676370434463\n",
            "Epoch  34 Batch  259 / 525  Training Loss  0.0006871279911138117\n",
            "Epoch  34 Batch  260 / 525  Training Loss  0.0015007199253886938\n",
            "Epoch  34 Batch  261 / 525  Training Loss  0.006053254008293152\n",
            "Epoch  34 Batch  262 / 525  Training Loss  0.0003159072366543114\n",
            "Epoch  34 Batch  263 / 525  Training Loss  0.002169282641261816\n",
            "Epoch  34 Batch  264 / 525  Training Loss  0.0003331045445520431\n",
            "Epoch  34 Batch  265 / 525  Training Loss  0.0023270118981599808\n",
            "Epoch  34 Batch  266 / 525  Training Loss  0.0007320686709135771\n",
            "Epoch  34 Batch  267 / 525  Training Loss  0.0014853909378871322\n",
            "Epoch  34 Batch  268 / 525  Training Loss  0.0038863327354192734\n",
            "Epoch  34 Batch  269 / 525  Training Loss  0.0016286777099594474\n",
            "Epoch  34 Batch  270 / 525  Training Loss  0.0011288233799859881\n",
            "Epoch  34 Batch  271 / 525  Training Loss  0.0007817237637937069\n",
            "Epoch  34 Batch  272 / 525  Training Loss  0.0036218208260834217\n",
            "Epoch  34 Batch  273 / 525  Training Loss  0.0010916860774159431\n",
            "Epoch  34 Batch  274 / 525  Training Loss  0.0016738094855099916\n",
            "Epoch  34 Batch  275 / 525  Training Loss  0.003576371120288968\n",
            "Epoch  34 Batch  276 / 525  Training Loss  0.005426036659628153\n",
            "Epoch  34 Batch  277 / 525  Training Loss  0.002731806132942438\n",
            "Epoch  34 Batch  278 / 525  Training Loss  0.001058054156601429\n",
            "Epoch  34 Batch  279 / 525  Training Loss  0.00156259851064533\n",
            "Epoch  34 Batch  280 / 525  Training Loss  0.001995225204154849\n",
            "Epoch  34 Batch  281 / 525  Training Loss  0.004749582149088383\n",
            "Epoch  34 Batch  282 / 525  Training Loss  0.0004623766872100532\n",
            "Epoch  34 Batch  283 / 525  Training Loss  0.002836310537531972\n",
            "Epoch  34 Batch  284 / 525  Training Loss  0.00023565709125250578\n",
            "Epoch  34 Batch  285 / 525  Training Loss  0.0008098905673250556\n",
            "Epoch  34 Batch  286 / 525  Training Loss  0.002782513154670596\n",
            "Epoch  34 Batch  287 / 525  Training Loss  0.0018213711446151137\n",
            "Epoch  34 Batch  288 / 525  Training Loss  0.0003605139208957553\n",
            "Epoch  34 Batch  289 / 525  Training Loss  0.0026413698215037584\n",
            "Epoch  34 Batch  290 / 525  Training Loss  0.0032125641591846943\n",
            "Epoch  34 Batch  291 / 525  Training Loss  0.0005943020223639905\n",
            "Epoch  34 Batch  292 / 525  Training Loss  0.0006934489356353879\n",
            "Epoch  34 Batch  293 / 525  Training Loss  0.00259254383854568\n",
            "Epoch  34 Batch  294 / 525  Training Loss  0.0015181717462837696\n",
            "Epoch  34 Batch  295 / 525  Training Loss  0.0013640177203342319\n",
            "Epoch  34 Batch  296 / 525  Training Loss  0.004369043745100498\n",
            "Epoch  34 Batch  297 / 525  Training Loss  0.0015184637159109116\n",
            "Epoch  34 Batch  298 / 525  Training Loss  0.0038717719726264477\n",
            "Epoch  34 Batch  299 / 525  Training Loss  0.002048380207270384\n",
            "Epoch  34 Batch  300 / 525  Training Loss  0.0027690776623785496\n",
            "Epoch  34 Batch  301 / 525  Training Loss  0.0029543156269937754\n",
            "Epoch  34 Batch  302 / 525  Training Loss  0.001405311981216073\n",
            "Epoch  34 Batch  303 / 525  Training Loss  0.0035076416097581387\n",
            "Epoch  34 Batch  304 / 525  Training Loss  0.0010827495716512203\n",
            "Epoch  34 Batch  305 / 525  Training Loss  0.0062398603186011314\n",
            "Epoch  34 Batch  306 / 525  Training Loss  0.0030493824742734432\n",
            "Epoch  34 Batch  307 / 525  Training Loss  0.007173990365117788\n",
            "Epoch  34 Batch  308 / 525  Training Loss  0.005785738117992878\n",
            "Epoch  34 Batch  309 / 525  Training Loss  0.0025559545028954744\n",
            "Epoch  34 Batch  310 / 525  Training Loss  0.010155697353184223\n",
            "Epoch  34 Batch  311 / 525  Training Loss  0.003205252345651388\n",
            "Epoch  34 Batch  312 / 525  Training Loss  0.003964542411267757\n",
            "Epoch  34 Batch  313 / 525  Training Loss  0.007412177510559559\n",
            "Epoch  34 Batch  314 / 525  Training Loss  0.0031808088533580303\n",
            "Epoch  34 Batch  315 / 525  Training Loss  0.00027947896160185337\n",
            "Epoch  34 Batch  316 / 525  Training Loss  0.005248232278972864\n",
            "Epoch  34 Batch  317 / 525  Training Loss  0.002245068084448576\n",
            "Epoch  34 Batch  318 / 525  Training Loss  0.006462037563323975\n",
            "Epoch  34 Batch  319 / 525  Training Loss  0.0025473032146692276\n",
            "Epoch  34 Batch  320 / 525  Training Loss  0.0012835109373554587\n",
            "Epoch  34 Batch  321 / 525  Training Loss  0.012594613246619701\n",
            "Epoch  34 Batch  322 / 525  Training Loss  0.004150605760514736\n",
            "Epoch  34 Batch  323 / 525  Training Loss  0.004363367334008217\n",
            "Epoch  34 Batch  324 / 525  Training Loss  0.004268678370863199\n",
            "Epoch  34 Batch  325 / 525  Training Loss  0.002165460493415594\n",
            "Epoch  34 Batch  326 / 525  Training Loss  0.002374684903770685\n",
            "Epoch  34 Batch  327 / 525  Training Loss  0.0012839308474212885\n",
            "Epoch  34 Batch  328 / 525  Training Loss  0.0015305543784052134\n",
            "Epoch  34 Batch  329 / 525  Training Loss  0.015441740863025188\n",
            "Epoch  34 Batch  330 / 525  Training Loss  0.0019112465670332313\n",
            "Epoch  34 Batch  331 / 525  Training Loss  0.002906559035181999\n",
            "Epoch  34 Batch  332 / 525  Training Loss  0.008290620520710945\n",
            "Epoch  34 Batch  333 / 525  Training Loss  0.0006450659711845219\n",
            "Epoch  34 Batch  334 / 525  Training Loss  0.00474867969751358\n",
            "Epoch  34 Batch  335 / 525  Training Loss  0.005245755426585674\n",
            "Epoch  34 Batch  336 / 525  Training Loss  0.000883314001839608\n",
            "Epoch  34 Batch  337 / 525  Training Loss  0.007415960542857647\n",
            "Epoch  34 Batch  338 / 525  Training Loss  0.0013210910838097334\n",
            "Epoch  34 Batch  339 / 525  Training Loss  0.003027126891538501\n",
            "Epoch  34 Batch  340 / 525  Training Loss  0.007445626892149448\n",
            "Epoch  34 Batch  341 / 525  Training Loss  0.011614633724093437\n",
            "Epoch  34 Batch  342 / 525  Training Loss  0.0008579768473282456\n",
            "Epoch  34 Batch  343 / 525  Training Loss  0.001779904356226325\n",
            "Epoch  34 Batch  344 / 525  Training Loss  0.00036430972977541387\n",
            "Epoch  34 Batch  345 / 525  Training Loss  0.0008403297397308052\n",
            "Epoch  34 Batch  346 / 525  Training Loss  0.0028655200731009245\n",
            "Epoch  34 Batch  347 / 525  Training Loss  0.004320361651480198\n",
            "Epoch  34 Batch  348 / 525  Training Loss  0.005244280211627483\n",
            "Epoch  34 Batch  349 / 525  Training Loss  0.001363880466669798\n",
            "Epoch  34 Batch  350 / 525  Training Loss  0.006037767045199871\n",
            "Epoch  34 Batch  351 / 525  Training Loss  0.0004107931745238602\n",
            "Epoch  34 Batch  352 / 525  Training Loss  0.0009751394391059875\n",
            "Epoch  34 Batch  353 / 525  Training Loss  0.0006490419618785381\n",
            "Epoch  34 Batch  354 / 525  Training Loss  0.007697027176618576\n",
            "Epoch  34 Batch  355 / 525  Training Loss  0.004658378195017576\n",
            "Epoch  34 Batch  356 / 525  Training Loss  0.0004471881838981062\n",
            "Epoch  34 Batch  357 / 525  Training Loss  0.015491400845348835\n",
            "Epoch  34 Batch  358 / 525  Training Loss  0.009059062227606773\n",
            "Epoch  34 Batch  359 / 525  Training Loss  0.0017330910777673125\n",
            "Epoch  34 Batch  360 / 525  Training Loss  0.003138246713206172\n",
            "Epoch  34 Batch  361 / 525  Training Loss  0.008064284920692444\n",
            "Epoch  34 Batch  362 / 525  Training Loss  0.004354987759143114\n",
            "Epoch  34 Batch  363 / 525  Training Loss  0.0017760073533281684\n",
            "Epoch  34 Batch  364 / 525  Training Loss  0.007756218314170837\n",
            "Epoch  34 Batch  365 / 525  Training Loss  0.003441035747528076\n",
            "Epoch  34 Batch  366 / 525  Training Loss  0.0066621676087379456\n",
            "Epoch  34 Batch  367 / 525  Training Loss  0.003667970420792699\n",
            "Epoch  34 Batch  368 / 525  Training Loss  0.0005843464168719947\n",
            "Epoch  34 Batch  369 / 525  Training Loss  0.0019946121610701084\n",
            "Epoch  34 Batch  370 / 525  Training Loss  0.00122662796638906\n",
            "Epoch  34 Batch  371 / 525  Training Loss  0.002259939443320036\n",
            "Epoch  34 Batch  372 / 525  Training Loss  0.0032903223764151335\n",
            "Epoch  34 Batch  373 / 525  Training Loss  0.0063324966467916965\n",
            "Epoch  34 Batch  374 / 525  Training Loss  0.0009555484284646809\n",
            "Epoch  34 Batch  375 / 525  Training Loss  0.004781962372362614\n",
            "Epoch  34 Batch  376 / 525  Training Loss  0.003561152843758464\n",
            "Epoch  34 Batch  377 / 525  Training Loss  0.001487556379288435\n",
            "Epoch  34 Batch  378 / 525  Training Loss  0.005898819770663977\n",
            "Epoch  34 Batch  379 / 525  Training Loss  0.0012216001050546765\n",
            "Epoch  34 Batch  380 / 525  Training Loss  0.0016688501928001642\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  34 Batch  381 / 525  Training Loss  0.00040890564559958875\n",
            "Epoch  34 Batch  382 / 525  Training Loss  0.0015598003519698977\n",
            "Epoch  34 Batch  383 / 525  Training Loss  0.008571183308959007\n",
            "Epoch  34 Batch  384 / 525  Training Loss  0.005410200450569391\n",
            "Epoch  34 Batch  385 / 525  Training Loss  0.0006543989293277264\n",
            "Epoch  34 Batch  386 / 525  Training Loss  0.004633840639144182\n",
            "Epoch  34 Batch  387 / 525  Training Loss  0.0006465236656367779\n",
            "Epoch  34 Batch  388 / 525  Training Loss  0.001105406554415822\n",
            "Epoch  34 Batch  389 / 525  Training Loss  0.003392555518075824\n",
            "Epoch  34 Batch  390 / 525  Training Loss  0.004253102466464043\n",
            "Epoch  34 Batch  391 / 525  Training Loss  0.004499565809965134\n",
            "Epoch  34 Batch  392 / 525  Training Loss  0.004044233355671167\n",
            "Epoch  34 Batch  393 / 525  Training Loss  0.012606965377926826\n",
            "Epoch  34 Batch  394 / 525  Training Loss  0.001385003444738686\n",
            "Epoch  34 Batch  395 / 525  Training Loss  0.007896610535681248\n",
            "Epoch  34 Batch  396 / 525  Training Loss  0.005151013378053904\n",
            "Epoch  34 Batch  397 / 525  Training Loss  0.0024940157309174538\n",
            "Epoch  34 Batch  398 / 525  Training Loss  0.0015656653558835387\n",
            "Epoch  34 Batch  399 / 525  Training Loss  0.009435197338461876\n",
            "Epoch  34 Batch  400 / 525  Training Loss  0.007631533779203892\n",
            "Epoch  34 Batch  401 / 525  Training Loss  0.0011924385325983167\n",
            "Epoch  34 Batch  402 / 525  Training Loss  0.0035046543926000595\n",
            "Epoch  34 Batch  403 / 525  Training Loss  0.007893403992056847\n",
            "Epoch  34 Batch  404 / 525  Training Loss  0.001926435506902635\n",
            "Epoch  34 Batch  405 / 525  Training Loss  0.000462041178252548\n",
            "Epoch  34 Batch  406 / 525  Training Loss  0.0022134713362902403\n",
            "Epoch  34 Batch  407 / 525  Training Loss  0.007934030145406723\n",
            "Epoch  34 Batch  408 / 525  Training Loss  0.015006271190941334\n",
            "Epoch  34 Batch  409 / 525  Training Loss  0.0027784674894064665\n",
            "Epoch  34 Batch  410 / 525  Training Loss  0.0008773870067670941\n",
            "Epoch  34 Batch  411 / 525  Training Loss  0.0008330339333042502\n",
            "Epoch  34 Batch  412 / 525  Training Loss  0.0020506144501268864\n",
            "Epoch  34 Batch  413 / 525  Training Loss  0.003403209848329425\n",
            "Epoch  34 Batch  414 / 525  Training Loss  0.0022082149516791105\n",
            "Epoch  34 Batch  415 / 525  Training Loss  0.004611837677657604\n",
            "Epoch  34 Batch  416 / 525  Training Loss  0.003851731773465872\n",
            "Epoch  34 Batch  417 / 525  Training Loss  0.012384188361465931\n",
            "Epoch  34 Batch  418 / 525  Training Loss  0.0012945677153766155\n",
            "Epoch  34 Batch  419 / 525  Training Loss  0.02884923852980137\n",
            "Epoch  34 Batch  420 / 525  Training Loss  0.019656475633382797\n",
            "Epoch  34 Batch  421 / 525  Training Loss  0.0035062339156866074\n",
            "Epoch  34 Batch  422 / 525  Training Loss  0.007889611646533012\n",
            "Epoch  34 Batch  423 / 525  Training Loss  0.0032004863023757935\n",
            "Epoch  34 Batch  424 / 525  Training Loss  0.004405467305332422\n",
            "Epoch  34 Batch  425 / 525  Training Loss  0.009972267784178257\n",
            "Epoch  34 Batch  426 / 525  Training Loss  0.006044028792530298\n",
            "Epoch  34 Batch  427 / 525  Training Loss  0.007919950410723686\n",
            "Epoch  34 Batch  428 / 525  Training Loss  0.003065698081627488\n",
            "Epoch  34 Batch  429 / 525  Training Loss  0.016663312911987305\n",
            "Epoch  34 Batch  430 / 525  Training Loss  0.004580060951411724\n",
            "Epoch  34 Batch  431 / 525  Training Loss  0.004844952840358019\n",
            "Epoch  34 Batch  432 / 525  Training Loss  0.007330499589443207\n",
            "Epoch  34 Batch  433 / 525  Training Loss  0.01599346101284027\n",
            "Epoch  34 Batch  434 / 525  Training Loss  0.010487721301615238\n",
            "Epoch  34 Batch  435 / 525  Training Loss  0.0041596544906497\n",
            "Epoch  34 Batch  436 / 525  Training Loss  0.004580441862344742\n",
            "Epoch  34 Batch  437 / 525  Training Loss  0.017986144870519638\n",
            "Epoch  34 Batch  438 / 525  Training Loss  0.005922430660575628\n",
            "Epoch  34 Batch  439 / 525  Training Loss  0.010187431238591671\n",
            "Epoch  34 Batch  440 / 525  Training Loss  0.006712460424751043\n",
            "Epoch  34 Batch  441 / 525  Training Loss  0.0022070002742111683\n",
            "Epoch  34 Batch  442 / 525  Training Loss  0.0016717739636078477\n",
            "Epoch  34 Batch  443 / 525  Training Loss  0.0010320941219106317\n",
            "Epoch  34 Batch  444 / 525  Training Loss  0.0009341022232547402\n",
            "Epoch  34 Batch  445 / 525  Training Loss  0.014463013038039207\n",
            "Epoch  34 Batch  446 / 525  Training Loss  0.0025649103336036205\n",
            "Epoch  34 Batch  447 / 525  Training Loss  0.0018077325075864792\n",
            "Epoch  34 Batch  448 / 525  Training Loss  0.017781253904104233\n",
            "Epoch  34 Batch  449 / 525  Training Loss  0.001859116367995739\n",
            "Epoch  34 Batch  450 / 525  Training Loss  0.003911635838449001\n",
            "Epoch  34 Batch  451 / 525  Training Loss  0.0040018074214458466\n",
            "Epoch  34 Batch  452 / 525  Training Loss  0.005705235991626978\n",
            "Epoch  34 Batch  453 / 525  Training Loss  0.0004892150755040348\n",
            "Epoch  34 Batch  454 / 525  Training Loss  0.01272397767752409\n",
            "Epoch  34 Batch  455 / 525  Training Loss  0.0011825037654489279\n",
            "Epoch  34 Batch  456 / 525  Training Loss  0.003239233046770096\n",
            "Epoch  34 Batch  457 / 525  Training Loss  0.0011317930184304714\n",
            "Epoch  34 Batch  458 / 525  Training Loss  0.003365142270922661\n",
            "Epoch  34 Batch  459 / 525  Training Loss  0.003102012211456895\n",
            "Epoch  34 Batch  460 / 525  Training Loss  0.008829801343381405\n",
            "Epoch  34 Batch  461 / 525  Training Loss  0.006456606090068817\n",
            "Epoch  34 Batch  462 / 525  Training Loss  0.0026156415697187185\n",
            "Epoch  34 Batch  463 / 525  Training Loss  0.005143399350345135\n",
            "Epoch  34 Batch  464 / 525  Training Loss  0.003989416174590588\n",
            "Epoch  34 Batch  465 / 525  Training Loss  0.0007452007266692817\n",
            "Epoch  34 Batch  466 / 525  Training Loss  0.009483881294727325\n",
            "Epoch  34 Batch  467 / 525  Training Loss  0.008971044793725014\n",
            "Epoch  34 Batch  468 / 525  Training Loss  0.0032632488291710615\n",
            "Epoch  34 Batch  469 / 525  Training Loss  0.0017825404647737741\n",
            "Epoch  34 Batch  470 / 525  Training Loss  0.0074167088605463505\n",
            "Epoch  34 Batch  471 / 525  Training Loss  0.007784670684486628\n",
            "Epoch  34 Batch  472 / 525  Training Loss  0.007411669008433819\n",
            "Epoch  34 Batch  473 / 525  Training Loss  0.0009951406391337514\n",
            "Epoch  34 Batch  474 / 525  Training Loss  0.007222496904432774\n",
            "Epoch  34 Batch  475 / 525  Training Loss  0.001198726356960833\n",
            "Epoch  34 Batch  476 / 525  Training Loss  0.000913833559025079\n",
            "Epoch  34 Batch  477 / 525  Training Loss  0.0019405229249969125\n",
            "Epoch  34 Batch  478 / 525  Training Loss  0.001821892336010933\n",
            "Epoch  34 Batch  479 / 525  Training Loss  0.01049054879695177\n",
            "Epoch  34 Batch  480 / 525  Training Loss  0.0021896299440413713\n",
            "Epoch  34 Batch  481 / 525  Training Loss  0.012470824643969536\n",
            "Epoch  34 Batch  482 / 525  Training Loss  0.0049192821606993675\n",
            "Epoch  34 Batch  483 / 525  Training Loss  0.0037342174910008907\n",
            "Epoch  34 Batch  484 / 525  Training Loss  0.003780968952924013\n",
            "Epoch  34 Batch  485 / 525  Training Loss  0.0042909905314445496\n",
            "Epoch  34 Batch  486 / 525  Training Loss  0.0009503861656412482\n",
            "Epoch  34 Batch  487 / 525  Training Loss  0.004779912065714598\n",
            "Epoch  34 Batch  488 / 525  Training Loss  0.0023752073757350445\n",
            "Epoch  34 Batch  489 / 525  Training Loss  0.0038375116419047117\n",
            "Epoch  34 Batch  490 / 525  Training Loss  0.0034688350278884172\n",
            "Epoch  34 Batch  491 / 525  Training Loss  0.0036957282572984695\n",
            "Epoch  34 Batch  492 / 525  Training Loss  0.0022509596310555935\n",
            "Epoch  34 Batch  493 / 525  Training Loss  0.0011298151221126318\n",
            "Epoch  34 Batch  494 / 525  Training Loss  0.0019272702047601342\n",
            "Epoch  34 Batch  495 / 525  Training Loss  0.004198577720671892\n",
            "Epoch  34 Batch  496 / 525  Training Loss  0.0011093055363744497\n",
            "Epoch  34 Batch  497 / 525  Training Loss  0.0011762898648157716\n",
            "Epoch  34 Batch  498 / 525  Training Loss  0.0033807691652327776\n",
            "Epoch  34 Batch  499 / 525  Training Loss  0.003745210822671652\n",
            "Epoch  34 Batch  500 / 525  Training Loss  0.0022696289233863354\n",
            "Epoch  34 Batch  501 / 525  Training Loss  0.0015766024589538574\n",
            "Epoch  34 Batch  502 / 525  Training Loss  0.001059092814102769\n",
            "Epoch  34 Batch  503 / 525  Training Loss  0.0020916881039738655\n",
            "Epoch  34 Batch  504 / 525  Training Loss  0.007347674109041691\n",
            "Epoch  34 Batch  505 / 525  Training Loss  0.015457449480891228\n",
            "Epoch  34 Batch  506 / 525  Training Loss  0.00999494083225727\n",
            "Epoch  34 Batch  507 / 525  Training Loss  0.004088923800736666\n",
            "Epoch  34 Batch  508 / 525  Training Loss  0.012817086651921272\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  34 Batch  509 / 525  Training Loss  0.012611925601959229\n",
            "Epoch  34 Batch  510 / 525  Training Loss  0.00040922840707935393\n",
            "Epoch  34 Batch  511 / 525  Training Loss  0.0016662727575749159\n",
            "Epoch  34 Batch  512 / 525  Training Loss  0.013413148000836372\n",
            "Epoch  34 Batch  513 / 525  Training Loss  0.0010755769908428192\n",
            "Epoch  34 Batch  514 / 525  Training Loss  0.00390924047678709\n",
            "Epoch  34 Batch  515 / 525  Training Loss  0.0024131792597472668\n",
            "Epoch  34 Batch  516 / 525  Training Loss  0.010027805343270302\n",
            "Epoch  34 Batch  517 / 525  Training Loss  0.0013531369622796774\n",
            "Epoch  34 Batch  518 / 525  Training Loss  0.004991379100829363\n",
            "Epoch  34 Batch  519 / 525  Training Loss  0.002686512190848589\n",
            "Epoch  34 Batch  520 / 525  Training Loss  0.009324952960014343\n",
            "Epoch  34 Batch  521 / 525  Training Loss  0.00100802443921566\n",
            "Epoch  34 Batch  522 / 525  Training Loss  0.0033484133891761303\n",
            "Epoch  34 Batch  523 / 525  Training Loss  0.0014014107873663306\n",
            "Epoch  34 Batch  524 / 525  Training Loss  0.00183278345502913\n",
            "  35    |    -    |   0.003584   | 61.275000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 35\n",
            "Epoch  35 Batch  0 / 525  Training Loss  0.0014580225106328726\n",
            "Epoch  35 Batch  1 / 525  Training Loss  0.011415700428187847\n",
            "Epoch  35 Batch  2 / 525  Training Loss  0.003893781453371048\n",
            "Epoch  35 Batch  3 / 525  Training Loss  0.006464948412030935\n",
            "Epoch  35 Batch  4 / 525  Training Loss  0.001397541956976056\n",
            "Epoch  35 Batch  5 / 525  Training Loss  0.014197451993823051\n",
            "Epoch  35 Batch  6 / 525  Training Loss  0.003175676567479968\n",
            "Epoch  35 Batch  7 / 525  Training Loss  0.010258388705551624\n",
            "Epoch  35 Batch  8 / 525  Training Loss  0.002432566601783037\n",
            "Epoch  35 Batch  9 / 525  Training Loss  0.0011716396547853947\n",
            "Epoch  35 Batch  10 / 525  Training Loss  0.00464590871706605\n",
            "Epoch  35 Batch  11 / 525  Training Loss  0.005358401220291853\n",
            "Epoch  35 Batch  12 / 525  Training Loss  0.006982285529375076\n",
            "Epoch  35 Batch  13 / 525  Training Loss  0.011799164116382599\n",
            "Epoch  35 Batch  14 / 525  Training Loss  0.011912825517356396\n",
            "Epoch  35 Batch  15 / 525  Training Loss  0.0013020049082115293\n",
            "Epoch  35 Batch  16 / 525  Training Loss  0.0012384008150547743\n",
            "Epoch  35 Batch  17 / 525  Training Loss  0.0015231822617352009\n",
            "Epoch  35 Batch  18 / 525  Training Loss  0.001927595934830606\n",
            "Epoch  35 Batch  19 / 525  Training Loss  0.00317047955468297\n",
            "Epoch  35 Batch  20 / 525  Training Loss  0.0010246795136481524\n",
            "Epoch  35 Batch  21 / 525  Training Loss  0.0010902972426265478\n",
            "Epoch  35 Batch  22 / 525  Training Loss  0.002531875856220722\n",
            "Epoch  35 Batch  23 / 525  Training Loss  0.001285620266571641\n",
            "Epoch  35 Batch  24 / 525  Training Loss  0.007621072232723236\n",
            "Epoch  35 Batch  25 / 525  Training Loss  0.013908644206821918\n",
            "Epoch  35 Batch  26 / 525  Training Loss  0.004601930733770132\n",
            "Epoch  35 Batch  27 / 525  Training Loss  0.0030006219167262316\n",
            "Epoch  35 Batch  28 / 525  Training Loss  0.0030674883164465427\n",
            "Epoch  35 Batch  29 / 525  Training Loss  0.0037866272032260895\n",
            "Epoch  35 Batch  30 / 525  Training Loss  0.0036715450696647167\n",
            "Epoch  35 Batch  31 / 525  Training Loss  0.005761171691119671\n",
            "Epoch  35 Batch  32 / 525  Training Loss  0.0022733923979103565\n",
            "Epoch  35 Batch  33 / 525  Training Loss  0.0010065182577818632\n",
            "Epoch  35 Batch  34 / 525  Training Loss  0.0038889602292329073\n",
            "Epoch  35 Batch  35 / 525  Training Loss  0.0011647503124549985\n",
            "Epoch  35 Batch  36 / 525  Training Loss  0.0021849675104022026\n",
            "Epoch  35 Batch  37 / 525  Training Loss  0.0009917119750753045\n",
            "Epoch  35 Batch  38 / 525  Training Loss  0.0009267240529879928\n",
            "Epoch  35 Batch  39 / 525  Training Loss  0.005122908856719732\n",
            "Epoch  35 Batch  40 / 525  Training Loss  0.0005563596496358514\n",
            "Epoch  35 Batch  41 / 525  Training Loss  0.0034963064827024937\n",
            "Epoch  35 Batch  42 / 525  Training Loss  0.001034385059028864\n",
            "Epoch  35 Batch  43 / 525  Training Loss  0.00031568569829687476\n",
            "Epoch  35 Batch  44 / 525  Training Loss  0.0013597429497167468\n",
            "Epoch  35 Batch  45 / 525  Training Loss  0.003059705253690481\n",
            "Epoch  35 Batch  46 / 525  Training Loss  0.00043559103505685925\n",
            "Epoch  35 Batch  47 / 525  Training Loss  0.0032246082555502653\n",
            "Epoch  35 Batch  48 / 525  Training Loss  0.004389255307614803\n",
            "Epoch  35 Batch  49 / 525  Training Loss  0.0032893233001232147\n",
            "Epoch  35 Batch  50 / 525  Training Loss  0.0010801233584061265\n",
            "Epoch  35 Batch  51 / 525  Training Loss  0.00030553032411262393\n",
            "Epoch  35 Batch  52 / 525  Training Loss  0.002225116128101945\n",
            "Epoch  35 Batch  53 / 525  Training Loss  0.00023742832127027214\n",
            "Epoch  35 Batch  54 / 525  Training Loss  0.0009293251787312329\n",
            "Epoch  35 Batch  55 / 525  Training Loss  0.0009616657043807209\n",
            "Epoch  35 Batch  56 / 525  Training Loss  0.0014018681831657887\n",
            "Epoch  35 Batch  57 / 525  Training Loss  0.001485353452153504\n",
            "Epoch  35 Batch  58 / 525  Training Loss  0.00207923399284482\n",
            "Epoch  35 Batch  59 / 525  Training Loss  0.009839064441621304\n",
            "Epoch  35 Batch  60 / 525  Training Loss  0.002089779358357191\n",
            "Epoch  35 Batch  61 / 525  Training Loss  0.008642111904919147\n",
            "Epoch  35 Batch  62 / 525  Training Loss  0.007885011844336987\n",
            "Epoch  35 Batch  63 / 525  Training Loss  0.0017547737807035446\n",
            "Epoch  35 Batch  64 / 525  Training Loss  0.006065058521926403\n",
            "Epoch  35 Batch  65 / 525  Training Loss  0.003923098091036081\n",
            "Epoch  35 Batch  66 / 525  Training Loss  0.0028968113474547863\n",
            "Epoch  35 Batch  67 / 525  Training Loss  0.0016028496902436018\n",
            "Epoch  35 Batch  68 / 525  Training Loss  0.0036455572117120028\n",
            "Epoch  35 Batch  69 / 525  Training Loss  0.0003380735288374126\n",
            "Epoch  35 Batch  70 / 525  Training Loss  0.002083246363326907\n",
            "Epoch  35 Batch  71 / 525  Training Loss  0.0017412949819117785\n",
            "Epoch  35 Batch  72 / 525  Training Loss  0.003302420722320676\n",
            "Epoch  35 Batch  73 / 525  Training Loss  0.0062970928847789764\n",
            "Epoch  35 Batch  74 / 525  Training Loss  0.0016031665727496147\n",
            "Epoch  35 Batch  75 / 525  Training Loss  0.0006963643245398998\n",
            "Epoch  35 Batch  76 / 525  Training Loss  0.0011169506469741464\n",
            "Epoch  35 Batch  77 / 525  Training Loss  0.001138355233706534\n",
            "Epoch  35 Batch  78 / 525  Training Loss  0.005197278223931789\n",
            "Epoch  35 Batch  79 / 525  Training Loss  0.0009474433027207851\n",
            "Epoch  35 Batch  80 / 525  Training Loss  0.0014815863687545061\n",
            "Epoch  35 Batch  81 / 525  Training Loss  0.005989856086671352\n",
            "Epoch  35 Batch  82 / 525  Training Loss  0.003878717077895999\n",
            "Epoch  35 Batch  83 / 525  Training Loss  0.00218291487544775\n",
            "Epoch  35 Batch  84 / 525  Training Loss  0.00048043191782198846\n",
            "Epoch  35 Batch  85 / 525  Training Loss  0.0013377510476857424\n",
            "Epoch  35 Batch  86 / 525  Training Loss  0.0007076001493260264\n",
            "Epoch  35 Batch  87 / 525  Training Loss  0.0009363602730445564\n",
            "Epoch  35 Batch  88 / 525  Training Loss  0.0016201718244701624\n",
            "Epoch  35 Batch  89 / 525  Training Loss  0.004548950120806694\n",
            "Epoch  35 Batch  90 / 525  Training Loss  0.00535031920298934\n",
            "Epoch  35 Batch  91 / 525  Training Loss  0.0048991041257977486\n",
            "Epoch  35 Batch  92 / 525  Training Loss  0.00595206581056118\n",
            "Epoch  35 Batch  93 / 525  Training Loss  0.0004482154326979071\n",
            "Epoch  35 Batch  94 / 525  Training Loss  0.0015190959675237536\n",
            "Epoch  35 Batch  95 / 525  Training Loss  0.006505239754915237\n",
            "Epoch  35 Batch  96 / 525  Training Loss  0.0006723011611029506\n",
            "Epoch  35 Batch  97 / 525  Training Loss  0.004010336939245462\n",
            "Epoch  35 Batch  98 / 525  Training Loss  0.00597237516194582\n",
            "Epoch  35 Batch  99 / 525  Training Loss  0.002674330258741975\n",
            "Epoch  35 Batch  100 / 525  Training Loss  0.004774798639118671\n",
            "Epoch  35 Batch  101 / 525  Training Loss  0.0006259728688746691\n",
            "Epoch  35 Batch  102 / 525  Training Loss  0.00062244274886325\n",
            "Epoch  35 Batch  103 / 525  Training Loss  0.001127937575802207\n",
            "Epoch  35 Batch  104 / 525  Training Loss  0.004868562798947096\n",
            "Epoch  35 Batch  105 / 525  Training Loss  0.0007882878999225795\n",
            "Epoch  35 Batch  106 / 525  Training Loss  0.006665512919425964\n",
            "Epoch  35 Batch  107 / 525  Training Loss  0.0013522851513698697\n",
            "Epoch  35 Batch  108 / 525  Training Loss  0.005850673653185368\n",
            "Epoch  35 Batch  109 / 525  Training Loss  0.0003755103680305183\n",
            "Epoch  35 Batch  110 / 525  Training Loss  0.0002744342782534659\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  35 Batch  111 / 525  Training Loss  0.0006437584524974227\n",
            "Epoch  35 Batch  112 / 525  Training Loss  0.0015366150764748454\n",
            "Epoch  35 Batch  113 / 525  Training Loss  0.0008402749663218856\n",
            "Epoch  35 Batch  114 / 525  Training Loss  0.0005271792178973556\n",
            "Epoch  35 Batch  115 / 525  Training Loss  0.0007244516164064407\n",
            "Epoch  35 Batch  116 / 525  Training Loss  0.0054188990034163\n",
            "Epoch  35 Batch  117 / 525  Training Loss  0.0003631944127846509\n",
            "Epoch  35 Batch  118 / 525  Training Loss  0.001810844405554235\n",
            "Epoch  35 Batch  119 / 525  Training Loss  0.000455797475297004\n",
            "Epoch  35 Batch  120 / 525  Training Loss  0.0003520519530866295\n",
            "Epoch  35 Batch  121 / 525  Training Loss  0.0005912488559260964\n",
            "Epoch  35 Batch  122 / 525  Training Loss  0.0013776150299236178\n",
            "Epoch  35 Batch  123 / 525  Training Loss  0.001549186185002327\n",
            "Epoch  35 Batch  124 / 525  Training Loss  0.003166858572512865\n",
            "Epoch  35 Batch  125 / 525  Training Loss  0.002508441684767604\n",
            "Epoch  35 Batch  126 / 525  Training Loss  0.0011339321499690413\n",
            "Epoch  35 Batch  127 / 525  Training Loss  0.0005668813246302307\n",
            "Epoch  35 Batch  128 / 525  Training Loss  0.0011814532335847616\n",
            "Epoch  35 Batch  129 / 525  Training Loss  0.0030471892096102238\n",
            "Epoch  35 Batch  130 / 525  Training Loss  0.004726056009531021\n",
            "Epoch  35 Batch  131 / 525  Training Loss  0.0008038411615416408\n",
            "Epoch  35 Batch  132 / 525  Training Loss  0.002271647099405527\n",
            "Epoch  35 Batch  133 / 525  Training Loss  0.010520080104470253\n",
            "Epoch  35 Batch  134 / 525  Training Loss  0.0014401363441720605\n",
            "Epoch  35 Batch  135 / 525  Training Loss  0.004495235159993172\n",
            "Epoch  35 Batch  136 / 525  Training Loss  0.0030229606200009584\n",
            "Epoch  35 Batch  137 / 525  Training Loss  0.003023796249181032\n",
            "Epoch  35 Batch  138 / 525  Training Loss  0.00036635101423598826\n",
            "Epoch  35 Batch  139 / 525  Training Loss  0.0006717673968523741\n",
            "Epoch  35 Batch  140 / 525  Training Loss  0.0010841073235496879\n",
            "Epoch  35 Batch  141 / 525  Training Loss  0.0016454595606774092\n",
            "Epoch  35 Batch  142 / 525  Training Loss  0.001376452622935176\n",
            "Epoch  35 Batch  143 / 525  Training Loss  0.0009389721672050655\n",
            "Epoch  35 Batch  144 / 525  Training Loss  0.00473914947360754\n",
            "Epoch  35 Batch  145 / 525  Training Loss  0.0005991578218527138\n",
            "Epoch  35 Batch  146 / 525  Training Loss  0.004230905324220657\n",
            "Epoch  35 Batch  147 / 525  Training Loss  0.0006619603955186903\n",
            "Epoch  35 Batch  148 / 525  Training Loss  0.011856968514621258\n",
            "Epoch  35 Batch  149 / 525  Training Loss  0.0018300588708370924\n",
            "Epoch  35 Batch  150 / 525  Training Loss  0.002368495799601078\n",
            "Epoch  35 Batch  151 / 525  Training Loss  0.001263283658772707\n",
            "Epoch  35 Batch  152 / 525  Training Loss  0.0007510253926739097\n",
            "Epoch  35 Batch  153 / 525  Training Loss  0.002662623068317771\n",
            "Epoch  35 Batch  154 / 525  Training Loss  0.0009513773838989437\n",
            "Epoch  35 Batch  155 / 525  Training Loss  0.002888006391003728\n",
            "Epoch  35 Batch  156 / 525  Training Loss  0.001328888931311667\n",
            "Epoch  35 Batch  157 / 525  Training Loss  0.006791715510189533\n",
            "Epoch  35 Batch  158 / 525  Training Loss  0.0018460728460922837\n",
            "Epoch  35 Batch  159 / 525  Training Loss  0.0010321511654183269\n",
            "Epoch  35 Batch  160 / 525  Training Loss  0.0010098310885950923\n",
            "Epoch  35 Batch  161 / 525  Training Loss  0.008658010512590408\n",
            "Epoch  35 Batch  162 / 525  Training Loss  0.0004120338708162308\n",
            "Epoch  35 Batch  163 / 525  Training Loss  0.0012743640691041946\n",
            "Epoch  35 Batch  164 / 525  Training Loss  0.0015262158121913671\n",
            "Epoch  35 Batch  165 / 525  Training Loss  0.0005240220343694091\n",
            "Epoch  35 Batch  166 / 525  Training Loss  0.002939735073596239\n",
            "Epoch  35 Batch  167 / 525  Training Loss  0.002591992961242795\n",
            "Epoch  35 Batch  168 / 525  Training Loss  0.001967631746083498\n",
            "Epoch  35 Batch  169 / 525  Training Loss  0.001375408610329032\n",
            "Epoch  35 Batch  170 / 525  Training Loss  0.000567358045373112\n",
            "Epoch  35 Batch  171 / 525  Training Loss  0.002883055480197072\n",
            "Epoch  35 Batch  172 / 525  Training Loss  0.0015102671459317207\n",
            "Epoch  35 Batch  173 / 525  Training Loss  0.0012881706934422255\n",
            "Epoch  35 Batch  174 / 525  Training Loss  0.004687243141233921\n",
            "Epoch  35 Batch  175 / 525  Training Loss  0.0013577614445239305\n",
            "Epoch  35 Batch  176 / 525  Training Loss  0.00116833602078259\n",
            "Epoch  35 Batch  177 / 525  Training Loss  0.0012292892206460238\n",
            "Epoch  35 Batch  178 / 525  Training Loss  0.006694766227155924\n",
            "Epoch  35 Batch  179 / 525  Training Loss  0.0020416234619915485\n",
            "Epoch  35 Batch  180 / 525  Training Loss  0.0006398552795872092\n",
            "Epoch  35 Batch  181 / 525  Training Loss  0.0017354385927319527\n",
            "Epoch  35 Batch  182 / 525  Training Loss  0.0013708100887015462\n",
            "Epoch  35 Batch  183 / 525  Training Loss  0.002986854175105691\n",
            "Epoch  35 Batch  184 / 525  Training Loss  0.0005358902853913605\n",
            "Epoch  35 Batch  185 / 525  Training Loss  0.0011480869725346565\n",
            "Epoch  35 Batch  186 / 525  Training Loss  0.002927829045802355\n",
            "Epoch  35 Batch  187 / 525  Training Loss  0.0003594730223994702\n",
            "Epoch  35 Batch  188 / 525  Training Loss  0.0064307330176234245\n",
            "Epoch  35 Batch  189 / 525  Training Loss  0.0020009621512144804\n",
            "Epoch  35 Batch  190 / 525  Training Loss  0.0031247674487531185\n",
            "Epoch  35 Batch  191 / 525  Training Loss  0.001811110763810575\n",
            "Epoch  35 Batch  192 / 525  Training Loss  0.005865288898348808\n",
            "Epoch  35 Batch  193 / 525  Training Loss  0.003663530107587576\n",
            "Epoch  35 Batch  194 / 525  Training Loss  0.0014107030583545566\n",
            "Epoch  35 Batch  195 / 525  Training Loss  0.0036768794525414705\n",
            "Epoch  35 Batch  196 / 525  Training Loss  0.004072478972375393\n",
            "Epoch  35 Batch  197 / 525  Training Loss  0.005288857966661453\n",
            "Epoch  35 Batch  198 / 525  Training Loss  0.0009521669708192348\n",
            "Epoch  35 Batch  199 / 525  Training Loss  0.0010489634005352855\n",
            "Epoch  35 Batch  200 / 525  Training Loss  0.003550038207322359\n",
            "Epoch  35 Batch  201 / 525  Training Loss  0.0024019728880375624\n",
            "Epoch  35 Batch  202 / 525  Training Loss  0.0007435508887283504\n",
            "Epoch  35 Batch  203 / 525  Training Loss  0.0009094035485759377\n",
            "Epoch  35 Batch  204 / 525  Training Loss  0.001957783242687583\n",
            "Epoch  35 Batch  205 / 525  Training Loss  0.0007381688919849694\n",
            "Epoch  35 Batch  206 / 525  Training Loss  0.0007090208237059414\n",
            "Epoch  35 Batch  207 / 525  Training Loss  0.0008934408542700112\n",
            "Epoch  35 Batch  208 / 525  Training Loss  0.0031716604717075825\n",
            "Epoch  35 Batch  209 / 525  Training Loss  0.00042647370719350874\n",
            "Epoch  35 Batch  210 / 525  Training Loss  0.0014904328854754567\n",
            "Epoch  35 Batch  211 / 525  Training Loss  0.0016695462400093675\n",
            "Epoch  35 Batch  212 / 525  Training Loss  0.0002992776280734688\n",
            "Epoch  35 Batch  213 / 525  Training Loss  0.008137085475027561\n",
            "Epoch  35 Batch  214 / 525  Training Loss  0.0021454067900776863\n",
            "Epoch  35 Batch  215 / 525  Training Loss  0.003006983082741499\n",
            "Epoch  35 Batch  216 / 525  Training Loss  0.0006030424265190959\n",
            "Epoch  35 Batch  217 / 525  Training Loss  0.0014134534867480397\n",
            "Epoch  35 Batch  218 / 525  Training Loss  0.0007734574610367417\n",
            "Epoch  35 Batch  219 / 525  Training Loss  0.0010504578240215778\n",
            "Epoch  35 Batch  220 / 525  Training Loss  0.0034995414316654205\n",
            "Epoch  35 Batch  221 / 525  Training Loss  0.0038272500969469547\n",
            "Epoch  35 Batch  222 / 525  Training Loss  0.001546336687169969\n",
            "Epoch  35 Batch  223 / 525  Training Loss  0.0005229883827269077\n",
            "Epoch  35 Batch  224 / 525  Training Loss  0.009256089106202126\n",
            "Epoch  35 Batch  225 / 525  Training Loss  0.010092485696077347\n",
            "Epoch  35 Batch  226 / 525  Training Loss  0.0010398253798484802\n",
            "Epoch  35 Batch  227 / 525  Training Loss  0.0028636630158871412\n",
            "Epoch  35 Batch  228 / 525  Training Loss  0.0005891037872061133\n",
            "Epoch  35 Batch  229 / 525  Training Loss  0.0022250907495617867\n",
            "Epoch  35 Batch  230 / 525  Training Loss  0.0004160537791904062\n",
            "Epoch  35 Batch  231 / 525  Training Loss  0.0016202160622924566\n",
            "Epoch  35 Batch  232 / 525  Training Loss  0.000834202510304749\n",
            "Epoch  35 Batch  233 / 525  Training Loss  0.0007410148391500115\n",
            "Epoch  35 Batch  234 / 525  Training Loss  0.0010357358260080218\n",
            "Epoch  35 Batch  235 / 525  Training Loss  0.0006303365807980299\n",
            "Epoch  35 Batch  236 / 525  Training Loss  0.00665008369833231\n",
            "Epoch  35 Batch  237 / 525  Training Loss  0.013861307874321938\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  35 Batch  238 / 525  Training Loss  0.0007416970329359174\n",
            "Epoch  35 Batch  239 / 525  Training Loss  0.0016010316321626306\n",
            "Epoch  35 Batch  240 / 525  Training Loss  0.0019940410275012255\n",
            "Epoch  35 Batch  241 / 525  Training Loss  0.0038338482845574617\n",
            "Epoch  35 Batch  242 / 525  Training Loss  0.001652421778999269\n",
            "Epoch  35 Batch  243 / 525  Training Loss  0.0038887106347829103\n",
            "Epoch  35 Batch  244 / 525  Training Loss  0.002907808404415846\n",
            "Epoch  35 Batch  245 / 525  Training Loss  0.0024366911966353655\n",
            "Epoch  35 Batch  246 / 525  Training Loss  0.0005562691367231309\n",
            "Epoch  35 Batch  247 / 525  Training Loss  0.0006307982839643955\n",
            "Epoch  35 Batch  248 / 525  Training Loss  0.0013738017296418548\n",
            "Epoch  35 Batch  249 / 525  Training Loss  0.000522290647495538\n",
            "Epoch  35 Batch  250 / 525  Training Loss  0.0008340071653947234\n",
            "Epoch  35 Batch  251 / 525  Training Loss  0.001277437200769782\n",
            "Epoch  35 Batch  252 / 525  Training Loss  0.0005288383108563721\n",
            "Epoch  35 Batch  253 / 525  Training Loss  0.0008804333629086614\n",
            "Epoch  35 Batch  254 / 525  Training Loss  0.0004793511179741472\n",
            "Epoch  35 Batch  255 / 525  Training Loss  0.00089307373855263\n",
            "Epoch  35 Batch  256 / 525  Training Loss  0.003262807847931981\n",
            "Epoch  35 Batch  257 / 525  Training Loss  0.0006168283289298415\n",
            "Epoch  35 Batch  258 / 525  Training Loss  0.0018821072299033403\n",
            "Epoch  35 Batch  259 / 525  Training Loss  0.00022890479885973036\n",
            "Epoch  35 Batch  260 / 525  Training Loss  0.001148077892139554\n",
            "Epoch  35 Batch  261 / 525  Training Loss  0.0005261918413452804\n",
            "Epoch  35 Batch  262 / 525  Training Loss  0.000938534562010318\n",
            "Epoch  35 Batch  263 / 525  Training Loss  0.0002862365508917719\n",
            "Epoch  35 Batch  264 / 525  Training Loss  0.0008640663581900299\n",
            "Epoch  35 Batch  265 / 525  Training Loss  0.00046712360926903784\n",
            "Epoch  35 Batch  266 / 525  Training Loss  0.0008620443986728787\n",
            "Epoch  35 Batch  267 / 525  Training Loss  0.0006414477829821408\n",
            "Epoch  35 Batch  268 / 525  Training Loss  0.0020109389442950487\n",
            "Epoch  35 Batch  269 / 525  Training Loss  0.02450922690331936\n",
            "Epoch  35 Batch  270 / 525  Training Loss  0.0017061980906873941\n",
            "Epoch  35 Batch  271 / 525  Training Loss  0.0009793941862881184\n",
            "Epoch  35 Batch  272 / 525  Training Loss  0.0020260028541088104\n",
            "Epoch  35 Batch  273 / 525  Training Loss  0.0010790769010782242\n",
            "Epoch  35 Batch  274 / 525  Training Loss  0.0035576943773776293\n",
            "Epoch  35 Batch  275 / 525  Training Loss  0.0015643719816580415\n",
            "Epoch  35 Batch  276 / 525  Training Loss  0.00170031376183033\n",
            "Epoch  35 Batch  277 / 525  Training Loss  0.001993572572246194\n",
            "Epoch  35 Batch  278 / 525  Training Loss  0.0017238643486052752\n",
            "Epoch  35 Batch  279 / 525  Training Loss  0.0009263508254662156\n",
            "Epoch  35 Batch  280 / 525  Training Loss  0.0006267355638556182\n",
            "Epoch  35 Batch  281 / 525  Training Loss  0.0009110405808314681\n",
            "Epoch  35 Batch  282 / 525  Training Loss  0.0013602969702333212\n",
            "Epoch  35 Batch  283 / 525  Training Loss  0.0033465928863734007\n",
            "Epoch  35 Batch  284 / 525  Training Loss  0.0035805378574877977\n",
            "Epoch  35 Batch  285 / 525  Training Loss  0.0035246096085757017\n",
            "Epoch  35 Batch  286 / 525  Training Loss  0.001531245419755578\n",
            "Epoch  35 Batch  287 / 525  Training Loss  0.0004948924179188907\n",
            "Epoch  35 Batch  288 / 525  Training Loss  0.00014642080350313336\n",
            "Epoch  35 Batch  289 / 525  Training Loss  0.001346927834674716\n",
            "Epoch  35 Batch  290 / 525  Training Loss  0.0009623856167308986\n",
            "Epoch  35 Batch  291 / 525  Training Loss  0.007509605027735233\n",
            "Epoch  35 Batch  292 / 525  Training Loss  0.000523621216416359\n",
            "Epoch  35 Batch  293 / 525  Training Loss  0.0025707955937832594\n",
            "Epoch  35 Batch  294 / 525  Training Loss  0.0006281774258241057\n",
            "Epoch  35 Batch  295 / 525  Training Loss  0.001318798866122961\n",
            "Epoch  35 Batch  296 / 525  Training Loss  0.0008844409021548927\n",
            "Epoch  35 Batch  297 / 525  Training Loss  0.0009733490878716111\n",
            "Epoch  35 Batch  298 / 525  Training Loss  0.0019234189530834556\n",
            "Epoch  35 Batch  299 / 525  Training Loss  0.0005106687312945724\n",
            "Epoch  35 Batch  300 / 525  Training Loss  0.00042897480307146907\n",
            "Epoch  35 Batch  301 / 525  Training Loss  0.003862664569169283\n",
            "Epoch  35 Batch  302 / 525  Training Loss  0.0022527400869876146\n",
            "Epoch  35 Batch  303 / 525  Training Loss  0.006052365060895681\n",
            "Epoch  35 Batch  304 / 525  Training Loss  0.0010561192175373435\n",
            "Epoch  35 Batch  305 / 525  Training Loss  0.001461212756112218\n",
            "Epoch  35 Batch  306 / 525  Training Loss  0.0004178434028290212\n",
            "Epoch  35 Batch  307 / 525  Training Loss  0.003718253690749407\n",
            "Epoch  35 Batch  308 / 525  Training Loss  0.00031438039150089025\n",
            "Epoch  35 Batch  309 / 525  Training Loss  0.0009032247471623123\n",
            "Epoch  35 Batch  310 / 525  Training Loss  0.00016437112935818732\n",
            "Epoch  35 Batch  311 / 525  Training Loss  0.0008907282608561218\n",
            "Epoch  35 Batch  312 / 525  Training Loss  0.0019846982322633266\n",
            "Epoch  35 Batch  313 / 525  Training Loss  0.0045908293686807156\n",
            "Epoch  35 Batch  314 / 525  Training Loss  0.00072507094591856\n",
            "Epoch  35 Batch  315 / 525  Training Loss  0.0008888834272511303\n",
            "Epoch  35 Batch  316 / 525  Training Loss  0.0006509132799692452\n",
            "Epoch  35 Batch  317 / 525  Training Loss  0.001864784979261458\n",
            "Epoch  35 Batch  318 / 525  Training Loss  0.0007485937094315886\n",
            "Epoch  35 Batch  319 / 525  Training Loss  0.00025865560746751726\n",
            "Epoch  35 Batch  320 / 525  Training Loss  0.0021849381737411022\n",
            "Epoch  35 Batch  321 / 525  Training Loss  0.00032498338259756565\n",
            "Epoch  35 Batch  322 / 525  Training Loss  0.0007686764583922923\n",
            "Epoch  35 Batch  323 / 525  Training Loss  0.001224953681230545\n",
            "Epoch  35 Batch  324 / 525  Training Loss  0.000300510844681412\n",
            "Epoch  35 Batch  325 / 525  Training Loss  0.0004987879656255245\n",
            "Epoch  35 Batch  326 / 525  Training Loss  0.0005133373197168112\n",
            "Epoch  35 Batch  327 / 525  Training Loss  0.007372853811830282\n",
            "Epoch  35 Batch  328 / 525  Training Loss  0.0008384024840779603\n",
            "Epoch  35 Batch  329 / 525  Training Loss  0.0006123482598923147\n",
            "Epoch  35 Batch  330 / 525  Training Loss  0.0004544003459159285\n",
            "Epoch  35 Batch  331 / 525  Training Loss  0.00811772234737873\n",
            "Epoch  35 Batch  332 / 525  Training Loss  0.0009806964080780745\n",
            "Epoch  35 Batch  333 / 525  Training Loss  0.01021427009254694\n",
            "Epoch  35 Batch  334 / 525  Training Loss  0.0003764622670132667\n",
            "Epoch  35 Batch  335 / 525  Training Loss  0.002084441250190139\n",
            "Epoch  35 Batch  336 / 525  Training Loss  0.01383692305535078\n",
            "Epoch  35 Batch  337 / 525  Training Loss  0.0008188815554603934\n",
            "Epoch  35 Batch  338 / 525  Training Loss  0.0006729477318003774\n",
            "Epoch  35 Batch  339 / 525  Training Loss  0.0024368700105696917\n",
            "Epoch  35 Batch  340 / 525  Training Loss  0.0019380496814846992\n",
            "Epoch  35 Batch  341 / 525  Training Loss  0.0006329755997285247\n",
            "Epoch  35 Batch  342 / 525  Training Loss  0.0005798607598990202\n",
            "Epoch  35 Batch  343 / 525  Training Loss  0.0010999421356245875\n",
            "Epoch  35 Batch  344 / 525  Training Loss  0.002612002659589052\n",
            "Epoch  35 Batch  345 / 525  Training Loss  0.00119660678319633\n",
            "Epoch  35 Batch  346 / 525  Training Loss  0.0012256273766979575\n",
            "Epoch  35 Batch  347 / 525  Training Loss  0.003526215208694339\n",
            "Epoch  35 Batch  348 / 525  Training Loss  0.0035341144539415836\n",
            "Epoch  35 Batch  349 / 525  Training Loss  0.0002549158234614879\n",
            "Epoch  35 Batch  350 / 525  Training Loss  0.0007272710790857673\n",
            "Epoch  35 Batch  351 / 525  Training Loss  0.006213695742189884\n",
            "Epoch  35 Batch  352 / 525  Training Loss  0.0006541950860992074\n",
            "Epoch  35 Batch  353 / 525  Training Loss  0.0007470851996913552\n",
            "Epoch  35 Batch  354 / 525  Training Loss  0.0013177578803151846\n",
            "Epoch  35 Batch  355 / 525  Training Loss  0.00020324306387919933\n",
            "Epoch  35 Batch  356 / 525  Training Loss  0.0027928254567086697\n",
            "Epoch  35 Batch  357 / 525  Training Loss  0.0015989260282367468\n",
            "Epoch  35 Batch  358 / 525  Training Loss  0.008846918120980263\n",
            "Epoch  35 Batch  359 / 525  Training Loss  0.0016386725474148989\n",
            "Epoch  35 Batch  360 / 525  Training Loss  0.0005020821699872613\n",
            "Epoch  35 Batch  361 / 525  Training Loss  0.0006287608994171023\n",
            "Epoch  35 Batch  362 / 525  Training Loss  0.0006936862482689321\n",
            "Epoch  35 Batch  363 / 525  Training Loss  0.002504707081243396\n",
            "Epoch  35 Batch  364 / 525  Training Loss  0.0007350911619141698\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  35 Batch  365 / 525  Training Loss  0.0007456714520230889\n",
            "Epoch  35 Batch  366 / 525  Training Loss  0.006868890020996332\n",
            "Epoch  35 Batch  367 / 525  Training Loss  0.0009182345238514245\n",
            "Epoch  35 Batch  368 / 525  Training Loss  0.004560752771794796\n",
            "Epoch  35 Batch  369 / 525  Training Loss  0.0037760387640446424\n",
            "Epoch  35 Batch  370 / 525  Training Loss  0.0002444020356051624\n",
            "Epoch  35 Batch  371 / 525  Training Loss  0.00249123340472579\n",
            "Epoch  35 Batch  372 / 525  Training Loss  0.0020238091237843037\n",
            "Epoch  35 Batch  373 / 525  Training Loss  0.0023459389340132475\n",
            "Epoch  35 Batch  374 / 525  Training Loss  0.0017366006504744291\n",
            "Epoch  35 Batch  375 / 525  Training Loss  0.0010634181089699268\n",
            "Epoch  35 Batch  376 / 525  Training Loss  0.0041475677862763405\n",
            "Epoch  35 Batch  377 / 525  Training Loss  0.00040369777707383037\n",
            "Epoch  35 Batch  378 / 525  Training Loss  0.0006085322820581496\n",
            "Epoch  35 Batch  379 / 525  Training Loss  0.0004899821360595524\n",
            "Epoch  35 Batch  380 / 525  Training Loss  0.007879691198468208\n",
            "Epoch  35 Batch  381 / 525  Training Loss  0.0011137512046843767\n",
            "Epoch  35 Batch  382 / 525  Training Loss  0.0035822573117911816\n",
            "Epoch  35 Batch  383 / 525  Training Loss  0.0015551478136330843\n",
            "Epoch  35 Batch  384 / 525  Training Loss  0.0017361690988764167\n",
            "Epoch  35 Batch  385 / 525  Training Loss  0.002001146785914898\n",
            "Epoch  35 Batch  386 / 525  Training Loss  0.0008647231152281165\n",
            "Epoch  35 Batch  387 / 525  Training Loss  0.0005398789653554559\n",
            "Epoch  35 Batch  388 / 525  Training Loss  0.004196410067379475\n",
            "Epoch  35 Batch  389 / 525  Training Loss  0.0016026828670874238\n",
            "Epoch  35 Batch  390 / 525  Training Loss  0.0002068289031740278\n",
            "Epoch  35 Batch  391 / 525  Training Loss  0.0009179824846796691\n",
            "Epoch  35 Batch  392 / 525  Training Loss  0.001210458343848586\n",
            "Epoch  35 Batch  393 / 525  Training Loss  0.0013855015859007835\n",
            "Epoch  35 Batch  394 / 525  Training Loss  0.0024662730284035206\n",
            "Epoch  35 Batch  395 / 525  Training Loss  0.001180256949737668\n",
            "Epoch  35 Batch  396 / 525  Training Loss  0.0013181406538933516\n",
            "Epoch  35 Batch  397 / 525  Training Loss  0.0003976127482019365\n",
            "Epoch  35 Batch  398 / 525  Training Loss  0.009455852210521698\n",
            "Epoch  35 Batch  399 / 525  Training Loss  0.0011858618818223476\n",
            "Epoch  35 Batch  400 / 525  Training Loss  0.0016377909341827035\n",
            "Epoch  35 Batch  401 / 525  Training Loss  0.00359754404053092\n",
            "Epoch  35 Batch  402 / 525  Training Loss  0.0022532481234520674\n",
            "Epoch  35 Batch  403 / 525  Training Loss  0.004119270481169224\n",
            "Epoch  35 Batch  404 / 525  Training Loss  0.0037065153010189533\n",
            "Epoch  35 Batch  405 / 525  Training Loss  0.0007493292214348912\n",
            "Epoch  35 Batch  406 / 525  Training Loss  0.0014789020642638206\n",
            "Epoch  35 Batch  407 / 525  Training Loss  0.0006810774211771786\n",
            "Epoch  35 Batch  408 / 525  Training Loss  0.0013442656490951777\n",
            "Epoch  35 Batch  409 / 525  Training Loss  0.0021044837776571512\n",
            "Epoch  35 Batch  410 / 525  Training Loss  0.0011228511575609446\n",
            "Epoch  35 Batch  411 / 525  Training Loss  0.0016786509659141302\n",
            "Epoch  35 Batch  412 / 525  Training Loss  0.005751012824475765\n",
            "Epoch  35 Batch  413 / 525  Training Loss  0.001258681295439601\n",
            "Epoch  35 Batch  414 / 525  Training Loss  0.0008298593456856906\n",
            "Epoch  35 Batch  415 / 525  Training Loss  0.00341666117310524\n",
            "Epoch  35 Batch  416 / 525  Training Loss  0.002151529537513852\n",
            "Epoch  35 Batch  417 / 525  Training Loss  0.0053393119014799595\n",
            "Epoch  35 Batch  418 / 525  Training Loss  0.0015073928516358137\n",
            "Epoch  35 Batch  419 / 525  Training Loss  0.0018924775067716837\n",
            "Epoch  35 Batch  420 / 525  Training Loss  0.0027214279398322105\n",
            "Epoch  35 Batch  421 / 525  Training Loss  0.0005543400766327977\n",
            "Epoch  35 Batch  422 / 525  Training Loss  0.005431517027318478\n",
            "Epoch  35 Batch  423 / 525  Training Loss  0.0017407163977622986\n",
            "Epoch  35 Batch  424 / 525  Training Loss  0.0008510635234415531\n",
            "Epoch  35 Batch  425 / 525  Training Loss  0.006576002575457096\n",
            "Epoch  35 Batch  426 / 525  Training Loss  0.005532960407435894\n",
            "Epoch  35 Batch  427 / 525  Training Loss  0.016049610450863838\n",
            "Epoch  35 Batch  428 / 525  Training Loss  0.005074264016002417\n",
            "Epoch  35 Batch  429 / 525  Training Loss  0.0090288445353508\n",
            "Epoch  35 Batch  430 / 525  Training Loss  0.0007283054292201996\n",
            "Epoch  35 Batch  431 / 525  Training Loss  0.0020144653972238302\n",
            "Epoch  35 Batch  432 / 525  Training Loss  0.004832965787500143\n",
            "Epoch  35 Batch  433 / 525  Training Loss  0.000899092759937048\n",
            "Epoch  35 Batch  434 / 525  Training Loss  0.0010504076490178704\n",
            "Epoch  35 Batch  435 / 525  Training Loss  0.0016054513398557901\n",
            "Epoch  35 Batch  436 / 525  Training Loss  0.001969745149835944\n",
            "Epoch  35 Batch  437 / 525  Training Loss  0.0028418656438589096\n",
            "Epoch  35 Batch  438 / 525  Training Loss  0.0005394231411628425\n",
            "Epoch  35 Batch  439 / 525  Training Loss  0.0007855609874241054\n",
            "Epoch  35 Batch  440 / 525  Training Loss  0.0046638199128210545\n",
            "Epoch  35 Batch  441 / 525  Training Loss  0.0018872007494792342\n",
            "Epoch  35 Batch  442 / 525  Training Loss  0.0004979117074981332\n",
            "Epoch  35 Batch  443 / 525  Training Loss  0.0026772445999085903\n",
            "Epoch  35 Batch  444 / 525  Training Loss  0.0018878879491239786\n",
            "Epoch  35 Batch  445 / 525  Training Loss  0.003285516519099474\n",
            "Epoch  35 Batch  446 / 525  Training Loss  0.0012132732663303614\n",
            "Epoch  35 Batch  447 / 525  Training Loss  0.0002454756177030504\n",
            "Epoch  35 Batch  448 / 525  Training Loss  0.0016208095476031303\n",
            "Epoch  35 Batch  449 / 525  Training Loss  0.0007154659833759069\n",
            "Epoch  35 Batch  450 / 525  Training Loss  0.0008738236501812935\n",
            "Epoch  35 Batch  451 / 525  Training Loss  0.002175932517275214\n",
            "Epoch  35 Batch  452 / 525  Training Loss  0.0010210677282884717\n",
            "Epoch  35 Batch  453 / 525  Training Loss  0.013034649193286896\n",
            "Epoch  35 Batch  454 / 525  Training Loss  0.000501898757647723\n",
            "Epoch  35 Batch  455 / 525  Training Loss  0.001639721216633916\n",
            "Epoch  35 Batch  456 / 525  Training Loss  0.0007551410235464573\n",
            "Epoch  35 Batch  457 / 525  Training Loss  0.0013292370131239295\n",
            "Epoch  35 Batch  458 / 525  Training Loss  0.001970760291442275\n",
            "Epoch  35 Batch  459 / 525  Training Loss  0.0005406350246630609\n",
            "Epoch  35 Batch  460 / 525  Training Loss  0.0007035640301182866\n",
            "Epoch  35 Batch  461 / 525  Training Loss  0.00017176978872157633\n",
            "Epoch  35 Batch  462 / 525  Training Loss  0.001044881297275424\n",
            "Epoch  35 Batch  463 / 525  Training Loss  0.0006896210252307355\n",
            "Epoch  35 Batch  464 / 525  Training Loss  0.001003478653728962\n",
            "Epoch  35 Batch  465 / 525  Training Loss  0.0012798365205526352\n",
            "Epoch  35 Batch  466 / 525  Training Loss  0.00016176895587705076\n",
            "Epoch  35 Batch  467 / 525  Training Loss  0.0022071662824600935\n",
            "Epoch  35 Batch  468 / 525  Training Loss  0.0007690946222282946\n",
            "Epoch  35 Batch  469 / 525  Training Loss  0.0004074352909810841\n",
            "Epoch  35 Batch  470 / 525  Training Loss  0.003631058381870389\n",
            "Epoch  35 Batch  471 / 525  Training Loss  0.002334770979359746\n",
            "Epoch  35 Batch  472 / 525  Training Loss  0.00035007920814678073\n",
            "Epoch  35 Batch  473 / 525  Training Loss  0.010656412690877914\n",
            "Epoch  35 Batch  474 / 525  Training Loss  0.0038463198579847813\n",
            "Epoch  35 Batch  475 / 525  Training Loss  0.002586220158264041\n",
            "Epoch  35 Batch  476 / 525  Training Loss  0.000693300913553685\n",
            "Epoch  35 Batch  477 / 525  Training Loss  0.0029749490786343813\n",
            "Epoch  35 Batch  478 / 525  Training Loss  0.002172242384403944\n",
            "Epoch  35 Batch  479 / 525  Training Loss  0.00960756279528141\n",
            "Epoch  35 Batch  480 / 525  Training Loss  0.001739865867421031\n",
            "Epoch  35 Batch  481 / 525  Training Loss  0.010629390366375446\n",
            "Epoch  35 Batch  482 / 525  Training Loss  0.000642803031951189\n",
            "Epoch  35 Batch  483 / 525  Training Loss  0.0022330856882035732\n",
            "Epoch  35 Batch  484 / 525  Training Loss  0.0002815934712998569\n",
            "Epoch  35 Batch  485 / 525  Training Loss  0.0035040187649428844\n",
            "Epoch  35 Batch  486 / 525  Training Loss  0.0021236776374280453\n",
            "Epoch  35 Batch  487 / 525  Training Loss  0.0014779793564230204\n",
            "Epoch  35 Batch  488 / 525  Training Loss  0.013490134850144386\n",
            "Epoch  35 Batch  489 / 525  Training Loss  0.0023045851849019527\n",
            "Epoch  35 Batch  490 / 525  Training Loss  0.01666964963078499\n",
            "Epoch  35 Batch  491 / 525  Training Loss  0.0009994367137551308\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  35 Batch  492 / 525  Training Loss  0.0022018563468009233\n",
            "Epoch  35 Batch  493 / 525  Training Loss  0.004743911325931549\n",
            "Epoch  35 Batch  494 / 525  Training Loss  0.004807106219232082\n",
            "Epoch  35 Batch  495 / 525  Training Loss  0.0012051466619595885\n",
            "Epoch  35 Batch  496 / 525  Training Loss  0.002539581386372447\n",
            "Epoch  35 Batch  497 / 525  Training Loss  0.004891025833785534\n",
            "Epoch  35 Batch  498 / 525  Training Loss  0.0023143943399190903\n",
            "Epoch  35 Batch  499 / 525  Training Loss  0.0045270537957549095\n",
            "Epoch  35 Batch  500 / 525  Training Loss  0.002869480988010764\n",
            "Epoch  35 Batch  501 / 525  Training Loss  0.003203463274985552\n",
            "Epoch  35 Batch  502 / 525  Training Loss  0.0012729933951050043\n",
            "Epoch  35 Batch  503 / 525  Training Loss  0.0030593941919505596\n",
            "Epoch  35 Batch  504 / 525  Training Loss  0.0006106295622885227\n",
            "Epoch  35 Batch  505 / 525  Training Loss  0.0016262440476566553\n",
            "Epoch  35 Batch  506 / 525  Training Loss  0.0028160030487924814\n",
            "Epoch  35 Batch  507 / 525  Training Loss  0.00447330204769969\n",
            "Epoch  35 Batch  508 / 525  Training Loss  0.0002203293115599081\n",
            "Epoch  35 Batch  509 / 525  Training Loss  0.003192241070792079\n",
            "Epoch  35 Batch  510 / 525  Training Loss  0.004330256953835487\n",
            "Epoch  35 Batch  511 / 525  Training Loss  0.001419082866050303\n",
            "Epoch  35 Batch  512 / 525  Training Loss  0.0037108459509909153\n",
            "Epoch  35 Batch  513 / 525  Training Loss  0.0061387126334011555\n",
            "Epoch  35 Batch  514 / 525  Training Loss  0.00020275588030926883\n",
            "Epoch  35 Batch  515 / 525  Training Loss  0.003183392807841301\n",
            "Epoch  35 Batch  516 / 525  Training Loss  0.0038838640321046114\n",
            "Epoch  35 Batch  517 / 525  Training Loss  0.001984588336199522\n",
            "Epoch  35 Batch  518 / 525  Training Loss  0.0025112535804510117\n",
            "Epoch  35 Batch  519 / 525  Training Loss  0.002057950710877776\n",
            "Epoch  35 Batch  520 / 525  Training Loss  0.007339212112128735\n",
            "Epoch  35 Batch  521 / 525  Training Loss  0.0006196149624884129\n",
            "Epoch  35 Batch  522 / 525  Training Loss  0.012763408944010735\n",
            "Epoch  35 Batch  523 / 525  Training Loss  0.0016131665324792266\n",
            "Epoch  35 Batch  524 / 525  Training Loss  0.003342155832797289\n",
            "  36    |    -    |   0.002609   | 60.425000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 36\n",
            "Epoch  36 Batch  0 / 525  Training Loss  0.0004377936420496553\n",
            "Epoch  36 Batch  1 / 525  Training Loss  0.0017106275772675872\n",
            "Epoch  36 Batch  2 / 525  Training Loss  0.0010004055220633745\n",
            "Epoch  36 Batch  3 / 525  Training Loss  0.0016776530537754297\n",
            "Epoch  36 Batch  4 / 525  Training Loss  0.00040441262535750866\n",
            "Epoch  36 Batch  5 / 525  Training Loss  0.0007039564079605043\n",
            "Epoch  36 Batch  6 / 525  Training Loss  0.0037363250739872456\n",
            "Epoch  36 Batch  7 / 525  Training Loss  0.0016911306884139776\n",
            "Epoch  36 Batch  8 / 525  Training Loss  0.0017043404513970017\n",
            "Epoch  36 Batch  9 / 525  Training Loss  0.0022924761287868023\n",
            "Epoch  36 Batch  10 / 525  Training Loss  0.007624458521604538\n",
            "Epoch  36 Batch  11 / 525  Training Loss  0.002175596309825778\n",
            "Epoch  36 Batch  12 / 525  Training Loss  0.0071769035421311855\n",
            "Epoch  36 Batch  13 / 525  Training Loss  0.0008054676582105458\n",
            "Epoch  36 Batch  14 / 525  Training Loss  0.0002958906115964055\n",
            "Epoch  36 Batch  15 / 525  Training Loss  0.00788346491754055\n",
            "Epoch  36 Batch  16 / 525  Training Loss  0.003660361748188734\n",
            "Epoch  36 Batch  17 / 525  Training Loss  0.0019878102466464043\n",
            "Epoch  36 Batch  18 / 525  Training Loss  0.0014805726241320372\n",
            "Epoch  36 Batch  19 / 525  Training Loss  0.0022900374606251717\n",
            "Epoch  36 Batch  20 / 525  Training Loss  0.0002777688787318766\n",
            "Epoch  36 Batch  21 / 525  Training Loss  0.0005492570926435292\n",
            "Epoch  36 Batch  22 / 525  Training Loss  0.001009431784041226\n",
            "Epoch  36 Batch  23 / 525  Training Loss  0.0002981502912007272\n",
            "Epoch  36 Batch  24 / 525  Training Loss  0.0019879196770489216\n",
            "Epoch  36 Batch  25 / 525  Training Loss  0.0012261125957593322\n",
            "Epoch  36 Batch  26 / 525  Training Loss  0.0010599130764603615\n",
            "Epoch  36 Batch  27 / 525  Training Loss  0.0011148314224556088\n",
            "Epoch  36 Batch  28 / 525  Training Loss  0.0007758697611279786\n",
            "Epoch  36 Batch  29 / 525  Training Loss  0.0004714992828667164\n",
            "Epoch  36 Batch  30 / 525  Training Loss  0.0006548286182805896\n",
            "Epoch  36 Batch  31 / 525  Training Loss  0.0006798350950703025\n",
            "Epoch  36 Batch  32 / 525  Training Loss  0.0027487732004374266\n",
            "Epoch  36 Batch  33 / 525  Training Loss  0.009335766546428204\n",
            "Epoch  36 Batch  34 / 525  Training Loss  0.00015129944949876517\n",
            "Epoch  36 Batch  35 / 525  Training Loss  0.0010247115278616548\n",
            "Epoch  36 Batch  36 / 525  Training Loss  0.005476555787026882\n",
            "Epoch  36 Batch  37 / 525  Training Loss  0.001752248965203762\n",
            "Epoch  36 Batch  38 / 525  Training Loss  0.0004617578233592212\n",
            "Epoch  36 Batch  39 / 525  Training Loss  0.00032469743746332824\n",
            "Epoch  36 Batch  40 / 525  Training Loss  0.00015405690646730363\n",
            "Epoch  36 Batch  41 / 525  Training Loss  0.0012456255499273539\n",
            "Epoch  36 Batch  42 / 525  Training Loss  0.0019424697384238243\n",
            "Epoch  36 Batch  43 / 525  Training Loss  0.0014955729711800814\n",
            "Epoch  36 Batch  44 / 525  Training Loss  0.0007277562981471419\n",
            "Epoch  36 Batch  45 / 525  Training Loss  0.0007754809339530766\n",
            "Epoch  36 Batch  46 / 525  Training Loss  0.0020235555712133646\n",
            "Epoch  36 Batch  47 / 525  Training Loss  0.0011231036623939872\n",
            "Epoch  36 Batch  48 / 525  Training Loss  0.0013394575798884034\n",
            "Epoch  36 Batch  49 / 525  Training Loss  0.00024642242351546884\n",
            "Epoch  36 Batch  50 / 525  Training Loss  0.0009788714814931154\n",
            "Epoch  36 Batch  51 / 525  Training Loss  0.0006120713660493493\n",
            "Epoch  36 Batch  52 / 525  Training Loss  0.0004171727632638067\n",
            "Epoch  36 Batch  53 / 525  Training Loss  0.0002075892116408795\n",
            "Epoch  36 Batch  54 / 525  Training Loss  0.0029313506092876196\n",
            "Epoch  36 Batch  55 / 525  Training Loss  0.0006386186578311026\n",
            "Epoch  36 Batch  56 / 525  Training Loss  0.0005216286517679691\n",
            "Epoch  36 Batch  57 / 525  Training Loss  0.0002538793778512627\n",
            "Epoch  36 Batch  58 / 525  Training Loss  0.0010711031500250101\n",
            "Epoch  36 Batch  59 / 525  Training Loss  0.0005859631346538663\n",
            "Epoch  36 Batch  60 / 525  Training Loss  0.0004173176421318203\n",
            "Epoch  36 Batch  61 / 525  Training Loss  0.001490205293521285\n",
            "Epoch  36 Batch  62 / 525  Training Loss  0.00013607266009785235\n",
            "Epoch  36 Batch  63 / 525  Training Loss  0.0016228785971179605\n",
            "Epoch  36 Batch  64 / 525  Training Loss  0.00016852606495376676\n",
            "Epoch  36 Batch  65 / 525  Training Loss  0.0005109607591293752\n",
            "Epoch  36 Batch  66 / 525  Training Loss  0.002406161045655608\n",
            "Epoch  36 Batch  67 / 525  Training Loss  0.0006188052939251065\n",
            "Epoch  36 Batch  68 / 525  Training Loss  0.000994376139715314\n",
            "Epoch  36 Batch  69 / 525  Training Loss  0.0028414686676114798\n",
            "Epoch  36 Batch  70 / 525  Training Loss  0.0002797069610096514\n",
            "Epoch  36 Batch  71 / 525  Training Loss  0.004366295877844095\n",
            "Epoch  36 Batch  72 / 525  Training Loss  0.0004047092515975237\n",
            "Epoch  36 Batch  73 / 525  Training Loss  0.00032259165891446173\n",
            "Epoch  36 Batch  74 / 525  Training Loss  0.005542484112083912\n",
            "Epoch  36 Batch  75 / 525  Training Loss  0.0036842438858002424\n",
            "Epoch  36 Batch  76 / 525  Training Loss  0.0042966678738594055\n",
            "Epoch  36 Batch  77 / 525  Training Loss  0.006262300070375204\n",
            "Epoch  36 Batch  78 / 525  Training Loss  0.0011362738441675901\n",
            "Epoch  36 Batch  79 / 525  Training Loss  0.0013751692604273558\n",
            "Epoch  36 Batch  80 / 525  Training Loss  0.006823576055467129\n",
            "Epoch  36 Batch  81 / 525  Training Loss  0.0008791191503405571\n",
            "Epoch  36 Batch  82 / 525  Training Loss  0.0004859715118072927\n",
            "Epoch  36 Batch  83 / 525  Training Loss  0.001484992797486484\n",
            "Epoch  36 Batch  84 / 525  Training Loss  0.0015605819644406438\n",
            "Epoch  36 Batch  85 / 525  Training Loss  0.0009438723209314048\n",
            "Epoch  36 Batch  86 / 525  Training Loss  0.0007134905317798257\n",
            "Epoch  36 Batch  87 / 525  Training Loss  0.000541699817404151\n",
            "Epoch  36 Batch  88 / 525  Training Loss  0.0008751728455536067\n",
            "Epoch  36 Batch  89 / 525  Training Loss  0.0006588202668353915\n",
            "Epoch  36 Batch  90 / 525  Training Loss  0.009892025962471962\n",
            "Epoch  36 Batch  91 / 525  Training Loss  0.0008738312753848732\n",
            "Epoch  36 Batch  92 / 525  Training Loss  0.008713222108781338\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  36 Batch  93 / 525  Training Loss  0.00019013437849935144\n",
            "Epoch  36 Batch  94 / 525  Training Loss  0.0005075321532785892\n",
            "Epoch  36 Batch  95 / 525  Training Loss  0.0006612533470615745\n",
            "Epoch  36 Batch  96 / 525  Training Loss  0.00029794100555591285\n",
            "Epoch  36 Batch  97 / 525  Training Loss  0.0015389802865684032\n",
            "Epoch  36 Batch  98 / 525  Training Loss  0.001039830967783928\n",
            "Epoch  36 Batch  99 / 525  Training Loss  0.00283171609044075\n",
            "Epoch  36 Batch  100 / 525  Training Loss  0.00027427924214862287\n",
            "Epoch  36 Batch  101 / 525  Training Loss  0.0006869651260785758\n",
            "Epoch  36 Batch  102 / 525  Training Loss  0.0005150642828084528\n",
            "Epoch  36 Batch  103 / 525  Training Loss  0.0007140895468182862\n",
            "Epoch  36 Batch  104 / 525  Training Loss  0.0003467167553026229\n",
            "Epoch  36 Batch  105 / 525  Training Loss  0.0008952271891757846\n",
            "Epoch  36 Batch  106 / 525  Training Loss  0.0013400119496509433\n",
            "Epoch  36 Batch  107 / 525  Training Loss  0.0005924205179326236\n",
            "Epoch  36 Batch  108 / 525  Training Loss  0.0007441788329742849\n",
            "Epoch  36 Batch  109 / 525  Training Loss  0.002272205427289009\n",
            "Epoch  36 Batch  110 / 525  Training Loss  0.0003827590844593942\n",
            "Epoch  36 Batch  111 / 525  Training Loss  0.00010605882562231272\n",
            "Epoch  36 Batch  112 / 525  Training Loss  0.0005112320650368929\n",
            "Epoch  36 Batch  113 / 525  Training Loss  0.00046901864698156714\n",
            "Epoch  36 Batch  114 / 525  Training Loss  0.0017692428082227707\n",
            "Epoch  36 Batch  115 / 525  Training Loss  0.0008226929348893464\n",
            "Epoch  36 Batch  116 / 525  Training Loss  0.0007287818007171154\n",
            "Epoch  36 Batch  117 / 525  Training Loss  0.0003490180242806673\n",
            "Epoch  36 Batch  118 / 525  Training Loss  0.0008000601083040237\n",
            "Epoch  36 Batch  119 / 525  Training Loss  0.0007304380415007472\n",
            "Epoch  36 Batch  120 / 525  Training Loss  0.0006906348280608654\n",
            "Epoch  36 Batch  121 / 525  Training Loss  0.0017025753622874618\n",
            "Epoch  36 Batch  122 / 525  Training Loss  0.0001414519501850009\n",
            "Epoch  36 Batch  123 / 525  Training Loss  0.002069521928206086\n",
            "Epoch  36 Batch  124 / 525  Training Loss  0.0003946719807572663\n",
            "Epoch  36 Batch  125 / 525  Training Loss  0.000870930147357285\n",
            "Epoch  36 Batch  126 / 525  Training Loss  0.0015428229235112667\n",
            "Epoch  36 Batch  127 / 525  Training Loss  0.002321197185665369\n",
            "Epoch  36 Batch  128 / 525  Training Loss  0.0050614033825695515\n",
            "Epoch  36 Batch  129 / 525  Training Loss  0.0021800182294100523\n",
            "Epoch  36 Batch  130 / 525  Training Loss  0.0003833610680885613\n",
            "Epoch  36 Batch  131 / 525  Training Loss  0.000510069658048451\n",
            "Epoch  36 Batch  132 / 525  Training Loss  0.0005109794437885284\n",
            "Epoch  36 Batch  133 / 525  Training Loss  0.0005567194311879575\n",
            "Epoch  36 Batch  134 / 525  Training Loss  0.00047609832836315036\n",
            "Epoch  36 Batch  135 / 525  Training Loss  0.00012566676014102995\n",
            "Epoch  36 Batch  136 / 525  Training Loss  0.0007683911826461554\n",
            "Epoch  36 Batch  137 / 525  Training Loss  0.00027275606407783926\n",
            "Epoch  36 Batch  138 / 525  Training Loss  0.0005261137266643345\n",
            "Epoch  36 Batch  139 / 525  Training Loss  0.0007322279852814972\n",
            "Epoch  36 Batch  140 / 525  Training Loss  0.0010682918364182115\n",
            "Epoch  36 Batch  141 / 525  Training Loss  0.0006234748288989067\n",
            "Epoch  36 Batch  142 / 525  Training Loss  0.0002643271873239428\n",
            "Epoch  36 Batch  143 / 525  Training Loss  0.0038330138195306063\n",
            "Epoch  36 Batch  144 / 525  Training Loss  0.0003109048120677471\n",
            "Epoch  36 Batch  145 / 525  Training Loss  0.001097067492082715\n",
            "Epoch  36 Batch  146 / 525  Training Loss  0.000454579247161746\n",
            "Epoch  36 Batch  147 / 525  Training Loss  0.0011450102319940925\n",
            "Epoch  36 Batch  148 / 525  Training Loss  0.0010785701451823115\n",
            "Epoch  36 Batch  149 / 525  Training Loss  0.0008774704183451831\n",
            "Epoch  36 Batch  150 / 525  Training Loss  0.00026493033510632813\n",
            "Epoch  36 Batch  151 / 525  Training Loss  0.001187100075185299\n",
            "Epoch  36 Batch  152 / 525  Training Loss  0.006544767878949642\n",
            "Epoch  36 Batch  153 / 525  Training Loss  0.00631288206204772\n",
            "Epoch  36 Batch  154 / 525  Training Loss  0.004675538279116154\n",
            "Epoch  36 Batch  155 / 525  Training Loss  0.00038753802073188126\n",
            "Epoch  36 Batch  156 / 525  Training Loss  0.001578273600898683\n",
            "Epoch  36 Batch  157 / 525  Training Loss  0.010589007288217545\n",
            "Epoch  36 Batch  158 / 525  Training Loss  0.0017144907033070922\n",
            "Epoch  36 Batch  159 / 525  Training Loss  0.000531985773704946\n",
            "Epoch  36 Batch  160 / 525  Training Loss  0.006931082811206579\n",
            "Epoch  36 Batch  161 / 525  Training Loss  0.0007279393030330539\n",
            "Epoch  36 Batch  162 / 525  Training Loss  0.0018745122943073511\n",
            "Epoch  36 Batch  163 / 525  Training Loss  0.0021125865168869495\n",
            "Epoch  36 Batch  164 / 525  Training Loss  9.38205630518496e-05\n",
            "Epoch  36 Batch  165 / 525  Training Loss  0.000426761427661404\n",
            "Epoch  36 Batch  166 / 525  Training Loss  0.001182009233161807\n",
            "Epoch  36 Batch  167 / 525  Training Loss  0.0019828130025416613\n",
            "Epoch  36 Batch  168 / 525  Training Loss  0.0006328917224891484\n",
            "Epoch  36 Batch  169 / 525  Training Loss  0.002925397828221321\n",
            "Epoch  36 Batch  170 / 525  Training Loss  0.0012987724039703608\n",
            "Epoch  36 Batch  171 / 525  Training Loss  0.00029890978476032615\n",
            "Epoch  36 Batch  172 / 525  Training Loss  0.000923578510992229\n",
            "Epoch  36 Batch  173 / 525  Training Loss  0.0009086019126698375\n",
            "Epoch  36 Batch  174 / 525  Training Loss  0.0037775051314383745\n",
            "Epoch  36 Batch  175 / 525  Training Loss  0.0022221864201128483\n",
            "Epoch  36 Batch  176 / 525  Training Loss  0.0007652517524547875\n",
            "Epoch  36 Batch  177 / 525  Training Loss  0.0017346801469102502\n",
            "Epoch  36 Batch  178 / 525  Training Loss  0.0011553566437214613\n",
            "Epoch  36 Batch  179 / 525  Training Loss  0.0021393164061009884\n",
            "Epoch  36 Batch  180 / 525  Training Loss  0.005275370087474585\n",
            "Epoch  36 Batch  181 / 525  Training Loss  0.0005245963111519814\n",
            "Epoch  36 Batch  182 / 525  Training Loss  0.0010067480616271496\n",
            "Epoch  36 Batch  183 / 525  Training Loss  0.0008212359389290214\n",
            "Epoch  36 Batch  184 / 525  Training Loss  0.00020489783491939306\n",
            "Epoch  36 Batch  185 / 525  Training Loss  0.0002572436642367393\n",
            "Epoch  36 Batch  186 / 525  Training Loss  0.023214489221572876\n",
            "Epoch  36 Batch  187 / 525  Training Loss  0.0012916115811094642\n",
            "Epoch  36 Batch  188 / 525  Training Loss  0.0003796858072746545\n",
            "Epoch  36 Batch  189 / 525  Training Loss  0.0008253435371443629\n",
            "Epoch  36 Batch  190 / 525  Training Loss  0.00032741535687819123\n",
            "Epoch  36 Batch  191 / 525  Training Loss  0.00031507122912444174\n",
            "Epoch  36 Batch  192 / 525  Training Loss  0.002992860274389386\n",
            "Epoch  36 Batch  193 / 525  Training Loss  0.0016180711099877954\n",
            "Epoch  36 Batch  194 / 525  Training Loss  0.0004898443585261703\n",
            "Epoch  36 Batch  195 / 525  Training Loss  0.0003019516298081726\n",
            "Epoch  36 Batch  196 / 525  Training Loss  0.0029652914963662624\n",
            "Epoch  36 Batch  197 / 525  Training Loss  0.0006009467178955674\n",
            "Epoch  36 Batch  198 / 525  Training Loss  0.001338872709311545\n",
            "Epoch  36 Batch  199 / 525  Training Loss  0.0011840949300676584\n",
            "Epoch  36 Batch  200 / 525  Training Loss  0.0010005071526393294\n",
            "Epoch  36 Batch  201 / 525  Training Loss  0.0006772675551474094\n",
            "Epoch  36 Batch  202 / 525  Training Loss  0.0004756971320603043\n",
            "Epoch  36 Batch  203 / 525  Training Loss  0.00048649561358615756\n",
            "Epoch  36 Batch  204 / 525  Training Loss  0.00039863461279310286\n",
            "Epoch  36 Batch  205 / 525  Training Loss  0.00013998951180838048\n",
            "Epoch  36 Batch  206 / 525  Training Loss  0.0008233351400122046\n",
            "Epoch  36 Batch  207 / 525  Training Loss  0.0002452564949635416\n",
            "Epoch  36 Batch  208 / 525  Training Loss  0.00032978312810882926\n",
            "Epoch  36 Batch  209 / 525  Training Loss  0.000339412858011201\n",
            "Epoch  36 Batch  210 / 525  Training Loss  0.00038005877286195755\n",
            "Epoch  36 Batch  211 / 525  Training Loss  0.0003376564127393067\n",
            "Epoch  36 Batch  212 / 525  Training Loss  0.0023094285279512405\n",
            "Epoch  36 Batch  213 / 525  Training Loss  0.0005257814773358405\n",
            "Epoch  36 Batch  214 / 525  Training Loss  0.00036780539085157216\n",
            "Epoch  36 Batch  215 / 525  Training Loss  0.0011878873920068145\n",
            "Epoch  36 Batch  216 / 525  Training Loss  0.0008380316430702806\n",
            "Epoch  36 Batch  217 / 525  Training Loss  0.0007116291671991348\n",
            "Epoch  36 Batch  218 / 525  Training Loss  0.0006926953210495412\n",
            "Epoch  36 Batch  219 / 525  Training Loss  0.0003943112678825855\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  36 Batch  220 / 525  Training Loss  0.002923420397564769\n",
            "Epoch  36 Batch  221 / 525  Training Loss  0.00040979916229844093\n",
            "Epoch  36 Batch  222 / 525  Training Loss  0.0014656985877081752\n",
            "Epoch  36 Batch  223 / 525  Training Loss  0.0030225045047700405\n",
            "Epoch  36 Batch  224 / 525  Training Loss  0.0011741297785192728\n",
            "Epoch  36 Batch  225 / 525  Training Loss  0.0008942708373069763\n",
            "Epoch  36 Batch  226 / 525  Training Loss  0.0018754515331238508\n",
            "Epoch  36 Batch  227 / 525  Training Loss  0.002115891082212329\n",
            "Epoch  36 Batch  228 / 525  Training Loss  0.0007804027991369367\n",
            "Epoch  36 Batch  229 / 525  Training Loss  0.0015058156568557024\n",
            "Epoch  36 Batch  230 / 525  Training Loss  0.0010236374801024795\n",
            "Epoch  36 Batch  231 / 525  Training Loss  0.0012722315732389688\n",
            "Epoch  36 Batch  232 / 525  Training Loss  0.0003809895715676248\n",
            "Epoch  36 Batch  233 / 525  Training Loss  0.0012290874728932977\n",
            "Epoch  36 Batch  234 / 525  Training Loss  0.0007860716432332993\n",
            "Epoch  36 Batch  235 / 525  Training Loss  0.002880917163565755\n",
            "Epoch  36 Batch  236 / 525  Training Loss  0.0011901765828952193\n",
            "Epoch  36 Batch  237 / 525  Training Loss  0.0003412463702261448\n",
            "Epoch  36 Batch  238 / 525  Training Loss  0.0006180721102282405\n",
            "Epoch  36 Batch  239 / 525  Training Loss  0.0004292637459002435\n",
            "Epoch  36 Batch  240 / 525  Training Loss  0.0009468302014283836\n",
            "Epoch  36 Batch  241 / 525  Training Loss  0.0021587791852653027\n",
            "Epoch  36 Batch  242 / 525  Training Loss  0.00039051586645655334\n",
            "Epoch  36 Batch  243 / 525  Training Loss  0.005786844529211521\n",
            "Epoch  36 Batch  244 / 525  Training Loss  0.0007069645216688514\n",
            "Epoch  36 Batch  245 / 525  Training Loss  0.0008735501323826611\n",
            "Epoch  36 Batch  246 / 525  Training Loss  0.0013156214263290167\n",
            "Epoch  36 Batch  247 / 525  Training Loss  0.0003744277637451887\n",
            "Epoch  36 Batch  248 / 525  Training Loss  0.0013218685053288937\n",
            "Epoch  36 Batch  249 / 525  Training Loss  0.000687410298269242\n",
            "Epoch  36 Batch  250 / 525  Training Loss  0.0003538982418831438\n",
            "Epoch  36 Batch  251 / 525  Training Loss  0.0006798057584092021\n",
            "Epoch  36 Batch  252 / 525  Training Loss  0.0019195580389350653\n",
            "Epoch  36 Batch  253 / 525  Training Loss  0.0011192542733624578\n",
            "Epoch  36 Batch  254 / 525  Training Loss  0.0015063028549775481\n",
            "Epoch  36 Batch  255 / 525  Training Loss  0.0005578658310696483\n",
            "Epoch  36 Batch  256 / 525  Training Loss  0.0007308402564376593\n",
            "Epoch  36 Batch  257 / 525  Training Loss  0.005802709609270096\n",
            "Epoch  36 Batch  258 / 525  Training Loss  0.004774396773427725\n",
            "Epoch  36 Batch  259 / 525  Training Loss  0.0006229598075151443\n",
            "Epoch  36 Batch  260 / 525  Training Loss  0.0005478019593283534\n",
            "Epoch  36 Batch  261 / 525  Training Loss  0.00029528685263358057\n",
            "Epoch  36 Batch  262 / 525  Training Loss  0.0012482914607971907\n",
            "Epoch  36 Batch  263 / 525  Training Loss  0.0013066452229395509\n",
            "Epoch  36 Batch  264 / 525  Training Loss  0.0011286797234788537\n",
            "Epoch  36 Batch  265 / 525  Training Loss  0.0026227685157209635\n",
            "Epoch  36 Batch  266 / 525  Training Loss  0.0002906958688981831\n",
            "Epoch  36 Batch  267 / 525  Training Loss  0.0012230722932145\n",
            "Epoch  36 Batch  268 / 525  Training Loss  0.0019619124941527843\n",
            "Epoch  36 Batch  269 / 525  Training Loss  0.006352577358484268\n",
            "Epoch  36 Batch  270 / 525  Training Loss  0.002391985384747386\n",
            "Epoch  36 Batch  271 / 525  Training Loss  0.0005539583507925272\n",
            "Epoch  36 Batch  272 / 525  Training Loss  0.0002134539681719616\n",
            "Epoch  36 Batch  273 / 525  Training Loss  0.0008545573800802231\n",
            "Epoch  36 Batch  274 / 525  Training Loss  0.0012945535127073526\n",
            "Epoch  36 Batch  275 / 525  Training Loss  0.001241894788108766\n",
            "Epoch  36 Batch  276 / 525  Training Loss  0.0010209765750914812\n",
            "Epoch  36 Batch  277 / 525  Training Loss  0.0011436239583417773\n",
            "Epoch  36 Batch  278 / 525  Training Loss  0.0006215600878931582\n",
            "Epoch  36 Batch  279 / 525  Training Loss  0.0002849720767699182\n",
            "Epoch  36 Batch  280 / 525  Training Loss  0.0003319147217553109\n",
            "Epoch  36 Batch  281 / 525  Training Loss  0.0003433290112297982\n",
            "Epoch  36 Batch  282 / 525  Training Loss  0.0029209465719759464\n",
            "Epoch  36 Batch  283 / 525  Training Loss  0.0023619907442480326\n",
            "Epoch  36 Batch  284 / 525  Training Loss  0.0006760398973710835\n",
            "Epoch  36 Batch  285 / 525  Training Loss  0.0007911907741799951\n",
            "Epoch  36 Batch  286 / 525  Training Loss  0.00029099377570673823\n",
            "Epoch  36 Batch  287 / 525  Training Loss  8.269148383988068e-05\n",
            "Epoch  36 Batch  288 / 525  Training Loss  0.00309672555886209\n",
            "Epoch  36 Batch  289 / 525  Training Loss  0.001253123627975583\n",
            "Epoch  36 Batch  290 / 525  Training Loss  0.0007710722275078297\n",
            "Epoch  36 Batch  291 / 525  Training Loss  0.0022075274027884007\n",
            "Epoch  36 Batch  292 / 525  Training Loss  0.0013008282985538244\n",
            "Epoch  36 Batch  293 / 525  Training Loss  0.0012735780328512192\n",
            "Epoch  36 Batch  294 / 525  Training Loss  0.005751785822212696\n",
            "Epoch  36 Batch  295 / 525  Training Loss  0.00028234621277078986\n",
            "Epoch  36 Batch  296 / 525  Training Loss  0.00021530727099161595\n",
            "Epoch  36 Batch  297 / 525  Training Loss  0.0015099410666152835\n",
            "Epoch  36 Batch  298 / 525  Training Loss  0.00025420848396606743\n",
            "Epoch  36 Batch  299 / 525  Training Loss  0.0010104661341756582\n",
            "Epoch  36 Batch  300 / 525  Training Loss  0.0018796460935845971\n",
            "Epoch  36 Batch  301 / 525  Training Loss  0.00071243493584916\n",
            "Epoch  36 Batch  302 / 525  Training Loss  0.00033686848473735154\n",
            "Epoch  36 Batch  303 / 525  Training Loss  0.0012631600257009268\n",
            "Epoch  36 Batch  304 / 525  Training Loss  0.00025671994080767035\n",
            "Epoch  36 Batch  305 / 525  Training Loss  0.0002293840516358614\n",
            "Epoch  36 Batch  306 / 525  Training Loss  0.003464578418061137\n",
            "Epoch  36 Batch  307 / 525  Training Loss  0.0006795889930799603\n",
            "Epoch  36 Batch  308 / 525  Training Loss  0.0038563564885407686\n",
            "Epoch  36 Batch  309 / 525  Training Loss  0.0014364278176799417\n",
            "Epoch  36 Batch  310 / 525  Training Loss  9.395416418556124e-05\n",
            "Epoch  36 Batch  311 / 525  Training Loss  0.005522090010344982\n",
            "Epoch  36 Batch  312 / 525  Training Loss  0.0010330125223845243\n",
            "Epoch  36 Batch  313 / 525  Training Loss  0.0021257218904793262\n",
            "Epoch  36 Batch  314 / 525  Training Loss  0.0014722679043188691\n",
            "Epoch  36 Batch  315 / 525  Training Loss  0.0002988628693856299\n",
            "Epoch  36 Batch  316 / 525  Training Loss  0.00046644103713333607\n",
            "Epoch  36 Batch  317 / 525  Training Loss  0.0004298933199606836\n",
            "Epoch  36 Batch  318 / 525  Training Loss  0.0034226798452436924\n",
            "Epoch  36 Batch  319 / 525  Training Loss  0.0025195772759616375\n",
            "Epoch  36 Batch  320 / 525  Training Loss  0.002642091829329729\n",
            "Epoch  36 Batch  321 / 525  Training Loss  0.002169233513996005\n",
            "Epoch  36 Batch  322 / 525  Training Loss  0.0019191764295101166\n",
            "Epoch  36 Batch  323 / 525  Training Loss  0.0004202291020192206\n",
            "Epoch  36 Batch  324 / 525  Training Loss  0.0016654068604111671\n",
            "Epoch  36 Batch  325 / 525  Training Loss  0.00034222935209982097\n",
            "Epoch  36 Batch  326 / 525  Training Loss  6.2133367464412e-05\n",
            "Epoch  36 Batch  327 / 525  Training Loss  0.0004296904371585697\n",
            "Epoch  36 Batch  328 / 525  Training Loss  0.0010651422198861837\n",
            "Epoch  36 Batch  329 / 525  Training Loss  0.0007317072013393044\n",
            "Epoch  36 Batch  330 / 525  Training Loss  0.000576022663153708\n",
            "Epoch  36 Batch  331 / 525  Training Loss  0.0007488128612749279\n",
            "Epoch  36 Batch  332 / 525  Training Loss  0.000656463613267988\n",
            "Epoch  36 Batch  333 / 525  Training Loss  0.0012727163266390562\n",
            "Epoch  36 Batch  334 / 525  Training Loss  0.0022510592825710773\n",
            "Epoch  36 Batch  335 / 525  Training Loss  0.0008095185039564967\n",
            "Epoch  36 Batch  336 / 525  Training Loss  0.0014211514499038458\n",
            "Epoch  36 Batch  337 / 525  Training Loss  0.006090959999710321\n",
            "Epoch  36 Batch  338 / 525  Training Loss  0.000513695937115699\n",
            "Epoch  36 Batch  339 / 525  Training Loss  0.00021714367903769016\n",
            "Epoch  36 Batch  340 / 525  Training Loss  0.000565438240300864\n",
            "Epoch  36 Batch  341 / 525  Training Loss  0.0001851030538091436\n",
            "Epoch  36 Batch  342 / 525  Training Loss  0.0003337958478368819\n",
            "Epoch  36 Batch  343 / 525  Training Loss  0.000979919102974236\n",
            "Epoch  36 Batch  344 / 525  Training Loss  0.000200836147996597\n",
            "Epoch  36 Batch  345 / 525  Training Loss  0.000594625947996974\n",
            "Epoch  36 Batch  346 / 525  Training Loss  0.00170179083943367\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  36 Batch  347 / 525  Training Loss  0.00043582147918641567\n",
            "Epoch  36 Batch  348 / 525  Training Loss  0.0003120659966953099\n",
            "Epoch  36 Batch  349 / 525  Training Loss  0.0006749508902430534\n",
            "Epoch  36 Batch  350 / 525  Training Loss  0.0007209909381344914\n",
            "Epoch  36 Batch  351 / 525  Training Loss  0.00019830581732094288\n",
            "Epoch  36 Batch  352 / 525  Training Loss  0.0017021739622578025\n",
            "Epoch  36 Batch  353 / 525  Training Loss  0.00048179193981923163\n",
            "Epoch  36 Batch  354 / 525  Training Loss  0.0008185869082808495\n",
            "Epoch  36 Batch  355 / 525  Training Loss  0.0004902660148218274\n",
            "Epoch  36 Batch  356 / 525  Training Loss  0.0009172678110189736\n",
            "Epoch  36 Batch  357 / 525  Training Loss  0.00014618129353038967\n",
            "Epoch  36 Batch  358 / 525  Training Loss  0.0003454865363892168\n",
            "Epoch  36 Batch  359 / 525  Training Loss  0.00034904509084299207\n",
            "Epoch  36 Batch  360 / 525  Training Loss  0.0010512475855648518\n",
            "Epoch  36 Batch  361 / 525  Training Loss  0.00201774132438004\n",
            "Epoch  36 Batch  362 / 525  Training Loss  0.0005957031389698386\n",
            "Epoch  36 Batch  363 / 525  Training Loss  0.0081298453733325\n",
            "Epoch  36 Batch  364 / 525  Training Loss  0.00038463628152385354\n",
            "Epoch  36 Batch  365 / 525  Training Loss  0.000157631206093356\n",
            "Epoch  36 Batch  366 / 525  Training Loss  0.010342742316424847\n",
            "Epoch  36 Batch  367 / 525  Training Loss  0.0020952257327735424\n",
            "Epoch  36 Batch  368 / 525  Training Loss  0.0008680711616761982\n",
            "Epoch  36 Batch  369 / 525  Training Loss  0.00031758815748617053\n",
            "Epoch  36 Batch  370 / 525  Training Loss  0.0004717097617685795\n",
            "Epoch  36 Batch  371 / 525  Training Loss  0.0028809504583477974\n",
            "Epoch  36 Batch  372 / 525  Training Loss  0.00028431069222278893\n",
            "Epoch  36 Batch  373 / 525  Training Loss  0.0025626218412071466\n",
            "Epoch  36 Batch  374 / 525  Training Loss  0.0006621152861043811\n",
            "Epoch  36 Batch  375 / 525  Training Loss  0.0005097578978165984\n",
            "Epoch  36 Batch  376 / 525  Training Loss  0.0004994111368432641\n",
            "Epoch  36 Batch  377 / 525  Training Loss  0.00034212192986160517\n",
            "Epoch  36 Batch  378 / 525  Training Loss  0.0003788582398556173\n",
            "Epoch  36 Batch  379 / 525  Training Loss  0.00023028049326967448\n",
            "Epoch  36 Batch  380 / 525  Training Loss  0.0021594087593257427\n",
            "Epoch  36 Batch  381 / 525  Training Loss  0.0003736373037099838\n",
            "Epoch  36 Batch  382 / 525  Training Loss  0.00037169852294027805\n",
            "Epoch  36 Batch  383 / 525  Training Loss  0.0013764511095359921\n",
            "Epoch  36 Batch  384 / 525  Training Loss  0.002518610330298543\n",
            "Epoch  36 Batch  385 / 525  Training Loss  0.0009791513439267874\n",
            "Epoch  36 Batch  386 / 525  Training Loss  0.00048552616499364376\n",
            "Epoch  36 Batch  387 / 525  Training Loss  0.0007691624923609197\n",
            "Epoch  36 Batch  388 / 525  Training Loss  0.000814364873804152\n",
            "Epoch  36 Batch  389 / 525  Training Loss  0.0016341215232387185\n",
            "Epoch  36 Batch  390 / 525  Training Loss  0.0004322604218032211\n",
            "Epoch  36 Batch  391 / 525  Training Loss  0.0023376818280667067\n",
            "Epoch  36 Batch  392 / 525  Training Loss  0.004834176041185856\n",
            "Epoch  36 Batch  393 / 525  Training Loss  0.004047592636197805\n",
            "Epoch  36 Batch  394 / 525  Training Loss  0.0013816615100950003\n",
            "Epoch  36 Batch  395 / 525  Training Loss  0.0004238739493303001\n",
            "Epoch  36 Batch  396 / 525  Training Loss  0.00031267906888388097\n",
            "Epoch  36 Batch  397 / 525  Training Loss  0.001325131393969059\n",
            "Epoch  36 Batch  398 / 525  Training Loss  0.0003858449053950608\n",
            "Epoch  36 Batch  399 / 525  Training Loss  0.00014038747758604586\n",
            "Epoch  36 Batch  400 / 525  Training Loss  0.00040946542867459357\n",
            "Epoch  36 Batch  401 / 525  Training Loss  0.001518545439466834\n",
            "Epoch  36 Batch  402 / 525  Training Loss  0.0012821834534406662\n",
            "Epoch  36 Batch  403 / 525  Training Loss  0.0003245761035941541\n",
            "Epoch  36 Batch  404 / 525  Training Loss  0.0004701203142758459\n",
            "Epoch  36 Batch  405 / 525  Training Loss  0.0004439843469299376\n",
            "Epoch  36 Batch  406 / 525  Training Loss  0.0008556802058592439\n",
            "Epoch  36 Batch  407 / 525  Training Loss  0.0008761849021539092\n",
            "Epoch  36 Batch  408 / 525  Training Loss  0.00328764203004539\n",
            "Epoch  36 Batch  409 / 525  Training Loss  0.0012459387071430683\n",
            "Epoch  36 Batch  410 / 525  Training Loss  0.00033590386738069355\n",
            "Epoch  36 Batch  411 / 525  Training Loss  0.0036531765945255756\n",
            "Epoch  36 Batch  412 / 525  Training Loss  0.003040722804144025\n",
            "Epoch  36 Batch  413 / 525  Training Loss  0.007199539802968502\n",
            "Epoch  36 Batch  414 / 525  Training Loss  0.0003119765315204859\n",
            "Epoch  36 Batch  415 / 525  Training Loss  0.0013580399099737406\n",
            "Epoch  36 Batch  416 / 525  Training Loss  0.0026577890384942293\n",
            "Epoch  36 Batch  417 / 525  Training Loss  0.0008196088601835072\n",
            "Epoch  36 Batch  418 / 525  Training Loss  0.0004569462616927922\n",
            "Epoch  36 Batch  419 / 525  Training Loss  0.0015319738304242492\n",
            "Epoch  36 Batch  420 / 525  Training Loss  0.0058133359998464584\n",
            "Epoch  36 Batch  421 / 525  Training Loss  0.0009019232238642871\n",
            "Epoch  36 Batch  422 / 525  Training Loss  0.002510646590963006\n",
            "Epoch  36 Batch  423 / 525  Training Loss  0.0002712688292376697\n",
            "Epoch  36 Batch  424 / 525  Training Loss  0.0010796795831993222\n",
            "Epoch  36 Batch  425 / 525  Training Loss  0.00020422421221155673\n",
            "Epoch  36 Batch  426 / 525  Training Loss  0.0017479878151789308\n",
            "Epoch  36 Batch  427 / 525  Training Loss  0.001958918059244752\n",
            "Epoch  36 Batch  428 / 525  Training Loss  0.0027397333178669214\n",
            "Epoch  36 Batch  429 / 525  Training Loss  0.004213492386043072\n",
            "Epoch  36 Batch  430 / 525  Training Loss  0.0004182937555015087\n",
            "Epoch  36 Batch  431 / 525  Training Loss  0.0021359589882194996\n",
            "Epoch  36 Batch  432 / 525  Training Loss  0.0007522159721702337\n",
            "Epoch  36 Batch  433 / 525  Training Loss  0.0009548035450279713\n",
            "Epoch  36 Batch  434 / 525  Training Loss  0.0012788902968168259\n",
            "Epoch  36 Batch  435 / 525  Training Loss  0.0017329476540908217\n",
            "Epoch  36 Batch  436 / 525  Training Loss  0.0006578124011866748\n",
            "Epoch  36 Batch  437 / 525  Training Loss  0.002615434816107154\n",
            "Epoch  36 Batch  438 / 525  Training Loss  0.001955749001353979\n",
            "Epoch  36 Batch  439 / 525  Training Loss  0.0003421066212467849\n",
            "Epoch  36 Batch  440 / 525  Training Loss  0.0005327750113792717\n",
            "Epoch  36 Batch  441 / 525  Training Loss  0.0024845325388014317\n",
            "Epoch  36 Batch  442 / 525  Training Loss  0.001131137483753264\n",
            "Epoch  36 Batch  443 / 525  Training Loss  0.0015260338550433517\n",
            "Epoch  36 Batch  444 / 525  Training Loss  0.000834347796626389\n",
            "Epoch  36 Batch  445 / 525  Training Loss  0.0009784860303625464\n",
            "Epoch  36 Batch  446 / 525  Training Loss  0.0016704139998182654\n",
            "Epoch  36 Batch  447 / 525  Training Loss  0.004858984146267176\n",
            "Epoch  36 Batch  448 / 525  Training Loss  0.001345766824670136\n",
            "Epoch  36 Batch  449 / 525  Training Loss  0.0002391048037679866\n",
            "Epoch  36 Batch  450 / 525  Training Loss  0.0004709369386546314\n",
            "Epoch  36 Batch  451 / 525  Training Loss  0.002674966584891081\n",
            "Epoch  36 Batch  452 / 525  Training Loss  0.0012037205742672086\n",
            "Epoch  36 Batch  453 / 525  Training Loss  0.0002444440615363419\n",
            "Epoch  36 Batch  454 / 525  Training Loss  0.000984810059890151\n",
            "Epoch  36 Batch  455 / 525  Training Loss  0.0006974758580327034\n",
            "Epoch  36 Batch  456 / 525  Training Loss  0.0006744136335328221\n",
            "Epoch  36 Batch  457 / 525  Training Loss  0.003550486173480749\n",
            "Epoch  36 Batch  458 / 525  Training Loss  0.0002745106758084148\n",
            "Epoch  36 Batch  459 / 525  Training Loss  0.00048691569827497005\n",
            "Epoch  36 Batch  460 / 525  Training Loss  0.0009253357420675457\n",
            "Epoch  36 Batch  461 / 525  Training Loss  0.0003928217920474708\n",
            "Epoch  36 Batch  462 / 525  Training Loss  0.0007488037226721644\n",
            "Epoch  36 Batch  463 / 525  Training Loss  0.0012321879621595144\n",
            "Epoch  36 Batch  464 / 525  Training Loss  0.0035839644260704517\n",
            "Epoch  36 Batch  465 / 525  Training Loss  0.0005417101201601326\n",
            "Epoch  36 Batch  466 / 525  Training Loss  0.0005414977786131203\n",
            "Epoch  36 Batch  467 / 525  Training Loss  0.0007734126411378384\n",
            "Epoch  36 Batch  468 / 525  Training Loss  0.0004760454467032105\n",
            "Epoch  36 Batch  469 / 525  Training Loss  0.0006716092466376722\n",
            "Epoch  36 Batch  470 / 525  Training Loss  0.0007538372883573174\n",
            "Epoch  36 Batch  471 / 525  Training Loss  0.002622076775878668\n",
            "Epoch  36 Batch  472 / 525  Training Loss  0.0022709632758051157\n",
            "Epoch  36 Batch  473 / 525  Training Loss  0.0005977202672511339\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  36 Batch  474 / 525  Training Loss  0.0018482264131307602\n",
            "Epoch  36 Batch  475 / 525  Training Loss  0.00037624023389071226\n",
            "Epoch  36 Batch  476 / 525  Training Loss  0.001175448065623641\n",
            "Epoch  36 Batch  477 / 525  Training Loss  0.0006318658706732094\n",
            "Epoch  36 Batch  478 / 525  Training Loss  0.0011587922926992178\n",
            "Epoch  36 Batch  479 / 525  Training Loss  0.0005279276520013809\n",
            "Epoch  36 Batch  480 / 525  Training Loss  0.0005286801606416702\n",
            "Epoch  36 Batch  481 / 525  Training Loss  0.0008148361812345684\n",
            "Epoch  36 Batch  482 / 525  Training Loss  0.000566997507121414\n",
            "Epoch  36 Batch  483 / 525  Training Loss  0.001034809509292245\n",
            "Epoch  36 Batch  484 / 525  Training Loss  0.004040207248181105\n",
            "Epoch  36 Batch  485 / 525  Training Loss  0.0006169830448925495\n",
            "Epoch  36 Batch  486 / 525  Training Loss  0.0006776786176487803\n",
            "Epoch  36 Batch  487 / 525  Training Loss  0.0007145964773371816\n",
            "Epoch  36 Batch  488 / 525  Training Loss  0.0004594246856868267\n",
            "Epoch  36 Batch  489 / 525  Training Loss  0.001731485826894641\n",
            "Epoch  36 Batch  490 / 525  Training Loss  0.0014153525698930025\n",
            "Epoch  36 Batch  491 / 525  Training Loss  0.0016136051854118705\n",
            "Epoch  36 Batch  492 / 525  Training Loss  0.000517701671924442\n",
            "Epoch  36 Batch  493 / 525  Training Loss  0.0011469469172880054\n",
            "Epoch  36 Batch  494 / 525  Training Loss  0.0005567007465288043\n",
            "Epoch  36 Batch  495 / 525  Training Loss  0.000483910262119025\n",
            "Epoch  36 Batch  496 / 525  Training Loss  0.000777510111220181\n",
            "Epoch  36 Batch  497 / 525  Training Loss  0.0008664999040775001\n",
            "Epoch  36 Batch  498 / 525  Training Loss  0.005160958040505648\n",
            "Epoch  36 Batch  499 / 525  Training Loss  0.000327559479046613\n",
            "Epoch  36 Batch  500 / 525  Training Loss  0.0010741373989731073\n",
            "Epoch  36 Batch  501 / 525  Training Loss  0.0010163879487663507\n",
            "Epoch  36 Batch  502 / 525  Training Loss  0.0005873182089999318\n",
            "Epoch  36 Batch  503 / 525  Training Loss  0.0002370179572608322\n",
            "Epoch  36 Batch  504 / 525  Training Loss  0.002591628348454833\n",
            "Epoch  36 Batch  505 / 525  Training Loss  0.0002894756617024541\n",
            "Epoch  36 Batch  506 / 525  Training Loss  0.00013448161189444363\n",
            "Epoch  36 Batch  507 / 525  Training Loss  0.002454186789691448\n",
            "Epoch  36 Batch  508 / 525  Training Loss  0.002640679944306612\n",
            "Epoch  36 Batch  509 / 525  Training Loss  0.0022046510130167007\n",
            "Epoch  36 Batch  510 / 525  Training Loss  0.005041851662099361\n",
            "Epoch  36 Batch  511 / 525  Training Loss  0.00805567018687725\n",
            "Epoch  36 Batch  512 / 525  Training Loss  0.00039246081723831594\n",
            "Epoch  36 Batch  513 / 525  Training Loss  0.0004960424266755581\n",
            "Epoch  36 Batch  514 / 525  Training Loss  0.0024969223886728287\n",
            "Epoch  36 Batch  515 / 525  Training Loss  0.004880933556705713\n",
            "Epoch  36 Batch  516 / 525  Training Loss  0.0007964959368109703\n",
            "Epoch  36 Batch  517 / 525  Training Loss  0.0012323983246460557\n",
            "Epoch  36 Batch  518 / 525  Training Loss  0.0004097806813661009\n",
            "Epoch  36 Batch  519 / 525  Training Loss  0.0005021524848416448\n",
            "Epoch  36 Batch  520 / 525  Training Loss  0.00026317776064388454\n",
            "Epoch  36 Batch  521 / 525  Training Loss  0.001393057405948639\n",
            "Epoch  36 Batch  522 / 525  Training Loss  0.0022418564185500145\n",
            "Epoch  36 Batch  523 / 525  Training Loss  0.0006376007222570479\n",
            "Epoch  36 Batch  524 / 525  Training Loss  0.0009067055070772767\n",
            "  37    |    -    |   0.001489   | 61.658333\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 37\n",
            "Epoch  37 Batch  0 / 525  Training Loss  0.000464216573163867\n",
            "Epoch  37 Batch  1 / 525  Training Loss  0.0003719317610375583\n",
            "Epoch  37 Batch  2 / 525  Training Loss  0.0008475170470774174\n",
            "Epoch  37 Batch  3 / 525  Training Loss  0.0007827028748579323\n",
            "Epoch  37 Batch  4 / 525  Training Loss  0.0008266005897894502\n",
            "Epoch  37 Batch  5 / 525  Training Loss  0.0005218753358349204\n",
            "Epoch  37 Batch  6 / 525  Training Loss  0.0008764869417063892\n",
            "Epoch  37 Batch  7 / 525  Training Loss  0.00022260301921050996\n",
            "Epoch  37 Batch  8 / 525  Training Loss  0.0004461928328964859\n",
            "Epoch  37 Batch  9 / 525  Training Loss  0.00026668765349313617\n",
            "Epoch  37 Batch  10 / 525  Training Loss  0.0005622684839181602\n",
            "Epoch  37 Batch  11 / 525  Training Loss  0.0005722398636862636\n",
            "Epoch  37 Batch  12 / 525  Training Loss  0.00042479438707232475\n",
            "Epoch  37 Batch  13 / 525  Training Loss  0.0001726816117297858\n",
            "Epoch  37 Batch  14 / 525  Training Loss  0.0011174740502610803\n",
            "Epoch  37 Batch  15 / 525  Training Loss  0.004261743742972612\n",
            "Epoch  37 Batch  16 / 525  Training Loss  0.0001326277561020106\n",
            "Epoch  37 Batch  17 / 525  Training Loss  0.0004050580901093781\n",
            "Epoch  37 Batch  18 / 525  Training Loss  0.0008741944329813123\n",
            "Epoch  37 Batch  19 / 525  Training Loss  0.0008430442539975047\n",
            "Epoch  37 Batch  20 / 525  Training Loss  0.0014790327986702323\n",
            "Epoch  37 Batch  21 / 525  Training Loss  0.0004341994062997401\n",
            "Epoch  37 Batch  22 / 525  Training Loss  0.00015944988990668207\n",
            "Epoch  37 Batch  23 / 525  Training Loss  0.00029145251028239727\n",
            "Epoch  37 Batch  24 / 525  Training Loss  0.0012956351274624467\n",
            "Epoch  37 Batch  25 / 525  Training Loss  0.00034207230783067644\n",
            "Epoch  37 Batch  26 / 525  Training Loss  0.0004919646889902651\n",
            "Epoch  37 Batch  27 / 525  Training Loss  0.0005288302199915051\n",
            "Epoch  37 Batch  28 / 525  Training Loss  0.0005598095012828708\n",
            "Epoch  37 Batch  29 / 525  Training Loss  0.00046597496839240193\n",
            "Epoch  37 Batch  30 / 525  Training Loss  0.0015070674708113074\n",
            "Epoch  37 Batch  31 / 525  Training Loss  0.0021283344831317663\n",
            "Epoch  37 Batch  32 / 525  Training Loss  0.0005960592534393072\n",
            "Epoch  37 Batch  33 / 525  Training Loss  0.00022137872292660177\n",
            "Epoch  37 Batch  34 / 525  Training Loss  0.00028675750945694745\n",
            "Epoch  37 Batch  35 / 525  Training Loss  0.00025899658794514835\n",
            "Epoch  37 Batch  36 / 525  Training Loss  0.0003505131753627211\n",
            "Epoch  37 Batch  37 / 525  Training Loss  0.00016115294420160353\n",
            "Epoch  37 Batch  38 / 525  Training Loss  0.0007450065459124744\n",
            "Epoch  37 Batch  39 / 525  Training Loss  0.0003011976950801909\n",
            "Epoch  37 Batch  40 / 525  Training Loss  0.0004956008633598685\n",
            "Epoch  37 Batch  41 / 525  Training Loss  0.0007564235711470246\n",
            "Epoch  37 Batch  42 / 525  Training Loss  0.012235695496201515\n",
            "Epoch  37 Batch  43 / 525  Training Loss  0.0002652903494890779\n",
            "Epoch  37 Batch  44 / 525  Training Loss  0.0033785600680857897\n",
            "Epoch  37 Batch  45 / 525  Training Loss  0.0001959583314601332\n",
            "Epoch  37 Batch  46 / 525  Training Loss  0.002423888770863414\n",
            "Epoch  37 Batch  47 / 525  Training Loss  0.0002510062768124044\n",
            "Epoch  37 Batch  48 / 525  Training Loss  0.00014572180225513875\n",
            "Epoch  37 Batch  49 / 525  Training Loss  0.00027081239386461675\n",
            "Epoch  37 Batch  50 / 525  Training Loss  0.0009421409922651947\n",
            "Epoch  37 Batch  51 / 525  Training Loss  0.000448919425252825\n",
            "Epoch  37 Batch  52 / 525  Training Loss  0.000255293445661664\n",
            "Epoch  37 Batch  53 / 525  Training Loss  0.00047599259414710104\n",
            "Epoch  37 Batch  54 / 525  Training Loss  0.0012242102529853582\n",
            "Epoch  37 Batch  55 / 525  Training Loss  0.0006230810540728271\n",
            "Epoch  37 Batch  56 / 525  Training Loss  0.0004529083089437336\n",
            "Epoch  37 Batch  57 / 525  Training Loss  7.651872874703258e-05\n",
            "Epoch  37 Batch  58 / 525  Training Loss  9.744843555381522e-05\n",
            "Epoch  37 Batch  59 / 525  Training Loss  0.0002675804134923965\n",
            "Epoch  37 Batch  60 / 525  Training Loss  0.0003848321794066578\n",
            "Epoch  37 Batch  61 / 525  Training Loss  0.00015337756485678256\n",
            "Epoch  37 Batch  62 / 525  Training Loss  0.0003163878864143044\n",
            "Epoch  37 Batch  63 / 525  Training Loss  0.0004927742993459105\n",
            "Epoch  37 Batch  64 / 525  Training Loss  0.0007406899821944535\n",
            "Epoch  37 Batch  65 / 525  Training Loss  0.00039313064189627767\n",
            "Epoch  37 Batch  66 / 525  Training Loss  0.00018472386000212282\n",
            "Epoch  37 Batch  67 / 525  Training Loss  0.0002801849623210728\n",
            "Epoch  37 Batch  68 / 525  Training Loss  0.00037048920057713985\n",
            "Epoch  37 Batch  69 / 525  Training Loss  0.0006805196753703058\n",
            "Epoch  37 Batch  70 / 525  Training Loss  0.00021022807050030679\n",
            "Epoch  37 Batch  71 / 525  Training Loss  0.0005353728192858398\n",
            "Epoch  37 Batch  72 / 525  Training Loss  0.002296584425494075\n",
            "Epoch  37 Batch  73 / 525  Training Loss  0.0002825429546646774\n",
            "Epoch  37 Batch  74 / 525  Training Loss  0.00018780239042825997\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  37 Batch  75 / 525  Training Loss  0.00018430718046147376\n",
            "Epoch  37 Batch  76 / 525  Training Loss  0.0003213253803551197\n",
            "Epoch  37 Batch  77 / 525  Training Loss  0.00020369957201182842\n",
            "Epoch  37 Batch  78 / 525  Training Loss  0.001181511441245675\n",
            "Epoch  37 Batch  79 / 525  Training Loss  0.00016549207794014364\n",
            "Epoch  37 Batch  80 / 525  Training Loss  0.0010205976432189345\n",
            "Epoch  37 Batch  81 / 525  Training Loss  0.0003701228415593505\n",
            "Epoch  37 Batch  82 / 525  Training Loss  0.00032902773818932474\n",
            "Epoch  37 Batch  83 / 525  Training Loss  0.0003093855339102447\n",
            "Epoch  37 Batch  84 / 525  Training Loss  0.0003490871167741716\n",
            "Epoch  37 Batch  85 / 525  Training Loss  0.0004116277559660375\n",
            "Epoch  37 Batch  86 / 525  Training Loss  9.43271879805252e-05\n",
            "Epoch  37 Batch  87 / 525  Training Loss  0.0004935443284921348\n",
            "Epoch  37 Batch  88 / 525  Training Loss  0.000479111768072471\n",
            "Epoch  37 Batch  89 / 525  Training Loss  0.00034039380261674523\n",
            "Epoch  37 Batch  90 / 525  Training Loss  0.0002740174240898341\n",
            "Epoch  37 Batch  91 / 525  Training Loss  0.0005029336316511035\n",
            "Epoch  37 Batch  92 / 525  Training Loss  0.0011240871390327811\n",
            "Epoch  37 Batch  93 / 525  Training Loss  0.000248644151724875\n",
            "Epoch  37 Batch  94 / 525  Training Loss  0.0002815093321260065\n",
            "Epoch  37 Batch  95 / 525  Training Loss  0.0005739281768910587\n",
            "Epoch  37 Batch  96 / 525  Training Loss  0.0016443615313619375\n",
            "Epoch  37 Batch  97 / 525  Training Loss  0.00021286596893332899\n",
            "Epoch  37 Batch  98 / 525  Training Loss  0.00040464784251526\n",
            "Epoch  37 Batch  99 / 525  Training Loss  0.0007267051259987056\n",
            "Epoch  37 Batch  100 / 525  Training Loss  0.008947557769715786\n",
            "Epoch  37 Batch  101 / 525  Training Loss  0.0031501923222094774\n",
            "Epoch  37 Batch  102 / 525  Training Loss  0.00011425234697526321\n",
            "Epoch  37 Batch  103 / 525  Training Loss  0.0003676151973195374\n",
            "Epoch  37 Batch  104 / 525  Training Loss  0.00013444252545014024\n",
            "Epoch  37 Batch  105 / 525  Training Loss  0.0013870716793462634\n",
            "Epoch  37 Batch  106 / 525  Training Loss  0.00018255690520163625\n",
            "Epoch  37 Batch  107 / 525  Training Loss  0.0018822808051481843\n",
            "Epoch  37 Batch  108 / 525  Training Loss  0.00033044462907128036\n",
            "Epoch  37 Batch  109 / 525  Training Loss  0.00027792766923084855\n",
            "Epoch  37 Batch  110 / 525  Training Loss  0.000773586449213326\n",
            "Epoch  37 Batch  111 / 525  Training Loss  0.0004986061248928308\n",
            "Epoch  37 Batch  112 / 525  Training Loss  0.0017911052564159036\n",
            "Epoch  37 Batch  113 / 525  Training Loss  0.00022930973500479013\n",
            "Epoch  37 Batch  114 / 525  Training Loss  0.00042435768409632146\n",
            "Epoch  37 Batch  115 / 525  Training Loss  0.00020163047884125262\n",
            "Epoch  37 Batch  116 / 525  Training Loss  0.0016550257569178939\n",
            "Epoch  37 Batch  117 / 525  Training Loss  0.0005762139917351305\n",
            "Epoch  37 Batch  118 / 525  Training Loss  0.0006474486435763538\n",
            "Epoch  37 Batch  119 / 525  Training Loss  0.000698029063642025\n",
            "Epoch  37 Batch  120 / 525  Training Loss  0.0037982799112796783\n",
            "Epoch  37 Batch  121 / 525  Training Loss  0.00024592955014668405\n",
            "Epoch  37 Batch  122 / 525  Training Loss  0.00016798387514427304\n",
            "Epoch  37 Batch  123 / 525  Training Loss  0.0002547649201005697\n",
            "Epoch  37 Batch  124 / 525  Training Loss  6.0484315326903015e-05\n",
            "Epoch  37 Batch  125 / 525  Training Loss  0.0004530878795776516\n",
            "Epoch  37 Batch  126 / 525  Training Loss  0.0011333331931382418\n",
            "Epoch  37 Batch  127 / 525  Training Loss  0.0005109254270792007\n",
            "Epoch  37 Batch  128 / 525  Training Loss  0.0009645209647715092\n",
            "Epoch  37 Batch  129 / 525  Training Loss  0.0003343505668453872\n",
            "Epoch  37 Batch  130 / 525  Training Loss  0.003879354801028967\n",
            "Epoch  37 Batch  131 / 525  Training Loss  0.00034640933154150844\n",
            "Epoch  37 Batch  132 / 525  Training Loss  0.00023487626458518207\n",
            "Epoch  37 Batch  133 / 525  Training Loss  0.00023602464352734387\n",
            "Epoch  37 Batch  134 / 525  Training Loss  0.0010173976188525558\n",
            "Epoch  37 Batch  135 / 525  Training Loss  0.0002431690227240324\n",
            "Epoch  37 Batch  136 / 525  Training Loss  0.0013921065255999565\n",
            "Epoch  37 Batch  137 / 525  Training Loss  0.015410980209708214\n",
            "Epoch  37 Batch  138 / 525  Training Loss  0.00017364692757837474\n",
            "Epoch  37 Batch  139 / 525  Training Loss  0.00032797205494716763\n",
            "Epoch  37 Batch  140 / 525  Training Loss  0.0036229491233825684\n",
            "Epoch  37 Batch  141 / 525  Training Loss  0.00016290583880618215\n",
            "Epoch  37 Batch  142 / 525  Training Loss  0.0006280007655732334\n",
            "Epoch  37 Batch  143 / 525  Training Loss  0.0004451561835594475\n",
            "Epoch  37 Batch  144 / 525  Training Loss  0.0008125041495077312\n",
            "Epoch  37 Batch  145 / 525  Training Loss  0.002059964230284095\n",
            "Epoch  37 Batch  146 / 525  Training Loss  0.000854204990901053\n",
            "Epoch  37 Batch  147 / 525  Training Loss  0.0007769871153868735\n",
            "Epoch  37 Batch  148 / 525  Training Loss  0.0011605622712522745\n",
            "Epoch  37 Batch  149 / 525  Training Loss  0.0003869402571581304\n",
            "Epoch  37 Batch  150 / 525  Training Loss  0.004226981196552515\n",
            "Epoch  37 Batch  151 / 525  Training Loss  0.0008568881312385201\n",
            "Epoch  37 Batch  152 / 525  Training Loss  0.00033263821387663484\n",
            "Epoch  37 Batch  153 / 525  Training Loss  0.00032549002207815647\n",
            "Epoch  37 Batch  154 / 525  Training Loss  0.00027355196652933955\n",
            "Epoch  37 Batch  155 / 525  Training Loss  0.00017678082804195583\n",
            "Epoch  37 Batch  156 / 525  Training Loss  0.0003665172553155571\n",
            "Epoch  37 Batch  157 / 525  Training Loss  0.0012033192906528711\n",
            "Epoch  37 Batch  158 / 525  Training Loss  0.0006001412402838469\n",
            "Epoch  37 Batch  159 / 525  Training Loss  0.00468315277248621\n",
            "Epoch  37 Batch  160 / 525  Training Loss  0.00017920757818501443\n",
            "Epoch  37 Batch  161 / 525  Training Loss  0.001564373611472547\n",
            "Epoch  37 Batch  162 / 525  Training Loss  0.0013316187541931868\n",
            "Epoch  37 Batch  163 / 525  Training Loss  0.0001538172655273229\n",
            "Epoch  37 Batch  164 / 525  Training Loss  0.0008065509609878063\n",
            "Epoch  37 Batch  165 / 525  Training Loss  8.028990123420954e-05\n",
            "Epoch  37 Batch  166 / 525  Training Loss  0.0003999743494205177\n",
            "Epoch  37 Batch  167 / 525  Training Loss  0.005312611348927021\n",
            "Epoch  37 Batch  168 / 525  Training Loss  0.00016009484534151852\n",
            "Epoch  37 Batch  169 / 525  Training Loss  0.00020879266958218068\n",
            "Epoch  37 Batch  170 / 525  Training Loss  0.00044932085438631475\n",
            "Epoch  37 Batch  171 / 525  Training Loss  0.0006029740907251835\n",
            "Epoch  37 Batch  172 / 525  Training Loss  0.00048052630154415965\n",
            "Epoch  37 Batch  173 / 525  Training Loss  0.003489704802632332\n",
            "Epoch  37 Batch  174 / 525  Training Loss  0.0001595010980963707\n",
            "Epoch  37 Batch  175 / 525  Training Loss  0.0007912384462542832\n",
            "Epoch  37 Batch  176 / 525  Training Loss  0.00027614046121016145\n",
            "Epoch  37 Batch  177 / 525  Training Loss  0.0004510479629971087\n",
            "Epoch  37 Batch  178 / 525  Training Loss  0.0007088646525517106\n",
            "Epoch  37 Batch  179 / 525  Training Loss  0.00021920562721788883\n",
            "Epoch  37 Batch  180 / 525  Training Loss  0.0004538248758763075\n",
            "Epoch  37 Batch  181 / 525  Training Loss  0.00014103384455665946\n",
            "Epoch  37 Batch  182 / 525  Training Loss  0.0003468017966952175\n",
            "Epoch  37 Batch  183 / 525  Training Loss  0.0013809234369546175\n",
            "Epoch  37 Batch  184 / 525  Training Loss  0.00031168837449513376\n",
            "Epoch  37 Batch  185 / 525  Training Loss  0.00013798456348013133\n",
            "Epoch  37 Batch  186 / 525  Training Loss  0.0005794757162220776\n",
            "Epoch  37 Batch  187 / 525  Training Loss  0.0003066226199734956\n",
            "Epoch  37 Batch  188 / 525  Training Loss  0.00023941225663293153\n",
            "Epoch  37 Batch  189 / 525  Training Loss  0.00033768045250326395\n",
            "Epoch  37 Batch  190 / 525  Training Loss  0.0001421552151441574\n",
            "Epoch  37 Batch  191 / 525  Training Loss  8.648699440527707e-05\n",
            "Epoch  37 Batch  192 / 525  Training Loss  0.0008783158846199512\n",
            "Epoch  37 Batch  193 / 525  Training Loss  0.0005411177990026772\n",
            "Epoch  37 Batch  194 / 525  Training Loss  0.0014944238355383277\n",
            "Epoch  37 Batch  195 / 525  Training Loss  0.0001396533625666052\n",
            "Epoch  37 Batch  196 / 525  Training Loss  0.0002746475802268833\n",
            "Epoch  37 Batch  197 / 525  Training Loss  0.0003935721761081368\n",
            "Epoch  37 Batch  198 / 525  Training Loss  0.0018377579981461167\n",
            "Epoch  37 Batch  199 / 525  Training Loss  0.0004098037607036531\n",
            "Epoch  37 Batch  200 / 525  Training Loss  0.0005391297745518386\n",
            "Epoch  37 Batch  201 / 525  Training Loss  0.00036356886266730726\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  37 Batch  202 / 525  Training Loss  0.00010761884186649695\n",
            "Epoch  37 Batch  203 / 525  Training Loss  0.0003324580320622772\n",
            "Epoch  37 Batch  204 / 525  Training Loss  0.0009214522433467209\n",
            "Epoch  37 Batch  205 / 525  Training Loss  0.00026959198294207454\n",
            "Epoch  37 Batch  206 / 525  Training Loss  0.00037885771598666906\n",
            "Epoch  37 Batch  207 / 525  Training Loss  0.0002682879858184606\n",
            "Epoch  37 Batch  208 / 525  Training Loss  0.00022495849407278\n",
            "Epoch  37 Batch  209 / 525  Training Loss  0.0006592373829334974\n",
            "Epoch  37 Batch  210 / 525  Training Loss  0.00012956095451954752\n",
            "Epoch  37 Batch  211 / 525  Training Loss  0.00047949020517989993\n",
            "Epoch  37 Batch  212 / 525  Training Loss  0.0002921160194091499\n",
            "Epoch  37 Batch  213 / 525  Training Loss  0.00047731964150443673\n",
            "Epoch  37 Batch  214 / 525  Training Loss  0.00013085606042295694\n",
            "Epoch  37 Batch  215 / 525  Training Loss  0.0005322402575984597\n",
            "Epoch  37 Batch  216 / 525  Training Loss  0.00040016035200096667\n",
            "Epoch  37 Batch  217 / 525  Training Loss  0.0005850254092365503\n",
            "Epoch  37 Batch  218 / 525  Training Loss  0.00019974629685748369\n",
            "Epoch  37 Batch  219 / 525  Training Loss  0.00022923652431927621\n",
            "Epoch  37 Batch  220 / 525  Training Loss  0.0001971092278836295\n",
            "Epoch  37 Batch  221 / 525  Training Loss  0.00026518385857343674\n",
            "Epoch  37 Batch  222 / 525  Training Loss  0.0003829531779047102\n",
            "Epoch  37 Batch  223 / 525  Training Loss  0.0008890523458831012\n",
            "Epoch  37 Batch  224 / 525  Training Loss  0.0002831234014593065\n",
            "Epoch  37 Batch  225 / 525  Training Loss  0.00026606989558786154\n",
            "Epoch  37 Batch  226 / 525  Training Loss  0.000913353287614882\n",
            "Epoch  37 Batch  227 / 525  Training Loss  0.0002662313636392355\n",
            "Epoch  37 Batch  228 / 525  Training Loss  0.0001934280153363943\n",
            "Epoch  37 Batch  229 / 525  Training Loss  0.00012503808829933405\n",
            "Epoch  37 Batch  230 / 525  Training Loss  0.0002298688777955249\n",
            "Epoch  37 Batch  231 / 525  Training Loss  0.00022664354764856398\n",
            "Epoch  37 Batch  232 / 525  Training Loss  0.0003758103121072054\n",
            "Epoch  37 Batch  233 / 525  Training Loss  0.0001579284726176411\n",
            "Epoch  37 Batch  234 / 525  Training Loss  0.0007633337518200278\n",
            "Epoch  37 Batch  235 / 525  Training Loss  0.0007560391677543521\n",
            "Epoch  37 Batch  236 / 525  Training Loss  0.0004836249281652272\n",
            "Epoch  37 Batch  237 / 525  Training Loss  0.00016418426821473986\n",
            "Epoch  37 Batch  238 / 525  Training Loss  0.0010462681530043483\n",
            "Epoch  37 Batch  239 / 525  Training Loss  0.00013979346840642393\n",
            "Epoch  37 Batch  240 / 525  Training Loss  0.00012117669393774122\n",
            "Epoch  37 Batch  241 / 525  Training Loss  0.00015848642215132713\n",
            "Epoch  37 Batch  242 / 525  Training Loss  0.00020017963834106922\n",
            "Epoch  37 Batch  243 / 525  Training Loss  0.000945508829317987\n",
            "Epoch  37 Batch  244 / 525  Training Loss  0.00012387057358864695\n",
            "Epoch  37 Batch  245 / 525  Training Loss  0.00020326036610640585\n",
            "Epoch  37 Batch  246 / 525  Training Loss  0.0001737941347528249\n",
            "Epoch  37 Batch  247 / 525  Training Loss  0.0002015820937231183\n",
            "Epoch  37 Batch  248 / 525  Training Loss  0.000530734658241272\n",
            "Epoch  37 Batch  249 / 525  Training Loss  0.0044029997661709785\n",
            "Epoch  37 Batch  250 / 525  Training Loss  0.00034206826239824295\n",
            "Epoch  37 Batch  251 / 525  Training Loss  0.0005610140506178141\n",
            "Epoch  37 Batch  252 / 525  Training Loss  0.00019820837769657373\n",
            "Epoch  37 Batch  253 / 525  Training Loss  0.00011392360465833917\n",
            "Epoch  37 Batch  254 / 525  Training Loss  5.847365900990553e-05\n",
            "Epoch  37 Batch  255 / 525  Training Loss  0.00043361252755858004\n",
            "Epoch  37 Batch  256 / 525  Training Loss  0.00026255002012476325\n",
            "Epoch  37 Batch  257 / 525  Training Loss  0.0001753189426381141\n",
            "Epoch  37 Batch  258 / 525  Training Loss  0.00012171678099548444\n",
            "Epoch  37 Batch  259 / 525  Training Loss  0.001546572777442634\n",
            "Epoch  37 Batch  260 / 525  Training Loss  0.00023349959519691765\n",
            "Epoch  37 Batch  261 / 525  Training Loss  0.0002087027532979846\n",
            "Epoch  37 Batch  262 / 525  Training Loss  0.00024916132679209113\n",
            "Epoch  37 Batch  263 / 525  Training Loss  0.0004600461106747389\n",
            "Epoch  37 Batch  264 / 525  Training Loss  0.00025551184080541134\n",
            "Epoch  37 Batch  265 / 525  Training Loss  0.00011875158088514581\n",
            "Epoch  37 Batch  266 / 525  Training Loss  0.00043864440522156656\n",
            "Epoch  37 Batch  267 / 525  Training Loss  0.0003765863075386733\n",
            "Epoch  37 Batch  268 / 525  Training Loss  0.00023357981990557164\n",
            "Epoch  37 Batch  269 / 525  Training Loss  0.00019536924082785845\n",
            "Epoch  37 Batch  270 / 525  Training Loss  0.00031112381839193404\n",
            "Epoch  37 Batch  271 / 525  Training Loss  0.0001178784150397405\n",
            "Epoch  37 Batch  272 / 525  Training Loss  0.000105490573332645\n",
            "Epoch  37 Batch  273 / 525  Training Loss  0.0009698027861304581\n",
            "Epoch  37 Batch  274 / 525  Training Loss  0.0004564370901789516\n",
            "Epoch  37 Batch  275 / 525  Training Loss  0.0018007196485996246\n",
            "Epoch  37 Batch  276 / 525  Training Loss  0.0001536899508209899\n",
            "Epoch  37 Batch  277 / 525  Training Loss  0.00012383978173602372\n",
            "Epoch  37 Batch  278 / 525  Training Loss  0.00026988430181518197\n",
            "Epoch  37 Batch  279 / 525  Training Loss  0.0003500009188428521\n",
            "Epoch  37 Batch  280 / 525  Training Loss  7.311820809263736e-05\n",
            "Epoch  37 Batch  281 / 525  Training Loss  0.0002892513293772936\n",
            "Epoch  37 Batch  282 / 525  Training Loss  0.00010772676614578813\n",
            "Epoch  37 Batch  283 / 525  Training Loss  0.00013819635205436498\n",
            "Epoch  37 Batch  284 / 525  Training Loss  0.00027569764642976224\n",
            "Epoch  37 Batch  285 / 525  Training Loss  0.0004616529622580856\n",
            "Epoch  37 Batch  286 / 525  Training Loss  0.0006662283558398485\n",
            "Epoch  37 Batch  287 / 525  Training Loss  0.0003906837082467973\n",
            "Epoch  37 Batch  288 / 525  Training Loss  0.001835313974879682\n",
            "Epoch  37 Batch  289 / 525  Training Loss  0.0002726017846725881\n",
            "Epoch  37 Batch  290 / 525  Training Loss  0.00020647207566071302\n",
            "Epoch  37 Batch  291 / 525  Training Loss  8.972328942036256e-05\n",
            "Epoch  37 Batch  292 / 525  Training Loss  0.00012931064702570438\n",
            "Epoch  37 Batch  293 / 525  Training Loss  0.0011669371742755175\n",
            "Epoch  37 Batch  294 / 525  Training Loss  0.00017409409338142723\n",
            "Epoch  37 Batch  295 / 525  Training Loss  0.000250840384978801\n",
            "Epoch  37 Batch  296 / 525  Training Loss  0.009191959165036678\n",
            "Epoch  37 Batch  297 / 525  Training Loss  0.0007673440268263221\n",
            "Epoch  37 Batch  298 / 525  Training Loss  0.0001675066159805283\n",
            "Epoch  37 Batch  299 / 525  Training Loss  0.0001968990545719862\n",
            "Epoch  37 Batch  300 / 525  Training Loss  0.00025419617304578424\n",
            "Epoch  37 Batch  301 / 525  Training Loss  0.0001871926651801914\n",
            "Epoch  37 Batch  302 / 525  Training Loss  0.0004214851651340723\n",
            "Epoch  37 Batch  303 / 525  Training Loss  0.0006492016837000847\n",
            "Epoch  37 Batch  304 / 525  Training Loss  0.0021737345959991217\n",
            "Epoch  37 Batch  305 / 525  Training Loss  0.0005659247981384397\n",
            "Epoch  37 Batch  306 / 525  Training Loss  0.0021218247711658478\n",
            "Epoch  37 Batch  307 / 525  Training Loss  0.00022571111912839115\n",
            "Epoch  37 Batch  308 / 525  Training Loss  0.0015547126531600952\n",
            "Epoch  37 Batch  309 / 525  Training Loss  0.00021006024326197803\n",
            "Epoch  37 Batch  310 / 525  Training Loss  0.0005980433779768646\n",
            "Epoch  37 Batch  311 / 525  Training Loss  7.578384975204244e-05\n",
            "Epoch  37 Batch  312 / 525  Training Loss  0.00016543723177164793\n",
            "Epoch  37 Batch  313 / 525  Training Loss  0.0007886139792390168\n",
            "Epoch  37 Batch  314 / 525  Training Loss  0.00045486498856917024\n",
            "Epoch  37 Batch  315 / 525  Training Loss  0.00027032385696657\n",
            "Epoch  37 Batch  316 / 525  Training Loss  0.00021024435409344733\n",
            "Epoch  37 Batch  317 / 525  Training Loss  0.00011193505633855239\n",
            "Epoch  37 Batch  318 / 525  Training Loss  0.00012134174903621897\n",
            "Epoch  37 Batch  319 / 525  Training Loss  0.00043885124614462256\n",
            "Epoch  37 Batch  320 / 525  Training Loss  0.00040978711331263185\n",
            "Epoch  37 Batch  321 / 525  Training Loss  0.00017811686848290265\n",
            "Epoch  37 Batch  322 / 525  Training Loss  0.00013260635023470968\n",
            "Epoch  37 Batch  323 / 525  Training Loss  0.0001426805683877319\n",
            "Epoch  37 Batch  324 / 525  Training Loss  0.00046640090295113623\n",
            "Epoch  37 Batch  325 / 525  Training Loss  0.0003249715955462307\n",
            "Epoch  37 Batch  326 / 525  Training Loss  0.00014454493066295981\n",
            "Epoch  37 Batch  327 / 525  Training Loss  0.00015777933003846556\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  37 Batch  328 / 525  Training Loss  0.00018856141832657158\n",
            "Epoch  37 Batch  329 / 525  Training Loss  0.0002433632907923311\n",
            "Epoch  37 Batch  330 / 525  Training Loss  0.00018241317593492568\n",
            "Epoch  37 Batch  331 / 525  Training Loss  0.000587779562920332\n",
            "Epoch  37 Batch  332 / 525  Training Loss  9.719621448311955e-05\n",
            "Epoch  37 Batch  333 / 525  Training Loss  0.0003002897428814322\n",
            "Epoch  37 Batch  334 / 525  Training Loss  0.0003172121651005\n",
            "Epoch  37 Batch  335 / 525  Training Loss  0.000721592572517693\n",
            "Epoch  37 Batch  336 / 525  Training Loss  0.0007756822160445154\n",
            "Epoch  37 Batch  337 / 525  Training Loss  0.0001829319808166474\n",
            "Epoch  37 Batch  338 / 525  Training Loss  0.00014274759450927377\n",
            "Epoch  37 Batch  339 / 525  Training Loss  0.0003174261364620179\n",
            "Epoch  37 Batch  340 / 525  Training Loss  0.0001077602501027286\n",
            "Epoch  37 Batch  341 / 525  Training Loss  0.0002762854564934969\n",
            "Epoch  37 Batch  342 / 525  Training Loss  0.000906123430468142\n",
            "Epoch  37 Batch  343 / 525  Training Loss  0.0006423150189220905\n",
            "Epoch  37 Batch  344 / 525  Training Loss  0.00044086528941988945\n",
            "Epoch  37 Batch  345 / 525  Training Loss  0.000399444397771731\n",
            "Epoch  37 Batch  346 / 525  Training Loss  0.00038852967554703355\n",
            "Epoch  37 Batch  347 / 525  Training Loss  0.0005730364937335253\n",
            "Epoch  37 Batch  348 / 525  Training Loss  0.00039522055885754526\n",
            "Epoch  37 Batch  349 / 525  Training Loss  7.24185083527118e-05\n",
            "Epoch  37 Batch  350 / 525  Training Loss  6.029617361491546e-05\n",
            "Epoch  37 Batch  351 / 525  Training Loss  9.798358951229602e-05\n",
            "Epoch  37 Batch  352 / 525  Training Loss  0.0006100476020947099\n",
            "Epoch  37 Batch  353 / 525  Training Loss  4.503282980294898e-05\n",
            "Epoch  37 Batch  354 / 525  Training Loss  0.0001837007876019925\n",
            "Epoch  37 Batch  355 / 525  Training Loss  0.0003638816997408867\n",
            "Epoch  37 Batch  356 / 525  Training Loss  0.002195677487179637\n",
            "Epoch  37 Batch  357 / 525  Training Loss  0.00014959165127947927\n",
            "Epoch  37 Batch  358 / 525  Training Loss  0.00018306143465451896\n",
            "Epoch  37 Batch  359 / 525  Training Loss  0.00033065141178667545\n",
            "Epoch  37 Batch  360 / 525  Training Loss  0.0011250305688008666\n",
            "Epoch  37 Batch  361 / 525  Training Loss  0.00012429137132130563\n",
            "Epoch  37 Batch  362 / 525  Training Loss  0.000653071328997612\n",
            "Epoch  37 Batch  363 / 525  Training Loss  0.0002699913748074323\n",
            "Epoch  37 Batch  364 / 525  Training Loss  0.00012894521933048964\n",
            "Epoch  37 Batch  365 / 525  Training Loss  0.00012232708104420453\n",
            "Epoch  37 Batch  366 / 525  Training Loss  0.0001953184837475419\n",
            "Epoch  37 Batch  367 / 525  Training Loss  0.0002009398303925991\n",
            "Epoch  37 Batch  368 / 525  Training Loss  0.00032408908009529114\n",
            "Epoch  37 Batch  369 / 525  Training Loss  0.0002492921776138246\n",
            "Epoch  37 Batch  370 / 525  Training Loss  0.00024481487344019115\n",
            "Epoch  37 Batch  371 / 525  Training Loss  0.00016325114120263606\n",
            "Epoch  37 Batch  372 / 525  Training Loss  0.013457459397614002\n",
            "Epoch  37 Batch  373 / 525  Training Loss  0.004507662262767553\n",
            "Epoch  37 Batch  374 / 525  Training Loss  0.0002620436134748161\n",
            "Epoch  37 Batch  375 / 525  Training Loss  0.00017896182544063777\n",
            "Epoch  37 Batch  376 / 525  Training Loss  0.0009642738732509315\n",
            "Epoch  37 Batch  377 / 525  Training Loss  0.0002859334053937346\n",
            "Epoch  37 Batch  378 / 525  Training Loss  0.0005738448817282915\n",
            "Epoch  37 Batch  379 / 525  Training Loss  0.00020181576837785542\n",
            "Epoch  37 Batch  380 / 525  Training Loss  0.005145496688783169\n",
            "Epoch  37 Batch  381 / 525  Training Loss  0.0005278713651932776\n",
            "Epoch  37 Batch  382 / 525  Training Loss  0.0004511609731707722\n",
            "Epoch  37 Batch  383 / 525  Training Loss  0.00016076747851911932\n",
            "Epoch  37 Batch  384 / 525  Training Loss  0.00019852157856803387\n",
            "Epoch  37 Batch  385 / 525  Training Loss  0.00033684971276670694\n",
            "Epoch  37 Batch  386 / 525  Training Loss  0.00012148487439844757\n",
            "Epoch  37 Batch  387 / 525  Training Loss  0.00014437493518926203\n",
            "Epoch  37 Batch  388 / 525  Training Loss  0.0009324393467977643\n",
            "Epoch  37 Batch  389 / 525  Training Loss  0.00023229108774103224\n",
            "Epoch  37 Batch  390 / 525  Training Loss  0.0006961474427953362\n",
            "Epoch  37 Batch  391 / 525  Training Loss  9.080475138034672e-05\n",
            "Epoch  37 Batch  392 / 525  Training Loss  0.0009637417970225215\n",
            "Epoch  37 Batch  393 / 525  Training Loss  0.00033151902607642114\n",
            "Epoch  37 Batch  394 / 525  Training Loss  0.0004656640812754631\n",
            "Epoch  37 Batch  395 / 525  Training Loss  0.0008606376359239221\n",
            "Epoch  37 Batch  396 / 525  Training Loss  0.00016468313697259873\n",
            "Epoch  37 Batch  397 / 525  Training Loss  0.00024955783737823367\n",
            "Epoch  37 Batch  398 / 525  Training Loss  0.0007078495109453797\n",
            "Epoch  37 Batch  399 / 525  Training Loss  0.0003889081417582929\n",
            "Epoch  37 Batch  400 / 525  Training Loss  0.0001825144572649151\n",
            "Epoch  37 Batch  401 / 525  Training Loss  0.00013549895083997399\n",
            "Epoch  37 Batch  402 / 525  Training Loss  0.0003111733531113714\n",
            "Epoch  37 Batch  403 / 525  Training Loss  0.0006839631823822856\n",
            "Epoch  37 Batch  404 / 525  Training Loss  0.0002848330477718264\n",
            "Epoch  37 Batch  405 / 525  Training Loss  0.00022652065672446042\n",
            "Epoch  37 Batch  406 / 525  Training Loss  0.0001493714953539893\n",
            "Epoch  37 Batch  407 / 525  Training Loss  0.002274612430483103\n",
            "Epoch  37 Batch  408 / 525  Training Loss  0.0005547722685150802\n",
            "Epoch  37 Batch  409 / 525  Training Loss  0.0002597809652797878\n",
            "Epoch  37 Batch  410 / 525  Training Loss  0.0007223503780551255\n",
            "Epoch  37 Batch  411 / 525  Training Loss  0.0001517589553259313\n",
            "Epoch  37 Batch  412 / 525  Training Loss  0.00045654410496354103\n",
            "Epoch  37 Batch  413 / 525  Training Loss  0.00035160608240403235\n",
            "Epoch  37 Batch  414 / 525  Training Loss  0.0007638413808308542\n",
            "Epoch  37 Batch  415 / 525  Training Loss  0.0001976160128833726\n",
            "Epoch  37 Batch  416 / 525  Training Loss  0.00016304291784763336\n",
            "Epoch  37 Batch  417 / 525  Training Loss  0.0009417483815923333\n",
            "Epoch  37 Batch  418 / 525  Training Loss  0.00033463159343227744\n",
            "Epoch  37 Batch  419 / 525  Training Loss  0.0002696220180951059\n",
            "Epoch  37 Batch  420 / 525  Training Loss  8.321232598973438e-05\n",
            "Epoch  37 Batch  421 / 525  Training Loss  0.0017906377324834466\n",
            "Epoch  37 Batch  422 / 525  Training Loss  0.00031060242326930165\n",
            "Epoch  37 Batch  423 / 525  Training Loss  0.0012914116960018873\n",
            "Epoch  37 Batch  424 / 525  Training Loss  0.00022058332979213446\n",
            "Epoch  37 Batch  425 / 525  Training Loss  0.0007400811882689595\n",
            "Epoch  37 Batch  426 / 525  Training Loss  0.00040098768658936024\n",
            "Epoch  37 Batch  427 / 525  Training Loss  0.00031770142959430814\n",
            "Epoch  37 Batch  428 / 525  Training Loss  0.0002977481053676456\n",
            "Epoch  37 Batch  429 / 525  Training Loss  0.005355612374842167\n",
            "Epoch  37 Batch  430 / 525  Training Loss  0.00038946321001276374\n",
            "Epoch  37 Batch  431 / 525  Training Loss  0.0013088721316307783\n",
            "Epoch  37 Batch  432 / 525  Training Loss  0.00021330818708520383\n",
            "Epoch  37 Batch  433 / 525  Training Loss  0.00021012006618548185\n",
            "Epoch  37 Batch  434 / 525  Training Loss  0.00020235012925695628\n",
            "Epoch  37 Batch  435 / 525  Training Loss  0.0013387077488005161\n",
            "Epoch  37 Batch  436 / 525  Training Loss  0.0010188299929723144\n",
            "Epoch  37 Batch  437 / 525  Training Loss  0.0008238484151661396\n",
            "Epoch  37 Batch  438 / 525  Training Loss  0.0002502013521734625\n",
            "Epoch  37 Batch  439 / 525  Training Loss  0.0007396972505375743\n",
            "Epoch  37 Batch  440 / 525  Training Loss  0.0002917639212682843\n",
            "Epoch  37 Batch  441 / 525  Training Loss  0.0005239637102931738\n",
            "Epoch  37 Batch  442 / 525  Training Loss  0.0009876734111458063\n",
            "Epoch  37 Batch  443 / 525  Training Loss  0.0003005935868714005\n",
            "Epoch  37 Batch  444 / 525  Training Loss  0.00010796314745675772\n",
            "Epoch  37 Batch  445 / 525  Training Loss  0.0007773722754791379\n",
            "Epoch  37 Batch  446 / 525  Training Loss  0.001336076995357871\n",
            "Epoch  37 Batch  447 / 525  Training Loss  0.0009713359177112579\n",
            "Epoch  37 Batch  448 / 525  Training Loss  0.00026544855791144073\n",
            "Epoch  37 Batch  449 / 525  Training Loss  0.0007726217154413462\n",
            "Epoch  37 Batch  450 / 525  Training Loss  0.00040201121009886265\n",
            "Epoch  37 Batch  451 / 525  Training Loss  0.0020172689110040665\n",
            "Epoch  37 Batch  452 / 525  Training Loss  0.002049100585281849\n",
            "Epoch  37 Batch  453 / 525  Training Loss  0.0006586579838767648\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  37 Batch  454 / 525  Training Loss  0.0020138791296631098\n",
            "Epoch  37 Batch  455 / 525  Training Loss  0.0005681312177330256\n",
            "Epoch  37 Batch  456 / 525  Training Loss  0.003743234556168318\n",
            "Epoch  37 Batch  457 / 525  Training Loss  0.0018826648592948914\n",
            "Epoch  37 Batch  458 / 525  Training Loss  0.0010804061312228441\n",
            "Epoch  37 Batch  459 / 525  Training Loss  0.00020331339328549802\n",
            "Epoch  37 Batch  460 / 525  Training Loss  0.0002151761727873236\n",
            "Epoch  37 Batch  461 / 525  Training Loss  0.0005363457603380084\n",
            "Epoch  37 Batch  462 / 525  Training Loss  0.0002510282793082297\n",
            "Epoch  37 Batch  463 / 525  Training Loss  0.0011878248769789934\n",
            "Epoch  37 Batch  464 / 525  Training Loss  0.0002738557814154774\n",
            "Epoch  37 Batch  465 / 525  Training Loss  0.0001642573333811015\n",
            "Epoch  37 Batch  466 / 525  Training Loss  0.0003225780965294689\n",
            "Epoch  37 Batch  467 / 525  Training Loss  0.00022700728732161224\n",
            "Epoch  37 Batch  468 / 525  Training Loss  0.0002086970052914694\n",
            "Epoch  37 Batch  469 / 525  Training Loss  0.0007222481071949005\n",
            "Epoch  37 Batch  470 / 525  Training Loss  0.00010334112448617816\n",
            "Epoch  37 Batch  471 / 525  Training Loss  0.0004260036221239716\n",
            "Epoch  37 Batch  472 / 525  Training Loss  0.00198323093354702\n",
            "Epoch  37 Batch  473 / 525  Training Loss  0.001024488708935678\n",
            "Epoch  37 Batch  474 / 525  Training Loss  0.0007238929392769933\n",
            "Epoch  37 Batch  475 / 525  Training Loss  0.0005025409045629203\n",
            "Epoch  37 Batch  476 / 525  Training Loss  0.0005360877839848399\n",
            "Epoch  37 Batch  477 / 525  Training Loss  0.00011733672727132216\n",
            "Epoch  37 Batch  478 / 525  Training Loss  0.00020678930741269141\n",
            "Epoch  37 Batch  479 / 525  Training Loss  0.0008857686189003289\n",
            "Epoch  37 Batch  480 / 525  Training Loss  7.879076292738318e-05\n",
            "Epoch  37 Batch  481 / 525  Training Loss  0.00018168933456763625\n",
            "Epoch  37 Batch  482 / 525  Training Loss  0.0019429782405495644\n",
            "Epoch  37 Batch  483 / 525  Training Loss  0.004435613751411438\n",
            "Epoch  37 Batch  484 / 525  Training Loss  0.00017851895245257765\n",
            "Epoch  37 Batch  485 / 525  Training Loss  0.0003668964491225779\n",
            "Epoch  37 Batch  486 / 525  Training Loss  0.00022732900106348097\n",
            "Epoch  37 Batch  487 / 525  Training Loss  0.00025394136901013553\n",
            "Epoch  37 Batch  488 / 525  Training Loss  0.0001722052984405309\n",
            "Epoch  37 Batch  489 / 525  Training Loss  0.001759847393259406\n",
            "Epoch  37 Batch  490 / 525  Training Loss  0.00035852775909006596\n",
            "Epoch  37 Batch  491 / 525  Training Loss  0.00047353110858239233\n",
            "Epoch  37 Batch  492 / 525  Training Loss  0.0008643360924907029\n",
            "Epoch  37 Batch  493 / 525  Training Loss  0.00017074296192731708\n",
            "Epoch  37 Batch  494 / 525  Training Loss  0.00023449418949894607\n",
            "Epoch  37 Batch  495 / 525  Training Loss  0.0054242294281721115\n",
            "Epoch  37 Batch  496 / 525  Training Loss  0.0013637393712997437\n",
            "Epoch  37 Batch  497 / 525  Training Loss  0.0002673228154890239\n",
            "Epoch  37 Batch  498 / 525  Training Loss  0.00014264092897064984\n",
            "Epoch  37 Batch  499 / 525  Training Loss  0.0001938587665790692\n",
            "Epoch  37 Batch  500 / 525  Training Loss  0.0011253494303673506\n",
            "Epoch  37 Batch  501 / 525  Training Loss  0.0015460618305951357\n",
            "Epoch  37 Batch  502 / 525  Training Loss  0.0006797536043450236\n",
            "Epoch  37 Batch  503 / 525  Training Loss  0.000126689687022008\n",
            "Epoch  37 Batch  504 / 525  Training Loss  0.00039099258719943464\n",
            "Epoch  37 Batch  505 / 525  Training Loss  0.00024061180010903627\n",
            "Epoch  37 Batch  506 / 525  Training Loss  0.0001313058310188353\n",
            "Epoch  37 Batch  507 / 525  Training Loss  0.0002406888670520857\n",
            "Epoch  37 Batch  508 / 525  Training Loss  0.0003150583361275494\n",
            "Epoch  37 Batch  509 / 525  Training Loss  0.0006173080182634294\n",
            "Epoch  37 Batch  510 / 525  Training Loss  0.00024029446649365127\n",
            "Epoch  37 Batch  511 / 525  Training Loss  0.0004426332307048142\n",
            "Epoch  37 Batch  512 / 525  Training Loss  0.00026611710200086236\n",
            "Epoch  37 Batch  513 / 525  Training Loss  0.0001094330073101446\n",
            "Epoch  37 Batch  514 / 525  Training Loss  0.00023311315453611314\n",
            "Epoch  37 Batch  515 / 525  Training Loss  0.0002998445415869355\n",
            "Epoch  37 Batch  516 / 525  Training Loss  0.00042144983308389783\n",
            "Epoch  37 Batch  517 / 525  Training Loss  0.002324734115973115\n",
            "Epoch  37 Batch  518 / 525  Training Loss  0.0003006777842529118\n",
            "Epoch  37 Batch  519 / 525  Training Loss  0.0002985837054438889\n",
            "Epoch  37 Batch  520 / 525  Training Loss  0.0001446008391212672\n",
            "Epoch  37 Batch  521 / 525  Training Loss  0.0014596821274608374\n",
            "Epoch  37 Batch  522 / 525  Training Loss  0.0006264530238695443\n",
            "Epoch  37 Batch  523 / 525  Training Loss  0.00033553625689819455\n",
            "Epoch  37 Batch  524 / 525  Training Loss  0.00019645915017463267\n",
            "  38    |    -    |   0.000733   | 63.266667\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 38\n",
            "Epoch  38 Batch  0 / 525  Training Loss  0.0002062205458059907\n",
            "Epoch  38 Batch  1 / 525  Training Loss  0.0011466869618743658\n",
            "Epoch  38 Batch  2 / 525  Training Loss  0.0005528704496100545\n",
            "Epoch  38 Batch  3 / 525  Training Loss  0.00012077789142495021\n",
            "Epoch  38 Batch  4 / 525  Training Loss  0.0038966331630945206\n",
            "Epoch  38 Batch  5 / 525  Training Loss  0.0002480405673850328\n",
            "Epoch  38 Batch  6 / 525  Training Loss  0.00016155400953721255\n",
            "Epoch  38 Batch  7 / 525  Training Loss  0.00010433364514028654\n",
            "Epoch  38 Batch  8 / 525  Training Loss  0.0002594179823063314\n",
            "Epoch  38 Batch  9 / 525  Training Loss  0.00018653608276508749\n",
            "Epoch  38 Batch  10 / 525  Training Loss  0.00013114188914187253\n",
            "Epoch  38 Batch  11 / 525  Training Loss  0.0001137635117629543\n",
            "Epoch  38 Batch  12 / 525  Training Loss  0.00015427006292156875\n",
            "Epoch  38 Batch  13 / 525  Training Loss  0.00017824271344579756\n",
            "Epoch  38 Batch  14 / 525  Training Loss  0.00022520750644616783\n",
            "Epoch  38 Batch  15 / 525  Training Loss  0.00017367575492244214\n",
            "Epoch  38 Batch  16 / 525  Training Loss  0.0002030485775321722\n",
            "Epoch  38 Batch  17 / 525  Training Loss  0.00013917226169724017\n",
            "Epoch  38 Batch  18 / 525  Training Loss  0.00023590440105181187\n",
            "Epoch  38 Batch  19 / 525  Training Loss  0.00022610164887737483\n",
            "Epoch  38 Batch  20 / 525  Training Loss  0.00016272383800242096\n",
            "Epoch  38 Batch  21 / 525  Training Loss  0.00026358303148299456\n",
            "Epoch  38 Batch  22 / 525  Training Loss  0.0004244333249516785\n",
            "Epoch  38 Batch  23 / 525  Training Loss  0.00012159861216787249\n",
            "Epoch  38 Batch  24 / 525  Training Loss  0.00021555231069214642\n",
            "Epoch  38 Batch  25 / 525  Training Loss  0.0004049593990202993\n",
            "Epoch  38 Batch  26 / 525  Training Loss  0.00022160066873766482\n",
            "Epoch  38 Batch  27 / 525  Training Loss  7.70183905842714e-05\n",
            "Epoch  38 Batch  28 / 525  Training Loss  5.9689704357879236e-05\n",
            "Epoch  38 Batch  29 / 525  Training Loss  9.0183770225849e-05\n",
            "Epoch  38 Batch  30 / 525  Training Loss  0.0004721144796349108\n",
            "Epoch  38 Batch  31 / 525  Training Loss  0.00010438065510243177\n",
            "Epoch  38 Batch  32 / 525  Training Loss  0.00013993489847052842\n",
            "Epoch  38 Batch  33 / 525  Training Loss  8.590648940298706e-05\n",
            "Epoch  38 Batch  34 / 525  Training Loss  0.00019278527179267257\n",
            "Epoch  38 Batch  35 / 525  Training Loss  7.819289749022573e-05\n",
            "Epoch  38 Batch  36 / 525  Training Loss  8.552962390240282e-05\n",
            "Epoch  38 Batch  37 / 525  Training Loss  9.136500739259645e-05\n",
            "Epoch  38 Batch  38 / 525  Training Loss  0.0002727393002714962\n",
            "Epoch  38 Batch  39 / 525  Training Loss  0.00010223542631138116\n",
            "Epoch  38 Batch  40 / 525  Training Loss  7.801257015671581e-05\n",
            "Epoch  38 Batch  41 / 525  Training Loss  0.0002027848531724885\n",
            "Epoch  38 Batch  42 / 525  Training Loss  0.00012141183105995879\n",
            "Epoch  38 Batch  43 / 525  Training Loss  0.00017957965610548854\n",
            "Epoch  38 Batch  44 / 525  Training Loss  2.3643486201763153e-05\n",
            "Epoch  38 Batch  45 / 525  Training Loss  3.063554686377756e-05\n",
            "Epoch  38 Batch  46 / 525  Training Loss  9.838724508881569e-05\n",
            "Epoch  38 Batch  47 / 525  Training Loss  7.13134795660153e-05\n",
            "Epoch  38 Batch  48 / 525  Training Loss  7.861475751269609e-05\n",
            "Epoch  38 Batch  49 / 525  Training Loss  0.00016376323765143752\n",
            "Epoch  38 Batch  50 / 525  Training Loss  0.0001353561965515837\n",
            "Epoch  38 Batch  51 / 525  Training Loss  0.001018401701003313\n",
            "Epoch  38 Batch  52 / 525  Training Loss  0.01422045100480318\n",
            "Epoch  38 Batch  53 / 525  Training Loss  0.00011660432210192084\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  38 Batch  54 / 525  Training Loss  0.0005290919216349721\n",
            "Epoch  38 Batch  55 / 525  Training Loss  0.00021635449957102537\n",
            "Epoch  38 Batch  56 / 525  Training Loss  0.00041715038241818547\n",
            "Epoch  38 Batch  57 / 525  Training Loss  8.252706174971536e-05\n",
            "Epoch  38 Batch  58 / 525  Training Loss  5.4090785852167755e-05\n",
            "Epoch  38 Batch  59 / 525  Training Loss  0.0005031152395531535\n",
            "Epoch  38 Batch  60 / 525  Training Loss  0.0001888985570985824\n",
            "Epoch  38 Batch  61 / 525  Training Loss  0.00011451662430772558\n",
            "Epoch  38 Batch  62 / 525  Training Loss  0.00011107920727226883\n",
            "Epoch  38 Batch  63 / 525  Training Loss  0.00011508948227856308\n",
            "Epoch  38 Batch  64 / 525  Training Loss  9.263776883017272e-05\n",
            "Epoch  38 Batch  65 / 525  Training Loss  0.0006715680938214064\n",
            "Epoch  38 Batch  66 / 525  Training Loss  0.0001432210556231439\n",
            "Epoch  38 Batch  67 / 525  Training Loss  0.00038594179204665124\n",
            "Epoch  38 Batch  68 / 525  Training Loss  0.00025712145725265145\n",
            "Epoch  38 Batch  69 / 525  Training Loss  0.00018907335470430553\n",
            "Epoch  38 Batch  70 / 525  Training Loss  0.0002014069614233449\n",
            "Epoch  38 Batch  71 / 525  Training Loss  0.00047009700210765004\n",
            "Epoch  38 Batch  72 / 525  Training Loss  0.00014233867113944143\n",
            "Epoch  38 Batch  73 / 525  Training Loss  0.00017553448560647666\n",
            "Epoch  38 Batch  74 / 525  Training Loss  0.00012177957978565246\n",
            "Epoch  38 Batch  75 / 525  Training Loss  0.00016574440815020353\n",
            "Epoch  38 Batch  76 / 525  Training Loss  0.0004618805833160877\n",
            "Epoch  38 Batch  77 / 525  Training Loss  4.470357816899195e-05\n",
            "Epoch  38 Batch  78 / 525  Training Loss  0.00014239635493140668\n",
            "Epoch  38 Batch  79 / 525  Training Loss  0.00025720574194565415\n",
            "Epoch  38 Batch  80 / 525  Training Loss  0.000649522349704057\n",
            "Epoch  38 Batch  81 / 525  Training Loss  8.239001908805221e-05\n",
            "Epoch  38 Batch  82 / 525  Training Loss  6.100738391978666e-05\n",
            "Epoch  38 Batch  83 / 525  Training Loss  0.00022973907471168786\n",
            "Epoch  38 Batch  84 / 525  Training Loss  0.00020909700833726674\n",
            "Epoch  38 Batch  85 / 525  Training Loss  4.348265429143794e-05\n",
            "Epoch  38 Batch  86 / 525  Training Loss  0.00020434339239727706\n",
            "Epoch  38 Batch  87 / 525  Training Loss  9.660121577326208e-05\n",
            "Epoch  38 Batch  88 / 525  Training Loss  0.00033758528297767043\n",
            "Epoch  38 Batch  89 / 525  Training Loss  0.00012053458340233192\n",
            "Epoch  38 Batch  90 / 525  Training Loss  3.553343776729889e-05\n",
            "Epoch  38 Batch  91 / 525  Training Loss  0.000195244763744995\n",
            "Epoch  38 Batch  92 / 525  Training Loss  0.00016239470278378576\n",
            "Epoch  38 Batch  93 / 525  Training Loss  0.000423432735260576\n",
            "Epoch  38 Batch  94 / 525  Training Loss  0.00012878315465059131\n",
            "Epoch  38 Batch  95 / 525  Training Loss  0.00010518597991904244\n",
            "Epoch  38 Batch  96 / 525  Training Loss  0.0001505508553236723\n",
            "Epoch  38 Batch  97 / 525  Training Loss  0.00010777462739497423\n",
            "Epoch  38 Batch  98 / 525  Training Loss  0.00010325519542675465\n",
            "Epoch  38 Batch  99 / 525  Training Loss  0.00012771639740094543\n",
            "Epoch  38 Batch  100 / 525  Training Loss  0.003401403082534671\n",
            "Epoch  38 Batch  101 / 525  Training Loss  6.587391544599086e-05\n",
            "Epoch  38 Batch  102 / 525  Training Loss  0.0018202593782916665\n",
            "Epoch  38 Batch  103 / 525  Training Loss  0.000496187771204859\n",
            "Epoch  38 Batch  104 / 525  Training Loss  0.0006834001396782696\n",
            "Epoch  38 Batch  105 / 525  Training Loss  0.00010528757411520928\n",
            "Epoch  38 Batch  106 / 525  Training Loss  0.00018114197882823646\n",
            "Epoch  38 Batch  107 / 525  Training Loss  0.00022031681146472692\n",
            "Epoch  38 Batch  108 / 525  Training Loss  0.00014825037214905024\n",
            "Epoch  38 Batch  109 / 525  Training Loss  0.00033117952989414334\n",
            "Epoch  38 Batch  110 / 525  Training Loss  0.00018039561109617352\n",
            "Epoch  38 Batch  111 / 525  Training Loss  0.000201530841877684\n",
            "Epoch  38 Batch  112 / 525  Training Loss  0.0001589323510415852\n",
            "Epoch  38 Batch  113 / 525  Training Loss  7.714536332059652e-05\n",
            "Epoch  38 Batch  114 / 525  Training Loss  0.00017170909268315881\n",
            "Epoch  38 Batch  115 / 525  Training Loss  0.00012078697182005271\n",
            "Epoch  38 Batch  116 / 525  Training Loss  0.00021947771892882884\n",
            "Epoch  38 Batch  117 / 525  Training Loss  8.611094381194562e-05\n",
            "Epoch  38 Batch  118 / 525  Training Loss  0.0007521483930759132\n",
            "Epoch  38 Batch  119 / 525  Training Loss  0.0005100014386698604\n",
            "Epoch  38 Batch  120 / 525  Training Loss  9.657279588282108e-05\n",
            "Epoch  38 Batch  121 / 525  Training Loss  7.60347320465371e-05\n",
            "Epoch  38 Batch  122 / 525  Training Loss  0.00017781603673938662\n",
            "Epoch  38 Batch  123 / 525  Training Loss  0.00011327266838634387\n",
            "Epoch  38 Batch  124 / 525  Training Loss  0.00023968672030605376\n",
            "Epoch  38 Batch  125 / 525  Training Loss  0.00020583928562700748\n",
            "Epoch  38 Batch  126 / 525  Training Loss  7.496143371099606e-05\n",
            "Epoch  38 Batch  127 / 525  Training Loss  0.0012781890109181404\n",
            "Epoch  38 Batch  128 / 525  Training Loss  0.00013319056597538292\n",
            "Epoch  38 Batch  129 / 525  Training Loss  0.00044585653813555837\n",
            "Epoch  38 Batch  130 / 525  Training Loss  0.00010260604176437482\n",
            "Epoch  38 Batch  131 / 525  Training Loss  0.00011697829177137464\n",
            "Epoch  38 Batch  132 / 525  Training Loss  0.0002142653102055192\n",
            "Epoch  38 Batch  133 / 525  Training Loss  8.940357656683773e-05\n",
            "Epoch  38 Batch  134 / 525  Training Loss  0.00021760577510576695\n",
            "Epoch  38 Batch  135 / 525  Training Loss  6.608423427678645e-05\n",
            "Epoch  38 Batch  136 / 525  Training Loss  0.00013434413995128125\n",
            "Epoch  38 Batch  137 / 525  Training Loss  0.0002680380712263286\n",
            "Epoch  38 Batch  138 / 525  Training Loss  8.485781290801242e-05\n",
            "Epoch  38 Batch  139 / 525  Training Loss  0.00011610884394031018\n",
            "Epoch  38 Batch  140 / 525  Training Loss  0.00020362998475320637\n",
            "Epoch  38 Batch  141 / 525  Training Loss  9.119851893046871e-05\n",
            "Epoch  38 Batch  142 / 525  Training Loss  7.234422082547098e-05\n",
            "Epoch  38 Batch  143 / 525  Training Loss  5.4306543461279944e-05\n",
            "Epoch  38 Batch  144 / 525  Training Loss  6.346414738800377e-05\n",
            "Epoch  38 Batch  145 / 525  Training Loss  0.00041278329445049167\n",
            "Epoch  38 Batch  146 / 525  Training Loss  0.0004028685507364571\n",
            "Epoch  38 Batch  147 / 525  Training Loss  7.646100129932165e-05\n",
            "Epoch  38 Batch  148 / 525  Training Loss  9.923692414304242e-05\n",
            "Epoch  38 Batch  149 / 525  Training Loss  9.754499478731304e-05\n",
            "Epoch  38 Batch  150 / 525  Training Loss  0.0001489285205025226\n",
            "Epoch  38 Batch  151 / 525  Training Loss  9.955216955859214e-05\n",
            "Epoch  38 Batch  152 / 525  Training Loss  5.677709850715473e-05\n",
            "Epoch  38 Batch  153 / 525  Training Loss  0.00013035164738539606\n",
            "Epoch  38 Batch  154 / 525  Training Loss  5.3402804041979834e-05\n",
            "Epoch  38 Batch  155 / 525  Training Loss  0.000979932607151568\n",
            "Epoch  38 Batch  156 / 525  Training Loss  0.00027084763860329986\n",
            "Epoch  38 Batch  157 / 525  Training Loss  0.0006713004549965262\n",
            "Epoch  38 Batch  158 / 525  Training Loss  0.0004981489037163556\n",
            "Epoch  38 Batch  159 / 525  Training Loss  0.0001551700261188671\n",
            "Epoch  38 Batch  160 / 525  Training Loss  0.00011750224803108722\n",
            "Epoch  38 Batch  161 / 525  Training Loss  0.00023338012397289276\n",
            "Epoch  38 Batch  162 / 525  Training Loss  2.814972867781762e-05\n",
            "Epoch  38 Batch  163 / 525  Training Loss  0.00010498259507585317\n",
            "Epoch  38 Batch  164 / 525  Training Loss  0.00017984697478823364\n",
            "Epoch  38 Batch  165 / 525  Training Loss  0.0001245210150955245\n",
            "Epoch  38 Batch  166 / 525  Training Loss  0.0002139039133908227\n",
            "Epoch  38 Batch  167 / 525  Training Loss  0.00023382555809803307\n",
            "Epoch  38 Batch  168 / 525  Training Loss  0.0001335448760073632\n",
            "Epoch  38 Batch  169 / 525  Training Loss  0.00019785106997005641\n",
            "Epoch  38 Batch  170 / 525  Training Loss  0.0015913776587694883\n",
            "Epoch  38 Batch  171 / 525  Training Loss  5.017925286665559e-05\n",
            "Epoch  38 Batch  172 / 525  Training Loss  0.00022110293502919376\n",
            "Epoch  38 Batch  173 / 525  Training Loss  3.472993557807058e-05\n",
            "Epoch  38 Batch  174 / 525  Training Loss  0.00017703851335681975\n",
            "Epoch  38 Batch  175 / 525  Training Loss  7.721011934336275e-05\n",
            "Epoch  38 Batch  176 / 525  Training Loss  0.0001382925547659397\n",
            "Epoch  38 Batch  177 / 525  Training Loss  7.430864206980914e-05\n",
            "Epoch  38 Batch  178 / 525  Training Loss  0.00016201692051254213\n",
            "Epoch  38 Batch  179 / 525  Training Loss  0.00016109980060718954\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  38 Batch  180 / 525  Training Loss  9.86882732831873e-05\n",
            "Epoch  38 Batch  181 / 525  Training Loss  0.00011291357077425346\n",
            "Epoch  38 Batch  182 / 525  Training Loss  2.493576903361827e-05\n",
            "Epoch  38 Batch  183 / 525  Training Loss  3.9771810406818986e-05\n",
            "Epoch  38 Batch  184 / 525  Training Loss  0.00014907258446328342\n",
            "Epoch  38 Batch  185 / 525  Training Loss  6.044789188308641e-05\n",
            "Epoch  38 Batch  186 / 525  Training Loss  5.291857087286189e-05\n",
            "Epoch  38 Batch  187 / 525  Training Loss  0.00010872444545384496\n",
            "Epoch  38 Batch  188 / 525  Training Loss  0.0001227612519869581\n",
            "Epoch  38 Batch  189 / 525  Training Loss  0.00029130533221177757\n",
            "Epoch  38 Batch  190 / 525  Training Loss  0.00011790126154664904\n",
            "Epoch  38 Batch  191 / 525  Training Loss  7.884440128691494e-05\n",
            "Epoch  38 Batch  192 / 525  Training Loss  2.9554041248047724e-05\n",
            "Epoch  38 Batch  193 / 525  Training Loss  0.00014602838200517\n",
            "Epoch  38 Batch  194 / 525  Training Loss  9.829956252360716e-05\n",
            "Epoch  38 Batch  195 / 525  Training Loss  0.0001095558181987144\n",
            "Epoch  38 Batch  196 / 525  Training Loss  0.00011863140389323235\n",
            "Epoch  38 Batch  197 / 525  Training Loss  0.00013151313760317862\n",
            "Epoch  38 Batch  198 / 525  Training Loss  9.939666779246181e-05\n",
            "Epoch  38 Batch  199 / 525  Training Loss  0.00018813823407981545\n",
            "Epoch  38 Batch  200 / 525  Training Loss  0.00010095826291944832\n",
            "Epoch  38 Batch  201 / 525  Training Loss  0.00010904122609645128\n",
            "Epoch  38 Batch  202 / 525  Training Loss  0.00018462134175933897\n",
            "Epoch  38 Batch  203 / 525  Training Loss  0.0003399330307729542\n",
            "Epoch  38 Batch  204 / 525  Training Loss  5.409892764873803e-05\n",
            "Epoch  38 Batch  205 / 525  Training Loss  0.00011774218000937253\n",
            "Epoch  38 Batch  206 / 525  Training Loss  0.00017117473180405796\n",
            "Epoch  38 Batch  207 / 525  Training Loss  0.00019229846657253802\n",
            "Epoch  38 Batch  208 / 525  Training Loss  0.0007167091825976968\n",
            "Epoch  38 Batch  209 / 525  Training Loss  0.00015118351439014077\n",
            "Epoch  38 Batch  210 / 525  Training Loss  0.0002449120511300862\n",
            "Epoch  38 Batch  211 / 525  Training Loss  7.599691889481619e-05\n",
            "Epoch  38 Batch  212 / 525  Training Loss  0.0001231282076332718\n",
            "Epoch  38 Batch  213 / 525  Training Loss  7.809521775925532e-05\n",
            "Epoch  38 Batch  214 / 525  Training Loss  3.824933082796633e-05\n",
            "Epoch  38 Batch  215 / 525  Training Loss  9.184890950564295e-05\n",
            "Epoch  38 Batch  216 / 525  Training Loss  5.785074972664006e-05\n",
            "Epoch  38 Batch  217 / 525  Training Loss  7.921718497527763e-05\n",
            "Epoch  38 Batch  218 / 525  Training Loss  6.292456237133592e-05\n",
            "Epoch  38 Batch  219 / 525  Training Loss  6.104585918365046e-05\n",
            "Epoch  38 Batch  220 / 525  Training Loss  0.000304795044939965\n",
            "Epoch  38 Batch  221 / 525  Training Loss  7.842622289899737e-05\n",
            "Epoch  38 Batch  222 / 525  Training Loss  0.00011438738874858245\n",
            "Epoch  38 Batch  223 / 525  Training Loss  0.00011350130807841197\n",
            "Epoch  38 Batch  224 / 525  Training Loss  0.000546559109352529\n",
            "Epoch  38 Batch  225 / 525  Training Loss  0.0002277644380228594\n",
            "Epoch  38 Batch  226 / 525  Training Loss  6.529180973302573e-05\n",
            "Epoch  38 Batch  227 / 525  Training Loss  0.0001016934184008278\n",
            "Epoch  38 Batch  228 / 525  Training Loss  5.355732355383225e-05\n",
            "Epoch  38 Batch  229 / 525  Training Loss  0.00018880897550843656\n",
            "Epoch  38 Batch  230 / 525  Training Loss  8.733502181712538e-05\n",
            "Epoch  38 Batch  231 / 525  Training Loss  3.157405080855824e-05\n",
            "Epoch  38 Batch  232 / 525  Training Loss  0.0003424405585974455\n",
            "Epoch  38 Batch  233 / 525  Training Loss  0.0002931922790594399\n",
            "Epoch  38 Batch  234 / 525  Training Loss  5.207530193729326e-05\n",
            "Epoch  38 Batch  235 / 525  Training Loss  0.0002621243183966726\n",
            "Epoch  38 Batch  236 / 525  Training Loss  0.00010517555347178131\n",
            "Epoch  38 Batch  237 / 525  Training Loss  0.00013948683044873178\n",
            "Epoch  38 Batch  238 / 525  Training Loss  4.1375104046892375e-05\n",
            "Epoch  38 Batch  239 / 525  Training Loss  0.0001351257087662816\n",
            "Epoch  38 Batch  240 / 525  Training Loss  9.107969526667148e-05\n",
            "Epoch  38 Batch  241 / 525  Training Loss  0.00020883395336568356\n",
            "Epoch  38 Batch  242 / 525  Training Loss  0.00011185535549884662\n",
            "Epoch  38 Batch  243 / 525  Training Loss  8.423542021773756e-05\n",
            "Epoch  38 Batch  244 / 525  Training Loss  0.0001681285648373887\n",
            "Epoch  38 Batch  245 / 525  Training Loss  0.00010332659439882264\n",
            "Epoch  38 Batch  246 / 525  Training Loss  0.00013703262084163725\n",
            "Epoch  38 Batch  247 / 525  Training Loss  0.00013987536658532917\n",
            "Epoch  38 Batch  248 / 525  Training Loss  8.131722279358655e-05\n",
            "Epoch  38 Batch  249 / 525  Training Loss  0.00014601860311813653\n",
            "Epoch  38 Batch  250 / 525  Training Loss  0.0001139583982876502\n",
            "Epoch  38 Batch  251 / 525  Training Loss  7.065046520438045e-05\n",
            "Epoch  38 Batch  252 / 525  Training Loss  4.789884405909106e-05\n",
            "Epoch  38 Batch  253 / 525  Training Loss  0.00014018833462614566\n",
            "Epoch  38 Batch  254 / 525  Training Loss  6.210578430909663e-05\n",
            "Epoch  38 Batch  255 / 525  Training Loss  0.00015531569079030305\n",
            "Epoch  38 Batch  256 / 525  Training Loss  6.486910569947213e-05\n",
            "Epoch  38 Batch  257 / 525  Training Loss  0.00012121636245865375\n",
            "Epoch  38 Batch  258 / 525  Training Loss  7.931420259410515e-05\n",
            "Epoch  38 Batch  259 / 525  Training Loss  9.560213220538571e-05\n",
            "Epoch  38 Batch  260 / 525  Training Loss  0.00010893492435570806\n",
            "Epoch  38 Batch  261 / 525  Training Loss  5.0672060751821846e-05\n",
            "Epoch  38 Batch  262 / 525  Training Loss  1.6920575944823213e-05\n",
            "Epoch  38 Batch  263 / 525  Training Loss  0.00012684639659710228\n",
            "Epoch  38 Batch  264 / 525  Training Loss  0.0003491771058179438\n",
            "Epoch  38 Batch  265 / 525  Training Loss  0.00014862978423479944\n",
            "Epoch  38 Batch  266 / 525  Training Loss  0.00015288297436200082\n",
            "Epoch  38 Batch  267 / 525  Training Loss  0.00019351574883330613\n",
            "Epoch  38 Batch  268 / 525  Training Loss  5.053987115388736e-05\n",
            "Epoch  38 Batch  269 / 525  Training Loss  0.00010013314022216946\n",
            "Epoch  38 Batch  270 / 525  Training Loss  0.00013445098011288792\n",
            "Epoch  38 Batch  271 / 525  Training Loss  0.00011867268767673522\n",
            "Epoch  38 Batch  272 / 525  Training Loss  0.00037370360223576427\n",
            "Epoch  38 Batch  273 / 525  Training Loss  0.00013545344700105488\n",
            "Epoch  38 Batch  274 / 525  Training Loss  9.699761722004041e-05\n",
            "Epoch  38 Batch  275 / 525  Training Loss  0.0009047398343682289\n",
            "Epoch  38 Batch  276 / 525  Training Loss  0.000239749948377721\n",
            "Epoch  38 Batch  277 / 525  Training Loss  7.310924411285669e-05\n",
            "Epoch  38 Batch  278 / 525  Training Loss  0.0003519135934766382\n",
            "Epoch  38 Batch  279 / 525  Training Loss  8.800897921901196e-05\n",
            "Epoch  38 Batch  280 / 525  Training Loss  0.00015618637553416193\n",
            "Epoch  38 Batch  281 / 525  Training Loss  0.0001717677660053596\n",
            "Epoch  38 Batch  282 / 525  Training Loss  0.00014257455768529326\n",
            "Epoch  38 Batch  283 / 525  Training Loss  5.191741365706548e-05\n",
            "Epoch  38 Batch  284 / 525  Training Loss  5.425217023002915e-05\n",
            "Epoch  38 Batch  285 / 525  Training Loss  0.00017951757763512433\n",
            "Epoch  38 Batch  286 / 525  Training Loss  0.00014459036174230278\n",
            "Epoch  38 Batch  287 / 525  Training Loss  0.00019998126663267612\n",
            "Epoch  38 Batch  288 / 525  Training Loss  9.068221697816625e-05\n",
            "Epoch  38 Batch  289 / 525  Training Loss  0.00035978041705675423\n",
            "Epoch  38 Batch  290 / 525  Training Loss  3.496206409181468e-05\n",
            "Epoch  38 Batch  291 / 525  Training Loss  0.00010649268369888887\n",
            "Epoch  38 Batch  292 / 525  Training Loss  7.365831697825342e-05\n",
            "Epoch  38 Batch  293 / 525  Training Loss  0.0001135382117354311\n",
            "Epoch  38 Batch  294 / 525  Training Loss  0.0001240802084794268\n",
            "Epoch  38 Batch  295 / 525  Training Loss  7.309716602321714e-05\n",
            "Epoch  38 Batch  296 / 525  Training Loss  0.00012401673302520066\n",
            "Epoch  38 Batch  297 / 525  Training Loss  6.0167476476635784e-05\n",
            "Epoch  38 Batch  298 / 525  Training Loss  0.0002512328792363405\n",
            "Epoch  38 Batch  299 / 525  Training Loss  7.069872663123533e-05\n",
            "Epoch  38 Batch  300 / 525  Training Loss  9.261418745154515e-05\n",
            "Epoch  38 Batch  301 / 525  Training Loss  5.6149205192923546e-05\n",
            "Epoch  38 Batch  302 / 525  Training Loss  7.462060602847487e-05\n",
            "Epoch  38 Batch  303 / 525  Training Loss  6.652400770690292e-05\n",
            "Epoch  38 Batch  304 / 525  Training Loss  9.952510299626738e-05\n",
            "Epoch  38 Batch  305 / 525  Training Loss  9.361239790450782e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  38 Batch  306 / 525  Training Loss  0.00010926640970865265\n",
            "Epoch  38 Batch  307 / 525  Training Loss  0.00017456877685617656\n",
            "Epoch  38 Batch  308 / 525  Training Loss  7.244058360811323e-05\n",
            "Epoch  38 Batch  309 / 525  Training Loss  9.294436313211918e-05\n",
            "Epoch  38 Batch  310 / 525  Training Loss  0.0001281675649806857\n",
            "Epoch  38 Batch  311 / 525  Training Loss  0.00015560972678940743\n",
            "Epoch  38 Batch  312 / 525  Training Loss  6.470736843766645e-05\n",
            "Epoch  38 Batch  313 / 525  Training Loss  0.00177117926068604\n",
            "Epoch  38 Batch  314 / 525  Training Loss  0.0002812313032336533\n",
            "Epoch  38 Batch  315 / 525  Training Loss  4.733292371383868e-05\n",
            "Epoch  38 Batch  316 / 525  Training Loss  7.806108624208719e-05\n",
            "Epoch  38 Batch  317 / 525  Training Loss  0.00048464443534612656\n",
            "Epoch  38 Batch  318 / 525  Training Loss  6.375412340275943e-05\n",
            "Epoch  38 Batch  319 / 525  Training Loss  0.0006511929095722735\n",
            "Epoch  38 Batch  320 / 525  Training Loss  0.00028178616776131094\n",
            "Epoch  38 Batch  321 / 525  Training Loss  6.095704884501174e-05\n",
            "Epoch  38 Batch  322 / 525  Training Loss  0.00016884032811503857\n",
            "Epoch  38 Batch  323 / 525  Training Loss  8.014972263481468e-05\n",
            "Epoch  38 Batch  324 / 525  Training Loss  0.0002673905109986663\n",
            "Epoch  38 Batch  325 / 525  Training Loss  3.568234751583077e-05\n",
            "Epoch  38 Batch  326 / 525  Training Loss  0.00011662747419904917\n",
            "Epoch  38 Batch  327 / 525  Training Loss  0.0016974251484498382\n",
            "Epoch  38 Batch  328 / 525  Training Loss  0.0006467443890869617\n",
            "Epoch  38 Batch  329 / 525  Training Loss  0.00014350579294841737\n",
            "Epoch  38 Batch  330 / 525  Training Loss  7.844884385121986e-05\n",
            "Epoch  38 Batch  331 / 525  Training Loss  9.563763160258532e-05\n",
            "Epoch  38 Batch  332 / 525  Training Loss  4.0767714381217957e-05\n",
            "Epoch  38 Batch  333 / 525  Training Loss  5.91267307754606e-05\n",
            "Epoch  38 Batch  334 / 525  Training Loss  0.00043488192022778094\n",
            "Epoch  38 Batch  335 / 525  Training Loss  0.0001713522942736745\n",
            "Epoch  38 Batch  336 / 525  Training Loss  9.777890227269381e-05\n",
            "Epoch  38 Batch  337 / 525  Training Loss  6.0934351495234296e-05\n",
            "Epoch  38 Batch  338 / 525  Training Loss  6.09901144343894e-05\n",
            "Epoch  38 Batch  339 / 525  Training Loss  3.960311732953414e-05\n",
            "Epoch  38 Batch  340 / 525  Training Loss  0.0004014169389847666\n",
            "Epoch  38 Batch  341 / 525  Training Loss  0.00014892253966536373\n",
            "Epoch  38 Batch  342 / 525  Training Loss  7.559599180240184e-05\n",
            "Epoch  38 Batch  343 / 525  Training Loss  0.00013461253547575325\n",
            "Epoch  38 Batch  344 / 525  Training Loss  5.555250754696317e-05\n",
            "Epoch  38 Batch  345 / 525  Training Loss  0.00035065499832853675\n",
            "Epoch  38 Batch  346 / 525  Training Loss  0.00010839525930350646\n",
            "Epoch  38 Batch  347 / 525  Training Loss  6.269479490583763e-05\n",
            "Epoch  38 Batch  348 / 525  Training Loss  0.00013603418483398855\n",
            "Epoch  38 Batch  349 / 525  Training Loss  0.00011957340029766783\n",
            "Epoch  38 Batch  350 / 525  Training Loss  0.00015921118028927594\n",
            "Epoch  38 Batch  351 / 525  Training Loss  8.467837324133143e-05\n",
            "Epoch  38 Batch  352 / 525  Training Loss  0.0003841078723780811\n",
            "Epoch  38 Batch  353 / 525  Training Loss  0.00010346581984777004\n",
            "Epoch  38 Batch  354 / 525  Training Loss  0.00011252008698647842\n",
            "Epoch  38 Batch  355 / 525  Training Loss  7.256833487190306e-05\n",
            "Epoch  38 Batch  356 / 525  Training Loss  0.00023345465888269246\n",
            "Epoch  38 Batch  357 / 525  Training Loss  0.00010205784201389179\n",
            "Epoch  38 Batch  358 / 525  Training Loss  0.00010608899174258113\n",
            "Epoch  38 Batch  359 / 525  Training Loss  6.405754538718611e-05\n",
            "Epoch  38 Batch  360 / 525  Training Loss  8.517268724972382e-05\n",
            "Epoch  38 Batch  361 / 525  Training Loss  0.00019447284284979105\n",
            "Epoch  38 Batch  362 / 525  Training Loss  5.4769923735875636e-05\n",
            "Epoch  38 Batch  363 / 525  Training Loss  0.00011329919652780518\n",
            "Epoch  38 Batch  364 / 525  Training Loss  0.00012248498387634754\n",
            "Epoch  38 Batch  365 / 525  Training Loss  0.0001253060472663492\n",
            "Epoch  38 Batch  366 / 525  Training Loss  0.0003547251981217414\n",
            "Epoch  38 Batch  367 / 525  Training Loss  0.00024128826044034213\n",
            "Epoch  38 Batch  368 / 525  Training Loss  0.00011951547639910132\n",
            "Epoch  38 Batch  369 / 525  Training Loss  0.00010538368951529264\n",
            "Epoch  38 Batch  370 / 525  Training Loss  3.795266820816323e-05\n",
            "Epoch  38 Batch  371 / 525  Training Loss  4.024004010716453e-05\n",
            "Epoch  38 Batch  372 / 525  Training Loss  0.0002860481617972255\n",
            "Epoch  38 Batch  373 / 525  Training Loss  0.00012335162318777293\n",
            "Epoch  38 Batch  374 / 525  Training Loss  7.73364445194602e-05\n",
            "Epoch  38 Batch  375 / 525  Training Loss  0.00010994557669619098\n",
            "Epoch  38 Batch  376 / 525  Training Loss  0.00019301174324937165\n",
            "Epoch  38 Batch  377 / 525  Training Loss  0.00013265937741380185\n",
            "Epoch  38 Batch  378 / 525  Training Loss  0.0001831819099606946\n",
            "Epoch  38 Batch  379 / 525  Training Loss  7.5235751864966e-05\n",
            "Epoch  38 Batch  380 / 525  Training Loss  0.00014914851635694504\n",
            "Epoch  38 Batch  381 / 525  Training Loss  0.00016458096797578037\n",
            "Epoch  38 Batch  382 / 525  Training Loss  0.00032056064810603857\n",
            "Epoch  38 Batch  383 / 525  Training Loss  8.600548608228564e-05\n",
            "Epoch  38 Batch  384 / 525  Training Loss  0.0001457556791137904\n",
            "Epoch  38 Batch  385 / 525  Training Loss  4.666208405978978e-05\n",
            "Epoch  38 Batch  386 / 525  Training Loss  0.0001542062673252076\n",
            "Epoch  38 Batch  387 / 525  Training Loss  0.00010247969476040453\n",
            "Epoch  38 Batch  388 / 525  Training Loss  0.0001402711059199646\n",
            "Epoch  38 Batch  389 / 525  Training Loss  0.0001673269725870341\n",
            "Epoch  38 Batch  390 / 525  Training Loss  0.00011639916192507371\n",
            "Epoch  38 Batch  391 / 525  Training Loss  0.00024949427461251616\n",
            "Epoch  38 Batch  392 / 525  Training Loss  7.020121847745031e-05\n",
            "Epoch  38 Batch  393 / 525  Training Loss  7.086146069923416e-05\n",
            "Epoch  38 Batch  394 / 525  Training Loss  0.00011547594476724043\n",
            "Epoch  38 Batch  395 / 525  Training Loss  6.889063661219552e-05\n",
            "Epoch  38 Batch  396 / 525  Training Loss  0.00016151955060195178\n",
            "Epoch  38 Batch  397 / 525  Training Loss  9.770791803020984e-05\n",
            "Epoch  38 Batch  398 / 525  Training Loss  0.00015659278142265975\n",
            "Epoch  38 Batch  399 / 525  Training Loss  6.022148227202706e-05\n",
            "Epoch  38 Batch  400 / 525  Training Loss  9.067437349585816e-05\n",
            "Epoch  38 Batch  401 / 525  Training Loss  2.511846469133161e-05\n",
            "Epoch  38 Batch  402 / 525  Training Loss  0.00016309745842590928\n",
            "Epoch  38 Batch  403 / 525  Training Loss  6.32636874797754e-05\n",
            "Epoch  38 Batch  404 / 525  Training Loss  7.646349695278332e-05\n",
            "Epoch  38 Batch  405 / 525  Training Loss  0.00016816082643344998\n",
            "Epoch  38 Batch  406 / 525  Training Loss  0.00014737233868800104\n",
            "Epoch  38 Batch  407 / 525  Training Loss  0.0004258978588040918\n",
            "Epoch  38 Batch  408 / 525  Training Loss  0.00012465364125091583\n",
            "Epoch  38 Batch  409 / 525  Training Loss  0.0002395322808297351\n",
            "Epoch  38 Batch  410 / 525  Training Loss  0.00011661362077575177\n",
            "Epoch  38 Batch  411 / 525  Training Loss  0.0006854659295640886\n",
            "Epoch  38 Batch  412 / 525  Training Loss  2.942373976111412e-05\n",
            "Epoch  38 Batch  413 / 525  Training Loss  3.008810745086521e-05\n",
            "Epoch  38 Batch  414 / 525  Training Loss  0.00023154565133154392\n",
            "Epoch  38 Batch  415 / 525  Training Loss  9.663162927608937e-05\n",
            "Epoch  38 Batch  416 / 525  Training Loss  0.00012850189523305744\n",
            "Epoch  38 Batch  417 / 525  Training Loss  0.0001454817393096164\n",
            "Epoch  38 Batch  418 / 525  Training Loss  5.55859187443275e-05\n",
            "Epoch  38 Batch  419 / 525  Training Loss  8.593036909587681e-05\n",
            "Epoch  38 Batch  420 / 525  Training Loss  0.00013998511712998152\n",
            "Epoch  38 Batch  421 / 525  Training Loss  0.0009020863217301667\n",
            "Epoch  38 Batch  422 / 525  Training Loss  8.069413161138073e-05\n",
            "Epoch  38 Batch  423 / 525  Training Loss  4.7872483264654875e-05\n",
            "Epoch  38 Batch  424 / 525  Training Loss  0.00010703025327529758\n",
            "Epoch  38 Batch  425 / 525  Training Loss  0.00017993609071709216\n",
            "Epoch  38 Batch  426 / 525  Training Loss  0.00016622498515062034\n",
            "Epoch  38 Batch  427 / 525  Training Loss  4.192462074570358e-05\n",
            "Epoch  38 Batch  428 / 525  Training Loss  0.00013715523527935147\n",
            "Epoch  38 Batch  429 / 525  Training Loss  0.004816001281142235\n",
            "Epoch  38 Batch  430 / 525  Training Loss  0.00011094132059952244\n",
            "Epoch  38 Batch  431 / 525  Training Loss  0.0001120687011280097\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  38 Batch  432 / 525  Training Loss  0.0002651599352248013\n",
            "Epoch  38 Batch  433 / 525  Training Loss  0.0001223899016622454\n",
            "Epoch  38 Batch  434 / 525  Training Loss  4.368114605313167e-05\n",
            "Epoch  38 Batch  435 / 525  Training Loss  0.00011808057024609298\n",
            "Epoch  38 Batch  436 / 525  Training Loss  0.0001449134579161182\n",
            "Epoch  38 Batch  437 / 525  Training Loss  0.0001337768480880186\n",
            "Epoch  38 Batch  438 / 525  Training Loss  7.038323383312672e-05\n",
            "Epoch  38 Batch  439 / 525  Training Loss  0.00014386339171323925\n",
            "Epoch  38 Batch  440 / 525  Training Loss  0.00013364494952838868\n",
            "Epoch  38 Batch  441 / 525  Training Loss  0.001263967715203762\n",
            "Epoch  38 Batch  442 / 525  Training Loss  0.00010041242785518989\n",
            "Epoch  38 Batch  443 / 525  Training Loss  0.0001441858767066151\n",
            "Epoch  38 Batch  444 / 525  Training Loss  0.00010448379907757044\n",
            "Epoch  38 Batch  445 / 525  Training Loss  0.00011442041432019323\n",
            "Epoch  38 Batch  446 / 525  Training Loss  6.05569985054899e-05\n",
            "Epoch  38 Batch  447 / 525  Training Loss  0.00031335651874542236\n",
            "Epoch  38 Batch  448 / 525  Training Loss  7.326364720938727e-05\n",
            "Epoch  38 Batch  449 / 525  Training Loss  7.374641427304596e-05\n",
            "Epoch  38 Batch  450 / 525  Training Loss  0.000202342911507003\n",
            "Epoch  38 Batch  451 / 525  Training Loss  0.00010925075912382454\n",
            "Epoch  38 Batch  452 / 525  Training Loss  3.6558911233441904e-05\n",
            "Epoch  38 Batch  453 / 525  Training Loss  7.187473238445818e-05\n",
            "Epoch  38 Batch  454 / 525  Training Loss  6.906632916070521e-05\n",
            "Epoch  38 Batch  455 / 525  Training Loss  0.0001243636361323297\n",
            "Epoch  38 Batch  456 / 525  Training Loss  0.00019212774350307882\n",
            "Epoch  38 Batch  457 / 525  Training Loss  0.00011880522652063519\n",
            "Epoch  38 Batch  458 / 525  Training Loss  5.823546598549001e-05\n",
            "Epoch  38 Batch  459 / 525  Training Loss  5.60434527869802e-05\n",
            "Epoch  38 Batch  460 / 525  Training Loss  0.00019700790289789438\n",
            "Epoch  38 Batch  461 / 525  Training Loss  0.0001515718176960945\n",
            "Epoch  38 Batch  462 / 525  Training Loss  0.00017692612891551107\n",
            "Epoch  38 Batch  463 / 525  Training Loss  0.00012014022649964318\n",
            "Epoch  38 Batch  464 / 525  Training Loss  6.533542182296515e-05\n",
            "Epoch  38 Batch  465 / 525  Training Loss  0.00018005105084739625\n",
            "Epoch  38 Batch  466 / 525  Training Loss  0.0005006703431718051\n",
            "Epoch  38 Batch  467 / 525  Training Loss  0.00015565923240501434\n",
            "Epoch  38 Batch  468 / 525  Training Loss  8.47613118821755e-05\n",
            "Epoch  38 Batch  469 / 525  Training Loss  0.00013521462096832693\n",
            "Epoch  38 Batch  470 / 525  Training Loss  0.0001535506744403392\n",
            "Epoch  38 Batch  471 / 525  Training Loss  0.0005043483106419444\n",
            "Epoch  38 Batch  472 / 525  Training Loss  0.00013834731362294406\n",
            "Epoch  38 Batch  473 / 525  Training Loss  0.0005779044586233795\n",
            "Epoch  38 Batch  474 / 525  Training Loss  0.000303843233268708\n",
            "Epoch  38 Batch  475 / 525  Training Loss  0.0001404610666213557\n",
            "Epoch  38 Batch  476 / 525  Training Loss  0.0001788741210475564\n",
            "Epoch  38 Batch  477 / 525  Training Loss  0.00014148949412629008\n",
            "Epoch  38 Batch  478 / 525  Training Loss  0.00010554560140008107\n",
            "Epoch  38 Batch  479 / 525  Training Loss  4.885764792561531e-05\n",
            "Epoch  38 Batch  480 / 525  Training Loss  0.00034007764770649374\n",
            "Epoch  38 Batch  481 / 525  Training Loss  9.193587175104767e-05\n",
            "Epoch  38 Batch  482 / 525  Training Loss  4.980643279850483e-05\n",
            "Epoch  38 Batch  483 / 525  Training Loss  3.670967271318659e-05\n",
            "Epoch  38 Batch  484 / 525  Training Loss  9.170027624350041e-05\n",
            "Epoch  38 Batch  485 / 525  Training Loss  0.0009156662854366004\n",
            "Epoch  38 Batch  486 / 525  Training Loss  8.998361590784043e-05\n",
            "Epoch  38 Batch  487 / 525  Training Loss  3.485825800453313e-05\n",
            "Epoch  38 Batch  488 / 525  Training Loss  0.0002642546023707837\n",
            "Epoch  38 Batch  489 / 525  Training Loss  7.790893141645938e-05\n",
            "Epoch  38 Batch  490 / 525  Training Loss  8.480914402753115e-05\n",
            "Epoch  38 Batch  491 / 525  Training Loss  0.00014165000175125897\n",
            "Epoch  38 Batch  492 / 525  Training Loss  6.755590584361926e-05\n",
            "Epoch  38 Batch  493 / 525  Training Loss  0.000195904474821873\n",
            "Epoch  38 Batch  494 / 525  Training Loss  0.00040216403431259096\n",
            "Epoch  38 Batch  495 / 525  Training Loss  0.00018759681552182883\n",
            "Epoch  38 Batch  496 / 525  Training Loss  8.986737520899624e-05\n",
            "Epoch  38 Batch  497 / 525  Training Loss  9.67026935541071e-05\n",
            "Epoch  38 Batch  498 / 525  Training Loss  0.00011939967225771397\n",
            "Epoch  38 Batch  499 / 525  Training Loss  0.00013869772374164313\n",
            "Epoch  38 Batch  500 / 525  Training Loss  0.000137949304189533\n",
            "Epoch  38 Batch  501 / 525  Training Loss  2.2897645976627246e-05\n",
            "Epoch  38 Batch  502 / 525  Training Loss  7.814002310624346e-05\n",
            "Epoch  38 Batch  503 / 525  Training Loss  0.00010179532546317205\n",
            "Epoch  38 Batch  504 / 525  Training Loss  0.00015783838171046227\n",
            "Epoch  38 Batch  505 / 525  Training Loss  8.08716140454635e-05\n",
            "Epoch  38 Batch  506 / 525  Training Loss  0.0050812773406505585\n",
            "Epoch  38 Batch  507 / 525  Training Loss  9.684795077191666e-05\n",
            "Epoch  38 Batch  508 / 525  Training Loss  0.00022109788551460952\n",
            "Epoch  38 Batch  509 / 525  Training Loss  7.636709051439539e-05\n",
            "Epoch  38 Batch  510 / 525  Training Loss  9.047733328770846e-05\n",
            "Epoch  38 Batch  511 / 525  Training Loss  0.00011614232789725065\n",
            "Epoch  38 Batch  512 / 525  Training Loss  0.00021877656399738044\n",
            "Epoch  38 Batch  513 / 525  Training Loss  0.00018853048095479608\n",
            "Epoch  38 Batch  514 / 525  Training Loss  0.00012384404544718564\n",
            "Epoch  38 Batch  515 / 525  Training Loss  0.0002812567399814725\n",
            "Epoch  38 Batch  516 / 525  Training Loss  4.6891938836779445e-05\n",
            "Epoch  38 Batch  517 / 525  Training Loss  0.00016086135292425752\n",
            "Epoch  38 Batch  518 / 525  Training Loss  0.0003032859822269529\n",
            "Epoch  38 Batch  519 / 525  Training Loss  9.564183710608631e-05\n",
            "Epoch  38 Batch  520 / 525  Training Loss  0.0016226805746555328\n",
            "Epoch  38 Batch  521 / 525  Training Loss  8.193733810912818e-05\n",
            "Epoch  38 Batch  522 / 525  Training Loss  0.0004422913771122694\n",
            "Epoch  38 Batch  523 / 525  Training Loss  7.360467861872166e-05\n",
            "Epoch  38 Batch  524 / 525  Training Loss  0.00010754926915979013\n",
            "  39    |    -    |   0.000245   | 63.950000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 39\n",
            "Epoch  39 Batch  0 / 525  Training Loss  0.00014745038060937077\n",
            "Epoch  39 Batch  1 / 525  Training Loss  6.902484165038913e-05\n",
            "Epoch  39 Batch  2 / 525  Training Loss  3.036390080524143e-05\n",
            "Epoch  39 Batch  3 / 525  Training Loss  0.00020367966499179602\n",
            "Epoch  39 Batch  4 / 525  Training Loss  0.0011085537262260914\n",
            "Epoch  39 Batch  5 / 525  Training Loss  0.00012869045895058662\n",
            "Epoch  39 Batch  6 / 525  Training Loss  8.085754234343767e-05\n",
            "Epoch  39 Batch  7 / 525  Training Loss  5.1667604566318914e-05\n",
            "Epoch  39 Batch  8 / 525  Training Loss  7.391641702270135e-05\n",
            "Epoch  39 Batch  9 / 525  Training Loss  0.0002916839439421892\n",
            "Epoch  39 Batch  10 / 525  Training Loss  0.00010999695223290473\n",
            "Epoch  39 Batch  11 / 525  Training Loss  8.084202272584662e-05\n",
            "Epoch  39 Batch  12 / 525  Training Loss  8.778580377111211e-05\n",
            "Epoch  39 Batch  13 / 525  Training Loss  3.903707329300232e-05\n",
            "Epoch  39 Batch  14 / 525  Training Loss  7.813812408130616e-05\n",
            "Epoch  39 Batch  15 / 525  Training Loss  9.711476013762876e-05\n",
            "Epoch  39 Batch  16 / 525  Training Loss  5.934438013355248e-05\n",
            "Epoch  39 Batch  17 / 525  Training Loss  3.845795799861662e-05\n",
            "Epoch  39 Batch  18 / 525  Training Loss  0.00010666672460502014\n",
            "Epoch  39 Batch  19 / 525  Training Loss  5.328348561306484e-05\n",
            "Epoch  39 Batch  20 / 525  Training Loss  2.791370206978172e-05\n",
            "Epoch  39 Batch  21 / 525  Training Loss  5.001103272661567e-05\n",
            "Epoch  39 Batch  22 / 525  Training Loss  9.423695155419409e-05\n",
            "Epoch  39 Batch  23 / 525  Training Loss  6.38123819953762e-05\n",
            "Epoch  39 Batch  24 / 525  Training Loss  8.19451961433515e-05\n",
            "Epoch  39 Batch  25 / 525  Training Loss  3.367916360730305e-05\n",
            "Epoch  39 Batch  26 / 525  Training Loss  4.7503166570095345e-05\n",
            "Epoch  39 Batch  27 / 525  Training Loss  0.00010830650717252865\n",
            "Epoch  39 Batch  28 / 525  Training Loss  4.515283217187971e-05\n",
            "Epoch  39 Batch  29 / 525  Training Loss  4.549214645521715e-05\n",
            "Epoch  39 Batch  30 / 525  Training Loss  4.8699217586545274e-05\n",
            "Epoch  39 Batch  31 / 525  Training Loss  9.504207991994917e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  39 Batch  32 / 525  Training Loss  5.6966506235767156e-05\n",
            "Epoch  39 Batch  33 / 525  Training Loss  6.1215934692882e-05\n",
            "Epoch  39 Batch  34 / 525  Training Loss  4.5498360123019665e-05\n",
            "Epoch  39 Batch  35 / 525  Training Loss  9.532616968499497e-05\n",
            "Epoch  39 Batch  36 / 525  Training Loss  9.922834578901529e-05\n",
            "Epoch  39 Batch  37 / 525  Training Loss  5.3472089348360896e-05\n",
            "Epoch  39 Batch  38 / 525  Training Loss  5.069579856353812e-05\n",
            "Epoch  39 Batch  39 / 525  Training Loss  0.00013749077334068716\n",
            "Epoch  39 Batch  40 / 525  Training Loss  5.440507811727002e-05\n",
            "Epoch  39 Batch  41 / 525  Training Loss  7.230736810015514e-05\n",
            "Epoch  39 Batch  42 / 525  Training Loss  7.838071906007826e-05\n",
            "Epoch  39 Batch  43 / 525  Training Loss  5.07797121827025e-05\n",
            "Epoch  39 Batch  44 / 525  Training Loss  0.0001366005017189309\n",
            "Epoch  39 Batch  45 / 525  Training Loss  6.887396739330143e-05\n",
            "Epoch  39 Batch  46 / 525  Training Loss  8.208001963794231e-05\n",
            "Epoch  39 Batch  47 / 525  Training Loss  0.00014943181304261088\n",
            "Epoch  39 Batch  48 / 525  Training Loss  6.659110658802092e-05\n",
            "Epoch  39 Batch  49 / 525  Training Loss  9.522972686681896e-05\n",
            "Epoch  39 Batch  50 / 525  Training Loss  6.493843102362007e-05\n",
            "Epoch  39 Batch  51 / 525  Training Loss  6.825093441875651e-05\n",
            "Epoch  39 Batch  52 / 525  Training Loss  0.00011799769708886743\n",
            "Epoch  39 Batch  53 / 525  Training Loss  6.030856820871122e-05\n",
            "Epoch  39 Batch  54 / 525  Training Loss  4.5921529817860574e-05\n",
            "Epoch  39 Batch  55 / 525  Training Loss  6.282787944655865e-05\n",
            "Epoch  39 Batch  56 / 525  Training Loss  4.872526915278286e-05\n",
            "Epoch  39 Batch  57 / 525  Training Loss  8.648353832541034e-05\n",
            "Epoch  39 Batch  58 / 525  Training Loss  3.326008663862012e-05\n",
            "Epoch  39 Batch  59 / 525  Training Loss  6.550672696903348e-05\n",
            "Epoch  39 Batch  60 / 525  Training Loss  4.587466173688881e-05\n",
            "Epoch  39 Batch  61 / 525  Training Loss  8.996344695333391e-05\n",
            "Epoch  39 Batch  62 / 525  Training Loss  4.434775109984912e-05\n",
            "Epoch  39 Batch  63 / 525  Training Loss  6.01825668127276e-05\n",
            "Epoch  39 Batch  64 / 525  Training Loss  6.482664321083575e-05\n",
            "Epoch  39 Batch  65 / 525  Training Loss  4.539560177363455e-05\n",
            "Epoch  39 Batch  66 / 525  Training Loss  6.492950342362747e-05\n",
            "Epoch  39 Batch  67 / 525  Training Loss  5.128452175995335e-05\n",
            "Epoch  39 Batch  68 / 525  Training Loss  6.227302219485864e-05\n",
            "Epoch  39 Batch  69 / 525  Training Loss  3.813434886978939e-05\n",
            "Epoch  39 Batch  70 / 525  Training Loss  8.143726881826296e-05\n",
            "Epoch  39 Batch  71 / 525  Training Loss  3.979613757110201e-05\n",
            "Epoch  39 Batch  72 / 525  Training Loss  6.66329578962177e-05\n",
            "Epoch  39 Batch  73 / 525  Training Loss  6.53442766633816e-05\n",
            "Epoch  39 Batch  74 / 525  Training Loss  8.043766865739599e-05\n",
            "Epoch  39 Batch  75 / 525  Training Loss  5.259294994175434e-05\n",
            "Epoch  39 Batch  76 / 525  Training Loss  4.1323568439111114e-05\n",
            "Epoch  39 Batch  77 / 525  Training Loss  3.848904452752322e-05\n",
            "Epoch  39 Batch  78 / 525  Training Loss  7.313742389669642e-05\n",
            "Epoch  39 Batch  79 / 525  Training Loss  0.00011647334031295031\n",
            "Epoch  39 Batch  80 / 525  Training Loss  7.730945071671158e-05\n",
            "Epoch  39 Batch  81 / 525  Training Loss  3.375314554432407e-05\n",
            "Epoch  39 Batch  82 / 525  Training Loss  0.000136529139126651\n",
            "Epoch  39 Batch  83 / 525  Training Loss  2.9732735129073262e-05\n",
            "Epoch  39 Batch  84 / 525  Training Loss  4.449530388228595e-05\n",
            "Epoch  39 Batch  85 / 525  Training Loss  5.6412547564832494e-05\n",
            "Epoch  39 Batch  86 / 525  Training Loss  6.502353062387556e-05\n",
            "Epoch  39 Batch  87 / 525  Training Loss  8.022718975553289e-05\n",
            "Epoch  39 Batch  88 / 525  Training Loss  3.4841035812860355e-05\n",
            "Epoch  39 Batch  89 / 525  Training Loss  2.8943168217665516e-05\n",
            "Epoch  39 Batch  90 / 525  Training Loss  0.00018870833446271718\n",
            "Epoch  39 Batch  91 / 525  Training Loss  9.266128472518176e-05\n",
            "Epoch  39 Batch  92 / 525  Training Loss  6.742579716956243e-05\n",
            "Epoch  39 Batch  93 / 525  Training Loss  9.832509385887533e-05\n",
            "Epoch  39 Batch  94 / 525  Training Loss  9.074478293769062e-05\n",
            "Epoch  39 Batch  95 / 525  Training Loss  5.160494401934557e-05\n",
            "Epoch  39 Batch  96 / 525  Training Loss  0.00013017382298130542\n",
            "Epoch  39 Batch  97 / 525  Training Loss  6.159099575597793e-05\n",
            "Epoch  39 Batch  98 / 525  Training Loss  2.6176468963967636e-05\n",
            "Epoch  39 Batch  99 / 525  Training Loss  7.339451258303598e-05\n",
            "Epoch  39 Batch  100 / 525  Training Loss  3.435415419517085e-05\n",
            "Epoch  39 Batch  101 / 525  Training Loss  0.00010908307740464807\n",
            "Epoch  39 Batch  102 / 525  Training Loss  6.722153921145946e-05\n",
            "Epoch  39 Batch  103 / 525  Training Loss  0.00013630864850711077\n",
            "Epoch  39 Batch  104 / 525  Training Loss  0.004402278922498226\n",
            "Epoch  39 Batch  105 / 525  Training Loss  9.840606071520597e-05\n",
            "Epoch  39 Batch  106 / 525  Training Loss  6.689668953185901e-05\n",
            "Epoch  39 Batch  107 / 525  Training Loss  0.00012683977547567338\n",
            "Epoch  39 Batch  108 / 525  Training Loss  7.527355046477169e-05\n",
            "Epoch  39 Batch  109 / 525  Training Loss  6.238981586648151e-05\n",
            "Epoch  39 Batch  110 / 525  Training Loss  7.522931264247745e-05\n",
            "Epoch  39 Batch  111 / 525  Training Loss  7.050530257401988e-05\n",
            "Epoch  39 Batch  112 / 525  Training Loss  0.00047003646614030004\n",
            "Epoch  39 Batch  113 / 525  Training Loss  5.690140460501425e-05\n",
            "Epoch  39 Batch  114 / 525  Training Loss  6.373536598403007e-05\n",
            "Epoch  39 Batch  115 / 525  Training Loss  0.00012828603212255985\n",
            "Epoch  39 Batch  116 / 525  Training Loss  6.617909821216017e-05\n",
            "Epoch  39 Batch  117 / 525  Training Loss  3.4868018701672554e-05\n",
            "Epoch  39 Batch  118 / 525  Training Loss  7.45823053875938e-05\n",
            "Epoch  39 Batch  119 / 525  Training Loss  0.00010950640717055649\n",
            "Epoch  39 Batch  120 / 525  Training Loss  3.649446807685308e-05\n",
            "Epoch  39 Batch  121 / 525  Training Loss  3.551749250618741e-05\n",
            "Epoch  39 Batch  122 / 525  Training Loss  0.00020941905677318573\n",
            "Epoch  39 Batch  123 / 525  Training Loss  6.544186180690303e-05\n",
            "Epoch  39 Batch  124 / 525  Training Loss  0.00012460167636163533\n",
            "Epoch  39 Batch  125 / 525  Training Loss  0.00010869382094824687\n",
            "Epoch  39 Batch  126 / 525  Training Loss  2.6005433028331026e-05\n",
            "Epoch  39 Batch  127 / 525  Training Loss  6.931213283678517e-05\n",
            "Epoch  39 Batch  128 / 525  Training Loss  0.00023245687771122903\n",
            "Epoch  39 Batch  129 / 525  Training Loss  3.651442602858879e-05\n",
            "Epoch  39 Batch  130 / 525  Training Loss  2.8344817110337317e-05\n",
            "Epoch  39 Batch  131 / 525  Training Loss  8.555316890124232e-05\n",
            "Epoch  39 Batch  132 / 525  Training Loss  5.4083182476460934e-05\n",
            "Epoch  39 Batch  133 / 525  Training Loss  2.8044956707162783e-05\n",
            "Epoch  39 Batch  134 / 525  Training Loss  6.323931302176788e-05\n",
            "Epoch  39 Batch  135 / 525  Training Loss  5.01991125929635e-05\n",
            "Epoch  39 Batch  136 / 525  Training Loss  0.00011841711966553703\n",
            "Epoch  39 Batch  137 / 525  Training Loss  7.540861406596377e-05\n",
            "Epoch  39 Batch  138 / 525  Training Loss  5.538668483495712e-05\n",
            "Epoch  39 Batch  139 / 525  Training Loss  0.00028486354858614504\n",
            "Epoch  39 Batch  140 / 525  Training Loss  5.210696326685138e-05\n",
            "Epoch  39 Batch  141 / 525  Training Loss  5.892657281947322e-05\n",
            "Epoch  39 Batch  142 / 525  Training Loss  7.890714186942205e-05\n",
            "Epoch  39 Batch  143 / 525  Training Loss  7.801036554155871e-05\n",
            "Epoch  39 Batch  144 / 525  Training Loss  5.8016186812892556e-05\n",
            "Epoch  39 Batch  145 / 525  Training Loss  8.241854084189981e-05\n",
            "Epoch  39 Batch  146 / 525  Training Loss  6.8907807872165e-05\n",
            "Epoch  39 Batch  147 / 525  Training Loss  4.544644616544247e-05\n",
            "Epoch  39 Batch  148 / 525  Training Loss  0.0001345047785434872\n",
            "Epoch  39 Batch  149 / 525  Training Loss  0.00034834325197152793\n",
            "Epoch  39 Batch  150 / 525  Training Loss  6.999903416726738e-05\n",
            "Epoch  39 Batch  151 / 525  Training Loss  2.9283623007358983e-05\n",
            "Epoch  39 Batch  152 / 525  Training Loss  3.9063877920852974e-05\n",
            "Epoch  39 Batch  153 / 525  Training Loss  0.00010804545308928937\n",
            "Epoch  39 Batch  154 / 525  Training Loss  0.0003767641610465944\n",
            "Epoch  39 Batch  155 / 525  Training Loss  0.00017778927576728165\n",
            "Epoch  39 Batch  156 / 525  Training Loss  7.641036791028455e-05\n",
            "Epoch  39 Batch  157 / 525  Training Loss  0.00010851683327928185\n",
            "Epoch  39 Batch  158 / 525  Training Loss  8.207203791243955e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  39 Batch  159 / 525  Training Loss  4.49617909907829e-05\n",
            "Epoch  39 Batch  160 / 525  Training Loss  0.0005048458697274327\n",
            "Epoch  39 Batch  161 / 525  Training Loss  4.853257996728644e-05\n",
            "Epoch  39 Batch  162 / 525  Training Loss  2.2167780116433278e-05\n",
            "Epoch  39 Batch  163 / 525  Training Loss  8.103998698061332e-05\n",
            "Epoch  39 Batch  164 / 525  Training Loss  5.1124148740200326e-05\n",
            "Epoch  39 Batch  165 / 525  Training Loss  0.0009199188207276165\n",
            "Epoch  39 Batch  166 / 525  Training Loss  7.58157839300111e-05\n",
            "Epoch  39 Batch  167 / 525  Training Loss  0.0001633376523386687\n",
            "Epoch  39 Batch  168 / 525  Training Loss  8.685357897775248e-05\n",
            "Epoch  39 Batch  169 / 525  Training Loss  0.00310076167806983\n",
            "Epoch  39 Batch  170 / 525  Training Loss  4.113302202313207e-05\n",
            "Epoch  39 Batch  171 / 525  Training Loss  0.0006158527685329318\n",
            "Epoch  39 Batch  172 / 525  Training Loss  4.760880983667448e-05\n",
            "Epoch  39 Batch  173 / 525  Training Loss  6.0037080402253196e-05\n",
            "Epoch  39 Batch  174 / 525  Training Loss  0.0010854396969079971\n",
            "Epoch  39 Batch  175 / 525  Training Loss  7.712177466601133e-05\n",
            "Epoch  39 Batch  176 / 525  Training Loss  6.68235807097517e-05\n",
            "Epoch  39 Batch  177 / 525  Training Loss  3.418359119677916e-05\n",
            "Epoch  39 Batch  178 / 525  Training Loss  6.13787051406689e-05\n",
            "Epoch  39 Batch  179 / 525  Training Loss  0.00026352968416176736\n",
            "Epoch  39 Batch  180 / 525  Training Loss  6.730166933266446e-05\n",
            "Epoch  39 Batch  181 / 525  Training Loss  0.00014241578173823655\n",
            "Epoch  39 Batch  182 / 525  Training Loss  4.931677904096432e-05\n",
            "Epoch  39 Batch  183 / 525  Training Loss  8.309789700433612e-05\n",
            "Epoch  39 Batch  184 / 525  Training Loss  6.387282337527722e-05\n",
            "Epoch  39 Batch  185 / 525  Training Loss  0.00011502982670208439\n",
            "Epoch  39 Batch  186 / 525  Training Loss  7.050876592984423e-05\n",
            "Epoch  39 Batch  187 / 525  Training Loss  5.478057937580161e-05\n",
            "Epoch  39 Batch  188 / 525  Training Loss  7.434075087076053e-05\n",
            "Epoch  39 Batch  189 / 525  Training Loss  6.013482197886333e-05\n",
            "Epoch  39 Batch  190 / 525  Training Loss  1.8370692487224005e-05\n",
            "Epoch  39 Batch  191 / 525  Training Loss  4.383150371722877e-05\n",
            "Epoch  39 Batch  192 / 525  Training Loss  8.449913730146363e-05\n",
            "Epoch  39 Batch  193 / 525  Training Loss  5.408450306276791e-05\n",
            "Epoch  39 Batch  194 / 525  Training Loss  6.406962347682565e-05\n",
            "Epoch  39 Batch  195 / 525  Training Loss  5.537237666430883e-05\n",
            "Epoch  39 Batch  196 / 525  Training Loss  3.0024966690689325e-05\n",
            "Epoch  39 Batch  197 / 525  Training Loss  9.38306693569757e-05\n",
            "Epoch  39 Batch  198 / 525  Training Loss  5.845839041285217e-05\n",
            "Epoch  39 Batch  199 / 525  Training Loss  0.000188678182894364\n",
            "Epoch  39 Batch  200 / 525  Training Loss  5.7703902712091804e-05\n",
            "Epoch  39 Batch  201 / 525  Training Loss  0.00027969415532425046\n",
            "Epoch  39 Batch  202 / 525  Training Loss  0.0013665843289345503\n",
            "Epoch  39 Batch  203 / 525  Training Loss  0.00010464127262821421\n",
            "Epoch  39 Batch  204 / 525  Training Loss  0.00014375380123965442\n",
            "Epoch  39 Batch  205 / 525  Training Loss  5.732080899178982e-05\n",
            "Epoch  39 Batch  206 / 525  Training Loss  0.0010330736404284835\n",
            "Epoch  39 Batch  207 / 525  Training Loss  0.00015997882292140275\n",
            "Epoch  39 Batch  208 / 525  Training Loss  6.263697287067771e-05\n",
            "Epoch  39 Batch  209 / 525  Training Loss  9.395844244863838e-05\n",
            "Epoch  39 Batch  210 / 525  Training Loss  8.669482485856861e-05\n",
            "Epoch  39 Batch  211 / 525  Training Loss  6.730811583111063e-05\n",
            "Epoch  39 Batch  212 / 525  Training Loss  4.952828021487221e-05\n",
            "Epoch  39 Batch  213 / 525  Training Loss  5.747010072809644e-05\n",
            "Epoch  39 Batch  214 / 525  Training Loss  0.00011150196951348335\n",
            "Epoch  39 Batch  215 / 525  Training Loss  5.986493852105923e-05\n",
            "Epoch  39 Batch  216 / 525  Training Loss  3.117030064458959e-05\n",
            "Epoch  39 Batch  217 / 525  Training Loss  3.0822680855635554e-05\n",
            "Epoch  39 Batch  218 / 525  Training Loss  8.537373651051894e-05\n",
            "Epoch  39 Batch  219 / 525  Training Loss  6.261460657697171e-05\n",
            "Epoch  39 Batch  220 / 525  Training Loss  5.1183440518798307e-05\n",
            "Epoch  39 Batch  221 / 525  Training Loss  6.840188143542036e-05\n",
            "Epoch  39 Batch  222 / 525  Training Loss  6.877456326037645e-05\n",
            "Epoch  39 Batch  223 / 525  Training Loss  0.00011445413110777736\n",
            "Epoch  39 Batch  224 / 525  Training Loss  7.008905959082767e-05\n",
            "Epoch  39 Batch  225 / 525  Training Loss  6.398621917469427e-05\n",
            "Epoch  39 Batch  226 / 525  Training Loss  0.00012749541201628745\n",
            "Epoch  39 Batch  227 / 525  Training Loss  0.00011344500671839342\n",
            "Epoch  39 Batch  228 / 525  Training Loss  4.375798016553745e-05\n",
            "Epoch  39 Batch  229 / 525  Training Loss  0.00018150290998164564\n",
            "Epoch  39 Batch  230 / 525  Training Loss  9.914061229210347e-05\n",
            "Epoch  39 Batch  231 / 525  Training Loss  8.182342571672052e-05\n",
            "Epoch  39 Batch  232 / 525  Training Loss  6.381676212185994e-05\n",
            "Epoch  39 Batch  233 / 525  Training Loss  3.546067455317825e-05\n",
            "Epoch  39 Batch  234 / 525  Training Loss  8.092384814517573e-05\n",
            "Epoch  39 Batch  235 / 525  Training Loss  7.541589729953557e-05\n",
            "Epoch  39 Batch  236 / 525  Training Loss  3.086817014263943e-05\n",
            "Epoch  39 Batch  237 / 525  Training Loss  7.41373369237408e-05\n",
            "Epoch  39 Batch  238 / 525  Training Loss  4.743014869745821e-05\n",
            "Epoch  39 Batch  239 / 525  Training Loss  3.9621892938157544e-05\n",
            "Epoch  39 Batch  240 / 525  Training Loss  5.0655962695600465e-05\n",
            "Epoch  39 Batch  241 / 525  Training Loss  7.296681724255905e-05\n",
            "Epoch  39 Batch  242 / 525  Training Loss  8.33179074106738e-05\n",
            "Epoch  39 Batch  243 / 525  Training Loss  6.514640699606389e-05\n",
            "Epoch  39 Batch  244 / 525  Training Loss  9.192990546580404e-05\n",
            "Epoch  39 Batch  245 / 525  Training Loss  7.088355778250843e-05\n",
            "Epoch  39 Batch  246 / 525  Training Loss  0.00010293101513525471\n",
            "Epoch  39 Batch  247 / 525  Training Loss  8.304721268359572e-05\n",
            "Epoch  39 Batch  248 / 525  Training Loss  0.0002065398293780163\n",
            "Epoch  39 Batch  249 / 525  Training Loss  6.545053474837914e-05\n",
            "Epoch  39 Batch  250 / 525  Training Loss  8.006994903553277e-05\n",
            "Epoch  39 Batch  251 / 525  Training Loss  7.303126039914787e-05\n",
            "Epoch  39 Batch  252 / 525  Training Loss  0.00015949665976222605\n",
            "Epoch  39 Batch  253 / 525  Training Loss  5.090063132229261e-05\n",
            "Epoch  39 Batch  254 / 525  Training Loss  2.9760452889604494e-05\n",
            "Epoch  39 Batch  255 / 525  Training Loss  0.00024694768944755197\n",
            "Epoch  39 Batch  256 / 525  Training Loss  6.357418897096068e-05\n",
            "Epoch  39 Batch  257 / 525  Training Loss  9.159660839941353e-05\n",
            "Epoch  39 Batch  258 / 525  Training Loss  6.12517906120047e-05\n",
            "Epoch  39 Batch  259 / 525  Training Loss  0.00012640560453291982\n",
            "Epoch  39 Batch  260 / 525  Training Loss  0.0001487404661020264\n",
            "Epoch  39 Batch  261 / 525  Training Loss  6.960348400752991e-05\n",
            "Epoch  39 Batch  262 / 525  Training Loss  8.001601963769644e-05\n",
            "Epoch  39 Batch  263 / 525  Training Loss  5.662548210239038e-05\n",
            "Epoch  39 Batch  264 / 525  Training Loss  9.510284871794283e-05\n",
            "Epoch  39 Batch  265 / 525  Training Loss  8.75857294886373e-05\n",
            "Epoch  39 Batch  266 / 525  Training Loss  0.00011002555402228609\n",
            "Epoch  39 Batch  267 / 525  Training Loss  4.949774665874429e-05\n",
            "Epoch  39 Batch  268 / 525  Training Loss  5.8494180848356336e-05\n",
            "Epoch  39 Batch  269 / 525  Training Loss  4.278372216504067e-05\n",
            "Epoch  39 Batch  270 / 525  Training Loss  4.183092823950574e-05\n",
            "Epoch  39 Batch  271 / 525  Training Loss  9.548277739668265e-05\n",
            "Epoch  39 Batch  272 / 525  Training Loss  8.365888788830489e-05\n",
            "Epoch  39 Batch  273 / 525  Training Loss  4.25316029577516e-05\n",
            "Epoch  39 Batch  274 / 525  Training Loss  9.100856550503522e-05\n",
            "Epoch  39 Batch  275 / 525  Training Loss  0.0005528148612938821\n",
            "Epoch  39 Batch  276 / 525  Training Loss  7.735439430689439e-05\n",
            "Epoch  39 Batch  277 / 525  Training Loss  6.921325984876603e-05\n",
            "Epoch  39 Batch  278 / 525  Training Loss  0.0001048814010573551\n",
            "Epoch  39 Batch  279 / 525  Training Loss  0.0001291099179070443\n",
            "Epoch  39 Batch  280 / 525  Training Loss  4.8209574742941186e-05\n",
            "Epoch  39 Batch  281 / 525  Training Loss  3.13637538056355e-05\n",
            "Epoch  39 Batch  282 / 525  Training Loss  5.9870450058951974e-05\n",
            "Epoch  39 Batch  283 / 525  Training Loss  3.1848041544435546e-05\n",
            "Epoch  39 Batch  284 / 525  Training Loss  4.7693440137663856e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  39 Batch  285 / 525  Training Loss  7.48642924008891e-05\n",
            "Epoch  39 Batch  286 / 525  Training Loss  3.613865192164667e-05\n",
            "Epoch  39 Batch  287 / 525  Training Loss  7.074403401929885e-05\n",
            "Epoch  39 Batch  288 / 525  Training Loss  3.754224599106237e-05\n",
            "Epoch  39 Batch  289 / 525  Training Loss  4.356315184850246e-05\n",
            "Epoch  39 Batch  290 / 525  Training Loss  0.00019664267892949283\n",
            "Epoch  39 Batch  291 / 525  Training Loss  6.991416739765555e-05\n",
            "Epoch  39 Batch  292 / 525  Training Loss  5.445222632260993e-05\n",
            "Epoch  39 Batch  293 / 525  Training Loss  0.00013349787332117558\n",
            "Epoch  39 Batch  294 / 525  Training Loss  3.1504234357271343e-05\n",
            "Epoch  39 Batch  295 / 525  Training Loss  6.630386633332819e-05\n",
            "Epoch  39 Batch  296 / 525  Training Loss  5.0318292778683826e-05\n",
            "Epoch  39 Batch  297 / 525  Training Loss  5.0165497668785974e-05\n",
            "Epoch  39 Batch  298 / 525  Training Loss  8.816652552923188e-05\n",
            "Epoch  39 Batch  299 / 525  Training Loss  4.362999607110396e-05\n",
            "Epoch  39 Batch  300 / 525  Training Loss  3.663292227429338e-05\n",
            "Epoch  39 Batch  301 / 525  Training Loss  8.787212573224679e-05\n",
            "Epoch  39 Batch  302 / 525  Training Loss  2.257219784951303e-05\n",
            "Epoch  39 Batch  303 / 525  Training Loss  4.505186734604649e-05\n",
            "Epoch  39 Batch  304 / 525  Training Loss  3.6337827623356134e-05\n",
            "Epoch  39 Batch  305 / 525  Training Loss  6.068510629120283e-05\n",
            "Epoch  39 Batch  306 / 525  Training Loss  0.0002790524740703404\n",
            "Epoch  39 Batch  307 / 525  Training Loss  8.141111902659759e-05\n",
            "Epoch  39 Batch  308 / 525  Training Loss  7.695537351537496e-05\n",
            "Epoch  39 Batch  309 / 525  Training Loss  5.661840987158939e-05\n",
            "Epoch  39 Batch  310 / 525  Training Loss  6.995796866249293e-05\n",
            "Epoch  39 Batch  311 / 525  Training Loss  9.163950744550675e-05\n",
            "Epoch  39 Batch  312 / 525  Training Loss  8.219303708756343e-05\n",
            "Epoch  39 Batch  313 / 525  Training Loss  4.278282358427532e-05\n",
            "Epoch  39 Batch  314 / 525  Training Loss  5.444134149001911e-05\n",
            "Epoch  39 Batch  315 / 525  Training Loss  6.963163468753919e-05\n",
            "Epoch  39 Batch  316 / 525  Training Loss  0.0012712033931165934\n",
            "Epoch  39 Batch  317 / 525  Training Loss  2.3697662982158363e-05\n",
            "Epoch  39 Batch  318 / 525  Training Loss  7.33424531063065e-05\n",
            "Epoch  39 Batch  319 / 525  Training Loss  4.260367131792009e-05\n",
            "Epoch  39 Batch  320 / 525  Training Loss  0.00011819688370451331\n",
            "Epoch  39 Batch  321 / 525  Training Loss  8.683920168550685e-05\n",
            "Epoch  39 Batch  322 / 525  Training Loss  3.9209495298564434e-05\n",
            "Epoch  39 Batch  323 / 525  Training Loss  4.022097709821537e-05\n",
            "Epoch  39 Batch  324 / 525  Training Loss  1.865954982349649e-05\n",
            "Epoch  39 Batch  325 / 525  Training Loss  3.2253348763333634e-05\n",
            "Epoch  39 Batch  326 / 525  Training Loss  0.0001464226661482826\n",
            "Epoch  39 Batch  327 / 525  Training Loss  0.003317850176244974\n",
            "Epoch  39 Batch  328 / 525  Training Loss  0.00012401748972479254\n",
            "Epoch  39 Batch  329 / 525  Training Loss  0.008134428411722183\n",
            "Epoch  39 Batch  330 / 525  Training Loss  6.941329047549516e-05\n",
            "Epoch  39 Batch  331 / 525  Training Loss  4.502219962887466e-05\n",
            "Epoch  39 Batch  332 / 525  Training Loss  7.148433360271156e-05\n",
            "Epoch  39 Batch  333 / 525  Training Loss  0.00014987215399742126\n",
            "Epoch  39 Batch  334 / 525  Training Loss  0.0001318894064752385\n",
            "Epoch  39 Batch  335 / 525  Training Loss  5.300112388795242e-05\n",
            "Epoch  39 Batch  336 / 525  Training Loss  6.023364403517917e-05\n",
            "Epoch  39 Batch  337 / 525  Training Loss  0.00014262800686992705\n",
            "Epoch  39 Batch  338 / 525  Training Loss  3.471921809250489e-05\n",
            "Epoch  39 Batch  339 / 525  Training Loss  0.00010328559437766671\n",
            "Epoch  39 Batch  340 / 525  Training Loss  0.0018855336820706725\n",
            "Epoch  39 Batch  341 / 525  Training Loss  5.579860953730531e-05\n",
            "Epoch  39 Batch  342 / 525  Training Loss  8.895334758562967e-05\n",
            "Epoch  39 Batch  343 / 525  Training Loss  4.068178895977326e-05\n",
            "Epoch  39 Batch  344 / 525  Training Loss  2.826480704243295e-05\n",
            "Epoch  39 Batch  345 / 525  Training Loss  0.0001277327974094078\n",
            "Epoch  39 Batch  346 / 525  Training Loss  5.285823863232508e-05\n",
            "Epoch  39 Batch  347 / 525  Training Loss  7.596381328767166e-05\n",
            "Epoch  39 Batch  348 / 525  Training Loss  0.00010015512816607952\n",
            "Epoch  39 Batch  349 / 525  Training Loss  0.0003345163422636688\n",
            "Epoch  39 Batch  350 / 525  Training Loss  0.00011691301187966019\n",
            "Epoch  39 Batch  351 / 525  Training Loss  6.395640230039135e-05\n",
            "Epoch  39 Batch  352 / 525  Training Loss  4.567980795400217e-05\n",
            "Epoch  39 Batch  353 / 525  Training Loss  0.00038248219061642885\n",
            "Epoch  39 Batch  354 / 525  Training Loss  5.6456650781910866e-05\n",
            "Epoch  39 Batch  355 / 525  Training Loss  0.00011700750474119559\n",
            "Epoch  39 Batch  356 / 525  Training Loss  3.492521500447765e-05\n",
            "Epoch  39 Batch  357 / 525  Training Loss  6.734626367688179e-05\n",
            "Epoch  39 Batch  358 / 525  Training Loss  3.2386080420110375e-05\n",
            "Epoch  39 Batch  359 / 525  Training Loss  3.407087206142023e-05\n",
            "Epoch  39 Batch  360 / 525  Training Loss  0.00017802674847189337\n",
            "Epoch  39 Batch  361 / 525  Training Loss  0.00016863149357959628\n",
            "Epoch  39 Batch  362 / 525  Training Loss  5.4582342272624373e-05\n",
            "Epoch  39 Batch  363 / 525  Training Loss  6.45776599412784e-05\n",
            "Epoch  39 Batch  364 / 525  Training Loss  3.6856697988696396e-05\n",
            "Epoch  39 Batch  365 / 525  Training Loss  9.273839532397687e-05\n",
            "Epoch  39 Batch  366 / 525  Training Loss  0.00011617458949331194\n",
            "Epoch  39 Batch  367 / 525  Training Loss  0.00014502982958219945\n",
            "Epoch  39 Batch  368 / 525  Training Loss  8.217357390094548e-05\n",
            "Epoch  39 Batch  369 / 525  Training Loss  8.658051956444979e-05\n",
            "Epoch  39 Batch  370 / 525  Training Loss  2.509550904505886e-05\n",
            "Epoch  39 Batch  371 / 525  Training Loss  5.100297858007252e-05\n",
            "Epoch  39 Batch  372 / 525  Training Loss  0.00039636544534005225\n",
            "Epoch  39 Batch  373 / 525  Training Loss  0.0001259106647921726\n",
            "Epoch  39 Batch  374 / 525  Training Loss  2.4317781935678795e-05\n",
            "Epoch  39 Batch  375 / 525  Training Loss  0.0011031555477529764\n",
            "Epoch  39 Batch  376 / 525  Training Loss  7.867793465266004e-05\n",
            "Epoch  39 Batch  377 / 525  Training Loss  5.689698809874244e-05\n",
            "Epoch  39 Batch  378 / 525  Training Loss  3.788321191677824e-05\n",
            "Epoch  39 Batch  379 / 525  Training Loss  5.145356408320367e-05\n",
            "Epoch  39 Batch  380 / 525  Training Loss  0.00031841633608564734\n",
            "Epoch  39 Batch  381 / 525  Training Loss  7.406453369185328e-05\n",
            "Epoch  39 Batch  382 / 525  Training Loss  0.00020510840113274753\n",
            "Epoch  39 Batch  383 / 525  Training Loss  2.7666033929563127e-05\n",
            "Epoch  39 Batch  384 / 525  Training Loss  0.0001750639930833131\n",
            "Epoch  39 Batch  385 / 525  Training Loss  0.0001492096926085651\n",
            "Epoch  39 Batch  386 / 525  Training Loss  6.172079883981496e-05\n",
            "Epoch  39 Batch  387 / 525  Training Loss  0.00015195042942650616\n",
            "Epoch  39 Batch  388 / 525  Training Loss  0.00024466426111757755\n",
            "Epoch  39 Batch  389 / 525  Training Loss  8.857227658154443e-05\n",
            "Epoch  39 Batch  390 / 525  Training Loss  7.168402953539044e-05\n",
            "Epoch  39 Batch  391 / 525  Training Loss  7.616617222083732e-05\n",
            "Epoch  39 Batch  392 / 525  Training Loss  0.00012589330435730517\n",
            "Epoch  39 Batch  393 / 525  Training Loss  3.5677450796356425e-05\n",
            "Epoch  39 Batch  394 / 525  Training Loss  9.569394023856148e-05\n",
            "Epoch  39 Batch  395 / 525  Training Loss  8.036430517677218e-05\n",
            "Epoch  39 Batch  396 / 525  Training Loss  0.00015113894187379628\n",
            "Epoch  39 Batch  397 / 525  Training Loss  6.937597208889201e-05\n",
            "Epoch  39 Batch  398 / 525  Training Loss  9.997206507250667e-05\n",
            "Epoch  39 Batch  399 / 525  Training Loss  6.92244284437038e-05\n",
            "Epoch  39 Batch  400 / 525  Training Loss  6.159353506518528e-05\n",
            "Epoch  39 Batch  401 / 525  Training Loss  0.00016883088392205536\n",
            "Epoch  39 Batch  402 / 525  Training Loss  3.687858770717867e-05\n",
            "Epoch  39 Batch  403 / 525  Training Loss  8.002454706002027e-05\n",
            "Epoch  39 Batch  404 / 525  Training Loss  2.8643864425248466e-05\n",
            "Epoch  39 Batch  405 / 525  Training Loss  5.884126221644692e-05\n",
            "Epoch  39 Batch  406 / 525  Training Loss  4.0642549720359966e-05\n",
            "Epoch  39 Batch  407 / 525  Training Loss  1.7585505702299997e-05\n",
            "Epoch  39 Batch  408 / 525  Training Loss  4.803465708391741e-05\n",
            "Epoch  39 Batch  409 / 525  Training Loss  5.119989509694278e-05\n",
            "Epoch  39 Batch  410 / 525  Training Loss  0.00013489986304193735\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  39 Batch  411 / 525  Training Loss  0.00016969829448498785\n",
            "Epoch  39 Batch  412 / 525  Training Loss  0.0001669091871008277\n",
            "Epoch  39 Batch  413 / 525  Training Loss  0.00012091961980331689\n",
            "Epoch  39 Batch  414 / 525  Training Loss  6.336154183372855e-05\n",
            "Epoch  39 Batch  415 / 525  Training Loss  4.258205080986954e-05\n",
            "Epoch  39 Batch  416 / 525  Training Loss  0.00015366423758678138\n",
            "Epoch  39 Batch  417 / 525  Training Loss  0.0001472218136768788\n",
            "Epoch  39 Batch  418 / 525  Training Loss  0.0007948459242470562\n",
            "Epoch  39 Batch  419 / 525  Training Loss  5.348308332031593e-05\n",
            "Epoch  39 Batch  420 / 525  Training Loss  0.0001116740022553131\n",
            "Epoch  39 Batch  421 / 525  Training Loss  0.00018113759870175272\n",
            "Epoch  39 Batch  422 / 525  Training Loss  4.1010989662026986e-05\n",
            "Epoch  39 Batch  423 / 525  Training Loss  0.0001238302211277187\n",
            "Epoch  39 Batch  424 / 525  Training Loss  5.106244861963205e-05\n",
            "Epoch  39 Batch  425 / 525  Training Loss  7.309697684831917e-05\n",
            "Epoch  39 Batch  426 / 525  Training Loss  0.00014619315334130079\n",
            "Epoch  39 Batch  427 / 525  Training Loss  7.280423596967012e-05\n",
            "Epoch  39 Batch  428 / 525  Training Loss  0.00013366504572331905\n",
            "Epoch  39 Batch  429 / 525  Training Loss  9.650363790569827e-05\n",
            "Epoch  39 Batch  430 / 525  Training Loss  4.4984240958001465e-05\n",
            "Epoch  39 Batch  431 / 525  Training Loss  0.00013611494796350598\n",
            "Epoch  39 Batch  432 / 525  Training Loss  7.17825532774441e-05\n",
            "Epoch  39 Batch  433 / 525  Training Loss  4.9643182137515396e-05\n",
            "Epoch  39 Batch  434 / 525  Training Loss  0.00011758009350160137\n",
            "Epoch  39 Batch  435 / 525  Training Loss  0.0003608860424719751\n",
            "Epoch  39 Batch  436 / 525  Training Loss  0.00012198642070870847\n",
            "Epoch  39 Batch  437 / 525  Training Loss  0.0001240218261955306\n",
            "Epoch  39 Batch  438 / 525  Training Loss  6.908601062605157e-05\n",
            "Epoch  39 Batch  439 / 525  Training Loss  0.00010677722457330674\n",
            "Epoch  39 Batch  440 / 525  Training Loss  9.202572255162522e-05\n",
            "Epoch  39 Batch  441 / 525  Training Loss  6.162303179735318e-05\n",
            "Epoch  39 Batch  442 / 525  Training Loss  5.882947152713314e-05\n",
            "Epoch  39 Batch  443 / 525  Training Loss  5.516831879504025e-05\n",
            "Epoch  39 Batch  444 / 525  Training Loss  6.084257620386779e-05\n",
            "Epoch  39 Batch  445 / 525  Training Loss  8.848804282024503e-05\n",
            "Epoch  39 Batch  446 / 525  Training Loss  5.427106952993199e-05\n",
            "Epoch  39 Batch  447 / 525  Training Loss  4.4476160837803036e-05\n",
            "Epoch  39 Batch  448 / 525  Training Loss  6.626420508837327e-05\n",
            "Epoch  39 Batch  449 / 525  Training Loss  4.745960177388042e-05\n",
            "Epoch  39 Batch  450 / 525  Training Loss  0.0001560468808747828\n",
            "Epoch  39 Batch  451 / 525  Training Loss  0.00013287960609886795\n",
            "Epoch  39 Batch  452 / 525  Training Loss  0.00016231060726568103\n",
            "Epoch  39 Batch  453 / 525  Training Loss  0.00011096037633251399\n",
            "Epoch  39 Batch  454 / 525  Training Loss  5.7093573559541255e-05\n",
            "Epoch  39 Batch  455 / 525  Training Loss  9.79634714894928e-05\n",
            "Epoch  39 Batch  456 / 525  Training Loss  9.45094579947181e-05\n",
            "Epoch  39 Batch  457 / 525  Training Loss  7.385220669675618e-05\n",
            "Epoch  39 Batch  458 / 525  Training Loss  5.9563451941357926e-05\n",
            "Epoch  39 Batch  459 / 525  Training Loss  8.206699567381293e-05\n",
            "Epoch  39 Batch  460 / 525  Training Loss  0.0001607925078133121\n",
            "Epoch  39 Batch  461 / 525  Training Loss  2.7622474590316415e-05\n",
            "Epoch  39 Batch  462 / 525  Training Loss  0.000101502999314107\n",
            "Epoch  39 Batch  463 / 525  Training Loss  6.473951361840591e-05\n",
            "Epoch  39 Batch  464 / 525  Training Loss  4.6874927647877485e-05\n",
            "Epoch  39 Batch  465 / 525  Training Loss  7.094374450389296e-05\n",
            "Epoch  39 Batch  466 / 525  Training Loss  6.27670597168617e-05\n",
            "Epoch  39 Batch  467 / 525  Training Loss  4.015462036477402e-05\n",
            "Epoch  39 Batch  468 / 525  Training Loss  0.0001859997573774308\n",
            "Epoch  39 Batch  469 / 525  Training Loss  4.242480645189062e-05\n",
            "Epoch  39 Batch  470 / 525  Training Loss  7.435800216626376e-05\n",
            "Epoch  39 Batch  471 / 525  Training Loss  0.00014372181612998247\n",
            "Epoch  39 Batch  472 / 525  Training Loss  3.335080327815376e-05\n",
            "Epoch  39 Batch  473 / 525  Training Loss  7.550847658421844e-05\n",
            "Epoch  39 Batch  474 / 525  Training Loss  0.0001485419925302267\n",
            "Epoch  39 Batch  475 / 525  Training Loss  9.702534589450806e-05\n",
            "Epoch  39 Batch  476 / 525  Training Loss  8.0256380897481e-05\n",
            "Epoch  39 Batch  477 / 525  Training Loss  4.663637082558125e-05\n",
            "Epoch  39 Batch  478 / 525  Training Loss  5.5740110838087276e-05\n",
            "Epoch  39 Batch  479 / 525  Training Loss  0.00020174644305370748\n",
            "Epoch  39 Batch  480 / 525  Training Loss  3.289406595285982e-05\n",
            "Epoch  39 Batch  481 / 525  Training Loss  5.046552905696444e-05\n",
            "Epoch  39 Batch  482 / 525  Training Loss  6.919971929164603e-05\n",
            "Epoch  39 Batch  483 / 525  Training Loss  5.119643174111843e-05\n",
            "Epoch  39 Batch  484 / 525  Training Loss  6.854956882307306e-05\n",
            "Epoch  39 Batch  485 / 525  Training Loss  4.079095015185885e-05\n",
            "Epoch  39 Batch  486 / 525  Training Loss  7.579987141070887e-05\n",
            "Epoch  39 Batch  487 / 525  Training Loss  7.055122114252299e-05\n",
            "Epoch  39 Batch  488 / 525  Training Loss  2.9603314033010975e-05\n",
            "Epoch  39 Batch  489 / 525  Training Loss  2.6052133762277663e-05\n",
            "Epoch  39 Batch  490 / 525  Training Loss  7.045242091408e-05\n",
            "Epoch  39 Batch  491 / 525  Training Loss  2.8365053367451765e-05\n",
            "Epoch  39 Batch  492 / 525  Training Loss  5.49532996956259e-05\n",
            "Epoch  39 Batch  493 / 525  Training Loss  0.00012143656931584701\n",
            "Epoch  39 Batch  494 / 525  Training Loss  3.846673644147813e-05\n",
            "Epoch  39 Batch  495 / 525  Training Loss  4.4713633542414755e-05\n",
            "Epoch  39 Batch  496 / 525  Training Loss  7.205781730590388e-05\n",
            "Epoch  39 Batch  497 / 525  Training Loss  9.674848115537316e-05\n",
            "Epoch  39 Batch  498 / 525  Training Loss  4.388797970023006e-05\n",
            "Epoch  39 Batch  499 / 525  Training Loss  3.7743498978670686e-05\n",
            "Epoch  39 Batch  500 / 525  Training Loss  4.528899080469273e-05\n",
            "Epoch  39 Batch  501 / 525  Training Loss  4.089816502528265e-05\n",
            "Epoch  39 Batch  502 / 525  Training Loss  5.056244117440656e-05\n",
            "Epoch  39 Batch  503 / 525  Training Loss  0.00010980570368701592\n",
            "Epoch  39 Batch  504 / 525  Training Loss  9.757701627677307e-05\n",
            "Epoch  39 Batch  505 / 525  Training Loss  6.95945491315797e-05\n",
            "Epoch  39 Batch  506 / 525  Training Loss  6.788205064367503e-05\n",
            "Epoch  39 Batch  507 / 525  Training Loss  5.039774623583071e-05\n",
            "Epoch  39 Batch  508 / 525  Training Loss  0.00017535724327899516\n",
            "Epoch  39 Batch  509 / 525  Training Loss  7.376099529210478e-05\n",
            "Epoch  39 Batch  510 / 525  Training Loss  3.894742258125916e-05\n",
            "Epoch  39 Batch  511 / 525  Training Loss  2.4780218154774047e-05\n",
            "Epoch  39 Batch  512 / 525  Training Loss  7.862548955017701e-05\n",
            "Epoch  39 Batch  513 / 525  Training Loss  8.036151120904833e-05\n",
            "Epoch  39 Batch  514 / 525  Training Loss  0.00012088348012184724\n",
            "Epoch  39 Batch  515 / 525  Training Loss  0.00011242761684115976\n",
            "Epoch  39 Batch  516 / 525  Training Loss  0.0003755164216272533\n",
            "Epoch  39 Batch  517 / 525  Training Loss  8.685165084898472e-05\n",
            "Epoch  39 Batch  518 / 525  Training Loss  8.998053090181202e-05\n",
            "Epoch  39 Batch  519 / 525  Training Loss  4.5146331103751436e-05\n",
            "Epoch  39 Batch  520 / 525  Training Loss  0.00011664142220979556\n",
            "Epoch  39 Batch  521 / 525  Training Loss  5.177062121219933e-05\n",
            "Epoch  39 Batch  522 / 525  Training Loss  3.625300450948998e-05\n",
            "Epoch  39 Batch  523 / 525  Training Loss  4.78369474876672e-05\n",
            "Epoch  39 Batch  524 / 525  Training Loss  3.7384867027867585e-05\n",
            "  40    |    -    |   0.000142   | 64.275000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 40\n",
            "Epoch  40 Batch  0 / 525  Training Loss  5.7641016610432416e-05\n",
            "Epoch  40 Batch  1 / 525  Training Loss  5.254176357993856e-05\n",
            "Epoch  40 Batch  2 / 525  Training Loss  6.635001773247495e-05\n",
            "Epoch  40 Batch  3 / 525  Training Loss  3.7254849303280935e-05\n",
            "Epoch  40 Batch  4 / 525  Training Loss  4.4820171751780435e-05\n",
            "Epoch  40 Batch  5 / 525  Training Loss  4.9800233682617545e-05\n",
            "Epoch  40 Batch  6 / 525  Training Loss  3.149977055727504e-05\n",
            "Epoch  40 Batch  7 / 525  Training Loss  3.7144895031815395e-05\n",
            "Epoch  40 Batch  8 / 525  Training Loss  4.354946577223018e-05\n",
            "Epoch  40 Batch  9 / 525  Training Loss  4.827100201509893e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  40 Batch  10 / 525  Training Loss  2.455760295561049e-05\n",
            "Epoch  40 Batch  11 / 525  Training Loss  5.710653567803092e-05\n",
            "Epoch  40 Batch  12 / 525  Training Loss  6.430267239920795e-05\n",
            "Epoch  40 Batch  13 / 525  Training Loss  5.124385279486887e-05\n",
            "Epoch  40 Batch  14 / 525  Training Loss  2.6499052182771266e-05\n",
            "Epoch  40 Batch  15 / 525  Training Loss  5.150830838829279e-05\n",
            "Epoch  40 Batch  16 / 525  Training Loss  2.7194331778446212e-05\n",
            "Epoch  40 Batch  17 / 525  Training Loss  6.080510138417594e-05\n",
            "Epoch  40 Batch  18 / 525  Training Loss  7.564677071059123e-05\n",
            "Epoch  40 Batch  19 / 525  Training Loss  2.491253508196678e-05\n",
            "Epoch  40 Batch  20 / 525  Training Loss  6.21720973867923e-05\n",
            "Epoch  40 Batch  21 / 525  Training Loss  3.0725532269570976e-05\n",
            "Epoch  40 Batch  22 / 525  Training Loss  1.9257837266195565e-05\n",
            "Epoch  40 Batch  23 / 525  Training Loss  4.2174604459432885e-05\n",
            "Epoch  40 Batch  24 / 525  Training Loss  3.818567347479984e-05\n",
            "Epoch  40 Batch  25 / 525  Training Loss  6.196791218826547e-05\n",
            "Epoch  40 Batch  26 / 525  Training Loss  3.713964906637557e-05\n",
            "Epoch  40 Batch  27 / 525  Training Loss  4.663256186177023e-05\n",
            "Epoch  40 Batch  28 / 525  Training Loss  2.7928303097723983e-05\n",
            "Epoch  40 Batch  29 / 525  Training Loss  3.462544555077329e-05\n",
            "Epoch  40 Batch  30 / 525  Training Loss  2.1434623704408295e-05\n",
            "Epoch  40 Batch  31 / 525  Training Loss  4.222044299240224e-05\n",
            "Epoch  40 Batch  32 / 525  Training Loss  2.9617851396324113e-05\n",
            "Epoch  40 Batch  33 / 525  Training Loss  4.855175575357862e-05\n",
            "Epoch  40 Batch  34 / 525  Training Loss  2.7773698093369603e-05\n",
            "Epoch  40 Batch  35 / 525  Training Loss  5.225078348303214e-05\n",
            "Epoch  40 Batch  36 / 525  Training Loss  4.1091661842074245e-05\n",
            "Epoch  40 Batch  37 / 525  Training Loss  3.543865750543773e-05\n",
            "Epoch  40 Batch  38 / 525  Training Loss  4.439035910763778e-05\n",
            "Epoch  40 Batch  39 / 525  Training Loss  4.7489818825852126e-05\n",
            "Epoch  40 Batch  40 / 525  Training Loss  2.800465517793782e-05\n",
            "Epoch  40 Batch  41 / 525  Training Loss  6.970857793930918e-05\n",
            "Epoch  40 Batch  42 / 525  Training Loss  7.780396845191717e-05\n",
            "Epoch  40 Batch  43 / 525  Training Loss  3.550080509739928e-05\n",
            "Epoch  40 Batch  44 / 525  Training Loss  6.647696864092723e-05\n",
            "Epoch  40 Batch  45 / 525  Training Loss  4.785886630997993e-05\n",
            "Epoch  40 Batch  46 / 525  Training Loss  5.8504752814769745e-05\n",
            "Epoch  40 Batch  47 / 525  Training Loss  3.168957482557744e-05\n",
            "Epoch  40 Batch  48 / 525  Training Loss  4.258718035998754e-05\n",
            "Epoch  40 Batch  49 / 525  Training Loss  4.312367309466936e-05\n",
            "Epoch  40 Batch  50 / 525  Training Loss  3.559092874638736e-05\n",
            "Epoch  40 Batch  51 / 525  Training Loss  3.7774821976199746e-05\n",
            "Epoch  40 Batch  52 / 525  Training Loss  1.841712582972832e-05\n",
            "Epoch  40 Batch  53 / 525  Training Loss  3.1613635655958205e-05\n",
            "Epoch  40 Batch  54 / 525  Training Loss  6.70146182528697e-05\n",
            "Epoch  40 Batch  55 / 525  Training Loss  3.575460868887603e-05\n",
            "Epoch  40 Batch  56 / 525  Training Loss  2.764177042990923e-05\n",
            "Epoch  40 Batch  57 / 525  Training Loss  2.6031990273622796e-05\n",
            "Epoch  40 Batch  58 / 525  Training Loss  6.730113818775862e-05\n",
            "Epoch  40 Batch  59 / 525  Training Loss  2.568337367847562e-05\n",
            "Epoch  40 Batch  60 / 525  Training Loss  3.9145586924860254e-05\n",
            "Epoch  40 Batch  61 / 525  Training Loss  2.824019429681357e-05\n",
            "Epoch  40 Batch  62 / 525  Training Loss  3.802459832513705e-05\n",
            "Epoch  40 Batch  63 / 525  Training Loss  4.618153252522461e-05\n",
            "Epoch  40 Batch  64 / 525  Training Loss  3.95263014070224e-05\n",
            "Epoch  40 Batch  65 / 525  Training Loss  4.648448884836398e-05\n",
            "Epoch  40 Batch  66 / 525  Training Loss  1.7760863556759432e-05\n",
            "Epoch  40 Batch  67 / 525  Training Loss  5.088860780233517e-05\n",
            "Epoch  40 Batch  68 / 525  Training Loss  6.182784272823483e-05\n",
            "Epoch  40 Batch  69 / 525  Training Loss  0.00010322841990273446\n",
            "Epoch  40 Batch  70 / 525  Training Loss  2.25188814511057e-05\n",
            "Epoch  40 Batch  71 / 525  Training Loss  2.652948023751378e-05\n",
            "Epoch  40 Batch  72 / 525  Training Loss  5.1562827138695866e-05\n",
            "Epoch  40 Batch  73 / 525  Training Loss  3.5832686990033835e-05\n",
            "Epoch  40 Batch  74 / 525  Training Loss  4.813152918359265e-05\n",
            "Epoch  40 Batch  75 / 525  Training Loss  1.9642528059193864e-05\n",
            "Epoch  40 Batch  76 / 525  Training Loss  7.262545841513202e-05\n",
            "Epoch  40 Batch  77 / 525  Training Loss  2.5492185159237124e-05\n",
            "Epoch  40 Batch  78 / 525  Training Loss  1.9918963516829535e-05\n",
            "Epoch  40 Batch  79 / 525  Training Loss  2.5112269213423133e-05\n",
            "Epoch  40 Batch  80 / 525  Training Loss  4.393553899717517e-05\n",
            "Epoch  40 Batch  81 / 525  Training Loss  3.458990977378562e-05\n",
            "Epoch  40 Batch  82 / 525  Training Loss  2.9664941394003108e-05\n",
            "Epoch  40 Batch  83 / 525  Training Loss  4.8780937504488975e-05\n",
            "Epoch  40 Batch  84 / 525  Training Loss  3.46250890288502e-05\n",
            "Epoch  40 Batch  85 / 525  Training Loss  4.19039570260793e-05\n",
            "Epoch  40 Batch  86 / 525  Training Loss  3.9913862565299496e-05\n",
            "Epoch  40 Batch  87 / 525  Training Loss  7.041398203000426e-05\n",
            "Epoch  40 Batch  88 / 525  Training Loss  2.8395133995218202e-05\n",
            "Epoch  40 Batch  89 / 525  Training Loss  1.9874465579050593e-05\n",
            "Epoch  40 Batch  90 / 525  Training Loss  6.297357322182506e-05\n",
            "Epoch  40 Batch  91 / 525  Training Loss  1.3329752619029023e-05\n",
            "Epoch  40 Batch  92 / 525  Training Loss  8.278230961877853e-05\n",
            "Epoch  40 Batch  93 / 525  Training Loss  4.948378773406148e-05\n",
            "Epoch  40 Batch  94 / 525  Training Loss  3.620703500928357e-05\n",
            "Epoch  40 Batch  95 / 525  Training Loss  3.6146426282357424e-05\n",
            "Epoch  40 Batch  96 / 525  Training Loss  4.6068747906247154e-05\n",
            "Epoch  40 Batch  97 / 525  Training Loss  1.7572834622114897e-05\n",
            "Epoch  40 Batch  98 / 525  Training Loss  2.1602410924970172e-05\n",
            "Epoch  40 Batch  99 / 525  Training Loss  5.083723954157904e-05\n",
            "Epoch  40 Batch  100 / 525  Training Loss  6.100122118368745e-05\n",
            "Epoch  40 Batch  101 / 525  Training Loss  2.9541324693127535e-05\n",
            "Epoch  40 Batch  102 / 525  Training Loss  5.902252814848907e-05\n",
            "Epoch  40 Batch  103 / 525  Training Loss  3.0885898013366386e-05\n",
            "Epoch  40 Batch  104 / 525  Training Loss  4.003711001132615e-05\n",
            "Epoch  40 Batch  105 / 525  Training Loss  4.279729910194874e-05\n",
            "Epoch  40 Batch  106 / 525  Training Loss  4.677642209571786e-05\n",
            "Epoch  40 Batch  107 / 525  Training Loss  4.543227987596765e-05\n",
            "Epoch  40 Batch  108 / 525  Training Loss  2.014178244280629e-05\n",
            "Epoch  40 Batch  109 / 525  Training Loss  2.867865259759128e-05\n",
            "Epoch  40 Batch  110 / 525  Training Loss  5.02750335726887e-05\n",
            "Epoch  40 Batch  111 / 525  Training Loss  4.772080137627199e-05\n",
            "Epoch  40 Batch  112 / 525  Training Loss  3.656218177638948e-05\n",
            "Epoch  40 Batch  113 / 525  Training Loss  5.079101538285613e-05\n",
            "Epoch  40 Batch  114 / 525  Training Loss  2.8670785468420945e-05\n",
            "Epoch  40 Batch  115 / 525  Training Loss  2.7610803954303265e-05\n",
            "Epoch  40 Batch  116 / 525  Training Loss  3.574944639694877e-05\n",
            "Epoch  40 Batch  117 / 525  Training Loss  3.765703149838373e-05\n",
            "Epoch  40 Batch  118 / 525  Training Loss  4.804638592759147e-05\n",
            "Epoch  40 Batch  119 / 525  Training Loss  2.9756611183984205e-05\n",
            "Epoch  40 Batch  120 / 525  Training Loss  5.385824988479726e-05\n",
            "Epoch  40 Batch  121 / 525  Training Loss  0.00015717241331003606\n",
            "Epoch  40 Batch  122 / 525  Training Loss  6.728256994392723e-05\n",
            "Epoch  40 Batch  123 / 525  Training Loss  5.8377372624818236e-05\n",
            "Epoch  40 Batch  124 / 525  Training Loss  6.181788921821862e-05\n",
            "Epoch  40 Batch  125 / 525  Training Loss  6.440696597564965e-05\n",
            "Epoch  40 Batch  126 / 525  Training Loss  2.9492581234080717e-05\n",
            "Epoch  40 Batch  127 / 525  Training Loss  5.0982023822143674e-05\n",
            "Epoch  40 Batch  128 / 525  Training Loss  3.93187947338447e-05\n",
            "Epoch  40 Batch  129 / 525  Training Loss  3.366349483258091e-05\n",
            "Epoch  40 Batch  130 / 525  Training Loss  4.121765596210025e-05\n",
            "Epoch  40 Batch  131 / 525  Training Loss  3.176910831825808e-05\n",
            "Epoch  40 Batch  132 / 525  Training Loss  5.889204840059392e-05\n",
            "Epoch  40 Batch  133 / 525  Training Loss  3.2060957892099395e-05\n",
            "Epoch  40 Batch  134 / 525  Training Loss  4.9484595365356654e-05\n",
            "Epoch  40 Batch  135 / 525  Training Loss  3.081265458604321e-05\n",
            "Epoch  40 Batch  136 / 525  Training Loss  5.938073809375055e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  40 Batch  137 / 525  Training Loss  4.115384217584506e-05\n",
            "Epoch  40 Batch  138 / 525  Training Loss  5.996432082611136e-05\n",
            "Epoch  40 Batch  139 / 525  Training Loss  3.430479409871623e-05\n",
            "Epoch  40 Batch  140 / 525  Training Loss  4.410149995237589e-05\n",
            "Epoch  40 Batch  141 / 525  Training Loss  3.535339783411473e-05\n",
            "Epoch  40 Batch  142 / 525  Training Loss  3.2376672606915236e-05\n",
            "Epoch  40 Batch  143 / 525  Training Loss  3.584293881431222e-05\n",
            "Epoch  40 Batch  144 / 525  Training Loss  4.684110899688676e-05\n",
            "Epoch  40 Batch  145 / 525  Training Loss  3.811178612522781e-05\n",
            "Epoch  40 Batch  146 / 525  Training Loss  4.552369864541106e-05\n",
            "Epoch  40 Batch  147 / 525  Training Loss  2.5909283067448996e-05\n",
            "Epoch  40 Batch  148 / 525  Training Loss  5.7721073972061276e-05\n",
            "Epoch  40 Batch  149 / 525  Training Loss  2.4253477022284642e-05\n",
            "Epoch  40 Batch  150 / 525  Training Loss  2.2744005036656745e-05\n",
            "Epoch  40 Batch  151 / 525  Training Loss  4.5787204726366326e-05\n",
            "Epoch  40 Batch  152 / 525  Training Loss  3.641345756477676e-05\n",
            "Epoch  40 Batch  153 / 525  Training Loss  7.932689186418429e-05\n",
            "Epoch  40 Batch  154 / 525  Training Loss  2.3536147637059912e-05\n",
            "Epoch  40 Batch  155 / 525  Training Loss  4.790509046870284e-05\n",
            "Epoch  40 Batch  156 / 525  Training Loss  5.385881013353355e-05\n",
            "Epoch  40 Batch  157 / 525  Training Loss  7.654850196558982e-05\n",
            "Epoch  40 Batch  158 / 525  Training Loss  4.975318006472662e-05\n",
            "Epoch  40 Batch  159 / 525  Training Loss  2.7086973204859532e-05\n",
            "Epoch  40 Batch  160 / 525  Training Loss  3.624975943239406e-05\n",
            "Epoch  40 Batch  161 / 525  Training Loss  6.511753599625081e-05\n",
            "Epoch  40 Batch  162 / 525  Training Loss  4.016203820356168e-05\n",
            "Epoch  40 Batch  163 / 525  Training Loss  4.537606218946166e-05\n",
            "Epoch  40 Batch  164 / 525  Training Loss  3.8766673242207617e-05\n",
            "Epoch  40 Batch  165 / 525  Training Loss  2.8093247237848118e-05\n",
            "Epoch  40 Batch  166 / 525  Training Loss  4.0656635974301025e-05\n",
            "Epoch  40 Batch  167 / 525  Training Loss  2.4030527129070833e-05\n",
            "Epoch  40 Batch  168 / 525  Training Loss  2.3160373530117795e-05\n",
            "Epoch  40 Batch  169 / 525  Training Loss  9.65807521424722e-06\n",
            "Epoch  40 Batch  170 / 525  Training Loss  5.2070692618144676e-05\n",
            "Epoch  40 Batch  171 / 525  Training Loss  4.05982464144472e-05\n",
            "Epoch  40 Batch  172 / 525  Training Loss  4.941267616231926e-05\n",
            "Epoch  40 Batch  173 / 525  Training Loss  3.5942965041613206e-05\n",
            "Epoch  40 Batch  174 / 525  Training Loss  2.9539052775362507e-05\n",
            "Epoch  40 Batch  175 / 525  Training Loss  3.86602187063545e-05\n",
            "Epoch  40 Batch  176 / 525  Training Loss  3.0283990781754255e-05\n",
            "Epoch  40 Batch  177 / 525  Training Loss  5.797815538244322e-05\n",
            "Epoch  40 Batch  178 / 525  Training Loss  8.791840809863061e-05\n",
            "Epoch  40 Batch  179 / 525  Training Loss  4.470042767934501e-05\n",
            "Epoch  40 Batch  180 / 525  Training Loss  4.711700967163779e-05\n",
            "Epoch  40 Batch  181 / 525  Training Loss  3.277097857790068e-05\n",
            "Epoch  40 Batch  182 / 525  Training Loss  4.2626190406735986e-05\n",
            "Epoch  40 Batch  183 / 525  Training Loss  5.9188470913795754e-05\n",
            "Epoch  40 Batch  184 / 525  Training Loss  5.9260946727590635e-05\n",
            "Epoch  40 Batch  185 / 525  Training Loss  7.142182585084811e-05\n",
            "Epoch  40 Batch  186 / 525  Training Loss  5.797449193778448e-05\n",
            "Epoch  40 Batch  187 / 525  Training Loss  3.678174834931269e-05\n",
            "Epoch  40 Batch  188 / 525  Training Loss  6.6242108005099e-05\n",
            "Epoch  40 Batch  189 / 525  Training Loss  2.1869973352295347e-05\n",
            "Epoch  40 Batch  190 / 525  Training Loss  4.5738921471638605e-05\n",
            "Epoch  40 Batch  191 / 525  Training Loss  5.2900715672876686e-05\n",
            "Epoch  40 Batch  192 / 525  Training Loss  4.666121458285488e-05\n",
            "Epoch  40 Batch  193 / 525  Training Loss  2.9801140044583008e-05\n",
            "Epoch  40 Batch  194 / 525  Training Loss  2.6923255063593388e-05\n",
            "Epoch  40 Batch  195 / 525  Training Loss  4.1883969970513135e-05\n",
            "Epoch  40 Batch  196 / 525  Training Loss  3.884640318574384e-05\n",
            "Epoch  40 Batch  197 / 525  Training Loss  2.981234501930885e-05\n",
            "Epoch  40 Batch  198 / 525  Training Loss  5.082019924884662e-05\n",
            "Epoch  40 Batch  199 / 525  Training Loss  3.03981032629963e-05\n",
            "Epoch  40 Batch  200 / 525  Training Loss  2.725900776567869e-05\n",
            "Epoch  40 Batch  201 / 525  Training Loss  3.3406391594326124e-05\n",
            "Epoch  40 Batch  202 / 525  Training Loss  6.945128552615643e-05\n",
            "Epoch  40 Batch  203 / 525  Training Loss  4.6166504034772515e-05\n",
            "Epoch  40 Batch  204 / 525  Training Loss  2.426234641461633e-05\n",
            "Epoch  40 Batch  205 / 525  Training Loss  2.8614906113944016e-05\n",
            "Epoch  40 Batch  206 / 525  Training Loss  2.953946568595711e-05\n",
            "Epoch  40 Batch  207 / 525  Training Loss  3.7641315429937094e-05\n",
            "Epoch  40 Batch  208 / 525  Training Loss  3.073830885114148e-05\n",
            "Epoch  40 Batch  209 / 525  Training Loss  2.2608595827477984e-05\n",
            "Epoch  40 Batch  210 / 525  Training Loss  2.6032943424070254e-05\n",
            "Epoch  40 Batch  211 / 525  Training Loss  4.379181700642221e-05\n",
            "Epoch  40 Batch  212 / 525  Training Loss  4.5627439249074087e-05\n",
            "Epoch  40 Batch  213 / 525  Training Loss  2.7149519155500457e-05\n",
            "Epoch  40 Batch  214 / 525  Training Loss  4.616170917870477e-05\n",
            "Epoch  40 Batch  215 / 525  Training Loss  2.778450470941607e-05\n",
            "Epoch  40 Batch  216 / 525  Training Loss  8.296883606817573e-05\n",
            "Epoch  40 Batch  217 / 525  Training Loss  3.4774147934513167e-05\n",
            "Epoch  40 Batch  218 / 525  Training Loss  3.157206083415076e-05\n",
            "Epoch  40 Batch  219 / 525  Training Loss  0.00010973536700475961\n",
            "Epoch  40 Batch  220 / 525  Training Loss  3.434855170780793e-05\n",
            "Epoch  40 Batch  221 / 525  Training Loss  4.385936335893348e-05\n",
            "Epoch  40 Batch  222 / 525  Training Loss  4.66745113953948e-05\n",
            "Epoch  40 Batch  223 / 525  Training Loss  2.8501181077444926e-05\n",
            "Epoch  40 Batch  224 / 525  Training Loss  5.098459223518148e-05\n",
            "Epoch  40 Batch  225 / 525  Training Loss  3.6591409298125654e-05\n",
            "Epoch  40 Batch  226 / 525  Training Loss  4.600837564794347e-05\n",
            "Epoch  40 Batch  227 / 525  Training Loss  4.490667924983427e-05\n",
            "Epoch  40 Batch  228 / 525  Training Loss  2.006119211728219e-05\n",
            "Epoch  40 Batch  229 / 525  Training Loss  2.8635718990699388e-05\n",
            "Epoch  40 Batch  230 / 525  Training Loss  3.2536125218030065e-05\n",
            "Epoch  40 Batch  231 / 525  Training Loss  1.8436530808685347e-05\n",
            "Epoch  40 Batch  232 / 525  Training Loss  5.049292303738184e-05\n",
            "Epoch  40 Batch  233 / 525  Training Loss  5.5199721828103065e-05\n",
            "Epoch  40 Batch  234 / 525  Training Loss  5.387888086261228e-05\n",
            "Epoch  40 Batch  235 / 525  Training Loss  3.3440177503507584e-05\n",
            "Epoch  40 Batch  236 / 525  Training Loss  3.5725221096072346e-05\n",
            "Epoch  40 Batch  237 / 525  Training Loss  2.9611808713525534e-05\n",
            "Epoch  40 Batch  238 / 525  Training Loss  2.1660765924025327e-05\n",
            "Epoch  40 Batch  239 / 525  Training Loss  1.8205089872935787e-05\n",
            "Epoch  40 Batch  240 / 525  Training Loss  4.430585613590665e-05\n",
            "Epoch  40 Batch  241 / 525  Training Loss  3.746073707588948e-05\n",
            "Epoch  40 Batch  242 / 525  Training Loss  1.534500734123867e-05\n",
            "Epoch  40 Batch  243 / 525  Training Loss  5.89711416978389e-05\n",
            "Epoch  40 Batch  244 / 525  Training Loss  3.31916926370468e-05\n",
            "Epoch  40 Batch  245 / 525  Training Loss  4.9923371989279985e-05\n",
            "Epoch  40 Batch  246 / 525  Training Loss  1.4713499695062637e-05\n",
            "Epoch  40 Batch  247 / 525  Training Loss  0.0001301865850109607\n",
            "Epoch  40 Batch  248 / 525  Training Loss  2.775259235932026e-05\n",
            "Epoch  40 Batch  249 / 525  Training Loss  5.084976874059066e-05\n",
            "Epoch  40 Batch  250 / 525  Training Loss  5.144751048646867e-05\n",
            "Epoch  40 Batch  251 / 525  Training Loss  5.118633998790756e-05\n",
            "Epoch  40 Batch  252 / 525  Training Loss  3.3354026527376845e-05\n",
            "Epoch  40 Batch  253 / 525  Training Loss  4.94083869853057e-05\n",
            "Epoch  40 Batch  254 / 525  Training Loss  3.458842547843233e-05\n",
            "Epoch  40 Batch  255 / 525  Training Loss  7.289166387636214e-05\n",
            "Epoch  40 Batch  256 / 525  Training Loss  2.890854375436902e-05\n",
            "Epoch  40 Batch  257 / 525  Training Loss  3.088089943048544e-05\n",
            "Epoch  40 Batch  258 / 525  Training Loss  3.50224509020336e-05\n",
            "Epoch  40 Batch  259 / 525  Training Loss  4.493241794989444e-05\n",
            "Epoch  40 Batch  260 / 525  Training Loss  4.84554584545549e-05\n",
            "Epoch  40 Batch  261 / 525  Training Loss  4.1092458559433e-05\n",
            "Epoch  40 Batch  262 / 525  Training Loss  2.5515264496789314e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  40 Batch  263 / 525  Training Loss  3.8693720853189006e-05\n",
            "Epoch  40 Batch  264 / 525  Training Loss  1.398229505866766e-05\n",
            "Epoch  40 Batch  265 / 525  Training Loss  7.635655492777005e-05\n",
            "Epoch  40 Batch  266 / 525  Training Loss  9.601777492207475e-06\n",
            "Epoch  40 Batch  267 / 525  Training Loss  7.48407474020496e-05\n",
            "Epoch  40 Batch  268 / 525  Training Loss  5.020586104365066e-05\n",
            "Epoch  40 Batch  269 / 525  Training Loss  2.4662289433763362e-05\n",
            "Epoch  40 Batch  270 / 525  Training Loss  5.0956216000486165e-05\n",
            "Epoch  40 Batch  271 / 525  Training Loss  3.690099765663035e-05\n",
            "Epoch  40 Batch  272 / 525  Training Loss  2.9368800824158825e-05\n",
            "Epoch  40 Batch  273 / 525  Training Loss  3.330123217892833e-05\n",
            "Epoch  40 Batch  274 / 525  Training Loss  5.622049866360612e-05\n",
            "Epoch  40 Batch  275 / 525  Training Loss  6.277214561123401e-05\n",
            "Epoch  40 Batch  276 / 525  Training Loss  3.189898416167125e-05\n",
            "Epoch  40 Batch  277 / 525  Training Loss  3.0478815460810438e-05\n",
            "Epoch  40 Batch  278 / 525  Training Loss  3.549190660123713e-05\n",
            "Epoch  40 Batch  279 / 525  Training Loss  4.24721984018106e-05\n",
            "Epoch  40 Batch  280 / 525  Training Loss  4.039327541249804e-05\n",
            "Epoch  40 Batch  281 / 525  Training Loss  3.4606629924383014e-05\n",
            "Epoch  40 Batch  282 / 525  Training Loss  4.056701800436713e-05\n",
            "Epoch  40 Batch  283 / 525  Training Loss  4.0202223317464814e-05\n",
            "Epoch  40 Batch  284 / 525  Training Loss  3.2734795240685344e-05\n",
            "Epoch  40 Batch  285 / 525  Training Loss  4.375421485747211e-05\n",
            "Epoch  40 Batch  286 / 525  Training Loss  4.715613613370806e-05\n",
            "Epoch  40 Batch  287 / 525  Training Loss  3.502520848996937e-05\n",
            "Epoch  40 Batch  288 / 525  Training Loss  4.980977246304974e-05\n",
            "Epoch  40 Batch  289 / 525  Training Loss  3.46392989740707e-05\n",
            "Epoch  40 Batch  290 / 525  Training Loss  5.864204285899177e-05\n",
            "Epoch  40 Batch  291 / 525  Training Loss  1.28437850435148e-05\n",
            "Epoch  40 Batch  292 / 525  Training Loss  3.090995960519649e-05\n",
            "Epoch  40 Batch  293 / 525  Training Loss  6.113440758781508e-05\n",
            "Epoch  40 Batch  294 / 525  Training Loss  3.077917790506035e-05\n",
            "Epoch  40 Batch  295 / 525  Training Loss  3.951359394704923e-05\n",
            "Epoch  40 Batch  296 / 525  Training Loss  3.045779340027366e-05\n",
            "Epoch  40 Batch  297 / 525  Training Loss  5.263760613161139e-05\n",
            "Epoch  40 Batch  298 / 525  Training Loss  3.177181497449055e-05\n",
            "Epoch  40 Batch  299 / 525  Training Loss  7.520812505390495e-05\n",
            "Epoch  40 Batch  300 / 525  Training Loss  5.538899131352082e-05\n",
            "Epoch  40 Batch  301 / 525  Training Loss  3.915869820048101e-05\n",
            "Epoch  40 Batch  302 / 525  Training Loss  2.3215627152239904e-05\n",
            "Epoch  40 Batch  303 / 525  Training Loss  5.373976455302909e-05\n",
            "Epoch  40 Batch  304 / 525  Training Loss  4.081578299519606e-05\n",
            "Epoch  40 Batch  305 / 525  Training Loss  6.27729095867835e-05\n",
            "Epoch  40 Batch  306 / 525  Training Loss  3.810443013207987e-05\n",
            "Epoch  40 Batch  307 / 525  Training Loss  2.390663757978473e-05\n",
            "Epoch  40 Batch  308 / 525  Training Loss  2.671838410606142e-05\n",
            "Epoch  40 Batch  309 / 525  Training Loss  0.0001164816931122914\n",
            "Epoch  40 Batch  310 / 525  Training Loss  2.00875656446442e-05\n",
            "Epoch  40 Batch  311 / 525  Training Loss  6.191238935571164e-05\n",
            "Epoch  40 Batch  312 / 525  Training Loss  3.634944368968718e-05\n",
            "Epoch  40 Batch  313 / 525  Training Loss  5.274841169011779e-05\n",
            "Epoch  40 Batch  314 / 525  Training Loss  3.843042577500455e-05\n",
            "Epoch  40 Batch  315 / 525  Training Loss  3.67733919119928e-05\n",
            "Epoch  40 Batch  316 / 525  Training Loss  2.015277641476132e-05\n",
            "Epoch  40 Batch  317 / 525  Training Loss  5.571916699409485e-05\n",
            "Epoch  40 Batch  318 / 525  Training Loss  3.6827703297603875e-05\n",
            "Epoch  40 Batch  319 / 525  Training Loss  2.3697806682321243e-05\n",
            "Epoch  40 Batch  320 / 525  Training Loss  2.7978978323517367e-05\n",
            "Epoch  40 Batch  321 / 525  Training Loss  1.85268945642747e-05\n",
            "Epoch  40 Batch  322 / 525  Training Loss  4.903043736703694e-05\n",
            "Epoch  40 Batch  323 / 525  Training Loss  3.1168252462521195e-05\n",
            "Epoch  40 Batch  324 / 525  Training Loss  3.656520857475698e-05\n",
            "Epoch  40 Batch  325 / 525  Training Loss  2.9121769330231473e-05\n",
            "Epoch  40 Batch  326 / 525  Training Loss  2.6667379643185996e-05\n",
            "Epoch  40 Batch  327 / 525  Training Loss  2.9955190257169306e-05\n",
            "Epoch  40 Batch  328 / 525  Training Loss  2.9979613827890716e-05\n",
            "Epoch  40 Batch  329 / 525  Training Loss  5.0702372391242534e-05\n",
            "Epoch  40 Batch  330 / 525  Training Loss  2.0682522517745383e-05\n",
            "Epoch  40 Batch  331 / 525  Training Loss  3.9998751162784174e-05\n",
            "Epoch  40 Batch  332 / 525  Training Loss  2.8699805625365116e-05\n",
            "Epoch  40 Batch  333 / 525  Training Loss  3.5420103813521564e-05\n",
            "Epoch  40 Batch  334 / 525  Training Loss  4.151299799559638e-05\n",
            "Epoch  40 Batch  335 / 525  Training Loss  4.223052383167669e-05\n",
            "Epoch  40 Batch  336 / 525  Training Loss  4.202115087537095e-05\n",
            "Epoch  40 Batch  337 / 525  Training Loss  2.948506153188646e-05\n",
            "Epoch  40 Batch  338 / 525  Training Loss  5.48296156921424e-05\n",
            "Epoch  40 Batch  339 / 525  Training Loss  3.230496804462746e-05\n",
            "Epoch  40 Batch  340 / 525  Training Loss  2.5733515940373763e-05\n",
            "Epoch  40 Batch  341 / 525  Training Loss  7.910998829174787e-05\n",
            "Epoch  40 Batch  342 / 525  Training Loss  3.727051807800308e-05\n",
            "Epoch  40 Batch  343 / 525  Training Loss  4.5566786866402254e-05\n",
            "Epoch  40 Batch  344 / 525  Training Loss  0.00011307032400509343\n",
            "Epoch  40 Batch  345 / 525  Training Loss  4.591619290295057e-05\n",
            "Epoch  40 Batch  346 / 525  Training Loss  2.717362076509744e-05\n",
            "Epoch  40 Batch  347 / 525  Training Loss  2.1878597181057557e-05\n",
            "Epoch  40 Batch  348 / 525  Training Loss  3.905439734808169e-05\n",
            "Epoch  40 Batch  349 / 525  Training Loss  5.1153445383533835e-05\n",
            "Epoch  40 Batch  350 / 525  Training Loss  4.5288172259461135e-05\n",
            "Epoch  40 Batch  351 / 525  Training Loss  3.405504685360938e-05\n",
            "Epoch  40 Batch  352 / 525  Training Loss  3.331517655169591e-05\n",
            "Epoch  40 Batch  353 / 525  Training Loss  2.2992651793174446e-05\n",
            "Epoch  40 Batch  354 / 525  Training Loss  3.9385256968671456e-05\n",
            "Epoch  40 Batch  355 / 525  Training Loss  5.4588996135862544e-05\n",
            "Epoch  40 Batch  356 / 525  Training Loss  3.051087878702674e-05\n",
            "Epoch  40 Batch  357 / 525  Training Loss  2.2046984668122604e-05\n",
            "Epoch  40 Batch  358 / 525  Training Loss  6.253601895878091e-05\n",
            "Epoch  40 Batch  359 / 525  Training Loss  1.5119023373699747e-05\n",
            "Epoch  40 Batch  360 / 525  Training Loss  3.2115411158883944e-05\n",
            "Epoch  40 Batch  361 / 525  Training Loss  4.79603186249733e-05\n",
            "Epoch  40 Batch  362 / 525  Training Loss  4.43175267719198e-05\n",
            "Epoch  40 Batch  363 / 525  Training Loss  1.4145074601401575e-05\n",
            "Epoch  40 Batch  364 / 525  Training Loss  4.811754479305819e-05\n",
            "Epoch  40 Batch  365 / 525  Training Loss  3.3670054108370095e-05\n",
            "Epoch  40 Batch  366 / 525  Training Loss  2.1750529413111508e-05\n",
            "Epoch  40 Batch  367 / 525  Training Loss  4.2724273953353986e-05\n",
            "Epoch  40 Batch  368 / 525  Training Loss  3.520823520375416e-05\n",
            "Epoch  40 Batch  369 / 525  Training Loss  3.2457930501550436e-05\n",
            "Epoch  40 Batch  370 / 525  Training Loss  3.8009944546502084e-05\n",
            "Epoch  40 Batch  371 / 525  Training Loss  3.9695958548691124e-05\n",
            "Epoch  40 Batch  372 / 525  Training Loss  4.974597322870977e-05\n",
            "Epoch  40 Batch  373 / 525  Training Loss  5.5221713409991935e-05\n",
            "Epoch  40 Batch  374 / 525  Training Loss  2.8969210688956082e-05\n",
            "Epoch  40 Batch  375 / 525  Training Loss  2.0268391381250694e-05\n",
            "Epoch  40 Batch  376 / 525  Training Loss  2.3375654564006254e-05\n",
            "Epoch  40 Batch  377 / 525  Training Loss  3.729182208189741e-05\n",
            "Epoch  40 Batch  378 / 525  Training Loss  1.4916770851414185e-05\n",
            "Epoch  40 Batch  379 / 525  Training Loss  1.4742321582161821e-05\n",
            "Epoch  40 Batch  380 / 525  Training Loss  3.35883887601085e-05\n",
            "Epoch  40 Batch  381 / 525  Training Loss  3.177015605615452e-05\n",
            "Epoch  40 Batch  382 / 525  Training Loss  4.216930392431095e-05\n",
            "Epoch  40 Batch  383 / 525  Training Loss  3.42335770255886e-05\n",
            "Epoch  40 Batch  384 / 525  Training Loss  2.7700170903699473e-05\n",
            "Epoch  40 Batch  385 / 525  Training Loss  4.2068717448273674e-05\n",
            "Epoch  40 Batch  386 / 525  Training Loss  5.682708433596417e-05\n",
            "Epoch  40 Batch  387 / 525  Training Loss  2.6709623853093944e-05\n",
            "Epoch  40 Batch  388 / 525  Training Loss  3.0382900149561465e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  40 Batch  389 / 525  Training Loss  4.3550859118113294e-05\n",
            "Epoch  40 Batch  390 / 525  Training Loss  4.9216625484405085e-05\n",
            "Epoch  40 Batch  391 / 525  Training Loss  3.70344496332109e-05\n",
            "Epoch  40 Batch  392 / 525  Training Loss  3.807373650488444e-05\n",
            "Epoch  40 Batch  393 / 525  Training Loss  2.7539534130482934e-05\n",
            "Epoch  40 Batch  394 / 525  Training Loss  3.682313399622217e-05\n",
            "Epoch  40 Batch  395 / 525  Training Loss  3.0937408155296e-05\n",
            "Epoch  40 Batch  396 / 525  Training Loss  3.7192803574725986e-05\n",
            "Epoch  40 Batch  397 / 525  Training Loss  3.1856521673034877e-05\n",
            "Epoch  40 Batch  398 / 525  Training Loss  1.031908868753817e-05\n",
            "Epoch  40 Batch  399 / 525  Training Loss  3.196486068191007e-05\n",
            "Epoch  40 Batch  400 / 525  Training Loss  3.534930510795675e-05\n",
            "Epoch  40 Batch  401 / 525  Training Loss  1.9498835172271356e-05\n",
            "Epoch  40 Batch  402 / 525  Training Loss  3.167239628965035e-05\n",
            "Epoch  40 Batch  403 / 525  Training Loss  3.406573159736581e-05\n",
            "Epoch  40 Batch  404 / 525  Training Loss  2.8601987651200034e-05\n",
            "Epoch  40 Batch  405 / 525  Training Loss  5.066412268206477e-05\n",
            "Epoch  40 Batch  406 / 525  Training Loss  3.2213130907621235e-05\n",
            "Epoch  40 Batch  407 / 525  Training Loss  4.093056850251742e-05\n",
            "Epoch  40 Batch  408 / 525  Training Loss  4.8318928747903556e-05\n",
            "Epoch  40 Batch  409 / 525  Training Loss  3.6072371585760266e-05\n",
            "Epoch  40 Batch  410 / 525  Training Loss  2.4255972675746307e-05\n",
            "Epoch  40 Batch  411 / 525  Training Loss  3.261224628658965e-05\n",
            "Epoch  40 Batch  412 / 525  Training Loss  3.7638612411683425e-05\n",
            "Epoch  40 Batch  413 / 525  Training Loss  6.150500848889351e-05\n",
            "Epoch  40 Batch  414 / 525  Training Loss  2.96757098112721e-05\n",
            "Epoch  40 Batch  415 / 525  Training Loss  3.0645463994005695e-05\n",
            "Epoch  40 Batch  416 / 525  Training Loss  4.634754077414982e-05\n",
            "Epoch  40 Batch  417 / 525  Training Loss  2.3944530767039396e-05\n",
            "Epoch  40 Batch  418 / 525  Training Loss  4.08278574468568e-05\n",
            "Epoch  40 Batch  419 / 525  Training Loss  2.429833148198668e-05\n",
            "Epoch  40 Batch  420 / 525  Training Loss  4.251186692272313e-05\n",
            "Epoch  40 Batch  421 / 525  Training Loss  3.751992335310206e-05\n",
            "Epoch  40 Batch  422 / 525  Training Loss  2.9366794478846714e-05\n",
            "Epoch  40 Batch  423 / 525  Training Loss  5.955977758276276e-05\n",
            "Epoch  40 Batch  424 / 525  Training Loss  3.044716504518874e-05\n",
            "Epoch  40 Batch  425 / 525  Training Loss  3.893377652275376e-05\n",
            "Epoch  40 Batch  426 / 525  Training Loss  3.139104228466749e-05\n",
            "Epoch  40 Batch  427 / 525  Training Loss  4.314287070883438e-05\n",
            "Epoch  40 Batch  428 / 525  Training Loss  2.8993821615586057e-05\n",
            "Epoch  40 Batch  429 / 525  Training Loss  3.784434375120327e-05\n",
            "Epoch  40 Batch  430 / 525  Training Loss  4.1434759623371065e-05\n",
            "Epoch  40 Batch  431 / 525  Training Loss  3.781656050705351e-05\n",
            "Epoch  40 Batch  432 / 525  Training Loss  3.490701419650577e-05\n",
            "Epoch  40 Batch  433 / 525  Training Loss  4.497739428188652e-05\n",
            "Epoch  40 Batch  434 / 525  Training Loss  3.7526006053667516e-05\n",
            "Epoch  40 Batch  435 / 525  Training Loss  3.6643697967519984e-05\n",
            "Epoch  40 Batch  436 / 525  Training Loss  3.9761693187756464e-05\n",
            "Epoch  40 Batch  437 / 525  Training Loss  5.0237613322678953e-05\n",
            "Epoch  40 Batch  438 / 525  Training Loss  3.3464642910985276e-05\n",
            "Epoch  40 Batch  439 / 525  Training Loss  4.956070188200101e-05\n",
            "Epoch  40 Batch  440 / 525  Training Loss  1.246014016942354e-05\n",
            "Epoch  40 Batch  441 / 525  Training Loss  2.11379119718913e-05\n",
            "Epoch  40 Batch  442 / 525  Training Loss  3.743906199815683e-05\n",
            "Epoch  40 Batch  443 / 525  Training Loss  5.5686086852801964e-05\n",
            "Epoch  40 Batch  444 / 525  Training Loss  6.317995575955138e-05\n",
            "Epoch  40 Batch  445 / 525  Training Loss  3.877747803926468e-05\n",
            "Epoch  40 Batch  446 / 525  Training Loss  2.053500611509662e-05\n",
            "Epoch  40 Batch  447 / 525  Training Loss  4.409950997796841e-05\n",
            "Epoch  40 Batch  448 / 525  Training Loss  3.291439497843385e-05\n",
            "Epoch  40 Batch  449 / 525  Training Loss  3.92476431443356e-05\n",
            "Epoch  40 Batch  450 / 525  Training Loss  2.2783537133364007e-05\n",
            "Epoch  40 Batch  451 / 525  Training Loss  3.195145109202713e-05\n",
            "Epoch  40 Batch  452 / 525  Training Loss  4.0183269447879866e-05\n",
            "Epoch  40 Batch  453 / 525  Training Loss  2.49736422119895e-05\n",
            "Epoch  40 Batch  454 / 525  Training Loss  4.888185139861889e-05\n",
            "Epoch  40 Batch  455 / 525  Training Loss  1.928587516886182e-05\n",
            "Epoch  40 Batch  456 / 525  Training Loss  3.792800271185115e-05\n",
            "Epoch  40 Batch  457 / 525  Training Loss  2.7801424948847853e-05\n",
            "Epoch  40 Batch  458 / 525  Training Loss  3.3501783036626875e-05\n",
            "Epoch  40 Batch  459 / 525  Training Loss  5.423864786280319e-05\n",
            "Epoch  40 Batch  460 / 525  Training Loss  1.5032981536933221e-05\n",
            "Epoch  40 Batch  461 / 525  Training Loss  1.9742376025533304e-05\n",
            "Epoch  40 Batch  462 / 525  Training Loss  3.828085391432978e-05\n",
            "Epoch  40 Batch  463 / 525  Training Loss  2.307018803548999e-05\n",
            "Epoch  40 Batch  464 / 525  Training Loss  5.5666034313617274e-05\n",
            "Epoch  40 Batch  465 / 525  Training Loss  2.649690941325389e-05\n",
            "Epoch  40 Batch  466 / 525  Training Loss  4.2985564505215734e-05\n",
            "Epoch  40 Batch  467 / 525  Training Loss  4.298371641198173e-05\n",
            "Epoch  40 Batch  468 / 525  Training Loss  5.283959399093874e-05\n",
            "Epoch  40 Batch  469 / 525  Training Loss  4.6426728658843786e-05\n",
            "Epoch  40 Batch  470 / 525  Training Loss  4.6243949327617884e-05\n",
            "Epoch  40 Batch  471 / 525  Training Loss  3.230042784707621e-05\n",
            "Epoch  40 Batch  472 / 525  Training Loss  6.988306995481253e-05\n",
            "Epoch  40 Batch  473 / 525  Training Loss  5.1279686886118725e-05\n",
            "Epoch  40 Batch  474 / 525  Training Loss  4.0525104850530624e-05\n",
            "Epoch  40 Batch  475 / 525  Training Loss  3.171495700371452e-05\n",
            "Epoch  40 Batch  476 / 525  Training Loss  2.9137314413674176e-05\n",
            "Epoch  40 Batch  477 / 525  Training Loss  1.889493432827294e-05\n",
            "Epoch  40 Batch  478 / 525  Training Loss  3.4030825190711766e-05\n",
            "Epoch  40 Batch  479 / 525  Training Loss  2.7448088076198474e-05\n",
            "Epoch  40 Batch  480 / 525  Training Loss  2.1742207536590286e-05\n",
            "Epoch  40 Batch  481 / 525  Training Loss  3.3103908208431676e-05\n",
            "Epoch  40 Batch  482 / 525  Training Loss  3.803708386840299e-05\n",
            "Epoch  40 Batch  483 / 525  Training Loss  2.79096893791575e-05\n",
            "Epoch  40 Batch  484 / 525  Training Loss  4.4620330299949273e-05\n",
            "Epoch  40 Batch  485 / 525  Training Loss  2.6950137907988392e-05\n",
            "Epoch  40 Batch  486 / 525  Training Loss  2.1290767108439468e-05\n",
            "Epoch  40 Batch  487 / 525  Training Loss  4.3671745515894145e-05\n",
            "Epoch  40 Batch  488 / 525  Training Loss  3.391510836081579e-05\n",
            "Epoch  40 Batch  489 / 525  Training Loss  2.8825443223468028e-05\n",
            "Epoch  40 Batch  490 / 525  Training Loss  2.6652585802366957e-05\n",
            "Epoch  40 Batch  491 / 525  Training Loss  3.189663402736187e-05\n",
            "Epoch  40 Batch  492 / 525  Training Loss  2.9296541470102966e-05\n",
            "Epoch  40 Batch  493 / 525  Training Loss  2.236213185824454e-05\n",
            "Epoch  40 Batch  494 / 525  Training Loss  2.00007671082858e-05\n",
            "Epoch  40 Batch  495 / 525  Training Loss  2.9167753382353112e-05\n",
            "Epoch  40 Batch  496 / 525  Training Loss  2.894154386012815e-05\n",
            "Epoch  40 Batch  497 / 525  Training Loss  5.52688907191623e-05\n",
            "Epoch  40 Batch  498 / 525  Training Loss  3.5847999242832884e-05\n",
            "Epoch  40 Batch  499 / 525  Training Loss  6.17386685917154e-05\n",
            "Epoch  40 Batch  500 / 525  Training Loss  2.4582510377513245e-05\n",
            "Epoch  40 Batch  501 / 525  Training Loss  3.453079261817038e-05\n",
            "Epoch  40 Batch  502 / 525  Training Loss  5.143026282894425e-05\n",
            "Epoch  40 Batch  503 / 525  Training Loss  2.7762604076997377e-05\n",
            "Epoch  40 Batch  504 / 525  Training Loss  5.6845456128939986e-05\n",
            "Epoch  40 Batch  505 / 525  Training Loss  2.735661473707296e-05\n",
            "Epoch  40 Batch  506 / 525  Training Loss  1.8145790818380192e-05\n",
            "Epoch  40 Batch  507 / 525  Training Loss  5.921256888541393e-05\n",
            "Epoch  40 Batch  508 / 525  Training Loss  2.8615799237741157e-05\n",
            "Epoch  40 Batch  509 / 525  Training Loss  3.5966844734502956e-05\n",
            "Epoch  40 Batch  510 / 525  Training Loss  1.8474511307431385e-05\n",
            "Epoch  40 Batch  511 / 525  Training Loss  0.00015669349522795528\n",
            "Epoch  40 Batch  512 / 525  Training Loss  1.579190393385943e-05\n",
            "Epoch  40 Batch  513 / 525  Training Loss  4.72604988317471e-05\n",
            "Epoch  40 Batch  514 / 525  Training Loss  4.5379689254332334e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  40 Batch  515 / 525  Training Loss  3.089383608312346e-05\n",
            "Epoch  40 Batch  516 / 525  Training Loss  1.9889230316039175e-05\n",
            "Epoch  40 Batch  517 / 525  Training Loss  4.3931158870691434e-05\n",
            "Epoch  40 Batch  518 / 525  Training Loss  2.8783146262867376e-05\n",
            "Epoch  40 Batch  519 / 525  Training Loss  3.5644803574541584e-05\n",
            "Epoch  40 Batch  520 / 525  Training Loss  3.1192172173177823e-05\n",
            "Epoch  40 Batch  521 / 525  Training Loss  1.5784258721396327e-05\n",
            "Epoch  40 Batch  522 / 525  Training Loss  4.841401459998451e-05\n",
            "Epoch  40 Batch  523 / 525  Training Loss  4.564117261907086e-05\n",
            "Epoch  40 Batch  524 / 525  Training Loss  2.345841858186759e-05\n",
            "  41    |    -    |   0.000040   | 64.225000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 41\n",
            "Epoch  41 Batch  0 / 525  Training Loss  2.1683177692466415e-05\n",
            "Epoch  41 Batch  1 / 525  Training Loss  1.6750027498346753e-05\n",
            "Epoch  41 Batch  2 / 525  Training Loss  2.3281547328224406e-05\n",
            "Epoch  41 Batch  3 / 525  Training Loss  3.709446536959149e-05\n",
            "Epoch  41 Batch  4 / 525  Training Loss  2.645365566422697e-05\n",
            "Epoch  41 Batch  5 / 525  Training Loss  2.283889989485033e-05\n",
            "Epoch  41 Batch  6 / 525  Training Loss  2.6540510589256883e-05\n",
            "Epoch  41 Batch  7 / 525  Training Loss  3.034281326108612e-05\n",
            "Epoch  41 Batch  8 / 525  Training Loss  3.4188553399872035e-05\n",
            "Epoch  41 Batch  9 / 525  Training Loss  2.126442086591851e-05\n",
            "Epoch  41 Batch  10 / 525  Training Loss  3.318173548905179e-05\n",
            "Epoch  41 Batch  11 / 525  Training Loss  2.653199408086948e-05\n",
            "Epoch  41 Batch  12 / 525  Training Loss  2.5170485969283618e-05\n",
            "Epoch  41 Batch  13 / 525  Training Loss  2.1134004782652482e-05\n",
            "Epoch  41 Batch  14 / 525  Training Loss  3.4349668567301705e-05\n",
            "Epoch  41 Batch  15 / 525  Training Loss  1.6573740140302107e-05\n",
            "Epoch  41 Batch  16 / 525  Training Loss  4.52676868007984e-05\n",
            "Epoch  41 Batch  17 / 525  Training Loss  2.7769758162321523e-05\n",
            "Epoch  41 Batch  18 / 525  Training Loss  2.25747971853707e-05\n",
            "Epoch  41 Batch  19 / 525  Training Loss  2.3100596081349067e-05\n",
            "Epoch  41 Batch  20 / 525  Training Loss  2.5528628611937165e-05\n",
            "Epoch  41 Batch  21 / 525  Training Loss  2.1133195332367904e-05\n",
            "Epoch  41 Batch  22 / 525  Training Loss  2.979860619234387e-05\n",
            "Epoch  41 Batch  23 / 525  Training Loss  3.0218949177651666e-05\n",
            "Epoch  41 Batch  24 / 525  Training Loss  3.0468401746475138e-05\n",
            "Epoch  41 Batch  25 / 525  Training Loss  1.4368260053743143e-05\n",
            "Epoch  41 Batch  26 / 525  Training Loss  4.750898006022908e-05\n",
            "Epoch  41 Batch  27 / 525  Training Loss  1.9359207726665772e-05\n",
            "Epoch  41 Batch  28 / 525  Training Loss  2.2505864762933925e-05\n",
            "Epoch  41 Batch  29 / 525  Training Loss  1.8739548977464437e-05\n",
            "Epoch  41 Batch  30 / 525  Training Loss  3.531468610162847e-05\n",
            "Epoch  41 Batch  31 / 525  Training Loss  3.829191336990334e-05\n",
            "Epoch  41 Batch  32 / 525  Training Loss  1.9278837498859502e-05\n",
            "Epoch  41 Batch  33 / 525  Training Loss  3.267301508458331e-05\n",
            "Epoch  41 Batch  34 / 525  Training Loss  2.893241799029056e-05\n",
            "Epoch  41 Batch  35 / 525  Training Loss  2.006082831940148e-05\n",
            "Epoch  41 Batch  36 / 525  Training Loss  3.2758271117927507e-05\n",
            "Epoch  41 Batch  37 / 525  Training Loss  2.9078171792207286e-05\n",
            "Epoch  41 Batch  38 / 525  Training Loss  4.1060528019443154e-05\n",
            "Epoch  41 Batch  39 / 525  Training Loss  4.489420825848356e-05\n",
            "Epoch  41 Batch  40 / 525  Training Loss  4.008651740150526e-05\n",
            "Epoch  41 Batch  41 / 525  Training Loss  3.297227158327587e-05\n",
            "Epoch  41 Batch  42 / 525  Training Loss  1.9286835595266894e-05\n",
            "Epoch  41 Batch  43 / 525  Training Loss  3.353838837938383e-05\n",
            "Epoch  41 Batch  44 / 525  Training Loss  2.733572546276264e-05\n",
            "Epoch  41 Batch  45 / 525  Training Loss  4.4442749640438706e-05\n",
            "Epoch  41 Batch  46 / 525  Training Loss  3.0232356948545203e-05\n",
            "Epoch  41 Batch  47 / 525  Training Loss  4.373902265797369e-05\n",
            "Epoch  41 Batch  48 / 525  Training Loss  2.5627148716012016e-05\n",
            "Epoch  41 Batch  49 / 525  Training Loss  2.1316593119991012e-05\n",
            "Epoch  41 Batch  50 / 525  Training Loss  1.9663057173602283e-05\n",
            "Epoch  41 Batch  51 / 525  Training Loss  2.069580295938067e-05\n",
            "Epoch  41 Batch  52 / 525  Training Loss  1.4203607861418277e-05\n",
            "Epoch  41 Batch  53 / 525  Training Loss  2.0842160665779375e-05\n",
            "Epoch  41 Batch  54 / 525  Training Loss  2.0747977032442577e-05\n",
            "Epoch  41 Batch  55 / 525  Training Loss  9.791039701667614e-06\n",
            "Epoch  41 Batch  56 / 525  Training Loss  3.053980981349014e-05\n",
            "Epoch  41 Batch  57 / 525  Training Loss  1.9248675016569905e-05\n",
            "Epoch  41 Batch  58 / 525  Training Loss  4.1337912989547476e-05\n",
            "Epoch  41 Batch  59 / 525  Training Loss  3.0450095437117852e-05\n",
            "Epoch  41 Batch  60 / 525  Training Loss  2.7586123906075954e-05\n",
            "Epoch  41 Batch  61 / 525  Training Loss  3.0075587346800603e-05\n",
            "Epoch  41 Batch  62 / 525  Training Loss  3.721361645148136e-05\n",
            "Epoch  41 Batch  63 / 525  Training Loss  3.510787428240292e-05\n",
            "Epoch  41 Batch  64 / 525  Training Loss  3.1293729989556596e-05\n",
            "Epoch  41 Batch  65 / 525  Training Loss  2.731250788201578e-05\n",
            "Epoch  41 Batch  66 / 525  Training Loss  2.4605333237559535e-05\n",
            "Epoch  41 Batch  67 / 525  Training Loss  3.492924588499591e-05\n",
            "Epoch  41 Batch  68 / 525  Training Loss  1.4320408808998764e-05\n",
            "Epoch  41 Batch  69 / 525  Training Loss  3.111115438514389e-05\n",
            "Epoch  41 Batch  70 / 525  Training Loss  2.746757490967866e-05\n",
            "Epoch  41 Batch  71 / 525  Training Loss  3.7043260817881674e-05\n",
            "Epoch  41 Batch  72 / 525  Training Loss  2.068373214569874e-05\n",
            "Epoch  41 Batch  73 / 525  Training Loss  1.0828031008713879e-05\n",
            "Epoch  41 Batch  74 / 525  Training Loss  4.025326779810712e-05\n",
            "Epoch  41 Batch  75 / 525  Training Loss  2.938237230409868e-05\n",
            "Epoch  41 Batch  76 / 525  Training Loss  2.985965329571627e-05\n",
            "Epoch  41 Batch  77 / 525  Training Loss  2.420360760879703e-05\n",
            "Epoch  41 Batch  78 / 525  Training Loss  1.7886715795611963e-05\n",
            "Epoch  41 Batch  79 / 525  Training Loss  2.684875471459236e-05\n",
            "Epoch  41 Batch  80 / 525  Training Loss  3.134281359962188e-05\n",
            "Epoch  41 Batch  81 / 525  Training Loss  2.4524139007553458e-05\n",
            "Epoch  41 Batch  82 / 525  Training Loss  1.572074143041391e-05\n",
            "Epoch  41 Batch  83 / 525  Training Loss  2.141694312740583e-05\n",
            "Epoch  41 Batch  84 / 525  Training Loss  4.161911056144163e-05\n",
            "Epoch  41 Batch  85 / 525  Training Loss  2.4841599952196702e-05\n",
            "Epoch  41 Batch  86 / 525  Training Loss  2.6149686163989827e-05\n",
            "Epoch  41 Batch  87 / 525  Training Loss  1.0995006050507072e-05\n",
            "Epoch  41 Batch  88 / 525  Training Loss  3.712466423166916e-05\n",
            "Epoch  41 Batch  89 / 525  Training Loss  2.724281875998713e-05\n",
            "Epoch  41 Batch  90 / 525  Training Loss  2.5492092390777543e-05\n",
            "Epoch  41 Batch  91 / 525  Training Loss  2.2494154109153897e-05\n",
            "Epoch  41 Batch  92 / 525  Training Loss  3.461371306912042e-05\n",
            "Epoch  41 Batch  93 / 525  Training Loss  4.1631486965343356e-05\n",
            "Epoch  41 Batch  94 / 525  Training Loss  2.3701661120867357e-05\n",
            "Epoch  41 Batch  95 / 525  Training Loss  2.2275489754974842e-05\n",
            "Epoch  41 Batch  96 / 525  Training Loss  1.833395435824059e-05\n",
            "Epoch  41 Batch  97 / 525  Training Loss  2.9375023586908355e-05\n",
            "Epoch  41 Batch  98 / 525  Training Loss  3.89924316550605e-05\n",
            "Epoch  41 Batch  99 / 525  Training Loss  2.0036473870277405e-05\n",
            "Epoch  41 Batch  100 / 525  Training Loss  3.388912955415435e-05\n",
            "Epoch  41 Batch  101 / 525  Training Loss  3.0391940526897088e-05\n",
            "Epoch  41 Batch  102 / 525  Training Loss  1.97079007193679e-05\n",
            "Epoch  41 Batch  103 / 525  Training Loss  4.612436532624997e-05\n",
            "Epoch  41 Batch  104 / 525  Training Loss  2.3018013962428086e-05\n",
            "Epoch  41 Batch  105 / 525  Training Loss  3.6456036468734965e-05\n",
            "Epoch  41 Batch  106 / 525  Training Loss  1.8107533833244815e-05\n",
            "Epoch  41 Batch  107 / 525  Training Loss  1.8981529137818143e-05\n",
            "Epoch  41 Batch  108 / 525  Training Loss  1.906280522234738e-05\n",
            "Epoch  41 Batch  109 / 525  Training Loss  3.2065850973594934e-05\n",
            "Epoch  41 Batch  110 / 525  Training Loss  5.5043172324076295e-05\n",
            "Epoch  41 Batch  111 / 525  Training Loss  3.5746950743487105e-05\n",
            "Epoch  41 Batch  112 / 525  Training Loss  2.7319456421537325e-05\n",
            "Epoch  41 Batch  113 / 525  Training Loss  1.1232969882257748e-05\n",
            "Epoch  41 Batch  114 / 525  Training Loss  1.307260936300736e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  41 Batch  115 / 525  Training Loss  2.1181276679271832e-05\n",
            "Epoch  41 Batch  116 / 525  Training Loss  3.0348648579092696e-05\n",
            "Epoch  41 Batch  117 / 525  Training Loss  2.1470446881721728e-05\n",
            "Epoch  41 Batch  118 / 525  Training Loss  2.9405771783785895e-05\n",
            "Epoch  41 Batch  119 / 525  Training Loss  1.8004689991357736e-05\n",
            "Epoch  41 Batch  120 / 525  Training Loss  1.9630617316579446e-05\n",
            "Epoch  41 Batch  121 / 525  Training Loss  3.177310645696707e-05\n",
            "Epoch  41 Batch  122 / 525  Training Loss  1.96309520106297e-05\n",
            "Epoch  41 Batch  123 / 525  Training Loss  2.0114908693358302e-05\n",
            "Epoch  41 Batch  124 / 525  Training Loss  1.9634211639640853e-05\n",
            "Epoch  41 Batch  125 / 525  Training Loss  1.9263265130575746e-05\n",
            "Epoch  41 Batch  126 / 525  Training Loss  1.976997737074271e-05\n",
            "Epoch  41 Batch  127 / 525  Training Loss  3.002103403559886e-05\n",
            "Epoch  41 Batch  128 / 525  Training Loss  1.5627114407834597e-05\n",
            "Epoch  41 Batch  129 / 525  Training Loss  2.198883521487005e-05\n",
            "Epoch  41 Batch  130 / 525  Training Loss  2.108290027535986e-05\n",
            "Epoch  41 Batch  131 / 525  Training Loss  2.0563848011079244e-05\n",
            "Epoch  41 Batch  132 / 525  Training Loss  2.5272369384765625e-05\n",
            "Epoch  41 Batch  133 / 525  Training Loss  3.143251160508953e-05\n",
            "Epoch  41 Batch  134 / 525  Training Loss  2.8226315407664515e-05\n",
            "Epoch  41 Batch  135 / 525  Training Loss  1.8567890947451815e-05\n",
            "Epoch  41 Batch  136 / 525  Training Loss  1.3750088328379206e-05\n",
            "Epoch  41 Batch  137 / 525  Training Loss  3.129720062133856e-05\n",
            "Epoch  41 Batch  138 / 525  Training Loss  2.2630223611486144e-05\n",
            "Epoch  41 Batch  139 / 525  Training Loss  3.507847577566281e-05\n",
            "Epoch  41 Batch  140 / 525  Training Loss  3.0066687031649053e-05\n",
            "Epoch  41 Batch  141 / 525  Training Loss  1.5978173905750737e-05\n",
            "Epoch  41 Batch  142 / 525  Training Loss  1.1002955034200568e-05\n",
            "Epoch  41 Batch  143 / 525  Training Loss  1.640541449887678e-05\n",
            "Epoch  41 Batch  144 / 525  Training Loss  2.0849995053140447e-05\n",
            "Epoch  41 Batch  145 / 525  Training Loss  2.7136697099194862e-05\n",
            "Epoch  41 Batch  146 / 525  Training Loss  1.792268085409887e-05\n",
            "Epoch  41 Batch  147 / 525  Training Loss  2.8301941711106338e-05\n",
            "Epoch  41 Batch  148 / 525  Training Loss  2.6187641196884215e-05\n",
            "Epoch  41 Batch  149 / 525  Training Loss  2.750864041445311e-05\n",
            "Epoch  41 Batch  150 / 525  Training Loss  1.854080801422242e-05\n",
            "Epoch  41 Batch  151 / 525  Training Loss  2.843746551661752e-05\n",
            "Epoch  41 Batch  152 / 525  Training Loss  3.3127562346635386e-05\n",
            "Epoch  41 Batch  153 / 525  Training Loss  2.3173895897343755e-05\n",
            "Epoch  41 Batch  154 / 525  Training Loss  2.7893005608348176e-05\n",
            "Epoch  41 Batch  155 / 525  Training Loss  2.490706719981972e-05\n",
            "Epoch  41 Batch  156 / 525  Training Loss  8.810793588054366e-06\n",
            "Epoch  41 Batch  157 / 525  Training Loss  2.8298591132625006e-05\n",
            "Epoch  41 Batch  158 / 525  Training Loss  2.60752894973848e-05\n",
            "Epoch  41 Batch  159 / 525  Training Loss  3.5478245990816504e-05\n",
            "Epoch  41 Batch  160 / 525  Training Loss  2.9362741770455614e-05\n",
            "Epoch  41 Batch  161 / 525  Training Loss  2.714467336772941e-05\n",
            "Epoch  41 Batch  162 / 525  Training Loss  1.7666347048361786e-05\n",
            "Epoch  41 Batch  163 / 525  Training Loss  2.684016726561822e-05\n",
            "Epoch  41 Batch  164 / 525  Training Loss  3.580841439543292e-05\n",
            "Epoch  41 Batch  165 / 525  Training Loss  1.9162698663421907e-05\n",
            "Epoch  41 Batch  166 / 525  Training Loss  3.244923573220149e-05\n",
            "Epoch  41 Batch  167 / 525  Training Loss  1.7100970580941066e-05\n",
            "Epoch  41 Batch  168 / 525  Training Loss  3.613659646362066e-05\n",
            "Epoch  41 Batch  169 / 525  Training Loss  3.087722507189028e-05\n",
            "Epoch  41 Batch  170 / 525  Training Loss  2.28583267016802e-05\n",
            "Epoch  41 Batch  171 / 525  Training Loss  2.7165000574314035e-05\n",
            "Epoch  41 Batch  172 / 525  Training Loss  1.9073217117693275e-05\n",
            "Epoch  41 Batch  173 / 525  Training Loss  3.973143975599669e-05\n",
            "Epoch  41 Batch  174 / 525  Training Loss  5.416070416686125e-05\n",
            "Epoch  41 Batch  175 / 525  Training Loss  2.2550613721250556e-05\n",
            "Epoch  41 Batch  176 / 525  Training Loss  2.2324065866996534e-05\n",
            "Epoch  41 Batch  177 / 525  Training Loss  2.1373958588810638e-05\n",
            "Epoch  41 Batch  178 / 525  Training Loss  2.14048213820206e-05\n",
            "Epoch  41 Batch  179 / 525  Training Loss  3.2712239772081375e-05\n",
            "Epoch  41 Batch  180 / 525  Training Loss  1.5670189895899966e-05\n",
            "Epoch  41 Batch  181 / 525  Training Loss  2.5990002541220747e-05\n",
            "Epoch  41 Batch  182 / 525  Training Loss  1.775928285496775e-05\n",
            "Epoch  41 Batch  183 / 525  Training Loss  1.2133187738072593e-05\n",
            "Epoch  41 Batch  184 / 525  Training Loss  2.7929996576858684e-05\n",
            "Epoch  41 Batch  185 / 525  Training Loss  1.7430160369258374e-05\n",
            "Epoch  41 Batch  186 / 525  Training Loss  2.084865809592884e-05\n",
            "Epoch  41 Batch  187 / 525  Training Loss  1.3871333976567257e-05\n",
            "Epoch  41 Batch  188 / 525  Training Loss  1.8970173186971806e-05\n",
            "Epoch  41 Batch  189 / 525  Training Loss  2.3576274543302134e-05\n",
            "Epoch  41 Batch  190 / 525  Training Loss  1.2203786354803015e-05\n",
            "Epoch  41 Batch  191 / 525  Training Loss  2.341695835639257e-05\n",
            "Epoch  41 Batch  192 / 525  Training Loss  1.9052045900025405e-05\n",
            "Epoch  41 Batch  193 / 525  Training Loss  2.3250890080817044e-05\n",
            "Epoch  41 Batch  194 / 525  Training Loss  2.618183134472929e-05\n",
            "Epoch  41 Batch  195 / 525  Training Loss  3.8859030610183254e-05\n",
            "Epoch  41 Batch  196 / 525  Training Loss  2.0178034901618958e-05\n",
            "Epoch  41 Batch  197 / 525  Training Loss  2.1840143745066598e-05\n",
            "Epoch  41 Batch  198 / 525  Training Loss  3.528713568812236e-05\n",
            "Epoch  41 Batch  199 / 525  Training Loss  2.9157377866795287e-05\n",
            "Epoch  41 Batch  200 / 525  Training Loss  2.2173688194015995e-05\n",
            "Epoch  41 Batch  201 / 525  Training Loss  2.1826188458362594e-05\n",
            "Epoch  41 Batch  202 / 525  Training Loss  2.5445358915021643e-05\n",
            "Epoch  41 Batch  203 / 525  Training Loss  3.9382583054248244e-05\n",
            "Epoch  41 Batch  204 / 525  Training Loss  4.3954372813459486e-05\n",
            "Epoch  41 Batch  205 / 525  Training Loss  1.990896271308884e-05\n",
            "Epoch  41 Batch  206 / 525  Training Loss  3.762029155041091e-05\n",
            "Epoch  41 Batch  207 / 525  Training Loss  4.376670767669566e-05\n",
            "Epoch  41 Batch  208 / 525  Training Loss  4.004163565696217e-05\n",
            "Epoch  41 Batch  209 / 525  Training Loss  3.578282121452503e-05\n",
            "Epoch  41 Batch  210 / 525  Training Loss  1.2666614566114731e-05\n",
            "Epoch  41 Batch  211 / 525  Training Loss  3.065627970499918e-05\n",
            "Epoch  41 Batch  212 / 525  Training Loss  1.3313013369042892e-05\n",
            "Epoch  41 Batch  213 / 525  Training Loss  2.0240124285919592e-05\n",
            "Epoch  41 Batch  214 / 525  Training Loss  3.2520125387236476e-05\n",
            "Epoch  41 Batch  215 / 525  Training Loss  2.0209812646498904e-05\n",
            "Epoch  41 Batch  216 / 525  Training Loss  1.6089172277133912e-05\n",
            "Epoch  41 Batch  217 / 525  Training Loss  2.398194010311272e-05\n",
            "Epoch  41 Batch  218 / 525  Training Loss  2.6053841793327592e-05\n",
            "Epoch  41 Batch  219 / 525  Training Loss  3.191556243109517e-05\n",
            "Epoch  41 Batch  220 / 525  Training Loss  3.538383316481486e-05\n",
            "Epoch  41 Batch  221 / 525  Training Loss  2.5342562366859056e-05\n",
            "Epoch  41 Batch  222 / 525  Training Loss  3.071270839427598e-05\n",
            "Epoch  41 Batch  223 / 525  Training Loss  2.2700538465869613e-05\n",
            "Epoch  41 Batch  224 / 525  Training Loss  3.516855576890521e-05\n",
            "Epoch  41 Batch  225 / 525  Training Loss  2.0188443158986047e-05\n",
            "Epoch  41 Batch  226 / 525  Training Loss  2.4453611331409775e-05\n",
            "Epoch  41 Batch  227 / 525  Training Loss  2.318994484085124e-05\n",
            "Epoch  41 Batch  228 / 525  Training Loss  3.359735637786798e-05\n",
            "Epoch  41 Batch  229 / 525  Training Loss  3.437807026784867e-05\n",
            "Epoch  41 Batch  230 / 525  Training Loss  3.652172381407581e-05\n",
            "Epoch  41 Batch  231 / 525  Training Loss  1.830494511523284e-05\n",
            "Epoch  41 Batch  232 / 525  Training Loss  3.3856551453936845e-05\n",
            "Epoch  41 Batch  233 / 525  Training Loss  1.839750439103227e-05\n",
            "Epoch  41 Batch  234 / 525  Training Loss  2.2147947674966417e-05\n",
            "Epoch  41 Batch  235 / 525  Training Loss  1.8868056940846145e-05\n",
            "Epoch  41 Batch  236 / 525  Training Loss  3.946398646803573e-05\n",
            "Epoch  41 Batch  237 / 525  Training Loss  2.0478697479120456e-05\n",
            "Epoch  41 Batch  238 / 525  Training Loss  2.197534922743216e-05\n",
            "Epoch  41 Batch  239 / 525  Training Loss  3.0678602342959493e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  41 Batch  240 / 525  Training Loss  1.9994697140646167e-05\n",
            "Epoch  41 Batch  241 / 525  Training Loss  1.8307069694856182e-05\n",
            "Epoch  41 Batch  242 / 525  Training Loss  3.0065493774600327e-05\n",
            "Epoch  41 Batch  243 / 525  Training Loss  2.5504938093945384e-05\n",
            "Epoch  41 Batch  244 / 525  Training Loss  3.149194162688218e-05\n",
            "Epoch  41 Batch  245 / 525  Training Loss  2.9538650778704323e-05\n",
            "Epoch  41 Batch  246 / 525  Training Loss  4.4093598262406886e-05\n",
            "Epoch  41 Batch  247 / 525  Training Loss  2.751531428657472e-05\n",
            "Epoch  41 Batch  248 / 525  Training Loss  4.718131458503194e-05\n",
            "Epoch  41 Batch  249 / 525  Training Loss  4.260919740772806e-05\n",
            "Epoch  41 Batch  250 / 525  Training Loss  3.955482316087e-05\n",
            "Epoch  41 Batch  251 / 525  Training Loss  1.1795907994383015e-05\n",
            "Epoch  41 Batch  252 / 525  Training Loss  2.0763318389072083e-05\n",
            "Epoch  41 Batch  253 / 525  Training Loss  2.4074788598227315e-05\n",
            "Epoch  41 Batch  254 / 525  Training Loss  3.393332735868171e-05\n",
            "Epoch  41 Batch  255 / 525  Training Loss  2.3057084035826847e-05\n",
            "Epoch  41 Batch  256 / 525  Training Loss  3.382279464858584e-05\n",
            "Epoch  41 Batch  257 / 525  Training Loss  2.5537436158629134e-05\n",
            "Epoch  41 Batch  258 / 525  Training Loss  3.296437353128567e-05\n",
            "Epoch  41 Batch  259 / 525  Training Loss  3.811700298683718e-05\n",
            "Epoch  41 Batch  260 / 525  Training Loss  1.4242230463423766e-05\n",
            "Epoch  41 Batch  261 / 525  Training Loss  2.602023960207589e-05\n",
            "Epoch  41 Batch  262 / 525  Training Loss  2.8369420761009678e-05\n",
            "Epoch  41 Batch  263 / 525  Training Loss  1.256047653441783e-05\n",
            "Epoch  41 Batch  264 / 525  Training Loss  1.9804971088888124e-05\n",
            "Epoch  41 Batch  265 / 525  Training Loss  1.3898797988076694e-05\n",
            "Epoch  41 Batch  266 / 525  Training Loss  2.6957879526889883e-05\n",
            "Epoch  41 Batch  267 / 525  Training Loss  3.10669383907225e-05\n",
            "Epoch  41 Batch  268 / 525  Training Loss  3.3305354008916765e-05\n",
            "Epoch  41 Batch  269 / 525  Training Loss  3.115997969871387e-05\n",
            "Epoch  41 Batch  270 / 525  Training Loss  2.7796922950074077e-05\n",
            "Epoch  41 Batch  271 / 525  Training Loss  3.151708369841799e-05\n",
            "Epoch  41 Batch  272 / 525  Training Loss  3.272346657468006e-05\n",
            "Epoch  41 Batch  273 / 525  Training Loss  2.989063796121627e-05\n",
            "Epoch  41 Batch  274 / 525  Training Loss  1.754847471602261e-05\n",
            "Epoch  41 Batch  275 / 525  Training Loss  2.8665850550169125e-05\n",
            "Epoch  41 Batch  276 / 525  Training Loss  1.55914640345145e-05\n",
            "Epoch  41 Batch  277 / 525  Training Loss  2.9792903660563752e-05\n",
            "Epoch  41 Batch  278 / 525  Training Loss  2.4294020477100275e-05\n",
            "Epoch  41 Batch  279 / 525  Training Loss  2.440390562696848e-05\n",
            "Epoch  41 Batch  280 / 525  Training Loss  3.843669401248917e-05\n",
            "Epoch  41 Batch  281 / 525  Training Loss  2.9105369321769103e-05\n",
            "Epoch  41 Batch  282 / 525  Training Loss  2.1271394871291704e-05\n",
            "Epoch  41 Batch  283 / 525  Training Loss  2.5779891075217165e-05\n",
            "Epoch  41 Batch  284 / 525  Training Loss  3.9138190913945436e-05\n",
            "Epoch  41 Batch  285 / 525  Training Loss  1.8021111827692948e-05\n",
            "Epoch  41 Batch  286 / 525  Training Loss  1.6654266801197082e-05\n",
            "Epoch  41 Batch  287 / 525  Training Loss  3.515866046654992e-05\n",
            "Epoch  41 Batch  288 / 525  Training Loss  2.8032711270498112e-05\n",
            "Epoch  41 Batch  289 / 525  Training Loss  2.2643265765509568e-05\n",
            "Epoch  41 Batch  290 / 525  Training Loss  4.568839358398691e-05\n",
            "Epoch  41 Batch  291 / 525  Training Loss  2.2313703084364533e-05\n",
            "Epoch  41 Batch  292 / 525  Training Loss  3.71700843970757e-05\n",
            "Epoch  41 Batch  293 / 525  Training Loss  2.6573476134217344e-05\n",
            "Epoch  41 Batch  294 / 525  Training Loss  2.3816170141799375e-05\n",
            "Epoch  41 Batch  295 / 525  Training Loss  2.3825956304790452e-05\n",
            "Epoch  41 Batch  296 / 525  Training Loss  3.7363941373769194e-05\n",
            "Epoch  41 Batch  297 / 525  Training Loss  2.4041773940552957e-05\n",
            "Epoch  41 Batch  298 / 525  Training Loss  1.0311708138033282e-05\n",
            "Epoch  41 Batch  299 / 525  Training Loss  1.8873217413784005e-05\n",
            "Epoch  41 Batch  300 / 525  Training Loss  2.4972614482976496e-05\n",
            "Epoch  41 Batch  301 / 525  Training Loss  2.215046333731152e-05\n",
            "Epoch  41 Batch  302 / 525  Training Loss  3.900670708389953e-05\n",
            "Epoch  41 Batch  303 / 525  Training Loss  1.592902663105633e-05\n",
            "Epoch  41 Batch  304 / 525  Training Loss  1.035711193253519e-05\n",
            "Epoch  41 Batch  305 / 525  Training Loss  3.0069437343627214e-05\n",
            "Epoch  41 Batch  306 / 525  Training Loss  2.0162515284027904e-05\n",
            "Epoch  41 Batch  307 / 525  Training Loss  1.1344970516802277e-05\n",
            "Epoch  41 Batch  308 / 525  Training Loss  2.949458394141402e-05\n",
            "Epoch  41 Batch  309 / 525  Training Loss  2.0748191673192196e-05\n",
            "Epoch  41 Batch  310 / 525  Training Loss  1.810698267945554e-05\n",
            "Epoch  41 Batch  311 / 525  Training Loss  2.722048338910099e-05\n",
            "Epoch  41 Batch  312 / 525  Training Loss  2.3234617401612923e-05\n",
            "Epoch  41 Batch  313 / 525  Training Loss  1.7272199329454452e-05\n",
            "Epoch  41 Batch  314 / 525  Training Loss  1.8887920305132866e-05\n",
            "Epoch  41 Batch  315 / 525  Training Loss  2.7519778086571023e-05\n",
            "Epoch  41 Batch  316 / 525  Training Loss  2.744270932453219e-05\n",
            "Epoch  41 Batch  317 / 525  Training Loss  2.09086065297015e-05\n",
            "Epoch  41 Batch  318 / 525  Training Loss  1.746253292367328e-05\n",
            "Epoch  41 Batch  319 / 525  Training Loss  5.218081787461415e-05\n",
            "Epoch  41 Batch  320 / 525  Training Loss  2.4664808734087273e-05\n",
            "Epoch  41 Batch  321 / 525  Training Loss  1.9704999431269243e-05\n",
            "Epoch  41 Batch  322 / 525  Training Loss  2.5937875761883333e-05\n",
            "Epoch  41 Batch  323 / 525  Training Loss  3.806057429756038e-05\n",
            "Epoch  41 Batch  324 / 525  Training Loss  2.4440823835902847e-05\n",
            "Epoch  41 Batch  325 / 525  Training Loss  2.99447219731519e-05\n",
            "Epoch  41 Batch  326 / 525  Training Loss  3.180844942107797e-05\n",
            "Epoch  41 Batch  327 / 525  Training Loss  1.8587177692097612e-05\n",
            "Epoch  41 Batch  328 / 525  Training Loss  1.5133600754779764e-05\n",
            "Epoch  41 Batch  329 / 525  Training Loss  2.1022276996518485e-05\n",
            "Epoch  41 Batch  330 / 525  Training Loss  3.223894600523636e-05\n",
            "Epoch  41 Batch  331 / 525  Training Loss  2.35939041886013e-05\n",
            "Epoch  41 Batch  332 / 525  Training Loss  2.8754806407960132e-05\n",
            "Epoch  41 Batch  333 / 525  Training Loss  1.8153332348447293e-05\n",
            "Epoch  41 Batch  334 / 525  Training Loss  1.4583424672309775e-05\n",
            "Epoch  41 Batch  335 / 525  Training Loss  3.3017386158462614e-05\n",
            "Epoch  41 Batch  336 / 525  Training Loss  1.9215351130696945e-05\n",
            "Epoch  41 Batch  337 / 525  Training Loss  2.3420601792167872e-05\n",
            "Epoch  41 Batch  338 / 525  Training Loss  3.039253715542145e-05\n",
            "Epoch  41 Batch  339 / 525  Training Loss  3.345173172419891e-05\n",
            "Epoch  41 Batch  340 / 525  Training Loss  2.901261359511409e-05\n",
            "Epoch  41 Batch  341 / 525  Training Loss  2.0180599676677957e-05\n",
            "Epoch  41 Batch  342 / 525  Training Loss  1.3991013474878855e-05\n",
            "Epoch  41 Batch  343 / 525  Training Loss  1.4130152521829586e-05\n",
            "Epoch  41 Batch  344 / 525  Training Loss  3.6365796404425055e-05\n",
            "Epoch  41 Batch  345 / 525  Training Loss  3.440552609390579e-05\n",
            "Epoch  41 Batch  346 / 525  Training Loss  1.719886131468229e-05\n",
            "Epoch  41 Batch  347 / 525  Training Loss  1.7473317711846903e-05\n",
            "Epoch  41 Batch  348 / 525  Training Loss  2.9384707886492833e-05\n",
            "Epoch  41 Batch  349 / 525  Training Loss  2.3980679543456063e-05\n",
            "Epoch  41 Batch  350 / 525  Training Loss  1.610195977264084e-05\n",
            "Epoch  41 Batch  351 / 525  Training Loss  4.430206536198966e-05\n",
            "Epoch  41 Batch  352 / 525  Training Loss  3.053482942050323e-05\n",
            "Epoch  41 Batch  353 / 525  Training Loss  2.1981020836392418e-05\n",
            "Epoch  41 Batch  354 / 525  Training Loss  1.1135983186250087e-05\n",
            "Epoch  41 Batch  355 / 525  Training Loss  1.9270810298621655e-05\n",
            "Epoch  41 Batch  356 / 525  Training Loss  1.0878483408305328e-05\n",
            "Epoch  41 Batch  357 / 525  Training Loss  2.944718289654702e-05\n",
            "Epoch  41 Batch  358 / 525  Training Loss  3.5508553992258385e-05\n",
            "Epoch  41 Batch  359 / 525  Training Loss  2.193421642004978e-05\n",
            "Epoch  41 Batch  360 / 525  Training Loss  3.3047432225430384e-05\n",
            "Epoch  41 Batch  361 / 525  Training Loss  3.072830804740079e-05\n",
            "Epoch  41 Batch  362 / 525  Training Loss  2.9428323614411056e-05\n",
            "Epoch  41 Batch  363 / 525  Training Loss  2.848006261046976e-05\n",
            "Epoch  41 Batch  364 / 525  Training Loss  2.1995187125867233e-05\n",
            "Epoch  41 Batch  365 / 525  Training Loss  3.0940449505578727e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  41 Batch  366 / 525  Training Loss  3.914528133464046e-05\n",
            "Epoch  41 Batch  367 / 525  Training Loss  3.0041386708035134e-05\n",
            "Epoch  41 Batch  368 / 525  Training Loss  2.825666706485208e-05\n",
            "Epoch  41 Batch  369 / 525  Training Loss  3.4955741284647956e-05\n",
            "Epoch  41 Batch  370 / 525  Training Loss  1.4647445823356975e-05\n",
            "Epoch  41 Batch  371 / 525  Training Loss  3.1554180168313906e-05\n",
            "Epoch  41 Batch  372 / 525  Training Loss  3.231515074730851e-05\n",
            "Epoch  41 Batch  373 / 525  Training Loss  2.913605385401752e-05\n",
            "Epoch  41 Batch  374 / 525  Training Loss  1.937100569193717e-05\n",
            "Epoch  41 Batch  375 / 525  Training Loss  1.9501352653605863e-05\n",
            "Epoch  41 Batch  376 / 525  Training Loss  3.621811265475117e-05\n",
            "Epoch  41 Batch  377 / 525  Training Loss  1.9609144146670587e-05\n",
            "Epoch  41 Batch  378 / 525  Training Loss  2.5976052711484954e-05\n",
            "Epoch  41 Batch  379 / 525  Training Loss  2.1611560441670008e-05\n",
            "Epoch  41 Batch  380 / 525  Training Loss  2.0816987671423703e-05\n",
            "Epoch  41 Batch  381 / 525  Training Loss  2.4888126063160598e-05\n",
            "Epoch  41 Batch  382 / 525  Training Loss  3.7692476325901225e-05\n",
            "Epoch  41 Batch  383 / 525  Training Loss  3.681189991766587e-05\n",
            "Epoch  41 Batch  384 / 525  Training Loss  2.4486047550453804e-05\n",
            "Epoch  41 Batch  385 / 525  Training Loss  2.306706846866291e-05\n",
            "Epoch  41 Batch  386 / 525  Training Loss  2.108730041072704e-05\n",
            "Epoch  41 Batch  387 / 525  Training Loss  2.139759817509912e-05\n",
            "Epoch  41 Batch  388 / 525  Training Loss  2.426829632895533e-05\n",
            "Epoch  41 Batch  389 / 525  Training Loss  2.2684009309159592e-05\n",
            "Epoch  41 Batch  390 / 525  Training Loss  3.042876414838247e-05\n",
            "Epoch  41 Batch  391 / 525  Training Loss  2.5768516934476793e-05\n",
            "Epoch  41 Batch  392 / 525  Training Loss  1.9328617781866342e-05\n",
            "Epoch  41 Batch  393 / 525  Training Loss  2.3713142581982538e-05\n",
            "Epoch  41 Batch  394 / 525  Training Loss  2.2329626517603174e-05\n",
            "Epoch  41 Batch  395 / 525  Training Loss  2.787836638162844e-05\n",
            "Epoch  41 Batch  396 / 525  Training Loss  2.3048618459142745e-05\n",
            "Epoch  41 Batch  397 / 525  Training Loss  3.325872967252508e-05\n",
            "Epoch  41 Batch  398 / 525  Training Loss  2.3981692720553838e-05\n",
            "Epoch  41 Batch  399 / 525  Training Loss  2.0348325051600114e-05\n",
            "Epoch  41 Batch  400 / 525  Training Loss  3.335861401865259e-05\n",
            "Epoch  41 Batch  401 / 525  Training Loss  3.337319867569022e-05\n",
            "Epoch  41 Batch  402 / 525  Training Loss  2.1985810235491954e-05\n",
            "Epoch  41 Batch  403 / 525  Training Loss  2.5411276510567404e-05\n",
            "Epoch  41 Batch  404 / 525  Training Loss  3.0342536774696782e-05\n",
            "Epoch  41 Batch  405 / 525  Training Loss  2.7464924642117694e-05\n",
            "Epoch  41 Batch  406 / 525  Training Loss  1.6967109331744723e-05\n",
            "Epoch  41 Batch  407 / 525  Training Loss  2.237100670754444e-05\n",
            "Epoch  41 Batch  408 / 525  Training Loss  3.8837039028294384e-05\n",
            "Epoch  41 Batch  409 / 525  Training Loss  2.2118325432529673e-05\n",
            "Epoch  41 Batch  410 / 525  Training Loss  2.6831170544028282e-05\n",
            "Epoch  41 Batch  411 / 525  Training Loss  2.4140806999639608e-05\n",
            "Epoch  41 Batch  412 / 525  Training Loss  2.2849051674711518e-05\n",
            "Epoch  41 Batch  413 / 525  Training Loss  1.1154595085827168e-05\n",
            "Epoch  41 Batch  414 / 525  Training Loss  1.557575524202548e-05\n",
            "Epoch  41 Batch  415 / 525  Training Loss  2.3691565729677677e-05\n",
            "Epoch  41 Batch  416 / 525  Training Loss  2.2074516891734675e-05\n",
            "Epoch  41 Batch  417 / 525  Training Loss  2.1636777091771364e-05\n",
            "Epoch  41 Batch  418 / 525  Training Loss  2.603463872219436e-05\n",
            "Epoch  41 Batch  419 / 525  Training Loss  2.8340014978311956e-05\n",
            "Epoch  41 Batch  420 / 525  Training Loss  2.201380812039133e-05\n",
            "Epoch  41 Batch  421 / 525  Training Loss  2.803227471304126e-05\n",
            "Epoch  41 Batch  422 / 525  Training Loss  2.066392517008353e-05\n",
            "Epoch  41 Batch  423 / 525  Training Loss  3.8285379559965804e-05\n",
            "Epoch  41 Batch  424 / 525  Training Loss  2.752618456725031e-05\n",
            "Epoch  41 Batch  425 / 525  Training Loss  3.1726965971756727e-05\n",
            "Epoch  41 Batch  426 / 525  Training Loss  2.857482832041569e-05\n",
            "Epoch  41 Batch  427 / 525  Training Loss  2.5902170818881132e-05\n",
            "Epoch  41 Batch  428 / 525  Training Loss  3.8920174119994044e-05\n",
            "Epoch  41 Batch  429 / 525  Training Loss  3.5998331441078335e-05\n",
            "Epoch  41 Batch  430 / 525  Training Loss  1.8538466974860057e-05\n",
            "Epoch  41 Batch  431 / 525  Training Loss  2.1188896425883286e-05\n",
            "Epoch  41 Batch  432 / 525  Training Loss  2.9571750928880647e-05\n",
            "Epoch  41 Batch  433 / 525  Training Loss  3.555016155587509e-05\n",
            "Epoch  41 Batch  434 / 525  Training Loss  2.600222433102317e-05\n",
            "Epoch  41 Batch  435 / 525  Training Loss  2.6980222173733637e-05\n",
            "Epoch  41 Batch  436 / 525  Training Loss  3.7286547012627125e-05\n",
            "Epoch  41 Batch  437 / 525  Training Loss  3.265529448981397e-05\n",
            "Epoch  41 Batch  438 / 525  Training Loss  2.463905002514366e-05\n",
            "Epoch  41 Batch  439 / 525  Training Loss  1.227297070727218e-05\n",
            "Epoch  41 Batch  440 / 525  Training Loss  3.7293128116289154e-05\n",
            "Epoch  41 Batch  441 / 525  Training Loss  1.2763322047248948e-05\n",
            "Epoch  41 Batch  442 / 525  Training Loss  1.2372211131150834e-05\n",
            "Epoch  41 Batch  443 / 525  Training Loss  2.2997304768068716e-05\n",
            "Epoch  41 Batch  444 / 525  Training Loss  2.772705556708388e-05\n",
            "Epoch  41 Batch  445 / 525  Training Loss  2.145355028915219e-05\n",
            "Epoch  41 Batch  446 / 525  Training Loss  3.433456731727347e-05\n",
            "Epoch  41 Batch  447 / 525  Training Loss  2.3114767827792093e-05\n",
            "Epoch  41 Batch  448 / 525  Training Loss  1.799415986170061e-05\n",
            "Epoch  41 Batch  449 / 525  Training Loss  3.463661050773226e-05\n",
            "Epoch  41 Batch  450 / 525  Training Loss  2.3095684809959494e-05\n",
            "Epoch  41 Batch  451 / 525  Training Loss  2.2652026018477045e-05\n",
            "Epoch  41 Batch  452 / 525  Training Loss  2.9654776881216094e-05\n",
            "Epoch  41 Batch  453 / 525  Training Loss  2.8429032681742683e-05\n",
            "Epoch  41 Batch  454 / 525  Training Loss  2.7295222025713883e-05\n",
            "Epoch  41 Batch  455 / 525  Training Loss  2.6614859962137416e-05\n",
            "Epoch  41 Batch  456 / 525  Training Loss  1.9154404071741737e-05\n",
            "Epoch  41 Batch  457 / 525  Training Loss  2.4178381863748655e-05\n",
            "Epoch  41 Batch  458 / 525  Training Loss  2.240270077891182e-05\n",
            "Epoch  41 Batch  459 / 525  Training Loss  4.781644747708924e-05\n",
            "Epoch  41 Batch  460 / 525  Training Loss  1.5759283996885642e-05\n",
            "Epoch  41 Batch  461 / 525  Training Loss  3.0447825338342227e-05\n",
            "Epoch  41 Batch  462 / 525  Training Loss  3.316990478197113e-05\n",
            "Epoch  41 Batch  463 / 525  Training Loss  2.7784215490100905e-05\n",
            "Epoch  41 Batch  464 / 525  Training Loss  2.0720475731650367e-05\n",
            "Epoch  41 Batch  465 / 525  Training Loss  2.940653394034598e-05\n",
            "Epoch  41 Batch  466 / 525  Training Loss  1.8999473468284123e-05\n",
            "Epoch  41 Batch  467 / 525  Training Loss  1.8908880520029925e-05\n",
            "Epoch  41 Batch  468 / 525  Training Loss  2.260765540995635e-05\n",
            "Epoch  41 Batch  469 / 525  Training Loss  3.422476947889663e-05\n",
            "Epoch  41 Batch  470 / 525  Training Loss  2.485068944224622e-05\n",
            "Epoch  41 Batch  471 / 525  Training Loss  2.9415221433737315e-05\n",
            "Epoch  41 Batch  472 / 525  Training Loss  1.8289423678652383e-05\n",
            "Epoch  41 Batch  473 / 525  Training Loss  3.3147814974654466e-05\n",
            "Epoch  41 Batch  474 / 525  Training Loss  2.832270911312662e-05\n",
            "Epoch  41 Batch  475 / 525  Training Loss  3.0309378416859545e-05\n",
            "Epoch  41 Batch  476 / 525  Training Loss  4.430702392710373e-05\n",
            "Epoch  41 Batch  477 / 525  Training Loss  2.6033867470687255e-05\n",
            "Epoch  41 Batch  478 / 525  Training Loss  2.668418710527476e-05\n",
            "Epoch  41 Batch  479 / 525  Training Loss  1.8731621821643785e-05\n",
            "Epoch  41 Batch  480 / 525  Training Loss  1.7953441783902235e-05\n",
            "Epoch  41 Batch  481 / 525  Training Loss  2.7432319257059135e-05\n",
            "Epoch  41 Batch  482 / 525  Training Loss  4.486024408834055e-05\n",
            "Epoch  41 Batch  483 / 525  Training Loss  3.2473319151904434e-05\n",
            "Epoch  41 Batch  484 / 525  Training Loss  3.501590254018083e-05\n",
            "Epoch  41 Batch  485 / 525  Training Loss  2.7452635549707338e-05\n",
            "Epoch  41 Batch  486 / 525  Training Loss  2.6352132408646867e-05\n",
            "Epoch  41 Batch  487 / 525  Training Loss  2.4428376491414383e-05\n",
            "Epoch  41 Batch  488 / 525  Training Loss  1.7208434655913152e-05\n",
            "Epoch  41 Batch  489 / 525  Training Loss  2.110316199832596e-05\n",
            "Epoch  41 Batch  490 / 525  Training Loss  2.0138979380135424e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  41 Batch  491 / 525  Training Loss  2.2364305550581776e-05\n",
            "Epoch  41 Batch  492 / 525  Training Loss  2.6507861548452638e-05\n",
            "Epoch  41 Batch  493 / 525  Training Loss  2.016079997702036e-05\n",
            "Epoch  41 Batch  494 / 525  Training Loss  3.0320141377160326e-05\n",
            "Epoch  41 Batch  495 / 525  Training Loss  2.0870164007646963e-05\n",
            "Epoch  41 Batch  496 / 525  Training Loss  4.2256051528966054e-05\n",
            "Epoch  41 Batch  497 / 525  Training Loss  3.111823389190249e-05\n",
            "Epoch  41 Batch  498 / 525  Training Loss  1.6285437595797703e-05\n",
            "Epoch  41 Batch  499 / 525  Training Loss  2.5414949050173163e-05\n",
            "Epoch  41 Batch  500 / 525  Training Loss  3.0338336728163995e-05\n",
            "Epoch  41 Batch  501 / 525  Training Loss  1.5886538676568307e-05\n",
            "Epoch  41 Batch  502 / 525  Training Loss  5.05817515659146e-05\n",
            "Epoch  41 Batch  503 / 525  Training Loss  3.810417911154218e-05\n",
            "Epoch  41 Batch  504 / 525  Training Loss  3.694773477036506e-05\n",
            "Epoch  41 Batch  505 / 525  Training Loss  2.241141555714421e-05\n",
            "Epoch  41 Batch  506 / 525  Training Loss  3.0292712835944258e-05\n",
            "Epoch  41 Batch  507 / 525  Training Loss  1.8616166926221922e-05\n",
            "Epoch  41 Batch  508 / 525  Training Loss  2.7801452233688906e-05\n",
            "Epoch  41 Batch  509 / 525  Training Loss  2.5612442186684348e-05\n",
            "Epoch  41 Batch  510 / 525  Training Loss  2.721547571127303e-05\n",
            "Epoch  41 Batch  511 / 525  Training Loss  2.8018437660648488e-05\n",
            "Epoch  41 Batch  512 / 525  Training Loss  2.6093164706253447e-05\n",
            "Epoch  41 Batch  513 / 525  Training Loss  2.1242796719889157e-05\n",
            "Epoch  41 Batch  514 / 525  Training Loss  4.524937685346231e-05\n",
            "Epoch  41 Batch  515 / 525  Training Loss  3.897317947121337e-05\n",
            "Epoch  41 Batch  516 / 525  Training Loss  2.1141373508726247e-05\n",
            "Epoch  41 Batch  517 / 525  Training Loss  2.0655947082559578e-05\n",
            "Epoch  41 Batch  518 / 525  Training Loss  3.8779584428993985e-05\n",
            "Epoch  41 Batch  519 / 525  Training Loss  2.434054113109596e-05\n",
            "Epoch  41 Batch  520 / 525  Training Loss  1.780539059836883e-05\n",
            "Epoch  41 Batch  521 / 525  Training Loss  1.903741758724209e-05\n",
            "Epoch  41 Batch  522 / 525  Training Loss  2.3955009964993224e-05\n",
            "Epoch  41 Batch  523 / 525  Training Loss  1.3471144484356046e-05\n",
            "Epoch  41 Batch  524 / 525  Training Loss  4.9069909437093884e-05\n",
            "  42    |    -    |   0.000026   | 64.500000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 42\n",
            "Epoch  42 Batch  0 / 525  Training Loss  2.2773116143071093e-05\n",
            "Epoch  42 Batch  1 / 525  Training Loss  2.171215965063311e-05\n",
            "Epoch  42 Batch  2 / 525  Training Loss  1.6714202502043918e-05\n",
            "Epoch  42 Batch  3 / 525  Training Loss  1.6843152479850687e-05\n",
            "Epoch  42 Batch  4 / 525  Training Loss  1.0008127901528496e-05\n",
            "Epoch  42 Batch  5 / 525  Training Loss  1.805009742383845e-05\n",
            "Epoch  42 Batch  6 / 525  Training Loss  1.9632581825135276e-05\n",
            "Epoch  42 Batch  7 / 525  Training Loss  2.018300256168004e-05\n",
            "Epoch  42 Batch  8 / 525  Training Loss  1.6990870790323243e-05\n",
            "Epoch  42 Batch  9 / 525  Training Loss  2.5347064365632832e-05\n",
            "Epoch  42 Batch  10 / 525  Training Loss  1.999701817112509e-05\n",
            "Epoch  42 Batch  11 / 525  Training Loss  1.703085217741318e-05\n",
            "Epoch  42 Batch  12 / 525  Training Loss  2.5315428501926363e-05\n",
            "Epoch  42 Batch  13 / 525  Training Loss  2.6218820494250394e-05\n",
            "Epoch  42 Batch  14 / 525  Training Loss  2.4721197405597195e-05\n",
            "Epoch  42 Batch  15 / 525  Training Loss  1.4948588614061009e-05\n",
            "Epoch  42 Batch  16 / 525  Training Loss  2.0940706235705875e-05\n",
            "Epoch  42 Batch  17 / 525  Training Loss  1.7690406821202487e-05\n",
            "Epoch  42 Batch  18 / 525  Training Loss  3.40688566211611e-05\n",
            "Epoch  42 Batch  19 / 525  Training Loss  4.153983172727749e-05\n",
            "Epoch  42 Batch  20 / 525  Training Loss  1.690588760538958e-05\n",
            "Epoch  42 Batch  21 / 525  Training Loss  1.8699407519306988e-05\n",
            "Epoch  42 Batch  22 / 525  Training Loss  3.747987648239359e-05\n",
            "Epoch  42 Batch  23 / 525  Training Loss  1.8667891708901152e-05\n",
            "Epoch  42 Batch  24 / 525  Training Loss  1.9388568034628406e-05\n",
            "Epoch  42 Batch  25 / 525  Training Loss  2.3504380806116387e-05\n",
            "Epoch  42 Batch  26 / 525  Training Loss  2.6275001800968312e-05\n",
            "Epoch  42 Batch  27 / 525  Training Loss  2.587231938377954e-05\n",
            "Epoch  42 Batch  28 / 525  Training Loss  1.7223224858753383e-05\n",
            "Epoch  42 Batch  29 / 525  Training Loss  4.003873254987411e-05\n",
            "Epoch  42 Batch  30 / 525  Training Loss  2.5231453037122265e-05\n",
            "Epoch  42 Batch  31 / 525  Training Loss  1.7925356587511487e-05\n",
            "Epoch  42 Batch  32 / 525  Training Loss  2.386389678576961e-05\n",
            "Epoch  42 Batch  33 / 525  Training Loss  3.613360968302004e-05\n",
            "Epoch  42 Batch  34 / 525  Training Loss  2.7519199647940695e-05\n",
            "Epoch  42 Batch  35 / 525  Training Loss  2.0204195607220754e-05\n",
            "Epoch  42 Batch  36 / 525  Training Loss  2.346742621739395e-05\n",
            "Epoch  42 Batch  37 / 525  Training Loss  2.1529460354940966e-05\n",
            "Epoch  42 Batch  38 / 525  Training Loss  2.989283893839456e-05\n",
            "Epoch  42 Batch  39 / 525  Training Loss  2.671645415830426e-05\n",
            "Epoch  42 Batch  40 / 525  Training Loss  2.113044865836855e-05\n",
            "Epoch  42 Batch  41 / 525  Training Loss  1.820828219933901e-05\n",
            "Epoch  42 Batch  42 / 525  Training Loss  1.079997127817478e-05\n",
            "Epoch  42 Batch  43 / 525  Training Loss  1.2607068129000254e-05\n",
            "Epoch  42 Batch  44 / 525  Training Loss  1.876318856375292e-05\n",
            "Epoch  42 Batch  45 / 525  Training Loss  2.3863671231083572e-05\n",
            "Epoch  42 Batch  46 / 525  Training Loss  2.073714495054446e-05\n",
            "Epoch  42 Batch  47 / 525  Training Loss  1.759748556651175e-05\n",
            "Epoch  42 Batch  48 / 525  Training Loss  2.406657768005971e-05\n",
            "Epoch  42 Batch  49 / 525  Training Loss  1.5637628166587092e-05\n",
            "Epoch  42 Batch  50 / 525  Training Loss  2.149066494894214e-05\n",
            "Epoch  42 Batch  51 / 525  Training Loss  2.255359686387237e-05\n",
            "Epoch  42 Batch  52 / 525  Training Loss  1.0451304660819005e-05\n",
            "Epoch  42 Batch  53 / 525  Training Loss  1.5391900888062082e-05\n",
            "Epoch  42 Batch  54 / 525  Training Loss  2.8381004085531458e-05\n",
            "Epoch  42 Batch  55 / 525  Training Loss  1.0611451216391288e-05\n",
            "Epoch  42 Batch  56 / 525  Training Loss  1.7346013919450343e-05\n",
            "Epoch  42 Batch  57 / 525  Training Loss  3.264857150497846e-05\n",
            "Epoch  42 Batch  58 / 525  Training Loss  3.064651900785975e-05\n",
            "Epoch  42 Batch  59 / 525  Training Loss  1.9270999473519623e-05\n",
            "Epoch  42 Batch  60 / 525  Training Loss  2.718049472605344e-05\n",
            "Epoch  42 Batch  61 / 525  Training Loss  1.6474536096211523e-05\n",
            "Epoch  42 Batch  62 / 525  Training Loss  3.414028469705954e-05\n",
            "Epoch  42 Batch  63 / 525  Training Loss  1.531560337753035e-05\n",
            "Epoch  42 Batch  64 / 525  Training Loss  3.1013842090032995e-05\n",
            "Epoch  42 Batch  65 / 525  Training Loss  1.969220829778351e-05\n",
            "Epoch  42 Batch  66 / 525  Training Loss  2.6310119210393168e-05\n",
            "Epoch  42 Batch  67 / 525  Training Loss  2.0572739231283776e-05\n",
            "Epoch  42 Batch  68 / 525  Training Loss  2.229944038845133e-05\n",
            "Epoch  42 Batch  69 / 525  Training Loss  2.323310218343977e-05\n",
            "Epoch  42 Batch  70 / 525  Training Loss  3.528784873196855e-05\n",
            "Epoch  42 Batch  71 / 525  Training Loss  2.4867491447366774e-05\n",
            "Epoch  42 Batch  72 / 525  Training Loss  1.3964888239570428e-05\n",
            "Epoch  42 Batch  73 / 525  Training Loss  2.7706215405487455e-05\n",
            "Epoch  42 Batch  74 / 525  Training Loss  1.8820917830453254e-05\n",
            "Epoch  42 Batch  75 / 525  Training Loss  2.9449360226863064e-05\n",
            "Epoch  42 Batch  76 / 525  Training Loss  1.5713992979726754e-05\n",
            "Epoch  42 Batch  77 / 525  Training Loss  2.4124008632497862e-05\n",
            "Epoch  42 Batch  78 / 525  Training Loss  2.3500866518588737e-05\n",
            "Epoch  42 Batch  79 / 525  Training Loss  1.539137701911386e-05\n",
            "Epoch  42 Batch  80 / 525  Training Loss  2.028469134529587e-05\n",
            "Epoch  42 Batch  81 / 525  Training Loss  3.098366505582817e-05\n",
            "Epoch  42 Batch  82 / 525  Training Loss  2.3092146875569597e-05\n",
            "Epoch  42 Batch  83 / 525  Training Loss  1.0300307621946558e-05\n",
            "Epoch  42 Batch  84 / 525  Training Loss  1.9163349861628376e-05\n",
            "Epoch  42 Batch  85 / 525  Training Loss  3.0104714824119583e-05\n",
            "Epoch  42 Batch  86 / 525  Training Loss  1.61796579050133e-05\n",
            "Epoch  42 Batch  87 / 525  Training Loss  1.7353448129142635e-05\n",
            "Epoch  42 Batch  88 / 525  Training Loss  1.5344910934800282e-05\n",
            "Epoch  42 Batch  89 / 525  Training Loss  2.8055019356543198e-05\n",
            "Epoch  42 Batch  90 / 525  Training Loss  2.2861899196868762e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  42 Batch  91 / 525  Training Loss  1.9389250155654736e-05\n",
            "Epoch  42 Batch  92 / 525  Training Loss  2.3366312234429643e-05\n",
            "Epoch  42 Batch  93 / 525  Training Loss  3.259811637690291e-05\n",
            "Epoch  42 Batch  94 / 525  Training Loss  1.9520985006238334e-05\n",
            "Epoch  42 Batch  95 / 525  Training Loss  1.9423292542342097e-05\n",
            "Epoch  42 Batch  96 / 525  Training Loss  1.903505835798569e-05\n",
            "Epoch  42 Batch  97 / 525  Training Loss  1.8894941604230553e-05\n",
            "Epoch  42 Batch  98 / 525  Training Loss  2.309641058673151e-05\n",
            "Epoch  42 Batch  99 / 525  Training Loss  3.086576543864794e-05\n",
            "Epoch  42 Batch  100 / 525  Training Loss  1.5672394511057064e-05\n",
            "Epoch  42 Batch  101 / 525  Training Loss  2.791694350889884e-05\n",
            "Epoch  42 Batch  102 / 525  Training Loss  2.042304549831897e-05\n",
            "Epoch  42 Batch  103 / 525  Training Loss  9.379200491821393e-06\n",
            "Epoch  42 Batch  104 / 525  Training Loss  4.01709767174907e-05\n",
            "Epoch  42 Batch  105 / 525  Training Loss  1.4033270417712629e-05\n",
            "Epoch  42 Batch  106 / 525  Training Loss  2.1646006644004956e-05\n",
            "Epoch  42 Batch  107 / 525  Training Loss  2.340082573937252e-05\n",
            "Epoch  42 Batch  108 / 525  Training Loss  3.062497853534296e-05\n",
            "Epoch  42 Batch  109 / 525  Training Loss  1.7459597074775957e-05\n",
            "Epoch  42 Batch  110 / 525  Training Loss  1.788258487067651e-05\n",
            "Epoch  42 Batch  111 / 525  Training Loss  3.2359799661207944e-05\n",
            "Epoch  42 Batch  112 / 525  Training Loss  1.771164170349948e-05\n",
            "Epoch  42 Batch  113 / 525  Training Loss  2.3173148292698897e-05\n",
            "Epoch  42 Batch  114 / 525  Training Loss  2.3939293896546587e-05\n",
            "Epoch  42 Batch  115 / 525  Training Loss  2.0823401428060606e-05\n",
            "Epoch  42 Batch  116 / 525  Training Loss  2.210747879871633e-05\n",
            "Epoch  42 Batch  117 / 525  Training Loss  4.1484145185677335e-05\n",
            "Epoch  42 Batch  118 / 525  Training Loss  1.7863780158222653e-05\n",
            "Epoch  42 Batch  119 / 525  Training Loss  2.2149402866489254e-05\n",
            "Epoch  42 Batch  120 / 525  Training Loss  1.532329770270735e-05\n",
            "Epoch  42 Batch  121 / 525  Training Loss  1.823968705139123e-05\n",
            "Epoch  42 Batch  122 / 525  Training Loss  2.75786060228711e-05\n",
            "Epoch  42 Batch  123 / 525  Training Loss  2.0754470824613236e-05\n",
            "Epoch  42 Batch  124 / 525  Training Loss  1.3399607269093394e-05\n",
            "Epoch  42 Batch  125 / 525  Training Loss  2.4427858079434372e-05\n",
            "Epoch  42 Batch  126 / 525  Training Loss  2.2949976482777856e-05\n",
            "Epoch  42 Batch  127 / 525  Training Loss  2.3433720343746245e-05\n",
            "Epoch  42 Batch  128 / 525  Training Loss  1.4299459508038126e-05\n",
            "Epoch  42 Batch  129 / 525  Training Loss  3.04144668916706e-05\n",
            "Epoch  42 Batch  130 / 525  Training Loss  2.3430889996234328e-05\n",
            "Epoch  42 Batch  131 / 525  Training Loss  1.899934432003647e-05\n",
            "Epoch  42 Batch  132 / 525  Training Loss  1.3800262422591913e-05\n",
            "Epoch  42 Batch  133 / 525  Training Loss  3.1533076253253967e-05\n",
            "Epoch  42 Batch  134 / 525  Training Loss  3.3739390346454456e-05\n",
            "Epoch  42 Batch  135 / 525  Training Loss  1.9680401237565093e-05\n",
            "Epoch  42 Batch  136 / 525  Training Loss  1.535707269795239e-05\n",
            "Epoch  42 Batch  137 / 525  Training Loss  2.759380186034832e-05\n",
            "Epoch  42 Batch  138 / 525  Training Loss  2.6176197934546508e-05\n",
            "Epoch  42 Batch  139 / 525  Training Loss  3.530304093146697e-05\n",
            "Epoch  42 Batch  140 / 525  Training Loss  1.8603293938213028e-05\n",
            "Epoch  42 Batch  141 / 525  Training Loss  2.103557562804781e-05\n",
            "Epoch  42 Batch  142 / 525  Training Loss  2.228429730166681e-05\n",
            "Epoch  42 Batch  143 / 525  Training Loss  2.2866390281706117e-05\n",
            "Epoch  42 Batch  144 / 525  Training Loss  2.5982128136092797e-05\n",
            "Epoch  42 Batch  145 / 525  Training Loss  2.445095742587e-05\n",
            "Epoch  42 Batch  146 / 525  Training Loss  2.1046940673841164e-05\n",
            "Epoch  42 Batch  147 / 525  Training Loss  2.336877332709264e-05\n",
            "Epoch  42 Batch  148 / 525  Training Loss  2.2479118342744187e-05\n",
            "Epoch  42 Batch  149 / 525  Training Loss  1.734843863232527e-05\n",
            "Epoch  42 Batch  150 / 525  Training Loss  2.0757892343681306e-05\n",
            "Epoch  42 Batch  151 / 525  Training Loss  1.663160765019711e-05\n",
            "Epoch  42 Batch  152 / 525  Training Loss  1.5386714949272573e-05\n",
            "Epoch  42 Batch  153 / 525  Training Loss  2.862234578060452e-05\n",
            "Epoch  42 Batch  154 / 525  Training Loss  1.3126131307217292e-05\n",
            "Epoch  42 Batch  155 / 525  Training Loss  3.0381750548258424e-05\n",
            "Epoch  42 Batch  156 / 525  Training Loss  2.5527808247716166e-05\n",
            "Epoch  42 Batch  157 / 525  Training Loss  1.6265292288153432e-05\n",
            "Epoch  42 Batch  158 / 525  Training Loss  2.1616011508740485e-05\n",
            "Epoch  42 Batch  159 / 525  Training Loss  1.7286225556745194e-05\n",
            "Epoch  42 Batch  160 / 525  Training Loss  1.1918555173906498e-05\n",
            "Epoch  42 Batch  161 / 525  Training Loss  9.558161764289252e-06\n",
            "Epoch  42 Batch  162 / 525  Training Loss  2.0663763280026615e-05\n",
            "Epoch  42 Batch  163 / 525  Training Loss  2.1484038370545022e-05\n",
            "Epoch  42 Batch  164 / 525  Training Loss  2.6655539841158316e-05\n",
            "Epoch  42 Batch  165 / 525  Training Loss  2.3759970645187423e-05\n",
            "Epoch  42 Batch  166 / 525  Training Loss  3.240405203541741e-05\n",
            "Epoch  42 Batch  167 / 525  Training Loss  2.401781421212945e-05\n",
            "Epoch  42 Batch  168 / 525  Training Loss  2.031395342783071e-05\n",
            "Epoch  42 Batch  169 / 525  Training Loss  1.9158311260980554e-05\n",
            "Epoch  42 Batch  170 / 525  Training Loss  2.5158695279969834e-05\n",
            "Epoch  42 Batch  171 / 525  Training Loss  2.3035352569422685e-05\n",
            "Epoch  42 Batch  172 / 525  Training Loss  2.0834471797570586e-05\n",
            "Epoch  42 Batch  173 / 525  Training Loss  3.290208405815065e-05\n",
            "Epoch  42 Batch  174 / 525  Training Loss  1.8545904822531156e-05\n",
            "Epoch  42 Batch  175 / 525  Training Loss  1.4798404663451947e-05\n",
            "Epoch  42 Batch  176 / 525  Training Loss  2.081357888528146e-05\n",
            "Epoch  42 Batch  177 / 525  Training Loss  2.441541619191412e-05\n",
            "Epoch  42 Batch  178 / 525  Training Loss  2.730814776441548e-05\n",
            "Epoch  42 Batch  179 / 525  Training Loss  2.8349293643259443e-05\n",
            "Epoch  42 Batch  180 / 525  Training Loss  1.959715700650122e-05\n",
            "Epoch  42 Batch  181 / 525  Training Loss  2.627319554449059e-05\n",
            "Epoch  42 Batch  182 / 525  Training Loss  1.4471972463070415e-05\n",
            "Epoch  42 Batch  183 / 525  Training Loss  2.8525717425509356e-05\n",
            "Epoch  42 Batch  184 / 525  Training Loss  2.9825940146110952e-05\n",
            "Epoch  42 Batch  185 / 525  Training Loss  1.959592555067502e-05\n",
            "Epoch  42 Batch  186 / 525  Training Loss  2.7231242711422965e-05\n",
            "Epoch  42 Batch  187 / 525  Training Loss  2.152960041712504e-05\n",
            "Epoch  42 Batch  188 / 525  Training Loss  2.2244015781325288e-05\n",
            "Epoch  42 Batch  189 / 525  Training Loss  2.3125099687604234e-05\n",
            "Epoch  42 Batch  190 / 525  Training Loss  1.8812786947819404e-05\n",
            "Epoch  42 Batch  191 / 525  Training Loss  2.1978125005261973e-05\n",
            "Epoch  42 Batch  192 / 525  Training Loss  2.2210469978745095e-05\n",
            "Epoch  42 Batch  193 / 525  Training Loss  1.937761226145085e-05\n",
            "Epoch  42 Batch  194 / 525  Training Loss  1.7498296074336395e-05\n",
            "Epoch  42 Batch  195 / 525  Training Loss  2.349301939830184e-05\n",
            "Epoch  42 Batch  196 / 525  Training Loss  2.119396776834037e-05\n",
            "Epoch  42 Batch  197 / 525  Training Loss  2.454193599987775e-05\n",
            "Epoch  42 Batch  198 / 525  Training Loss  2.241677611891646e-05\n",
            "Epoch  42 Batch  199 / 525  Training Loss  2.5163724785670638e-05\n",
            "Epoch  42 Batch  200 / 525  Training Loss  1.3931430657976307e-05\n",
            "Epoch  42 Batch  201 / 525  Training Loss  1.1206118870177306e-05\n",
            "Epoch  42 Batch  202 / 525  Training Loss  3.8078691432019696e-05\n",
            "Epoch  42 Batch  203 / 525  Training Loss  1.050418086379068e-05\n",
            "Epoch  42 Batch  204 / 525  Training Loss  2.1927211491856724e-05\n",
            "Epoch  42 Batch  205 / 525  Training Loss  1.0533471140661277e-05\n",
            "Epoch  42 Batch  206 / 525  Training Loss  1.9263710782979615e-05\n",
            "Epoch  42 Batch  207 / 525  Training Loss  1.5757865185150877e-05\n",
            "Epoch  42 Batch  208 / 525  Training Loss  1.7045880667865276e-05\n",
            "Epoch  42 Batch  209 / 525  Training Loss  3.4108383260900155e-05\n",
            "Epoch  42 Batch  210 / 525  Training Loss  2.0202744053676724e-05\n",
            "Epoch  42 Batch  211 / 525  Training Loss  1.8625263692229055e-05\n",
            "Epoch  42 Batch  212 / 525  Training Loss  1.9078237528447062e-05\n",
            "Epoch  42 Batch  213 / 525  Training Loss  3.109068711637519e-05\n",
            "Epoch  42 Batch  214 / 525  Training Loss  8.284796422231011e-06\n",
            "Epoch  42 Batch  215 / 525  Training Loss  1.2836618225264829e-05\n",
            "Epoch  42 Batch  216 / 525  Training Loss  2.366051558055915e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  42 Batch  217 / 525  Training Loss  2.144551217497792e-05\n",
            "Epoch  42 Batch  218 / 525  Training Loss  3.0094393878243864e-05\n",
            "Epoch  42 Batch  219 / 525  Training Loss  3.176916652591899e-05\n",
            "Epoch  42 Batch  220 / 525  Training Loss  2.0443982066353783e-05\n",
            "Epoch  42 Batch  221 / 525  Training Loss  1.9086150132352486e-05\n",
            "Epoch  42 Batch  222 / 525  Training Loss  1.416690975020174e-05\n",
            "Epoch  42 Batch  223 / 525  Training Loss  2.358619349251967e-05\n",
            "Epoch  42 Batch  224 / 525  Training Loss  2.1655152522725984e-05\n",
            "Epoch  42 Batch  225 / 525  Training Loss  2.6637668270268477e-05\n",
            "Epoch  42 Batch  226 / 525  Training Loss  3.693423786899075e-05\n",
            "Epoch  42 Batch  227 / 525  Training Loss  1.4933784768800251e-05\n",
            "Epoch  42 Batch  228 / 525  Training Loss  1.914249332912732e-05\n",
            "Epoch  42 Batch  229 / 525  Training Loss  2.377289092692081e-05\n",
            "Epoch  42 Batch  230 / 525  Training Loss  2.2875214199302718e-05\n",
            "Epoch  42 Batch  231 / 525  Training Loss  1.9339560822118074e-05\n",
            "Epoch  42 Batch  232 / 525  Training Loss  2.2977081243880093e-05\n",
            "Epoch  42 Batch  233 / 525  Training Loss  1.2807386156055145e-05\n",
            "Epoch  42 Batch  234 / 525  Training Loss  1.902540861919988e-05\n",
            "Epoch  42 Batch  235 / 525  Training Loss  1.4799750715610571e-05\n",
            "Epoch  42 Batch  236 / 525  Training Loss  2.6201276341453195e-05\n",
            "Epoch  42 Batch  237 / 525  Training Loss  3.0799175874562934e-05\n",
            "Epoch  42 Batch  238 / 525  Training Loss  3.131023913738318e-05\n",
            "Epoch  42 Batch  239 / 525  Training Loss  2.8352378649287857e-05\n",
            "Epoch  42 Batch  240 / 525  Training Loss  2.8548796763061546e-05\n",
            "Epoch  42 Batch  241 / 525  Training Loss  2.561249493737705e-05\n",
            "Epoch  42 Batch  242 / 525  Training Loss  1.4215816918294877e-05\n",
            "Epoch  42 Batch  243 / 525  Training Loss  1.9218568922951818e-05\n",
            "Epoch  42 Batch  244 / 525  Training Loss  1.968460674106609e-05\n",
            "Epoch  42 Batch  245 / 525  Training Loss  2.1304420442902483e-05\n",
            "Epoch  42 Batch  246 / 525  Training Loss  2.440127900626976e-05\n",
            "Epoch  42 Batch  247 / 525  Training Loss  1.7758724425220862e-05\n",
            "Epoch  42 Batch  248 / 525  Training Loss  2.1900119463680312e-05\n",
            "Epoch  42 Batch  249 / 525  Training Loss  1.0481478966539726e-05\n",
            "Epoch  42 Batch  250 / 525  Training Loss  2.184640834457241e-05\n",
            "Epoch  42 Batch  251 / 525  Training Loss  1.9048728063353337e-05\n",
            "Epoch  42 Batch  252 / 525  Training Loss  1.9646622604341246e-05\n",
            "Epoch  42 Batch  253 / 525  Training Loss  1.1068410458392464e-05\n",
            "Epoch  42 Batch  254 / 525  Training Loss  1.452594187867362e-05\n",
            "Epoch  42 Batch  255 / 525  Training Loss  2.0710718672489747e-05\n",
            "Epoch  42 Batch  256 / 525  Training Loss  1.1221804925298784e-05\n",
            "Epoch  42 Batch  257 / 525  Training Loss  2.836406019923743e-05\n",
            "Epoch  42 Batch  258 / 525  Training Loss  2.7236750611336902e-05\n",
            "Epoch  42 Batch  259 / 525  Training Loss  1.8861039279727265e-05\n",
            "Epoch  42 Batch  260 / 525  Training Loss  2.8965976525796577e-05\n",
            "Epoch  42 Batch  261 / 525  Training Loss  2.2327058104565367e-05\n",
            "Epoch  42 Batch  262 / 525  Training Loss  2.2814598196418956e-05\n",
            "Epoch  42 Batch  263 / 525  Training Loss  1.769414302543737e-05\n",
            "Epoch  42 Batch  264 / 525  Training Loss  1.6573943867115304e-05\n",
            "Epoch  42 Batch  265 / 525  Training Loss  3.0020271879038773e-05\n",
            "Epoch  42 Batch  266 / 525  Training Loss  1.4751793059986085e-05\n",
            "Epoch  42 Batch  267 / 525  Training Loss  3.9478076359955594e-05\n",
            "Epoch  42 Batch  268 / 525  Training Loss  2.335492172278464e-05\n",
            "Epoch  42 Batch  269 / 525  Training Loss  1.9205783246434294e-05\n",
            "Epoch  42 Batch  270 / 525  Training Loss  1.247004092874704e-05\n",
            "Epoch  42 Batch  271 / 525  Training Loss  1.4433180695050396e-05\n",
            "Epoch  42 Batch  272 / 525  Training Loss  1.4809198546572588e-05\n",
            "Epoch  42 Batch  273 / 525  Training Loss  1.7917505829245783e-05\n",
            "Epoch  42 Batch  274 / 525  Training Loss  2.341312210774049e-05\n",
            "Epoch  42 Batch  275 / 525  Training Loss  1.9806435375357978e-05\n",
            "Epoch  42 Batch  276 / 525  Training Loss  2.233695522591006e-05\n",
            "Epoch  42 Batch  277 / 525  Training Loss  2.0828740161960013e-05\n",
            "Epoch  42 Batch  278 / 525  Training Loss  2.6973571948474273e-05\n",
            "Epoch  42 Batch  279 / 525  Training Loss  2.4653138098074123e-05\n",
            "Epoch  42 Batch  280 / 525  Training Loss  1.7466469216742553e-05\n",
            "Epoch  42 Batch  281 / 525  Training Loss  2.3687307475483976e-05\n",
            "Epoch  42 Batch  282 / 525  Training Loss  2.0634801330743358e-05\n",
            "Epoch  42 Batch  283 / 525  Training Loss  1.724277353787329e-05\n",
            "Epoch  42 Batch  284 / 525  Training Loss  3.2539865060243756e-05\n",
            "Epoch  42 Batch  285 / 525  Training Loss  2.2800679289503023e-05\n",
            "Epoch  42 Batch  286 / 525  Training Loss  1.815585528675001e-05\n",
            "Epoch  42 Batch  287 / 525  Training Loss  1.8338809240958653e-05\n",
            "Epoch  42 Batch  288 / 525  Training Loss  3.168659895891324e-05\n",
            "Epoch  42 Batch  289 / 525  Training Loss  2.1151277906028554e-05\n",
            "Epoch  42 Batch  290 / 525  Training Loss  1.643755240365863e-05\n",
            "Epoch  42 Batch  291 / 525  Training Loss  2.3830009013181552e-05\n",
            "Epoch  42 Batch  292 / 525  Training Loss  2.6937443180941045e-05\n",
            "Epoch  42 Batch  293 / 525  Training Loss  1.6979449355858378e-05\n",
            "Epoch  42 Batch  294 / 525  Training Loss  1.664144656388089e-05\n",
            "Epoch  42 Batch  295 / 525  Training Loss  1.8561211618361995e-05\n",
            "Epoch  42 Batch  296 / 525  Training Loss  1.1698342859745026e-05\n",
            "Epoch  42 Batch  297 / 525  Training Loss  2.3622327717021108e-05\n",
            "Epoch  42 Batch  298 / 525  Training Loss  2.6679797883844003e-05\n",
            "Epoch  42 Batch  299 / 525  Training Loss  2.5334355086670257e-05\n",
            "Epoch  42 Batch  300 / 525  Training Loss  2.0883055185549892e-05\n",
            "Epoch  42 Batch  301 / 525  Training Loss  1.3952405424788594e-05\n",
            "Epoch  42 Batch  302 / 525  Training Loss  2.5699642719700933e-05\n",
            "Epoch  42 Batch  303 / 525  Training Loss  1.3987079910293687e-05\n",
            "Epoch  42 Batch  304 / 525  Training Loss  1.813140625017695e-05\n",
            "Epoch  42 Batch  305 / 525  Training Loss  2.0121733541600406e-05\n",
            "Epoch  42 Batch  306 / 525  Training Loss  2.3843973394832574e-05\n",
            "Epoch  42 Batch  307 / 525  Training Loss  2.910592229454778e-05\n",
            "Epoch  42 Batch  308 / 525  Training Loss  1.9325361790833995e-05\n",
            "Epoch  42 Batch  309 / 525  Training Loss  1.8158654711442068e-05\n",
            "Epoch  42 Batch  310 / 525  Training Loss  1.4867046047584154e-05\n",
            "Epoch  42 Batch  311 / 525  Training Loss  1.886661266325973e-05\n",
            "Epoch  42 Batch  312 / 525  Training Loss  1.6914384104893543e-05\n",
            "Epoch  42 Batch  313 / 525  Training Loss  2.069909533020109e-05\n",
            "Epoch  42 Batch  314 / 525  Training Loss  2.9154001822462305e-05\n",
            "Epoch  42 Batch  315 / 525  Training Loss  2.4019045667955652e-05\n",
            "Epoch  42 Batch  316 / 525  Training Loss  2.5719094992382452e-05\n",
            "Epoch  42 Batch  317 / 525  Training Loss  1.2727328794426285e-05\n",
            "Epoch  42 Batch  318 / 525  Training Loss  1.480257196817547e-05\n",
            "Epoch  42 Batch  319 / 525  Training Loss  1.6001959011191502e-05\n",
            "Epoch  42 Batch  320 / 525  Training Loss  2.8139835194451734e-05\n",
            "Epoch  42 Batch  321 / 525  Training Loss  1.2917211279273033e-05\n",
            "Epoch  42 Batch  322 / 525  Training Loss  2.3566744857816957e-05\n",
            "Epoch  42 Batch  323 / 525  Training Loss  1.6892994608497247e-05\n",
            "Epoch  42 Batch  324 / 525  Training Loss  1.3944438251201063e-05\n",
            "Epoch  42 Batch  325 / 525  Training Loss  1.9156173948431388e-05\n",
            "Epoch  42 Batch  326 / 525  Training Loss  2.1622425265377387e-05\n",
            "Epoch  42 Batch  327 / 525  Training Loss  2.921223313023802e-05\n",
            "Epoch  42 Batch  328 / 525  Training Loss  2.211276841990184e-05\n",
            "Epoch  42 Batch  329 / 525  Training Loss  3.556785668479279e-05\n",
            "Epoch  42 Batch  330 / 525  Training Loss  1.388754571962636e-05\n",
            "Epoch  42 Batch  331 / 525  Training Loss  2.844763184839394e-05\n",
            "Epoch  42 Batch  332 / 525  Training Loss  2.273087011417374e-05\n",
            "Epoch  42 Batch  333 / 525  Training Loss  2.0292800400056876e-05\n",
            "Epoch  42 Batch  334 / 525  Training Loss  1.5857231119298376e-05\n",
            "Epoch  42 Batch  335 / 525  Training Loss  2.0093868442927487e-05\n",
            "Epoch  42 Batch  336 / 525  Training Loss  1.9847022485919297e-05\n",
            "Epoch  42 Batch  337 / 525  Training Loss  2.8372698579914868e-05\n",
            "Epoch  42 Batch  338 / 525  Training Loss  2.102814323734492e-05\n",
            "Epoch  42 Batch  339 / 525  Training Loss  2.4120152374962345e-05\n",
            "Epoch  42 Batch  340 / 525  Training Loss  2.4730334189371206e-05\n",
            "Epoch  42 Batch  341 / 525  Training Loss  2.5958657715818845e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  42 Batch  342 / 525  Training Loss  2.515383494028356e-05\n",
            "Epoch  42 Batch  343 / 525  Training Loss  1.206745673698606e-05\n",
            "Epoch  42 Batch  344 / 525  Training Loss  1.8300381270819344e-05\n",
            "Epoch  42 Batch  345 / 525  Training Loss  2.606222733447794e-05\n",
            "Epoch  42 Batch  346 / 525  Training Loss  1.906483157654293e-05\n",
            "Epoch  42 Batch  347 / 525  Training Loss  2.8770464268745854e-05\n",
            "Epoch  42 Batch  348 / 525  Training Loss  2.1564455892075785e-05\n",
            "Epoch  42 Batch  349 / 525  Training Loss  1.692597652436234e-05\n",
            "Epoch  42 Batch  350 / 525  Training Loss  1.534504917799495e-05\n",
            "Epoch  42 Batch  351 / 525  Training Loss  2.102660982927773e-05\n",
            "Epoch  42 Batch  352 / 525  Training Loss  2.1025261958129704e-05\n",
            "Epoch  42 Batch  353 / 525  Training Loss  2.1984829800203443e-05\n",
            "Epoch  42 Batch  354 / 525  Training Loss  1.8505668776924722e-05\n",
            "Epoch  42 Batch  355 / 525  Training Loss  1.6393771147704683e-05\n",
            "Epoch  42 Batch  356 / 525  Training Loss  1.5731682651676238e-05\n",
            "Epoch  42 Batch  357 / 525  Training Loss  3.6040262784808874e-05\n",
            "Epoch  42 Batch  358 / 525  Training Loss  2.5050630938494578e-05\n",
            "Epoch  42 Batch  359 / 525  Training Loss  1.2501863238867372e-05\n",
            "Epoch  42 Batch  360 / 525  Training Loss  1.7017671780195087e-05\n",
            "Epoch  42 Batch  361 / 525  Training Loss  2.174957080569584e-05\n",
            "Epoch  42 Batch  362 / 525  Training Loss  1.796982905943878e-05\n",
            "Epoch  42 Batch  363 / 525  Training Loss  2.1096613636473194e-05\n",
            "Epoch  42 Batch  364 / 525  Training Loss  1.957735548785422e-05\n",
            "Epoch  42 Batch  365 / 525  Training Loss  2.7520693038241006e-05\n",
            "Epoch  42 Batch  366 / 525  Training Loss  2.151985972886905e-05\n",
            "Epoch  42 Batch  367 / 525  Training Loss  1.5355952200479805e-05\n",
            "Epoch  42 Batch  368 / 525  Training Loss  2.6808189431903884e-05\n",
            "Epoch  42 Batch  369 / 525  Training Loss  2.040436265815515e-05\n",
            "Epoch  42 Batch  370 / 525  Training Loss  1.8636317690834403e-05\n",
            "Epoch  42 Batch  371 / 525  Training Loss  2.4000506527954713e-05\n",
            "Epoch  42 Batch  372 / 525  Training Loss  2.2318037736113183e-05\n",
            "Epoch  42 Batch  373 / 525  Training Loss  1.9880990294041112e-05\n",
            "Epoch  42 Batch  374 / 525  Training Loss  3.4831755328923464e-05\n",
            "Epoch  42 Batch  375 / 525  Training Loss  2.0172048607491888e-05\n",
            "Epoch  42 Batch  376 / 525  Training Loss  2.9209040803834796e-05\n",
            "Epoch  42 Batch  377 / 525  Training Loss  2.0725039576063864e-05\n",
            "Epoch  42 Batch  378 / 525  Training Loss  2.297023638675455e-05\n",
            "Epoch  42 Batch  379 / 525  Training Loss  1.076922762877075e-05\n",
            "Epoch  42 Batch  380 / 525  Training Loss  1.5307628927985206e-05\n",
            "Epoch  42 Batch  381 / 525  Training Loss  1.4932918929844163e-05\n",
            "Epoch  42 Batch  382 / 525  Training Loss  2.3838623746996745e-05\n",
            "Epoch  42 Batch  383 / 525  Training Loss  2.1448833649628796e-05\n",
            "Epoch  42 Batch  384 / 525  Training Loss  2.4493851014995016e-05\n",
            "Epoch  42 Batch  385 / 525  Training Loss  2.5559562345733866e-05\n",
            "Epoch  42 Batch  386 / 525  Training Loss  2.930249866039958e-05\n",
            "Epoch  42 Batch  387 / 525  Training Loss  1.3908802429796197e-05\n",
            "Epoch  42 Batch  388 / 525  Training Loss  1.7874303011922166e-05\n",
            "Epoch  42 Batch  389 / 525  Training Loss  1.6286578102153726e-05\n",
            "Epoch  42 Batch  390 / 525  Training Loss  1.61690386448754e-05\n",
            "Epoch  42 Batch  391 / 525  Training Loss  1.0206044862570707e-05\n",
            "Epoch  42 Batch  392 / 525  Training Loss  2.3720593162579462e-05\n",
            "Epoch  42 Batch  393 / 525  Training Loss  1.6419169696746394e-05\n",
            "Epoch  42 Batch  394 / 525  Training Loss  1.09204829641385e-05\n",
            "Epoch  42 Batch  395 / 525  Training Loss  1.6532254448975436e-05\n",
            "Epoch  42 Batch  396 / 525  Training Loss  2.041446350631304e-05\n",
            "Epoch  42 Batch  397 / 525  Training Loss  1.7075813957490027e-05\n",
            "Epoch  42 Batch  398 / 525  Training Loss  3.0413921194849536e-05\n",
            "Epoch  42 Batch  399 / 525  Training Loss  1.4584827113139909e-05\n",
            "Epoch  42 Batch  400 / 525  Training Loss  1.833763781178277e-05\n",
            "Epoch  42 Batch  401 / 525  Training Loss  1.2284192962397356e-05\n",
            "Epoch  42 Batch  402 / 525  Training Loss  1.881019488791935e-05\n",
            "Epoch  42 Batch  403 / 525  Training Loss  1.4673227269668132e-05\n",
            "Epoch  42 Batch  404 / 525  Training Loss  1.6823672922328115e-05\n",
            "Epoch  42 Batch  405 / 525  Training Loss  1.716422411845997e-05\n",
            "Epoch  42 Batch  406 / 525  Training Loss  1.6079222405096516e-05\n",
            "Epoch  42 Batch  407 / 525  Training Loss  2.2189464289112948e-05\n",
            "Epoch  42 Batch  408 / 525  Training Loss  4.1388564568478614e-05\n",
            "Epoch  42 Batch  409 / 525  Training Loss  2.0491685063461773e-05\n",
            "Epoch  42 Batch  410 / 525  Training Loss  1.4450279195443727e-05\n",
            "Epoch  42 Batch  411 / 525  Training Loss  1.7581620340934023e-05\n",
            "Epoch  42 Batch  412 / 525  Training Loss  1.8985088900080882e-05\n",
            "Epoch  42 Batch  413 / 525  Training Loss  2.1230243874015287e-05\n",
            "Epoch  42 Batch  414 / 525  Training Loss  1.7913449482875876e-05\n",
            "Epoch  42 Batch  415 / 525  Training Loss  3.222171653760597e-05\n",
            "Epoch  42 Batch  416 / 525  Training Loss  2.7565130949369632e-05\n",
            "Epoch  42 Batch  417 / 525  Training Loss  2.3217529815156013e-05\n",
            "Epoch  42 Batch  418 / 525  Training Loss  2.0979909095331095e-05\n",
            "Epoch  42 Batch  419 / 525  Training Loss  1.9637736841104925e-05\n",
            "Epoch  42 Batch  420 / 525  Training Loss  1.136885111918673e-05\n",
            "Epoch  42 Batch  421 / 525  Training Loss  2.6135434382013045e-05\n",
            "Epoch  42 Batch  422 / 525  Training Loss  1.1288391760899685e-05\n",
            "Epoch  42 Batch  423 / 525  Training Loss  2.3187796614365652e-05\n",
            "Epoch  42 Batch  424 / 525  Training Loss  1.988199437619187e-05\n",
            "Epoch  42 Batch  425 / 525  Training Loss  4.0137598261935636e-05\n",
            "Epoch  42 Batch  426 / 525  Training Loss  2.2740092390449718e-05\n",
            "Epoch  42 Batch  427 / 525  Training Loss  3.002654921147041e-05\n",
            "Epoch  42 Batch  428 / 525  Training Loss  2.585998117865529e-05\n",
            "Epoch  42 Batch  429 / 525  Training Loss  1.6330242942785844e-05\n",
            "Epoch  42 Batch  430 / 525  Training Loss  1.912687366711907e-05\n",
            "Epoch  42 Batch  431 / 525  Training Loss  1.865337799245026e-05\n",
            "Epoch  42 Batch  432 / 525  Training Loss  2.6202676963293925e-05\n",
            "Epoch  42 Batch  433 / 525  Training Loss  2.4962686438811943e-05\n",
            "Epoch  42 Batch  434 / 525  Training Loss  1.8564265701570548e-05\n",
            "Epoch  42 Batch  435 / 525  Training Loss  2.3325381334871054e-05\n",
            "Epoch  42 Batch  436 / 525  Training Loss  1.9588955183280632e-05\n",
            "Epoch  42 Batch  437 / 525  Training Loss  1.455709025321994e-05\n",
            "Epoch  42 Batch  438 / 525  Training Loss  1.8701468434301205e-05\n",
            "Epoch  42 Batch  439 / 525  Training Loss  2.0916550056426786e-05\n",
            "Epoch  42 Batch  440 / 525  Training Loss  2.489862708898727e-05\n",
            "Epoch  42 Batch  441 / 525  Training Loss  1.4007773643243127e-05\n",
            "Epoch  42 Batch  442 / 525  Training Loss  2.2673126295558177e-05\n",
            "Epoch  42 Batch  443 / 525  Training Loss  2.8740891139023006e-05\n",
            "Epoch  42 Batch  444 / 525  Training Loss  1.6578131180722266e-05\n",
            "Epoch  42 Batch  445 / 525  Training Loss  1.5327039363910444e-05\n",
            "Epoch  42 Batch  446 / 525  Training Loss  2.974273229483515e-05\n",
            "Epoch  42 Batch  447 / 525  Training Loss  1.7693688278086483e-05\n",
            "Epoch  42 Batch  448 / 525  Training Loss  1.944586074387189e-05\n",
            "Epoch  42 Batch  449 / 525  Training Loss  1.315483677899465e-05\n",
            "Epoch  42 Batch  450 / 525  Training Loss  1.3179434063204098e-05\n",
            "Epoch  42 Batch  451 / 525  Training Loss  3.734867641469464e-05\n",
            "Epoch  42 Batch  452 / 525  Training Loss  2.5829023797996342e-05\n",
            "Epoch  42 Batch  453 / 525  Training Loss  2.5067292881431058e-05\n",
            "Epoch  42 Batch  454 / 525  Training Loss  1.5680849173804745e-05\n",
            "Epoch  42 Batch  455 / 525  Training Loss  1.4822586308582686e-05\n",
            "Epoch  42 Batch  456 / 525  Training Loss  2.5365481633343734e-05\n",
            "Epoch  42 Batch  457 / 525  Training Loss  2.528565892134793e-05\n",
            "Epoch  42 Batch  458 / 525  Training Loss  2.49616896326188e-05\n",
            "Epoch  42 Batch  459 / 525  Training Loss  3.711854151333682e-05\n",
            "Epoch  42 Batch  460 / 525  Training Loss  1.4240496966522187e-05\n",
            "Epoch  42 Batch  461 / 525  Training Loss  2.3076590878190473e-05\n",
            "Epoch  42 Batch  462 / 525  Training Loss  1.4748931789654307e-05\n",
            "Epoch  42 Batch  463 / 525  Training Loss  1.9974319002358243e-05\n",
            "Epoch  42 Batch  464 / 525  Training Loss  2.0359040718176402e-05\n",
            "Epoch  42 Batch  465 / 525  Training Loss  3.081889371969737e-05\n",
            "Epoch  42 Batch  466 / 525  Training Loss  1.4211192137736361e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  42 Batch  467 / 525  Training Loss  2.548687916714698e-05\n",
            "Epoch  42 Batch  468 / 525  Training Loss  2.8093734727008268e-05\n",
            "Epoch  42 Batch  469 / 525  Training Loss  1.573277586430777e-05\n",
            "Epoch  42 Batch  470 / 525  Training Loss  2.3223228708957322e-05\n",
            "Epoch  42 Batch  471 / 525  Training Loss  2.2138216081657447e-05\n",
            "Epoch  42 Batch  472 / 525  Training Loss  1.6870155377546325e-05\n",
            "Epoch  42 Batch  473 / 525  Training Loss  2.460309406160377e-05\n",
            "Epoch  42 Batch  474 / 525  Training Loss  1.6008747479645535e-05\n",
            "Epoch  42 Batch  475 / 525  Training Loss  2.029658753599506e-05\n",
            "Epoch  42 Batch  476 / 525  Training Loss  3.0103063181741163e-05\n",
            "Epoch  42 Batch  477 / 525  Training Loss  1.0164716513827443e-05\n",
            "Epoch  42 Batch  478 / 525  Training Loss  2.5574268875061534e-05\n",
            "Epoch  42 Batch  479 / 525  Training Loss  3.1774725357536227e-05\n",
            "Epoch  42 Batch  480 / 525  Training Loss  2.1913781893090345e-05\n",
            "Epoch  42 Batch  481 / 525  Training Loss  1.5593117495882325e-05\n",
            "Epoch  42 Batch  482 / 525  Training Loss  2.246082112833392e-05\n",
            "Epoch  42 Batch  483 / 525  Training Loss  2.0070281607331708e-05\n",
            "Epoch  42 Batch  484 / 525  Training Loss  2.281056367792189e-05\n",
            "Epoch  42 Batch  485 / 525  Training Loss  3.5681579902302474e-05\n",
            "Epoch  42 Batch  486 / 525  Training Loss  2.511134880478494e-05\n",
            "Epoch  42 Batch  487 / 525  Training Loss  2.4789531380520202e-05\n",
            "Epoch  42 Batch  488 / 525  Training Loss  1.7341906641377136e-05\n",
            "Epoch  42 Batch  489 / 525  Training Loss  1.9924253138015047e-05\n",
            "Epoch  42 Batch  490 / 525  Training Loss  1.074049487215234e-05\n",
            "Epoch  42 Batch  491 / 525  Training Loss  2.0088957171537913e-05\n",
            "Epoch  42 Batch  492 / 525  Training Loss  1.3727483747061342e-05\n",
            "Epoch  42 Batch  493 / 525  Training Loss  3.178611950716004e-05\n",
            "Epoch  42 Batch  494 / 525  Training Loss  2.917327765317168e-05\n",
            "Epoch  42 Batch  495 / 525  Training Loss  1.8368224118603393e-05\n",
            "Epoch  42 Batch  496 / 525  Training Loss  1.5652363799745217e-05\n",
            "Epoch  42 Batch  497 / 525  Training Loss  2.396360105194617e-05\n",
            "Epoch  42 Batch  498 / 525  Training Loss  2.0378367480589077e-05\n",
            "Epoch  42 Batch  499 / 525  Training Loss  2.1232439394225366e-05\n",
            "Epoch  42 Batch  500 / 525  Training Loss  2.7474938178784214e-05\n",
            "Epoch  42 Batch  501 / 525  Training Loss  2.1393174392869696e-05\n",
            "Epoch  42 Batch  502 / 525  Training Loss  1.8828530301107094e-05\n",
            "Epoch  42 Batch  503 / 525  Training Loss  1.9784671167144552e-05\n",
            "Epoch  42 Batch  504 / 525  Training Loss  2.988179949170444e-05\n",
            "Epoch  42 Batch  505 / 525  Training Loss  1.6441643310827203e-05\n",
            "Epoch  42 Batch  506 / 525  Training Loss  1.7153983208118007e-05\n",
            "Epoch  42 Batch  507 / 525  Training Loss  1.934753527166322e-05\n",
            "Epoch  42 Batch  508 / 525  Training Loss  1.733279350446537e-05\n",
            "Epoch  42 Batch  509 / 525  Training Loss  1.9587208953453228e-05\n",
            "Epoch  42 Batch  510 / 525  Training Loss  2.5512772481306456e-05\n",
            "Epoch  42 Batch  511 / 525  Training Loss  2.5824690965237096e-05\n",
            "Epoch  42 Batch  512 / 525  Training Loss  2.6308169253752567e-05\n",
            "Epoch  42 Batch  513 / 525  Training Loss  3.520375321386382e-05\n",
            "Epoch  42 Batch  514 / 525  Training Loss  1.713983147055842e-05\n",
            "Epoch  42 Batch  515 / 525  Training Loss  1.4693483535666019e-05\n",
            "Epoch  42 Batch  516 / 525  Training Loss  1.2155792319390457e-05\n",
            "Epoch  42 Batch  517 / 525  Training Loss  1.8000364434556104e-05\n",
            "Epoch  42 Batch  518 / 525  Training Loss  2.556918843765743e-05\n",
            "Epoch  42 Batch  519 / 525  Training Loss  1.8693677702685818e-05\n",
            "Epoch  42 Batch  520 / 525  Training Loss  1.2329032870184164e-05\n",
            "Epoch  42 Batch  521 / 525  Training Loss  2.161336669814773e-05\n",
            "Epoch  42 Batch  522 / 525  Training Loss  2.923627835116349e-05\n",
            "Epoch  42 Batch  523 / 525  Training Loss  2.1003910660510883e-05\n",
            "Epoch  42 Batch  524 / 525  Training Loss  1.8869110135710798e-05\n",
            "  43    |    -    |   0.000022   | 64.416667\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 43\n",
            "Epoch  43 Batch  0 / 525  Training Loss  1.6328018318745308e-05\n",
            "Epoch  43 Batch  1 / 525  Training Loss  1.101640191336628e-05\n",
            "Epoch  43 Batch  2 / 525  Training Loss  1.2997965313843451e-05\n",
            "Epoch  43 Batch  3 / 525  Training Loss  1.4368008123710752e-05\n",
            "Epoch  43 Batch  4 / 525  Training Loss  1.4960201042413246e-05\n",
            "Epoch  43 Batch  5 / 525  Training Loss  1.5866393368924037e-05\n",
            "Epoch  43 Batch  6 / 525  Training Loss  2.1614159777527675e-05\n",
            "Epoch  43 Batch  7 / 525  Training Loss  1.7427159036742523e-05\n",
            "Epoch  43 Batch  8 / 525  Training Loss  1.690605677140411e-05\n",
            "Epoch  43 Batch  9 / 525  Training Loss  2.746530299191363e-05\n",
            "Epoch  43 Batch  10 / 525  Training Loss  1.6365107512683608e-05\n",
            "Epoch  43 Batch  11 / 525  Training Loss  1.0576442946330644e-05\n",
            "Epoch  43 Batch  12 / 525  Training Loss  2.2631880710832775e-05\n",
            "Epoch  43 Batch  13 / 525  Training Loss  2.256423613289371e-05\n",
            "Epoch  43 Batch  14 / 525  Training Loss  1.6640366084175184e-05\n",
            "Epoch  43 Batch  15 / 525  Training Loss  2.2200492821866646e-05\n",
            "Epoch  43 Batch  16 / 525  Training Loss  1.4729079339304008e-05\n",
            "Epoch  43 Batch  17 / 525  Training Loss  1.3343473256099969e-05\n",
            "Epoch  43 Batch  18 / 525  Training Loss  1.7892591131385416e-05\n",
            "Epoch  43 Batch  19 / 525  Training Loss  2.4458937332383357e-05\n",
            "Epoch  43 Batch  20 / 525  Training Loss  1.6193715055123903e-05\n",
            "Epoch  43 Batch  21 / 525  Training Loss  1.302563487115549e-05\n",
            "Epoch  43 Batch  22 / 525  Training Loss  1.978774889721535e-05\n",
            "Epoch  43 Batch  23 / 525  Training Loss  2.8676269721472636e-05\n",
            "Epoch  43 Batch  24 / 525  Training Loss  1.1188857570232358e-05\n",
            "Epoch  43 Batch  25 / 525  Training Loss  7.983905561559368e-06\n",
            "Epoch  43 Batch  26 / 525  Training Loss  2.006269460252952e-05\n",
            "Epoch  43 Batch  27 / 525  Training Loss  2.322828549949918e-05\n",
            "Epoch  43 Batch  28 / 525  Training Loss  1.7446016499889083e-05\n",
            "Epoch  43 Batch  29 / 525  Training Loss  2.1053114323876798e-05\n",
            "Epoch  43 Batch  30 / 525  Training Loss  2.0558458345476538e-05\n",
            "Epoch  43 Batch  31 / 525  Training Loss  2.7603256967267953e-05\n",
            "Epoch  43 Batch  32 / 525  Training Loss  1.988470830838196e-05\n",
            "Epoch  43 Batch  33 / 525  Training Loss  1.4898822882969398e-05\n",
            "Epoch  43 Batch  34 / 525  Training Loss  2.145057442248799e-05\n",
            "Epoch  43 Batch  35 / 525  Training Loss  1.0036710591521114e-05\n",
            "Epoch  43 Batch  36 / 525  Training Loss  1.3131621926731896e-05\n",
            "Epoch  43 Batch  37 / 525  Training Loss  1.241773679794278e-05\n",
            "Epoch  43 Batch  38 / 525  Training Loss  1.4160136743157636e-05\n",
            "Epoch  43 Batch  39 / 525  Training Loss  2.2289250409812666e-05\n",
            "Epoch  43 Batch  40 / 525  Training Loss  1.9169658116879873e-05\n",
            "Epoch  43 Batch  41 / 525  Training Loss  1.420353783032624e-05\n",
            "Epoch  43 Batch  42 / 525  Training Loss  2.047346788458526e-05\n",
            "Epoch  43 Batch  43 / 525  Training Loss  9.12530413188506e-06\n",
            "Epoch  43 Batch  44 / 525  Training Loss  2.358350138820242e-05\n",
            "Epoch  43 Batch  45 / 525  Training Loss  1.0315945473848842e-05\n",
            "Epoch  43 Batch  46 / 525  Training Loss  1.7151824067695998e-05\n",
            "Epoch  43 Batch  47 / 525  Training Loss  1.917605368362274e-05\n",
            "Epoch  43 Batch  48 / 525  Training Loss  1.9194205378880724e-05\n",
            "Epoch  43 Batch  49 / 525  Training Loss  1.7487696823081933e-05\n",
            "Epoch  43 Batch  50 / 525  Training Loss  2.0223971660016105e-05\n",
            "Epoch  43 Batch  51 / 525  Training Loss  1.6258431060123257e-05\n",
            "Epoch  43 Batch  52 / 525  Training Loss  2.498959111107979e-05\n",
            "Epoch  43 Batch  53 / 525  Training Loss  1.740887455525808e-05\n",
            "Epoch  43 Batch  54 / 525  Training Loss  1.6841919205035083e-05\n",
            "Epoch  43 Batch  55 / 525  Training Loss  2.8144806492491625e-05\n",
            "Epoch  43 Batch  56 / 525  Training Loss  1.2051784324285109e-05\n",
            "Epoch  43 Batch  57 / 525  Training Loss  1.672761754889507e-05\n",
            "Epoch  43 Batch  58 / 525  Training Loss  1.6028154277591966e-05\n",
            "Epoch  43 Batch  59 / 525  Training Loss  1.9359926227480173e-05\n",
            "Epoch  43 Batch  60 / 525  Training Loss  1.9787334167631343e-05\n",
            "Epoch  43 Batch  61 / 525  Training Loss  1.4947012459742837e-05\n",
            "Epoch  43 Batch  62 / 525  Training Loss  2.135917566192802e-05\n",
            "Epoch  43 Batch  63 / 525  Training Loss  1.4452823961619288e-05\n",
            "Epoch  43 Batch  64 / 525  Training Loss  1.3025116459175479e-05\n",
            "Epoch  43 Batch  65 / 525  Training Loss  2.3758828319841996e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  43 Batch  66 / 525  Training Loss  2.0024803234264255e-05\n",
            "Epoch  43 Batch  67 / 525  Training Loss  2.26327138079796e-05\n",
            "Epoch  43 Batch  68 / 525  Training Loss  2.263242276967503e-05\n",
            "Epoch  43 Batch  69 / 525  Training Loss  1.925894684973173e-05\n",
            "Epoch  43 Batch  70 / 525  Training Loss  1.7114642105298117e-05\n",
            "Epoch  43 Batch  71 / 525  Training Loss  1.6033425708883442e-05\n",
            "Epoch  43 Batch  72 / 525  Training Loss  1.4104218280408531e-05\n",
            "Epoch  43 Batch  73 / 525  Training Loss  1.5774445273564197e-05\n",
            "Epoch  43 Batch  74 / 525  Training Loss  1.1804592759290244e-05\n",
            "Epoch  43 Batch  75 / 525  Training Loss  1.9491273633320816e-05\n",
            "Epoch  43 Batch  76 / 525  Training Loss  1.753352626110427e-05\n",
            "Epoch  43 Batch  77 / 525  Training Loss  1.1646873645076994e-05\n",
            "Epoch  43 Batch  78 / 525  Training Loss  1.9287139366497286e-05\n",
            "Epoch  43 Batch  79 / 525  Training Loss  2.25158546527382e-05\n",
            "Epoch  43 Batch  80 / 525  Training Loss  1.9807243006653152e-05\n",
            "Epoch  43 Batch  81 / 525  Training Loss  1.8256745534017682e-05\n",
            "Epoch  43 Batch  82 / 525  Training Loss  2.144564496120438e-05\n",
            "Epoch  43 Batch  83 / 525  Training Loss  1.7065900465240702e-05\n",
            "Epoch  43 Batch  84 / 525  Training Loss  3.195447789039463e-05\n",
            "Epoch  43 Batch  85 / 525  Training Loss  1.2268710634089075e-05\n",
            "Epoch  43 Batch  86 / 525  Training Loss  1.8346920114709064e-05\n",
            "Epoch  43 Batch  87 / 525  Training Loss  2.9524657293222845e-05\n",
            "Epoch  43 Batch  88 / 525  Training Loss  1.7863249013316818e-05\n",
            "Epoch  43 Batch  89 / 525  Training Loss  1.4819821444689296e-05\n",
            "Epoch  43 Batch  90 / 525  Training Loss  2.162970122299157e-05\n",
            "Epoch  43 Batch  91 / 525  Training Loss  1.64111697813496e-05\n",
            "Epoch  43 Batch  92 / 525  Training Loss  2.8405876946635544e-05\n",
            "Epoch  43 Batch  93 / 525  Training Loss  1.563628393341787e-05\n",
            "Epoch  43 Batch  94 / 525  Training Loss  2.1773852495243773e-05\n",
            "Epoch  43 Batch  95 / 525  Training Loss  1.7041806131601334e-05\n",
            "Epoch  43 Batch  96 / 525  Training Loss  1.83857209776761e-05\n",
            "Epoch  43 Batch  97 / 525  Training Loss  2.068981120828539e-05\n",
            "Epoch  43 Batch  98 / 525  Training Loss  1.9936775061069056e-05\n",
            "Epoch  43 Batch  99 / 525  Training Loss  1.519680972705828e-05\n",
            "Epoch  43 Batch  100 / 525  Training Loss  3.666821066872217e-05\n",
            "Epoch  43 Batch  101 / 525  Training Loss  1.3909430890635122e-05\n",
            "Epoch  43 Batch  102 / 525  Training Loss  2.131331348209642e-05\n",
            "Epoch  43 Batch  103 / 525  Training Loss  1.7009209841489792e-05\n",
            "Epoch  43 Batch  104 / 525  Training Loss  2.4570583263994195e-05\n",
            "Epoch  43 Batch  105 / 525  Training Loss  1.5981962860678323e-05\n",
            "Epoch  43 Batch  106 / 525  Training Loss  2.3272588805411942e-05\n",
            "Epoch  43 Batch  107 / 525  Training Loss  2.227930053777527e-05\n",
            "Epoch  43 Batch  108 / 525  Training Loss  1.3183833289076574e-05\n",
            "Epoch  43 Batch  109 / 525  Training Loss  2.233279519714415e-05\n",
            "Epoch  43 Batch  110 / 525  Training Loss  2.067239438474644e-05\n",
            "Epoch  43 Batch  111 / 525  Training Loss  2.1627380192512646e-05\n",
            "Epoch  43 Batch  112 / 525  Training Loss  2.6341536795371212e-05\n",
            "Epoch  43 Batch  113 / 525  Training Loss  2.2232487026485614e-05\n",
            "Epoch  43 Batch  114 / 525  Training Loss  1.4064476999919862e-05\n",
            "Epoch  43 Batch  115 / 525  Training Loss  8.203842298826203e-06\n",
            "Epoch  43 Batch  116 / 525  Training Loss  1.5165855074883439e-05\n",
            "Epoch  43 Batch  117 / 525  Training Loss  2.5496319722151384e-05\n",
            "Epoch  43 Batch  118 / 525  Training Loss  2.1265248506097123e-05\n",
            "Epoch  43 Batch  119 / 525  Training Loss  1.7633406969252974e-05\n",
            "Epoch  43 Batch  120 / 525  Training Loss  1.5046464795887005e-05\n",
            "Epoch  43 Batch  121 / 525  Training Loss  1.7431380911148153e-05\n",
            "Epoch  43 Batch  122 / 525  Training Loss  1.0788999134092592e-05\n",
            "Epoch  43 Batch  123 / 525  Training Loss  1.9103474187431857e-05\n",
            "Epoch  43 Batch  124 / 525  Training Loss  1.605796023795847e-05\n",
            "Epoch  43 Batch  125 / 525  Training Loss  1.37680399348028e-05\n",
            "Epoch  43 Batch  126 / 525  Training Loss  2.0235935153323226e-05\n",
            "Epoch  43 Batch  127 / 525  Training Loss  2.753509943431709e-05\n",
            "Epoch  43 Batch  128 / 525  Training Loss  2.2633772459812462e-05\n",
            "Epoch  43 Batch  129 / 525  Training Loss  1.3672475688508712e-05\n",
            "Epoch  43 Batch  130 / 525  Training Loss  2.1838761313119903e-05\n",
            "Epoch  43 Batch  131 / 525  Training Loss  2.355967808398418e-05\n",
            "Epoch  43 Batch  132 / 525  Training Loss  1.582387812959496e-05\n",
            "Epoch  43 Batch  133 / 525  Training Loss  1.8743616237770766e-05\n",
            "Epoch  43 Batch  134 / 525  Training Loss  2.129329186573159e-05\n",
            "Epoch  43 Batch  135 / 525  Training Loss  1.3554449651564937e-05\n",
            "Epoch  43 Batch  136 / 525  Training Loss  1.4683794688608032e-05\n",
            "Epoch  43 Batch  137 / 525  Training Loss  2.1272648154990748e-05\n",
            "Epoch  43 Batch  138 / 525  Training Loss  2.3825004973332398e-05\n",
            "Epoch  43 Batch  139 / 525  Training Loss  1.3188738194003236e-05\n",
            "Epoch  43 Batch  140 / 525  Training Loss  1.6759910067776218e-05\n",
            "Epoch  43 Batch  141 / 525  Training Loss  2.755412060650997e-05\n",
            "Epoch  43 Batch  142 / 525  Training Loss  1.486302244302351e-05\n",
            "Epoch  43 Batch  143 / 525  Training Loss  1.9743316443054937e-05\n",
            "Epoch  43 Batch  144 / 525  Training Loss  2.1609466784866527e-05\n",
            "Epoch  43 Batch  145 / 525  Training Loss  2.2219846869120374e-05\n",
            "Epoch  43 Batch  146 / 525  Training Loss  1.871512722573243e-05\n",
            "Epoch  43 Batch  147 / 525  Training Loss  2.404263250355143e-05\n",
            "Epoch  43 Batch  148 / 525  Training Loss  2.2940916096558794e-05\n",
            "Epoch  43 Batch  149 / 525  Training Loss  2.5875506253214553e-05\n",
            "Epoch  43 Batch  150 / 525  Training Loss  2.5098444893956184e-05\n",
            "Epoch  43 Batch  151 / 525  Training Loss  2.1831663616467267e-05\n",
            "Epoch  43 Batch  152 / 525  Training Loss  2.125734681612812e-05\n",
            "Epoch  43 Batch  153 / 525  Training Loss  1.816110307117924e-05\n",
            "Epoch  43 Batch  154 / 525  Training Loss  1.3567354471888393e-05\n",
            "Epoch  43 Batch  155 / 525  Training Loss  1.9170553059666418e-05\n",
            "Epoch  43 Batch  156 / 525  Training Loss  2.3437482013832778e-05\n",
            "Epoch  43 Batch  157 / 525  Training Loss  1.905407043523155e-05\n",
            "Epoch  43 Batch  158 / 525  Training Loss  1.768257061485201e-05\n",
            "Epoch  43 Batch  159 / 525  Training Loss  2.2949843696551397e-05\n",
            "Epoch  43 Batch  160 / 525  Training Loss  1.9044264263357036e-05\n",
            "Epoch  43 Batch  161 / 525  Training Loss  1.4762078535568435e-05\n",
            "Epoch  43 Batch  162 / 525  Training Loss  1.4387589544639923e-05\n",
            "Epoch  43 Batch  163 / 525  Training Loss  1.3319372555997688e-05\n",
            "Epoch  43 Batch  164 / 525  Training Loss  1.270797110919375e-05\n",
            "Epoch  43 Batch  165 / 525  Training Loss  1.2386980415612925e-05\n",
            "Epoch  43 Batch  166 / 525  Training Loss  1.8498974895919673e-05\n",
            "Epoch  43 Batch  167 / 525  Training Loss  3.289679443696514e-05\n",
            "Epoch  43 Batch  168 / 525  Training Loss  1.4711185940541327e-05\n",
            "Epoch  43 Batch  169 / 525  Training Loss  1.4143642147246283e-05\n",
            "Epoch  43 Batch  170 / 525  Training Loss  2.363838575547561e-05\n",
            "Epoch  43 Batch  171 / 525  Training Loss  1.612052074051462e-05\n",
            "Epoch  43 Batch  172 / 525  Training Loss  1.3980396033730358e-05\n",
            "Epoch  43 Batch  173 / 525  Training Loss  2.1602951164823025e-05\n",
            "Epoch  43 Batch  174 / 525  Training Loss  2.4054217647062615e-05\n",
            "Epoch  43 Batch  175 / 525  Training Loss  2.2702939531882294e-05\n",
            "Epoch  43 Batch  176 / 525  Training Loss  1.6765432519605383e-05\n",
            "Epoch  43 Batch  177 / 525  Training Loss  2.7077683625975624e-05\n",
            "Epoch  43 Batch  178 / 525  Training Loss  3.206693145330064e-05\n",
            "Epoch  43 Batch  179 / 525  Training Loss  2.175537883886136e-05\n",
            "Epoch  43 Batch  180 / 525  Training Loss  1.1813399396487512e-05\n",
            "Epoch  43 Batch  181 / 525  Training Loss  1.5493515093112364e-05\n",
            "Epoch  43 Batch  182 / 525  Training Loss  1.9580784282879904e-05\n",
            "Epoch  43 Batch  183 / 525  Training Loss  1.6571651940466836e-05\n",
            "Epoch  43 Batch  184 / 525  Training Loss  1.9570967197068967e-05\n",
            "Epoch  43 Batch  185 / 525  Training Loss  2.6649107894627377e-05\n",
            "Epoch  43 Batch  186 / 525  Training Loss  2.446034886816051e-05\n",
            "Epoch  43 Batch  187 / 525  Training Loss  1.748592512740288e-05\n",
            "Epoch  43 Batch  188 / 525  Training Loss  1.571325810800772e-05\n",
            "Epoch  43 Batch  189 / 525  Training Loss  2.4522387320757844e-05\n",
            "Epoch  43 Batch  190 / 525  Training Loss  1.1630270819296129e-05\n",
            "Epoch  43 Batch  191 / 525  Training Loss  1.2200985111121554e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  43 Batch  192 / 525  Training Loss  1.1628176252997946e-05\n",
            "Epoch  43 Batch  193 / 525  Training Loss  1.6078622138593346e-05\n",
            "Epoch  43 Batch  194 / 525  Training Loss  2.1385843865573406e-05\n",
            "Epoch  43 Batch  195 / 525  Training Loss  2.1028754417784512e-05\n",
            "Epoch  43 Batch  196 / 525  Training Loss  1.2918698303110432e-05\n",
            "Epoch  43 Batch  197 / 525  Training Loss  1.962160058610607e-05\n",
            "Epoch  43 Batch  198 / 525  Training Loss  1.1001342500094324e-05\n",
            "Epoch  43 Batch  199 / 525  Training Loss  2.67257091763895e-05\n",
            "Epoch  43 Batch  200 / 525  Training Loss  1.5155613255046774e-05\n",
            "Epoch  43 Batch  201 / 525  Training Loss  1.312616041104775e-05\n",
            "Epoch  43 Batch  202 / 525  Training Loss  2.5352559532620944e-05\n",
            "Epoch  43 Batch  203 / 525  Training Loss  2.0304541976656765e-05\n",
            "Epoch  43 Batch  204 / 525  Training Loss  2.4988787117763422e-05\n",
            "Epoch  43 Batch  205 / 525  Training Loss  1.080005858966615e-05\n",
            "Epoch  43 Batch  206 / 525  Training Loss  1.4023707990418188e-05\n",
            "Epoch  43 Batch  207 / 525  Training Loss  1.794009040168021e-05\n",
            "Epoch  43 Batch  208 / 525  Training Loss  1.7480178939877078e-05\n",
            "Epoch  43 Batch  209 / 525  Training Loss  1.3646928891830612e-05\n",
            "Epoch  43 Batch  210 / 525  Training Loss  2.646179018483963e-05\n",
            "Epoch  43 Batch  211 / 525  Training Loss  2.4105913325911388e-05\n",
            "Epoch  43 Batch  212 / 525  Training Loss  1.1211483069928363e-05\n",
            "Epoch  43 Batch  213 / 525  Training Loss  1.2982377484149765e-05\n",
            "Epoch  43 Batch  214 / 525  Training Loss  2.210711500083562e-05\n",
            "Epoch  43 Batch  215 / 525  Training Loss  9.12505856831558e-06\n",
            "Epoch  43 Batch  216 / 525  Training Loss  2.0664101612055674e-05\n",
            "Epoch  43 Batch  217 / 525  Training Loss  1.3243365174275823e-05\n",
            "Epoch  43 Batch  218 / 525  Training Loss  9.388428225065582e-06\n",
            "Epoch  43 Batch  219 / 525  Training Loss  1.4688009287056047e-05\n",
            "Epoch  43 Batch  220 / 525  Training Loss  1.651831917115487e-05\n",
            "Epoch  43 Batch  221 / 525  Training Loss  2.946642598544713e-05\n",
            "Epoch  43 Batch  222 / 525  Training Loss  2.7040447093895636e-05\n",
            "Epoch  43 Batch  223 / 525  Training Loss  1.369545680063311e-05\n",
            "Epoch  43 Batch  224 / 525  Training Loss  1.2863085430581123e-05\n",
            "Epoch  43 Batch  225 / 525  Training Loss  2.012790719163604e-05\n",
            "Epoch  43 Batch  226 / 525  Training Loss  2.1728657884523273e-05\n",
            "Epoch  43 Batch  227 / 525  Training Loss  2.358818346692715e-05\n",
            "Epoch  43 Batch  228 / 525  Training Loss  1.6537122064619325e-05\n",
            "Epoch  43 Batch  229 / 525  Training Loss  2.2213722331798635e-05\n",
            "Epoch  43 Batch  230 / 525  Training Loss  2.170433072024025e-05\n",
            "Epoch  43 Batch  231 / 525  Training Loss  1.9586535927373916e-05\n",
            "Epoch  43 Batch  232 / 525  Training Loss  2.006526119657792e-05\n",
            "Epoch  43 Batch  233 / 525  Training Loss  1.166792480944423e-05\n",
            "Epoch  43 Batch  234 / 525  Training Loss  2.919112375820987e-05\n",
            "Epoch  43 Batch  235 / 525  Training Loss  1.8833230569725856e-05\n",
            "Epoch  43 Batch  236 / 525  Training Loss  1.659367080719676e-05\n",
            "Epoch  43 Batch  237 / 525  Training Loss  1.4917684893589467e-05\n",
            "Epoch  43 Batch  238 / 525  Training Loss  1.9615263227024116e-05\n",
            "Epoch  43 Batch  239 / 525  Training Loss  2.444481106067542e-05\n",
            "Epoch  43 Batch  240 / 525  Training Loss  4.75208935313276e-06\n",
            "Epoch  43 Batch  241 / 525  Training Loss  1.777797660906799e-05\n",
            "Epoch  43 Batch  242 / 525  Training Loss  2.9017242923146114e-05\n",
            "Epoch  43 Batch  243 / 525  Training Loss  1.7264450434595346e-05\n",
            "Epoch  43 Batch  244 / 525  Training Loss  1.5920250007184222e-05\n",
            "Epoch  43 Batch  245 / 525  Training Loss  1.7367479813401587e-05\n",
            "Epoch  43 Batch  246 / 525  Training Loss  1.0229892723145895e-05\n",
            "Epoch  43 Batch  247 / 525  Training Loss  2.361230508540757e-05\n",
            "Epoch  43 Batch  248 / 525  Training Loss  1.665231684455648e-05\n",
            "Epoch  43 Batch  249 / 525  Training Loss  1.287580107600661e-05\n",
            "Epoch  43 Batch  250 / 525  Training Loss  1.4621044101659209e-05\n",
            "Epoch  43 Batch  251 / 525  Training Loss  2.2886308215674944e-05\n",
            "Epoch  43 Batch  252 / 525  Training Loss  1.360352871415671e-05\n",
            "Epoch  43 Batch  253 / 525  Training Loss  1.3967837730888277e-05\n",
            "Epoch  43 Batch  254 / 525  Training Loss  2.051594856311567e-05\n",
            "Epoch  43 Batch  255 / 525  Training Loss  1.9167597201885656e-05\n",
            "Epoch  43 Batch  256 / 525  Training Loss  2.079769183183089e-05\n",
            "Epoch  43 Batch  257 / 525  Training Loss  2.1523068426176906e-05\n",
            "Epoch  43 Batch  258 / 525  Training Loss  1.2061425877618603e-05\n",
            "Epoch  43 Batch  259 / 525  Training Loss  1.780936509021558e-05\n",
            "Epoch  43 Batch  260 / 525  Training Loss  1.7512717022327706e-05\n",
            "Epoch  43 Batch  261 / 525  Training Loss  2.7625454094959423e-05\n",
            "Epoch  43 Batch  262 / 525  Training Loss  1.5024177628220059e-05\n",
            "Epoch  43 Batch  263 / 525  Training Loss  1.3044244951743167e-05\n",
            "Epoch  43 Batch  264 / 525  Training Loss  2.8761898647644557e-05\n",
            "Epoch  43 Batch  265 / 525  Training Loss  2.1083949832245708e-05\n",
            "Epoch  43 Batch  266 / 525  Training Loss  1.0864772775676101e-05\n",
            "Epoch  43 Batch  267 / 525  Training Loss  1.9873030396411195e-05\n",
            "Epoch  43 Batch  268 / 525  Training Loss  2.0132163626840338e-05\n",
            "Epoch  43 Batch  269 / 525  Training Loss  2.3021999368211254e-05\n",
            "Epoch  43 Batch  270 / 525  Training Loss  1.698503183433786e-05\n",
            "Epoch  43 Batch  271 / 525  Training Loss  2.4147002477548085e-05\n",
            "Epoch  43 Batch  272 / 525  Training Loss  1.3221846529631875e-05\n",
            "Epoch  43 Batch  273 / 525  Training Loss  2.1256779291434214e-05\n",
            "Epoch  43 Batch  274 / 525  Training Loss  2.0223878891556524e-05\n",
            "Epoch  43 Batch  275 / 525  Training Loss  1.1845876542793121e-05\n",
            "Epoch  43 Batch  276 / 525  Training Loss  1.4914284292899538e-05\n",
            "Epoch  43 Batch  277 / 525  Training Loss  2.9873312087147497e-05\n",
            "Epoch  43 Batch  278 / 525  Training Loss  1.9961766156484373e-05\n",
            "Epoch  43 Batch  279 / 525  Training Loss  1.9565206457627937e-05\n",
            "Epoch  43 Batch  280 / 525  Training Loss  2.697473973967135e-05\n",
            "Epoch  43 Batch  281 / 525  Training Loss  1.4952699530113023e-05\n",
            "Epoch  43 Batch  282 / 525  Training Loss  1.6362417227355763e-05\n",
            "Epoch  43 Batch  283 / 525  Training Loss  1.1341455319779925e-05\n",
            "Epoch  43 Batch  284 / 525  Training Loss  1.865265949163586e-05\n",
            "Epoch  43 Batch  285 / 525  Training Loss  1.6638641682220623e-05\n",
            "Epoch  43 Batch  286 / 525  Training Loss  1.9879487808793783e-05\n",
            "Epoch  43 Batch  287 / 525  Training Loss  1.929715472215321e-05\n",
            "Epoch  43 Batch  288 / 525  Training Loss  1.6598187357885763e-05\n",
            "Epoch  43 Batch  289 / 525  Training Loss  1.5420551790157333e-05\n",
            "Epoch  43 Batch  290 / 525  Training Loss  1.723454988677986e-05\n",
            "Epoch  43 Batch  291 / 525  Training Loss  2.31379071919946e-05\n",
            "Epoch  43 Batch  292 / 525  Training Loss  2.420832242933102e-05\n",
            "Epoch  43 Batch  293 / 525  Training Loss  2.4463835870847106e-05\n",
            "Epoch  43 Batch  294 / 525  Training Loss  2.074268377327826e-05\n",
            "Epoch  43 Batch  295 / 525  Training Loss  2.1527048374991864e-05\n",
            "Epoch  43 Batch  296 / 525  Training Loss  1.592098305991385e-05\n",
            "Epoch  43 Batch  297 / 525  Training Loss  1.8773054762277752e-05\n",
            "Epoch  43 Batch  298 / 525  Training Loss  1.890753992483951e-05\n",
            "Epoch  43 Batch  299 / 525  Training Loss  2.150857835658826e-05\n",
            "Epoch  43 Batch  300 / 525  Training Loss  2.842803041858133e-05\n",
            "Epoch  43 Batch  301 / 525  Training Loss  3.638947237050161e-05\n",
            "Epoch  43 Batch  302 / 525  Training Loss  1.2429823982529342e-05\n",
            "Epoch  43 Batch  303 / 525  Training Loss  2.0585086531355046e-05\n",
            "Epoch  43 Batch  304 / 525  Training Loss  2.4670507627888583e-05\n",
            "Epoch  43 Batch  305 / 525  Training Loss  1.4471729627985042e-05\n",
            "Epoch  43 Batch  306 / 525  Training Loss  8.562013135815505e-06\n",
            "Epoch  43 Batch  307 / 525  Training Loss  2.235953252238687e-05\n",
            "Epoch  43 Batch  308 / 525  Training Loss  1.9375262127141468e-05\n",
            "Epoch  43 Batch  309 / 525  Training Loss  1.621055525902193e-05\n",
            "Epoch  43 Batch  310 / 525  Training Loss  2.559828499215655e-05\n",
            "Epoch  43 Batch  311 / 525  Training Loss  1.8428656403557397e-05\n",
            "Epoch  43 Batch  312 / 525  Training Loss  1.916972905746661e-05\n",
            "Epoch  43 Batch  313 / 525  Training Loss  2.0830048015341163e-05\n",
            "Epoch  43 Batch  314 / 525  Training Loss  2.1038034901721403e-05\n",
            "Epoch  43 Batch  315 / 525  Training Loss  1.4881203242111951e-05\n",
            "Epoch  43 Batch  316 / 525  Training Loss  1.0722090337367263e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  43 Batch  317 / 525  Training Loss  2.2418735170504078e-05\n",
            "Epoch  43 Batch  318 / 525  Training Loss  2.1429354092106223e-05\n",
            "Epoch  43 Batch  319 / 525  Training Loss  1.6364147086278535e-05\n",
            "Epoch  43 Batch  320 / 525  Training Loss  2.2190797608345747e-05\n",
            "Epoch  43 Batch  321 / 525  Training Loss  3.153743455186486e-05\n",
            "Epoch  43 Batch  322 / 525  Training Loss  2.5603581889299676e-05\n",
            "Epoch  43 Batch  323 / 525  Training Loss  1.5413468645419925e-05\n",
            "Epoch  43 Batch  324 / 525  Training Loss  1.7457037756685168e-05\n",
            "Epoch  43 Batch  325 / 525  Training Loss  1.501276437920751e-05\n",
            "Epoch  43 Batch  326 / 525  Training Loss  2.4319078875123523e-05\n",
            "Epoch  43 Batch  327 / 525  Training Loss  1.411121957062278e-05\n",
            "Epoch  43 Batch  328 / 525  Training Loss  9.401305760547984e-06\n",
            "Epoch  43 Batch  329 / 525  Training Loss  1.778494151949417e-05\n",
            "Epoch  43 Batch  330 / 525  Training Loss  2.5544050004100427e-05\n",
            "Epoch  43 Batch  331 / 525  Training Loss  1.5455205357284285e-05\n",
            "Epoch  43 Batch  332 / 525  Training Loss  1.3199484783399384e-05\n",
            "Epoch  43 Batch  333 / 525  Training Loss  1.6149939256138168e-05\n",
            "Epoch  43 Batch  334 / 525  Training Loss  1.48968229041202e-05\n",
            "Epoch  43 Batch  335 / 525  Training Loss  1.1532162716321182e-05\n",
            "Epoch  43 Batch  336 / 525  Training Loss  2.0201938241370954e-05\n",
            "Epoch  43 Batch  337 / 525  Training Loss  1.1384772733435966e-05\n",
            "Epoch  43 Batch  338 / 525  Training Loss  1.6427551599917933e-05\n",
            "Epoch  43 Batch  339 / 525  Training Loss  1.484244148741709e-05\n",
            "Epoch  43 Batch  340 / 525  Training Loss  2.1469368221005425e-05\n",
            "Epoch  43 Batch  341 / 525  Training Loss  1.687929398030974e-05\n",
            "Epoch  43 Batch  342 / 525  Training Loss  1.9034649085369892e-05\n",
            "Epoch  43 Batch  343 / 525  Training Loss  2.0345523807918653e-05\n",
            "Epoch  43 Batch  344 / 525  Training Loss  1.1571807590371463e-05\n",
            "Epoch  43 Batch  345 / 525  Training Loss  1.3792950994684361e-05\n",
            "Epoch  43 Batch  346 / 525  Training Loss  1.5180974514805712e-05\n",
            "Epoch  43 Batch  347 / 525  Training Loss  2.800675792968832e-05\n",
            "Epoch  43 Batch  348 / 525  Training Loss  1.4529458894685376e-05\n",
            "Epoch  43 Batch  349 / 525  Training Loss  1.4485534848063253e-05\n",
            "Epoch  43 Batch  350 / 525  Training Loss  2.5584846298443154e-05\n",
            "Epoch  43 Batch  351 / 525  Training Loss  1.5729254300822504e-05\n",
            "Epoch  43 Batch  352 / 525  Training Loss  3.189428389305249e-05\n",
            "Epoch  43 Batch  353 / 525  Training Loss  1.1938960597035475e-05\n",
            "Epoch  43 Batch  354 / 525  Training Loss  1.5936415366013534e-05\n",
            "Epoch  43 Batch  355 / 525  Training Loss  1.295770471188007e-05\n",
            "Epoch  43 Batch  356 / 525  Training Loss  1.5148616512306035e-05\n",
            "Epoch  43 Batch  357 / 525  Training Loss  1.8394131984678097e-05\n",
            "Epoch  43 Batch  358 / 525  Training Loss  3.279731026850641e-05\n",
            "Epoch  43 Batch  359 / 525  Training Loss  2.8367485356284305e-05\n",
            "Epoch  43 Batch  360 / 525  Training Loss  2.249003227916546e-05\n",
            "Epoch  43 Batch  361 / 525  Training Loss  1.4706214642501436e-05\n",
            "Epoch  43 Batch  362 / 525  Training Loss  1.753465403453447e-05\n",
            "Epoch  43 Batch  363 / 525  Training Loss  1.846750092227012e-05\n",
            "Epoch  43 Batch  364 / 525  Training Loss  1.8131535398424603e-05\n",
            "Epoch  43 Batch  365 / 525  Training Loss  2.0243251128704287e-05\n",
            "Epoch  43 Batch  366 / 525  Training Loss  9.388795660925098e-06\n",
            "Epoch  43 Batch  367 / 525  Training Loss  2.5027120500453748e-05\n",
            "Epoch  43 Batch  368 / 525  Training Loss  1.0677865247998852e-05\n",
            "Epoch  43 Batch  369 / 525  Training Loss  1.7564450899953954e-05\n",
            "Epoch  43 Batch  370 / 525  Training Loss  1.6716268874006346e-05\n",
            "Epoch  43 Batch  371 / 525  Training Loss  1.6255182345048524e-05\n",
            "Epoch  43 Batch  372 / 525  Training Loss  2.214578853454441e-05\n",
            "Epoch  43 Batch  373 / 525  Training Loss  1.4271809959609527e-05\n",
            "Epoch  43 Batch  374 / 525  Training Loss  1.3283136468089651e-05\n",
            "Epoch  43 Batch  375 / 525  Training Loss  1.886206700874027e-05\n",
            "Epoch  43 Batch  376 / 525  Training Loss  1.4078179447096772e-05\n",
            "Epoch  43 Batch  377 / 525  Training Loss  2.5245168217225e-05\n",
            "Epoch  43 Batch  378 / 525  Training Loss  2.569858406786807e-05\n",
            "Epoch  43 Batch  379 / 525  Training Loss  1.5623327271896414e-05\n",
            "Epoch  43 Batch  380 / 525  Training Loss  1.4685661881230772e-05\n",
            "Epoch  43 Batch  381 / 525  Training Loss  2.2150730728753842e-05\n",
            "Epoch  43 Batch  382 / 525  Training Loss  2.0816913092858158e-05\n",
            "Epoch  43 Batch  383 / 525  Training Loss  1.4501354598905891e-05\n",
            "Epoch  43 Batch  384 / 525  Training Loss  2.4888868210837245e-05\n",
            "Epoch  43 Batch  385 / 525  Training Loss  1.5515757695538923e-05\n",
            "Epoch  43 Batch  386 / 525  Training Loss  1.8737286154646426e-05\n",
            "Epoch  43 Batch  387 / 525  Training Loss  1.1331509995216038e-05\n",
            "Epoch  43 Batch  388 / 525  Training Loss  2.3196640540845692e-05\n",
            "Epoch  43 Batch  389 / 525  Training Loss  1.6658639651723206e-05\n",
            "Epoch  43 Batch  390 / 525  Training Loss  1.7181631847051904e-05\n",
            "Epoch  43 Batch  391 / 525  Training Loss  1.4614101019105874e-05\n",
            "Epoch  43 Batch  392 / 525  Training Loss  2.4871376808732748e-05\n",
            "Epoch  43 Batch  393 / 525  Training Loss  1.1878638360940386e-05\n",
            "Epoch  43 Batch  394 / 525  Training Loss  2.021921318373643e-05\n",
            "Epoch  43 Batch  395 / 525  Training Loss  1.8462122170603834e-05\n",
            "Epoch  43 Batch  396 / 525  Training Loss  1.9273502402938902e-05\n",
            "Epoch  43 Batch  397 / 525  Training Loss  2.737732938840054e-05\n",
            "Epoch  43 Batch  398 / 525  Training Loss  2.084019433823414e-05\n",
            "Epoch  43 Batch  399 / 525  Training Loss  1.7056883734767325e-05\n",
            "Epoch  43 Batch  400 / 525  Training Loss  2.244778625026811e-05\n",
            "Epoch  43 Batch  401 / 525  Training Loss  2.177677561121527e-05\n",
            "Epoch  43 Batch  402 / 525  Training Loss  1.3345660590857733e-05\n",
            "Epoch  43 Batch  403 / 525  Training Loss  2.8121177820139565e-05\n",
            "Epoch  43 Batch  404 / 525  Training Loss  1.7867003407445736e-05\n",
            "Epoch  43 Batch  405 / 525  Training Loss  2.1300844309735112e-05\n",
            "Epoch  43 Batch  406 / 525  Training Loss  2.4016164388740435e-05\n",
            "Epoch  43 Batch  407 / 525  Training Loss  1.6273301298497245e-05\n",
            "Epoch  43 Batch  408 / 525  Training Loss  1.3708023288927507e-05\n",
            "Epoch  43 Batch  409 / 525  Training Loss  1.565361708344426e-05\n",
            "Epoch  43 Batch  410 / 525  Training Loss  1.7047756045940332e-05\n",
            "Epoch  43 Batch  411 / 525  Training Loss  1.3897489225200843e-05\n",
            "Epoch  43 Batch  412 / 525  Training Loss  1.4000385817780625e-05\n",
            "Epoch  43 Batch  413 / 525  Training Loss  2.3169181076809764e-05\n",
            "Epoch  43 Batch  414 / 525  Training Loss  2.5246441509807482e-05\n",
            "Epoch  43 Batch  415 / 525  Training Loss  2.067456443910487e-05\n",
            "Epoch  43 Batch  416 / 525  Training Loss  1.2437735676940065e-05\n",
            "Epoch  43 Batch  417 / 525  Training Loss  2.260406472487375e-05\n",
            "Epoch  43 Batch  418 / 525  Training Loss  1.5128245649975725e-05\n",
            "Epoch  43 Batch  419 / 525  Training Loss  1.4720606486662291e-05\n",
            "Epoch  43 Batch  420 / 525  Training Loss  2.247878728667274e-05\n",
            "Epoch  43 Batch  421 / 525  Training Loss  2.596723243186716e-05\n",
            "Epoch  43 Batch  422 / 525  Training Loss  2.6404461095808074e-05\n",
            "Epoch  43 Batch  423 / 525  Training Loss  1.6273328583338298e-05\n",
            "Epoch  43 Batch  424 / 525  Training Loss  2.0471803509281017e-05\n",
            "Epoch  43 Batch  425 / 525  Training Loss  1.5152688320085872e-05\n",
            "Epoch  43 Batch  426 / 525  Training Loss  2.30398236453766e-05\n",
            "Epoch  43 Batch  427 / 525  Training Loss  2.3982815037015826e-05\n",
            "Epoch  43 Batch  428 / 525  Training Loss  2.0263709302525967e-05\n",
            "Epoch  43 Batch  429 / 525  Training Loss  1.9933338990085758e-05\n",
            "Epoch  43 Batch  430 / 525  Training Loss  1.5626024833181873e-05\n",
            "Epoch  43 Batch  431 / 525  Training Loss  1.237528158526402e-05\n",
            "Epoch  43 Batch  432 / 525  Training Loss  1.0429896974528674e-05\n",
            "Epoch  43 Batch  433 / 525  Training Loss  1.4917566659278236e-05\n",
            "Epoch  43 Batch  434 / 525  Training Loss  1.542100108054001e-05\n",
            "Epoch  43 Batch  435 / 525  Training Loss  1.7391317669535056e-05\n",
            "Epoch  43 Batch  436 / 525  Training Loss  1.1712555533449631e-05\n",
            "Epoch  43 Batch  437 / 525  Training Loss  2.1996900613885373e-05\n",
            "Epoch  43 Batch  438 / 525  Training Loss  2.3720547687844373e-05\n",
            "Epoch  43 Batch  439 / 525  Training Loss  9.222907465300523e-06\n",
            "Epoch  43 Batch  440 / 525  Training Loss  2.0099218090763316e-05\n",
            "Epoch  43 Batch  441 / 525  Training Loss  1.5229421478579752e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  43 Batch  442 / 525  Training Loss  1.8466427718522027e-05\n",
            "Epoch  43 Batch  443 / 525  Training Loss  1.485037228121655e-05\n",
            "Epoch  43 Batch  444 / 525  Training Loss  1.0872247003135271e-05\n",
            "Epoch  43 Batch  445 / 525  Training Loss  1.8957662177854218e-05\n",
            "Epoch  43 Batch  446 / 525  Training Loss  2.6772100682137534e-05\n",
            "Epoch  43 Batch  447 / 525  Training Loss  1.2484709259297233e-05\n",
            "Epoch  43 Batch  448 / 525  Training Loss  3.926027420675382e-05\n",
            "Epoch  43 Batch  449 / 525  Training Loss  1.3975955880596302e-05\n",
            "Epoch  43 Batch  450 / 525  Training Loss  1.3581522580352612e-05\n",
            "Epoch  43 Batch  451 / 525  Training Loss  2.133127418346703e-05\n",
            "Epoch  43 Batch  452 / 525  Training Loss  2.066998786176555e-05\n",
            "Epoch  43 Batch  453 / 525  Training Loss  1.710026481305249e-05\n",
            "Epoch  43 Batch  454 / 525  Training Loss  1.88793201232329e-05\n",
            "Epoch  43 Batch  455 / 525  Training Loss  1.3719586604565848e-05\n",
            "Epoch  43 Batch  456 / 525  Training Loss  1.6904592484934255e-05\n",
            "Epoch  43 Batch  457 / 525  Training Loss  1.1162834198330529e-05\n",
            "Epoch  43 Batch  458 / 525  Training Loss  8.876235369825736e-06\n",
            "Epoch  43 Batch  459 / 525  Training Loss  1.4236162314773537e-05\n",
            "Epoch  43 Batch  460 / 525  Training Loss  1.0957196536764968e-05\n",
            "Epoch  43 Batch  461 / 525  Training Loss  8.612234523752704e-06\n",
            "Epoch  43 Batch  462 / 525  Training Loss  1.844093276304193e-05\n",
            "Epoch  43 Batch  463 / 525  Training Loss  1.5787150914547965e-05\n",
            "Epoch  43 Batch  464 / 525  Training Loss  1.929886275320314e-05\n",
            "Epoch  43 Batch  465 / 525  Training Loss  2.391661109868437e-05\n",
            "Epoch  43 Batch  466 / 525  Training Loss  1.0307348929927684e-05\n",
            "Epoch  43 Batch  467 / 525  Training Loss  1.697083280305378e-05\n",
            "Epoch  43 Batch  468 / 525  Training Loss  1.6844791389303282e-05\n",
            "Epoch  43 Batch  469 / 525  Training Loss  2.28257522394415e-05\n",
            "Epoch  43 Batch  470 / 525  Training Loss  1.2032526683469769e-05\n",
            "Epoch  43 Batch  471 / 525  Training Loss  2.2834696210338734e-05\n",
            "Epoch  43 Batch  472 / 525  Training Loss  2.1619835024466738e-05\n",
            "Epoch  43 Batch  473 / 525  Training Loss  1.3277509424369782e-05\n",
            "Epoch  43 Batch  474 / 525  Training Loss  2.6397659894428216e-05\n",
            "Epoch  43 Batch  475 / 525  Training Loss  1.9390279703657143e-05\n",
            "Epoch  43 Batch  476 / 525  Training Loss  3.40225815307349e-05\n",
            "Epoch  43 Batch  477 / 525  Training Loss  1.557803261675872e-05\n",
            "Epoch  43 Batch  478 / 525  Training Loss  2.921969280578196e-05\n",
            "Epoch  43 Batch  479 / 525  Training Loss  2.234002204204444e-05\n",
            "Epoch  43 Batch  480 / 525  Training Loss  2.339605998713523e-05\n",
            "Epoch  43 Batch  481 / 525  Training Loss  1.5825975424377248e-05\n",
            "Epoch  43 Batch  482 / 525  Training Loss  1.1743037248379551e-05\n",
            "Epoch  43 Batch  483 / 525  Training Loss  1.1191071280336473e-05\n",
            "Epoch  43 Batch  484 / 525  Training Loss  1.3950954780739266e-05\n",
            "Epoch  43 Batch  485 / 525  Training Loss  1.7522379494039342e-05\n",
            "Epoch  43 Batch  486 / 525  Training Loss  2.309511364728678e-05\n",
            "Epoch  43 Batch  487 / 525  Training Loss  1.921336297527887e-05\n",
            "Epoch  43 Batch  488 / 525  Training Loss  1.719502870400902e-05\n",
            "Epoch  43 Batch  489 / 525  Training Loss  1.9870069081662223e-05\n",
            "Epoch  43 Batch  490 / 525  Training Loss  1.4585447388526518e-05\n",
            "Epoch  43 Batch  491 / 525  Training Loss  1.963152681128122e-05\n",
            "Epoch  43 Batch  492 / 525  Training Loss  2.808669160003774e-05\n",
            "Epoch  43 Batch  493 / 525  Training Loss  1.5806352166691795e-05\n",
            "Epoch  43 Batch  494 / 525  Training Loss  1.7634736650506966e-05\n",
            "Epoch  43 Batch  495 / 525  Training Loss  2.3267009964911267e-05\n",
            "Epoch  43 Batch  496 / 525  Training Loss  1.6154506738530472e-05\n",
            "Epoch  43 Batch  497 / 525  Training Loss  2.104595841956325e-05\n",
            "Epoch  43 Batch  498 / 525  Training Loss  2.294483783771284e-05\n",
            "Epoch  43 Batch  499 / 525  Training Loss  2.3796101231710054e-05\n",
            "Epoch  43 Batch  500 / 525  Training Loss  2.1422118152258918e-05\n",
            "Epoch  43 Batch  501 / 525  Training Loss  2.0239542209310457e-05\n",
            "Epoch  43 Batch  502 / 525  Training Loss  1.7667740394244902e-05\n",
            "Epoch  43 Batch  503 / 525  Training Loss  2.4439175831503235e-05\n",
            "Epoch  43 Batch  504 / 525  Training Loss  3.236279371776618e-05\n",
            "Epoch  43 Batch  505 / 525  Training Loss  2.0071362087037414e-05\n",
            "Epoch  43 Batch  506 / 525  Training Loss  1.563367186463438e-05\n",
            "Epoch  43 Batch  507 / 525  Training Loss  1.283392612094758e-05\n",
            "Epoch  43 Batch  508 / 525  Training Loss  1.931488986883778e-05\n",
            "Epoch  43 Batch  509 / 525  Training Loss  1.8225240637548268e-05\n",
            "Epoch  43 Batch  510 / 525  Training Loss  1.960235385922715e-05\n",
            "Epoch  43 Batch  511 / 525  Training Loss  1.4904449926689267e-05\n",
            "Epoch  43 Batch  512 / 525  Training Loss  1.839360993471928e-05\n",
            "Epoch  43 Batch  513 / 525  Training Loss  2.136631155735813e-05\n",
            "Epoch  43 Batch  514 / 525  Training Loss  1.5302124666050076e-05\n",
            "Epoch  43 Batch  515 / 525  Training Loss  2.0597110051312484e-05\n",
            "Epoch  43 Batch  516 / 525  Training Loss  1.569706182635855e-05\n",
            "Epoch  43 Batch  517 / 525  Training Loss  2.661116377566941e-05\n",
            "Epoch  43 Batch  518 / 525  Training Loss  1.80482747964561e-05\n",
            "Epoch  43 Batch  519 / 525  Training Loss  3.179377381457016e-05\n",
            "Epoch  43 Batch  520 / 525  Training Loss  1.6259584299405105e-05\n",
            "Epoch  43 Batch  521 / 525  Training Loss  1.6536096154595725e-05\n",
            "Epoch  43 Batch  522 / 525  Training Loss  1.130720011133235e-05\n",
            "Epoch  43 Batch  523 / 525  Training Loss  2.239358400402125e-05\n",
            "Epoch  43 Batch  524 / 525  Training Loss  1.2593486644618679e-05\n",
            "  44    |    -    |   0.000019   | 64.541667\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 44\n",
            "Epoch  44 Batch  0 / 525  Training Loss  2.3315598809858784e-05\n",
            "Epoch  44 Batch  1 / 525  Training Loss  1.2228170817252249e-05\n",
            "Epoch  44 Batch  2 / 525  Training Loss  1.9300534404464997e-05\n",
            "Epoch  44 Batch  3 / 525  Training Loss  1.3178010703995824e-05\n",
            "Epoch  44 Batch  4 / 525  Training Loss  1.1845829249068629e-05\n",
            "Epoch  44 Batch  5 / 525  Training Loss  1.524118306406308e-05\n",
            "Epoch  44 Batch  6 / 525  Training Loss  1.619867907720618e-05\n",
            "Epoch  44 Batch  7 / 525  Training Loss  1.945256553881336e-05\n",
            "Epoch  44 Batch  8 / 525  Training Loss  2.2677768356516026e-05\n",
            "Epoch  44 Batch  9 / 525  Training Loss  1.0080031643155962e-05\n",
            "Epoch  44 Batch  10 / 525  Training Loss  1.8173315766034648e-05\n",
            "Epoch  44 Batch  11 / 525  Training Loss  1.921558032336179e-05\n",
            "Epoch  44 Batch  12 / 525  Training Loss  1.2224535566929262e-05\n",
            "Epoch  44 Batch  13 / 525  Training Loss  1.099685141525697e-05\n",
            "Epoch  44 Batch  14 / 525  Training Loss  1.5268175047822297e-05\n",
            "Epoch  44 Batch  15 / 525  Training Loss  1.7562193534104154e-05\n",
            "Epoch  44 Batch  16 / 525  Training Loss  1.2180787962279283e-05\n",
            "Epoch  44 Batch  17 / 525  Training Loss  2.0263836631784216e-05\n",
            "Epoch  44 Batch  18 / 525  Training Loss  1.249551132787019e-05\n",
            "Epoch  44 Batch  19 / 525  Training Loss  2.895832949434407e-05\n",
            "Epoch  44 Batch  20 / 525  Training Loss  1.7453716282034293e-05\n",
            "Epoch  44 Batch  21 / 525  Training Loss  1.4254968846216798e-05\n",
            "Epoch  44 Batch  22 / 525  Training Loss  1.7931690308614634e-05\n",
            "Epoch  44 Batch  23 / 525  Training Loss  1.0328206371923443e-05\n",
            "Epoch  44 Batch  24 / 525  Training Loss  2.1455332898767665e-05\n",
            "Epoch  44 Batch  25 / 525  Training Loss  1.01207824627636e-05\n",
            "Epoch  44 Batch  26 / 525  Training Loss  2.0876181224593893e-05\n",
            "Epoch  44 Batch  27 / 525  Training Loss  2.1807765733683482e-05\n",
            "Epoch  44 Batch  28 / 525  Training Loss  1.5691792214056477e-05\n",
            "Epoch  44 Batch  29 / 525  Training Loss  1.4675194506708067e-05\n",
            "Epoch  44 Batch  30 / 525  Training Loss  9.189122465613764e-06\n",
            "Epoch  44 Batch  31 / 525  Training Loss  2.0125937226112e-05\n",
            "Epoch  44 Batch  32 / 525  Training Loss  7.27631868358003e-06\n",
            "Epoch  44 Batch  33 / 525  Training Loss  1.8033015294349752e-05\n",
            "Epoch  44 Batch  34 / 525  Training Loss  2.0100729670957662e-05\n",
            "Epoch  44 Batch  35 / 525  Training Loss  1.7002059394144453e-05\n",
            "Epoch  44 Batch  36 / 525  Training Loss  2.710742955969181e-05\n",
            "Epoch  44 Batch  37 / 525  Training Loss  2.6977626475854777e-05\n",
            "Epoch  44 Batch  38 / 525  Training Loss  2.56114435615018e-05\n",
            "Epoch  44 Batch  39 / 525  Training Loss  1.741376399877481e-05\n",
            "Epoch  44 Batch  40 / 525  Training Loss  1.298210827371804e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  44 Batch  41 / 525  Training Loss  1.5301853636628948e-05\n",
            "Epoch  44 Batch  42 / 525  Training Loss  1.506760781921912e-05\n",
            "Epoch  44 Batch  43 / 525  Training Loss  1.7454647604608908e-05\n",
            "Epoch  44 Batch  44 / 525  Training Loss  2.359613063163124e-05\n",
            "Epoch  44 Batch  45 / 525  Training Loss  2.0365094314911403e-05\n",
            "Epoch  44 Batch  46 / 525  Training Loss  1.523804348835256e-05\n",
            "Epoch  44 Batch  47 / 525  Training Loss  1.90021291928133e-05\n",
            "Epoch  44 Batch  48 / 525  Training Loss  1.9508777768351138e-05\n",
            "Epoch  44 Batch  49 / 525  Training Loss  2.4201804990298115e-05\n",
            "Epoch  44 Batch  50 / 525  Training Loss  1.1744800758606289e-05\n",
            "Epoch  44 Batch  51 / 525  Training Loss  2.2675678337691352e-05\n",
            "Epoch  44 Batch  52 / 525  Training Loss  1.4260777788877022e-05\n",
            "Epoch  44 Batch  53 / 525  Training Loss  1.581635297043249e-05\n",
            "Epoch  44 Batch  54 / 525  Training Loss  8.737903954170179e-06\n",
            "Epoch  44 Batch  55 / 525  Training Loss  1.6717935068299994e-05\n",
            "Epoch  44 Batch  56 / 525  Training Loss  2.041733023361303e-05\n",
            "Epoch  44 Batch  57 / 525  Training Loss  1.1174775863764808e-05\n",
            "Epoch  44 Batch  58 / 525  Training Loss  2.6547995730652474e-05\n",
            "Epoch  44 Batch  59 / 525  Training Loss  1.287844770558877e-05\n",
            "Epoch  44 Batch  60 / 525  Training Loss  2.2364953110809438e-05\n",
            "Epoch  44 Batch  61 / 525  Training Loss  1.1559999620658346e-05\n",
            "Epoch  44 Batch  62 / 525  Training Loss  1.3372894500207622e-05\n",
            "Epoch  44 Batch  63 / 525  Training Loss  9.903177669912111e-06\n",
            "Epoch  44 Batch  64 / 525  Training Loss  1.7875383491627872e-05\n",
            "Epoch  44 Batch  65 / 525  Training Loss  1.3388851584750228e-05\n",
            "Epoch  44 Batch  66 / 525  Training Loss  1.5109911146282684e-05\n",
            "Epoch  44 Batch  67 / 525  Training Loss  1.4494939932774287e-05\n",
            "Epoch  44 Batch  68 / 525  Training Loss  1.0818588634720072e-05\n",
            "Epoch  44 Batch  69 / 525  Training Loss  1.5731398889329284e-05\n",
            "Epoch  44 Batch  70 / 525  Training Loss  1.481381150369998e-05\n",
            "Epoch  44 Batch  71 / 525  Training Loss  2.4291573936352506e-05\n",
            "Epoch  44 Batch  72 / 525  Training Loss  1.876704118330963e-05\n",
            "Epoch  44 Batch  73 / 525  Training Loss  2.4416589440079406e-05\n",
            "Epoch  44 Batch  74 / 525  Training Loss  1.75605764525244e-05\n",
            "Epoch  44 Batch  75 / 525  Training Loss  1.4523352547257673e-05\n",
            "Epoch  44 Batch  76 / 525  Training Loss  1.4563223885488696e-05\n",
            "Epoch  44 Batch  77 / 525  Training Loss  1.5041847291286103e-05\n",
            "Epoch  44 Batch  78 / 525  Training Loss  1.563933619763702e-05\n",
            "Epoch  44 Batch  79 / 525  Training Loss  1.3232655874162447e-05\n",
            "Epoch  44 Batch  80 / 525  Training Loss  2.0444316760404035e-05\n",
            "Epoch  44 Batch  81 / 525  Training Loss  2.5461840778007172e-05\n",
            "Epoch  44 Batch  82 / 525  Training Loss  1.2315130334172864e-05\n",
            "Epoch  44 Batch  83 / 525  Training Loss  1.2994011740374845e-05\n",
            "Epoch  44 Batch  84 / 525  Training Loss  1.5840809282963164e-05\n",
            "Epoch  44 Batch  85 / 525  Training Loss  1.5121729120437521e-05\n",
            "Epoch  44 Batch  86 / 525  Training Loss  7.965803888509981e-06\n",
            "Epoch  44 Batch  87 / 525  Training Loss  1.9606213754741475e-05\n",
            "Epoch  44 Batch  88 / 525  Training Loss  1.8469601855031215e-05\n",
            "Epoch  44 Batch  89 / 525  Training Loss  1.1823153727164026e-05\n",
            "Epoch  44 Batch  90 / 525  Training Loss  1.4568007827620022e-05\n",
            "Epoch  44 Batch  91 / 525  Training Loss  1.8996859580511227e-05\n",
            "Epoch  44 Batch  92 / 525  Training Loss  2.427759136480745e-05\n",
            "Epoch  44 Batch  93 / 525  Training Loss  1.4451024981099181e-05\n",
            "Epoch  44 Batch  94 / 525  Training Loss  1.6403897461714223e-05\n",
            "Epoch  44 Batch  95 / 525  Training Loss  1.3956710063212086e-05\n",
            "Epoch  44 Batch  96 / 525  Training Loss  1.9355238691787235e-05\n",
            "Epoch  44 Batch  97 / 525  Training Loss  1.893047010526061e-05\n",
            "Epoch  44 Batch  98 / 525  Training Loss  9.917506758938543e-06\n",
            "Epoch  44 Batch  99 / 525  Training Loss  1.581153082952369e-05\n",
            "Epoch  44 Batch  100 / 525  Training Loss  2.4410957848886028e-05\n",
            "Epoch  44 Batch  101 / 525  Training Loss  2.513142862881068e-05\n",
            "Epoch  44 Batch  102 / 525  Training Loss  2.3107739252736792e-05\n",
            "Epoch  44 Batch  103 / 525  Training Loss  1.5708043065387756e-05\n",
            "Epoch  44 Batch  104 / 525  Training Loss  2.4360051611438394e-05\n",
            "Epoch  44 Batch  105 / 525  Training Loss  1.745876215863973e-05\n",
            "Epoch  44 Batch  106 / 525  Training Loss  1.8008937331615016e-05\n",
            "Epoch  44 Batch  107 / 525  Training Loss  2.503869473002851e-05\n",
            "Epoch  44 Batch  108 / 525  Training Loss  2.3317126760957763e-05\n",
            "Epoch  44 Batch  109 / 525  Training Loss  1.9581160813686438e-05\n",
            "Epoch  44 Batch  110 / 525  Training Loss  1.3972234228276648e-05\n",
            "Epoch  44 Batch  111 / 525  Training Loss  2.0932518964400515e-05\n",
            "Epoch  44 Batch  112 / 525  Training Loss  1.5056504707899876e-05\n",
            "Epoch  44 Batch  113 / 525  Training Loss  1.2257643902557902e-05\n",
            "Epoch  44 Batch  114 / 525  Training Loss  1.2279051588848233e-05\n",
            "Epoch  44 Batch  115 / 525  Training Loss  1.7583815861144103e-05\n",
            "Epoch  44 Batch  116 / 525  Training Loss  1.2396516467561014e-05\n",
            "Epoch  44 Batch  117 / 525  Training Loss  1.9159004295943305e-05\n",
            "Epoch  44 Batch  118 / 525  Training Loss  2.5717357857502066e-05\n",
            "Epoch  44 Batch  119 / 525  Training Loss  1.6784590116003528e-05\n",
            "Epoch  44 Batch  120 / 525  Training Loss  1.2076688108209055e-05\n",
            "Epoch  44 Batch  121 / 525  Training Loss  1.0667625247151591e-05\n",
            "Epoch  44 Batch  122 / 525  Training Loss  9.971347935788799e-06\n",
            "Epoch  44 Batch  123 / 525  Training Loss  2.0307583326939493e-05\n",
            "Epoch  44 Batch  124 / 525  Training Loss  1.45082331073354e-05\n",
            "Epoch  44 Batch  125 / 525  Training Loss  1.4807199477218091e-05\n",
            "Epoch  44 Batch  126 / 525  Training Loss  1.984661867027171e-05\n",
            "Epoch  44 Batch  127 / 525  Training Loss  9.363476237922441e-06\n",
            "Epoch  44 Batch  128 / 525  Training Loss  2.3098125893739052e-05\n",
            "Epoch  44 Batch  129 / 525  Training Loss  1.823011007218156e-05\n",
            "Epoch  44 Batch  130 / 525  Training Loss  1.196972698380705e-05\n",
            "Epoch  44 Batch  131 / 525  Training Loss  1.8806091247824952e-05\n",
            "Epoch  44 Batch  132 / 525  Training Loss  1.4164676031214185e-05\n",
            "Epoch  44 Batch  133 / 525  Training Loss  1.434077239537146e-05\n",
            "Epoch  44 Batch  134 / 525  Training Loss  1.7209227735293098e-05\n",
            "Epoch  44 Batch  135 / 525  Training Loss  1.4632435522798914e-05\n",
            "Epoch  44 Batch  136 / 525  Training Loss  1.3429033060674556e-05\n",
            "Epoch  44 Batch  137 / 525  Training Loss  1.2744219930027612e-05\n",
            "Epoch  44 Batch  138 / 525  Training Loss  1.9665936633828096e-05\n",
            "Epoch  44 Batch  139 / 525  Training Loss  1.5787161828484386e-05\n",
            "Epoch  44 Batch  140 / 525  Training Loss  1.1006784916389734e-05\n",
            "Epoch  44 Batch  141 / 525  Training Loss  1.7836448023444973e-05\n",
            "Epoch  44 Batch  142 / 525  Training Loss  1.2153928764746524e-05\n",
            "Epoch  44 Batch  143 / 525  Training Loss  1.3612132534035482e-05\n",
            "Epoch  44 Batch  144 / 525  Training Loss  1.8481950974091887e-05\n",
            "Epoch  44 Batch  145 / 525  Training Loss  1.5154168067965657e-05\n",
            "Epoch  44 Batch  146 / 525  Training Loss  1.9709894331754185e-05\n",
            "Epoch  44 Batch  147 / 525  Training Loss  2.299649167980533e-05\n",
            "Epoch  44 Batch  148 / 525  Training Loss  1.833200076362118e-05\n",
            "Epoch  44 Batch  149 / 525  Training Loss  2.2175849153427407e-05\n",
            "Epoch  44 Batch  150 / 525  Training Loss  2.14076426345855e-05\n",
            "Epoch  44 Batch  151 / 525  Training Loss  1.0189869499299675e-05\n",
            "Epoch  44 Batch  152 / 525  Training Loss  2.5024000933626667e-05\n",
            "Epoch  44 Batch  153 / 525  Training Loss  2.9176866519264877e-05\n",
            "Epoch  44 Batch  154 / 525  Training Loss  1.5526971765211783e-05\n",
            "Epoch  44 Batch  155 / 525  Training Loss  1.6465804947074503e-05\n",
            "Epoch  44 Batch  156 / 525  Training Loss  1.5549507224932313e-05\n",
            "Epoch  44 Batch  157 / 525  Training Loss  3.593055680539692e-06\n",
            "Epoch  44 Batch  158 / 525  Training Loss  1.7882888641906902e-05\n",
            "Epoch  44 Batch  159 / 525  Training Loss  1.9224753486923873e-05\n",
            "Epoch  44 Batch  160 / 525  Training Loss  1.5230008102662396e-05\n",
            "Epoch  44 Batch  161 / 525  Training Loss  1.1008754881913774e-05\n",
            "Epoch  44 Batch  162 / 525  Training Loss  1.4244072190194856e-05\n",
            "Epoch  44 Batch  163 / 525  Training Loss  9.860057616606355e-06\n",
            "Epoch  44 Batch  164 / 525  Training Loss  1.9299264749861322e-05\n",
            "Epoch  44 Batch  165 / 525  Training Loss  1.798744051484391e-05\n",
            "Epoch  44 Batch  166 / 525  Training Loss  1.1608555723796599e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  44 Batch  167 / 525  Training Loss  1.552455432829447e-05\n",
            "Epoch  44 Batch  168 / 525  Training Loss  1.9518429326126352e-05\n",
            "Epoch  44 Batch  169 / 525  Training Loss  1.8779745005303994e-05\n",
            "Epoch  44 Batch  170 / 525  Training Loss  9.993857929657679e-06\n",
            "Epoch  44 Batch  171 / 525  Training Loss  1.222919217980234e-05\n",
            "Epoch  44 Batch  172 / 525  Training Loss  2.457783011777792e-05\n",
            "Epoch  44 Batch  173 / 525  Training Loss  2.1543168259086087e-05\n",
            "Epoch  44 Batch  174 / 525  Training Loss  2.8752509024343453e-05\n",
            "Epoch  44 Batch  175 / 525  Training Loss  1.5184685253188945e-05\n",
            "Epoch  44 Batch  176 / 525  Training Loss  2.5142930098809302e-05\n",
            "Epoch  44 Batch  177 / 525  Training Loss  2.04072093765717e-05\n",
            "Epoch  44 Batch  178 / 525  Training Loss  1.4330854355648626e-05\n",
            "Epoch  44 Batch  179 / 525  Training Loss  1.041533596435329e-05\n",
            "Epoch  44 Batch  180 / 525  Training Loss  1.4806641956965905e-05\n",
            "Epoch  44 Batch  181 / 525  Training Loss  2.36276082432596e-05\n",
            "Epoch  44 Batch  182 / 525  Training Loss  1.0901428140641656e-05\n",
            "Epoch  44 Batch  183 / 525  Training Loss  2.378836325078737e-05\n",
            "Epoch  44 Batch  184 / 525  Training Loss  1.5458823327207938e-05\n",
            "Epoch  44 Batch  185 / 525  Training Loss  2.4718505301279947e-05\n",
            "Epoch  44 Batch  186 / 525  Training Loss  2.0014480469399132e-05\n",
            "Epoch  44 Batch  187 / 525  Training Loss  1.35332957142964e-05\n",
            "Epoch  44 Batch  188 / 525  Training Loss  1.6381069144699723e-05\n",
            "Epoch  44 Batch  189 / 525  Training Loss  2.39284254348604e-05\n",
            "Epoch  44 Batch  190 / 525  Training Loss  1.7416310583939776e-05\n",
            "Epoch  44 Batch  191 / 525  Training Loss  1.4822895536781289e-05\n",
            "Epoch  44 Batch  192 / 525  Training Loss  1.4648144315287936e-05\n",
            "Epoch  44 Batch  193 / 525  Training Loss  1.7408416169928387e-05\n",
            "Epoch  44 Batch  194 / 525  Training Loss  2.5081497369683348e-05\n",
            "Epoch  44 Batch  195 / 525  Training Loss  1.9629896996775642e-05\n",
            "Epoch  44 Batch  196 / 525  Training Loss  1.76187495526392e-05\n",
            "Epoch  44 Batch  197 / 525  Training Loss  1.147134662460303e-05\n",
            "Epoch  44 Batch  198 / 525  Training Loss  2.0427691197255626e-05\n",
            "Epoch  44 Batch  199 / 525  Training Loss  1.1929058018722571e-05\n",
            "Epoch  44 Batch  200 / 525  Training Loss  1.684152812231332e-05\n",
            "Epoch  44 Batch  201 / 525  Training Loss  1.0072128134197555e-05\n",
            "Epoch  44 Batch  202 / 525  Training Loss  2.7086774935014546e-05\n",
            "Epoch  44 Batch  203 / 525  Training Loss  1.436181082681287e-05\n",
            "Epoch  44 Batch  204 / 525  Training Loss  1.5295659977709875e-05\n",
            "Epoch  44 Batch  205 / 525  Training Loss  1.3047379979980178e-05\n",
            "Epoch  44 Batch  206 / 525  Training Loss  1.709318530629389e-05\n",
            "Epoch  44 Batch  207 / 525  Training Loss  1.1618511052802205e-05\n",
            "Epoch  44 Batch  208 / 525  Training Loss  1.017898648569826e-05\n",
            "Epoch  44 Batch  209 / 525  Training Loss  1.7649274013820104e-05\n",
            "Epoch  44 Batch  210 / 525  Training Loss  1.4265278878156096e-05\n",
            "Epoch  44 Batch  211 / 525  Training Loss  1.0208078492723871e-05\n",
            "Epoch  44 Batch  212 / 525  Training Loss  1.59393730427837e-05\n",
            "Epoch  44 Batch  213 / 525  Training Loss  1.2146258086431772e-05\n",
            "Epoch  44 Batch  214 / 525  Training Loss  1.815302312024869e-05\n",
            "Epoch  44 Batch  215 / 525  Training Loss  1.726209302432835e-05\n",
            "Epoch  44 Batch  216 / 525  Training Loss  1.3424554708763026e-05\n",
            "Epoch  44 Batch  217 / 525  Training Loss  2.397707248746883e-05\n",
            "Epoch  44 Batch  218 / 525  Training Loss  1.3550755284086335e-05\n",
            "Epoch  44 Batch  219 / 525  Training Loss  2.3369715563603677e-05\n",
            "Epoch  44 Batch  220 / 525  Training Loss  8.222343240049668e-06\n",
            "Epoch  44 Batch  221 / 525  Training Loss  1.8883965822169557e-05\n",
            "Epoch  44 Batch  222 / 525  Training Loss  1.246554711542558e-05\n",
            "Epoch  44 Batch  223 / 525  Training Loss  1.8907152480096556e-05\n",
            "Epoch  44 Batch  224 / 525  Training Loss  2.849331394827459e-05\n",
            "Epoch  44 Batch  225 / 525  Training Loss  2.1524499970837496e-05\n",
            "Epoch  44 Batch  226 / 525  Training Loss  2.5219622330041602e-05\n",
            "Epoch  44 Batch  227 / 525  Training Loss  1.2677814993367065e-05\n",
            "Epoch  44 Batch  228 / 525  Training Loss  2.7473131922306493e-05\n",
            "Epoch  44 Batch  229 / 525  Training Loss  1.4450628441409208e-05\n",
            "Epoch  44 Batch  230 / 525  Training Loss  1.2696827070612926e-05\n",
            "Epoch  44 Batch  231 / 525  Training Loss  1.1429150617914274e-05\n",
            "Epoch  44 Batch  232 / 525  Training Loss  1.5835223166504875e-05\n",
            "Epoch  44 Batch  233 / 525  Training Loss  1.573426925460808e-05\n",
            "Epoch  44 Batch  234 / 525  Training Loss  1.5129900930332951e-05\n",
            "Epoch  44 Batch  235 / 525  Training Loss  1.7503227354609407e-05\n",
            "Epoch  44 Batch  236 / 525  Training Loss  1.3325945474207401e-05\n",
            "Epoch  44 Batch  237 / 525  Training Loss  1.8076892956742086e-05\n",
            "Epoch  44 Batch  238 / 525  Training Loss  1.67956568475347e-05\n",
            "Epoch  44 Batch  239 / 525  Training Loss  2.0611239960999228e-05\n",
            "Epoch  44 Batch  240 / 525  Training Loss  1.1334070222801529e-05\n",
            "Epoch  44 Batch  241 / 525  Training Loss  1.0642453162290622e-05\n",
            "Epoch  44 Batch  242 / 525  Training Loss  1.2909974429931026e-05\n",
            "Epoch  44 Batch  243 / 525  Training Loss  1.0151043170480989e-05\n",
            "Epoch  44 Batch  244 / 525  Training Loss  1.4617880879086442e-05\n",
            "Epoch  44 Batch  245 / 525  Training Loss  1.1151663784403354e-05\n",
            "Epoch  44 Batch  246 / 525  Training Loss  1.1089849976997357e-05\n",
            "Epoch  44 Batch  247 / 525  Training Loss  2.0335810404503718e-05\n",
            "Epoch  44 Batch  248 / 525  Training Loss  1.220900139742298e-05\n",
            "Epoch  44 Batch  249 / 525  Training Loss  1.851857996371109e-05\n",
            "Epoch  44 Batch  250 / 525  Training Loss  1.3106150618114043e-05\n",
            "Epoch  44 Batch  251 / 525  Training Loss  1.4498284144792706e-05\n",
            "Epoch  44 Batch  252 / 525  Training Loss  1.6408970623160712e-05\n",
            "Epoch  44 Batch  253 / 525  Training Loss  2.2953314328333363e-05\n",
            "Epoch  44 Batch  254 / 525  Training Loss  8.570692443754524e-06\n",
            "Epoch  44 Batch  255 / 525  Training Loss  2.2774958779336885e-05\n",
            "Epoch  44 Batch  256 / 525  Training Loss  1.6008747479645535e-05\n",
            "Epoch  44 Batch  257 / 525  Training Loss  1.5856907339184545e-05\n",
            "Epoch  44 Batch  258 / 525  Training Loss  9.628502994019073e-06\n",
            "Epoch  44 Batch  259 / 525  Training Loss  1.800350219127722e-05\n",
            "Epoch  44 Batch  260 / 525  Training Loss  1.650848935241811e-05\n",
            "Epoch  44 Batch  261 / 525  Training Loss  1.946496922755614e-05\n",
            "Epoch  44 Batch  262 / 525  Training Loss  1.3448724530462641e-05\n",
            "Epoch  44 Batch  263 / 525  Training Loss  2.733740802796092e-05\n",
            "Epoch  44 Batch  264 / 525  Training Loss  1.7440170267946087e-05\n",
            "Epoch  44 Batch  265 / 525  Training Loss  2.0649815269280225e-05\n",
            "Epoch  44 Batch  266 / 525  Training Loss  1.9588911527534947e-05\n",
            "Epoch  44 Batch  267 / 525  Training Loss  1.2909555152873509e-05\n",
            "Epoch  44 Batch  268 / 525  Training Loss  1.8075737898470834e-05\n",
            "Epoch  44 Batch  269 / 525  Training Loss  1.1724717296601739e-05\n",
            "Epoch  44 Batch  270 / 525  Training Loss  2.3001450244919397e-05\n",
            "Epoch  44 Batch  271 / 525  Training Loss  2.120276440109592e-05\n",
            "Epoch  44 Batch  272 / 525  Training Loss  1.3820454114465974e-05\n",
            "Epoch  44 Batch  273 / 525  Training Loss  1.3545521142077632e-05\n",
            "Epoch  44 Batch  274 / 525  Training Loss  1.6436697478638962e-05\n",
            "Epoch  44 Batch  275 / 525  Training Loss  1.3084170859656297e-05\n",
            "Epoch  44 Batch  276 / 525  Training Loss  1.821549994929228e-05\n",
            "Epoch  44 Batch  277 / 525  Training Loss  1.539228469482623e-05\n",
            "Epoch  44 Batch  278 / 525  Training Loss  1.7778806068236008e-05\n",
            "Epoch  44 Batch  279 / 525  Training Loss  2.608182876429055e-05\n",
            "Epoch  44 Batch  280 / 525  Training Loss  1.4319879483082332e-05\n",
            "Epoch  44 Batch  281 / 525  Training Loss  1.459546729165595e-05\n",
            "Epoch  44 Batch  282 / 525  Training Loss  9.711722668725997e-06\n",
            "Epoch  44 Batch  283 / 525  Training Loss  1.4271012332756072e-05\n",
            "Epoch  44 Batch  284 / 525  Training Loss  1.367394725093618e-05\n",
            "Epoch  44 Batch  285 / 525  Training Loss  1.6481852071592584e-05\n",
            "Epoch  44 Batch  286 / 525  Training Loss  1.3825733731209766e-05\n",
            "Epoch  44 Batch  287 / 525  Training Loss  2.171423147956375e-05\n",
            "Epoch  44 Batch  288 / 525  Training Loss  1.4576432477042545e-05\n",
            "Epoch  44 Batch  289 / 525  Training Loss  1.4464270861935802e-05\n",
            "Epoch  44 Batch  290 / 525  Training Loss  1.694239108473994e-05\n",
            "Epoch  44 Batch  291 / 525  Training Loss  1.6301193682011217e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  44 Batch  292 / 525  Training Loss  1.7947604646906257e-05\n",
            "Epoch  44 Batch  293 / 525  Training Loss  2.0630639482988045e-05\n",
            "Epoch  44 Batch  294 / 525  Training Loss  1.792093098629266e-05\n",
            "Epoch  44 Batch  295 / 525  Training Loss  2.3960488761076704e-05\n",
            "Epoch  44 Batch  296 / 525  Training Loss  1.2635447092179675e-05\n",
            "Epoch  44 Batch  297 / 525  Training Loss  1.2412741853040643e-05\n",
            "Epoch  44 Batch  298 / 525  Training Loss  1.4140050552668981e-05\n",
            "Epoch  44 Batch  299 / 525  Training Loss  1.420908847649116e-05\n",
            "Epoch  44 Batch  300 / 525  Training Loss  1.2215326933073811e-05\n",
            "Epoch  44 Batch  301 / 525  Training Loss  1.2512425200839061e-05\n",
            "Epoch  44 Batch  302 / 525  Training Loss  1.510004949523136e-05\n",
            "Epoch  44 Batch  303 / 525  Training Loss  1.2420340681273956e-05\n",
            "Epoch  44 Batch  304 / 525  Training Loss  1.7334519725409336e-05\n",
            "Epoch  44 Batch  305 / 525  Training Loss  1.530425106466282e-05\n",
            "Epoch  44 Batch  306 / 525  Training Loss  1.8425269445287995e-05\n",
            "Epoch  44 Batch  307 / 525  Training Loss  2.581739317975007e-05\n",
            "Epoch  44 Batch  308 / 525  Training Loss  1.1429367077653296e-05\n",
            "Epoch  44 Batch  309 / 525  Training Loss  3.085324351559393e-05\n",
            "Epoch  44 Batch  310 / 525  Training Loss  1.2881437214673497e-05\n",
            "Epoch  44 Batch  311 / 525  Training Loss  1.428854011464864e-05\n",
            "Epoch  44 Batch  312 / 525  Training Loss  1.9379429431864992e-05\n",
            "Epoch  44 Batch  313 / 525  Training Loss  1.51015574374469e-05\n",
            "Epoch  44 Batch  314 / 525  Training Loss  1.8522116079111584e-05\n",
            "Epoch  44 Batch  315 / 525  Training Loss  1.1781742614402901e-05\n",
            "Epoch  44 Batch  316 / 525  Training Loss  2.63814927166095e-05\n",
            "Epoch  44 Batch  317 / 525  Training Loss  1.327631343883695e-05\n",
            "Epoch  44 Batch  318 / 525  Training Loss  1.9374936528038234e-05\n",
            "Epoch  44 Batch  319 / 525  Training Loss  2.1883575755055062e-05\n",
            "Epoch  44 Batch  320 / 525  Training Loss  2.2393662220565602e-05\n",
            "Epoch  44 Batch  321 / 525  Training Loss  2.0504146959865466e-05\n",
            "Epoch  44 Batch  322 / 525  Training Loss  2.5948433176381513e-05\n",
            "Epoch  44 Batch  323 / 525  Training Loss  2.220055102952756e-05\n",
            "Epoch  44 Batch  324 / 525  Training Loss  2.442886216158513e-05\n",
            "Epoch  44 Batch  325 / 525  Training Loss  1.9613109543570317e-05\n",
            "Epoch  44 Batch  326 / 525  Training Loss  1.1384163371985778e-05\n",
            "Epoch  44 Batch  327 / 525  Training Loss  1.6249749023700133e-05\n",
            "Epoch  44 Batch  328 / 525  Training Loss  1.7481361282989383e-05\n",
            "Epoch  44 Batch  329 / 525  Training Loss  7.704555173404515e-06\n",
            "Epoch  44 Batch  330 / 525  Training Loss  1.1638996511464939e-05\n",
            "Epoch  44 Batch  331 / 525  Training Loss  1.7212176317116246e-05\n",
            "Epoch  44 Batch  332 / 525  Training Loss  1.4182369341142476e-05\n",
            "Epoch  44 Batch  333 / 525  Training Loss  1.7333313735434785e-05\n",
            "Epoch  44 Batch  334 / 525  Training Loss  1.1399371942388825e-05\n",
            "Epoch  44 Batch  335 / 525  Training Loss  1.0524250683374703e-05\n",
            "Epoch  44 Batch  336 / 525  Training Loss  1.4007579011376947e-05\n",
            "Epoch  44 Batch  337 / 525  Training Loss  1.921428702189587e-05\n",
            "Epoch  44 Batch  338 / 525  Training Loss  1.2728068213618826e-05\n",
            "Epoch  44 Batch  339 / 525  Training Loss  2.3602842702530324e-05\n",
            "Epoch  44 Batch  340 / 525  Training Loss  1.8628137695486657e-05\n",
            "Epoch  44 Batch  341 / 525  Training Loss  1.6416046491940506e-05\n",
            "Epoch  44 Batch  342 / 525  Training Loss  2.2553356757271104e-05\n",
            "Epoch  44 Batch  343 / 525  Training Loss  2.3504835553467274e-05\n",
            "Epoch  44 Batch  344 / 525  Training Loss  1.3069096894469112e-05\n",
            "Epoch  44 Batch  345 / 525  Training Loss  1.7012012904160656e-05\n",
            "Epoch  44 Batch  346 / 525  Training Loss  2.312952528882306e-05\n",
            "Epoch  44 Batch  347 / 525  Training Loss  8.685141438036226e-06\n",
            "Epoch  44 Batch  348 / 525  Training Loss  1.822259400796611e-05\n",
            "Epoch  44 Batch  349 / 525  Training Loss  1.0074997589981649e-05\n",
            "Epoch  44 Batch  350 / 525  Training Loss  1.2576352673931979e-05\n",
            "Epoch  44 Batch  351 / 525  Training Loss  2.4890812710509636e-05\n",
            "Epoch  44 Batch  352 / 525  Training Loss  1.3303104424267076e-05\n",
            "Epoch  44 Batch  353 / 525  Training Loss  9.430767931917217e-06\n",
            "Epoch  44 Batch  354 / 525  Training Loss  1.6191555914701894e-05\n",
            "Epoch  44 Batch  355 / 525  Training Loss  1.0956857295241207e-05\n",
            "Epoch  44 Batch  356 / 525  Training Loss  1.968993092305027e-05\n",
            "Epoch  44 Batch  357 / 525  Training Loss  1.78510817931965e-05\n",
            "Epoch  44 Batch  358 / 525  Training Loss  1.9505789168761112e-05\n",
            "Epoch  44 Batch  359 / 525  Training Loss  1.7860904335975647e-05\n",
            "Epoch  44 Batch  360 / 525  Training Loss  1.4538380128215067e-05\n",
            "Epoch  44 Batch  361 / 525  Training Loss  1.6663747373968363e-05\n",
            "Epoch  44 Batch  362 / 525  Training Loss  1.207262266689213e-05\n",
            "Epoch  44 Batch  363 / 525  Training Loss  1.7679458323982544e-05\n",
            "Epoch  44 Batch  364 / 525  Training Loss  3.188328264513984e-05\n",
            "Epoch  44 Batch  365 / 525  Training Loss  2.4234093871200457e-05\n",
            "Epoch  44 Batch  366 / 525  Training Loss  2.180912451876793e-05\n",
            "Epoch  44 Batch  367 / 525  Training Loss  1.5764615454827435e-05\n",
            "Epoch  44 Batch  368 / 525  Training Loss  1.1820910003734753e-05\n",
            "Epoch  44 Batch  369 / 525  Training Loss  1.9180924937245436e-05\n",
            "Epoch  44 Batch  370 / 525  Training Loss  1.5969924788805656e-05\n",
            "Epoch  44 Batch  371 / 525  Training Loss  1.4039173038327135e-05\n",
            "Epoch  44 Batch  372 / 525  Training Loss  1.2114973287680186e-05\n",
            "Epoch  44 Batch  373 / 525  Training Loss  1.9829001757898368e-05\n",
            "Epoch  44 Batch  374 / 525  Training Loss  1.850887383625377e-05\n",
            "Epoch  44 Batch  375 / 525  Training Loss  1.5914356481516734e-05\n",
            "Epoch  44 Batch  376 / 525  Training Loss  1.9422717741690576e-05\n",
            "Epoch  44 Batch  377 / 525  Training Loss  1.573076042404864e-05\n",
            "Epoch  44 Batch  378 / 525  Training Loss  1.7151107385871e-05\n",
            "Epoch  44 Batch  379 / 525  Training Loss  1.7471005776314996e-05\n",
            "Epoch  44 Batch  380 / 525  Training Loss  1.705238719296176e-05\n",
            "Epoch  44 Batch  381 / 525  Training Loss  2.3090802642400376e-05\n",
            "Epoch  44 Batch  382 / 525  Training Loss  1.7595282770344056e-05\n",
            "Epoch  44 Batch  383 / 525  Training Loss  1.3149734513717704e-05\n",
            "Epoch  44 Batch  384 / 525  Training Loss  6.620967269554967e-06\n",
            "Epoch  44 Batch  385 / 525  Training Loss  1.6162757674464956e-05\n",
            "Epoch  44 Batch  386 / 525  Training Loss  1.3209910321165808e-05\n",
            "Epoch  44 Batch  387 / 525  Training Loss  1.2520196833065711e-05\n",
            "Epoch  44 Batch  388 / 525  Training Loss  1.9992008674307726e-05\n",
            "Epoch  44 Batch  389 / 525  Training Loss  1.552608227939345e-05\n",
            "Epoch  44 Batch  390 / 525  Training Loss  1.357180553895887e-05\n",
            "Epoch  44 Batch  391 / 525  Training Loss  1.584952769917436e-05\n",
            "Epoch  44 Batch  392 / 525  Training Loss  1.802169208531268e-05\n",
            "Epoch  44 Batch  393 / 525  Training Loss  1.5421654097735882e-05\n",
            "Epoch  44 Batch  394 / 525  Training Loss  1.407781928719487e-05\n",
            "Epoch  44 Batch  395 / 525  Training Loss  1.161769796453882e-05\n",
            "Epoch  44 Batch  396 / 525  Training Loss  1.4807330444455147e-05\n",
            "Epoch  44 Batch  397 / 525  Training Loss  1.4125801499176305e-05\n",
            "Epoch  44 Batch  398 / 525  Training Loss  1.5471281585632823e-05\n",
            "Epoch  44 Batch  399 / 525  Training Loss  1.5294444892788306e-05\n",
            "Epoch  44 Batch  400 / 525  Training Loss  1.769468144630082e-05\n",
            "Epoch  44 Batch  401 / 525  Training Loss  1.814081406337209e-05\n",
            "Epoch  44 Batch  402 / 525  Training Loss  1.0570769518380985e-05\n",
            "Epoch  44 Batch  403 / 525  Training Loss  2.4110477170324884e-05\n",
            "Epoch  44 Batch  404 / 525  Training Loss  1.7125292288255878e-05\n",
            "Epoch  44 Batch  405 / 525  Training Loss  1.330607665295247e-05\n",
            "Epoch  44 Batch  406 / 525  Training Loss  1.4558868315361906e-05\n",
            "Epoch  44 Batch  407 / 525  Training Loss  9.892769412545022e-06\n",
            "Epoch  44 Batch  408 / 525  Training Loss  1.5750283637316898e-05\n",
            "Epoch  44 Batch  409 / 525  Training Loss  1.3755387044511735e-05\n",
            "Epoch  44 Batch  410 / 525  Training Loss  1.3031220078119077e-05\n",
            "Epoch  44 Batch  411 / 525  Training Loss  1.230470479640644e-05\n",
            "Epoch  44 Batch  412 / 525  Training Loss  1.686246832832694e-05\n",
            "Epoch  44 Batch  413 / 525  Training Loss  2.3234864784171805e-05\n",
            "Epoch  44 Batch  414 / 525  Training Loss  1.5919273209874518e-05\n",
            "Epoch  44 Batch  415 / 525  Training Loss  1.2636915016628336e-05\n",
            "Epoch  44 Batch  416 / 525  Training Loss  9.302340913563967e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  44 Batch  417 / 525  Training Loss  1.2384314686642028e-05\n",
            "Epoch  44 Batch  418 / 525  Training Loss  1.5501595044042915e-05\n",
            "Epoch  44 Batch  419 / 525  Training Loss  1.683439222688321e-05\n",
            "Epoch  44 Batch  420 / 525  Training Loss  1.1504122994665522e-05\n",
            "Epoch  44 Batch  421 / 525  Training Loss  1.1476990948722232e-05\n",
            "Epoch  44 Batch  422 / 525  Training Loss  1.139473715738859e-05\n",
            "Epoch  44 Batch  423 / 525  Training Loss  1.4507849300571252e-05\n",
            "Epoch  44 Batch  424 / 525  Training Loss  1.7443471733713523e-05\n",
            "Epoch  44 Batch  425 / 525  Training Loss  2.3012780729914084e-05\n",
            "Epoch  44 Batch  426 / 525  Training Loss  2.559468885010574e-05\n",
            "Epoch  44 Batch  427 / 525  Training Loss  1.4178333003656007e-05\n",
            "Epoch  44 Batch  428 / 525  Training Loss  1.5988585801096633e-05\n",
            "Epoch  44 Batch  429 / 525  Training Loss  1.987220093724318e-05\n",
            "Epoch  44 Batch  430 / 525  Training Loss  1.2540619536594022e-05\n",
            "Epoch  44 Batch  431 / 525  Training Loss  1.2137149496993516e-05\n",
            "Epoch  44 Batch  432 / 525  Training Loss  1.7385653336532414e-05\n",
            "Epoch  44 Batch  433 / 525  Training Loss  1.5724053810117766e-05\n",
            "Epoch  44 Batch  434 / 525  Training Loss  1.939648063853383e-05\n",
            "Epoch  44 Batch  435 / 525  Training Loss  2.343910455238074e-05\n",
            "Epoch  44 Batch  436 / 525  Training Loss  1.1597786397032905e-05\n",
            "Epoch  44 Batch  437 / 525  Training Loss  1.2193275324534625e-05\n",
            "Epoch  44 Batch  438 / 525  Training Loss  1.8156400983571075e-05\n",
            "Epoch  44 Batch  439 / 525  Training Loss  1.5532492398051545e-05\n",
            "Epoch  44 Batch  440 / 525  Training Loss  1.4007937352289446e-05\n",
            "Epoch  44 Batch  441 / 525  Training Loss  1.5156413610384334e-05\n",
            "Epoch  44 Batch  442 / 525  Training Loss  1.6083751688711345e-05\n",
            "Epoch  44 Batch  443 / 525  Training Loss  1.3566200323111843e-05\n",
            "Epoch  44 Batch  444 / 525  Training Loss  1.2674156096181832e-05\n",
            "Epoch  44 Batch  445 / 525  Training Loss  2.3085898646968417e-05\n",
            "Epoch  44 Batch  446 / 525  Training Loss  1.2756127034663223e-05\n",
            "Epoch  44 Batch  447 / 525  Training Loss  1.6139400031534024e-05\n",
            "Epoch  44 Batch  448 / 525  Training Loss  1.8374335922999308e-05\n",
            "Epoch  44 Batch  449 / 525  Training Loss  1.3233741810836364e-05\n",
            "Epoch  44 Batch  450 / 525  Training Loss  1.5053255083330441e-05\n",
            "Epoch  44 Batch  451 / 525  Training Loss  1.5790774341439828e-05\n",
            "Epoch  44 Batch  452 / 525  Training Loss  1.3240120097179897e-05\n",
            "Epoch  44 Batch  453 / 525  Training Loss  1.6083717127912678e-05\n",
            "Epoch  44 Batch  454 / 525  Training Loss  1.3414713976089843e-05\n",
            "Epoch  44 Batch  455 / 525  Training Loss  9.57941392698558e-06\n",
            "Epoch  44 Batch  456 / 525  Training Loss  1.6411737306043506e-05\n",
            "Epoch  44 Batch  457 / 525  Training Loss  1.5502406313316897e-05\n",
            "Epoch  44 Batch  458 / 525  Training Loss  1.9178534785169177e-05\n",
            "Epoch  44 Batch  459 / 525  Training Loss  1.2482174497563392e-05\n",
            "Epoch  44 Batch  460 / 525  Training Loss  1.8011163774644956e-05\n",
            "Epoch  44 Batch  461 / 525  Training Loss  1.9139182768412866e-05\n",
            "Epoch  44 Batch  462 / 525  Training Loss  1.3021823178860359e-05\n",
            "Epoch  44 Batch  463 / 525  Training Loss  2.363915518799331e-05\n",
            "Epoch  44 Batch  464 / 525  Training Loss  1.1399295544833876e-05\n",
            "Epoch  44 Batch  465 / 525  Training Loss  2.1974590708850883e-05\n",
            "Epoch  44 Batch  466 / 525  Training Loss  1.911472099891398e-05\n",
            "Epoch  44 Batch  467 / 525  Training Loss  1.1544839253474493e-05\n",
            "Epoch  44 Batch  468 / 525  Training Loss  1.2122919542889576e-05\n",
            "Epoch  44 Batch  469 / 525  Training Loss  9.605665582057554e-06\n",
            "Epoch  44 Batch  470 / 525  Training Loss  9.891869922284968e-06\n",
            "Epoch  44 Batch  471 / 525  Training Loss  1.4676928913104348e-05\n",
            "Epoch  44 Batch  472 / 525  Training Loss  1.5464065654668957e-05\n",
            "Epoch  44 Batch  473 / 525  Training Loss  1.9578572391765192e-05\n",
            "Epoch  44 Batch  474 / 525  Training Loss  1.507768320152536e-05\n",
            "Epoch  44 Batch  475 / 525  Training Loss  1.6865420548128895e-05\n",
            "Epoch  44 Batch  476 / 525  Training Loss  2.058220525213983e-05\n",
            "Epoch  44 Batch  477 / 525  Training Loss  1.8031176296062768e-05\n",
            "Epoch  44 Batch  478 / 525  Training Loss  1.3227622730482835e-05\n",
            "Epoch  44 Batch  479 / 525  Training Loss  1.5127990991459228e-05\n",
            "Epoch  44 Batch  480 / 525  Training Loss  1.7777992979972623e-05\n",
            "Epoch  44 Batch  481 / 525  Training Loss  1.4927032680134289e-05\n",
            "Epoch  44 Batch  482 / 525  Training Loss  1.758888174663298e-05\n",
            "Epoch  44 Batch  483 / 525  Training Loss  2.2174401237862185e-05\n",
            "Epoch  44 Batch  484 / 525  Training Loss  1.4900188944011461e-05\n",
            "Epoch  44 Batch  485 / 525  Training Loss  1.6871119441930205e-05\n",
            "Epoch  44 Batch  486 / 525  Training Loss  1.595569119672291e-05\n",
            "Epoch  44 Batch  487 / 525  Training Loss  1.4375704267877154e-05\n",
            "Epoch  44 Batch  488 / 525  Training Loss  1.2948713447258342e-05\n",
            "Epoch  44 Batch  489 / 525  Training Loss  1.5185692063823808e-05\n",
            "Epoch  44 Batch  490 / 525  Training Loss  2.1676072719856165e-05\n",
            "Epoch  44 Batch  491 / 525  Training Loss  1.6612397303106263e-05\n",
            "Epoch  44 Batch  492 / 525  Training Loss  2.2171434466145e-05\n",
            "Epoch  44 Batch  493 / 525  Training Loss  1.3758450222667307e-05\n",
            "Epoch  44 Batch  494 / 525  Training Loss  2.4079990907921456e-05\n",
            "Epoch  44 Batch  495 / 525  Training Loss  1.5237494153552689e-05\n",
            "Epoch  44 Batch  496 / 525  Training Loss  6.689423116768012e-06\n",
            "Epoch  44 Batch  497 / 525  Training Loss  2.6499032173887827e-05\n",
            "Epoch  44 Batch  498 / 525  Training Loss  1.625508775759954e-05\n",
            "Epoch  44 Batch  499 / 525  Training Loss  1.1292388990113977e-05\n",
            "Epoch  44 Batch  500 / 525  Training Loss  1.5856629033805802e-05\n",
            "Epoch  44 Batch  501 / 525  Training Loss  1.37715278469841e-05\n",
            "Epoch  44 Batch  502 / 525  Training Loss  1.0733946510299575e-05\n",
            "Epoch  44 Batch  503 / 525  Training Loss  1.59036680997815e-05\n",
            "Epoch  44 Batch  504 / 525  Training Loss  1.5295398043235764e-05\n",
            "Epoch  44 Batch  505 / 525  Training Loss  1.630468796065543e-05\n",
            "Epoch  44 Batch  506 / 525  Training Loss  1.2615778359759133e-05\n",
            "Epoch  44 Batch  507 / 525  Training Loss  1.8019607523456216e-05\n",
            "Epoch  44 Batch  508 / 525  Training Loss  1.4101477063377388e-05\n",
            "Epoch  44 Batch  509 / 525  Training Loss  1.8618691683514044e-05\n",
            "Epoch  44 Batch  510 / 525  Training Loss  3.1863728509051725e-05\n",
            "Epoch  44 Batch  511 / 525  Training Loss  1.7829908756539226e-05\n",
            "Epoch  44 Batch  512 / 525  Training Loss  6.703510734951124e-06\n",
            "Epoch  44 Batch  513 / 525  Training Loss  1.766576497175265e-05\n",
            "Epoch  44 Batch  514 / 525  Training Loss  9.723442417453043e-06\n",
            "Epoch  44 Batch  515 / 525  Training Loss  1.7523951100884005e-05\n",
            "Epoch  44 Batch  516 / 525  Training Loss  1.6582967873546295e-05\n",
            "Epoch  44 Batch  517 / 525  Training Loss  1.599037386768032e-05\n",
            "Epoch  44 Batch  518 / 525  Training Loss  1.7858672435977496e-05\n",
            "Epoch  44 Batch  519 / 525  Training Loss  1.34604506456526e-05\n",
            "Epoch  44 Batch  520 / 525  Training Loss  2.2229787646210752e-05\n",
            "Epoch  44 Batch  521 / 525  Training Loss  2.1999563614372164e-05\n",
            "Epoch  44 Batch  522 / 525  Training Loss  1.8593475033412687e-05\n",
            "Epoch  44 Batch  523 / 525  Training Loss  1.3084344573144335e-05\n",
            "Epoch  44 Batch  524 / 525  Training Loss  1.770087328623049e-05\n",
            "  45    |    -    |   0.000017   | 64.525000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 45\n",
            "Epoch  45 Batch  0 / 525  Training Loss  1.7327854948234744e-05\n",
            "Epoch  45 Batch  1 / 525  Training Loss  1.591648288012948e-05\n",
            "Epoch  45 Batch  2 / 525  Training Loss  7.631659173057415e-06\n",
            "Epoch  45 Batch  3 / 525  Training Loss  1.9307602997287177e-05\n",
            "Epoch  45 Batch  4 / 525  Training Loss  1.3831881915393751e-05\n",
            "Epoch  45 Batch  5 / 525  Training Loss  9.374592991662212e-06\n",
            "Epoch  45 Batch  6 / 525  Training Loss  1.1318898941681255e-05\n",
            "Epoch  45 Batch  7 / 525  Training Loss  1.1943118806811981e-05\n",
            "Epoch  45 Batch  8 / 525  Training Loss  1.3833910998073407e-05\n",
            "Epoch  45 Batch  9 / 525  Training Loss  1.3230528566055e-05\n",
            "Epoch  45 Batch  10 / 525  Training Loss  9.945120837073773e-06\n",
            "Epoch  45 Batch  11 / 525  Training Loss  1.713954407023266e-05\n",
            "Epoch  45 Batch  12 / 525  Training Loss  1.8046908735414036e-05\n",
            "Epoch  45 Batch  13 / 525  Training Loss  1.4683717381558381e-05\n",
            "Epoch  45 Batch  14 / 525  Training Loss  1.2673930541495793e-05\n",
            "Epoch  45 Batch  15 / 525  Training Loss  1.615345354366582e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  45 Batch  16 / 525  Training Loss  2.0590949134202674e-05\n",
            "Epoch  45 Batch  17 / 525  Training Loss  1.9688439351739362e-05\n",
            "Epoch  45 Batch  18 / 525  Training Loss  1.552888898004312e-05\n",
            "Epoch  45 Batch  19 / 525  Training Loss  1.4935435501683969e-05\n",
            "Epoch  45 Batch  20 / 525  Training Loss  1.5590849216096103e-05\n",
            "Epoch  45 Batch  21 / 525  Training Loss  1.656333915889263e-05\n",
            "Epoch  45 Batch  22 / 525  Training Loss  1.0114659744431265e-05\n",
            "Epoch  45 Batch  23 / 525  Training Loss  2.2314130546874367e-05\n",
            "Epoch  45 Batch  24 / 525  Training Loss  1.2929466720379423e-05\n",
            "Epoch  45 Batch  25 / 525  Training Loss  8.603405149187893e-06\n",
            "Epoch  45 Batch  26 / 525  Training Loss  1.1416994311730377e-05\n",
            "Epoch  45 Batch  27 / 525  Training Loss  1.0945937901851721e-05\n",
            "Epoch  45 Batch  28 / 525  Training Loss  1.4344538612931501e-05\n",
            "Epoch  45 Batch  29 / 525  Training Loss  2.269459582748823e-05\n",
            "Epoch  45 Batch  30 / 525  Training Loss  1.971343044715468e-05\n",
            "Epoch  45 Batch  31 / 525  Training Loss  1.4662105058960151e-05\n",
            "Epoch  45 Batch  32 / 525  Training Loss  2.0501098333625123e-05\n",
            "Epoch  45 Batch  33 / 525  Training Loss  2.013290759350639e-05\n",
            "Epoch  45 Batch  34 / 525  Training Loss  1.5101792996574659e-05\n",
            "Epoch  45 Batch  35 / 525  Training Loss  1.9409724700381048e-05\n",
            "Epoch  45 Batch  36 / 525  Training Loss  1.4180604011926334e-05\n",
            "Epoch  45 Batch  37 / 525  Training Loss  1.0981544619426131e-05\n",
            "Epoch  45 Batch  38 / 525  Training Loss  9.300200872530695e-06\n",
            "Epoch  45 Batch  39 / 525  Training Loss  1.545999475638382e-05\n",
            "Epoch  45 Batch  40 / 525  Training Loss  2.5309627744718455e-05\n",
            "Epoch  45 Batch  41 / 525  Training Loss  1.481319031881867e-05\n",
            "Epoch  45 Batch  42 / 525  Training Loss  9.863220839179121e-06\n",
            "Epoch  45 Batch  43 / 525  Training Loss  1.50395881064469e-05\n",
            "Epoch  45 Batch  44 / 525  Training Loss  1.1525655281729996e-05\n",
            "Epoch  45 Batch  45 / 525  Training Loss  1.736849117150996e-05\n",
            "Epoch  45 Batch  46 / 525  Training Loss  2.0148008843534626e-05\n",
            "Epoch  45 Batch  47 / 525  Training Loss  1.4639529581472743e-05\n",
            "Epoch  45 Batch  48 / 525  Training Loss  1.8414604710415006e-05\n",
            "Epoch  45 Batch  49 / 525  Training Loss  1.2834067092626356e-05\n",
            "Epoch  45 Batch  50 / 525  Training Loss  1.3554135875892825e-05\n",
            "Epoch  45 Batch  51 / 525  Training Loss  1.1538875696714967e-05\n",
            "Epoch  45 Batch  52 / 525  Training Loss  1.7072628907044418e-05\n",
            "Epoch  45 Batch  53 / 525  Training Loss  1.6540579963475466e-05\n",
            "Epoch  45 Batch  54 / 525  Training Loss  1.3955582289781887e-05\n",
            "Epoch  45 Batch  55 / 525  Training Loss  7.236395504151005e-06\n",
            "Epoch  45 Batch  56 / 525  Training Loss  2.1560557797783986e-05\n",
            "Epoch  45 Batch  57 / 525  Training Loss  1.6756483091739938e-05\n",
            "Epoch  45 Batch  58 / 525  Training Loss  1.1631685993052088e-05\n",
            "Epoch  45 Batch  59 / 525  Training Loss  2.1951851522317156e-05\n",
            "Epoch  45 Batch  60 / 525  Training Loss  1.5557121514575556e-05\n",
            "Epoch  45 Batch  61 / 525  Training Loss  1.1213224752282258e-05\n",
            "Epoch  45 Batch  62 / 525  Training Loss  1.4433704563998617e-05\n",
            "Epoch  45 Batch  63 / 525  Training Loss  1.2847570360463578e-05\n",
            "Epoch  45 Batch  64 / 525  Training Loss  1.5427933249156922e-05\n",
            "Epoch  45 Batch  65 / 525  Training Loss  1.206109573104186e-05\n",
            "Epoch  45 Batch  66 / 525  Training Loss  1.5643032384105027e-05\n",
            "Epoch  45 Batch  67 / 525  Training Loss  1.2192047506687231e-05\n",
            "Epoch  45 Batch  68 / 525  Training Loss  2.1087746063130908e-05\n",
            "Epoch  45 Batch  69 / 525  Training Loss  1.864757177827414e-05\n",
            "Epoch  45 Batch  70 / 525  Training Loss  1.9430674001341686e-05\n",
            "Epoch  45 Batch  71 / 525  Training Loss  1.6904978110687807e-05\n",
            "Epoch  45 Batch  72 / 525  Training Loss  6.689895144518232e-06\n",
            "Epoch  45 Batch  73 / 525  Training Loss  6.561881491506938e-06\n",
            "Epoch  45 Batch  74 / 525  Training Loss  1.8563721823738888e-05\n",
            "Epoch  45 Batch  75 / 525  Training Loss  1.3332035450730473e-05\n",
            "Epoch  45 Batch  76 / 525  Training Loss  1.5280313164112158e-05\n",
            "Epoch  45 Batch  77 / 525  Training Loss  1.4888716577843297e-05\n",
            "Epoch  45 Batch  78 / 525  Training Loss  1.0214978829026222e-05\n",
            "Epoch  45 Batch  79 / 525  Training Loss  1.508957939222455e-05\n",
            "Epoch  45 Batch  80 / 525  Training Loss  1.5237365914799739e-05\n",
            "Epoch  45 Batch  81 / 525  Training Loss  1.6562607925152406e-05\n",
            "Epoch  45 Batch  82 / 525  Training Loss  1.7417940398445353e-05\n",
            "Epoch  45 Batch  83 / 525  Training Loss  1.2501173841883428e-05\n",
            "Epoch  45 Batch  84 / 525  Training Loss  2.129519089066889e-05\n",
            "Epoch  45 Batch  85 / 525  Training Loss  1.5623056242475286e-05\n",
            "Epoch  45 Batch  86 / 525  Training Loss  1.090045589080546e-05\n",
            "Epoch  45 Batch  87 / 525  Training Loss  1.218599390995223e-05\n",
            "Epoch  45 Batch  88 / 525  Training Loss  1.3214807950134855e-05\n",
            "Epoch  45 Batch  89 / 525  Training Loss  1.2345709365035873e-05\n",
            "Epoch  45 Batch  90 / 525  Training Loss  2.2902539058122784e-05\n",
            "Epoch  45 Batch  91 / 525  Training Loss  2.07671237149043e-05\n",
            "Epoch  45 Batch  92 / 525  Training Loss  1.8828231986844912e-05\n",
            "Epoch  45 Batch  93 / 525  Training Loss  1.6661711924825795e-05\n",
            "Epoch  45 Batch  94 / 525  Training Loss  8.497842827637214e-06\n",
            "Epoch  45 Batch  95 / 525  Training Loss  1.935391810548026e-05\n",
            "Epoch  45 Batch  96 / 525  Training Loss  1.3660066542797722e-05\n",
            "Epoch  45 Batch  97 / 525  Training Loss  1.7127578757936135e-05\n",
            "Epoch  45 Batch  98 / 525  Training Loss  1.2915365914523136e-05\n",
            "Epoch  45 Batch  99 / 525  Training Loss  1.2859437447332311e-05\n",
            "Epoch  45 Batch  100 / 525  Training Loss  9.993339517677668e-06\n",
            "Epoch  45 Batch  101 / 525  Training Loss  1.1088854080298916e-05\n",
            "Epoch  45 Batch  102 / 525  Training Loss  1.755620542098768e-05\n",
            "Epoch  45 Batch  103 / 525  Training Loss  9.469189535593614e-06\n",
            "Epoch  45 Batch  104 / 525  Training Loss  1.7262193068745546e-05\n",
            "Epoch  45 Batch  105 / 525  Training Loss  2.7381203835830092e-05\n",
            "Epoch  45 Batch  106 / 525  Training Loss  1.590824467712082e-05\n",
            "Epoch  45 Batch  107 / 525  Training Loss  9.924575351760723e-06\n",
            "Epoch  45 Batch  108 / 525  Training Loss  1.1476311556180008e-05\n",
            "Epoch  45 Batch  109 / 525  Training Loss  1.7438671420677565e-05\n",
            "Epoch  45 Batch  110 / 525  Training Loss  1.2063293979736045e-05\n",
            "Epoch  45 Batch  111 / 525  Training Loss  1.4402886336029042e-05\n",
            "Epoch  45 Batch  112 / 525  Training Loss  1.4295537766884081e-05\n",
            "Epoch  45 Batch  113 / 525  Training Loss  1.3416591173154302e-05\n",
            "Epoch  45 Batch  114 / 525  Training Loss  1.0877347449422814e-05\n",
            "Epoch  45 Batch  115 / 525  Training Loss  1.7360018318868242e-05\n",
            "Epoch  45 Batch  116 / 525  Training Loss  1.0554511391092092e-05\n",
            "Epoch  45 Batch  117 / 525  Training Loss  1.0907937394222245e-05\n",
            "Epoch  45 Batch  118 / 525  Training Loss  1.0999618098139763e-05\n",
            "Epoch  45 Batch  119 / 525  Training Loss  1.5278848877642304e-05\n",
            "Epoch  45 Batch  120 / 525  Training Loss  1.4291355910245329e-05\n",
            "Epoch  45 Batch  121 / 525  Training Loss  8.90322098712204e-06\n",
            "Epoch  45 Batch  122 / 525  Training Loss  1.4140236089588143e-05\n",
            "Epoch  45 Batch  123 / 525  Training Loss  1.4281899893830996e-05\n",
            "Epoch  45 Batch  124 / 525  Training Loss  1.4822902812738903e-05\n",
            "Epoch  45 Batch  125 / 525  Training Loss  1.0515972462599166e-05\n",
            "Epoch  45 Batch  126 / 525  Training Loss  1.8080439986079e-05\n",
            "Epoch  45 Batch  127 / 525  Training Loss  2.2429248929256573e-05\n",
            "Epoch  45 Batch  128 / 525  Training Loss  7.30948886484839e-06\n",
            "Epoch  45 Batch  129 / 525  Training Loss  7.994242878339719e-06\n",
            "Epoch  45 Batch  130 / 525  Training Loss  1.0010693586082198e-05\n",
            "Epoch  45 Batch  131 / 525  Training Loss  1.3800241504213773e-05\n",
            "Epoch  45 Batch  132 / 525  Training Loss  1.2999227692489512e-05\n",
            "Epoch  45 Batch  133 / 525  Training Loss  2.0994073565816507e-05\n",
            "Epoch  45 Batch  134 / 525  Training Loss  1.6569343642913736e-05\n",
            "Epoch  45 Batch  135 / 525  Training Loss  1.1123172953375615e-05\n",
            "Epoch  45 Batch  136 / 525  Training Loss  2.0034456611028872e-05\n",
            "Epoch  45 Batch  137 / 525  Training Loss  1.0575654414424207e-05\n",
            "Epoch  45 Batch  138 / 525  Training Loss  1.531173438706901e-05\n",
            "Epoch  45 Batch  139 / 525  Training Loss  1.8911441657110117e-05\n",
            "Epoch  45 Batch  140 / 525  Training Loss  1.4716708392370492e-05\n",
            "Epoch  45 Batch  141 / 525  Training Loss  1.1383746823412366e-05\n",
            "Epoch  45 Batch  142 / 525  Training Loss  1.5828296454856172e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  45 Batch  143 / 525  Training Loss  1.4363208720169496e-05\n",
            "Epoch  45 Batch  144 / 525  Training Loss  1.7246951756533235e-05\n",
            "Epoch  45 Batch  145 / 525  Training Loss  1.8649379853741266e-05\n",
            "Epoch  45 Batch  146 / 525  Training Loss  1.615177643543575e-05\n",
            "Epoch  45 Batch  147 / 525  Training Loss  2.1274832761264406e-05\n",
            "Epoch  45 Batch  148 / 525  Training Loss  2.0240593585185707e-05\n",
            "Epoch  45 Batch  149 / 525  Training Loss  2.2266369342105463e-05\n",
            "Epoch  45 Batch  150 / 525  Training Loss  2.6624324164004065e-05\n",
            "Epoch  45 Batch  151 / 525  Training Loss  1.6836991562740877e-05\n",
            "Epoch  45 Batch  152 / 525  Training Loss  1.4433195246965624e-05\n",
            "Epoch  45 Batch  153 / 525  Training Loss  1.3837097867508419e-05\n",
            "Epoch  45 Batch  154 / 525  Training Loss  1.4605558135372121e-05\n",
            "Epoch  45 Batch  155 / 525  Training Loss  8.298663487948943e-06\n",
            "Epoch  45 Batch  156 / 525  Training Loss  1.5120145690161735e-05\n",
            "Epoch  45 Batch  157 / 525  Training Loss  9.883278835332021e-06\n",
            "Epoch  45 Batch  158 / 525  Training Loss  1.86979777936358e-05\n",
            "Epoch  45 Batch  159 / 525  Training Loss  1.3660227523359936e-05\n",
            "Epoch  45 Batch  160 / 525  Training Loss  1.4661379282188136e-05\n",
            "Epoch  45 Batch  161 / 525  Training Loss  1.7793850929592736e-05\n",
            "Epoch  45 Batch  162 / 525  Training Loss  1.6723388398531824e-05\n",
            "Epoch  45 Batch  163 / 525  Training Loss  1.4428007489186712e-05\n",
            "Epoch  45 Batch  164 / 525  Training Loss  1.1413683751015924e-05\n",
            "Epoch  45 Batch  165 / 525  Training Loss  1.7897455109050497e-05\n",
            "Epoch  45 Batch  166 / 525  Training Loss  1.623207572265528e-05\n",
            "Epoch  45 Batch  167 / 525  Training Loss  1.770628477970604e-05\n",
            "Epoch  45 Batch  168 / 525  Training Loss  1.4178012861520983e-05\n",
            "Epoch  45 Batch  169 / 525  Training Loss  1.6047919416450895e-05\n",
            "Epoch  45 Batch  170 / 525  Training Loss  1.6588559446972795e-05\n",
            "Epoch  45 Batch  171 / 525  Training Loss  1.628422614885494e-05\n",
            "Epoch  45 Batch  172 / 525  Training Loss  1.2579458598338533e-05\n",
            "Epoch  45 Batch  173 / 525  Training Loss  1.4836096852377523e-05\n",
            "Epoch  45 Batch  174 / 525  Training Loss  1.3752678569289856e-05\n",
            "Epoch  45 Batch  175 / 525  Training Loss  1.5162007002800237e-05\n",
            "Epoch  45 Batch  176 / 525  Training Loss  6.073127678973833e-06\n",
            "Epoch  45 Batch  177 / 525  Training Loss  1.6193715055123903e-05\n",
            "Epoch  45 Batch  178 / 525  Training Loss  1.2226259968883824e-05\n",
            "Epoch  45 Batch  179 / 525  Training Loss  1.6267094906652346e-05\n",
            "Epoch  45 Batch  180 / 525  Training Loss  2.0674531697295606e-05\n",
            "Epoch  45 Batch  181 / 525  Training Loss  1.4222396202967502e-05\n",
            "Epoch  45 Batch  182 / 525  Training Loss  1.0480418495717458e-05\n",
            "Epoch  45 Batch  183 / 525  Training Loss  1.181087463919539e-05\n",
            "Epoch  45 Batch  184 / 525  Training Loss  1.713034180284012e-05\n",
            "Epoch  45 Batch  185 / 525  Training Loss  1.1595771866268478e-05\n",
            "Epoch  45 Batch  186 / 525  Training Loss  2.3089893147698604e-05\n",
            "Epoch  45 Batch  187 / 525  Training Loss  1.3008242603973486e-05\n",
            "Epoch  45 Batch  188 / 525  Training Loss  1.1047445696021896e-05\n",
            "Epoch  45 Batch  189 / 525  Training Loss  1.7987538740271702e-05\n",
            "Epoch  45 Batch  190 / 525  Training Loss  1.1296823686279822e-05\n",
            "Epoch  45 Batch  191 / 525  Training Loss  1.626916855457239e-05\n",
            "Epoch  45 Batch  192 / 525  Training Loss  1.901052200992126e-05\n",
            "Epoch  45 Batch  193 / 525  Training Loss  1.231533860845957e-05\n",
            "Epoch  45 Batch  194 / 525  Training Loss  1.1430736776674166e-05\n",
            "Epoch  45 Batch  195 / 525  Training Loss  1.4438521247939207e-05\n",
            "Epoch  45 Batch  196 / 525  Training Loss  1.4379484127857722e-05\n",
            "Epoch  45 Batch  197 / 525  Training Loss  2.0413786842254922e-05\n",
            "Epoch  45 Batch  198 / 525  Training Loss  8.762786819715984e-06\n",
            "Epoch  45 Batch  199 / 525  Training Loss  1.8354508938500658e-05\n",
            "Epoch  45 Batch  200 / 525  Training Loss  1.9633092961157672e-05\n",
            "Epoch  45 Batch  201 / 525  Training Loss  1.0778454452520236e-05\n",
            "Epoch  45 Batch  202 / 525  Training Loss  1.7598349586478435e-05\n",
            "Epoch  45 Batch  203 / 525  Training Loss  1.7475120330345817e-05\n",
            "Epoch  45 Batch  204 / 525  Training Loss  1.4326602467917837e-05\n",
            "Epoch  45 Batch  205 / 525  Training Loss  1.536629679321777e-05\n",
            "Epoch  45 Batch  206 / 525  Training Loss  1.7920465325005352e-05\n",
            "Epoch  45 Batch  207 / 525  Training Loss  1.7557751561980695e-05\n",
            "Epoch  45 Batch  208 / 525  Training Loss  1.594709829078056e-05\n",
            "Epoch  45 Batch  209 / 525  Training Loss  1.82265393959824e-05\n",
            "Epoch  45 Batch  210 / 525  Training Loss  1.263736339751631e-05\n",
            "Epoch  45 Batch  211 / 525  Training Loss  1.7020482118823566e-05\n",
            "Epoch  45 Batch  212 / 525  Training Loss  1.1008391084033065e-05\n",
            "Epoch  45 Batch  213 / 525  Training Loss  6.8062918217037804e-06\n",
            "Epoch  45 Batch  214 / 525  Training Loss  2.114426933985669e-05\n",
            "Epoch  45 Batch  215 / 525  Training Loss  9.21259015740361e-06\n",
            "Epoch  45 Batch  216 / 525  Training Loss  1.5633346265531145e-05\n",
            "Epoch  45 Batch  217 / 525  Training Loss  9.576088814355899e-06\n",
            "Epoch  45 Batch  218 / 525  Training Loss  8.656532372697257e-06\n",
            "Epoch  45 Batch  219 / 525  Training Loss  1.3533846868085675e-05\n",
            "Epoch  45 Batch  220 / 525  Training Loss  1.2305083146202378e-05\n",
            "Epoch  45 Batch  221 / 525  Training Loss  1.7772139472072013e-05\n",
            "Epoch  45 Batch  222 / 525  Training Loss  1.2604308722075075e-05\n",
            "Epoch  45 Batch  223 / 525  Training Loss  1.5556879588984884e-05\n",
            "Epoch  45 Batch  224 / 525  Training Loss  1.4694967831019312e-05\n",
            "Epoch  45 Batch  225 / 525  Training Loss  2.295398917340208e-05\n",
            "Epoch  45 Batch  226 / 525  Training Loss  1.8260185242979787e-05\n",
            "Epoch  45 Batch  227 / 525  Training Loss  1.5449177226400934e-05\n",
            "Epoch  45 Batch  228 / 525  Training Loss  2.050420516752638e-05\n",
            "Epoch  45 Batch  229 / 525  Training Loss  1.705140311969444e-05\n",
            "Epoch  45 Batch  230 / 525  Training Loss  2.400848279648926e-05\n",
            "Epoch  45 Batch  231 / 525  Training Loss  8.708891982678324e-06\n",
            "Epoch  45 Batch  232 / 525  Training Loss  1.588463237567339e-05\n",
            "Epoch  45 Batch  233 / 525  Training Loss  2.3666998458793387e-05\n",
            "Epoch  45 Batch  234 / 525  Training Loss  1.5664099919376895e-05\n",
            "Epoch  45 Batch  235 / 525  Training Loss  9.64195260166889e-06\n",
            "Epoch  45 Batch  236 / 525  Training Loss  1.8299431758350693e-05\n",
            "Epoch  45 Batch  237 / 525  Training Loss  1.4468305380432867e-05\n",
            "Epoch  45 Batch  238 / 525  Training Loss  1.0939880667137913e-05\n",
            "Epoch  45 Batch  239 / 525  Training Loss  1.2939655789523385e-05\n",
            "Epoch  45 Batch  240 / 525  Training Loss  1.4146527973935008e-05\n",
            "Epoch  45 Batch  241 / 525  Training Loss  1.3193190170568414e-05\n",
            "Epoch  45 Batch  242 / 525  Training Loss  5.310937467584154e-06\n",
            "Epoch  45 Batch  243 / 525  Training Loss  1.7137079339590855e-05\n",
            "Epoch  45 Batch  244 / 525  Training Loss  1.1980174349446315e-05\n",
            "Epoch  45 Batch  245 / 525  Training Loss  1.4944276699679904e-05\n",
            "Epoch  45 Batch  246 / 525  Training Loss  8.948054528445937e-06\n",
            "Epoch  45 Batch  247 / 525  Training Loss  2.3126402084017172e-05\n",
            "Epoch  45 Batch  248 / 525  Training Loss  1.262518526345957e-05\n",
            "Epoch  45 Batch  249 / 525  Training Loss  1.5558998711640015e-05\n",
            "Epoch  45 Batch  250 / 525  Training Loss  1.3764640243607573e-05\n",
            "Epoch  45 Batch  251 / 525  Training Loss  1.3773324099020101e-05\n",
            "Epoch  45 Batch  252 / 525  Training Loss  1.2117468941141851e-05\n",
            "Epoch  45 Batch  253 / 525  Training Loss  1.3466221389535349e-05\n",
            "Epoch  45 Batch  254 / 525  Training Loss  2.124937418557238e-05\n",
            "Epoch  45 Batch  255 / 525  Training Loss  1.595241337781772e-05\n",
            "Epoch  45 Batch  256 / 525  Training Loss  1.4687730072182603e-05\n",
            "Epoch  45 Batch  257 / 525  Training Loss  1.2344945389486384e-05\n",
            "Epoch  45 Batch  258 / 525  Training Loss  1.1284922038612422e-05\n",
            "Epoch  45 Batch  259 / 525  Training Loss  1.3485524505085777e-05\n",
            "Epoch  45 Batch  260 / 525  Training Loss  1.2268634236534126e-05\n",
            "Epoch  45 Batch  261 / 525  Training Loss  1.2114387573092245e-05\n",
            "Epoch  45 Batch  262 / 525  Training Loss  1.4504051250696648e-05\n",
            "Epoch  45 Batch  263 / 525  Training Loss  1.7319263861281797e-05\n",
            "Epoch  45 Batch  264 / 525  Training Loss  1.400945802743081e-05\n",
            "Epoch  45 Batch  265 / 525  Training Loss  1.8394932340015657e-05\n",
            "Epoch  45 Batch  266 / 525  Training Loss  1.3721713912673295e-05\n",
            "Epoch  45 Batch  267 / 525  Training Loss  1.1960264600929804e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  45 Batch  268 / 525  Training Loss  1.674551094765775e-05\n",
            "Epoch  45 Batch  269 / 525  Training Loss  1.0764406397356652e-05\n",
            "Epoch  45 Batch  270 / 525  Training Loss  1.0258528163831215e-05\n",
            "Epoch  45 Batch  271 / 525  Training Loss  1.1504276699270122e-05\n",
            "Epoch  45 Batch  272 / 525  Training Loss  4.631514002539916e-06\n",
            "Epoch  45 Batch  273 / 525  Training Loss  1.6695908925612457e-05\n",
            "Epoch  45 Batch  274 / 525  Training Loss  1.674115083005745e-05\n",
            "Epoch  45 Batch  275 / 525  Training Loss  1.832540874602273e-05\n",
            "Epoch  45 Batch  276 / 525  Training Loss  1.0037524589279201e-05\n",
            "Epoch  45 Batch  277 / 525  Training Loss  1.7398682757630013e-05\n",
            "Epoch  45 Batch  278 / 525  Training Loss  1.6842141121742316e-05\n",
            "Epoch  45 Batch  279 / 525  Training Loss  1.1386432561266702e-05\n",
            "Epoch  45 Batch  280 / 525  Training Loss  1.7683963960735127e-05\n",
            "Epoch  45 Batch  281 / 525  Training Loss  1.5532159522990696e-05\n",
            "Epoch  45 Batch  282 / 525  Training Loss  1.2105419955332763e-05\n",
            "Epoch  45 Batch  283 / 525  Training Loss  9.41036250878824e-06\n",
            "Epoch  45 Batch  284 / 525  Training Loss  1.5120032003324013e-05\n",
            "Epoch  45 Batch  285 / 525  Training Loss  1.4187736269377638e-05\n",
            "Epoch  45 Batch  286 / 525  Training Loss  1.0762823876575567e-05\n",
            "Epoch  45 Batch  287 / 525  Training Loss  1.3028309695073403e-05\n",
            "Epoch  45 Batch  288 / 525  Training Loss  1.2119020539103076e-05\n",
            "Epoch  45 Batch  289 / 525  Training Loss  1.2782074918504804e-05\n",
            "Epoch  45 Batch  290 / 525  Training Loss  1.1579389138205443e-05\n",
            "Epoch  45 Batch  291 / 525  Training Loss  9.692308594821952e-06\n",
            "Epoch  45 Batch  292 / 525  Training Loss  5.968166078673676e-06\n",
            "Epoch  45 Batch  293 / 525  Training Loss  8.603956302977167e-06\n",
            "Epoch  45 Batch  294 / 525  Training Loss  1.4746314263902605e-05\n",
            "Epoch  45 Batch  295 / 525  Training Loss  1.3361692253965884e-05\n",
            "Epoch  45 Batch  296 / 525  Training Loss  1.5374116628663614e-05\n",
            "Epoch  45 Batch  297 / 525  Training Loss  2.558597770985216e-05\n",
            "Epoch  45 Batch  298 / 525  Training Loss  9.497542123426683e-06\n",
            "Epoch  45 Batch  299 / 525  Training Loss  1.0549958460615017e-05\n",
            "Epoch  45 Batch  300 / 525  Training Loss  1.3743509953201283e-05\n",
            "Epoch  45 Batch  301 / 525  Training Loss  1.3134592336427886e-05\n",
            "Epoch  45 Batch  302 / 525  Training Loss  1.3575790035247337e-05\n",
            "Epoch  45 Batch  303 / 525  Training Loss  1.1112596439488698e-05\n",
            "Epoch  45 Batch  304 / 525  Training Loss  1.3018446225032676e-05\n",
            "Epoch  45 Batch  305 / 525  Training Loss  1.898043956316542e-05\n",
            "Epoch  45 Batch  306 / 525  Training Loss  1.52008724398911e-05\n",
            "Epoch  45 Batch  307 / 525  Training Loss  1.7451202438678592e-05\n",
            "Epoch  45 Batch  308 / 525  Training Loss  1.707036790321581e-05\n",
            "Epoch  45 Batch  309 / 525  Training Loss  1.5582199921482243e-05\n",
            "Epoch  45 Batch  310 / 525  Training Loss  1.854219772212673e-05\n",
            "Epoch  45 Batch  311 / 525  Training Loss  1.2213978152431082e-05\n",
            "Epoch  45 Batch  312 / 525  Training Loss  2.0251991372788325e-05\n",
            "Epoch  45 Batch  313 / 525  Training Loss  1.2797402632713784e-05\n",
            "Epoch  45 Batch  314 / 525  Training Loss  1.5798068488948047e-05\n",
            "Epoch  45 Batch  315 / 525  Training Loss  1.6104764654301107e-05\n",
            "Epoch  45 Batch  316 / 525  Training Loss  1.6780424630269408e-05\n",
            "Epoch  45 Batch  317 / 525  Training Loss  1.7117843526648358e-05\n",
            "Epoch  45 Batch  318 / 525  Training Loss  1.0575551641522907e-05\n",
            "Epoch  45 Batch  319 / 525  Training Loss  1.7097707313951105e-05\n",
            "Epoch  45 Batch  320 / 525  Training Loss  1.8151031326851808e-05\n",
            "Epoch  45 Batch  321 / 525  Training Loss  9.704996045911685e-06\n",
            "Epoch  45 Batch  322 / 525  Training Loss  1.735095247568097e-05\n",
            "Epoch  45 Batch  323 / 525  Training Loss  1.6653288184897974e-05\n",
            "Epoch  45 Batch  324 / 525  Training Loss  2.2776544938096777e-05\n",
            "Epoch  45 Batch  325 / 525  Training Loss  1.7778405890567228e-05\n",
            "Epoch  45 Batch  326 / 525  Training Loss  1.288962084800005e-05\n",
            "Epoch  45 Batch  327 / 525  Training Loss  1.812487607821822e-05\n",
            "Epoch  45 Batch  328 / 525  Training Loss  1.987694668059703e-05\n",
            "Epoch  45 Batch  329 / 525  Training Loss  1.603980672371108e-05\n",
            "Epoch  45 Batch  330 / 525  Training Loss  1.5114288544282317e-05\n",
            "Epoch  45 Batch  331 / 525  Training Loss  1.5572406482533552e-05\n",
            "Epoch  45 Batch  332 / 525  Training Loss  1.82135609065881e-05\n",
            "Epoch  45 Batch  333 / 525  Training Loss  1.9394263290450908e-05\n",
            "Epoch  45 Batch  334 / 525  Training Loss  1.1311204616504256e-05\n",
            "Epoch  45 Batch  335 / 525  Training Loss  1.620901639398653e-05\n",
            "Epoch  45 Batch  336 / 525  Training Loss  9.679934919404332e-06\n",
            "Epoch  45 Batch  337 / 525  Training Loss  2.3500915631302632e-05\n",
            "Epoch  45 Batch  338 / 525  Training Loss  1.1016024473065045e-05\n",
            "Epoch  45 Batch  339 / 525  Training Loss  1.8370996258454397e-05\n",
            "Epoch  45 Batch  340 / 525  Training Loss  1.8816856027115136e-05\n",
            "Epoch  45 Batch  341 / 525  Training Loss  1.5273048120434396e-05\n",
            "Epoch  45 Batch  342 / 525  Training Loss  1.5048167369968724e-05\n",
            "Epoch  45 Batch  343 / 525  Training Loss  9.69825305219274e-06\n",
            "Epoch  45 Batch  344 / 525  Training Loss  7.489084964618087e-06\n",
            "Epoch  45 Batch  345 / 525  Training Loss  1.2672346201725304e-05\n",
            "Epoch  45 Batch  346 / 525  Training Loss  2.0305840735090896e-05\n",
            "Epoch  45 Batch  347 / 525  Training Loss  1.4098181964072865e-05\n",
            "Epoch  45 Batch  348 / 525  Training Loss  1.0158129043702502e-05\n",
            "Epoch  45 Batch  349 / 525  Training Loss  2.527139804442413e-05\n",
            "Epoch  45 Batch  350 / 525  Training Loss  1.0839292372111231e-05\n",
            "Epoch  45 Batch  351 / 525  Training Loss  1.2976614925719332e-05\n",
            "Epoch  45 Batch  352 / 525  Training Loss  1.5446397810592316e-05\n",
            "Epoch  45 Batch  353 / 525  Training Loss  1.927046469063498e-05\n",
            "Epoch  45 Batch  354 / 525  Training Loss  1.3703213880944531e-05\n",
            "Epoch  45 Batch  355 / 525  Training Loss  1.355169115413446e-05\n",
            "Epoch  45 Batch  356 / 525  Training Loss  1.8842178178601898e-05\n",
            "Epoch  45 Batch  357 / 525  Training Loss  1.1406728845031466e-05\n",
            "Epoch  45 Batch  358 / 525  Training Loss  2.241583752038423e-05\n",
            "Epoch  45 Batch  359 / 525  Training Loss  1.2343034541117959e-05\n",
            "Epoch  45 Batch  360 / 525  Training Loss  2.1406671294244006e-05\n",
            "Epoch  45 Batch  361 / 525  Training Loss  1.909488310047891e-05\n",
            "Epoch  45 Batch  362 / 525  Training Loss  1.6143641914823093e-05\n",
            "Epoch  45 Batch  363 / 525  Training Loss  1.0743278835434467e-05\n",
            "Epoch  45 Batch  364 / 525  Training Loss  1.6652464182698168e-05\n",
            "Epoch  45 Batch  365 / 525  Training Loss  1.629616235732101e-05\n",
            "Epoch  45 Batch  366 / 525  Training Loss  1.9809978766716085e-05\n",
            "Epoch  45 Batch  367 / 525  Training Loss  1.0652905075403396e-05\n",
            "Epoch  45 Batch  368 / 525  Training Loss  1.8761998944683e-05\n",
            "Epoch  45 Batch  369 / 525  Training Loss  9.20431011763867e-06\n",
            "Epoch  45 Batch  370 / 525  Training Loss  1.8838949472410604e-05\n",
            "Epoch  45 Batch  371 / 525  Training Loss  1.2519691154011525e-05\n",
            "Epoch  45 Batch  372 / 525  Training Loss  1.52814118337119e-05\n",
            "Epoch  45 Batch  373 / 525  Training Loss  1.93091545952484e-05\n",
            "Epoch  45 Batch  374 / 525  Training Loss  1.4409873983822763e-05\n",
            "Epoch  45 Batch  375 / 525  Training Loss  2.4290740839205682e-05\n",
            "Epoch  45 Batch  376 / 525  Training Loss  1.0720328646129929e-05\n",
            "Epoch  45 Batch  377 / 525  Training Loss  1.461788451706525e-05\n",
            "Epoch  45 Batch  378 / 525  Training Loss  1.600793984835036e-05\n",
            "Epoch  45 Batch  379 / 525  Training Loss  1.487911504227668e-05\n",
            "Epoch  45 Batch  380 / 525  Training Loss  1.066162258211989e-05\n",
            "Epoch  45 Batch  381 / 525  Training Loss  1.309607887378661e-05\n",
            "Epoch  45 Batch  382 / 525  Training Loss  8.537126632290892e-06\n",
            "Epoch  45 Batch  383 / 525  Training Loss  1.570703534525819e-05\n",
            "Epoch  45 Batch  384 / 525  Training Loss  1.7912247130880132e-05\n",
            "Epoch  45 Batch  385 / 525  Training Loss  2.2823345716460608e-05\n",
            "Epoch  45 Batch  386 / 525  Training Loss  1.2856027751695365e-05\n",
            "Epoch  45 Batch  387 / 525  Training Loss  1.44979221659014e-05\n",
            "Epoch  45 Batch  388 / 525  Training Loss  1.954922845470719e-05\n",
            "Epoch  45 Batch  389 / 525  Training Loss  1.2130359209550079e-05\n",
            "Epoch  45 Batch  390 / 525  Training Loss  2.0910589228151366e-05\n",
            "Epoch  45 Batch  391 / 525  Training Loss  1.9889626855729148e-05\n",
            "Epoch  45 Batch  392 / 525  Training Loss  1.0952326192636974e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  45 Batch  393 / 525  Training Loss  1.3113480235915631e-05\n",
            "Epoch  45 Batch  394 / 525  Training Loss  1.5562911357847042e-05\n",
            "Epoch  45 Batch  395 / 525  Training Loss  1.3642636986332946e-05\n",
            "Epoch  45 Batch  396 / 525  Training Loss  9.091147148865275e-06\n",
            "Epoch  45 Batch  397 / 525  Training Loss  9.503821274847724e-06\n",
            "Epoch  45 Batch  398 / 525  Training Loss  1.1886023457918782e-05\n",
            "Epoch  45 Batch  399 / 525  Training Loss  1.8643382645677775e-05\n",
            "Epoch  45 Batch  400 / 525  Training Loss  1.4851296327833552e-05\n",
            "Epoch  45 Batch  401 / 525  Training Loss  1.4205876141204499e-05\n",
            "Epoch  45 Batch  402 / 525  Training Loss  9.07929461391177e-06\n",
            "Epoch  45 Batch  403 / 525  Training Loss  1.0327645213692449e-05\n",
            "Epoch  45 Batch  404 / 525  Training Loss  7.921518772491254e-06\n",
            "Epoch  45 Batch  405 / 525  Training Loss  2.5365035980939865e-05\n",
            "Epoch  45 Batch  406 / 525  Training Loss  1.3236318409326486e-05\n",
            "Epoch  45 Batch  407 / 525  Training Loss  1.7528327589388937e-05\n",
            "Epoch  45 Batch  408 / 525  Training Loss  1.5234087186399847e-05\n",
            "Epoch  45 Batch  409 / 525  Training Loss  2.081066668324638e-05\n",
            "Epoch  45 Batch  410 / 525  Training Loss  1.639759102545213e-05\n",
            "Epoch  45 Batch  411 / 525  Training Loss  1.9476021407172084e-05\n",
            "Epoch  45 Batch  412 / 525  Training Loss  1.07629948615795e-05\n",
            "Epoch  45 Batch  413 / 525  Training Loss  1.1840504157589749e-05\n",
            "Epoch  45 Batch  414 / 525  Training Loss  1.8526301573729143e-05\n",
            "Epoch  45 Batch  415 / 525  Training Loss  8.536953828297555e-06\n",
            "Epoch  45 Batch  416 / 525  Training Loss  1.3008464520680718e-05\n",
            "Epoch  45 Batch  417 / 525  Training Loss  1.683213486103341e-05\n",
            "Epoch  45 Batch  418 / 525  Training Loss  1.5767087461426854e-05\n",
            "Epoch  45 Batch  419 / 525  Training Loss  9.15223154152045e-06\n",
            "Epoch  45 Batch  420 / 525  Training Loss  2.258766835439019e-05\n",
            "Epoch  45 Batch  421 / 525  Training Loss  1.172616430267226e-05\n",
            "Epoch  45 Batch  422 / 525  Training Loss  2.058185236819554e-05\n",
            "Epoch  45 Batch  423 / 525  Training Loss  1.3724973541684449e-05\n",
            "Epoch  45 Batch  424 / 525  Training Loss  9.081834832613822e-06\n",
            "Epoch  45 Batch  425 / 525  Training Loss  1.4278155504143797e-05\n",
            "Epoch  45 Batch  426 / 525  Training Loss  1.5598760001012124e-05\n",
            "Epoch  45 Batch  427 / 525  Training Loss  1.5264547982951626e-05\n",
            "Epoch  45 Batch  428 / 525  Training Loss  7.864571671234444e-06\n",
            "Epoch  45 Batch  429 / 525  Training Loss  1.0531768566579558e-05\n",
            "Epoch  45 Batch  430 / 525  Training Loss  1.563063960929867e-05\n",
            "Epoch  45 Batch  431 / 525  Training Loss  1.4461426872003358e-05\n",
            "Epoch  45 Batch  432 / 525  Training Loss  1.4273276974563487e-05\n",
            "Epoch  45 Batch  433 / 525  Training Loss  1.6667821910232306e-05\n",
            "Epoch  45 Batch  434 / 525  Training Loss  1.9992254237877205e-05\n",
            "Epoch  45 Batch  435 / 525  Training Loss  1.71170449903002e-05\n",
            "Epoch  45 Batch  436 / 525  Training Loss  1.7056330761988647e-05\n",
            "Epoch  45 Batch  437 / 525  Training Loss  1.8602881027618423e-05\n",
            "Epoch  45 Batch  438 / 525  Training Loss  6.688894700346282e-06\n",
            "Epoch  45 Batch  439 / 525  Training Loss  2.149663669115398e-05\n",
            "Epoch  45 Batch  440 / 525  Training Loss  1.7977987226913683e-05\n",
            "Epoch  45 Batch  441 / 525  Training Loss  1.697241714282427e-05\n",
            "Epoch  45 Batch  442 / 525  Training Loss  1.545460327179171e-05\n",
            "Epoch  45 Batch  443 / 525  Training Loss  1.3815879356116056e-05\n",
            "Epoch  45 Batch  444 / 525  Training Loss  1.3064479389868211e-05\n",
            "Epoch  45 Batch  445 / 525  Training Loss  8.467663974442985e-06\n",
            "Epoch  45 Batch  446 / 525  Training Loss  1.741748201311566e-05\n",
            "Epoch  45 Batch  447 / 525  Training Loss  8.641965905553661e-06\n",
            "Epoch  45 Batch  448 / 525  Training Loss  1.8600050680106506e-05\n",
            "Epoch  45 Batch  449 / 525  Training Loss  1.551617242512293e-05\n",
            "Epoch  45 Batch  450 / 525  Training Loss  1.2433723895810544e-05\n",
            "Epoch  45 Batch  451 / 525  Training Loss  1.4049718629394192e-05\n",
            "Epoch  45 Batch  452 / 525  Training Loss  1.4924759852874558e-05\n",
            "Epoch  45 Batch  453 / 525  Training Loss  1.26718314277241e-05\n",
            "Epoch  45 Batch  454 / 525  Training Loss  1.2218471965752542e-05\n",
            "Epoch  45 Batch  455 / 525  Training Loss  1.5246910152200144e-05\n",
            "Epoch  45 Batch  456 / 525  Training Loss  1.613628955965396e-05\n",
            "Epoch  45 Batch  457 / 525  Training Loss  8.492668712278828e-06\n",
            "Epoch  45 Batch  458 / 525  Training Loss  9.645367754274048e-06\n",
            "Epoch  45 Batch  459 / 525  Training Loss  1.907461046357639e-05\n",
            "Epoch  45 Batch  460 / 525  Training Loss  1.8330460079596378e-05\n",
            "Epoch  45 Batch  461 / 525  Training Loss  1.7523898350191303e-05\n",
            "Epoch  45 Batch  462 / 525  Training Loss  1.7025751731125638e-05\n",
            "Epoch  45 Batch  463 / 525  Training Loss  1.8651084246812388e-05\n",
            "Epoch  45 Batch  464 / 525  Training Loss  1.6685877199051902e-05\n",
            "Epoch  45 Batch  465 / 525  Training Loss  1.1459883353381883e-05\n",
            "Epoch  45 Batch  466 / 525  Training Loss  1.76693301909836e-05\n",
            "Epoch  45 Batch  467 / 525  Training Loss  1.2273629181436263e-05\n",
            "Epoch  45 Batch  468 / 525  Training Loss  1.4829854080744553e-05\n",
            "Epoch  45 Batch  469 / 525  Training Loss  1.4500833458441775e-05\n",
            "Epoch  45 Batch  470 / 525  Training Loss  1.4168965208227746e-05\n",
            "Epoch  45 Batch  471 / 525  Training Loss  1.54974313772982e-05\n",
            "Epoch  45 Batch  472 / 525  Training Loss  1.6705833331798203e-05\n",
            "Epoch  45 Batch  473 / 525  Training Loss  1.0862352610274684e-05\n",
            "Epoch  45 Batch  474 / 525  Training Loss  1.1624989383562934e-05\n",
            "Epoch  45 Batch  475 / 525  Training Loss  1.4873221516609192e-05\n",
            "Epoch  45 Batch  476 / 525  Training Loss  1.6441494153696112e-05\n",
            "Epoch  45 Batch  477 / 525  Training Loss  1.5335313946707174e-05\n",
            "Epoch  45 Batch  478 / 525  Training Loss  1.3821324500895571e-05\n",
            "Epoch  45 Batch  479 / 525  Training Loss  8.232613254222088e-06\n",
            "Epoch  45 Batch  480 / 525  Training Loss  1.7285799913224764e-05\n",
            "Epoch  45 Batch  481 / 525  Training Loss  2.1561461835517548e-05\n",
            "Epoch  45 Batch  482 / 525  Training Loss  1.3865806977264583e-05\n",
            "Epoch  45 Batch  483 / 525  Training Loss  1.6015657820389606e-05\n",
            "Epoch  45 Batch  484 / 525  Training Loss  2.064122963929549e-05\n",
            "Epoch  45 Batch  485 / 525  Training Loss  2.048726673820056e-05\n",
            "Epoch  45 Batch  486 / 525  Training Loss  2.3134902221499942e-05\n",
            "Epoch  45 Batch  487 / 525  Training Loss  1.5020841601653956e-05\n",
            "Epoch  45 Batch  488 / 525  Training Loss  1.3280757229949813e-05\n",
            "Epoch  45 Batch  489 / 525  Training Loss  1.5750087186461315e-05\n",
            "Epoch  45 Batch  490 / 525  Training Loss  1.7448119251639582e-05\n",
            "Epoch  45 Batch  491 / 525  Training Loss  1.263100693904562e-05\n",
            "Epoch  45 Batch  492 / 525  Training Loss  1.9547549527487718e-05\n",
            "Epoch  45 Batch  493 / 525  Training Loss  1.3403781849774532e-05\n",
            "Epoch  45 Batch  494 / 525  Training Loss  1.2487241292546969e-05\n",
            "Epoch  45 Batch  495 / 525  Training Loss  9.436969776288606e-06\n",
            "Epoch  45 Batch  496 / 525  Training Loss  1.3731578292208724e-05\n",
            "Epoch  45 Batch  497 / 525  Training Loss  1.0248292710457463e-05\n",
            "Epoch  45 Batch  498 / 525  Training Loss  2.8206221031723544e-05\n",
            "Epoch  45 Batch  499 / 525  Training Loss  1.668537333898712e-05\n",
            "Epoch  45 Batch  500 / 525  Training Loss  7.197882041509729e-06\n",
            "Epoch  45 Batch  501 / 525  Training Loss  1.5094218724698294e-05\n",
            "Epoch  45 Batch  502 / 525  Training Loss  1.3939544260210823e-05\n",
            "Epoch  45 Batch  503 / 525  Training Loss  1.473688280384522e-05\n",
            "Epoch  45 Batch  504 / 525  Training Loss  1.7992817447520792e-05\n",
            "Epoch  45 Batch  505 / 525  Training Loss  1.4995386663940735e-05\n",
            "Epoch  45 Batch  506 / 525  Training Loss  2.021579166466836e-05\n",
            "Epoch  45 Batch  507 / 525  Training Loss  1.3310344002093188e-05\n",
            "Epoch  45 Batch  508 / 525  Training Loss  1.1611458830884658e-05\n",
            "Epoch  45 Batch  509 / 525  Training Loss  1.8133239791495726e-05\n",
            "Epoch  45 Batch  510 / 525  Training Loss  2.0681141904788092e-05\n",
            "Epoch  45 Batch  511 / 525  Training Loss  1.3653587302542292e-05\n",
            "Epoch  45 Batch  512 / 525  Training Loss  1.342214545729803e-05\n",
            "Epoch  45 Batch  513 / 525  Training Loss  1.3505006791092455e-05\n",
            "Epoch  45 Batch  514 / 525  Training Loss  2.3695396521361545e-05\n",
            "Epoch  45 Batch  515 / 525  Training Loss  8.512266504112631e-06\n",
            "Epoch  45 Batch  516 / 525  Training Loss  1.2500841876317281e-05\n",
            "Epoch  45 Batch  517 / 525  Training Loss  1.1978037036897149e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  45 Batch  518 / 525  Training Loss  2.0301773474784568e-05\n",
            "Epoch  45 Batch  519 / 525  Training Loss  1.3276679965201765e-05\n",
            "Epoch  45 Batch  520 / 525  Training Loss  2.310595300514251e-05\n",
            "Epoch  45 Batch  521 / 525  Training Loss  1.8965340132126585e-05\n",
            "Epoch  45 Batch  522 / 525  Training Loss  1.4006206583871972e-05\n",
            "Epoch  45 Batch  523 / 525  Training Loss  1.2794922440662049e-05\n",
            "Epoch  45 Batch  524 / 525  Training Loss  1.9850895114359446e-05\n",
            "  46    |    -    |   0.000015   | 64.641667\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 46\n",
            "Epoch  46 Batch  0 / 525  Training Loss  1.4657578503829427e-05\n",
            "Epoch  46 Batch  1 / 525  Training Loss  1.740602237987332e-05\n",
            "Epoch  46 Batch  2 / 525  Training Loss  1.4980525520513766e-05\n",
            "Epoch  46 Batch  3 / 525  Training Loss  1.1290305337752216e-05\n",
            "Epoch  46 Batch  4 / 525  Training Loss  1.1672799701045733e-05\n",
            "Epoch  46 Batch  5 / 525  Training Loss  1.9433224224485457e-05\n",
            "Epoch  46 Batch  6 / 525  Training Loss  9.991104889195412e-06\n",
            "Epoch  46 Batch  7 / 525  Training Loss  1.8979862943524495e-05\n",
            "Epoch  46 Batch  8 / 525  Training Loss  1.085789244825719e-05\n",
            "Epoch  46 Batch  9 / 525  Training Loss  2.4301043595187366e-05\n",
            "Epoch  46 Batch  10 / 525  Training Loss  1.5248525414790493e-05\n",
            "Epoch  46 Batch  11 / 525  Training Loss  1.1857498975587077e-05\n",
            "Epoch  46 Batch  12 / 525  Training Loss  1.6163927284651436e-05\n",
            "Epoch  46 Batch  13 / 525  Training Loss  1.398438780597644e-05\n",
            "Epoch  46 Batch  14 / 525  Training Loss  7.170109711296391e-06\n",
            "Epoch  46 Batch  15 / 525  Training Loss  1.3785039300273638e-05\n",
            "Epoch  46 Batch  16 / 525  Training Loss  1.2387860806484241e-05\n",
            "Epoch  46 Batch  17 / 525  Training Loss  1.5387680832645856e-05\n",
            "Epoch  46 Batch  18 / 525  Training Loss  1.3865207620256115e-05\n",
            "Epoch  46 Batch  19 / 525  Training Loss  1.5011401956144255e-05\n",
            "Epoch  46 Batch  20 / 525  Training Loss  2.0043362383148633e-05\n",
            "Epoch  46 Batch  21 / 525  Training Loss  1.9428032828727737e-05\n",
            "Epoch  46 Batch  22 / 525  Training Loss  1.2171780326752923e-05\n",
            "Epoch  46 Batch  23 / 525  Training Loss  1.5180841728579253e-05\n",
            "Epoch  46 Batch  24 / 525  Training Loss  6.721393674524734e-06\n",
            "Epoch  46 Batch  25 / 525  Training Loss  1.776024146238342e-05\n",
            "Epoch  46 Batch  26 / 525  Training Loss  1.6816384231788106e-05\n",
            "Epoch  46 Batch  27 / 525  Training Loss  2.1734682377427816e-05\n",
            "Epoch  46 Batch  28 / 525  Training Loss  8.386094123125076e-06\n",
            "Epoch  46 Batch  29 / 525  Training Loss  1.2741645150526892e-05\n",
            "Epoch  46 Batch  30 / 525  Training Loss  1.3164195479475893e-05\n",
            "Epoch  46 Batch  31 / 525  Training Loss  1.4261827345762867e-05\n",
            "Epoch  46 Batch  32 / 525  Training Loss  1.6162377505679615e-05\n",
            "Epoch  46 Batch  33 / 525  Training Loss  1.4003305295773316e-05\n",
            "Epoch  46 Batch  34 / 525  Training Loss  1.2959445484739263e-05\n",
            "Epoch  46 Batch  35 / 525  Training Loss  1.3457002751238178e-05\n",
            "Epoch  46 Batch  36 / 525  Training Loss  2.07212106033694e-05\n",
            "Epoch  46 Batch  37 / 525  Training Loss  8.481209988531191e-06\n",
            "Epoch  46 Batch  38 / 525  Training Loss  9.428422345081344e-06\n",
            "Epoch  46 Batch  39 / 525  Training Loss  1.1268650268903002e-05\n",
            "Epoch  46 Batch  40 / 525  Training Loss  1.5776691725477576e-05\n",
            "Epoch  46 Batch  41 / 525  Training Loss  2.161863994842861e-05\n",
            "Epoch  46 Batch  42 / 525  Training Loss  1.5495432307943702e-05\n",
            "Epoch  46 Batch  43 / 525  Training Loss  1.717619670671411e-05\n",
            "Epoch  46 Batch  44 / 525  Training Loss  9.636110917199403e-06\n",
            "Epoch  46 Batch  45 / 525  Training Loss  1.5702189557487145e-05\n",
            "Epoch  46 Batch  46 / 525  Training Loss  8.456053365080152e-06\n",
            "Epoch  46 Batch  47 / 525  Training Loss  2.1855685190530494e-05\n",
            "Epoch  46 Batch  48 / 525  Training Loss  1.3534478966903407e-05\n",
            "Epoch  46 Batch  49 / 525  Training Loss  1.0250870218442287e-05\n",
            "Epoch  46 Batch  50 / 525  Training Loss  1.3136207599018235e-05\n",
            "Epoch  46 Batch  51 / 525  Training Loss  1.584990241099149e-05\n",
            "Epoch  46 Batch  52 / 525  Training Loss  1.4019162335898727e-05\n",
            "Epoch  46 Batch  53 / 525  Training Loss  1.6329329810105264e-05\n",
            "Epoch  46 Batch  54 / 525  Training Loss  2.1766421923530288e-05\n",
            "Epoch  46 Batch  55 / 525  Training Loss  1.4590487808163743e-05\n",
            "Epoch  46 Batch  56 / 525  Training Loss  1.310578500124393e-05\n",
            "Epoch  46 Batch  57 / 525  Training Loss  1.2313563274801709e-05\n",
            "Epoch  46 Batch  58 / 525  Training Loss  1.603526652615983e-05\n",
            "Epoch  46 Batch  59 / 525  Training Loss  1.4054191524337512e-05\n",
            "Epoch  46 Batch  60 / 525  Training Loss  1.3325552572496235e-05\n",
            "Epoch  46 Batch  61 / 525  Training Loss  1.725717811495997e-05\n",
            "Epoch  46 Batch  62 / 525  Training Loss  9.731910722621251e-06\n",
            "Epoch  46 Batch  63 / 525  Training Loss  1.232734757650178e-05\n",
            "Epoch  46 Batch  64 / 525  Training Loss  1.4027037650521379e-05\n",
            "Epoch  46 Batch  65 / 525  Training Loss  9.073709406948183e-06\n",
            "Epoch  46 Batch  66 / 525  Training Loss  1.139522919402225e-05\n",
            "Epoch  46 Batch  67 / 525  Training Loss  1.0885230949497782e-05\n",
            "Epoch  46 Batch  68 / 525  Training Loss  1.2810482985514682e-05\n",
            "Epoch  46 Batch  69 / 525  Training Loss  8.72325381351402e-06\n",
            "Epoch  46 Batch  70 / 525  Training Loss  1.4450381058850326e-05\n",
            "Epoch  46 Batch  71 / 525  Training Loss  1.3022096936765593e-05\n",
            "Epoch  46 Batch  72 / 525  Training Loss  1.302921555179637e-05\n",
            "Epoch  46 Batch  73 / 525  Training Loss  2.0618337657651864e-05\n",
            "Epoch  46 Batch  74 / 525  Training Loss  1.45406029332662e-05\n",
            "Epoch  46 Batch  75 / 525  Training Loss  6.751501587132225e-06\n",
            "Epoch  46 Batch  76 / 525  Training Loss  1.293438981520012e-05\n",
            "Epoch  46 Batch  77 / 525  Training Loss  1.553036054247059e-05\n",
            "Epoch  46 Batch  78 / 525  Training Loss  1.2800941476598382e-05\n",
            "Epoch  46 Batch  79 / 525  Training Loss  1.6010064427973703e-05\n",
            "Epoch  46 Batch  80 / 525  Training Loss  1.1323701073706616e-05\n",
            "Epoch  46 Batch  81 / 525  Training Loss  1.306168815062847e-05\n",
            "Epoch  46 Batch  82 / 525  Training Loss  1.4648938304162584e-05\n",
            "Epoch  46 Batch  83 / 525  Training Loss  1.5705994883319363e-05\n",
            "Epoch  46 Batch  84 / 525  Training Loss  1.4009240658197086e-05\n",
            "Epoch  46 Batch  85 / 525  Training Loss  1.6438692910014652e-05\n",
            "Epoch  46 Batch  86 / 525  Training Loss  1.4279615243140142e-05\n",
            "Epoch  46 Batch  87 / 525  Training Loss  1.4010333870828617e-05\n",
            "Epoch  46 Batch  88 / 525  Training Loss  9.143097486230545e-06\n",
            "Epoch  46 Batch  89 / 525  Training Loss  1.2235844224051107e-05\n",
            "Epoch  46 Batch  90 / 525  Training Loss  1.86833058251068e-05\n",
            "Epoch  46 Batch  91 / 525  Training Loss  2.224506897618994e-05\n",
            "Epoch  46 Batch  92 / 525  Training Loss  1.7585920431884006e-05\n",
            "Epoch  46 Batch  93 / 525  Training Loss  1.4257068869483192e-05\n",
            "Epoch  46 Batch  94 / 525  Training Loss  1.1528723916853778e-05\n",
            "Epoch  46 Batch  95 / 525  Training Loss  1.288829207624076e-05\n",
            "Epoch  46 Batch  96 / 525  Training Loss  1.490121940150857e-05\n",
            "Epoch  46 Batch  97 / 525  Training Loss  1.595668072695844e-05\n",
            "Epoch  46 Batch  98 / 525  Training Loss  1.0739351637312211e-05\n",
            "Epoch  46 Batch  99 / 525  Training Loss  1.1076671398768667e-05\n",
            "Epoch  46 Batch  100 / 525  Training Loss  1.2246277947269846e-05\n",
            "Epoch  46 Batch  101 / 525  Training Loss  1.0941757864202373e-05\n",
            "Epoch  46 Batch  102 / 525  Training Loss  1.339011487289099e-05\n",
            "Epoch  46 Batch  103 / 525  Training Loss  1.3074570233584382e-05\n",
            "Epoch  46 Batch  104 / 525  Training Loss  1.4875244232825935e-05\n",
            "Epoch  46 Batch  105 / 525  Training Loss  1.1287127563264221e-05\n",
            "Epoch  46 Batch  106 / 525  Training Loss  2.0710105673060752e-05\n",
            "Epoch  46 Batch  107 / 525  Training Loss  1.699337371974252e-05\n",
            "Epoch  46 Batch  108 / 525  Training Loss  1.7148873666883446e-05\n",
            "Epoch  46 Batch  109 / 525  Training Loss  1.631506165722385e-05\n",
            "Epoch  46 Batch  110 / 525  Training Loss  1.167677874036599e-05\n",
            "Epoch  46 Batch  111 / 525  Training Loss  1.1977513167948928e-05\n",
            "Epoch  46 Batch  112 / 525  Training Loss  1.2290431186556816e-05\n",
            "Epoch  46 Batch  113 / 525  Training Loss  7.5425814429763705e-06\n",
            "Epoch  46 Batch  114 / 525  Training Loss  6.127779670350719e-06\n",
            "Epoch  46 Batch  115 / 525  Training Loss  1.356854318146361e-05\n",
            "Epoch  46 Batch  116 / 525  Training Loss  1.2292139217606746e-05\n",
            "Epoch  46 Batch  117 / 525  Training Loss  1.4526684935844969e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  46 Batch  118 / 525  Training Loss  1.4161696526571177e-05\n",
            "Epoch  46 Batch  119 / 525  Training Loss  1.7366714018862695e-05\n",
            "Epoch  46 Batch  120 / 525  Training Loss  1.654177140153479e-05\n",
            "Epoch  46 Batch  121 / 525  Training Loss  1.6571411833865568e-05\n",
            "Epoch  46 Batch  122 / 525  Training Loss  1.7084395949495956e-05\n",
            "Epoch  46 Batch  123 / 525  Training Loss  1.7870666852104478e-05\n",
            "Epoch  46 Batch  124 / 525  Training Loss  9.72027555690147e-06\n",
            "Epoch  46 Batch  125 / 525  Training Loss  1.2820743904740084e-05\n",
            "Epoch  46 Batch  126 / 525  Training Loss  1.1935941074625589e-05\n",
            "Epoch  46 Batch  127 / 525  Training Loss  1.549324042571243e-05\n",
            "Epoch  46 Batch  128 / 525  Training Loss  1.3829366253048647e-05\n",
            "Epoch  46 Batch  129 / 525  Training Loss  1.6430722098448314e-05\n",
            "Epoch  46 Batch  130 / 525  Training Loss  9.209221389028244e-06\n",
            "Epoch  46 Batch  131 / 525  Training Loss  1.1677870134008117e-05\n",
            "Epoch  46 Batch  132 / 525  Training Loss  1.3016566299484111e-05\n",
            "Epoch  46 Batch  133 / 525  Training Loss  1.3317633602127898e-05\n",
            "Epoch  46 Batch  134 / 525  Training Loss  1.3059187040198594e-05\n",
            "Epoch  46 Batch  135 / 525  Training Loss  1.2400285413605161e-05\n",
            "Epoch  46 Batch  136 / 525  Training Loss  1.2625399904209189e-05\n",
            "Epoch  46 Batch  137 / 525  Training Loss  1.535643605166115e-05\n",
            "Epoch  46 Batch  138 / 525  Training Loss  1.5274161341949366e-05\n",
            "Epoch  46 Batch  139 / 525  Training Loss  9.88989540928742e-06\n",
            "Epoch  46 Batch  140 / 525  Training Loss  8.798706403467804e-06\n",
            "Epoch  46 Batch  141 / 525  Training Loss  2.1393836505012587e-05\n",
            "Epoch  46 Batch  142 / 525  Training Loss  1.2477088603191078e-05\n",
            "Epoch  46 Batch  143 / 525  Training Loss  1.2880748727184255e-05\n",
            "Epoch  46 Batch  144 / 525  Training Loss  1.351357059320435e-05\n",
            "Epoch  46 Batch  145 / 525  Training Loss  1.3637354641105048e-05\n",
            "Epoch  46 Batch  146 / 525  Training Loss  1.4944472241040785e-05\n",
            "Epoch  46 Batch  147 / 525  Training Loss  1.2078146028215997e-05\n",
            "Epoch  46 Batch  148 / 525  Training Loss  9.400819180882536e-06\n",
            "Epoch  46 Batch  149 / 525  Training Loss  1.1733349310816266e-05\n",
            "Epoch  46 Batch  150 / 525  Training Loss  7.591867415612796e-06\n",
            "Epoch  46 Batch  151 / 525  Training Loss  1.4977122191339731e-05\n",
            "Epoch  46 Batch  152 / 525  Training Loss  9.546761248202529e-06\n",
            "Epoch  46 Batch  153 / 525  Training Loss  1.380799676553579e-05\n",
            "Epoch  46 Batch  154 / 525  Training Loss  9.869865607470274e-06\n",
            "Epoch  46 Batch  155 / 525  Training Loss  1.044583314069314e-05\n",
            "Epoch  46 Batch  156 / 525  Training Loss  1.2583290299517103e-05\n",
            "Epoch  46 Batch  157 / 525  Training Loss  1.707975570752751e-05\n",
            "Epoch  46 Batch  158 / 525  Training Loss  1.4570605344488285e-05\n",
            "Epoch  46 Batch  159 / 525  Training Loss  1.8393078789813444e-05\n",
            "Epoch  46 Batch  160 / 525  Training Loss  1.3799185580865014e-05\n",
            "Epoch  46 Batch  161 / 525  Training Loss  1.3788905562250875e-05\n",
            "Epoch  46 Batch  162 / 525  Training Loss  1.2461750884540379e-05\n",
            "Epoch  46 Batch  163 / 525  Training Loss  9.974771273846272e-06\n",
            "Epoch  46 Batch  164 / 525  Training Loss  1.3855629731551744e-05\n",
            "Epoch  46 Batch  165 / 525  Training Loss  8.451896064798348e-06\n",
            "Epoch  46 Batch  166 / 525  Training Loss  1.6750951544963755e-05\n",
            "Epoch  46 Batch  167 / 525  Training Loss  1.92767838598229e-05\n",
            "Epoch  46 Batch  168 / 525  Training Loss  1.4531990927935112e-05\n",
            "Epoch  46 Batch  169 / 525  Training Loss  8.570263162255287e-06\n",
            "Epoch  46 Batch  170 / 525  Training Loss  1.4276281945058145e-05\n",
            "Epoch  46 Batch  171 / 525  Training Loss  1.5648925909772515e-05\n",
            "Epoch  46 Batch  172 / 525  Training Loss  9.734465493238531e-06\n",
            "Epoch  46 Batch  173 / 525  Training Loss  1.1078313036705367e-05\n",
            "Epoch  46 Batch  174 / 525  Training Loss  8.412474926444702e-06\n",
            "Epoch  46 Batch  175 / 525  Training Loss  1.0606327123241499e-05\n",
            "Epoch  46 Batch  176 / 525  Training Loss  1.7641092199482955e-05\n",
            "Epoch  46 Batch  177 / 525  Training Loss  1.4753362847841345e-05\n",
            "Epoch  46 Batch  178 / 525  Training Loss  1.0312274753232487e-05\n",
            "Epoch  46 Batch  179 / 525  Training Loss  1.3582897736341693e-05\n",
            "Epoch  46 Batch  180 / 525  Training Loss  1.334079570369795e-05\n",
            "Epoch  46 Batch  181 / 525  Training Loss  8.261066795967054e-06\n",
            "Epoch  46 Batch  182 / 525  Training Loss  7.383723186649149e-06\n",
            "Epoch  46 Batch  183 / 525  Training Loss  1.6223415514105e-05\n",
            "Epoch  46 Batch  184 / 525  Training Loss  1.5062653801578563e-05\n",
            "Epoch  46 Batch  185 / 525  Training Loss  1.4144568012852687e-05\n",
            "Epoch  46 Batch  186 / 525  Training Loss  1.672006692388095e-05\n",
            "Epoch  46 Batch  187 / 525  Training Loss  9.147361197392456e-06\n",
            "Epoch  46 Batch  188 / 525  Training Loss  1.1903041922778357e-05\n",
            "Epoch  46 Batch  189 / 525  Training Loss  1.683640221017413e-05\n",
            "Epoch  46 Batch  190 / 525  Training Loss  9.714856787468307e-06\n",
            "Epoch  46 Batch  191 / 525  Training Loss  1.4181214282871224e-05\n",
            "Epoch  46 Batch  192 / 525  Training Loss  1.4085890143178403e-05\n",
            "Epoch  46 Batch  193 / 525  Training Loss  1.1962419193878304e-05\n",
            "Epoch  46 Batch  194 / 525  Training Loss  1.2152386261732318e-05\n",
            "Epoch  46 Batch  195 / 525  Training Loss  1.805591455195099e-05\n",
            "Epoch  46 Batch  196 / 525  Training Loss  1.4222168829292059e-05\n",
            "Epoch  46 Batch  197 / 525  Training Loss  9.539896382193547e-06\n",
            "Epoch  46 Batch  198 / 525  Training Loss  8.204442565329373e-06\n",
            "Epoch  46 Batch  199 / 525  Training Loss  1.199010330310557e-05\n",
            "Epoch  46 Batch  200 / 525  Training Loss  1.6204192434088327e-05\n",
            "Epoch  46 Batch  201 / 525  Training Loss  5.870529548701597e-06\n",
            "Epoch  46 Batch  202 / 525  Training Loss  1.367412187391892e-05\n",
            "Epoch  46 Batch  203 / 525  Training Loss  1.8110489691025577e-05\n",
            "Epoch  46 Batch  204 / 525  Training Loss  1.2921065717819147e-05\n",
            "Epoch  46 Batch  205 / 525  Training Loss  1.4315917724161409e-05\n",
            "Epoch  46 Batch  206 / 525  Training Loss  1.5896115655777976e-05\n",
            "Epoch  46 Batch  207 / 525  Training Loss  1.1292238923488185e-05\n",
            "Epoch  46 Batch  208 / 525  Training Loss  1.6073312508524396e-05\n",
            "Epoch  46 Batch  209 / 525  Training Loss  1.4773568182135932e-05\n",
            "Epoch  46 Batch  210 / 525  Training Loss  1.5546818758593872e-05\n",
            "Epoch  46 Batch  211 / 525  Training Loss  1.6072301150416024e-05\n",
            "Epoch  46 Batch  212 / 525  Training Loss  9.555709766573273e-06\n",
            "Epoch  46 Batch  213 / 525  Training Loss  9.851312825048808e-06\n",
            "Epoch  46 Batch  214 / 525  Training Loss  1.6720803614589386e-05\n",
            "Epoch  46 Batch  215 / 525  Training Loss  1.3576142009696923e-05\n",
            "Epoch  46 Batch  216 / 525  Training Loss  1.4661282875749748e-05\n",
            "Epoch  46 Batch  217 / 525  Training Loss  9.931211025104858e-06\n",
            "Epoch  46 Batch  218 / 525  Training Loss  1.2363927453407086e-05\n",
            "Epoch  46 Batch  219 / 525  Training Loss  6.689210749755148e-06\n",
            "Epoch  46 Batch  220 / 525  Training Loss  2.010118078032974e-05\n",
            "Epoch  46 Batch  221 / 525  Training Loss  1.09143265945022e-05\n",
            "Epoch  46 Batch  222 / 525  Training Loss  9.552482879371382e-06\n",
            "Epoch  46 Batch  223 / 525  Training Loss  2.12608247238677e-05\n",
            "Epoch  46 Batch  224 / 525  Training Loss  9.913403118844144e-06\n",
            "Epoch  46 Batch  225 / 525  Training Loss  1.7392711015418172e-05\n",
            "Epoch  46 Batch  226 / 525  Training Loss  1.332245119556319e-05\n",
            "Epoch  46 Batch  227 / 525  Training Loss  1.3307037988852244e-05\n",
            "Epoch  46 Batch  228 / 525  Training Loss  1.1736628039216157e-05\n",
            "Epoch  46 Batch  229 / 525  Training Loss  1.1919169992324896e-05\n",
            "Epoch  46 Batch  230 / 525  Training Loss  1.4963488865760155e-05\n",
            "Epoch  46 Batch  231 / 525  Training Loss  1.4441020539379679e-05\n",
            "Epoch  46 Batch  232 / 525  Training Loss  1.9292545403004624e-05\n",
            "Epoch  46 Batch  233 / 525  Training Loss  1.3671737178810872e-05\n",
            "Epoch  46 Batch  234 / 525  Training Loss  1.785312451829668e-05\n",
            "Epoch  46 Batch  235 / 525  Training Loss  1.0217672752332874e-05\n",
            "Epoch  46 Batch  236 / 525  Training Loss  4.8871834223973565e-06\n",
            "Epoch  46 Batch  237 / 525  Training Loss  1.0951241165457759e-05\n",
            "Epoch  46 Batch  238 / 525  Training Loss  1.5026582332211547e-05\n",
            "Epoch  46 Batch  239 / 525  Training Loss  1.1899136552528944e-05\n",
            "Epoch  46 Batch  240 / 525  Training Loss  1.2907245945825707e-05\n",
            "Epoch  46 Batch  241 / 525  Training Loss  1.2340086868789513e-05\n",
            "Epoch  46 Batch  242 / 525  Training Loss  1.2749454072036315e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  46 Batch  243 / 525  Training Loss  1.4406813534151297e-05\n",
            "Epoch  46 Batch  244 / 525  Training Loss  1.1969504157605115e-05\n",
            "Epoch  46 Batch  245 / 525  Training Loss  2.4699384084669873e-05\n",
            "Epoch  46 Batch  246 / 525  Training Loss  1.3940972166892607e-05\n",
            "Epoch  46 Batch  247 / 525  Training Loss  1.2241554031788837e-05\n",
            "Epoch  46 Batch  248 / 525  Training Loss  1.4337124412122648e-05\n",
            "Epoch  46 Batch  249 / 525  Training Loss  8.799690476735123e-06\n",
            "Epoch  46 Batch  250 / 525  Training Loss  1.6469402908114716e-05\n",
            "Epoch  46 Batch  251 / 525  Training Loss  1.5715226254542358e-05\n",
            "Epoch  46 Batch  252 / 525  Training Loss  1.2725558008241933e-05\n",
            "Epoch  46 Batch  253 / 525  Training Loss  1.581850301590748e-05\n",
            "Epoch  46 Batch  254 / 525  Training Loss  1.7586455214768648e-05\n",
            "Epoch  46 Batch  255 / 525  Training Loss  1.2596943633980118e-05\n",
            "Epoch  46 Batch  256 / 525  Training Loss  9.239249266101979e-06\n",
            "Epoch  46 Batch  257 / 525  Training Loss  1.8739003280643374e-05\n",
            "Epoch  46 Batch  258 / 525  Training Loss  1.816974690882489e-05\n",
            "Epoch  46 Batch  259 / 525  Training Loss  1.5864432498347014e-05\n",
            "Epoch  46 Batch  260 / 525  Training Loss  1.332083502347814e-05\n",
            "Epoch  46 Batch  261 / 525  Training Loss  1.2167065506218933e-05\n",
            "Epoch  46 Batch  262 / 525  Training Loss  1.1977066606050357e-05\n",
            "Epoch  46 Batch  263 / 525  Training Loss  8.893047379388008e-06\n",
            "Epoch  46 Batch  264 / 525  Training Loss  1.165553567261668e-05\n",
            "Epoch  46 Batch  265 / 525  Training Loss  1.5786657968419604e-05\n",
            "Epoch  46 Batch  266 / 525  Training Loss  1.7033569747582078e-05\n",
            "Epoch  46 Batch  267 / 525  Training Loss  1.3524993846658617e-05\n",
            "Epoch  46 Batch  268 / 525  Training Loss  1.3322237464308273e-05\n",
            "Epoch  46 Batch  269 / 525  Training Loss  2.3906852220534347e-05\n",
            "Epoch  46 Batch  270 / 525  Training Loss  1.1585335414565634e-05\n",
            "Epoch  46 Batch  271 / 525  Training Loss  1.0110733455803711e-05\n",
            "Epoch  46 Batch  272 / 525  Training Loss  9.243134627467953e-06\n",
            "Epoch  46 Batch  273 / 525  Training Loss  1.1650002306851093e-05\n",
            "Epoch  46 Batch  274 / 525  Training Loss  9.594555194780696e-06\n",
            "Epoch  46 Batch  275 / 525  Training Loss  1.7335718439426273e-05\n",
            "Epoch  46 Batch  276 / 525  Training Loss  7.346318398049334e-06\n",
            "Epoch  46 Batch  277 / 525  Training Loss  1.2581445844261907e-05\n",
            "Epoch  46 Batch  278 / 525  Training Loss  1.3183607734390534e-05\n",
            "Epoch  46 Batch  279 / 525  Training Loss  1.4566323443432339e-05\n",
            "Epoch  46 Batch  280 / 525  Training Loss  1.3051876521785744e-05\n",
            "Epoch  46 Batch  281 / 525  Training Loss  1.47601622302318e-05\n",
            "Epoch  46 Batch  282 / 525  Training Loss  1.552010144223459e-05\n",
            "Epoch  46 Batch  283 / 525  Training Loss  1.3803762158204336e-05\n",
            "Epoch  46 Batch  284 / 525  Training Loss  1.0872982784349006e-05\n",
            "Epoch  46 Batch  285 / 525  Training Loss  9.757510269992054e-06\n",
            "Epoch  46 Batch  286 / 525  Training Loss  1.492056435381528e-05\n",
            "Epoch  46 Batch  287 / 525  Training Loss  1.497426455898676e-05\n",
            "Epoch  46 Batch  288 / 525  Training Loss  1.0165625099034514e-05\n",
            "Epoch  46 Batch  289 / 525  Training Loss  1.411387165717315e-05\n",
            "Epoch  46 Batch  290 / 525  Training Loss  1.7812661099014804e-05\n",
            "Epoch  46 Batch  291 / 525  Training Loss  1.6603054973529652e-05\n",
            "Epoch  46 Batch  292 / 525  Training Loss  1.2936645362060517e-05\n",
            "Epoch  46 Batch  293 / 525  Training Loss  1.3517720617528539e-05\n",
            "Epoch  46 Batch  294 / 525  Training Loss  1.1306857231829781e-05\n",
            "Epoch  46 Batch  295 / 525  Training Loss  2.0101218979107216e-05\n",
            "Epoch  46 Batch  296 / 525  Training Loss  1.3246151866042055e-05\n",
            "Epoch  46 Batch  297 / 525  Training Loss  1.3704280718229711e-05\n",
            "Epoch  46 Batch  298 / 525  Training Loss  1.4566001482307911e-05\n",
            "Epoch  46 Batch  299 / 525  Training Loss  8.748125765123405e-06\n",
            "Epoch  46 Batch  300 / 525  Training Loss  2.127163315890357e-05\n",
            "Epoch  46 Batch  301 / 525  Training Loss  1.951030753843952e-05\n",
            "Epoch  46 Batch  302 / 525  Training Loss  1.6431606127298437e-05\n",
            "Epoch  46 Batch  303 / 525  Training Loss  1.1039690434699878e-05\n",
            "Epoch  46 Batch  304 / 525  Training Loss  2.0019950170535594e-05\n",
            "Epoch  46 Batch  305 / 525  Training Loss  1.3690363630303182e-05\n",
            "Epoch  46 Batch  306 / 525  Training Loss  1.4141248357191216e-05\n",
            "Epoch  46 Batch  307 / 525  Training Loss  1.6916204913286492e-05\n",
            "Epoch  46 Batch  308 / 525  Training Loss  1.608270758879371e-05\n",
            "Epoch  46 Batch  309 / 525  Training Loss  7.847823326301295e-06\n",
            "Epoch  46 Batch  310 / 525  Training Loss  1.0857342203962617e-05\n",
            "Epoch  46 Batch  311 / 525  Training Loss  8.482094017381314e-06\n",
            "Epoch  46 Batch  312 / 525  Training Loss  1.726544178382028e-05\n",
            "Epoch  46 Batch  313 / 525  Training Loss  1.3843523447576445e-05\n",
            "Epoch  46 Batch  314 / 525  Training Loss  7.222566637210548e-06\n",
            "Epoch  46 Batch  315 / 525  Training Loss  1.3354557268030476e-05\n",
            "Epoch  46 Batch  316 / 525  Training Loss  1.2650640201172791e-05\n",
            "Epoch  46 Batch  317 / 525  Training Loss  1.1647227438515984e-05\n",
            "Epoch  46 Batch  318 / 525  Training Loss  1.6901632989174686e-05\n",
            "Epoch  46 Batch  319 / 525  Training Loss  1.8359056412009522e-05\n",
            "Epoch  46 Batch  320 / 525  Training Loss  1.8045835531665944e-05\n",
            "Epoch  46 Batch  321 / 525  Training Loss  1.1611719855864067e-05\n",
            "Epoch  46 Batch  322 / 525  Training Loss  1.0511661457712762e-05\n",
            "Epoch  46 Batch  323 / 525  Training Loss  1.0078650120703969e-05\n",
            "Epoch  46 Batch  324 / 525  Training Loss  1.5749252270325087e-05\n",
            "Epoch  46 Batch  325 / 525  Training Loss  1.576099020894617e-05\n",
            "Epoch  46 Batch  326 / 525  Training Loss  9.24644700717181e-06\n",
            "Epoch  46 Batch  327 / 525  Training Loss  1.013849578157533e-05\n",
            "Epoch  46 Batch  328 / 525  Training Loss  1.2044789400533773e-05\n",
            "Epoch  46 Batch  329 / 525  Training Loss  9.427571967535187e-06\n",
            "Epoch  46 Batch  330 / 525  Training Loss  1.2083293768228032e-05\n",
            "Epoch  46 Batch  331 / 525  Training Loss  1.3889830370317213e-05\n",
            "Epoch  46 Batch  332 / 525  Training Loss  1.1266729416092858e-05\n",
            "Epoch  46 Batch  333 / 525  Training Loss  1.2011571016046219e-05\n",
            "Epoch  46 Batch  334 / 525  Training Loss  1.2222060831845738e-05\n",
            "Epoch  46 Batch  335 / 525  Training Loss  2.4036076865741052e-05\n",
            "Epoch  46 Batch  336 / 525  Training Loss  7.494576948374743e-06\n",
            "Epoch  46 Batch  337 / 525  Training Loss  1.6137968486873433e-05\n",
            "Epoch  46 Batch  338 / 525  Training Loss  1.3146775017958134e-05\n",
            "Epoch  46 Batch  339 / 525  Training Loss  1.1919282769667916e-05\n",
            "Epoch  46 Batch  340 / 525  Training Loss  9.808919458009768e-06\n",
            "Epoch  46 Batch  341 / 525  Training Loss  1.420241915184306e-05\n",
            "Epoch  46 Batch  342 / 525  Training Loss  1.1884019841090776e-05\n",
            "Epoch  46 Batch  343 / 525  Training Loss  2.1583546185865998e-05\n",
            "Epoch  46 Batch  344 / 525  Training Loss  1.579785930516664e-05\n",
            "Epoch  46 Batch  345 / 525  Training Loss  1.263269496121211e-05\n",
            "Epoch  46 Batch  346 / 525  Training Loss  1.7494894564151764e-05\n",
            "Epoch  46 Batch  347 / 525  Training Loss  1.4476655451289844e-05\n",
            "Epoch  46 Batch  348 / 525  Training Loss  1.5008619811851531e-05\n",
            "Epoch  46 Batch  349 / 525  Training Loss  1.2380594853311777e-05\n",
            "Epoch  46 Batch  350 / 525  Training Loss  1.338292804575758e-05\n",
            "Epoch  46 Batch  351 / 525  Training Loss  1.0868659046536777e-05\n",
            "Epoch  46 Batch  352 / 525  Training Loss  7.721175279584713e-06\n",
            "Epoch  46 Batch  353 / 525  Training Loss  1.1531174095580354e-05\n",
            "Epoch  46 Batch  354 / 525  Training Loss  7.906181053840555e-06\n",
            "Epoch  46 Batch  355 / 525  Training Loss  1.8274056856171228e-05\n",
            "Epoch  46 Batch  356 / 525  Training Loss  8.86050929693738e-06\n",
            "Epoch  46 Batch  357 / 525  Training Loss  1.8514718249207363e-05\n",
            "Epoch  46 Batch  358 / 525  Training Loss  7.5763941822515335e-06\n",
            "Epoch  46 Batch  359 / 525  Training Loss  7.14748148311628e-06\n",
            "Epoch  46 Batch  360 / 525  Training Loss  1.048786270985147e-05\n",
            "Epoch  46 Batch  361 / 525  Training Loss  1.3562504136643838e-05\n",
            "Epoch  46 Batch  362 / 525  Training Loss  1.5910718502709642e-05\n",
            "Epoch  46 Batch  363 / 525  Training Loss  5.396212600317085e-06\n",
            "Epoch  46 Batch  364 / 525  Training Loss  8.496848749928176e-06\n",
            "Epoch  46 Batch  365 / 525  Training Loss  1.175608213088708e-05\n",
            "Epoch  46 Batch  366 / 525  Training Loss  1.0484568520041648e-05\n",
            "Epoch  46 Batch  367 / 525  Training Loss  1.691198667685967e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  46 Batch  368 / 525  Training Loss  1.5050451111164875e-05\n",
            "Epoch  46 Batch  369 / 525  Training Loss  1.5671286746510305e-05\n",
            "Epoch  46 Batch  370 / 525  Training Loss  5.624308869300876e-06\n",
            "Epoch  46 Batch  371 / 525  Training Loss  1.8328684745938517e-05\n",
            "Epoch  46 Batch  372 / 525  Training Loss  1.0172096153837629e-05\n",
            "Epoch  46 Batch  373 / 525  Training Loss  1.8527065549278632e-05\n",
            "Epoch  46 Batch  374 / 525  Training Loss  1.2259763025213033e-05\n",
            "Epoch  46 Batch  375 / 525  Training Loss  1.219622754433658e-05\n",
            "Epoch  46 Batch  376 / 525  Training Loss  1.336183322564466e-05\n",
            "Epoch  46 Batch  377 / 525  Training Loss  2.063278589048423e-05\n",
            "Epoch  46 Batch  378 / 525  Training Loss  1.1773745427490212e-05\n",
            "Epoch  46 Batch  379 / 525  Training Loss  9.220837455359288e-06\n",
            "Epoch  46 Batch  380 / 525  Training Loss  1.447912109142635e-05\n",
            "Epoch  46 Batch  381 / 525  Training Loss  1.4683658264402766e-05\n",
            "Epoch  46 Batch  382 / 525  Training Loss  1.623522439331282e-05\n",
            "Epoch  46 Batch  383 / 525  Training Loss  2.4608007151982747e-05\n",
            "Epoch  46 Batch  384 / 525  Training Loss  1.463974876969587e-05\n",
            "Epoch  46 Batch  385 / 525  Training Loss  1.6595824490650557e-05\n",
            "Epoch  46 Batch  386 / 525  Training Loss  2.058529935311526e-05\n",
            "Epoch  46 Batch  387 / 525  Training Loss  7.547484074166277e-06\n",
            "Epoch  46 Batch  388 / 525  Training Loss  1.3653901987709105e-05\n",
            "Epoch  46 Batch  389 / 525  Training Loss  6.56917200103635e-06\n",
            "Epoch  46 Batch  390 / 525  Training Loss  9.549026799504645e-06\n",
            "Epoch  46 Batch  391 / 525  Training Loss  1.499913923908025e-05\n",
            "Epoch  46 Batch  392 / 525  Training Loss  1.629704092920292e-05\n",
            "Epoch  46 Batch  393 / 525  Training Loss  1.450930540158879e-05\n",
            "Epoch  46 Batch  394 / 525  Training Loss  1.6686262824805453e-05\n",
            "Epoch  46 Batch  395 / 525  Training Loss  1.5197362699836958e-05\n",
            "Epoch  46 Batch  396 / 525  Training Loss  1.755543053150177e-05\n",
            "Epoch  46 Batch  397 / 525  Training Loss  1.4159624697640538e-05\n",
            "Epoch  46 Batch  398 / 525  Training Loss  1.7338365069008432e-05\n",
            "Epoch  46 Batch  399 / 525  Training Loss  1.3277819562063087e-05\n",
            "Epoch  46 Batch  400 / 525  Training Loss  1.0881152775255032e-05\n",
            "Epoch  46 Batch  401 / 525  Training Loss  1.5844878362258896e-05\n",
            "Epoch  46 Batch  402 / 525  Training Loss  1.0149748959520366e-05\n",
            "Epoch  46 Batch  403 / 525  Training Loss  9.621811841498129e-06\n",
            "Epoch  46 Batch  404 / 525  Training Loss  1.766398963809479e-05\n",
            "Epoch  46 Batch  405 / 525  Training Loss  1.4542261851602234e-05\n",
            "Epoch  46 Batch  406 / 525  Training Loss  1.3957635019323789e-05\n",
            "Epoch  46 Batch  407 / 525  Training Loss  1.524318940937519e-05\n",
            "Epoch  46 Batch  408 / 525  Training Loss  1.8769604139379226e-05\n",
            "Epoch  46 Batch  409 / 525  Training Loss  1.614904249436222e-05\n",
            "Epoch  46 Batch  410 / 525  Training Loss  9.53542985371314e-06\n",
            "Epoch  46 Batch  411 / 525  Training Loss  1.2575203072628938e-05\n",
            "Epoch  46 Batch  412 / 525  Training Loss  1.9095994503004476e-05\n",
            "Epoch  46 Batch  413 / 525  Training Loss  7.1295967245532665e-06\n",
            "Epoch  46 Batch  414 / 525  Training Loss  1.116175735660363e-05\n",
            "Epoch  46 Batch  415 / 525  Training Loss  1.2943976798851509e-05\n",
            "Epoch  46 Batch  416 / 525  Training Loss  1.2296759450691752e-05\n",
            "Epoch  46 Batch  417 / 525  Training Loss  1.566214268677868e-05\n",
            "Epoch  46 Batch  418 / 525  Training Loss  1.4103954526945017e-05\n",
            "Epoch  46 Batch  419 / 525  Training Loss  1.3591161405202001e-05\n",
            "Epoch  46 Batch  420 / 525  Training Loss  1.2603707546077203e-05\n",
            "Epoch  46 Batch  421 / 525  Training Loss  1.6644757124595344e-05\n",
            "Epoch  46 Batch  422 / 525  Training Loss  1.1475685823825188e-05\n",
            "Epoch  46 Batch  423 / 525  Training Loss  2.2451467884820886e-05\n",
            "Epoch  46 Batch  424 / 525  Training Loss  1.5117363545869011e-05\n",
            "Epoch  46 Batch  425 / 525  Training Loss  1.529430301161483e-05\n",
            "Epoch  46 Batch  426 / 525  Training Loss  1.0229368854197673e-05\n",
            "Epoch  46 Batch  427 / 525  Training Loss  1.3416247384157032e-05\n",
            "Epoch  46 Batch  428 / 525  Training Loss  1.1028414519387297e-05\n",
            "Epoch  46 Batch  429 / 525  Training Loss  1.0359161024098285e-05\n",
            "Epoch  46 Batch  430 / 525  Training Loss  1.64588782354258e-05\n",
            "Epoch  46 Batch  431 / 525  Training Loss  1.5490519217564724e-05\n",
            "Epoch  46 Batch  432 / 525  Training Loss  1.806747604859993e-05\n",
            "Epoch  46 Batch  433 / 525  Training Loss  1.5306644854717888e-05\n",
            "Epoch  46 Batch  434 / 525  Training Loss  1.6698317267582752e-05\n",
            "Epoch  46 Batch  435 / 525  Training Loss  1.847923340392299e-05\n",
            "Epoch  46 Batch  436 / 525  Training Loss  1.288399380428018e-05\n",
            "Epoch  46 Batch  437 / 525  Training Loss  1.6607638826826587e-05\n",
            "Epoch  46 Batch  438 / 525  Training Loss  1.1653884939732961e-05\n",
            "Epoch  46 Batch  439 / 525  Training Loss  1.0411284165456891e-05\n",
            "Epoch  46 Batch  440 / 525  Training Loss  1.4293366803030949e-05\n",
            "Epoch  46 Batch  441 / 525  Training Loss  1.0862097042263485e-05\n",
            "Epoch  46 Batch  442 / 525  Training Loss  1.7513670172775164e-05\n",
            "Epoch  46 Batch  443 / 525  Training Loss  1.0249919796478935e-05\n",
            "Epoch  46 Batch  444 / 525  Training Loss  5.731839337386191e-06\n",
            "Epoch  46 Batch  445 / 525  Training Loss  9.223969755112194e-06\n",
            "Epoch  46 Batch  446 / 525  Training Loss  9.525077075522859e-06\n",
            "Epoch  46 Batch  447 / 525  Training Loss  2.1205254597589374e-05\n",
            "Epoch  46 Batch  448 / 525  Training Loss  9.625617167330347e-06\n",
            "Epoch  46 Batch  449 / 525  Training Loss  1.8084876501234248e-05\n",
            "Epoch  46 Batch  450 / 525  Training Loss  1.6550475265830755e-05\n",
            "Epoch  46 Batch  451 / 525  Training Loss  1.331044768448919e-05\n",
            "Epoch  46 Batch  452 / 525  Training Loss  1.4912542610545643e-05\n",
            "Epoch  46 Batch  453 / 525  Training Loss  1.3996172128827311e-05\n",
            "Epoch  46 Batch  454 / 525  Training Loss  1.940870424732566e-05\n",
            "Epoch  46 Batch  455 / 525  Training Loss  9.884667633741628e-06\n",
            "Epoch  46 Batch  456 / 525  Training Loss  1.1677124348352663e-05\n",
            "Epoch  46 Batch  457 / 525  Training Loss  1.4471527720161248e-05\n",
            "Epoch  46 Batch  458 / 525  Training Loss  1.2947336472279858e-05\n",
            "Epoch  46 Batch  459 / 525  Training Loss  1.7715286958264187e-05\n",
            "Epoch  46 Batch  460 / 525  Training Loss  1.5972449546097778e-05\n",
            "Epoch  46 Batch  461 / 525  Training Loss  8.976844583230559e-06\n",
            "Epoch  46 Batch  462 / 525  Training Loss  1.423146477463888e-05\n",
            "Epoch  46 Batch  463 / 525  Training Loss  2.1449126506922767e-05\n",
            "Epoch  46 Batch  464 / 525  Training Loss  1.7592265066923574e-05\n",
            "Epoch  46 Batch  465 / 525  Training Loss  9.845522981777322e-06\n",
            "Epoch  46 Batch  466 / 525  Training Loss  1.9809593140962534e-05\n",
            "Epoch  46 Batch  467 / 525  Training Loss  1.3912891517975368e-05\n",
            "Epoch  46 Batch  468 / 525  Training Loss  1.393576712871436e-05\n",
            "Epoch  46 Batch  469 / 525  Training Loss  1.663595139689278e-05\n",
            "Epoch  46 Batch  470 / 525  Training Loss  1.1191812518518418e-05\n",
            "Epoch  46 Batch  471 / 525  Training Loss  7.85164229455404e-06\n",
            "Epoch  46 Batch  472 / 525  Training Loss  1.6009511455195025e-05\n",
            "Epoch  46 Batch  473 / 525  Training Loss  1.633343708817847e-05\n",
            "Epoch  46 Batch  474 / 525  Training Loss  1.1628911124716979e-05\n",
            "Epoch  46 Batch  475 / 525  Training Loss  1.5173223800957203e-05\n",
            "Epoch  46 Batch  476 / 525  Training Loss  8.439130397164263e-06\n",
            "Epoch  46 Batch  477 / 525  Training Loss  1.714215977699496e-05\n",
            "Epoch  46 Batch  478 / 525  Training Loss  1.645197335164994e-05\n",
            "Epoch  46 Batch  479 / 525  Training Loss  1.838357093220111e-05\n",
            "Epoch  46 Batch  480 / 525  Training Loss  1.926468212332111e-05\n",
            "Epoch  46 Batch  481 / 525  Training Loss  1.6360376321244985e-05\n",
            "Epoch  46 Batch  482 / 525  Training Loss  1.4246063074097037e-05\n",
            "Epoch  46 Batch  483 / 525  Training Loss  1.9319790226290934e-05\n",
            "Epoch  46 Batch  484 / 525  Training Loss  1.1701200492097996e-05\n",
            "Epoch  46 Batch  485 / 525  Training Loss  1.192890886159148e-05\n",
            "Epoch  46 Batch  486 / 525  Training Loss  1.2498468095145654e-05\n",
            "Epoch  46 Batch  487 / 525  Training Loss  1.166386209661141e-05\n",
            "Epoch  46 Batch  488 / 525  Training Loss  1.5077178431965876e-05\n",
            "Epoch  46 Batch  489 / 525  Training Loss  1.7060257960110903e-05\n",
            "Epoch  46 Batch  490 / 525  Training Loss  1.4125143934506923e-05\n",
            "Epoch  46 Batch  491 / 525  Training Loss  1.2910742952954024e-05\n",
            "Epoch  46 Batch  492 / 525  Training Loss  7.252654086187249e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  46 Batch  493 / 525  Training Loss  7.324821581278229e-06\n",
            "Epoch  46 Batch  494 / 525  Training Loss  1.0368576113251038e-05\n",
            "Epoch  46 Batch  495 / 525  Training Loss  1.2034042811137624e-05\n",
            "Epoch  46 Batch  496 / 525  Training Loss  1.0667434253264219e-05\n",
            "Epoch  46 Batch  497 / 525  Training Loss  1.278600211662706e-05\n",
            "Epoch  46 Batch  498 / 525  Training Loss  1.1134028682135977e-05\n",
            "Epoch  46 Batch  499 / 525  Training Loss  8.049467396631371e-06\n",
            "Epoch  46 Batch  500 / 525  Training Loss  1.1454461855464615e-05\n",
            "Epoch  46 Batch  501 / 525  Training Loss  1.060034628608264e-05\n",
            "Epoch  46 Batch  502 / 525  Training Loss  1.2510192391346209e-05\n",
            "Epoch  46 Batch  503 / 525  Training Loss  1.5085841369000264e-05\n",
            "Epoch  46 Batch  504 / 525  Training Loss  1.0391462637926452e-05\n",
            "Epoch  46 Batch  505 / 525  Training Loss  1.2662215340242255e-05\n",
            "Epoch  46 Batch  506 / 525  Training Loss  1.0272573490510695e-05\n",
            "Epoch  46 Batch  507 / 525  Training Loss  1.588206214364618e-05\n",
            "Epoch  46 Batch  508 / 525  Training Loss  1.6712929209461436e-05\n",
            "Epoch  46 Batch  509 / 525  Training Loss  1.689664350124076e-05\n",
            "Epoch  46 Batch  510 / 525  Training Loss  1.1149514648423065e-05\n",
            "Epoch  46 Batch  511 / 525  Training Loss  1.9981813238700852e-05\n",
            "Epoch  46 Batch  512 / 525  Training Loss  1.6387681171181612e-05\n",
            "Epoch  46 Batch  513 / 525  Training Loss  1.7874734112410806e-05\n",
            "Epoch  46 Batch  514 / 525  Training Loss  1.5750727470731363e-05\n",
            "Epoch  46 Batch  515 / 525  Training Loss  1.1102819371444639e-05\n",
            "Epoch  46 Batch  516 / 525  Training Loss  1.3047727406956255e-05\n",
            "Epoch  46 Batch  517 / 525  Training Loss  1.1729551260941662e-05\n",
            "Epoch  46 Batch  518 / 525  Training Loss  1.7467271391069517e-05\n",
            "Epoch  46 Batch  519 / 525  Training Loss  1.2012530532956589e-05\n",
            "Epoch  46 Batch  520 / 525  Training Loss  8.641302883916069e-06\n",
            "Epoch  46 Batch  521 / 525  Training Loss  1.5481291484320536e-05\n",
            "Epoch  46 Batch  522 / 525  Training Loss  9.893470632960089e-06\n",
            "Epoch  46 Batch  523 / 525  Training Loss  8.65162473928649e-06\n",
            "Epoch  46 Batch  524 / 525  Training Loss  1.9732517102966085e-05\n",
            "  47    |    -    |   0.000014   | 64.608333\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 47\n",
            "Epoch  47 Batch  0 / 525  Training Loss  7.292203918041196e-06\n",
            "Epoch  47 Batch  1 / 525  Training Loss  1.7256646970054135e-05\n",
            "Epoch  47 Batch  2 / 525  Training Loss  1.0451684829604346e-05\n",
            "Epoch  47 Batch  3 / 525  Training Loss  1.7176222172565758e-05\n",
            "Epoch  47 Batch  4 / 525  Training Loss  1.7492284314357676e-05\n",
            "Epoch  47 Batch  5 / 525  Training Loss  9.377503374707885e-06\n",
            "Epoch  47 Batch  6 / 525  Training Loss  1.8686438124859706e-05\n",
            "Epoch  47 Batch  7 / 525  Training Loss  5.933215561526595e-06\n",
            "Epoch  47 Batch  8 / 525  Training Loss  1.7467074940213934e-05\n",
            "Epoch  47 Batch  9 / 525  Training Loss  1.0811731044668704e-05\n",
            "Epoch  47 Batch  10 / 525  Training Loss  1.1849895599880256e-05\n",
            "Epoch  47 Batch  11 / 525  Training Loss  7.795990313752554e-06\n",
            "Epoch  47 Batch  12 / 525  Training Loss  1.545761915622279e-05\n",
            "Epoch  47 Batch  13 / 525  Training Loss  1.0529134669923224e-05\n",
            "Epoch  47 Batch  14 / 525  Training Loss  1.1768293916247785e-05\n",
            "Epoch  47 Batch  15 / 525  Training Loss  1.3415218745649327e-05\n",
            "Epoch  47 Batch  16 / 525  Training Loss  1.2300270100240596e-05\n",
            "Epoch  47 Batch  17 / 525  Training Loss  1.425900245521916e-05\n",
            "Epoch  47 Batch  18 / 525  Training Loss  9.843524821917526e-06\n",
            "Epoch  47 Batch  19 / 525  Training Loss  1.1319576515234075e-05\n",
            "Epoch  47 Batch  20 / 525  Training Loss  1.4300423572422005e-05\n",
            "Epoch  47 Batch  21 / 525  Training Loss  1.5653469745302573e-05\n",
            "Epoch  47 Batch  22 / 525  Training Loss  9.754074199008755e-06\n",
            "Epoch  47 Batch  23 / 525  Training Loss  1.4536197340930812e-05\n",
            "Epoch  47 Batch  24 / 525  Training Loss  1.1775069651775993e-05\n",
            "Epoch  47 Batch  25 / 525  Training Loss  1.1249128874624148e-05\n",
            "Epoch  47 Batch  26 / 525  Training Loss  1.3761522495769896e-05\n",
            "Epoch  47 Batch  27 / 525  Training Loss  1.1283734238531906e-05\n",
            "Epoch  47 Batch  28 / 525  Training Loss  1.1145284588565119e-05\n",
            "Epoch  47 Batch  29 / 525  Training Loss  8.416770469921175e-06\n",
            "Epoch  47 Batch  30 / 525  Training Loss  1.511341270088451e-05\n",
            "Epoch  47 Batch  31 / 525  Training Loss  1.2213704394525848e-05\n",
            "Epoch  47 Batch  32 / 525  Training Loss  1.3554247743741143e-05\n",
            "Epoch  47 Batch  33 / 525  Training Loss  1.72886393556837e-05\n",
            "Epoch  47 Batch  34 / 525  Training Loss  1.1402081327105407e-05\n",
            "Epoch  47 Batch  35 / 525  Training Loss  1.1977974281762727e-05\n",
            "Epoch  47 Batch  36 / 525  Training Loss  1.4695424397359602e-05\n",
            "Epoch  47 Batch  37 / 525  Training Loss  1.0452505193825345e-05\n",
            "Epoch  47 Batch  38 / 525  Training Loss  1.488972884544637e-05\n",
            "Epoch  47 Batch  39 / 525  Training Loss  1.4251518223318271e-05\n",
            "Epoch  47 Batch  40 / 525  Training Loss  1.2427393812686205e-05\n",
            "Epoch  47 Batch  41 / 525  Training Loss  1.7764226868166588e-05\n",
            "Epoch  47 Batch  42 / 525  Training Loss  1.0929444215435069e-05\n",
            "Epoch  47 Batch  43 / 525  Training Loss  1.4035062122275122e-05\n",
            "Epoch  47 Batch  44 / 525  Training Loss  1.223668186867144e-05\n",
            "Epoch  47 Batch  45 / 525  Training Loss  1.5164863725658506e-05\n",
            "Epoch  47 Batch  46 / 525  Training Loss  1.1519648069224786e-05\n",
            "Epoch  47 Batch  47 / 525  Training Loss  1.3355673218029551e-05\n",
            "Epoch  47 Batch  48 / 525  Training Loss  1.1724296200554818e-05\n",
            "Epoch  47 Batch  49 / 525  Training Loss  1.5189338228083216e-05\n",
            "Epoch  47 Batch  50 / 525  Training Loss  9.476539162278641e-06\n",
            "Epoch  47 Batch  51 / 525  Training Loss  1.8419261323288083e-05\n",
            "Epoch  47 Batch  52 / 525  Training Loss  1.4542127246386372e-05\n",
            "Epoch  47 Batch  53 / 525  Training Loss  7.412277227558661e-06\n",
            "Epoch  47 Batch  54 / 525  Training Loss  8.951343261287548e-06\n",
            "Epoch  47 Batch  55 / 525  Training Loss  6.261698672460625e-06\n",
            "Epoch  47 Batch  56 / 525  Training Loss  1.2123160558985546e-05\n",
            "Epoch  47 Batch  57 / 525  Training Loss  1.1029688721464481e-05\n",
            "Epoch  47 Batch  58 / 525  Training Loss  1.035596778820036e-05\n",
            "Epoch  47 Batch  59 / 525  Training Loss  1.2539737326733302e-05\n",
            "Epoch  47 Batch  60 / 525  Training Loss  1.7981295968638733e-05\n",
            "Epoch  47 Batch  61 / 525  Training Loss  7.596002888021758e-06\n",
            "Epoch  47 Batch  62 / 525  Training Loss  1.7704318452160805e-05\n",
            "Epoch  47 Batch  63 / 525  Training Loss  8.990445167000871e-06\n",
            "Epoch  47 Batch  64 / 525  Training Loss  1.700669054116588e-05\n",
            "Epoch  47 Batch  65 / 525  Training Loss  1.4293528693087865e-05\n",
            "Epoch  47 Batch  66 / 525  Training Loss  1.0631631084834225e-05\n",
            "Epoch  47 Batch  67 / 525  Training Loss  1.2126938599976711e-05\n",
            "Epoch  47 Batch  68 / 525  Training Loss  1.4330091289593838e-05\n",
            "Epoch  47 Batch  69 / 525  Training Loss  1.0368175935582258e-05\n",
            "Epoch  47 Batch  70 / 525  Training Loss  8.679271559230983e-06\n",
            "Epoch  47 Batch  71 / 525  Training Loss  1.165215508081019e-05\n",
            "Epoch  47 Batch  72 / 525  Training Loss  1.2580871043610387e-05\n",
            "Epoch  47 Batch  73 / 525  Training Loss  1.037268611980835e-05\n",
            "Epoch  47 Batch  74 / 525  Training Loss  1.3511440556612797e-05\n",
            "Epoch  47 Batch  75 / 525  Training Loss  9.059827789315023e-06\n",
            "Epoch  47 Batch  76 / 525  Training Loss  1.0425766049593221e-05\n",
            "Epoch  47 Batch  77 / 525  Training Loss  1.0424022548249923e-05\n",
            "Epoch  47 Batch  78 / 525  Training Loss  1.2672322554863058e-05\n",
            "Epoch  47 Batch  79 / 525  Training Loss  1.7516744264867157e-05\n",
            "Epoch  47 Batch  80 / 525  Training Loss  6.593149009859189e-06\n",
            "Epoch  47 Batch  81 / 525  Training Loss  1.2211661669425666e-05\n",
            "Epoch  47 Batch  82 / 525  Training Loss  1.0798318726301659e-05\n",
            "Epoch  47 Batch  83 / 525  Training Loss  1.2191515452286694e-05\n",
            "Epoch  47 Batch  84 / 525  Training Loss  1.5090037777554244e-05\n",
            "Epoch  47 Batch  85 / 525  Training Loss  1.0952832781185862e-05\n",
            "Epoch  47 Batch  86 / 525  Training Loss  9.257220881409012e-06\n",
            "Epoch  47 Batch  87 / 525  Training Loss  1.3258487342682201e-05\n",
            "Epoch  47 Batch  88 / 525  Training Loss  1.0436855518491939e-05\n",
            "Epoch  47 Batch  89 / 525  Training Loss  8.30226872494677e-06\n",
            "Epoch  47 Batch  90 / 525  Training Loss  1.2361322660581209e-05\n",
            "Epoch  47 Batch  91 / 525  Training Loss  1.2128100934205577e-05\n",
            "Epoch  47 Batch  92 / 525  Training Loss  1.0559275324339978e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  47 Batch  93 / 525  Training Loss  1.3166844837542158e-05\n",
            "Epoch  47 Batch  94 / 525  Training Loss  1.0603946066112258e-05\n",
            "Epoch  47 Batch  95 / 525  Training Loss  1.3215591025073081e-05\n",
            "Epoch  47 Batch  96 / 525  Training Loss  1.1931109838769771e-05\n",
            "Epoch  47 Batch  97 / 525  Training Loss  1.3325150575838052e-05\n",
            "Epoch  47 Batch  98 / 525  Training Loss  1.3417085938272066e-05\n",
            "Epoch  47 Batch  99 / 525  Training Loss  1.9542239897418767e-05\n",
            "Epoch  47 Batch  100 / 525  Training Loss  1.4613261555496138e-05\n",
            "Epoch  47 Batch  101 / 525  Training Loss  1.0982611456711311e-05\n",
            "Epoch  47 Batch  102 / 525  Training Loss  9.666561709309462e-06\n",
            "Epoch  47 Batch  103 / 525  Training Loss  1.1802499102486763e-05\n",
            "Epoch  47 Batch  104 / 525  Training Loss  1.2337116459093522e-05\n",
            "Epoch  47 Batch  105 / 525  Training Loss  1.6161346138687804e-05\n",
            "Epoch  47 Batch  106 / 525  Training Loss  9.150468031293713e-06\n",
            "Epoch  47 Batch  107 / 525  Training Loss  8.307101779791992e-06\n",
            "Epoch  47 Batch  108 / 525  Training Loss  1.4082254892855417e-05\n",
            "Epoch  47 Batch  109 / 525  Training Loss  1.7338552424916998e-05\n",
            "Epoch  47 Batch  110 / 525  Training Loss  1.3959068382973783e-05\n",
            "Epoch  47 Batch  111 / 525  Training Loss  1.235276704392163e-05\n",
            "Epoch  47 Batch  112 / 525  Training Loss  1.6016934750950895e-05\n",
            "Epoch  47 Batch  113 / 525  Training Loss  1.9426564904279076e-05\n",
            "Epoch  47 Batch  114 / 525  Training Loss  7.997215107025113e-06\n",
            "Epoch  47 Batch  115 / 525  Training Loss  1.2970061106898356e-05\n",
            "Epoch  47 Batch  116 / 525  Training Loss  1.5555218851659447e-05\n",
            "Epoch  47 Batch  117 / 525  Training Loss  1.7933358321897686e-05\n",
            "Epoch  47 Batch  118 / 525  Training Loss  1.1010688467649743e-05\n",
            "Epoch  47 Batch  119 / 525  Training Loss  8.271941624116153e-06\n",
            "Epoch  47 Batch  120 / 525  Training Loss  1.3991904779686593e-05\n",
            "Epoch  47 Batch  121 / 525  Training Loss  7.109251782821957e-06\n",
            "Epoch  47 Batch  122 / 525  Training Loss  1.0567054232524242e-05\n",
            "Epoch  47 Batch  123 / 525  Training Loss  9.840704478847329e-06\n",
            "Epoch  47 Batch  124 / 525  Training Loss  1.488995440013241e-05\n",
            "Epoch  47 Batch  125 / 525  Training Loss  1.110965786210727e-05\n",
            "Epoch  47 Batch  126 / 525  Training Loss  1.7687543731881306e-05\n",
            "Epoch  47 Batch  127 / 525  Training Loss  9.42179212870542e-06\n",
            "Epoch  47 Batch  128 / 525  Training Loss  1.259620603377698e-05\n",
            "Epoch  47 Batch  129 / 525  Training Loss  1.8309652659809217e-05\n",
            "Epoch  47 Batch  130 / 525  Training Loss  1.905434146465268e-05\n",
            "Epoch  47 Batch  131 / 525  Training Loss  1.7932292394107208e-05\n",
            "Epoch  47 Batch  132 / 525  Training Loss  1.2566473742481321e-05\n",
            "Epoch  47 Batch  133 / 525  Training Loss  1.6362999303964898e-05\n",
            "Epoch  47 Batch  134 / 525  Training Loss  1.2254111425136216e-05\n",
            "Epoch  47 Batch  135 / 525  Training Loss  1.5006713510956615e-05\n",
            "Epoch  47 Batch  136 / 525  Training Loss  1.3926791325502563e-05\n",
            "Epoch  47 Batch  137 / 525  Training Loss  1.2240428986842744e-05\n",
            "Epoch  47 Batch  138 / 525  Training Loss  1.315545250690775e-05\n",
            "Epoch  47 Batch  139 / 525  Training Loss  9.544002750772052e-06\n",
            "Epoch  47 Batch  140 / 525  Training Loss  1.612231062608771e-05\n",
            "Epoch  47 Batch  141 / 525  Training Loss  1.6300180504913442e-05\n",
            "Epoch  47 Batch  142 / 525  Training Loss  1.0165574167331215e-05\n",
            "Epoch  47 Batch  143 / 525  Training Loss  1.2942844477947801e-05\n",
            "Epoch  47 Batch  144 / 525  Training Loss  9.78356820269255e-06\n",
            "Epoch  47 Batch  145 / 525  Training Loss  1.3018609024584293e-05\n",
            "Epoch  47 Batch  146 / 525  Training Loss  1.7901666069519706e-05\n",
            "Epoch  47 Batch  147 / 525  Training Loss  9.25924086914165e-06\n",
            "Epoch  47 Batch  148 / 525  Training Loss  1.1124651791760698e-05\n",
            "Epoch  47 Batch  149 / 525  Training Loss  8.759679076320026e-06\n",
            "Epoch  47 Batch  150 / 525  Training Loss  8.233621883846354e-06\n",
            "Epoch  47 Batch  151 / 525  Training Loss  8.125722160912119e-06\n",
            "Epoch  47 Batch  152 / 525  Training Loss  1.0842465599125717e-05\n",
            "Epoch  47 Batch  153 / 525  Training Loss  1.3237891835160553e-05\n",
            "Epoch  47 Batch  154 / 525  Training Loss  1.3381967619352508e-05\n",
            "Epoch  47 Batch  155 / 525  Training Loss  1.0595999810902867e-05\n",
            "Epoch  47 Batch  156 / 525  Training Loss  1.3003123058297206e-05\n",
            "Epoch  47 Batch  157 / 525  Training Loss  1.0803292752825655e-05\n",
            "Epoch  47 Batch  158 / 525  Training Loss  1.3550976291298866e-05\n",
            "Epoch  47 Batch  159 / 525  Training Loss  1.817753400246147e-05\n",
            "Epoch  47 Batch  160 / 525  Training Loss  1.9326796973473392e-05\n",
            "Epoch  47 Batch  161 / 525  Training Loss  9.58529744821135e-06\n",
            "Epoch  47 Batch  162 / 525  Training Loss  1.8855072994483635e-05\n",
            "Epoch  47 Batch  163 / 525  Training Loss  9.58975579123944e-06\n",
            "Epoch  47 Batch  164 / 525  Training Loss  1.320664887316525e-05\n",
            "Epoch  47 Batch  165 / 525  Training Loss  9.307647815148812e-06\n",
            "Epoch  47 Batch  166 / 525  Training Loss  1.0306272997695487e-05\n",
            "Epoch  47 Batch  167 / 525  Training Loss  7.835784344933927e-06\n",
            "Epoch  47 Batch  168 / 525  Training Loss  8.969858754426241e-06\n",
            "Epoch  47 Batch  169 / 525  Training Loss  1.5127747246879153e-05\n",
            "Epoch  47 Batch  170 / 525  Training Loss  1.695429273240734e-05\n",
            "Epoch  47 Batch  171 / 525  Training Loss  8.83062784851063e-06\n",
            "Epoch  47 Batch  172 / 525  Training Loss  1.2409330338414293e-05\n",
            "Epoch  47 Batch  173 / 525  Training Loss  1.8860424461308867e-05\n",
            "Epoch  47 Batch  174 / 525  Training Loss  1.5097514733497519e-05\n",
            "Epoch  47 Batch  175 / 525  Training Loss  9.016548574436456e-06\n",
            "Epoch  47 Batch  176 / 525  Training Loss  1.575549686094746e-05\n",
            "Epoch  47 Batch  177 / 525  Training Loss  1.11668205136084e-05\n",
            "Epoch  47 Batch  178 / 525  Training Loss  1.2534941561170854e-05\n",
            "Epoch  47 Batch  179 / 525  Training Loss  1.3028043213125784e-05\n",
            "Epoch  47 Batch  180 / 525  Training Loss  7.4354188654979225e-06\n",
            "Epoch  47 Batch  181 / 525  Training Loss  1.1126102435810026e-05\n",
            "Epoch  47 Batch  182 / 525  Training Loss  2.04932894121157e-05\n",
            "Epoch  47 Batch  183 / 525  Training Loss  9.567490451445337e-06\n",
            "Epoch  47 Batch  184 / 525  Training Loss  6.635765657847514e-06\n",
            "Epoch  47 Batch  185 / 525  Training Loss  1.0413932614028454e-05\n",
            "Epoch  47 Batch  186 / 525  Training Loss  1.6445741493953392e-05\n",
            "Epoch  47 Batch  187 / 525  Training Loss  1.5188947145361453e-05\n",
            "Epoch  47 Batch  188 / 525  Training Loss  1.1892916518263519e-05\n",
            "Epoch  47 Batch  189 / 525  Training Loss  6.3607121774111874e-06\n",
            "Epoch  47 Batch  190 / 525  Training Loss  1.0601701433188282e-05\n",
            "Epoch  47 Batch  191 / 525  Training Loss  1.5338926459662616e-05\n",
            "Epoch  47 Batch  192 / 525  Training Loss  1.9987372070318088e-05\n",
            "Epoch  47 Batch  193 / 525  Training Loss  1.3620754543808289e-05\n",
            "Epoch  47 Batch  194 / 525  Training Loss  1.011632957670372e-05\n",
            "Epoch  47 Batch  195 / 525  Training Loss  7.491301857953658e-06\n",
            "Epoch  47 Batch  196 / 525  Training Loss  1.0035601917479653e-05\n",
            "Epoch  47 Batch  197 / 525  Training Loss  8.690411959833e-06\n",
            "Epoch  47 Batch  198 / 525  Training Loss  1.4279663446359336e-05\n",
            "Epoch  47 Batch  199 / 525  Training Loss  1.5763493138365448e-05\n",
            "Epoch  47 Batch  200 / 525  Training Loss  1.464686374674784e-05\n",
            "Epoch  47 Batch  201 / 525  Training Loss  1.332840201939689e-05\n",
            "Epoch  47 Batch  202 / 525  Training Loss  1.070629423338687e-05\n",
            "Epoch  47 Batch  203 / 525  Training Loss  1.7160911738756113e-05\n",
            "Epoch  47 Batch  204 / 525  Training Loss  5.782894731964916e-06\n",
            "Epoch  47 Batch  205 / 525  Training Loss  4.9863792810356244e-06\n",
            "Epoch  47 Batch  206 / 525  Training Loss  1.3846282854501624e-05\n",
            "Epoch  47 Batch  207 / 525  Training Loss  1.486642304371344e-05\n",
            "Epoch  47 Batch  208 / 525  Training Loss  9.427848453924526e-06\n",
            "Epoch  47 Batch  209 / 525  Training Loss  1.1990361599600874e-05\n",
            "Epoch  47 Batch  210 / 525  Training Loss  2.3256605345522985e-05\n",
            "Epoch  47 Batch  211 / 525  Training Loss  1.2721296116069425e-05\n",
            "Epoch  47 Batch  212 / 525  Training Loss  7.49943365008221e-06\n",
            "Epoch  47 Batch  213 / 525  Training Loss  1.0608900993247516e-05\n",
            "Epoch  47 Batch  214 / 525  Training Loss  1.1982070645899512e-05\n",
            "Epoch  47 Batch  215 / 525  Training Loss  1.5243344932969194e-05\n",
            "Epoch  47 Batch  216 / 525  Training Loss  8.172942216333468e-06\n",
            "Epoch  47 Batch  217 / 525  Training Loss  8.703537787368987e-06\n",
            "Epoch  47 Batch  218 / 525  Training Loss  1.799634992494248e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  47 Batch  219 / 525  Training Loss  8.524477379978634e-06\n",
            "Epoch  47 Batch  220 / 525  Training Loss  1.346085082332138e-05\n",
            "Epoch  47 Batch  221 / 525  Training Loss  1.2283363503229339e-05\n",
            "Epoch  47 Batch  222 / 525  Training Loss  1.3895316442358308e-05\n",
            "Epoch  47 Batch  223 / 525  Training Loss  1.089408942789305e-05\n",
            "Epoch  47 Batch  224 / 525  Training Loss  1.2343657544988673e-05\n",
            "Epoch  47 Batch  225 / 525  Training Loss  1.059165060723899e-05\n",
            "Epoch  47 Batch  226 / 525  Training Loss  1.3479058907250874e-05\n",
            "Epoch  47 Batch  227 / 525  Training Loss  2.207184661529027e-05\n",
            "Epoch  47 Batch  228 / 525  Training Loss  9.343808414996602e-06\n",
            "Epoch  47 Batch  229 / 525  Training Loss  1.3010200746066403e-05\n",
            "Epoch  47 Batch  230 / 525  Training Loss  8.476554285152815e-06\n",
            "Epoch  47 Batch  231 / 525  Training Loss  8.785738828009926e-06\n",
            "Epoch  47 Batch  232 / 525  Training Loss  1.1000349331879988e-05\n",
            "Epoch  47 Batch  233 / 525  Training Loss  1.2891381629742682e-05\n",
            "Epoch  47 Batch  234 / 525  Training Loss  1.5378713214886375e-05\n",
            "Epoch  47 Batch  235 / 525  Training Loss  1.2440058526408393e-05\n",
            "Epoch  47 Batch  236 / 525  Training Loss  1.4739356629434042e-05\n",
            "Epoch  47 Batch  237 / 525  Training Loss  1.3575738194049336e-05\n",
            "Epoch  47 Batch  238 / 525  Training Loss  1.2078275176463649e-05\n",
            "Epoch  47 Batch  239 / 525  Training Loss  1.7547896277392283e-05\n",
            "Epoch  47 Batch  240 / 525  Training Loss  6.171952463773778e-06\n",
            "Epoch  47 Batch  241 / 525  Training Loss  1.5310517483158037e-05\n",
            "Epoch  47 Batch  242 / 525  Training Loss  1.1730644473573193e-05\n",
            "Epoch  47 Batch  243 / 525  Training Loss  5.010837867303053e-06\n",
            "Epoch  47 Batch  244 / 525  Training Loss  1.1113317668787204e-05\n",
            "Epoch  47 Batch  245 / 525  Training Loss  7.755990736768581e-06\n",
            "Epoch  47 Batch  246 / 525  Training Loss  1.385083123750519e-05\n",
            "Epoch  47 Batch  247 / 525  Training Loss  9.671872248873115e-06\n",
            "Epoch  47 Batch  248 / 525  Training Loss  9.149633115157485e-06\n",
            "Epoch  47 Batch  249 / 525  Training Loss  1.2454094758140855e-05\n",
            "Epoch  47 Batch  250 / 525  Training Loss  9.734185368870385e-06\n",
            "Epoch  47 Batch  251 / 525  Training Loss  1.4676491446152795e-05\n",
            "Epoch  47 Batch  252 / 525  Training Loss  1.095294464903418e-05\n",
            "Epoch  47 Batch  253 / 525  Training Loss  1.3078289157419931e-05\n",
            "Epoch  47 Batch  254 / 525  Training Loss  1.2065665032423567e-05\n",
            "Epoch  47 Batch  255 / 525  Training Loss  2.0316623704275116e-05\n",
            "Epoch  47 Batch  256 / 525  Training Loss  1.2180161320429761e-05\n",
            "Epoch  47 Batch  257 / 525  Training Loss  1.2302613868087064e-05\n",
            "Epoch  47 Batch  258 / 525  Training Loss  1.3921718164056074e-05\n",
            "Epoch  47 Batch  259 / 525  Training Loss  8.069488103501499e-06\n",
            "Epoch  47 Batch  260 / 525  Training Loss  1.2684690773312468e-05\n",
            "Epoch  47 Batch  261 / 525  Training Loss  1.780075217538979e-05\n",
            "Epoch  47 Batch  262 / 525  Training Loss  1.0688260772440117e-05\n",
            "Epoch  47 Batch  263 / 525  Training Loss  1.2056570085405838e-05\n",
            "Epoch  47 Batch  264 / 525  Training Loss  1.789724592526909e-05\n",
            "Epoch  47 Batch  265 / 525  Training Loss  1.1284622814855538e-05\n",
            "Epoch  47 Batch  266 / 525  Training Loss  1.230389898410067e-05\n",
            "Epoch  47 Batch  267 / 525  Training Loss  1.1489652024465613e-05\n",
            "Epoch  47 Batch  268 / 525  Training Loss  1.0202433259109966e-05\n",
            "Epoch  47 Batch  269 / 525  Training Loss  1.4319904948933981e-05\n",
            "Epoch  47 Batch  270 / 525  Training Loss  1.5236108083627187e-05\n",
            "Epoch  47 Batch  271 / 525  Training Loss  1.2304645679250825e-05\n",
            "Epoch  47 Batch  272 / 525  Training Loss  9.509259143669624e-06\n",
            "Epoch  47 Batch  273 / 525  Training Loss  1.630943370400928e-05\n",
            "Epoch  47 Batch  274 / 525  Training Loss  1.4492919035546947e-05\n",
            "Epoch  47 Batch  275 / 525  Training Loss  1.1414753316785209e-05\n",
            "Epoch  47 Batch  276 / 525  Training Loss  8.74977013154421e-06\n",
            "Epoch  47 Batch  277 / 525  Training Loss  1.4613657185691409e-05\n",
            "Epoch  47 Batch  278 / 525  Training Loss  9.657747796154581e-06\n",
            "Epoch  47 Batch  279 / 525  Training Loss  5.5126138249761425e-06\n",
            "Epoch  47 Batch  280 / 525  Training Loss  1.4383678717422299e-05\n",
            "Epoch  47 Batch  281 / 525  Training Loss  1.0361985914641991e-05\n",
            "Epoch  47 Batch  282 / 525  Training Loss  1.0337292223994154e-05\n",
            "Epoch  47 Batch  283 / 525  Training Loss  7.207305316114798e-06\n",
            "Epoch  47 Batch  284 / 525  Training Loss  1.4932413250789978e-05\n",
            "Epoch  47 Batch  285 / 525  Training Loss  1.5454163076356053e-05\n",
            "Epoch  47 Batch  286 / 525  Training Loss  1.0148452020075638e-05\n",
            "Epoch  47 Batch  287 / 525  Training Loss  1.0346599083277397e-05\n",
            "Epoch  47 Batch  288 / 525  Training Loss  1.3660441254614852e-05\n",
            "Epoch  47 Batch  289 / 525  Training Loss  9.913028407027014e-06\n",
            "Epoch  47 Batch  290 / 525  Training Loss  1.201909708470339e-05\n",
            "Epoch  47 Batch  291 / 525  Training Loss  2.3516588044003583e-05\n",
            "Epoch  47 Batch  292 / 525  Training Loss  1.1095994523202535e-05\n",
            "Epoch  47 Batch  293 / 525  Training Loss  1.5596197044942528e-05\n",
            "Epoch  47 Batch  294 / 525  Training Loss  1.2515032722149044e-05\n",
            "Epoch  47 Batch  295 / 525  Training Loss  1.1807142072939314e-05\n",
            "Epoch  47 Batch  296 / 525  Training Loss  1.871661879704334e-05\n",
            "Epoch  47 Batch  297 / 525  Training Loss  1.2789076208719052e-05\n",
            "Epoch  47 Batch  298 / 525  Training Loss  1.4093418030824978e-05\n",
            "Epoch  47 Batch  299 / 525  Training Loss  1.1952831300732214e-05\n",
            "Epoch  47 Batch  300 / 525  Training Loss  1.914346830744762e-05\n",
            "Epoch  47 Batch  301 / 525  Training Loss  1.5198575965769123e-05\n",
            "Epoch  47 Batch  302 / 525  Training Loss  1.194698597828392e-05\n",
            "Epoch  47 Batch  303 / 525  Training Loss  1.0117333658854477e-05\n",
            "Epoch  47 Batch  304 / 525  Training Loss  1.3628050510305911e-05\n",
            "Epoch  47 Batch  305 / 525  Training Loss  1.6668289390509017e-05\n",
            "Epoch  47 Batch  306 / 525  Training Loss  1.1104541954409797e-05\n",
            "Epoch  47 Batch  307 / 525  Training Loss  2.0049668819410726e-05\n",
            "Epoch  47 Batch  308 / 525  Training Loss  1.8044462194666266e-05\n",
            "Epoch  47 Batch  309 / 525  Training Loss  9.575433978170622e-06\n",
            "Epoch  47 Batch  310 / 525  Training Loss  1.4696071957587264e-05\n",
            "Epoch  47 Batch  311 / 525  Training Loss  1.8291420929017477e-05\n",
            "Epoch  47 Batch  312 / 525  Training Loss  1.1668869774439372e-05\n",
            "Epoch  47 Batch  313 / 525  Training Loss  7.492129043384921e-06\n",
            "Epoch  47 Batch  314 / 525  Training Loss  8.696158147358801e-06\n",
            "Epoch  47 Batch  315 / 525  Training Loss  9.512290489510633e-06\n",
            "Epoch  47 Batch  316 / 525  Training Loss  1.3688147191714961e-05\n",
            "Epoch  47 Batch  317 / 525  Training Loss  1.9086584870819934e-05\n",
            "Epoch  47 Batch  318 / 525  Training Loss  1.6600610251771286e-05\n",
            "Epoch  47 Batch  319 / 525  Training Loss  6.060549821995664e-06\n",
            "Epoch  47 Batch  320 / 525  Training Loss  8.936252925195731e-06\n",
            "Epoch  47 Batch  321 / 525  Training Loss  8.023091140785255e-06\n",
            "Epoch  47 Batch  322 / 525  Training Loss  7.857303899072576e-06\n",
            "Epoch  47 Batch  323 / 525  Training Loss  9.82048823061632e-06\n",
            "Epoch  47 Batch  324 / 525  Training Loss  1.0676746569515672e-05\n",
            "Epoch  47 Batch  325 / 525  Training Loss  8.17205091152573e-06\n",
            "Epoch  47 Batch  326 / 525  Training Loss  1.4220201592252124e-05\n",
            "Epoch  47 Batch  327 / 525  Training Loss  1.6745745597290806e-05\n",
            "Epoch  47 Batch  328 / 525  Training Loss  2.1374371499405243e-05\n",
            "Epoch  47 Batch  329 / 525  Training Loss  2.0962579583283514e-05\n",
            "Epoch  47 Batch  330 / 525  Training Loss  1.0791760360007174e-05\n",
            "Epoch  47 Batch  331 / 525  Training Loss  1.27973598864628e-05\n",
            "Epoch  47 Batch  332 / 525  Training Loss  8.102577339741401e-06\n",
            "Epoch  47 Batch  333 / 525  Training Loss  8.90830960997846e-06\n",
            "Epoch  47 Batch  334 / 525  Training Loss  7.6022406574338675e-06\n",
            "Epoch  47 Batch  335 / 525  Training Loss  1.8841910787159577e-05\n",
            "Epoch  47 Batch  336 / 525  Training Loss  1.5182406059466302e-05\n",
            "Epoch  47 Batch  337 / 525  Training Loss  9.174113074550405e-06\n",
            "Epoch  47 Batch  338 / 525  Training Loss  9.3500657385448e-06\n",
            "Epoch  47 Batch  339 / 525  Training Loss  1.3397642760537565e-05\n",
            "Epoch  47 Batch  340 / 525  Training Loss  1.776245881046634e-05\n",
            "Epoch  47 Batch  341 / 525  Training Loss  7.306168299692217e-06\n",
            "Epoch  47 Batch  342 / 525  Training Loss  9.35331354412483e-06\n",
            "Epoch  47 Batch  343 / 525  Training Loss  1.4910173376847524e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  47 Batch  344 / 525  Training Loss  1.6610472812317312e-05\n",
            "Epoch  47 Batch  345 / 525  Training Loss  9.142589988186955e-06\n",
            "Epoch  47 Batch  346 / 525  Training Loss  1.3323425264388788e-05\n",
            "Epoch  47 Batch  347 / 525  Training Loss  1.46596676131594e-05\n",
            "Epoch  47 Batch  348 / 525  Training Loss  1.391056412103353e-05\n",
            "Epoch  47 Batch  349 / 525  Training Loss  7.93280742072966e-06\n",
            "Epoch  47 Batch  350 / 525  Training Loss  1.3482307622325607e-05\n",
            "Epoch  47 Batch  351 / 525  Training Loss  1.6607145880698226e-05\n",
            "Epoch  47 Batch  352 / 525  Training Loss  9.446396688872483e-06\n",
            "Epoch  47 Batch  353 / 525  Training Loss  9.695371772977524e-06\n",
            "Epoch  47 Batch  354 / 525  Training Loss  1.281169807043625e-05\n",
            "Epoch  47 Batch  355 / 525  Training Loss  1.2529648302006535e-05\n",
            "Epoch  47 Batch  356 / 525  Training Loss  1.1271836228843313e-05\n",
            "Epoch  47 Batch  357 / 525  Training Loss  1.458458700653864e-05\n",
            "Epoch  47 Batch  358 / 525  Training Loss  1.0859198482648935e-05\n",
            "Epoch  47 Batch  359 / 525  Training Loss  1.7839625797932968e-05\n",
            "Epoch  47 Batch  360 / 525  Training Loss  1.401860936312005e-05\n",
            "Epoch  47 Batch  361 / 525  Training Loss  1.068261917680502e-05\n",
            "Epoch  47 Batch  362 / 525  Training Loss  1.1926940715056844e-05\n",
            "Epoch  47 Batch  363 / 525  Training Loss  1.3498695807356853e-05\n",
            "Epoch  47 Batch  364 / 525  Training Loss  1.2051071280438919e-05\n",
            "Epoch  47 Batch  365 / 525  Training Loss  8.856413842295296e-06\n",
            "Epoch  47 Batch  366 / 525  Training Loss  1.8790498870657757e-05\n",
            "Epoch  47 Batch  367 / 525  Training Loss  1.0308201126463246e-05\n",
            "Epoch  47 Batch  368 / 525  Training Loss  1.3072150977677666e-05\n",
            "Epoch  47 Batch  369 / 525  Training Loss  1.5803796486579813e-05\n",
            "Epoch  47 Batch  370 / 525  Training Loss  1.8315822671866044e-05\n",
            "Epoch  47 Batch  371 / 525  Training Loss  1.2554795830510557e-05\n",
            "Epoch  47 Batch  372 / 525  Training Loss  1.2050200894009322e-05\n",
            "Epoch  47 Batch  373 / 525  Training Loss  1.1917164556507487e-05\n",
            "Epoch  47 Batch  374 / 525  Training Loss  1.6480502381455153e-05\n",
            "Epoch  47 Batch  375 / 525  Training Loss  1.543268808745779e-05\n",
            "Epoch  47 Batch  376 / 525  Training Loss  1.5774789062561467e-05\n",
            "Epoch  47 Batch  377 / 525  Training Loss  1.4644676411990076e-05\n",
            "Epoch  47 Batch  378 / 525  Training Loss  1.0566656783339567e-05\n",
            "Epoch  47 Batch  379 / 525  Training Loss  1.0878013199544512e-05\n",
            "Epoch  47 Batch  380 / 525  Training Loss  1.124999653256964e-05\n",
            "Epoch  47 Batch  381 / 525  Training Loss  9.089866580325179e-06\n",
            "Epoch  47 Batch  382 / 525  Training Loss  1.1660344171104953e-05\n",
            "Epoch  47 Batch  383 / 525  Training Loss  8.41363817016827e-06\n",
            "Epoch  47 Batch  384 / 525  Training Loss  1.2821302334486973e-05\n",
            "Epoch  47 Batch  385 / 525  Training Loss  1.4488228771369904e-05\n",
            "Epoch  47 Batch  386 / 525  Training Loss  9.688313184597064e-06\n",
            "Epoch  47 Batch  387 / 525  Training Loss  9.353637324238662e-06\n",
            "Epoch  47 Batch  388 / 525  Training Loss  1.1554381671885494e-05\n",
            "Epoch  47 Batch  389 / 525  Training Loss  1.7215079424204305e-05\n",
            "Epoch  47 Batch  390 / 525  Training Loss  8.443459591944702e-06\n",
            "Epoch  47 Batch  391 / 525  Training Loss  9.670306098996662e-06\n",
            "Epoch  47 Batch  392 / 525  Training Loss  1.4263135199144017e-05\n",
            "Epoch  47 Batch  393 / 525  Training Loss  1.2076698112650774e-05\n",
            "Epoch  47 Batch  394 / 525  Training Loss  1.2726501154247671e-05\n",
            "Epoch  47 Batch  395 / 525  Training Loss  1.271929795620963e-05\n",
            "Epoch  47 Batch  396 / 525  Training Loss  1.131291537603829e-05\n",
            "Epoch  47 Batch  397 / 525  Training Loss  1.654598963796161e-05\n",
            "Epoch  47 Batch  398 / 525  Training Loss  1.3882860002922826e-05\n",
            "Epoch  47 Batch  399 / 525  Training Loss  9.794141988095362e-06\n",
            "Epoch  47 Batch  400 / 525  Training Loss  9.836215212999377e-06\n",
            "Epoch  47 Batch  401 / 525  Training Loss  1.2137154044467025e-05\n",
            "Epoch  47 Batch  402 / 525  Training Loss  7.228032245620852e-06\n",
            "Epoch  47 Batch  403 / 525  Training Loss  1.607312515261583e-05\n",
            "Epoch  47 Batch  404 / 525  Training Loss  1.0462003956490662e-05\n",
            "Epoch  47 Batch  405 / 525  Training Loss  1.1538085345819127e-05\n",
            "Epoch  47 Batch  406 / 525  Training Loss  7.959218237374444e-06\n",
            "Epoch  47 Batch  407 / 525  Training Loss  1.2466863154259045e-05\n",
            "Epoch  47 Batch  408 / 525  Training Loss  1.1910485227417666e-05\n",
            "Epoch  47 Batch  409 / 525  Training Loss  1.0607320291455835e-05\n",
            "Epoch  47 Batch  410 / 525  Training Loss  1.4732921954418998e-05\n",
            "Epoch  47 Batch  411 / 525  Training Loss  8.969326700025704e-06\n",
            "Epoch  47 Batch  412 / 525  Training Loss  1.023631830321392e-05\n",
            "Epoch  47 Batch  413 / 525  Training Loss  1.6107826013467275e-05\n",
            "Epoch  47 Batch  414 / 525  Training Loss  7.251099304994568e-06\n",
            "Epoch  47 Batch  415 / 525  Training Loss  1.0722650586103555e-05\n",
            "Epoch  47 Batch  416 / 525  Training Loss  2.0673258404713124e-05\n",
            "Epoch  47 Batch  417 / 525  Training Loss  1.3441094779409468e-05\n",
            "Epoch  47 Batch  418 / 525  Training Loss  1.0843668860616162e-05\n",
            "Epoch  47 Batch  419 / 525  Training Loss  2.117237090715207e-05\n",
            "Epoch  47 Batch  420 / 525  Training Loss  1.0621819455991499e-05\n",
            "Epoch  47 Batch  421 / 525  Training Loss  8.085972694971133e-06\n",
            "Epoch  47 Batch  422 / 525  Training Loss  1.4606855984311551e-05\n",
            "Epoch  47 Batch  423 / 525  Training Loss  1.0331564226362389e-05\n",
            "Epoch  47 Batch  424 / 525  Training Loss  1.3299335478222929e-05\n",
            "Epoch  47 Batch  425 / 525  Training Loss  1.9641727703856304e-05\n",
            "Epoch  47 Batch  426 / 525  Training Loss  7.480975909857079e-06\n",
            "Epoch  47 Batch  427 / 525  Training Loss  1.449446972401347e-05\n",
            "Epoch  47 Batch  428 / 525  Training Loss  1.3794201549899299e-05\n",
            "Epoch  47 Batch  429 / 525  Training Loss  1.0837336049007718e-05\n",
            "Epoch  47 Batch  430 / 525  Training Loss  1.0906270290433895e-05\n",
            "Epoch  47 Batch  431 / 525  Training Loss  9.893947208183818e-06\n",
            "Epoch  47 Batch  432 / 525  Training Loss  4.832683771383017e-06\n",
            "Epoch  47 Batch  433 / 525  Training Loss  1.253826940228464e-05\n",
            "Epoch  47 Batch  434 / 525  Training Loss  1.2199388947919942e-05\n",
            "Epoch  47 Batch  435 / 525  Training Loss  1.550579872855451e-05\n",
            "Epoch  47 Batch  436 / 525  Training Loss  1.4731243027199525e-05\n",
            "Epoch  47 Batch  437 / 525  Training Loss  1.1398546121199615e-05\n",
            "Epoch  47 Batch  438 / 525  Training Loss  1.0657157872628886e-05\n",
            "Epoch  47 Batch  439 / 525  Training Loss  7.683106559852604e-06\n",
            "Epoch  47 Batch  440 / 525  Training Loss  1.2134327334933914e-05\n",
            "Epoch  47 Batch  441 / 525  Training Loss  1.982119465537835e-05\n",
            "Epoch  47 Batch  442 / 525  Training Loss  1.2651284123421647e-05\n",
            "Epoch  47 Batch  443 / 525  Training Loss  1.6258296454907395e-05\n",
            "Epoch  47 Batch  444 / 525  Training Loss  1.1120258022856433e-05\n",
            "Epoch  47 Batch  445 / 525  Training Loss  1.875493398983963e-05\n",
            "Epoch  47 Batch  446 / 525  Training Loss  1.0915032362390775e-05\n",
            "Epoch  47 Batch  447 / 525  Training Loss  9.452382982999552e-06\n",
            "Epoch  47 Batch  448 / 525  Training Loss  2.051217234111391e-05\n",
            "Epoch  47 Batch  449 / 525  Training Loss  1.2600326954270713e-05\n",
            "Epoch  47 Batch  450 / 525  Training Loss  1.458172937418567e-05\n",
            "Epoch  47 Batch  451 / 525  Training Loss  8.507628081133589e-06\n",
            "Epoch  47 Batch  452 / 525  Training Loss  9.192352990794461e-06\n",
            "Epoch  47 Batch  453 / 525  Training Loss  1.3667228813574184e-05\n",
            "Epoch  47 Batch  454 / 525  Training Loss  1.4856284906272776e-05\n",
            "Epoch  47 Batch  455 / 525  Training Loss  1.2292507562960964e-05\n",
            "Epoch  47 Batch  456 / 525  Training Loss  1.1431997336330824e-05\n",
            "Epoch  47 Batch  457 / 525  Training Loss  1.1282912964816205e-05\n",
            "Epoch  47 Batch  458 / 525  Training Loss  1.4861793715681415e-05\n",
            "Epoch  47 Batch  459 / 525  Training Loss  1.7560592823429033e-05\n",
            "Epoch  47 Batch  460 / 525  Training Loss  1.484206131863175e-05\n",
            "Epoch  47 Batch  461 / 525  Training Loss  1.0462337741046213e-05\n",
            "Epoch  47 Batch  462 / 525  Training Loss  1.3275142009661067e-05\n",
            "Epoch  47 Batch  463 / 525  Training Loss  1.023570257530082e-05\n",
            "Epoch  47 Batch  464 / 525  Training Loss  1.2337264706729911e-05\n",
            "Epoch  47 Batch  465 / 525  Training Loss  1.2045207768096589e-05\n",
            "Epoch  47 Batch  466 / 525  Training Loss  1.1738442481146194e-05\n",
            "Epoch  47 Batch  467 / 525  Training Loss  1.8867660401156172e-05\n",
            "Epoch  47 Batch  468 / 525  Training Loss  1.305818477703724e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  47 Batch  469 / 525  Training Loss  1.6651652913424186e-05\n",
            "Epoch  47 Batch  470 / 525  Training Loss  1.7555945305502973e-05\n",
            "Epoch  47 Batch  471 / 525  Training Loss  1.7452377505833283e-05\n",
            "Epoch  47 Batch  472 / 525  Training Loss  1.4806651051912922e-05\n",
            "Epoch  47 Batch  473 / 525  Training Loss  1.2721478924504481e-05\n",
            "Epoch  47 Batch  474 / 525  Training Loss  1.2782295016222633e-05\n",
            "Epoch  47 Batch  475 / 525  Training Loss  1.9119142962154e-05\n",
            "Epoch  47 Batch  476 / 525  Training Loss  1.3555671102949418e-05\n",
            "Epoch  47 Batch  477 / 525  Training Loss  1.940878246387001e-05\n",
            "Epoch  47 Batch  478 / 525  Training Loss  1.123337096942123e-05\n",
            "Epoch  47 Batch  479 / 525  Training Loss  1.0597766959108412e-05\n",
            "Epoch  47 Batch  480 / 525  Training Loss  8.98764847079292e-06\n",
            "Epoch  47 Batch  481 / 525  Training Loss  1.3420960385701619e-05\n",
            "Epoch  47 Batch  482 / 525  Training Loss  1.644875737838447e-05\n",
            "Epoch  47 Batch  483 / 525  Training Loss  1.4387063856702298e-05\n",
            "Epoch  47 Batch  484 / 525  Training Loss  1.7342419596388936e-05\n",
            "Epoch  47 Batch  485 / 525  Training Loss  1.1735186490113847e-05\n",
            "Epoch  47 Batch  486 / 525  Training Loss  1.0687536814657506e-05\n",
            "Epoch  47 Batch  487 / 525  Training Loss  1.1158032066305168e-05\n",
            "Epoch  47 Batch  488 / 525  Training Loss  5.853304173797369e-06\n",
            "Epoch  47 Batch  489 / 525  Training Loss  1.1558687219803687e-05\n",
            "Epoch  47 Batch  490 / 525  Training Loss  1.059162514138734e-05\n",
            "Epoch  47 Batch  491 / 525  Training Loss  1.0207778359472286e-05\n",
            "Epoch  47 Batch  492 / 525  Training Loss  1.4867546269670129e-05\n",
            "Epoch  47 Batch  493 / 525  Training Loss  1.0246927558910102e-05\n",
            "Epoch  47 Batch  494 / 525  Training Loss  1.3922163816459943e-05\n",
            "Epoch  47 Batch  495 / 525  Training Loss  1.1502410416142084e-05\n",
            "Epoch  47 Batch  496 / 525  Training Loss  1.6618760128039867e-05\n",
            "Epoch  47 Batch  497 / 525  Training Loss  1.0416163604531903e-05\n",
            "Epoch  47 Batch  498 / 525  Training Loss  1.4003906471771188e-05\n",
            "Epoch  47 Batch  499 / 525  Training Loss  1.605908983037807e-05\n",
            "Epoch  47 Batch  500 / 525  Training Loss  1.3923153346695472e-05\n",
            "Epoch  47 Batch  501 / 525  Training Loss  1.2201811841805466e-05\n",
            "Epoch  47 Batch  502 / 525  Training Loss  1.314294058829546e-05\n",
            "Epoch  47 Batch  503 / 525  Training Loss  9.932028660841752e-06\n",
            "Epoch  47 Batch  504 / 525  Training Loss  1.4667020877823234e-05\n",
            "Epoch  47 Batch  505 / 525  Training Loss  8.929285286285449e-06\n",
            "Epoch  47 Batch  506 / 525  Training Loss  8.703491403139196e-06\n",
            "Epoch  47 Batch  507 / 525  Training Loss  1.526253981865011e-05\n",
            "Epoch  47 Batch  508 / 525  Training Loss  1.1596423064474948e-05\n",
            "Epoch  47 Batch  509 / 525  Training Loss  1.6483139916090295e-05\n",
            "Epoch  47 Batch  510 / 525  Training Loss  1.670499477768317e-05\n",
            "Epoch  47 Batch  511 / 525  Training Loss  6.5763260863604955e-06\n",
            "Epoch  47 Batch  512 / 525  Training Loss  1.4642186215496622e-05\n",
            "Epoch  47 Batch  513 / 525  Training Loss  9.532381227472797e-06\n",
            "Epoch  47 Batch  514 / 525  Training Loss  1.6075104213086888e-05\n",
            "Epoch  47 Batch  515 / 525  Training Loss  1.3792193385597784e-05\n",
            "Epoch  47 Batch  516 / 525  Training Loss  1.6598220099695027e-05\n",
            "Epoch  47 Batch  517 / 525  Training Loss  1.2419166523613967e-05\n",
            "Epoch  47 Batch  518 / 525  Training Loss  2.023442903009709e-05\n",
            "Epoch  47 Batch  519 / 525  Training Loss  1.4792569345445372e-05\n",
            "Epoch  47 Batch  520 / 525  Training Loss  8.511695341439918e-06\n",
            "Epoch  47 Batch  521 / 525  Training Loss  1.4640814697486348e-05\n",
            "Epoch  47 Batch  522 / 525  Training Loss  1.4402443412109278e-05\n",
            "Epoch  47 Batch  523 / 525  Training Loss  8.7266807895503e-06\n",
            "Epoch  47 Batch  524 / 525  Training Loss  1.1701973562594503e-05\n",
            "  48    |    -    |   0.000013   | 64.600000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 48\n",
            "Epoch  48 Batch  0 / 525  Training Loss  1.254341441381257e-05\n",
            "Epoch  48 Batch  1 / 525  Training Loss  7.6773076216341e-06\n",
            "Epoch  48 Batch  2 / 525  Training Loss  1.1114166227343958e-05\n",
            "Epoch  48 Batch  3 / 525  Training Loss  1.653104118304327e-05\n",
            "Epoch  48 Batch  4 / 525  Training Loss  1.3301914805197157e-05\n",
            "Epoch  48 Batch  5 / 525  Training Loss  1.483843971072929e-05\n",
            "Epoch  48 Batch  6 / 525  Training Loss  7.015150913503021e-06\n",
            "Epoch  48 Batch  7 / 525  Training Loss  8.281695045297965e-06\n",
            "Epoch  48 Batch  8 / 525  Training Loss  1.4177712728269398e-05\n",
            "Epoch  48 Batch  9 / 525  Training Loss  1.3063219739706255e-05\n",
            "Epoch  48 Batch  10 / 525  Training Loss  1.569610321894288e-05\n",
            "Epoch  48 Batch  11 / 525  Training Loss  8.111680472211447e-06\n",
            "Epoch  48 Batch  12 / 525  Training Loss  1.9895283912774175e-05\n",
            "Epoch  48 Batch  13 / 525  Training Loss  9.008379493025132e-06\n",
            "Epoch  48 Batch  14 / 525  Training Loss  9.271589988202322e-06\n",
            "Epoch  48 Batch  15 / 525  Training Loss  1.1075357178924605e-05\n",
            "Epoch  48 Batch  16 / 525  Training Loss  1.6633895938866772e-05\n",
            "Epoch  48 Batch  17 / 525  Training Loss  6.694814601360122e-06\n",
            "Epoch  48 Batch  18 / 525  Training Loss  7.254556294356007e-06\n",
            "Epoch  48 Batch  19 / 525  Training Loss  1.1675975656544324e-05\n",
            "Epoch  48 Batch  20 / 525  Training Loss  1.0601333997328766e-05\n",
            "Epoch  48 Batch  21 / 525  Training Loss  1.2314390914980322e-05\n",
            "Epoch  48 Batch  22 / 525  Training Loss  6.274548468354624e-06\n",
            "Epoch  48 Batch  23 / 525  Training Loss  1.0342015229980461e-05\n",
            "Epoch  48 Batch  24 / 525  Training Loss  1.344878000963945e-05\n",
            "Epoch  48 Batch  25 / 525  Training Loss  1.7259859305340797e-05\n",
            "Epoch  48 Batch  26 / 525  Training Loss  1.1130899110867176e-05\n",
            "Epoch  48 Batch  27 / 525  Training Loss  9.800705811358057e-06\n",
            "Epoch  48 Batch  28 / 525  Training Loss  8.976105164038017e-06\n",
            "Epoch  48 Batch  29 / 525  Training Loss  1.0186582585447468e-05\n",
            "Epoch  48 Batch  30 / 525  Training Loss  7.1227877924684435e-06\n",
            "Epoch  48 Batch  31 / 525  Training Loss  6.618339284614194e-06\n",
            "Epoch  48 Batch  32 / 525  Training Loss  9.129683348874096e-06\n",
            "Epoch  48 Batch  33 / 525  Training Loss  1.2540616808109917e-05\n",
            "Epoch  48 Batch  34 / 525  Training Loss  8.716129741515033e-06\n",
            "Epoch  48 Batch  35 / 525  Training Loss  1.0853536878130399e-05\n",
            "Epoch  48 Batch  36 / 525  Training Loss  1.601330950506963e-05\n",
            "Epoch  48 Batch  37 / 525  Training Loss  1.1024744708265644e-05\n",
            "Epoch  48 Batch  38 / 525  Training Loss  1.248981698154239e-05\n",
            "Epoch  48 Batch  39 / 525  Training Loss  1.0480005585122854e-05\n",
            "Epoch  48 Batch  40 / 525  Training Loss  1.2340364264673553e-05\n",
            "Epoch  48 Batch  41 / 525  Training Loss  7.552867373306071e-06\n",
            "Epoch  48 Batch  42 / 525  Training Loss  1.3467766621033661e-05\n",
            "Epoch  48 Batch  43 / 525  Training Loss  8.374896424356848e-06\n",
            "Epoch  48 Batch  44 / 525  Training Loss  1.688608062977437e-05\n",
            "Epoch  48 Batch  45 / 525  Training Loss  1.0542792551859748e-05\n",
            "Epoch  48 Batch  46 / 525  Training Loss  1.0123431820829865e-05\n",
            "Epoch  48 Batch  47 / 525  Training Loss  1.1614110007940326e-05\n",
            "Epoch  48 Batch  48 / 525  Training Loss  7.012812602624763e-06\n",
            "Epoch  48 Batch  49 / 525  Training Loss  2.0737947124871425e-05\n",
            "Epoch  48 Batch  50 / 525  Training Loss  1.2349066309980117e-05\n",
            "Epoch  48 Batch  51 / 525  Training Loss  1.2406993846525438e-05\n",
            "Epoch  48 Batch  52 / 525  Training Loss  1.1825706678791903e-05\n",
            "Epoch  48 Batch  53 / 525  Training Loss  1.155537484009983e-05\n",
            "Epoch  48 Batch  54 / 525  Training Loss  1.5404311852762476e-05\n",
            "Epoch  48 Batch  55 / 525  Training Loss  1.2446003893273883e-05\n",
            "Epoch  48 Batch  56 / 525  Training Loss  1.2193874681543093e-05\n",
            "Epoch  48 Batch  57 / 525  Training Loss  7.996930435183458e-06\n",
            "Epoch  48 Batch  58 / 525  Training Loss  1.3003218555240892e-05\n",
            "Epoch  48 Batch  59 / 525  Training Loss  1.661316491663456e-05\n",
            "Epoch  48 Batch  60 / 525  Training Loss  1.5257124687195756e-05\n",
            "Epoch  48 Batch  61 / 525  Training Loss  1.341680581390392e-05\n",
            "Epoch  48 Batch  62 / 525  Training Loss  1.286626957153203e-05\n",
            "Epoch  48 Batch  63 / 525  Training Loss  7.948401616886258e-06\n",
            "Epoch  48 Batch  64 / 525  Training Loss  1.2385398804326542e-05\n",
            "Epoch  48 Batch  65 / 525  Training Loss  9.587833119439892e-06\n",
            "Epoch  48 Batch  66 / 525  Training Loss  1.3364397091208957e-05\n",
            "Epoch  48 Batch  67 / 525  Training Loss  1.3346021660254337e-05\n",
            "Epoch  48 Batch  68 / 525  Training Loss  1.2683325621765107e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  48 Batch  69 / 525  Training Loss  1.1678281225613318e-05\n",
            "Epoch  48 Batch  70 / 525  Training Loss  1.4734800970472861e-05\n",
            "Epoch  48 Batch  71 / 525  Training Loss  1.2877739209216088e-05\n",
            "Epoch  48 Batch  72 / 525  Training Loss  1.201759914692957e-05\n",
            "Epoch  48 Batch  73 / 525  Training Loss  9.528261216473766e-06\n",
            "Epoch  48 Batch  74 / 525  Training Loss  1.089896795747336e-05\n",
            "Epoch  48 Batch  75 / 525  Training Loss  2.1962699975119904e-05\n",
            "Epoch  48 Batch  76 / 525  Training Loss  8.31050692795543e-06\n",
            "Epoch  48 Batch  77 / 525  Training Loss  1.0407524314359762e-05\n",
            "Epoch  48 Batch  78 / 525  Training Loss  9.119026799453422e-06\n",
            "Epoch  48 Batch  79 / 525  Training Loss  1.398336735292105e-05\n",
            "Epoch  48 Batch  80 / 525  Training Loss  1.1161338989040814e-05\n",
            "Epoch  48 Batch  81 / 525  Training Loss  1.7738546375767328e-05\n",
            "Epoch  48 Batch  82 / 525  Training Loss  1.0508168998057954e-05\n",
            "Epoch  48 Batch  83 / 525  Training Loss  8.768664883973543e-06\n",
            "Epoch  48 Batch  84 / 525  Training Loss  8.881646863301285e-06\n",
            "Epoch  48 Batch  85 / 525  Training Loss  1.483509004174266e-05\n",
            "Epoch  48 Batch  86 / 525  Training Loss  7.825204193068203e-06\n",
            "Epoch  48 Batch  87 / 525  Training Loss  1.4559154806192964e-05\n",
            "Epoch  48 Batch  88 / 525  Training Loss  1.0484743143024389e-05\n",
            "Epoch  48 Batch  89 / 525  Training Loss  1.1634512702585198e-05\n",
            "Epoch  48 Batch  90 / 525  Training Loss  1.307583170273574e-05\n",
            "Epoch  48 Batch  91 / 525  Training Loss  1.5424693629029207e-05\n",
            "Epoch  48 Batch  92 / 525  Training Loss  1.118800173571799e-05\n",
            "Epoch  48 Batch  93 / 525  Training Loss  1.3713854059460573e-05\n",
            "Epoch  48 Batch  94 / 525  Training Loss  1.2817198694392573e-05\n",
            "Epoch  48 Batch  95 / 525  Training Loss  9.294692972616758e-06\n",
            "Epoch  48 Batch  96 / 525  Training Loss  1.3511911674868315e-05\n",
            "Epoch  48 Batch  97 / 525  Training Loss  1.20322865768685e-05\n",
            "Epoch  48 Batch  98 / 525  Training Loss  1.2371956472634338e-05\n",
            "Epoch  48 Batch  99 / 525  Training Loss  1.3346919331524987e-05\n",
            "Epoch  48 Batch  100 / 525  Training Loss  1.3858196325600147e-05\n",
            "Epoch  48 Batch  101 / 525  Training Loss  9.112695806834381e-06\n",
            "Epoch  48 Batch  102 / 525  Training Loss  8.818708010949194e-06\n",
            "Epoch  48 Batch  103 / 525  Training Loss  1.5901241567917168e-05\n",
            "Epoch  48 Batch  104 / 525  Training Loss  1.3358230717130937e-05\n",
            "Epoch  48 Batch  105 / 525  Training Loss  9.53024846239714e-06\n",
            "Epoch  48 Batch  106 / 525  Training Loss  1.4812829249422066e-05\n",
            "Epoch  48 Batch  107 / 525  Training Loss  6.288357781158993e-06\n",
            "Epoch  48 Batch  108 / 525  Training Loss  7.260479378601303e-06\n",
            "Epoch  48 Batch  109 / 525  Training Loss  1.0314151950296946e-05\n",
            "Epoch  48 Batch  110 / 525  Training Loss  1.2293096915527713e-05\n",
            "Epoch  48 Batch  111 / 525  Training Loss  1.196147786686197e-05\n",
            "Epoch  48 Batch  112 / 525  Training Loss  1.2954327758052386e-05\n",
            "Epoch  48 Batch  113 / 525  Training Loss  9.194216545438394e-06\n",
            "Epoch  48 Batch  114 / 525  Training Loss  1.232933846040396e-05\n",
            "Epoch  48 Batch  115 / 525  Training Loss  1.187682937597856e-05\n",
            "Epoch  48 Batch  116 / 525  Training Loss  5.099723239254672e-06\n",
            "Epoch  48 Batch  117 / 525  Training Loss  9.23011339182267e-06\n",
            "Epoch  48 Batch  118 / 525  Training Loss  1.2214739399496466e-05\n",
            "Epoch  48 Batch  119 / 525  Training Loss  9.205867172568105e-06\n",
            "Epoch  48 Batch  120 / 525  Training Loss  9.6129733719863e-06\n",
            "Epoch  48 Batch  121 / 525  Training Loss  1.114726001105737e-05\n",
            "Epoch  48 Batch  122 / 525  Training Loss  1.1592564987950027e-05\n",
            "Epoch  48 Batch  123 / 525  Training Loss  1.67320449691033e-05\n",
            "Epoch  48 Batch  124 / 525  Training Loss  1.9338762285769917e-05\n",
            "Epoch  48 Batch  125 / 525  Training Loss  1.0315812687622383e-05\n",
            "Epoch  48 Batch  126 / 525  Training Loss  1.56247042468749e-05\n",
            "Epoch  48 Batch  127 / 525  Training Loss  9.894115464703646e-06\n",
            "Epoch  48 Batch  128 / 525  Training Loss  8.683349733473733e-06\n",
            "Epoch  48 Batch  129 / 525  Training Loss  1.2368422176223248e-05\n",
            "Epoch  48 Batch  130 / 525  Training Loss  1.1546108908078168e-05\n",
            "Epoch  48 Batch  131 / 525  Training Loss  1.3957193004898727e-05\n",
            "Epoch  48 Batch  132 / 525  Training Loss  1.4018027286510915e-05\n",
            "Epoch  48 Batch  133 / 525  Training Loss  9.359767318528611e-06\n",
            "Epoch  48 Batch  134 / 525  Training Loss  1.0732424925663508e-05\n",
            "Epoch  48 Batch  135 / 525  Training Loss  1.0779654076031875e-05\n",
            "Epoch  48 Batch  136 / 525  Training Loss  1.3690140804101247e-05\n",
            "Epoch  48 Batch  137 / 525  Training Loss  1.142173096013721e-05\n",
            "Epoch  48 Batch  138 / 525  Training Loss  1.3617786862596404e-05\n",
            "Epoch  48 Batch  139 / 525  Training Loss  8.370261639356613e-06\n",
            "Epoch  48 Batch  140 / 525  Training Loss  1.1608729437284637e-05\n",
            "Epoch  48 Batch  141 / 525  Training Loss  1.1752801583497785e-05\n",
            "Epoch  48 Batch  142 / 525  Training Loss  1.1970811101491563e-05\n",
            "Epoch  48 Batch  143 / 525  Training Loss  1.173719920188887e-05\n",
            "Epoch  48 Batch  144 / 525  Training Loss  6.432381724152947e-06\n",
            "Epoch  48 Batch  145 / 525  Training Loss  1.0378737897553947e-05\n",
            "Epoch  48 Batch  146 / 525  Training Loss  1.0210418622591533e-05\n",
            "Epoch  48 Batch  147 / 525  Training Loss  1.4483904124062974e-05\n",
            "Epoch  48 Batch  148 / 525  Training Loss  1.4752203242096584e-05\n",
            "Epoch  48 Batch  149 / 525  Training Loss  1.664449882810004e-05\n",
            "Epoch  48 Batch  150 / 525  Training Loss  1.5885099855950102e-05\n",
            "Epoch  48 Batch  151 / 525  Training Loss  1.3578258403867949e-05\n",
            "Epoch  48 Batch  152 / 525  Training Loss  9.532785952615086e-06\n",
            "Epoch  48 Batch  153 / 525  Training Loss  1.527332278783433e-05\n",
            "Epoch  48 Batch  154 / 525  Training Loss  7.5800389822688885e-06\n",
            "Epoch  48 Batch  155 / 525  Training Loss  1.1812984666903503e-05\n",
            "Epoch  48 Batch  156 / 525  Training Loss  9.409488484379835e-06\n",
            "Epoch  48 Batch  157 / 525  Training Loss  6.639504135819152e-06\n",
            "Epoch  48 Batch  158 / 525  Training Loss  9.311438589065801e-06\n",
            "Epoch  48 Batch  159 / 525  Training Loss  1.0787911378429271e-05\n",
            "Epoch  48 Batch  160 / 525  Training Loss  1.9081013306276873e-05\n",
            "Epoch  48 Batch  161 / 525  Training Loss  5.816368684463669e-06\n",
            "Epoch  48 Batch  162 / 525  Training Loss  1.1437985449447297e-05\n",
            "Epoch  48 Batch  163 / 525  Training Loss  9.347194463771302e-06\n",
            "Epoch  48 Batch  164 / 525  Training Loss  1.1958915820287075e-05\n",
            "Epoch  48 Batch  165 / 525  Training Loss  1.1711266779457219e-05\n",
            "Epoch  48 Batch  166 / 525  Training Loss  1.1442035429354291e-05\n",
            "Epoch  48 Batch  167 / 525  Training Loss  1.521312788099749e-05\n",
            "Epoch  48 Batch  168 / 525  Training Loss  1.2392985809128731e-05\n",
            "Epoch  48 Batch  169 / 525  Training Loss  1.194701690110378e-05\n",
            "Epoch  48 Batch  170 / 525  Training Loss  9.888766726362519e-06\n",
            "Epoch  48 Batch  171 / 525  Training Loss  1.0226162885373924e-05\n",
            "Epoch  48 Batch  172 / 525  Training Loss  1.2630238416022621e-05\n",
            "Epoch  48 Batch  173 / 525  Training Loss  9.88953706837492e-06\n",
            "Epoch  48 Batch  174 / 525  Training Loss  1.5955069102346897e-05\n",
            "Epoch  48 Batch  175 / 525  Training Loss  7.980610462254845e-06\n",
            "Epoch  48 Batch  176 / 525  Training Loss  9.097266229218803e-06\n",
            "Epoch  48 Batch  177 / 525  Training Loss  1.651758975640405e-05\n",
            "Epoch  48 Batch  178 / 525  Training Loss  9.601464626030065e-06\n",
            "Epoch  48 Batch  179 / 525  Training Loss  1.0200108590652235e-05\n",
            "Epoch  48 Batch  180 / 525  Training Loss  8.924035682866815e-06\n",
            "Epoch  48 Batch  181 / 525  Training Loss  1.1327055290166754e-05\n",
            "Epoch  48 Batch  182 / 525  Training Loss  1.5758419976918958e-05\n",
            "Epoch  48 Batch  183 / 525  Training Loss  9.158218745142221e-06\n",
            "Epoch  48 Batch  184 / 525  Training Loss  1.0536035006225575e-05\n",
            "Epoch  48 Batch  185 / 525  Training Loss  1.165855792351067e-05\n",
            "Epoch  48 Batch  186 / 525  Training Loss  1.3585643500846345e-05\n",
            "Epoch  48 Batch  187 / 525  Training Loss  1.6391313693020493e-05\n",
            "Epoch  48 Batch  188 / 525  Training Loss  1.1992767213087063e-05\n",
            "Epoch  48 Batch  189 / 525  Training Loss  9.566463631927036e-06\n",
            "Epoch  48 Batch  190 / 525  Training Loss  6.2909030020819046e-06\n",
            "Epoch  48 Batch  191 / 525  Training Loss  1.2552640328067355e-05\n",
            "Epoch  48 Batch  192 / 525  Training Loss  9.904786566039547e-06\n",
            "Epoch  48 Batch  193 / 525  Training Loss  9.276789569412358e-06\n",
            "Epoch  48 Batch  194 / 525  Training Loss  7.496654689020943e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  48 Batch  195 / 525  Training Loss  1.325421362707857e-05\n",
            "Epoch  48 Batch  196 / 525  Training Loss  1.2440259524737485e-05\n",
            "Epoch  48 Batch  197 / 525  Training Loss  1.3062048310530372e-05\n",
            "Epoch  48 Batch  198 / 525  Training Loss  6.76037370794802e-06\n",
            "Epoch  48 Batch  199 / 525  Training Loss  8.565672033000737e-06\n",
            "Epoch  48 Batch  200 / 525  Training Loss  1.6231722838710994e-05\n",
            "Epoch  48 Batch  201 / 525  Training Loss  9.679579306975938e-06\n",
            "Epoch  48 Batch  202 / 525  Training Loss  9.684763426776044e-06\n",
            "Epoch  48 Batch  203 / 525  Training Loss  1.4234565242077224e-05\n",
            "Epoch  48 Batch  204 / 525  Training Loss  1.2390402844175696e-05\n",
            "Epoch  48 Batch  205 / 525  Training Loss  1.4487619409919716e-05\n",
            "Epoch  48 Batch  206 / 525  Training Loss  1.1514659490785562e-05\n",
            "Epoch  48 Batch  207 / 525  Training Loss  6.387557277776068e-06\n",
            "Epoch  48 Batch  208 / 525  Training Loss  1.18364268928417e-05\n",
            "Epoch  48 Batch  209 / 525  Training Loss  1.1732934581232257e-05\n",
            "Epoch  48 Batch  210 / 525  Training Loss  1.0457675671204925e-05\n",
            "Epoch  48 Batch  211 / 525  Training Loss  2.032099655480124e-05\n",
            "Epoch  48 Batch  212 / 525  Training Loss  6.428609594877344e-06\n",
            "Epoch  48 Batch  213 / 525  Training Loss  1.0197066330874804e-05\n",
            "Epoch  48 Batch  214 / 525  Training Loss  1.0405570719740354e-05\n",
            "Epoch  48 Batch  215 / 525  Training Loss  1.1621305020526052e-05\n",
            "Epoch  48 Batch  216 / 525  Training Loss  1.6294170563924126e-05\n",
            "Epoch  48 Batch  217 / 525  Training Loss  1.0768529136839788e-05\n",
            "Epoch  48 Batch  218 / 525  Training Loss  9.810590199776925e-06\n",
            "Epoch  48 Batch  219 / 525  Training Loss  1.5731793610029854e-05\n",
            "Epoch  48 Batch  220 / 525  Training Loss  1.4448496585828252e-05\n",
            "Epoch  48 Batch  221 / 525  Training Loss  1.8128986994270235e-05\n",
            "Epoch  48 Batch  222 / 525  Training Loss  1.1251334399275947e-05\n",
            "Epoch  48 Batch  223 / 525  Training Loss  1.2057707863277756e-05\n",
            "Epoch  48 Batch  224 / 525  Training Loss  7.0677187977707945e-06\n",
            "Epoch  48 Batch  225 / 525  Training Loss  1.370669633615762e-05\n",
            "Epoch  48 Batch  226 / 525  Training Loss  1.0015590305556543e-05\n",
            "Epoch  48 Batch  227 / 525  Training Loss  9.851578397501726e-06\n",
            "Epoch  48 Batch  228 / 525  Training Loss  8.034341590246186e-06\n",
            "Epoch  48 Batch  229 / 525  Training Loss  1.5231038560159504e-05\n",
            "Epoch  48 Batch  230 / 525  Training Loss  9.91191882349085e-06\n",
            "Epoch  48 Batch  231 / 525  Training Loss  9.83590325631667e-06\n",
            "Epoch  48 Batch  232 / 525  Training Loss  1.0635781109158415e-05\n",
            "Epoch  48 Batch  233 / 525  Training Loss  9.206477443512995e-06\n",
            "Epoch  48 Batch  234 / 525  Training Loss  1.0447756721987389e-05\n",
            "Epoch  48 Batch  235 / 525  Training Loss  1.0708879926824011e-05\n",
            "Epoch  48 Batch  236 / 525  Training Loss  1.536411036795471e-05\n",
            "Epoch  48 Batch  237 / 525  Training Loss  1.6310514183714986e-05\n",
            "Epoch  48 Batch  238 / 525  Training Loss  1.346035878668772e-05\n",
            "Epoch  48 Batch  239 / 525  Training Loss  1.1292731869616546e-05\n",
            "Epoch  48 Batch  240 / 525  Training Loss  9.495449376117904e-06\n",
            "Epoch  48 Batch  241 / 525  Training Loss  1.5958736184984446e-05\n",
            "Epoch  48 Batch  242 / 525  Training Loss  1.0405996363260783e-05\n",
            "Epoch  48 Batch  243 / 525  Training Loss  1.3168330951884855e-05\n",
            "Epoch  48 Batch  244 / 525  Training Loss  1.490650356572587e-05\n",
            "Epoch  48 Batch  245 / 525  Training Loss  1.2296904060349334e-05\n",
            "Epoch  48 Batch  246 / 525  Training Loss  1.6414085621363483e-05\n",
            "Epoch  48 Batch  247 / 525  Training Loss  1.8636979802977294e-05\n",
            "Epoch  48 Batch  248 / 525  Training Loss  1.3672201021108776e-05\n",
            "Epoch  48 Batch  249 / 525  Training Loss  9.762535228219349e-06\n",
            "Epoch  48 Batch  250 / 525  Training Loss  1.0585744348645676e-05\n",
            "Epoch  48 Batch  251 / 525  Training Loss  1.7481992472312413e-05\n",
            "Epoch  48 Batch  252 / 525  Training Loss  8.10903475212399e-06\n",
            "Epoch  48 Batch  253 / 525  Training Loss  1.165723369922489e-05\n",
            "Epoch  48 Batch  254 / 525  Training Loss  1.803654777177144e-05\n",
            "Epoch  48 Batch  255 / 525  Training Loss  8.183359568647575e-06\n",
            "Epoch  48 Batch  256 / 525  Training Loss  1.4557915164914448e-05\n",
            "Epoch  48 Batch  257 / 525  Training Loss  1.1456292668299284e-05\n",
            "Epoch  48 Batch  258 / 525  Training Loss  7.160921086324379e-06\n",
            "Epoch  48 Batch  259 / 525  Training Loss  1.5907779015833512e-05\n",
            "Epoch  48 Batch  260 / 525  Training Loss  1.3543489330913872e-05\n",
            "Epoch  48 Batch  261 / 525  Training Loss  1.0183586709899828e-05\n",
            "Epoch  48 Batch  262 / 525  Training Loss  7.83345149102388e-06\n",
            "Epoch  48 Batch  263 / 525  Training Loss  1.138485640694853e-05\n",
            "Epoch  48 Batch  264 / 525  Training Loss  1.1593350791372359e-05\n",
            "Epoch  48 Batch  265 / 525  Training Loss  8.279585017589852e-06\n",
            "Epoch  48 Batch  266 / 525  Training Loss  1.1427438948885538e-05\n",
            "Epoch  48 Batch  267 / 525  Training Loss  1.7522053894936107e-05\n",
            "Epoch  48 Batch  268 / 525  Training Loss  1.2875684660684783e-05\n",
            "Epoch  48 Batch  269 / 525  Training Loss  1.0375868441769853e-05\n",
            "Epoch  48 Batch  270 / 525  Training Loss  1.1233928489673417e-05\n",
            "Epoch  48 Batch  271 / 525  Training Loss  1.4945801922294777e-05\n",
            "Epoch  48 Batch  272 / 525  Training Loss  7.299243407032918e-06\n",
            "Epoch  48 Batch  273 / 525  Training Loss  1.0902352187258657e-05\n",
            "Epoch  48 Batch  274 / 525  Training Loss  8.337321560247801e-06\n",
            "Epoch  48 Batch  275 / 525  Training Loss  7.610181000927696e-06\n",
            "Epoch  48 Batch  276 / 525  Training Loss  1.3760234651272185e-05\n",
            "Epoch  48 Batch  277 / 525  Training Loss  7.995722626219504e-06\n",
            "Epoch  48 Batch  278 / 525  Training Loss  1.1072637789766304e-05\n",
            "Epoch  48 Batch  279 / 525  Training Loss  1.3444894648273475e-05\n",
            "Epoch  48 Batch  280 / 525  Training Loss  1.3074645721644629e-05\n",
            "Epoch  48 Batch  281 / 525  Training Loss  1.902374242490623e-05\n",
            "Epoch  48 Batch  282 / 525  Training Loss  1.2571387742355e-05\n",
            "Epoch  48 Batch  283 / 525  Training Loss  2.2987989723333158e-05\n",
            "Epoch  48 Batch  284 / 525  Training Loss  1.5287549103959464e-05\n",
            "Epoch  48 Batch  285 / 525  Training Loss  7.516729965573177e-06\n",
            "Epoch  48 Batch  286 / 525  Training Loss  8.420072845183313e-06\n",
            "Epoch  48 Batch  287 / 525  Training Loss  1.5791269106557593e-05\n",
            "Epoch  48 Batch  288 / 525  Training Loss  1.4158142221276648e-05\n",
            "Epoch  48 Batch  289 / 525  Training Loss  1.043867723637959e-05\n",
            "Epoch  48 Batch  290 / 525  Training Loss  8.910808901418932e-06\n",
            "Epoch  48 Batch  291 / 525  Training Loss  8.233674634539057e-06\n",
            "Epoch  48 Batch  292 / 525  Training Loss  1.457384951208951e-05\n",
            "Epoch  48 Batch  293 / 525  Training Loss  1.2426867215253878e-05\n",
            "Epoch  48 Batch  294 / 525  Training Loss  1.2658914783969522e-05\n",
            "Epoch  48 Batch  295 / 525  Training Loss  1.0205667422269471e-05\n",
            "Epoch  48 Batch  296 / 525  Training Loss  1.0334338185202796e-05\n",
            "Epoch  48 Batch  297 / 525  Training Loss  1.1749340956157539e-05\n",
            "Epoch  48 Batch  298 / 525  Training Loss  1.3321348887984641e-05\n",
            "Epoch  48 Batch  299 / 525  Training Loss  1.5067404092405923e-05\n",
            "Epoch  48 Batch  300 / 525  Training Loss  1.0006229786085896e-05\n",
            "Epoch  48 Batch  301 / 525  Training Loss  1.4268848644860554e-05\n",
            "Epoch  48 Batch  302 / 525  Training Loss  6.00554494667449e-06\n",
            "Epoch  48 Batch  303 / 525  Training Loss  1.3398293049249332e-05\n",
            "Epoch  48 Batch  304 / 525  Training Loss  1.4897048458806239e-05\n",
            "Epoch  48 Batch  305 / 525  Training Loss  1.5264649846358225e-05\n",
            "Epoch  48 Batch  306 / 525  Training Loss  1.207618788612308e-05\n",
            "Epoch  48 Batch  307 / 525  Training Loss  1.0748890417744406e-05\n",
            "Epoch  48 Batch  308 / 525  Training Loss  9.529087947157677e-06\n",
            "Epoch  48 Batch  309 / 525  Training Loss  7.1476729317510035e-06\n",
            "Epoch  48 Batch  310 / 525  Training Loss  1.2435016287781764e-05\n",
            "Epoch  48 Batch  311 / 525  Training Loss  1.3628568922285922e-05\n",
            "Epoch  48 Batch  312 / 525  Training Loss  8.127162800519727e-06\n",
            "Epoch  48 Batch  313 / 525  Training Loss  1.5019662896520458e-05\n",
            "Epoch  48 Batch  314 / 525  Training Loss  1.3145334378350526e-05\n",
            "Epoch  48 Batch  315 / 525  Training Loss  1.0980991646647453e-05\n",
            "Epoch  48 Batch  316 / 525  Training Loss  1.058960424416e-05\n",
            "Epoch  48 Batch  317 / 525  Training Loss  1.169866300188005e-05\n",
            "Epoch  48 Batch  318 / 525  Training Loss  1.015376255963929e-05\n",
            "Epoch  48 Batch  319 / 525  Training Loss  1.2538065675471444e-05\n",
            "Epoch  48 Batch  320 / 525  Training Loss  7.793934855726548e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  48 Batch  321 / 525  Training Loss  1.2884345778729767e-05\n",
            "Epoch  48 Batch  322 / 525  Training Loss  9.01332350622397e-06\n",
            "Epoch  48 Batch  323 / 525  Training Loss  9.86805753200315e-06\n",
            "Epoch  48 Batch  324 / 525  Training Loss  1.027993221214274e-05\n",
            "Epoch  48 Batch  325 / 525  Training Loss  1.2071481251041405e-05\n",
            "Epoch  48 Batch  326 / 525  Training Loss  8.82383028510958e-06\n",
            "Epoch  48 Batch  327 / 525  Training Loss  9.010689609567635e-06\n",
            "Epoch  48 Batch  328 / 525  Training Loss  1.0856214430532418e-05\n",
            "Epoch  48 Batch  329 / 525  Training Loss  1.276924285775749e-05\n",
            "Epoch  48 Batch  330 / 525  Training Loss  8.464308848488145e-06\n",
            "Epoch  48 Batch  331 / 525  Training Loss  8.586479452787898e-06\n",
            "Epoch  48 Batch  332 / 525  Training Loss  1.401362351316493e-05\n",
            "Epoch  48 Batch  333 / 525  Training Loss  9.154775398201309e-06\n",
            "Epoch  48 Batch  334 / 525  Training Loss  1.2766577128786594e-05\n",
            "Epoch  48 Batch  335 / 525  Training Loss  1.6937789041548967e-05\n",
            "Epoch  48 Batch  336 / 525  Training Loss  1.096569758374244e-05\n",
            "Epoch  48 Batch  337 / 525  Training Loss  8.766839528107084e-06\n",
            "Epoch  48 Batch  338 / 525  Training Loss  9.823544132814277e-06\n",
            "Epoch  48 Batch  339 / 525  Training Loss  1.5291408999473788e-05\n",
            "Epoch  48 Batch  340 / 525  Training Loss  8.897320185496937e-06\n",
            "Epoch  48 Batch  341 / 525  Training Loss  1.3134731489117257e-05\n",
            "Epoch  48 Batch  342 / 525  Training Loss  1.0642233064572793e-05\n",
            "Epoch  48 Batch  343 / 525  Training Loss  1.3224937902123202e-05\n",
            "Epoch  48 Batch  344 / 525  Training Loss  1.0931211363640614e-05\n",
            "Epoch  48 Batch  345 / 525  Training Loss  1.3177576875023078e-05\n",
            "Epoch  48 Batch  346 / 525  Training Loss  1.842952406150289e-05\n",
            "Epoch  48 Batch  347 / 525  Training Loss  1.5688221537857316e-05\n",
            "Epoch  48 Batch  348 / 525  Training Loss  1.0860348993446678e-05\n",
            "Epoch  48 Batch  349 / 525  Training Loss  1.475934732297901e-05\n",
            "Epoch  48 Batch  350 / 525  Training Loss  1.442037319066003e-05\n",
            "Epoch  48 Batch  351 / 525  Training Loss  1.1528281902428716e-05\n",
            "Epoch  48 Batch  352 / 525  Training Loss  1.0608027878333814e-05\n",
            "Epoch  48 Batch  353 / 525  Training Loss  2.0219904399709776e-05\n",
            "Epoch  48 Batch  354 / 525  Training Loss  1.4086200280871708e-05\n",
            "Epoch  48 Batch  355 / 525  Training Loss  9.320546269009355e-06\n",
            "Epoch  48 Batch  356 / 525  Training Loss  4.715001068689162e-06\n",
            "Epoch  48 Batch  357 / 525  Training Loss  7.151531463023275e-06\n",
            "Epoch  48 Batch  358 / 525  Training Loss  1.5126465768844355e-05\n",
            "Epoch  48 Batch  359 / 525  Training Loss  1.621380215510726e-05\n",
            "Epoch  48 Batch  360 / 525  Training Loss  1.004410569294123e-05\n",
            "Epoch  48 Batch  361 / 525  Training Loss  1.4173792806104757e-05\n",
            "Epoch  48 Batch  362 / 525  Training Loss  1.6187415894819424e-05\n",
            "Epoch  48 Batch  363 / 525  Training Loss  1.074608189810533e-05\n",
            "Epoch  48 Batch  364 / 525  Training Loss  1.4662849935120903e-05\n",
            "Epoch  48 Batch  365 / 525  Training Loss  1.235188938153442e-05\n",
            "Epoch  48 Batch  366 / 525  Training Loss  9.724340998218395e-06\n",
            "Epoch  48 Batch  367 / 525  Training Loss  1.2329961464274675e-05\n",
            "Epoch  48 Batch  368 / 525  Training Loss  9.222962034982629e-06\n",
            "Epoch  48 Batch  369 / 525  Training Loss  1.0588380064291414e-05\n",
            "Epoch  48 Batch  370 / 525  Training Loss  8.944055480242241e-06\n",
            "Epoch  48 Batch  371 / 525  Training Loss  1.1953041394008324e-05\n",
            "Epoch  48 Batch  372 / 525  Training Loss  1.3053000657237135e-05\n",
            "Epoch  48 Batch  373 / 525  Training Loss  1.0183139238506556e-05\n",
            "Epoch  48 Batch  374 / 525  Training Loss  1.3991922969580628e-05\n",
            "Epoch  48 Batch  375 / 525  Training Loss  1.1870639355038293e-05\n",
            "Epoch  48 Batch  376 / 525  Training Loss  1.3548170500143897e-05\n",
            "Epoch  48 Batch  377 / 525  Training Loss  1.01163705039653e-05\n",
            "Epoch  48 Batch  378 / 525  Training Loss  1.5448542399099097e-05\n",
            "Epoch  48 Batch  379 / 525  Training Loss  1.1317992175463587e-05\n",
            "Epoch  48 Batch  380 / 525  Training Loss  1.1328059372317512e-05\n",
            "Epoch  48 Batch  381 / 525  Training Loss  1.677423460932914e-05\n",
            "Epoch  48 Batch  382 / 525  Training Loss  1.0628670679579955e-05\n",
            "Epoch  48 Batch  383 / 525  Training Loss  1.1754108527384233e-05\n",
            "Epoch  48 Batch  384 / 525  Training Loss  1.7469374142820016e-05\n",
            "Epoch  48 Batch  385 / 525  Training Loss  9.469753422308713e-06\n",
            "Epoch  48 Batch  386 / 525  Training Loss  1.5556710422970355e-05\n",
            "Epoch  48 Batch  387 / 525  Training Loss  8.060766049311496e-06\n",
            "Epoch  48 Batch  388 / 525  Training Loss  1.348719342786353e-05\n",
            "Epoch  48 Batch  389 / 525  Training Loss  6.262207079998916e-06\n",
            "Epoch  48 Batch  390 / 525  Training Loss  9.549691640131641e-06\n",
            "Epoch  48 Batch  391 / 525  Training Loss  1.0375767487857956e-05\n",
            "Epoch  48 Batch  392 / 525  Training Loss  1.2289264304854441e-05\n",
            "Epoch  48 Batch  393 / 525  Training Loss  1.0558540452620946e-05\n",
            "Epoch  48 Batch  394 / 525  Training Loss  1.3040090379945468e-05\n",
            "Epoch  48 Batch  395 / 525  Training Loss  9.106724974117242e-06\n",
            "Epoch  48 Batch  396 / 525  Training Loss  2.353537638555281e-05\n",
            "Epoch  48 Batch  397 / 525  Training Loss  7.89235855336301e-06\n",
            "Epoch  48 Batch  398 / 525  Training Loss  1.025161509460304e-05\n",
            "Epoch  48 Batch  399 / 525  Training Loss  1.030599196383264e-05\n",
            "Epoch  48 Batch  400 / 525  Training Loss  1.4412289601750672e-05\n",
            "Epoch  48 Batch  401 / 525  Training Loss  1.2283760952414013e-05\n",
            "Epoch  48 Batch  402 / 525  Training Loss  1.3918698641646188e-05\n",
            "Epoch  48 Batch  403 / 525  Training Loss  1.2704668733931612e-05\n",
            "Epoch  48 Batch  404 / 525  Training Loss  1.1745009942387696e-05\n",
            "Epoch  48 Batch  405 / 525  Training Loss  1.1990357961622067e-05\n",
            "Epoch  48 Batch  406 / 525  Training Loss  1.1567713045224082e-05\n",
            "Epoch  48 Batch  407 / 525  Training Loss  1.6273461369564757e-05\n",
            "Epoch  48 Batch  408 / 525  Training Loss  1.1732857274182606e-05\n",
            "Epoch  48 Batch  409 / 525  Training Loss  9.562458217260428e-06\n",
            "Epoch  48 Batch  410 / 525  Training Loss  1.4361993635247927e-05\n",
            "Epoch  48 Batch  411 / 525  Training Loss  8.68264578457456e-06\n",
            "Epoch  48 Batch  412 / 525  Training Loss  1.3319603567651939e-05\n",
            "Epoch  48 Batch  413 / 525  Training Loss  1.4206390005711e-05\n",
            "Epoch  48 Batch  414 / 525  Training Loss  1.746133784763515e-05\n",
            "Epoch  48 Batch  415 / 525  Training Loss  1.0120978913619183e-05\n",
            "Epoch  48 Batch  416 / 525  Training Loss  6.998635853960877e-06\n",
            "Epoch  48 Batch  417 / 525  Training Loss  6.28998259344371e-06\n",
            "Epoch  48 Batch  418 / 525  Training Loss  6.662349733232986e-06\n",
            "Epoch  48 Batch  419 / 525  Training Loss  1.376896852889331e-05\n",
            "Epoch  48 Batch  420 / 525  Training Loss  8.815200089884456e-06\n",
            "Epoch  48 Batch  421 / 525  Training Loss  6.484793175332015e-06\n",
            "Epoch  48 Batch  422 / 525  Training Loss  7.257623565237736e-06\n",
            "Epoch  48 Batch  423 / 525  Training Loss  1.4488537090073805e-05\n",
            "Epoch  48 Batch  424 / 525  Training Loss  1.0821597243193537e-05\n",
            "Epoch  48 Batch  425 / 525  Training Loss  1.2654770216613542e-05\n",
            "Epoch  48 Batch  426 / 525  Training Loss  1.2908887583762407e-05\n",
            "Epoch  48 Batch  427 / 525  Training Loss  5.101841907162452e-06\n",
            "Epoch  48 Batch  428 / 525  Training Loss  1.1228376934013795e-05\n",
            "Epoch  48 Batch  429 / 525  Training Loss  1.3296089491632301e-05\n",
            "Epoch  48 Batch  430 / 525  Training Loss  1.4924336937838234e-05\n",
            "Epoch  48 Batch  431 / 525  Training Loss  1.1152820661664009e-05\n",
            "Epoch  48 Batch  432 / 525  Training Loss  1.674315717536956e-05\n",
            "Epoch  48 Batch  433 / 525  Training Loss  1.1782558431150392e-05\n",
            "Epoch  48 Batch  434 / 525  Training Loss  7.397471108561149e-06\n",
            "Epoch  48 Batch  435 / 525  Training Loss  1.7566493625054136e-05\n",
            "Epoch  48 Batch  436 / 525  Training Loss  1.1426358469179831e-05\n",
            "Epoch  48 Batch  437 / 525  Training Loss  1.2094697922293562e-05\n",
            "Epoch  48 Batch  438 / 525  Training Loss  8.859643457981292e-06\n",
            "Epoch  48 Batch  439 / 525  Training Loss  1.0361319255025592e-05\n",
            "Epoch  48 Batch  440 / 525  Training Loss  9.083143595489673e-06\n",
            "Epoch  48 Batch  441 / 525  Training Loss  1.1552716387086548e-05\n",
            "Epoch  48 Batch  442 / 525  Training Loss  8.573888408136554e-06\n",
            "Epoch  48 Batch  443 / 525  Training Loss  8.202548997360282e-06\n",
            "Epoch  48 Batch  444 / 525  Training Loss  1.4047390322957654e-05\n",
            "Epoch  48 Batch  445 / 525  Training Loss  1.0667916285456158e-05\n",
            "Epoch  48 Batch  446 / 525  Training Loss  1.0779049262055196e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  48 Batch  447 / 525  Training Loss  8.671001523907762e-06\n",
            "Epoch  48 Batch  448 / 525  Training Loss  1.5037204320833553e-05\n",
            "Epoch  48 Batch  449 / 525  Training Loss  1.0903402653639205e-05\n",
            "Epoch  48 Batch  450 / 525  Training Loss  1.1829790310002863e-05\n",
            "Epoch  48 Batch  451 / 525  Training Loss  1.3611963368020952e-05\n",
            "Epoch  48 Batch  452 / 525  Training Loss  1.2615882951649837e-05\n",
            "Epoch  48 Batch  453 / 525  Training Loss  9.20603906706674e-06\n",
            "Epoch  48 Batch  454 / 525  Training Loss  1.8681917936191894e-05\n",
            "Epoch  48 Batch  455 / 525  Training Loss  1.279288153455127e-05\n",
            "Epoch  48 Batch  456 / 525  Training Loss  1.2628699550987221e-05\n",
            "Epoch  48 Batch  457 / 525  Training Loss  1.6052339560701512e-05\n",
            "Epoch  48 Batch  458 / 525  Training Loss  1.3394965208135545e-05\n",
            "Epoch  48 Batch  459 / 525  Training Loss  1.2248849088791758e-05\n",
            "Epoch  48 Batch  460 / 525  Training Loss  7.882972568040714e-06\n",
            "Epoch  48 Batch  461 / 525  Training Loss  1.0892102181969676e-05\n",
            "Epoch  48 Batch  462 / 525  Training Loss  1.1337791875121184e-05\n",
            "Epoch  48 Batch  463 / 525  Training Loss  1.0389668204879854e-05\n",
            "Epoch  48 Batch  464 / 525  Training Loss  1.404098111379426e-05\n",
            "Epoch  48 Batch  465 / 525  Training Loss  7.637317139597144e-06\n",
            "Epoch  48 Batch  466 / 525  Training Loss  1.3288622540130746e-05\n",
            "Epoch  48 Batch  467 / 525  Training Loss  9.990551916416734e-06\n",
            "Epoch  48 Batch  468 / 525  Training Loss  1.0871639460674487e-05\n",
            "Epoch  48 Batch  469 / 525  Training Loss  9.865184438240249e-06\n",
            "Epoch  48 Batch  470 / 525  Training Loss  8.21806588646723e-06\n",
            "Epoch  48 Batch  471 / 525  Training Loss  1.6528889318578877e-05\n",
            "Epoch  48 Batch  472 / 525  Training Loss  1.238559798366623e-05\n",
            "Epoch  48 Batch  473 / 525  Training Loss  1.5763611372676678e-05\n",
            "Epoch  48 Batch  474 / 525  Training Loss  1.460962539567845e-05\n",
            "Epoch  48 Batch  475 / 525  Training Loss  4.625767360266764e-06\n",
            "Epoch  48 Batch  476 / 525  Training Loss  1.1255276149313431e-05\n",
            "Epoch  48 Batch  477 / 525  Training Loss  1.1767140676965937e-05\n",
            "Epoch  48 Batch  478 / 525  Training Loss  1.5124338460736908e-05\n",
            "Epoch  48 Batch  479 / 525  Training Loss  1.1568607988010626e-05\n",
            "Epoch  48 Batch  480 / 525  Training Loss  1.420523039996624e-05\n",
            "Epoch  48 Batch  481 / 525  Training Loss  7.865866791689768e-06\n",
            "Epoch  48 Batch  482 / 525  Training Loss  1.2180506928416435e-05\n",
            "Epoch  48 Batch  483 / 525  Training Loss  1.6480564227094874e-05\n",
            "Epoch  48 Batch  484 / 525  Training Loss  1.4944258509785868e-05\n",
            "Epoch  48 Batch  485 / 525  Training Loss  1.3198885426390916e-05\n",
            "Epoch  48 Batch  486 / 525  Training Loss  1.1649382031464484e-05\n",
            "Epoch  48 Batch  487 / 525  Training Loss  1.506609987700358e-05\n",
            "Epoch  48 Batch  488 / 525  Training Loss  1.1234138582949527e-05\n",
            "Epoch  48 Batch  489 / 525  Training Loss  9.334826245321892e-06\n",
            "Epoch  48 Batch  490 / 525  Training Loss  6.7990622483193874e-06\n",
            "Epoch  48 Batch  491 / 525  Training Loss  1.1816598089353647e-05\n",
            "Epoch  48 Batch  492 / 525  Training Loss  1.7076094081858173e-05\n",
            "Epoch  48 Batch  493 / 525  Training Loss  1.1359873496985529e-05\n",
            "Epoch  48 Batch  494 / 525  Training Loss  9.826700079429429e-06\n",
            "Epoch  48 Batch  495 / 525  Training Loss  1.1817881386377849e-05\n",
            "Epoch  48 Batch  496 / 525  Training Loss  1.5320298189180903e-05\n",
            "Epoch  48 Batch  497 / 525  Training Loss  8.63806144479895e-06\n",
            "Epoch  48 Batch  498 / 525  Training Loss  1.3104632671456784e-05\n",
            "Epoch  48 Batch  499 / 525  Training Loss  1.5156813788053114e-05\n",
            "Epoch  48 Batch  500 / 525  Training Loss  9.240134204446804e-06\n",
            "Epoch  48 Batch  501 / 525  Training Loss  1.27911016534199e-05\n",
            "Epoch  48 Batch  502 / 525  Training Loss  1.4358518455992453e-05\n",
            "Epoch  48 Batch  503 / 525  Training Loss  9.508557923254557e-06\n",
            "Epoch  48 Batch  504 / 525  Training Loss  1.2083451110811438e-05\n",
            "Epoch  48 Batch  505 / 525  Training Loss  1.3777872482023668e-05\n",
            "Epoch  48 Batch  506 / 525  Training Loss  1.3300066711963154e-05\n",
            "Epoch  48 Batch  507 / 525  Training Loss  1.0179762284678873e-05\n",
            "Epoch  48 Batch  508 / 525  Training Loss  1.1720261682057753e-05\n",
            "Epoch  48 Batch  509 / 525  Training Loss  8.170986802724656e-06\n",
            "Epoch  48 Batch  510 / 525  Training Loss  9.201048669638112e-06\n",
            "Epoch  48 Batch  511 / 525  Training Loss  9.751256584422663e-06\n",
            "Epoch  48 Batch  512 / 525  Training Loss  1.282639004784869e-05\n",
            "Epoch  48 Batch  513 / 525  Training Loss  1.3632593436341267e-05\n",
            "Epoch  48 Batch  514 / 525  Training Loss  7.466946954082232e-06\n",
            "Epoch  48 Batch  515 / 525  Training Loss  1.0273074622091372e-05\n",
            "Epoch  48 Batch  516 / 525  Training Loss  1.5587072994094342e-05\n",
            "Epoch  48 Batch  517 / 525  Training Loss  1.1904594430234283e-05\n",
            "Epoch  48 Batch  518 / 525  Training Loss  7.707513759669382e-06\n",
            "Epoch  48 Batch  519 / 525  Training Loss  6.812014362367336e-06\n",
            "Epoch  48 Batch  520 / 525  Training Loss  1.632421299291309e-05\n",
            "Epoch  48 Batch  521 / 525  Training Loss  9.949457307811826e-06\n",
            "Epoch  48 Batch  522 / 525  Training Loss  1.2869588317698799e-05\n",
            "Epoch  48 Batch  523 / 525  Training Loss  8.490739674016368e-06\n",
            "Epoch  48 Batch  524 / 525  Training Loss  1.04332448245259e-05\n",
            "  49    |    -    |   0.000012   | 64.508333\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 49\n",
            "Epoch  49 Batch  0 / 525  Training Loss  7.504125733248657e-06\n",
            "Epoch  49 Batch  1 / 525  Training Loss  1.063988747773692e-05\n",
            "Epoch  49 Batch  2 / 525  Training Loss  1.2409199371177237e-05\n",
            "Epoch  49 Batch  3 / 525  Training Loss  1.3887159184378106e-05\n",
            "Epoch  49 Batch  4 / 525  Training Loss  1.0925866263278294e-05\n",
            "Epoch  49 Batch  5 / 525  Training Loss  1.0015636689786334e-05\n",
            "Epoch  49 Batch  6 / 525  Training Loss  8.130633432301693e-06\n",
            "Epoch  49 Batch  7 / 525  Training Loss  1.1075966540374793e-05\n",
            "Epoch  49 Batch  8 / 525  Training Loss  6.200443749548867e-06\n",
            "Epoch  49 Batch  9 / 525  Training Loss  7.502926564484369e-06\n",
            "Epoch  49 Batch  10 / 525  Training Loss  9.318866432295181e-06\n",
            "Epoch  49 Batch  11 / 525  Training Loss  1.4048577213543467e-05\n",
            "Epoch  49 Batch  12 / 525  Training Loss  9.642864824854769e-06\n",
            "Epoch  49 Batch  13 / 525  Training Loss  9.850440619629808e-06\n",
            "Epoch  49 Batch  14 / 525  Training Loss  1.1374152563803364e-05\n",
            "Epoch  49 Batch  15 / 525  Training Loss  8.919694664655253e-06\n",
            "Epoch  49 Batch  16 / 525  Training Loss  1.1794919373642188e-05\n",
            "Epoch  49 Batch  17 / 525  Training Loss  1.3248412869870663e-05\n",
            "Epoch  49 Batch  18 / 525  Training Loss  9.326708095613867e-06\n",
            "Epoch  49 Batch  19 / 525  Training Loss  5.415339728642721e-06\n",
            "Epoch  49 Batch  20 / 525  Training Loss  1.4959603504394181e-05\n",
            "Epoch  49 Batch  21 / 525  Training Loss  7.618028575961944e-06\n",
            "Epoch  49 Batch  22 / 525  Training Loss  1.3196366126067005e-05\n",
            "Epoch  49 Batch  23 / 525  Training Loss  1.8233185983262956e-05\n",
            "Epoch  49 Batch  24 / 525  Training Loss  8.20899640530115e-06\n",
            "Epoch  49 Batch  25 / 525  Training Loss  1.2191047062515281e-05\n",
            "Epoch  49 Batch  26 / 525  Training Loss  1.0304705938324332e-05\n",
            "Epoch  49 Batch  27 / 525  Training Loss  1.2750965652230661e-05\n",
            "Epoch  49 Batch  28 / 525  Training Loss  8.568867087888066e-06\n",
            "Epoch  49 Batch  29 / 525  Training Loss  6.757097253284883e-06\n",
            "Epoch  49 Batch  30 / 525  Training Loss  7.006564374023583e-06\n",
            "Epoch  49 Batch  31 / 525  Training Loss  1.3451792256091721e-05\n",
            "Epoch  49 Batch  32 / 525  Training Loss  1.0773984286061022e-05\n",
            "Epoch  49 Batch  33 / 525  Training Loss  1.193533626064891e-05\n",
            "Epoch  49 Batch  34 / 525  Training Loss  1.9751129002543166e-05\n",
            "Epoch  49 Batch  35 / 525  Training Loss  1.3488534023053944e-05\n",
            "Epoch  49 Batch  36 / 525  Training Loss  6.576591204066062e-06\n",
            "Epoch  49 Batch  37 / 525  Training Loss  9.203678018820938e-06\n",
            "Epoch  49 Batch  38 / 525  Training Loss  1.4541877135343384e-05\n",
            "Epoch  49 Batch  39 / 525  Training Loss  8.037718544073869e-06\n",
            "Epoch  49 Batch  40 / 525  Training Loss  1.1538768376340158e-05\n",
            "Epoch  49 Batch  41 / 525  Training Loss  1.509582762082573e-05\n",
            "Epoch  49 Batch  42 / 525  Training Loss  1.1607020496740006e-05\n",
            "Epoch  49 Batch  43 / 525  Training Loss  1.8384056602371857e-05\n",
            "Epoch  49 Batch  44 / 525  Training Loss  8.414639523834921e-06\n",
            "Epoch  49 Batch  45 / 525  Training Loss  1.1460191672085784e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  49 Batch  46 / 525  Training Loss  1.5827401512069628e-05\n",
            "Epoch  49 Batch  47 / 525  Training Loss  5.518031684914604e-06\n",
            "Epoch  49 Batch  48 / 525  Training Loss  9.098607733903918e-06\n",
            "Epoch  49 Batch  49 / 525  Training Loss  9.780715117813088e-06\n",
            "Epoch  49 Batch  50 / 525  Training Loss  7.336067938013002e-06\n",
            "Epoch  49 Batch  51 / 525  Training Loss  7.2250513767357916e-06\n",
            "Epoch  49 Batch  52 / 525  Training Loss  1.1537802492966875e-05\n",
            "Epoch  49 Batch  53 / 525  Training Loss  6.63518358123838e-06\n",
            "Epoch  49 Batch  54 / 525  Training Loss  6.794080491090426e-06\n",
            "Epoch  49 Batch  55 / 525  Training Loss  4.713228008768056e-06\n",
            "Epoch  49 Batch  56 / 525  Training Loss  1.2280686860322021e-05\n",
            "Epoch  49 Batch  57 / 525  Training Loss  9.605954801372718e-06\n",
            "Epoch  49 Batch  58 / 525  Training Loss  9.52111258811783e-06\n",
            "Epoch  49 Batch  59 / 525  Training Loss  7.420737347274553e-06\n",
            "Epoch  49 Batch  60 / 525  Training Loss  1.1725353033398278e-05\n",
            "Epoch  49 Batch  61 / 525  Training Loss  1.1188402822881471e-05\n",
            "Epoch  49 Batch  62 / 525  Training Loss  6.3468055486737285e-06\n",
            "Epoch  49 Batch  63 / 525  Training Loss  9.422308721696027e-06\n",
            "Epoch  49 Batch  64 / 525  Training Loss  1.3770503755949903e-05\n",
            "Epoch  49 Batch  65 / 525  Training Loss  1.8719296349445358e-05\n",
            "Epoch  49 Batch  66 / 525  Training Loss  1.2559461538330652e-05\n",
            "Epoch  49 Batch  67 / 525  Training Loss  8.8815522758523e-06\n",
            "Epoch  49 Batch  68 / 525  Training Loss  1.783326115400996e-05\n",
            "Epoch  49 Batch  69 / 525  Training Loss  1.1082493983849417e-05\n",
            "Epoch  49 Batch  70 / 525  Training Loss  9.143271199718583e-06\n",
            "Epoch  49 Batch  71 / 525  Training Loss  1.5149987120821606e-05\n",
            "Epoch  49 Batch  72 / 525  Training Loss  1.3780399058305193e-05\n",
            "Epoch  49 Batch  73 / 525  Training Loss  1.675203202466946e-05\n",
            "Epoch  49 Batch  74 / 525  Training Loss  1.6027461242629215e-05\n",
            "Epoch  49 Batch  75 / 525  Training Loss  1.150165462604491e-05\n",
            "Epoch  49 Batch  76 / 525  Training Loss  1.3676853086508345e-05\n",
            "Epoch  49 Batch  77 / 525  Training Loss  9.061179298441857e-06\n",
            "Epoch  49 Batch  78 / 525  Training Loss  6.493222372228047e-06\n",
            "Epoch  49 Batch  79 / 525  Training Loss  7.01210319675738e-06\n",
            "Epoch  49 Batch  80 / 525  Training Loss  1.4876526620355435e-05\n",
            "Epoch  49 Batch  81 / 525  Training Loss  7.097405614331365e-06\n",
            "Epoch  49 Batch  82 / 525  Training Loss  7.93727576819947e-06\n",
            "Epoch  49 Batch  83 / 525  Training Loss  6.793693955842173e-06\n",
            "Epoch  49 Batch  84 / 525  Training Loss  9.442748705623671e-06\n",
            "Epoch  49 Batch  85 / 525  Training Loss  7.0837440944160335e-06\n",
            "Epoch  49 Batch  86 / 525  Training Loss  1.1477881344035268e-05\n",
            "Epoch  49 Batch  87 / 525  Training Loss  8.727031854505185e-06\n",
            "Epoch  49 Batch  88 / 525  Training Loss  9.772251360118389e-06\n",
            "Epoch  49 Batch  89 / 525  Training Loss  1.4664098671346437e-05\n",
            "Epoch  49 Batch  90 / 525  Training Loss  1.0813680091814604e-05\n",
            "Epoch  49 Batch  91 / 525  Training Loss  9.413530278834514e-06\n",
            "Epoch  49 Batch  92 / 525  Training Loss  7.38398512112326e-06\n",
            "Epoch  49 Batch  93 / 525  Training Loss  1.3701192983717192e-05\n",
            "Epoch  49 Batch  94 / 525  Training Loss  1.0779040167108178e-05\n",
            "Epoch  49 Batch  95 / 525  Training Loss  6.255861080717295e-06\n",
            "Epoch  49 Batch  96 / 525  Training Loss  6.2630701904708985e-06\n",
            "Epoch  49 Batch  97 / 525  Training Loss  8.883739610610064e-06\n",
            "Epoch  49 Batch  98 / 525  Training Loss  9.115688953897916e-06\n",
            "Epoch  49 Batch  99 / 525  Training Loss  6.522667717945296e-06\n",
            "Epoch  49 Batch  100 / 525  Training Loss  9.928920007951092e-06\n",
            "Epoch  49 Batch  101 / 525  Training Loss  5.8025825637741946e-06\n",
            "Epoch  49 Batch  102 / 525  Training Loss  9.114672138821334e-06\n",
            "Epoch  49 Batch  103 / 525  Training Loss  3.693778808155912e-06\n",
            "Epoch  49 Batch  104 / 525  Training Loss  1.3201633919379674e-05\n",
            "Epoch  49 Batch  105 / 525  Training Loss  1.3169691555958707e-05\n",
            "Epoch  49 Batch  106 / 525  Training Loss  4.697426447819453e-06\n",
            "Epoch  49 Batch  107 / 525  Training Loss  9.691534614830744e-06\n",
            "Epoch  49 Batch  108 / 525  Training Loss  8.067037924774922e-06\n",
            "Epoch  49 Batch  109 / 525  Training Loss  1.0962428859784268e-05\n",
            "Epoch  49 Batch  110 / 525  Training Loss  6.026124992786208e-06\n",
            "Epoch  49 Batch  111 / 525  Training Loss  8.18934404378524e-06\n",
            "Epoch  49 Batch  112 / 525  Training Loss  9.637251423555426e-06\n",
            "Epoch  49 Batch  113 / 525  Training Loss  1.1696543879224919e-05\n",
            "Epoch  49 Batch  114 / 525  Training Loss  9.771203622221947e-06\n",
            "Epoch  49 Batch  115 / 525  Training Loss  1.1761005225707777e-05\n",
            "Epoch  49 Batch  116 / 525  Training Loss  9.348105777462479e-06\n",
            "Epoch  49 Batch  117 / 525  Training Loss  5.8022942539537326e-06\n",
            "Epoch  49 Batch  118 / 525  Training Loss  1.486981454945635e-05\n",
            "Epoch  49 Batch  119 / 525  Training Loss  1.0084377208841033e-05\n",
            "Epoch  49 Batch  120 / 525  Training Loss  8.757189789321274e-06\n",
            "Epoch  49 Batch  121 / 525  Training Loss  1.4659992302767932e-05\n",
            "Epoch  49 Batch  122 / 525  Training Loss  1.0862977433134802e-05\n",
            "Epoch  49 Batch  123 / 525  Training Loss  7.439643468387658e-06\n",
            "Epoch  49 Batch  124 / 525  Training Loss  1.2833947039325722e-05\n",
            "Epoch  49 Batch  125 / 525  Training Loss  1.4040048881724942e-05\n",
            "Epoch  49 Batch  126 / 525  Training Loss  1.203853935294319e-05\n",
            "Epoch  49 Batch  127 / 525  Training Loss  1.2688808965322096e-05\n",
            "Epoch  49 Batch  128 / 525  Training Loss  1.2881047950941138e-05\n",
            "Epoch  49 Batch  129 / 525  Training Loss  7.697282853769138e-06\n",
            "Epoch  49 Batch  130 / 525  Training Loss  6.4191854107775725e-06\n",
            "Epoch  49 Batch  131 / 525  Training Loss  8.508314749633428e-06\n",
            "Epoch  49 Batch  132 / 525  Training Loss  1.3769038559985347e-05\n",
            "Epoch  49 Batch  133 / 525  Training Loss  9.946969839802478e-06\n",
            "Epoch  49 Batch  134 / 525  Training Loss  9.445247997064143e-06\n",
            "Epoch  49 Batch  135 / 525  Training Loss  8.632349818071816e-06\n",
            "Epoch  49 Batch  136 / 525  Training Loss  1.1221992281207349e-05\n",
            "Epoch  49 Batch  137 / 525  Training Loss  1.0921086868620478e-05\n",
            "Epoch  49 Batch  138 / 525  Training Loss  7.805135282978881e-06\n",
            "Epoch  49 Batch  139 / 525  Training Loss  8.140425961755682e-06\n",
            "Epoch  49 Batch  140 / 525  Training Loss  9.211810720444191e-06\n",
            "Epoch  49 Batch  141 / 525  Training Loss  1.4872406609356403e-05\n",
            "Epoch  49 Batch  142 / 525  Training Loss  1.109241111407755e-05\n",
            "Epoch  49 Batch  143 / 525  Training Loss  1.1695448847603984e-05\n",
            "Epoch  49 Batch  144 / 525  Training Loss  1.6569831132073887e-05\n",
            "Epoch  49 Batch  145 / 525  Training Loss  6.996576757956063e-06\n",
            "Epoch  49 Batch  146 / 525  Training Loss  1.2294551197555847e-05\n",
            "Epoch  49 Batch  147 / 525  Training Loss  9.690589649835601e-06\n",
            "Epoch  49 Batch  148 / 525  Training Loss  1.010320738714654e-05\n",
            "Epoch  49 Batch  149 / 525  Training Loss  8.68843744683545e-06\n",
            "Epoch  49 Batch  150 / 525  Training Loss  1.1598190212680493e-05\n",
            "Epoch  49 Batch  151 / 525  Training Loss  1.090204568754416e-05\n",
            "Epoch  49 Batch  152 / 525  Training Loss  7.780101441312581e-06\n",
            "Epoch  49 Batch  153 / 525  Training Loss  1.2590411643031985e-05\n",
            "Epoch  49 Batch  154 / 525  Training Loss  8.321867426275276e-06\n",
            "Epoch  49 Batch  155 / 525  Training Loss  9.72652378550265e-06\n",
            "Epoch  49 Batch  156 / 525  Training Loss  1.531478119431995e-05\n",
            "Epoch  49 Batch  157 / 525  Training Loss  8.746485946176108e-06\n",
            "Epoch  49 Batch  158 / 525  Training Loss  1.4489276509266347e-05\n",
            "Epoch  49 Batch  159 / 525  Training Loss  1.2065701412211638e-05\n",
            "Epoch  49 Batch  160 / 525  Training Loss  1.1413362699386198e-05\n",
            "Epoch  49 Batch  161 / 525  Training Loss  1.3772958482149988e-05\n",
            "Epoch  49 Batch  162 / 525  Training Loss  1.5224532035063021e-05\n",
            "Epoch  49 Batch  163 / 525  Training Loss  7.59849717724137e-06\n",
            "Epoch  49 Batch  164 / 525  Training Loss  9.164416042040102e-06\n",
            "Epoch  49 Batch  165 / 525  Training Loss  8.771241482463665e-06\n",
            "Epoch  49 Batch  166 / 525  Training Loss  8.645059097034391e-06\n",
            "Epoch  49 Batch  167 / 525  Training Loss  1.2349193639238365e-05\n",
            "Epoch  49 Batch  168 / 525  Training Loss  1.2302443792577833e-05\n",
            "Epoch  49 Batch  169 / 525  Training Loss  8.922530469135381e-06\n",
            "Epoch  49 Batch  170 / 525  Training Loss  6.034456419001799e-06\n",
            "Epoch  49 Batch  171 / 525  Training Loss  1.303170392930042e-05\n",
            "Epoch  49 Batch  172 / 525  Training Loss  6.7312116698303726e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  49 Batch  173 / 525  Training Loss  1.1234948942728806e-05\n",
            "Epoch  49 Batch  174 / 525  Training Loss  1.0467067113495432e-05\n",
            "Epoch  49 Batch  175 / 525  Training Loss  1.1290012480458245e-05\n",
            "Epoch  49 Batch  176 / 525  Training Loss  1.2236761904205196e-05\n",
            "Epoch  49 Batch  177 / 525  Training Loss  1.8518237993703224e-05\n",
            "Epoch  49 Batch  178 / 525  Training Loss  1.3204933566157706e-05\n",
            "Epoch  49 Batch  179 / 525  Training Loss  1.2241182957950514e-05\n",
            "Epoch  49 Batch  180 / 525  Training Loss  1.0980571460095234e-05\n",
            "Epoch  49 Batch  181 / 525  Training Loss  7.559327059425414e-06\n",
            "Epoch  49 Batch  182 / 525  Training Loss  1.3488682270690333e-05\n",
            "Epoch  49 Batch  183 / 525  Training Loss  1.2691828487731982e-05\n",
            "Epoch  49 Batch  184 / 525  Training Loss  1.5435920431627892e-05\n",
            "Epoch  49 Batch  185 / 525  Training Loss  1.158704708359437e-05\n",
            "Epoch  49 Batch  186 / 525  Training Loss  8.80498282640474e-06\n",
            "Epoch  49 Batch  187 / 525  Training Loss  6.767118065909017e-06\n",
            "Epoch  49 Batch  188 / 525  Training Loss  1.226612221216783e-05\n",
            "Epoch  49 Batch  189 / 525  Training Loss  1.3499520719051361e-05\n",
            "Epoch  49 Batch  190 / 525  Training Loss  1.3242119166534394e-05\n",
            "Epoch  49 Batch  191 / 525  Training Loss  1.10686341940891e-05\n",
            "Epoch  49 Batch  192 / 525  Training Loss  1.51077647387865e-05\n",
            "Epoch  49 Batch  193 / 525  Training Loss  1.0177091098739766e-05\n",
            "Epoch  49 Batch  194 / 525  Training Loss  1.1170117431902327e-05\n",
            "Epoch  49 Batch  195 / 525  Training Loss  1.5105055354069918e-05\n",
            "Epoch  49 Batch  196 / 525  Training Loss  1.0709127309382893e-05\n",
            "Epoch  49 Batch  197 / 525  Training Loss  1.5442554285982624e-05\n",
            "Epoch  49 Batch  198 / 525  Training Loss  1.4330709745991044e-05\n",
            "Epoch  49 Batch  199 / 525  Training Loss  1.2286208402656484e-05\n",
            "Epoch  49 Batch  200 / 525  Training Loss  1.1234216799493879e-05\n",
            "Epoch  49 Batch  201 / 525  Training Loss  9.556034456181806e-06\n",
            "Epoch  49 Batch  202 / 525  Training Loss  1.5019892089185305e-05\n",
            "Epoch  49 Batch  203 / 525  Training Loss  1.4928739801689517e-05\n",
            "Epoch  49 Batch  204 / 525  Training Loss  1.1399096365494188e-05\n",
            "Epoch  49 Batch  205 / 525  Training Loss  1.1789240488724317e-05\n",
            "Epoch  49 Batch  206 / 525  Training Loss  1.0013389328378253e-05\n",
            "Epoch  49 Batch  207 / 525  Training Loss  1.4211349480319768e-05\n",
            "Epoch  49 Batch  208 / 525  Training Loss  5.503244210558478e-06\n",
            "Epoch  49 Batch  209 / 525  Training Loss  9.275916454498656e-06\n",
            "Epoch  49 Batch  210 / 525  Training Loss  7.112873390724417e-06\n",
            "Epoch  49 Batch  211 / 525  Training Loss  1.3041109014011454e-05\n",
            "Epoch  49 Batch  212 / 525  Training Loss  1.2088214134564623e-05\n",
            "Epoch  49 Batch  213 / 525  Training Loss  9.39049459702801e-06\n",
            "Epoch  49 Batch  214 / 525  Training Loss  9.969423444999848e-06\n",
            "Epoch  49 Batch  215 / 525  Training Loss  1.3212911653681658e-05\n",
            "Epoch  49 Batch  216 / 525  Training Loss  1.1358300980646163e-05\n",
            "Epoch  49 Batch  217 / 525  Training Loss  1.1892754628206603e-05\n",
            "Epoch  49 Batch  218 / 525  Training Loss  1.1105386874987744e-05\n",
            "Epoch  49 Batch  219 / 525  Training Loss  9.112065527006052e-06\n",
            "Epoch  49 Batch  220 / 525  Training Loss  9.075179150386248e-06\n",
            "Epoch  49 Batch  221 / 525  Training Loss  1.2486227205954492e-05\n",
            "Epoch  49 Batch  222 / 525  Training Loss  1.05135277408408e-05\n",
            "Epoch  49 Batch  223 / 525  Training Loss  8.867399628798012e-06\n",
            "Epoch  49 Batch  224 / 525  Training Loss  8.868548320606351e-06\n",
            "Epoch  49 Batch  225 / 525  Training Loss  1.1874984920723364e-05\n",
            "Epoch  49 Batch  226 / 525  Training Loss  9.96188373392215e-06\n",
            "Epoch  49 Batch  227 / 525  Training Loss  1.3322702216100879e-05\n",
            "Epoch  49 Batch  228 / 525  Training Loss  7.53217773308279e-06\n",
            "Epoch  49 Batch  229 / 525  Training Loss  1.250197510671569e-05\n",
            "Epoch  49 Batch  230 / 525  Training Loss  1.0436530828883406e-05\n",
            "Epoch  49 Batch  231 / 525  Training Loss  1.3018815479881596e-05\n",
            "Epoch  49 Batch  232 / 525  Training Loss  1.100553163269069e-05\n",
            "Epoch  49 Batch  233 / 525  Training Loss  8.071025149547495e-06\n",
            "Epoch  49 Batch  234 / 525  Training Loss  1.2591025551955681e-05\n",
            "Epoch  49 Batch  235 / 525  Training Loss  1.5119077943381853e-05\n",
            "Epoch  49 Batch  236 / 525  Training Loss  1.6055982996476814e-05\n",
            "Epoch  49 Batch  237 / 525  Training Loss  1.1749635632440913e-05\n",
            "Epoch  49 Batch  238 / 525  Training Loss  1.0207321793131996e-05\n",
            "Epoch  49 Batch  239 / 525  Training Loss  8.897751285985578e-06\n",
            "Epoch  49 Batch  240 / 525  Training Loss  1.1517549864947796e-05\n",
            "Epoch  49 Batch  241 / 525  Training Loss  8.972723662736826e-06\n",
            "Epoch  49 Batch  242 / 525  Training Loss  9.112087354878895e-06\n",
            "Epoch  49 Batch  243 / 525  Training Loss  1.0876186934183352e-05\n",
            "Epoch  49 Batch  244 / 525  Training Loss  1.4210023437044583e-05\n",
            "Epoch  49 Batch  245 / 525  Training Loss  1.0613972335704602e-05\n",
            "Epoch  49 Batch  246 / 525  Training Loss  1.3055930139671545e-05\n",
            "Epoch  49 Batch  247 / 525  Training Loss  1.339582922810223e-05\n",
            "Epoch  49 Batch  248 / 525  Training Loss  1.0581405149423517e-05\n",
            "Epoch  49 Batch  249 / 525  Training Loss  8.763401638134383e-06\n",
            "Epoch  49 Batch  250 / 525  Training Loss  1.2558414709928911e-05\n",
            "Epoch  49 Batch  251 / 525  Training Loss  9.423302799405064e-06\n",
            "Epoch  49 Batch  252 / 525  Training Loss  9.60258512350265e-06\n",
            "Epoch  49 Batch  253 / 525  Training Loss  1.220869853568729e-05\n",
            "Epoch  49 Batch  254 / 525  Training Loss  1.7478832887718454e-05\n",
            "Epoch  49 Batch  255 / 525  Training Loss  1.3428450984065421e-05\n",
            "Epoch  49 Batch  256 / 525  Training Loss  8.908844392863102e-06\n",
            "Epoch  49 Batch  257 / 525  Training Loss  1.232091472047614e-05\n",
            "Epoch  49 Batch  258 / 525  Training Loss  1.2666925613302737e-05\n",
            "Epoch  49 Batch  259 / 525  Training Loss  7.878794349380769e-06\n",
            "Epoch  49 Batch  260 / 525  Training Loss  1.3186521755415015e-05\n",
            "Epoch  49 Batch  261 / 525  Training Loss  9.84432153927628e-06\n",
            "Epoch  49 Batch  262 / 525  Training Loss  8.724172403162811e-06\n",
            "Epoch  49 Batch  263 / 525  Training Loss  1.3937513358541764e-05\n",
            "Epoch  49 Batch  264 / 525  Training Loss  1.2262389645911753e-05\n",
            "Epoch  49 Batch  265 / 525  Training Loss  9.772053999768104e-06\n",
            "Epoch  49 Batch  266 / 525  Training Loss  1.0392849617346656e-05\n",
            "Epoch  49 Batch  267 / 525  Training Loss  1.3284268788993359e-05\n",
            "Epoch  49 Batch  268 / 525  Training Loss  8.688411980983801e-06\n",
            "Epoch  49 Batch  269 / 525  Training Loss  1.3060045603197068e-05\n",
            "Epoch  49 Batch  270 / 525  Training Loss  1.0254356311634183e-05\n",
            "Epoch  49 Batch  271 / 525  Training Loss  1.0918177395069506e-05\n",
            "Epoch  49 Batch  272 / 525  Training Loss  8.610414624854457e-06\n",
            "Epoch  49 Batch  273 / 525  Training Loss  1.0425997061247472e-05\n",
            "Epoch  49 Batch  274 / 525  Training Loss  9.707677236292511e-06\n",
            "Epoch  49 Batch  275 / 525  Training Loss  8.357767001143657e-06\n",
            "Epoch  49 Batch  276 / 525  Training Loss  1.0668059985619038e-05\n",
            "Epoch  49 Batch  277 / 525  Training Loss  1.4566097888746299e-05\n",
            "Epoch  49 Batch  278 / 525  Training Loss  8.237911060859915e-06\n",
            "Epoch  49 Batch  279 / 525  Training Loss  1.0470037523191422e-05\n",
            "Epoch  49 Batch  280 / 525  Training Loss  1.3223841961007565e-05\n",
            "Epoch  49 Batch  281 / 525  Training Loss  1.2178451470390428e-05\n",
            "Epoch  49 Batch  282 / 525  Training Loss  1.688690099399537e-05\n",
            "Epoch  49 Batch  283 / 525  Training Loss  9.435529136680998e-06\n",
            "Epoch  49 Batch  284 / 525  Training Loss  9.41942744248081e-06\n",
            "Epoch  49 Batch  285 / 525  Training Loss  1.3938212759967428e-05\n",
            "Epoch  49 Batch  286 / 525  Training Loss  1.222227183461655e-05\n",
            "Epoch  49 Batch  287 / 525  Training Loss  1.069545942300465e-05\n",
            "Epoch  49 Batch  288 / 525  Training Loss  8.135542884701863e-06\n",
            "Epoch  49 Batch  289 / 525  Training Loss  1.2307451470405795e-05\n",
            "Epoch  49 Batch  290 / 525  Training Loss  1.0889760233112611e-05\n",
            "Epoch  49 Batch  291 / 525  Training Loss  9.01216480997391e-06\n",
            "Epoch  49 Batch  292 / 525  Training Loss  1.1186793017259333e-05\n",
            "Epoch  49 Batch  293 / 525  Training Loss  1.2721048733510543e-05\n",
            "Epoch  49 Batch  294 / 525  Training Loss  9.800411135074683e-06\n",
            "Epoch  49 Batch  295 / 525  Training Loss  8.827404599287547e-06\n",
            "Epoch  49 Batch  296 / 525  Training Loss  1.004634214041289e-05\n",
            "Epoch  49 Batch  297 / 525  Training Loss  1.6380916349589825e-05\n",
            "Epoch  49 Batch  298 / 525  Training Loss  9.101265277422499e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  49 Batch  299 / 525  Training Loss  1.6196117940125987e-05\n",
            "Epoch  49 Batch  300 / 525  Training Loss  9.819018487178255e-06\n",
            "Epoch  49 Batch  301 / 525  Training Loss  1.2826963029510807e-05\n",
            "Epoch  49 Batch  302 / 525  Training Loss  9.6164103524643e-06\n",
            "Epoch  49 Batch  303 / 525  Training Loss  9.457777196075767e-06\n",
            "Epoch  49 Batch  304 / 525  Training Loss  1.4692224794998765e-05\n",
            "Epoch  49 Batch  305 / 525  Training Loss  1.2607863936864305e-05\n",
            "Epoch  49 Batch  306 / 525  Training Loss  1.4500241377390921e-05\n",
            "Epoch  49 Batch  307 / 525  Training Loss  1.3283410225994885e-05\n",
            "Epoch  49 Batch  308 / 525  Training Loss  1.27516013890272e-05\n",
            "Epoch  49 Batch  309 / 525  Training Loss  1.2469659850466996e-05\n",
            "Epoch  49 Batch  310 / 525  Training Loss  1.2513070032582618e-05\n",
            "Epoch  49 Batch  311 / 525  Training Loss  1.4308405297924764e-05\n",
            "Epoch  49 Batch  312 / 525  Training Loss  1.6285423043882474e-05\n",
            "Epoch  49 Batch  313 / 525  Training Loss  1.4895764252287336e-05\n",
            "Epoch  49 Batch  314 / 525  Training Loss  1.2029042409267277e-05\n",
            "Epoch  49 Batch  315 / 525  Training Loss  1.4776948773942422e-05\n",
            "Epoch  49 Batch  316 / 525  Training Loss  1.2601754860952497e-05\n",
            "Epoch  49 Batch  317 / 525  Training Loss  9.235414836439304e-06\n",
            "Epoch  49 Batch  318 / 525  Training Loss  6.075797955418238e-06\n",
            "Epoch  49 Batch  319 / 525  Training Loss  9.801549822441302e-06\n",
            "Epoch  49 Batch  320 / 525  Training Loss  9.304008926847018e-06\n",
            "Epoch  49 Batch  321 / 525  Training Loss  1.6814776245155372e-05\n",
            "Epoch  49 Batch  322 / 525  Training Loss  1.1237957551202271e-05\n",
            "Epoch  49 Batch  323 / 525  Training Loss  1.119677381211659e-05\n",
            "Epoch  49 Batch  324 / 525  Training Loss  1.2529160812846385e-05\n",
            "Epoch  49 Batch  325 / 525  Training Loss  1.1272246410953812e-05\n",
            "Epoch  49 Batch  326 / 525  Training Loss  8.883466762199532e-06\n",
            "Epoch  49 Batch  327 / 525  Training Loss  6.49833327770466e-06\n",
            "Epoch  49 Batch  328 / 525  Training Loss  1.1270246432104614e-05\n",
            "Epoch  49 Batch  329 / 525  Training Loss  1.119461921916809e-05\n",
            "Epoch  49 Batch  330 / 525  Training Loss  1.037830861605471e-05\n",
            "Epoch  49 Batch  331 / 525  Training Loss  1.1706051736837253e-05\n",
            "Epoch  49 Batch  332 / 525  Training Loss  7.6216701927478425e-06\n",
            "Epoch  49 Batch  333 / 525  Training Loss  8.599582542956341e-06\n",
            "Epoch  49 Batch  334 / 525  Training Loss  8.293181963381357e-06\n",
            "Epoch  49 Batch  335 / 525  Training Loss  1.0019782166637015e-05\n",
            "Epoch  49 Batch  336 / 525  Training Loss  1.4427295354835223e-05\n",
            "Epoch  49 Batch  337 / 525  Training Loss  6.558543645951431e-06\n",
            "Epoch  49 Batch  338 / 525  Training Loss  1.7181551811518148e-05\n",
            "Epoch  49 Batch  339 / 525  Training Loss  9.518428669252899e-06\n",
            "Epoch  49 Batch  340 / 525  Training Loss  1.0054926860902924e-05\n",
            "Epoch  49 Batch  341 / 525  Training Loss  2.5000732421176508e-05\n",
            "Epoch  49 Batch  342 / 525  Training Loss  9.831693205342162e-06\n",
            "Epoch  49 Batch  343 / 525  Training Loss  1.2810237421945203e-05\n",
            "Epoch  49 Batch  344 / 525  Training Loss  8.942451131588314e-06\n",
            "Epoch  49 Batch  345 / 525  Training Loss  1.412243909726385e-05\n",
            "Epoch  49 Batch  346 / 525  Training Loss  8.3701006587944e-06\n",
            "Epoch  49 Batch  347 / 525  Training Loss  1.2707340829365421e-05\n",
            "Epoch  49 Batch  348 / 525  Training Loss  1.2090922609786503e-05\n",
            "Epoch  49 Batch  349 / 525  Training Loss  1.1265517059655394e-05\n",
            "Epoch  49 Batch  350 / 525  Training Loss  9.895678886095993e-06\n",
            "Epoch  49 Batch  351 / 525  Training Loss  8.501715456077363e-06\n",
            "Epoch  49 Batch  352 / 525  Training Loss  1.258698739547981e-05\n",
            "Epoch  49 Batch  353 / 525  Training Loss  1.0264279808325227e-05\n",
            "Epoch  49 Batch  354 / 525  Training Loss  6.885634320497047e-06\n",
            "Epoch  49 Batch  355 / 525  Training Loss  8.65167112351628e-06\n",
            "Epoch  49 Batch  356 / 525  Training Loss  8.078780410869513e-06\n",
            "Epoch  49 Batch  357 / 525  Training Loss  1.5080612683959771e-05\n",
            "Epoch  49 Batch  358 / 525  Training Loss  9.600888006389141e-06\n",
            "Epoch  49 Batch  359 / 525  Training Loss  1.1969898878305685e-05\n",
            "Epoch  49 Batch  360 / 525  Training Loss  1.3696648238692433e-05\n",
            "Epoch  49 Batch  361 / 525  Training Loss  9.319390301243402e-06\n",
            "Epoch  49 Batch  362 / 525  Training Loss  7.267588898685062e-06\n",
            "Epoch  49 Batch  363 / 525  Training Loss  1.8596192603581585e-05\n",
            "Epoch  49 Batch  364 / 525  Training Loss  1.0952249795082025e-05\n",
            "Epoch  49 Batch  365 / 525  Training Loss  1.0898992513830308e-05\n",
            "Epoch  49 Batch  366 / 525  Training Loss  7.734981409157626e-06\n",
            "Epoch  49 Batch  367 / 525  Training Loss  5.513023552339291e-06\n",
            "Epoch  49 Batch  368 / 525  Training Loss  1.0293842933606356e-05\n",
            "Epoch  49 Batch  369 / 525  Training Loss  7.67836445447756e-06\n",
            "Epoch  49 Batch  370 / 525  Training Loss  1.4452378309215419e-05\n",
            "Epoch  49 Batch  371 / 525  Training Loss  1.3012652743782382e-05\n",
            "Epoch  49 Batch  372 / 525  Training Loss  1.6144473192980513e-05\n",
            "Epoch  49 Batch  373 / 525  Training Loss  1.2628574040718377e-05\n",
            "Epoch  49 Batch  374 / 525  Training Loss  1.3163534276827704e-05\n",
            "Epoch  49 Batch  375 / 525  Training Loss  9.057106581167318e-06\n",
            "Epoch  49 Batch  376 / 525  Training Loss  6.0913139350304846e-06\n",
            "Epoch  49 Batch  377 / 525  Training Loss  9.349877473141532e-06\n",
            "Epoch  49 Batch  378 / 525  Training Loss  6.568369371962035e-06\n",
            "Epoch  49 Batch  379 / 525  Training Loss  1.475888529967051e-05\n",
            "Epoch  49 Batch  380 / 525  Training Loss  1.2453647286747582e-05\n",
            "Epoch  49 Batch  381 / 525  Training Loss  9.054502697836142e-06\n",
            "Epoch  49 Batch  382 / 525  Training Loss  1.06736315501621e-05\n",
            "Epoch  49 Batch  383 / 525  Training Loss  9.885789950203616e-06\n",
            "Epoch  49 Batch  384 / 525  Training Loss  7.359388291661162e-06\n",
            "Epoch  49 Batch  385 / 525  Training Loss  1.1692508451233152e-05\n",
            "Epoch  49 Batch  386 / 525  Training Loss  8.987886758404784e-06\n",
            "Epoch  49 Batch  387 / 525  Training Loss  1.2428916306816973e-05\n",
            "Epoch  49 Batch  388 / 525  Training Loss  6.940163530089194e-06\n",
            "Epoch  49 Batch  389 / 525  Training Loss  9.580907317285892e-06\n",
            "Epoch  49 Batch  390 / 525  Training Loss  8.940282896219287e-06\n",
            "Epoch  49 Batch  391 / 525  Training Loss  1.462250656913966e-05\n",
            "Epoch  49 Batch  392 / 525  Training Loss  1.9082250219071284e-05\n",
            "Epoch  49 Batch  393 / 525  Training Loss  9.699271686258726e-06\n",
            "Epoch  49 Batch  394 / 525  Training Loss  1.3269642295199446e-05\n",
            "Epoch  49 Batch  395 / 525  Training Loss  1.0025153642345686e-05\n",
            "Epoch  49 Batch  396 / 525  Training Loss  1.002855151455151e-05\n",
            "Epoch  49 Batch  397 / 525  Training Loss  1.029667146212887e-05\n",
            "Epoch  49 Batch  398 / 525  Training Loss  1.1928799722227268e-05\n",
            "Epoch  49 Batch  399 / 525  Training Loss  1.77050333149964e-05\n",
            "Epoch  49 Batch  400 / 525  Training Loss  8.318038453580812e-06\n",
            "Epoch  49 Batch  401 / 525  Training Loss  8.08648928796174e-06\n",
            "Epoch  49 Batch  402 / 525  Training Loss  7.699856723775156e-06\n",
            "Epoch  49 Batch  403 / 525  Training Loss  1.1823563909274526e-05\n",
            "Epoch  49 Batch  404 / 525  Training Loss  1.1107757018180564e-05\n",
            "Epoch  49 Batch  405 / 525  Training Loss  1.2054835679009557e-05\n",
            "Epoch  49 Batch  406 / 525  Training Loss  8.693667950865347e-06\n",
            "Epoch  49 Batch  407 / 525  Training Loss  1.1271074981777929e-05\n",
            "Epoch  49 Batch  408 / 525  Training Loss  1.06939460238209e-05\n",
            "Epoch  49 Batch  409 / 525  Training Loss  1.2409576811478473e-05\n",
            "Epoch  49 Batch  410 / 525  Training Loss  1.4348261174745858e-05\n",
            "Epoch  49 Batch  411 / 525  Training Loss  1.3424958524410613e-05\n",
            "Epoch  49 Batch  412 / 525  Training Loss  1.3660226613865234e-05\n",
            "Epoch  49 Batch  413 / 525  Training Loss  1.606044315849431e-05\n",
            "Epoch  49 Batch  414 / 525  Training Loss  1.1011495189450216e-05\n",
            "Epoch  49 Batch  415 / 525  Training Loss  8.850898666423745e-06\n",
            "Epoch  49 Batch  416 / 525  Training Loss  1.282146513403859e-05\n",
            "Epoch  49 Batch  417 / 525  Training Loss  1.2652312761929352e-05\n",
            "Epoch  49 Batch  418 / 525  Training Loss  5.308692379912827e-06\n",
            "Epoch  49 Batch  419 / 525  Training Loss  1.2760167010128498e-05\n",
            "Epoch  49 Batch  420 / 525  Training Loss  8.038145097089e-06\n",
            "Epoch  49 Batch  421 / 525  Training Loss  1.1985785022261553e-05\n",
            "Epoch  49 Batch  422 / 525  Training Loss  9.128621059062425e-06\n",
            "Epoch  49 Batch  423 / 525  Training Loss  1.2792492270818911e-05\n",
            "Epoch  49 Batch  424 / 525  Training Loss  1.1017359611287247e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  49 Batch  425 / 525  Training Loss  6.3448505898122676e-06\n",
            "Epoch  49 Batch  426 / 525  Training Loss  9.692751518741716e-06\n",
            "Epoch  49 Batch  427 / 525  Training Loss  1.382676146022277e-05\n",
            "Epoch  49 Batch  428 / 525  Training Loss  1.2740095371555071e-05\n",
            "Epoch  49 Batch  429 / 525  Training Loss  8.93459036888089e-06\n",
            "Epoch  49 Batch  430 / 525  Training Loss  1.2917375897814054e-05\n",
            "Epoch  49 Batch  431 / 525  Training Loss  1.1057403753511608e-05\n",
            "Epoch  49 Batch  432 / 525  Training Loss  1.425481059413869e-05\n",
            "Epoch  49 Batch  433 / 525  Training Loss  9.37077584239887e-06\n",
            "Epoch  49 Batch  434 / 525  Training Loss  1.4200634723238181e-05\n",
            "Epoch  49 Batch  435 / 525  Training Loss  1.7500362446298823e-05\n",
            "Epoch  49 Batch  436 / 525  Training Loss  1.2229225831106305e-05\n",
            "Epoch  49 Batch  437 / 525  Training Loss  7.188044946815353e-06\n",
            "Epoch  49 Batch  438 / 525  Training Loss  1.5209867342491634e-05\n",
            "Epoch  49 Batch  439 / 525  Training Loss  1.6609134036116302e-05\n",
            "Epoch  49 Batch  440 / 525  Training Loss  6.103673513280228e-06\n",
            "Epoch  49 Batch  441 / 525  Training Loss  9.231207513948902e-06\n",
            "Epoch  49 Batch  442 / 525  Training Loss  1.1500978871481493e-05\n",
            "Epoch  49 Batch  443 / 525  Training Loss  9.852085895545315e-06\n",
            "Epoch  49 Batch  444 / 525  Training Loss  1.2647056792047806e-05\n",
            "Epoch  49 Batch  445 / 525  Training Loss  1.6873873391887173e-05\n",
            "Epoch  49 Batch  446 / 525  Training Loss  1.02846825029701e-05\n",
            "Epoch  49 Batch  447 / 525  Training Loss  1.155032714450499e-05\n",
            "Epoch  49 Batch  448 / 525  Training Loss  1.191202591144247e-05\n",
            "Epoch  49 Batch  449 / 525  Training Loss  9.926217899192125e-06\n",
            "Epoch  49 Batch  450 / 525  Training Loss  9.737528671394102e-06\n",
            "Epoch  49 Batch  451 / 525  Training Loss  6.8414860834309366e-06\n",
            "Epoch  49 Batch  452 / 525  Training Loss  7.44096814742079e-06\n",
            "Epoch  49 Batch  453 / 525  Training Loss  6.278908585954923e-06\n",
            "Epoch  49 Batch  454 / 525  Training Loss  7.215159712359309e-06\n",
            "Epoch  49 Batch  455 / 525  Training Loss  1.5726403944427148e-05\n",
            "Epoch  49 Batch  456 / 525  Training Loss  7.09521782482625e-06\n",
            "Epoch  49 Batch  457 / 525  Training Loss  1.0154527444683481e-05\n",
            "Epoch  49 Batch  458 / 525  Training Loss  6.9642214839404915e-06\n",
            "Epoch  49 Batch  459 / 525  Training Loss  1.239617085957434e-05\n",
            "Epoch  49 Batch  460 / 525  Training Loss  1.2719596270471811e-05\n",
            "Epoch  49 Batch  461 / 525  Training Loss  1.5206856005534064e-05\n",
            "Epoch  49 Batch  462 / 525  Training Loss  1.3741773727815598e-05\n",
            "Epoch  49 Batch  463 / 525  Training Loss  1.2684266948781442e-05\n",
            "Epoch  49 Batch  464 / 525  Training Loss  8.879095730662812e-06\n",
            "Epoch  49 Batch  465 / 525  Training Loss  1.2356207662378438e-05\n",
            "Epoch  49 Batch  466 / 525  Training Loss  1.2654365491471253e-05\n",
            "Epoch  49 Batch  467 / 525  Training Loss  1.0085749636346009e-05\n",
            "Epoch  49 Batch  468 / 525  Training Loss  9.612143912818283e-06\n",
            "Epoch  49 Batch  469 / 525  Training Loss  1.7142327124020085e-05\n",
            "Epoch  49 Batch  470 / 525  Training Loss  1.1107885256933514e-05\n",
            "Epoch  49 Batch  471 / 525  Training Loss  7.412934792228043e-06\n",
            "Epoch  49 Batch  472 / 525  Training Loss  1.0669803486962337e-05\n",
            "Epoch  49 Batch  473 / 525  Training Loss  1.245880139322253e-05\n",
            "Epoch  49 Batch  474 / 525  Training Loss  8.612066267232876e-06\n",
            "Epoch  49 Batch  475 / 525  Training Loss  1.3833314369549043e-05\n",
            "Epoch  49 Batch  476 / 525  Training Loss  9.978259186027572e-06\n",
            "Epoch  49 Batch  477 / 525  Training Loss  6.7852915890398435e-06\n",
            "Epoch  49 Batch  478 / 525  Training Loss  1.2949954907526262e-05\n",
            "Epoch  49 Batch  479 / 525  Training Loss  6.451357421610737e-06\n",
            "Epoch  49 Batch  480 / 525  Training Loss  9.803577086131554e-06\n",
            "Epoch  49 Batch  481 / 525  Training Loss  8.579612767789513e-06\n",
            "Epoch  49 Batch  482 / 525  Training Loss  9.815094017540105e-06\n",
            "Epoch  49 Batch  483 / 525  Training Loss  8.707249435246922e-06\n",
            "Epoch  49 Batch  484 / 525  Training Loss  1.32465338538168e-05\n",
            "Epoch  49 Batch  485 / 525  Training Loss  1.2270150364201982e-05\n",
            "Epoch  49 Batch  486 / 525  Training Loss  1.121408240578603e-05\n",
            "Epoch  49 Batch  487 / 525  Training Loss  1.6003643395379186e-05\n",
            "Epoch  49 Batch  488 / 525  Training Loss  1.216452983499039e-05\n",
            "Epoch  49 Batch  489 / 525  Training Loss  1.3903913895774167e-05\n",
            "Epoch  49 Batch  490 / 525  Training Loss  1.6486048480146565e-05\n",
            "Epoch  49 Batch  491 / 525  Training Loss  1.1327292668283917e-05\n",
            "Epoch  49 Batch  492 / 525  Training Loss  1.6266883903881535e-05\n",
            "Epoch  49 Batch  493 / 525  Training Loss  1.0512612789170817e-05\n",
            "Epoch  49 Batch  494 / 525  Training Loss  9.892926755128428e-06\n",
            "Epoch  49 Batch  495 / 525  Training Loss  7.62059153203154e-06\n",
            "Epoch  49 Batch  496 / 525  Training Loss  1.0364999980083667e-05\n",
            "Epoch  49 Batch  497 / 525  Training Loss  1.604544922884088e-05\n",
            "Epoch  49 Batch  498 / 525  Training Loss  1.0714657037169673e-05\n",
            "Epoch  49 Batch  499 / 525  Training Loss  1.2077476640115492e-05\n",
            "Epoch  49 Batch  500 / 525  Training Loss  1.186737517855363e-05\n",
            "Epoch  49 Batch  501 / 525  Training Loss  1.3348292668524664e-05\n",
            "Epoch  49 Batch  502 / 525  Training Loss  1.1993064617854543e-05\n",
            "Epoch  49 Batch  503 / 525  Training Loss  1.6131840311572887e-05\n",
            "Epoch  49 Batch  504 / 525  Training Loss  8.099746082734782e-06\n",
            "Epoch  49 Batch  505 / 525  Training Loss  1.0263971489621326e-05\n",
            "Epoch  49 Batch  506 / 525  Training Loss  6.3383063206856605e-06\n",
            "Epoch  49 Batch  507 / 525  Training Loss  8.155819159583189e-06\n",
            "Epoch  49 Batch  508 / 525  Training Loss  1.2507773135439493e-05\n",
            "Epoch  49 Batch  509 / 525  Training Loss  1.2627933756448328e-05\n",
            "Epoch  49 Batch  510 / 525  Training Loss  9.09673599380767e-06\n",
            "Epoch  49 Batch  511 / 525  Training Loss  1.3122409654897638e-05\n",
            "Epoch  49 Batch  512 / 525  Training Loss  9.416530701855663e-06\n",
            "Epoch  49 Batch  513 / 525  Training Loss  1.3160354683350306e-05\n",
            "Epoch  49 Batch  514 / 525  Training Loss  8.834862455842085e-06\n",
            "Epoch  49 Batch  515 / 525  Training Loss  6.1872137848695274e-06\n",
            "Epoch  49 Batch  516 / 525  Training Loss  6.5066901697719e-06\n",
            "Epoch  49 Batch  517 / 525  Training Loss  8.36174967844272e-06\n",
            "Epoch  49 Batch  518 / 525  Training Loss  1.2454312127374578e-05\n",
            "Epoch  49 Batch  519 / 525  Training Loss  1.8044975149678066e-05\n",
            "Epoch  49 Batch  520 / 525  Training Loss  1.3977814887766726e-05\n",
            "Epoch  49 Batch  521 / 525  Training Loss  9.918801879393868e-06\n",
            "Epoch  49 Batch  522 / 525  Training Loss  8.631161108496599e-06\n",
            "Epoch  49 Batch  523 / 525  Training Loss  1.1123023796244524e-05\n",
            "Epoch  49 Batch  524 / 525  Training Loss  1.0831009603862185e-05\n",
            "  50    |    -    |   0.000011   | 64.575000\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 50\n",
            "Epoch  50 Batch  0 / 525  Training Loss  7.958435162436217e-06\n",
            "Epoch  50 Batch  1 / 525  Training Loss  7.048970473988447e-06\n",
            "Epoch  50 Batch  2 / 525  Training Loss  8.251141480286606e-06\n",
            "Epoch  50 Batch  3 / 525  Training Loss  1.3921945537731517e-05\n",
            "Epoch  50 Batch  4 / 525  Training Loss  8.83895336301066e-06\n",
            "Epoch  50 Batch  5 / 525  Training Loss  1.1874658412125427e-05\n",
            "Epoch  50 Batch  6 / 525  Training Loss  9.160712579614483e-06\n",
            "Epoch  50 Batch  7 / 525  Training Loss  6.482541266450426e-06\n",
            "Epoch  50 Batch  8 / 525  Training Loss  1.348111436527688e-05\n",
            "Epoch  50 Batch  9 / 525  Training Loss  1.2544970559247304e-05\n",
            "Epoch  50 Batch  10 / 525  Training Loss  1.1865578017022926e-05\n",
            "Epoch  50 Batch  11 / 525  Training Loss  1.6034664440667257e-05\n",
            "Epoch  50 Batch  12 / 525  Training Loss  1.3005822438572068e-05\n",
            "Epoch  50 Batch  13 / 525  Training Loss  8.912338671507314e-06\n",
            "Epoch  50 Batch  14 / 525  Training Loss  7.913222361821681e-06\n",
            "Epoch  50 Batch  15 / 525  Training Loss  8.46627608552808e-06\n",
            "Epoch  50 Batch  16 / 525  Training Loss  1.1973006621701643e-05\n",
            "Epoch  50 Batch  17 / 525  Training Loss  8.392574272875208e-06\n",
            "Epoch  50 Batch  18 / 525  Training Loss  9.964884156943299e-06\n",
            "Epoch  50 Batch  19 / 525  Training Loss  7.465868293365929e-06\n",
            "Epoch  50 Batch  20 / 525  Training Loss  1.3157143257558346e-05\n",
            "Epoch  50 Batch  21 / 525  Training Loss  1.15068305603927e-05\n",
            "Epoch  50 Batch  22 / 525  Training Loss  1.0604457202134654e-05\n",
            "Epoch  50 Batch  23 / 525  Training Loss  7.905063284852076e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  50 Batch  24 / 525  Training Loss  1.2877453627879731e-05\n",
            "Epoch  50 Batch  25 / 525  Training Loss  9.879977369564585e-06\n",
            "Epoch  50 Batch  26 / 525  Training Loss  8.960407285485417e-06\n",
            "Epoch  50 Batch  27 / 525  Training Loss  1.0975543773383833e-05\n",
            "Epoch  50 Batch  28 / 525  Training Loss  6.143378413980827e-06\n",
            "Epoch  50 Batch  29 / 525  Training Loss  9.351733751827851e-06\n",
            "Epoch  50 Batch  30 / 525  Training Loss  1.0926810318778735e-05\n",
            "Epoch  50 Batch  31 / 525  Training Loss  1.2635168786800932e-05\n",
            "Epoch  50 Batch  32 / 525  Training Loss  8.940814950619824e-06\n",
            "Epoch  50 Batch  33 / 525  Training Loss  9.38497487368295e-06\n",
            "Epoch  50 Batch  34 / 525  Training Loss  1.0954199751722626e-05\n",
            "Epoch  50 Batch  35 / 525  Training Loss  1.5693771274527535e-05\n",
            "Epoch  50 Batch  36 / 525  Training Loss  1.1358534720784519e-05\n",
            "Epoch  50 Batch  37 / 525  Training Loss  1.073683779395651e-05\n",
            "Epoch  50 Batch  38 / 525  Training Loss  8.710975635040086e-06\n",
            "Epoch  50 Batch  39 / 525  Training Loss  1.1562749932636507e-05\n",
            "Epoch  50 Batch  40 / 525  Training Loss  6.523313004436204e-06\n",
            "Epoch  50 Batch  41 / 525  Training Loss  1.3532427146856207e-05\n",
            "Epoch  50 Batch  42 / 525  Training Loss  1.0287395525665488e-05\n",
            "Epoch  50 Batch  43 / 525  Training Loss  1.3006702829443384e-05\n",
            "Epoch  50 Batch  44 / 525  Training Loss  8.03779585112352e-06\n",
            "Epoch  50 Batch  45 / 525  Training Loss  1.470670395065099e-05\n",
            "Epoch  50 Batch  46 / 525  Training Loss  1.3977597518533003e-05\n",
            "Epoch  50 Batch  47 / 525  Training Loss  1.1803786037489772e-05\n",
            "Epoch  50 Batch  48 / 525  Training Loss  9.97263578028651e-06\n",
            "Epoch  50 Batch  49 / 525  Training Loss  1.4806088984187227e-05\n",
            "Epoch  50 Batch  50 / 525  Training Loss  6.925784873601515e-06\n",
            "Epoch  50 Batch  51 / 525  Training Loss  8.95804441825021e-06\n",
            "Epoch  50 Batch  52 / 525  Training Loss  8.128535228024703e-06\n",
            "Epoch  50 Batch  53 / 525  Training Loss  1.2828213584725745e-05\n",
            "Epoch  50 Batch  54 / 525  Training Loss  8.189364962163381e-06\n",
            "Epoch  50 Batch  55 / 525  Training Loss  8.892361620382871e-06\n",
            "Epoch  50 Batch  56 / 525  Training Loss  1.0074468264065217e-05\n",
            "Epoch  50 Batch  57 / 525  Training Loss  6.196464710228611e-06\n",
            "Epoch  50 Batch  58 / 525  Training Loss  1.1666484169836622e-05\n",
            "Epoch  50 Batch  59 / 525  Training Loss  1.6564727047807537e-05\n",
            "Epoch  50 Batch  60 / 525  Training Loss  1.234595947607886e-05\n",
            "Epoch  50 Batch  61 / 525  Training Loss  7.416256266878918e-06\n",
            "Epoch  50 Batch  62 / 525  Training Loss  1.0161237696593162e-05\n",
            "Epoch  50 Batch  63 / 525  Training Loss  8.54962672747206e-06\n",
            "Epoch  50 Batch  64 / 525  Training Loss  1.0229680810880382e-05\n",
            "Epoch  50 Batch  65 / 525  Training Loss  1.0126685083378106e-05\n",
            "Epoch  50 Batch  66 / 525  Training Loss  1.0708683475968428e-05\n",
            "Epoch  50 Batch  67 / 525  Training Loss  1.3338141798158176e-05\n",
            "Epoch  50 Batch  68 / 525  Training Loss  5.333036824595183e-06\n",
            "Epoch  50 Batch  69 / 525  Training Loss  3.7907670957793016e-06\n",
            "Epoch  50 Batch  70 / 525  Training Loss  9.14126849238528e-06\n",
            "Epoch  50 Batch  71 / 525  Training Loss  8.786111720837653e-06\n",
            "Epoch  50 Batch  72 / 525  Training Loss  1.2889320714748465e-05\n",
            "Epoch  50 Batch  73 / 525  Training Loss  6.814744210714707e-06\n",
            "Epoch  50 Batch  74 / 525  Training Loss  1.2927257557748817e-05\n",
            "Epoch  50 Batch  75 / 525  Training Loss  8.37005973153282e-06\n",
            "Epoch  50 Batch  76 / 525  Training Loss  1.0955245670629665e-05\n",
            "Epoch  50 Batch  77 / 525  Training Loss  8.615968908998184e-06\n",
            "Epoch  50 Batch  78 / 525  Training Loss  6.113441486377269e-06\n",
            "Epoch  50 Batch  79 / 525  Training Loss  9.724275514599867e-06\n",
            "Epoch  50 Batch  80 / 525  Training Loss  1.282101402466651e-05\n",
            "Epoch  50 Batch  81 / 525  Training Loss  1.1269835340499412e-05\n",
            "Epoch  50 Batch  82 / 525  Training Loss  5.759659870818723e-06\n",
            "Epoch  50 Batch  83 / 525  Training Loss  8.804915523796808e-06\n",
            "Epoch  50 Batch  84 / 525  Training Loss  6.746709004801232e-06\n",
            "Epoch  50 Batch  85 / 525  Training Loss  1.023216282192152e-05\n",
            "Epoch  50 Batch  86 / 525  Training Loss  9.795914593269117e-06\n",
            "Epoch  50 Batch  87 / 525  Training Loss  1.0960380677715875e-05\n",
            "Epoch  50 Batch  88 / 525  Training Loss  1.0732590453699231e-05\n",
            "Epoch  50 Batch  89 / 525  Training Loss  1.4212198948371224e-05\n",
            "Epoch  50 Batch  90 / 525  Training Loss  8.916241313272621e-06\n",
            "Epoch  50 Batch  91 / 525  Training Loss  1.0031862075265963e-05\n",
            "Epoch  50 Batch  92 / 525  Training Loss  1.0878346984100062e-05\n",
            "Epoch  50 Batch  93 / 525  Training Loss  8.873854312696494e-06\n",
            "Epoch  50 Batch  94 / 525  Training Loss  9.578745448379777e-06\n",
            "Epoch  50 Batch  95 / 525  Training Loss  1.2275963854335714e-05\n",
            "Epoch  50 Batch  96 / 525  Training Loss  8.829030775814317e-06\n",
            "Epoch  50 Batch  97 / 525  Training Loss  1.2737587894662283e-05\n",
            "Epoch  50 Batch  98 / 525  Training Loss  9.015951036417391e-06\n",
            "Epoch  50 Batch  99 / 525  Training Loss  1.3215316357673146e-05\n",
            "Epoch  50 Batch  100 / 525  Training Loss  1.1176693078596145e-05\n",
            "Epoch  50 Batch  101 / 525  Training Loss  7.203757377283182e-06\n",
            "Epoch  50 Batch  102 / 525  Training Loss  7.119666406651959e-06\n",
            "Epoch  50 Batch  103 / 525  Training Loss  6.71869338475517e-06\n",
            "Epoch  50 Batch  104 / 525  Training Loss  8.390406947000884e-06\n",
            "Epoch  50 Batch  105 / 525  Training Loss  1.1135250133520458e-05\n",
            "Epoch  50 Batch  106 / 525  Training Loss  1.546847852296196e-05\n",
            "Epoch  50 Batch  107 / 525  Training Loss  8.813158274278976e-06\n",
            "Epoch  50 Batch  108 / 525  Training Loss  1.195022286992753e-05\n",
            "Epoch  50 Batch  109 / 525  Training Loss  1.3121968549967278e-05\n",
            "Epoch  50 Batch  110 / 525  Training Loss  8.19011347630294e-06\n",
            "Epoch  50 Batch  111 / 525  Training Loss  8.480903488816693e-06\n",
            "Epoch  50 Batch  112 / 525  Training Loss  8.524619261152111e-06\n",
            "Epoch  50 Batch  113 / 525  Training Loss  1.2884223906439729e-05\n",
            "Epoch  50 Batch  114 / 525  Training Loss  1.0862602721317671e-05\n",
            "Epoch  50 Batch  115 / 525  Training Loss  1.0477358955540694e-05\n",
            "Epoch  50 Batch  116 / 525  Training Loss  9.68734639172908e-06\n",
            "Epoch  50 Batch  117 / 525  Training Loss  8.885306669981219e-06\n",
            "Epoch  50 Batch  118 / 525  Training Loss  1.3242446584627032e-05\n",
            "Epoch  50 Batch  119 / 525  Training Loss  1.141896518674912e-05\n",
            "Epoch  50 Batch  120 / 525  Training Loss  7.524824468418956e-06\n",
            "Epoch  50 Batch  121 / 525  Training Loss  8.96219899004791e-06\n",
            "Epoch  50 Batch  122 / 525  Training Loss  1.1544241715455428e-05\n",
            "Epoch  50 Batch  123 / 525  Training Loss  9.561414117342792e-06\n",
            "Epoch  50 Batch  124 / 525  Training Loss  1.2643664376810193e-05\n",
            "Epoch  50 Batch  125 / 525  Training Loss  8.190881999325939e-06\n",
            "Epoch  50 Batch  126 / 525  Training Loss  6.434867827920243e-06\n",
            "Epoch  50 Batch  127 / 525  Training Loss  1.0096610822074581e-05\n",
            "Epoch  50 Batch  128 / 525  Training Loss  1.1101103154942393e-05\n",
            "Epoch  50 Batch  129 / 525  Training Loss  1.3312084774952382e-05\n",
            "Epoch  50 Batch  130 / 525  Training Loss  9.308979315392207e-06\n",
            "Epoch  50 Batch  131 / 525  Training Loss  1.0648619536368642e-05\n",
            "Epoch  50 Batch  132 / 525  Training Loss  1.4608228411816526e-05\n",
            "Epoch  50 Batch  133 / 525  Training Loss  8.91073614184279e-06\n",
            "Epoch  50 Batch  134 / 525  Training Loss  1.4739080143044703e-05\n",
            "Epoch  50 Batch  135 / 525  Training Loss  9.985324140870944e-06\n",
            "Epoch  50 Batch  136 / 525  Training Loss  1.5235511455102824e-05\n",
            "Epoch  50 Batch  137 / 525  Training Loss  9.1295314632589e-06\n",
            "Epoch  50 Batch  138 / 525  Training Loss  1.061871626006905e-05\n",
            "Epoch  50 Batch  139 / 525  Training Loss  1.307332240685355e-05\n",
            "Epoch  50 Batch  140 / 525  Training Loss  7.850117981433868e-06\n",
            "Epoch  50 Batch  141 / 525  Training Loss  8.192111636162736e-06\n",
            "Epoch  50 Batch  142 / 525  Training Loss  9.239717655873392e-06\n",
            "Epoch  50 Batch  143 / 525  Training Loss  1.697005791356787e-05\n",
            "Epoch  50 Batch  144 / 525  Training Loss  1.3295335520524532e-05\n",
            "Epoch  50 Batch  145 / 525  Training Loss  8.486751539749093e-06\n",
            "Epoch  50 Batch  146 / 525  Training Loss  5.457623956317548e-06\n",
            "Epoch  50 Batch  147 / 525  Training Loss  1.0187181032961234e-05\n",
            "Epoch  50 Batch  148 / 525  Training Loss  1.637760942685418e-05\n",
            "Epoch  50 Batch  149 / 525  Training Loss  1.4040577298146673e-05\n",
            "Epoch  50 Batch  150 / 525  Training Loss  8.852243809087668e-06\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  50 Batch  151 / 525  Training Loss  9.021508958539926e-06\n",
            "Epoch  50 Batch  152 / 525  Training Loss  9.344434147351421e-06\n",
            "Epoch  50 Batch  153 / 525  Training Loss  1.0734794159361627e-05\n",
            "Epoch  50 Batch  154 / 525  Training Loss  1.3885592125006951e-05\n",
            "Epoch  50 Batch  155 / 525  Training Loss  1.498148230894003e-05\n",
            "Epoch  50 Batch  156 / 525  Training Loss  9.308396329288371e-06\n",
            "Epoch  50 Batch  157 / 525  Training Loss  1.2726920431305189e-05\n",
            "Epoch  50 Batch  158 / 525  Training Loss  7.251590432133526e-06\n",
            "Epoch  50 Batch  159 / 525  Training Loss  7.00442888046382e-06\n",
            "Epoch  50 Batch  160 / 525  Training Loss  1.0926873073913157e-05\n",
            "Epoch  50 Batch  161 / 525  Training Loss  5.734921614930499e-06\n",
            "Epoch  50 Batch  162 / 525  Training Loss  8.341078682860825e-06\n",
            "Epoch  50 Batch  163 / 525  Training Loss  1.187491670862073e-05\n",
            "Epoch  50 Batch  164 / 525  Training Loss  1.1735537555068731e-05\n",
            "Epoch  50 Batch  165 / 525  Training Loss  9.205503374687396e-06\n",
            "Epoch  50 Batch  166 / 525  Training Loss  9.996716471505351e-06\n",
            "Epoch  50 Batch  167 / 525  Training Loss  1.2457489901862573e-05\n",
            "Epoch  50 Batch  168 / 525  Training Loss  1.4251823813538067e-05\n",
            "Epoch  50 Batch  169 / 525  Training Loss  8.601970876043197e-06\n",
            "Epoch  50 Batch  170 / 525  Training Loss  1.0683364052965771e-05\n",
            "Epoch  50 Batch  171 / 525  Training Loss  6.797161859140033e-06\n",
            "Epoch  50 Batch  172 / 525  Training Loss  9.18299519980792e-06\n",
            "Epoch  50 Batch  173 / 525  Training Loss  1.4331824786495417e-05\n",
            "Epoch  50 Batch  174 / 525  Training Loss  1.319792318099644e-05\n",
            "Epoch  50 Batch  175 / 525  Training Loss  1.109574077418074e-05\n",
            "Epoch  50 Batch  176 / 525  Training Loss  1.0132715033250861e-05\n",
            "Epoch  50 Batch  177 / 525  Training Loss  7.22719505574787e-06\n",
            "Epoch  50 Batch  178 / 525  Training Loss  8.464574420941062e-06\n",
            "Epoch  50 Batch  179 / 525  Training Loss  5.870152108400362e-06\n",
            "Epoch  50 Batch  180 / 525  Training Loss  1.1085558980994392e-05\n",
            "Epoch  50 Batch  181 / 525  Training Loss  1.1062925295846071e-05\n",
            "Epoch  50 Batch  182 / 525  Training Loss  9.420993592357263e-06\n",
            "Epoch  50 Batch  183 / 525  Training Loss  1.2550229257612955e-05\n",
            "Epoch  50 Batch  184 / 525  Training Loss  8.875009370967746e-06\n",
            "Epoch  50 Batch  185 / 525  Training Loss  9.394420885655563e-06\n",
            "Epoch  50 Batch  186 / 525  Training Loss  8.269983482023235e-06\n",
            "Epoch  50 Batch  187 / 525  Training Loss  6.99255815561628e-06\n",
            "Epoch  50 Batch  188 / 525  Training Loss  9.733435945236124e-06\n",
            "Epoch  50 Batch  189 / 525  Training Loss  1.0536927220528014e-05\n",
            "Epoch  50 Batch  190 / 525  Training Loss  7.680005182919558e-06\n",
            "Epoch  50 Batch  191 / 525  Training Loss  9.986410987039562e-06\n",
            "Epoch  50 Batch  192 / 525  Training Loss  1.279320986213861e-05\n",
            "Epoch  50 Batch  193 / 525  Training Loss  1.5964964404702187e-05\n",
            "Epoch  50 Batch  194 / 525  Training Loss  1.2580270777107216e-05\n",
            "Epoch  50 Batch  195 / 525  Training Loss  1.044739292410668e-05\n",
            "Epoch  50 Batch  196 / 525  Training Loss  1.0419005775474943e-05\n",
            "Epoch  50 Batch  197 / 525  Training Loss  1.0542586096562445e-05\n",
            "Epoch  50 Batch  198 / 525  Training Loss  7.662247298867442e-06\n",
            "Epoch  50 Batch  199 / 525  Training Loss  6.618007319048047e-06\n",
            "Epoch  50 Batch  200 / 525  Training Loss  7.468056082871044e-06\n",
            "Epoch  50 Batch  201 / 525  Training Loss  1.4625516996602528e-05\n",
            "Epoch  50 Batch  202 / 525  Training Loss  1.1235430065426044e-05\n",
            "Epoch  50 Batch  203 / 525  Training Loss  1.1454290870460682e-05\n",
            "Epoch  50 Batch  204 / 525  Training Loss  1.2367558156256564e-05\n",
            "Epoch  50 Batch  205 / 525  Training Loss  5.421394234872423e-06\n",
            "Epoch  50 Batch  206 / 525  Training Loss  1.1454011655587237e-05\n",
            "Epoch  50 Batch  207 / 525  Training Loss  8.88573322299635e-06\n",
            "Epoch  50 Batch  208 / 525  Training Loss  1.0415868928248528e-05\n",
            "Epoch  50 Batch  209 / 525  Training Loss  1.0581437891232781e-05\n",
            "Epoch  50 Batch  210 / 525  Training Loss  1.008860272122547e-05\n",
            "Epoch  50 Batch  211 / 525  Training Loss  9.861811122391373e-06\n",
            "Epoch  50 Batch  212 / 525  Training Loss  1.3368624422582798e-05\n",
            "Epoch  50 Batch  213 / 525  Training Loss  6.107739409344504e-06\n",
            "Epoch  50 Batch  214 / 525  Training Loss  1.661141141084954e-05\n",
            "Epoch  50 Batch  215 / 525  Training Loss  6.075632882129867e-06\n",
            "Epoch  50 Batch  216 / 525  Training Loss  1.2319069355726242e-05\n",
            "Epoch  50 Batch  217 / 525  Training Loss  1.3358076103031635e-05\n",
            "Epoch  50 Batch  218 / 525  Training Loss  1.4487882253888529e-05\n",
            "Epoch  50 Batch  219 / 525  Training Loss  1.2600762602232862e-05\n",
            "Epoch  50 Batch  220 / 525  Training Loss  7.78663616074482e-06\n",
            "Epoch  50 Batch  221 / 525  Training Loss  1.3435414984996896e-05\n",
            "Epoch  50 Batch  222 / 525  Training Loss  1.6371846868423745e-05\n",
            "Epoch  50 Batch  223 / 525  Training Loss  1.022180549625773e-05\n",
            "Epoch  50 Batch  224 / 525  Training Loss  9.686939847597387e-06\n",
            "Epoch  50 Batch  225 / 525  Training Loss  1.2688362403423525e-05\n",
            "Epoch  50 Batch  226 / 525  Training Loss  6.251750164665282e-06\n",
            "Epoch  50 Batch  227 / 525  Training Loss  1.4981624190113507e-05\n",
            "Epoch  50 Batch  228 / 525  Training Loss  1.2999761565879453e-05\n",
            "Epoch  50 Batch  229 / 525  Training Loss  7.646100129932165e-06\n",
            "Epoch  50 Batch  230 / 525  Training Loss  1.0456397831148934e-05\n",
            "Epoch  50 Batch  231 / 525  Training Loss  1.1241842003073543e-05\n",
            "Epoch  50 Batch  232 / 525  Training Loss  8.619394975539763e-06\n",
            "Epoch  50 Batch  233 / 525  Training Loss  9.266041161026806e-06\n",
            "Epoch  50 Batch  234 / 525  Training Loss  8.62006072566146e-06\n",
            "Epoch  50 Batch  235 / 525  Training Loss  8.406418601225596e-06\n",
            "Epoch  50 Batch  236 / 525  Training Loss  8.360290848941077e-06\n",
            "Epoch  50 Batch  237 / 525  Training Loss  8.58782914292533e-06\n",
            "Epoch  50 Batch  238 / 525  Training Loss  9.786273039935622e-06\n",
            "Epoch  50 Batch  239 / 525  Training Loss  1.1078754141635727e-05\n",
            "Epoch  50 Batch  240 / 525  Training Loss  1.461791543988511e-05\n",
            "Epoch  50 Batch  241 / 525  Training Loss  7.77486275183037e-06\n",
            "Epoch  50 Batch  242 / 525  Training Loss  7.257528977788752e-06\n",
            "Epoch  50 Batch  243 / 525  Training Loss  5.517416866496205e-06\n",
            "Epoch  50 Batch  244 / 525  Training Loss  1.2579512258525938e-05\n",
            "Epoch  50 Batch  245 / 525  Training Loss  1.044464352162322e-05\n",
            "Epoch  50 Batch  246 / 525  Training Loss  7.015433766355272e-06\n",
            "Epoch  50 Batch  247 / 525  Training Loss  1.1104197255917825e-05\n",
            "Epoch  50 Batch  248 / 525  Training Loss  9.280151061830111e-06\n",
            "Epoch  50 Batch  249 / 525  Training Loss  9.983705240301788e-06\n",
            "Epoch  50 Batch  250 / 525  Training Loss  9.002929800772108e-06\n",
            "Epoch  50 Batch  251 / 525  Training Loss  1.4623868992202915e-05\n",
            "Epoch  50 Batch  252 / 525  Training Loss  6.282895810727496e-06\n",
            "Epoch  50 Batch  253 / 525  Training Loss  1.480150422139559e-05\n",
            "Epoch  50 Batch  254 / 525  Training Loss  7.431486210407456e-06\n",
            "Epoch  50 Batch  255 / 525  Training Loss  1.2570919352583587e-05\n",
            "Epoch  50 Batch  256 / 525  Training Loss  1.3888983630749863e-05\n",
            "Epoch  50 Batch  257 / 525  Training Loss  1.3841976397088729e-05\n",
            "Epoch  50 Batch  258 / 525  Training Loss  8.502263881382532e-06\n",
            "Epoch  50 Batch  259 / 525  Training Loss  7.63111802370986e-06\n",
            "Epoch  50 Batch  260 / 525  Training Loss  1.3373265574045945e-05\n",
            "Epoch  50 Batch  261 / 525  Training Loss  1.3873594070901163e-05\n",
            "Epoch  50 Batch  262 / 525  Training Loss  1.0936732905975077e-05\n",
            "Epoch  50 Batch  263 / 525  Training Loss  9.486690032645129e-06\n",
            "Epoch  50 Batch  264 / 525  Training Loss  1.30571888803388e-05\n",
            "Epoch  50 Batch  265 / 525  Training Loss  9.327315638074651e-06\n",
            "Epoch  50 Batch  266 / 525  Training Loss  8.529927072231658e-06\n",
            "Epoch  50 Batch  267 / 525  Training Loss  9.583174687577412e-06\n",
            "Epoch  50 Batch  268 / 525  Training Loss  1.1765727322199382e-05\n",
            "Epoch  50 Batch  269 / 525  Training Loss  1.1706588338711299e-05\n",
            "Epoch  50 Batch  270 / 525  Training Loss  1.3102139746479224e-05\n",
            "Epoch  50 Batch  271 / 525  Training Loss  9.82968595053535e-06\n",
            "Epoch  50 Batch  272 / 525  Training Loss  9.043085810844786e-06\n",
            "Epoch  50 Batch  273 / 525  Training Loss  1.1322883437969722e-05\n",
            "Epoch  50 Batch  274 / 525  Training Loss  7.32710486772703e-06\n",
            "Epoch  50 Batch  275 / 525  Training Loss  1.358783174509881e-05\n",
            "Epoch  50 Batch  276 / 525  Training Loss  1.013924520520959e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  50 Batch  277 / 525  Training Loss  1.235928630194394e-05\n",
            "Epoch  50 Batch  278 / 525  Training Loss  1.548930049466435e-05\n",
            "Epoch  50 Batch  279 / 525  Training Loss  1.0970880794047844e-05\n",
            "Epoch  50 Batch  280 / 525  Training Loss  1.4233623005566187e-05\n",
            "Epoch  50 Batch  281 / 525  Training Loss  1.1766082025133073e-05\n",
            "Epoch  50 Batch  282 / 525  Training Loss  7.899949196144007e-06\n",
            "Epoch  50 Batch  283 / 525  Training Loss  1.3643742931890301e-05\n",
            "Epoch  50 Batch  284 / 525  Training Loss  8.40039319882635e-06\n",
            "Epoch  50 Batch  285 / 525  Training Loss  7.757937964925077e-06\n",
            "Epoch  50 Batch  286 / 525  Training Loss  1.2013938430754934e-05\n",
            "Epoch  50 Batch  287 / 525  Training Loss  1.3237071470939554e-05\n",
            "Epoch  50 Batch  288 / 525  Training Loss  7.771940545353573e-06\n",
            "Epoch  50 Batch  289 / 525  Training Loss  1.0675002158677671e-05\n",
            "Epoch  50 Batch  290 / 525  Training Loss  9.490551747148857e-06\n",
            "Epoch  50 Batch  291 / 525  Training Loss  1.4285855286289006e-05\n",
            "Epoch  50 Batch  292 / 525  Training Loss  1.230740781466011e-05\n",
            "Epoch  50 Batch  293 / 525  Training Loss  1.5219638953567483e-05\n",
            "Epoch  50 Batch  294 / 525  Training Loss  7.635039764863905e-06\n",
            "Epoch  50 Batch  295 / 525  Training Loss  1.4361893590830732e-05\n",
            "Epoch  50 Batch  296 / 525  Training Loss  8.869767043506727e-06\n",
            "Epoch  50 Batch  297 / 525  Training Loss  1.2262983545952011e-05\n",
            "Epoch  50 Batch  298 / 525  Training Loss  1.0035553714260459e-05\n",
            "Epoch  50 Batch  299 / 525  Training Loss  7.258450750668999e-06\n",
            "Epoch  50 Batch  300 / 525  Training Loss  5.9289864111633506e-06\n",
            "Epoch  50 Batch  301 / 525  Training Loss  7.024726983217988e-06\n",
            "Epoch  50 Batch  302 / 525  Training Loss  6.857182597741485e-06\n",
            "Epoch  50 Batch  303 / 525  Training Loss  1.1850737791974097e-05\n",
            "Epoch  50 Batch  304 / 525  Training Loss  8.5053252405487e-06\n",
            "Epoch  50 Batch  305 / 525  Training Loss  1.0297253538738005e-05\n",
            "Epoch  50 Batch  306 / 525  Training Loss  9.262696039513685e-06\n",
            "Epoch  50 Batch  307 / 525  Training Loss  8.128310582833365e-06\n",
            "Epoch  50 Batch  308 / 525  Training Loss  9.172460522677284e-06\n",
            "Epoch  50 Batch  309 / 525  Training Loss  1.6397660147049464e-05\n",
            "Epoch  50 Batch  310 / 525  Training Loss  8.609540600446053e-06\n",
            "Epoch  50 Batch  311 / 525  Training Loss  1.0964214197883848e-05\n",
            "Epoch  50 Batch  312 / 525  Training Loss  1.1122715477540623e-05\n",
            "Epoch  50 Batch  313 / 525  Training Loss  1.381375204800861e-05\n",
            "Epoch  50 Batch  314 / 525  Training Loss  9.63294405664783e-06\n",
            "Epoch  50 Batch  315 / 525  Training Loss  7.293453563761432e-06\n",
            "Epoch  50 Batch  316 / 525  Training Loss  1.0663842658686917e-05\n",
            "Epoch  50 Batch  317 / 525  Training Loss  7.843617822800297e-06\n",
            "Epoch  50 Batch  318 / 525  Training Loss  1.3732416846323758e-05\n",
            "Epoch  50 Batch  319 / 525  Training Loss  7.341398486460093e-06\n",
            "Epoch  50 Batch  320 / 525  Training Loss  5.633810360450298e-06\n",
            "Epoch  50 Batch  321 / 525  Training Loss  8.006443749764003e-06\n",
            "Epoch  50 Batch  322 / 525  Training Loss  1.0816041140060406e-05\n",
            "Epoch  50 Batch  323 / 525  Training Loss  7.280320460267831e-06\n",
            "Epoch  50 Batch  324 / 525  Training Loss  1.2211596185807139e-05\n",
            "Epoch  50 Batch  325 / 525  Training Loss  8.133136361720972e-06\n",
            "Epoch  50 Batch  326 / 525  Training Loss  8.199245712603442e-06\n",
            "Epoch  50 Batch  327 / 525  Training Loss  1.3853258678864222e-05\n",
            "Epoch  50 Batch  328 / 525  Training Loss  1.4011752682563383e-05\n",
            "Epoch  50 Batch  329 / 525  Training Loss  8.514455657859799e-06\n",
            "Epoch  50 Batch  330 / 525  Training Loss  9.560985745338257e-06\n",
            "Epoch  50 Batch  331 / 525  Training Loss  1.3674078218173236e-05\n",
            "Epoch  50 Batch  332 / 525  Training Loss  1.136651189881377e-05\n",
            "Epoch  50 Batch  333 / 525  Training Loss  7.776308848406188e-06\n",
            "Epoch  50 Batch  334 / 525  Training Loss  7.767283022985794e-06\n",
            "Epoch  50 Batch  335 / 525  Training Loss  1.3684574696526397e-05\n",
            "Epoch  50 Batch  336 / 525  Training Loss  1.2736399185087066e-05\n",
            "Epoch  50 Batch  337 / 525  Training Loss  1.1447830729593989e-05\n",
            "Epoch  50 Batch  338 / 525  Training Loss  1.4026705684955232e-05\n",
            "Epoch  50 Batch  339 / 525  Training Loss  8.509508916176856e-06\n",
            "Epoch  50 Batch  340 / 525  Training Loss  1.0443983228469733e-05\n",
            "Epoch  50 Batch  341 / 525  Training Loss  5.435279945231741e-06\n",
            "Epoch  50 Batch  342 / 525  Training Loss  8.933914614317473e-06\n",
            "Epoch  50 Batch  343 / 525  Training Loss  8.991528375190683e-06\n",
            "Epoch  50 Batch  344 / 525  Training Loss  9.341300938103814e-06\n",
            "Epoch  50 Batch  345 / 525  Training Loss  8.179988071788102e-06\n",
            "Epoch  50 Batch  346 / 525  Training Loss  1.383437484037131e-05\n",
            "Epoch  50 Batch  347 / 525  Training Loss  9.161389243672602e-06\n",
            "Epoch  50 Batch  348 / 525  Training Loss  1.0410571121610701e-05\n",
            "Epoch  50 Batch  349 / 525  Training Loss  8.86083216755651e-06\n",
            "Epoch  50 Batch  350 / 525  Training Loss  8.479721145704389e-06\n",
            "Epoch  50 Batch  351 / 525  Training Loss  1.0125759217771702e-05\n",
            "Epoch  50 Batch  352 / 525  Training Loss  8.955134035204537e-06\n",
            "Epoch  50 Batch  353 / 525  Training Loss  9.130446414928883e-06\n",
            "Epoch  50 Batch  354 / 525  Training Loss  1.2767489351972472e-05\n",
            "Epoch  50 Batch  355 / 525  Training Loss  9.125008546106983e-06\n",
            "Epoch  50 Batch  356 / 525  Training Loss  1.0549518265179358e-05\n",
            "Epoch  50 Batch  357 / 525  Training Loss  8.546066055714618e-06\n",
            "Epoch  50 Batch  358 / 525  Training Loss  7.918591109046247e-06\n",
            "Epoch  50 Batch  359 / 525  Training Loss  1.4814615497016348e-05\n",
            "Epoch  50 Batch  360 / 525  Training Loss  1.1669435480143875e-05\n",
            "Epoch  50 Batch  361 / 525  Training Loss  6.021799890731927e-06\n",
            "Epoch  50 Batch  362 / 525  Training Loss  6.862640475446824e-06\n",
            "Epoch  50 Batch  363 / 525  Training Loss  1.2396713827911299e-05\n",
            "Epoch  50 Batch  364 / 525  Training Loss  1.498952588008251e-05\n",
            "Epoch  50 Batch  365 / 525  Training Loss  1.2550231076602358e-05\n",
            "Epoch  50 Batch  366 / 525  Training Loss  9.025585313793272e-06\n",
            "Epoch  50 Batch  367 / 525  Training Loss  1.119694297813112e-05\n",
            "Epoch  50 Batch  368 / 525  Training Loss  1.1273658856225666e-05\n",
            "Epoch  50 Batch  369 / 525  Training Loss  1.0895169907598756e-05\n",
            "Epoch  50 Batch  370 / 525  Training Loss  1.3170314559829421e-05\n",
            "Epoch  50 Batch  371 / 525  Training Loss  1.24097959997016e-05\n",
            "Epoch  50 Batch  372 / 525  Training Loss  1.0361666682001669e-05\n",
            "Epoch  50 Batch  373 / 525  Training Loss  1.3792127901979256e-05\n",
            "Epoch  50 Batch  374 / 525  Training Loss  1.326747405983042e-05\n",
            "Epoch  50 Batch  375 / 525  Training Loss  1.1170604921062477e-05\n",
            "Epoch  50 Batch  376 / 525  Training Loss  1.4895122149027884e-05\n",
            "Epoch  50 Batch  377 / 525  Training Loss  9.70128985500196e-06\n",
            "Epoch  50 Batch  378 / 525  Training Loss  9.026547559187748e-06\n",
            "Epoch  50 Batch  379 / 525  Training Loss  1.1570455171749927e-05\n",
            "Epoch  50 Batch  380 / 525  Training Loss  7.931168511277065e-06\n",
            "Epoch  50 Batch  381 / 525  Training Loss  8.694227290106937e-06\n",
            "Epoch  50 Batch  382 / 525  Training Loss  1.1028173503291328e-05\n",
            "Epoch  50 Batch  383 / 525  Training Loss  1.6913159925024956e-05\n",
            "Epoch  50 Batch  384 / 525  Training Loss  1.263798367290292e-05\n",
            "Epoch  50 Batch  385 / 525  Training Loss  1.0385742825747002e-05\n",
            "Epoch  50 Batch  386 / 525  Training Loss  9.89139880402945e-06\n",
            "Epoch  50 Batch  387 / 525  Training Loss  1.0461955753271468e-05\n",
            "Epoch  50 Batch  388 / 525  Training Loss  9.009359018818941e-06\n",
            "Epoch  50 Batch  389 / 525  Training Loss  8.613046702521387e-06\n",
            "Epoch  50 Batch  390 / 525  Training Loss  9.378068170917686e-06\n",
            "Epoch  50 Batch  391 / 525  Training Loss  9.163467439066153e-06\n",
            "Epoch  50 Batch  392 / 525  Training Loss  1.033332955557853e-05\n",
            "Epoch  50 Batch  393 / 525  Training Loss  1.2577819688885938e-05\n",
            "Epoch  50 Batch  394 / 525  Training Loss  8.115217497106642e-06\n",
            "Epoch  50 Batch  395 / 525  Training Loss  1.178230104414979e-05\n",
            "Epoch  50 Batch  396 / 525  Training Loss  6.588127689610701e-06\n",
            "Epoch  50 Batch  397 / 525  Training Loss  8.391213668801356e-06\n",
            "Epoch  50 Batch  398 / 525  Training Loss  1.3017161109019071e-05\n",
            "Epoch  50 Batch  399 / 525  Training Loss  8.408807843807153e-06\n",
            "Epoch  50 Batch  400 / 525  Training Loss  1.1549595001270063e-05\n",
            "Epoch  50 Batch  401 / 525  Training Loss  1.0329885299142916e-05\n",
            "Epoch  50 Batch  402 / 525  Training Loss  1.3990880688652396e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  50 Batch  403 / 525  Training Loss  1.04077489595511e-05\n",
            "Epoch  50 Batch  404 / 525  Training Loss  5.719657565350644e-06\n",
            "Epoch  50 Batch  405 / 525  Training Loss  1.086516203940846e-05\n",
            "Epoch  50 Batch  406 / 525  Training Loss  9.94567290035775e-06\n",
            "Epoch  50 Batch  407 / 525  Training Loss  8.321216228068806e-06\n",
            "Epoch  50 Batch  408 / 525  Training Loss  1.0868587196455337e-05\n",
            "Epoch  50 Batch  409 / 525  Training Loss  1.3004248103243299e-05\n",
            "Epoch  50 Batch  410 / 525  Training Loss  6.775945621484425e-06\n",
            "Epoch  50 Batch  411 / 525  Training Loss  7.962796189531218e-06\n",
            "Epoch  50 Batch  412 / 525  Training Loss  7.547400855401065e-06\n",
            "Epoch  50 Batch  413 / 525  Training Loss  1.2004432392132003e-05\n",
            "Epoch  50 Batch  414 / 525  Training Loss  1.0687134817999322e-05\n",
            "Epoch  50 Batch  415 / 525  Training Loss  1.2654350030061323e-05\n",
            "Epoch  50 Batch  416 / 525  Training Loss  1.3851594303559978e-05\n",
            "Epoch  50 Batch  417 / 525  Training Loss  6.7831970227416605e-06\n",
            "Epoch  50 Batch  418 / 525  Training Loss  1.2152303497714456e-05\n",
            "Epoch  50 Batch  419 / 525  Training Loss  1.024048378894804e-05\n",
            "Epoch  50 Batch  420 / 525  Training Loss  4.581175744533539e-06\n",
            "Epoch  50 Batch  421 / 525  Training Loss  7.4684348874143325e-06\n",
            "Epoch  50 Batch  422 / 525  Training Loss  8.246545803558547e-06\n",
            "Epoch  50 Batch  423 / 525  Training Loss  9.34455692913616e-06\n",
            "Epoch  50 Batch  424 / 525  Training Loss  1.068995516106952e-05\n",
            "Epoch  50 Batch  425 / 525  Training Loss  1.1846232155221514e-05\n",
            "Epoch  50 Batch  426 / 525  Training Loss  1.2279017028049566e-05\n",
            "Epoch  50 Batch  427 / 525  Training Loss  7.933855158626102e-06\n",
            "Epoch  50 Batch  428 / 525  Training Loss  1.0275414751959033e-05\n",
            "Epoch  50 Batch  429 / 525  Training Loss  1.0831442523340229e-05\n",
            "Epoch  50 Batch  430 / 525  Training Loss  1.1029184861399699e-05\n",
            "Epoch  50 Batch  431 / 525  Training Loss  1.077034085028572e-05\n",
            "Epoch  50 Batch  432 / 525  Training Loss  1.274695841857465e-05\n",
            "Epoch  50 Batch  433 / 525  Training Loss  1.0518677299842238e-05\n",
            "Epoch  50 Batch  434 / 525  Training Loss  1.149333365901839e-05\n",
            "Epoch  50 Batch  435 / 525  Training Loss  7.213225671875989e-06\n",
            "Epoch  50 Batch  436 / 525  Training Loss  7.057337825244758e-06\n",
            "Epoch  50 Batch  437 / 525  Training Loss  1.0264464435749687e-05\n",
            "Epoch  50 Batch  438 / 525  Training Loss  1.3887974091630895e-05\n",
            "Epoch  50 Batch  439 / 525  Training Loss  8.831703780742828e-06\n",
            "Epoch  50 Batch  440 / 525  Training Loss  1.0634171303536277e-05\n",
            "Epoch  50 Batch  441 / 525  Training Loss  1.1407496458559763e-05\n",
            "Epoch  50 Batch  442 / 525  Training Loss  1.1062152225349564e-05\n",
            "Epoch  50 Batch  443 / 525  Training Loss  1.181463449029252e-05\n",
            "Epoch  50 Batch  444 / 525  Training Loss  1.044855707732495e-05\n",
            "Epoch  50 Batch  445 / 525  Training Loss  7.17612147127511e-06\n",
            "Epoch  50 Batch  446 / 525  Training Loss  1.4057800399314146e-05\n",
            "Epoch  50 Batch  447 / 525  Training Loss  8.042180525080767e-06\n",
            "Epoch  50 Batch  448 / 525  Training Loss  1.6249698091996834e-05\n",
            "Epoch  50 Batch  449 / 525  Training Loss  4.940650796925183e-06\n",
            "Epoch  50 Batch  450 / 525  Training Loss  1.0211488188360818e-05\n",
            "Epoch  50 Batch  451 / 525  Training Loss  1.0540014955040533e-05\n",
            "Epoch  50 Batch  452 / 525  Training Loss  9.325949577032588e-06\n",
            "Epoch  50 Batch  453 / 525  Training Loss  1.0615467544994317e-05\n",
            "Epoch  50 Batch  454 / 525  Training Loss  9.783043424249627e-06\n",
            "Epoch  50 Batch  455 / 525  Training Loss  9.469191354583018e-06\n",
            "Epoch  50 Batch  456 / 525  Training Loss  1.2971892829227727e-05\n",
            "Epoch  50 Batch  457 / 525  Training Loss  1.2034988685627468e-05\n",
            "Epoch  50 Batch  458 / 525  Training Loss  7.811376235622447e-06\n",
            "Epoch  50 Batch  459 / 525  Training Loss  1.5080501725606155e-05\n",
            "Epoch  50 Batch  460 / 525  Training Loss  1.4172624105412979e-05\n",
            "Epoch  50 Batch  461 / 525  Training Loss  1.391176920151338e-05\n",
            "Epoch  50 Batch  462 / 525  Training Loss  1.041596715367632e-05\n",
            "Epoch  50 Batch  463 / 525  Training Loss  9.691228115116246e-06\n",
            "Epoch  50 Batch  464 / 525  Training Loss  1.105806131818099e-05\n",
            "Epoch  50 Batch  465 / 525  Training Loss  9.222187145496719e-06\n",
            "Epoch  50 Batch  466 / 525  Training Loss  8.874712875694968e-06\n",
            "Epoch  50 Batch  467 / 525  Training Loss  5.841375241288915e-06\n",
            "Epoch  50 Batch  468 / 525  Training Loss  1.1387835911591537e-05\n",
            "Epoch  50 Batch  469 / 525  Training Loss  1.001100736175431e-05\n",
            "Epoch  50 Batch  470 / 525  Training Loss  9.381373274663929e-06\n",
            "Epoch  50 Batch  471 / 525  Training Loss  1.0389956514700316e-05\n",
            "Epoch  50 Batch  472 / 525  Training Loss  5.572820100496756e-06\n",
            "Epoch  50 Batch  473 / 525  Training Loss  9.09643313207198e-06\n",
            "Epoch  50 Batch  474 / 525  Training Loss  1.3699524060939439e-05\n",
            "Epoch  50 Batch  475 / 525  Training Loss  5.5649516070843674e-06\n",
            "Epoch  50 Batch  476 / 525  Training Loss  8.879806955519598e-06\n",
            "Epoch  50 Batch  477 / 525  Training Loss  1.7534595826873556e-05\n",
            "Epoch  50 Batch  478 / 525  Training Loss  8.854332008922938e-06\n",
            "Epoch  50 Batch  479 / 525  Training Loss  1.3831295291311108e-05\n",
            "Epoch  50 Batch  480 / 525  Training Loss  1.1776840437960345e-05\n",
            "Epoch  50 Batch  481 / 525  Training Loss  9.234799108526204e-06\n",
            "Epoch  50 Batch  482 / 525  Training Loss  8.974525371741038e-06\n",
            "Epoch  50 Batch  483 / 525  Training Loss  1.0333942555007525e-05\n",
            "Epoch  50 Batch  484 / 525  Training Loss  7.596273462695535e-06\n",
            "Epoch  50 Batch  485 / 525  Training Loss  1.1200345397810452e-05\n",
            "Epoch  50 Batch  486 / 525  Training Loss  1.22594383356045e-05\n",
            "Epoch  50 Batch  487 / 525  Training Loss  1.4870864106342196e-05\n",
            "Epoch  50 Batch  488 / 525  Training Loss  9.345432772533968e-06\n",
            "Epoch  50 Batch  489 / 525  Training Loss  7.2117859417630825e-06\n",
            "Epoch  50 Batch  490 / 525  Training Loss  1.3533988749259152e-05\n",
            "Epoch  50 Batch  491 / 525  Training Loss  1.0422052582725883e-05\n",
            "Epoch  50 Batch  492 / 525  Training Loss  1.0120073056896217e-05\n",
            "Epoch  50 Batch  493 / 525  Training Loss  1.5675015674787574e-05\n",
            "Epoch  50 Batch  494 / 525  Training Loss  5.485266228788532e-06\n",
            "Epoch  50 Batch  495 / 525  Training Loss  6.901655979163479e-06\n",
            "Epoch  50 Batch  496 / 525  Training Loss  8.490053005516529e-06\n",
            "Epoch  50 Batch  497 / 525  Training Loss  9.832090654526837e-06\n",
            "Epoch  50 Batch  498 / 525  Training Loss  1.14559934445424e-05\n",
            "Epoch  50 Batch  499 / 525  Training Loss  7.820623977750074e-06\n",
            "Epoch  50 Batch  500 / 525  Training Loss  1.3694310837308876e-05\n",
            "Epoch  50 Batch  501 / 525  Training Loss  9.780656910152175e-06\n",
            "Epoch  50 Batch  502 / 525  Training Loss  1.0370686140959151e-05\n",
            "Epoch  50 Batch  503 / 525  Training Loss  8.400065780733712e-06\n",
            "Epoch  50 Batch  504 / 525  Training Loss  1.0390672287030611e-05\n",
            "Epoch  50 Batch  505 / 525  Training Loss  9.048293577507138e-06\n",
            "Epoch  50 Batch  506 / 525  Training Loss  1.4033155821380205e-05\n",
            "Epoch  50 Batch  507 / 525  Training Loss  1.0100767212861683e-05\n",
            "Epoch  50 Batch  508 / 525  Training Loss  6.820585440436844e-06\n",
            "Epoch  50 Batch  509 / 525  Training Loss  1.2277031601115596e-05\n",
            "Epoch  50 Batch  510 / 525  Training Loss  1.0311351616110187e-05\n",
            "Epoch  50 Batch  511 / 525  Training Loss  9.312108886661008e-06\n",
            "Epoch  50 Batch  512 / 525  Training Loss  9.004417734104209e-06\n",
            "Epoch  50 Batch  513 / 525  Training Loss  1.1599561730690766e-05\n",
            "Epoch  50 Batch  514 / 525  Training Loss  9.959583621821366e-06\n",
            "Epoch  50 Batch  515 / 525  Training Loss  7.618436029588338e-06\n",
            "Epoch  50 Batch  516 / 525  Training Loss  1.2363033420115244e-05\n",
            "Epoch  50 Batch  517 / 525  Training Loss  1.2684993635048158e-05\n",
            "Epoch  50 Batch  518 / 525  Training Loss  7.530298717028927e-06\n",
            "Epoch  50 Batch  519 / 525  Training Loss  1.235885611095e-05\n",
            "Epoch  50 Batch  520 / 525  Training Loss  7.384480795735726e-06\n",
            "Epoch  50 Batch  521 / 525  Training Loss  9.13561052584555e-06\n",
            "Epoch  50 Batch  522 / 525  Training Loss  9.304191735282075e-06\n",
            "Epoch  50 Batch  523 / 525  Training Loss  8.14266695670085e-06\n",
            "Epoch  50 Batch  524 / 525  Training Loss  1.0897196261794306e-05\n",
            "  51    |    -    |   0.000010   | 64.525000\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmRO3Wvn2IxU"
      },
      "source": [
        "#PATH = \"FT_Masked_number_prediction_model_4684.pt\"\n",
        "#torch.save({\n",
        "#            'epoch': num_of_epochs,\n",
        "#            'model_state_dict': model.state_dict(),\n",
        "#            'optimizer_state_dict': optimizer.state_dict(),\n",
        "#            'loss': running_loss,\n",
        "#            }, PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZVd405WoNeM",
        "outputId": "8edcc418-d329-41b7-8ca5-44e713f20519"
      },
      "source": [
        "evaluate(model, val_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "75.525"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmMohRaT2ItP",
        "outputId": "4b0ad414-1965-493f-fe4f-70980e74594d"
      },
      "source": [
        "evaluate(model, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "75.74107142857143"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix9l9vyjoNeN"
      },
      "source": [
        "## Fine Tuning - Interpolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNtSewYCoNeN"
      },
      "source": [
        "num_of_epochs = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDtHFM9zoNeM"
      },
      "source": [
        "# Loading the configuration file for 't5-base' model\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-rwYmb0oNeN"
      },
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained('Pretrain_NPM_3op.bin', return_dict=True, config='t5-base-config.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8btz5TXLoNeN",
        "outputId": "8158ffd6-8b4b-4d0f-cee3-0286068b2e87"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "T5ForConditionalGeneration(\n",
              "  (shared): Embedding(32128, 768)\n",
              "  (encoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): T5Stack(\n",
              "    (embed_tokens): Embedding(32128, 768)\n",
              "    (block): ModuleList(\n",
              "      (0): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 12)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (2): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (3): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (4): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (5): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (6): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (7): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (8): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (9): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (10): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (11): T5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): T5LayerSelfAttention(\n",
              "            (SelfAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): T5LayerCrossAttention(\n",
              "            (EncDecAttention): T5Attention(\n",
              "              (q): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (k): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (v): Linear(in_features=768, out_features=768, bias=False)\n",
              "              (o): Linear(in_features=768, out_features=768, bias=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): T5LayerFF(\n",
              "            (DenseReluDense): T5DenseReluDense(\n",
              "              (wi): Linear(in_features=768, out_features=3072, bias=False)\n",
              "              (wo): Linear(in_features=3072, out_features=768, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (layer_norm): T5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): T5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=32128, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck0U7jpEoNeN"
      },
      "source": [
        "data = pd.read_csv('finetune_inter_dataset.csv', header=None, names=['inputs', 'target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYU22T3YoNeO"
      },
      "source": [
        "data_inter = data.sample(n = 40000, random_state = 42).reset_index(drop=True)\n",
        "\n",
        "train_inter, validation_inter = train_test_split(data_inter, test_size=0.3, random_state=42)\n",
        "train_inter, test_inter = train_test_split(train_inter, test_size=0.4, random_state=42)\n",
        "\n",
        "data_train_inter = train_inter.reset_index(drop=True)\n",
        "data_valid_inter = validation_inter.reset_index(drop=True)\n",
        "data_test_inter = test_inter.reset_index(drop=True)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "train_inputs_inter, train_masks_inter = get_word_embeddings(data_train_inter['inputs'])\n",
        "val_inputs_inter, val_masks_inter = get_word_embeddings(data_valid_inter['inputs'])\n",
        "test_inputs_inter, test_masks_inter = get_word_embeddings(data_test_inter['inputs'])\n",
        "\n",
        "data_train_inter['target_str'] = data_train_inter['target'].astype(str)\n",
        "data_valid_inter['target_str'] = data_valid_inter['target'].astype(str)\n",
        "data_test_inter['target_str'] = data_test_inter['target'].astype(str)\n",
        "\n",
        "#convert lists to tensors\n",
        "train_labels_inter = get_word_embeddings(data_train_inter['target_str'])[0]\n",
        "val_labels_inter = get_word_embeddings(data_valid_inter['target_str'])[0]\n",
        "test_labels_inter = get_word_embeddings(data_test_inter['target_str'])[0]\n",
        "\n",
        "batch_size = 32\n",
        "num_of_epochs = 30\n",
        "\n",
        "train_data_inter = TensorDataset(train_inputs_inter, train_masks_inter, train_labels_inter)\n",
        "train_dataloader_inter = DataLoader(train_data_inter, shuffle = True, batch_size = batch_size)\n",
        "\n",
        "val_data_inter = TensorDataset(val_inputs_inter, val_masks_inter, val_labels_inter)\n",
        "val_dataloader_inter = DataLoader(val_data_inter, shuffle = True, batch_size = batch_size)\n",
        "\n",
        "test_data_inter = TensorDataset(test_inputs_inter, test_masks_inter, test_labels_inter)\n",
        "test_dataloader_inter = DataLoader(test_data_inter, shuffle = True, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpR_V-0_oNeO"
      },
      "source": [
        "data_inputs_inter, data_masks_inter = get_word_embeddings(data_inter['inputs'])\n",
        "data_inter['target_str'] = data_inter['target'].astype(str)\n",
        "data_labels_inter = get_word_embeddings(data_inter['target_str'])[0]\n",
        "data_inter_loader = TensorDataset(data_inputs_inter, data_masks_inter, data_labels_inter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlCbmfAQ-CWJ",
        "outputId": "d0298540-e90b-40a7-905b-e77ec09c1f20"
      },
      "source": [
        "import gc\n",
        "\n",
        "val_acc = 0\n",
        "train_accuracy = 0\n",
        "\n",
        "# Sets the module in training mode\n",
        "model.train()\n",
        "\n",
        "for epoch in range(1,num_of_epochs+1):\n",
        "    print('Running epoch: {}'.format(epoch))\n",
        "    running_loss=0\n",
        "    # out = display(progress(1, num_of_batches+1), display_id=True)\n",
        "    i =0 \n",
        "    for batch in train_dataloader:\n",
        "    \n",
        "        input_ids, attn_mask, labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # clear out the gradients of all Variables \n",
        "        optimizer.zero_grad()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Forward propogation\n",
        "        # print(model(input_ids=input_ids, attention_mask=attn_mask, labels=labels))\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss_num=loss.item()\n",
        "        logits = outputs.logits\n",
        "        running_loss+=loss_num\n",
        "        # out.update(progress(loss_num,i, num_of_batches+1))\n",
        "\n",
        "        # calculating the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # updating the params\n",
        "        optimizer.step()\n",
        "\n",
        "        print(\"Epoch \", epoch, \"Batch \", i, \"/\", len(train_dataloader), \" Training Loss \", loss_num)\n",
        "        i += 1\n",
        "\n",
        "    running_loss = running_loss/len(train_dataloader)\n",
        "    # v_input_ids, v_attn_mask, v_labels = tuple(t.to(device) for t in data_valid)\n",
        "  \n",
        "    curr_accuracy = evaluate(model, val_dataloader)\n",
        "\n",
        "    # print('Epoch: {} , Running loss: {}'.format(epoch,running_loss))\n",
        "    print(f\"{epoch + 1:^7} | {'-':^7} | {running_loss:^12.6f} | {curr_accuracy:^9.2f}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    if curr_accuracy > val_acc:\n",
        "        val_acc = curr_accuracy\n",
        "        # Saving the best model\n",
        "        torch.save(model.state_dict(),'Best_FT_Model_Pre_Train_3num.bin')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running epoch: 1\n",
            "Epoch  1 Batch  0 / 525  Training Loss  2.7338666915893555\n",
            "Epoch  1 Batch  1 / 525  Training Loss  1.788694977760315\n",
            "Epoch  1 Batch  2 / 525  Training Loss  1.2431199550628662\n",
            "Epoch  1 Batch  3 / 525  Training Loss  0.9950159788131714\n",
            "Epoch  1 Batch  4 / 525  Training Loss  0.7868257761001587\n",
            "Epoch  1 Batch  5 / 525  Training Loss  0.7052675485610962\n",
            "Epoch  1 Batch  6 / 525  Training Loss  0.5944342017173767\n",
            "Epoch  1 Batch  7 / 525  Training Loss  0.5937036275863647\n",
            "Epoch  1 Batch  8 / 525  Training Loss  0.640777587890625\n",
            "Epoch  1 Batch  9 / 525  Training Loss  0.5935794711112976\n",
            "Epoch  1 Batch  10 / 525  Training Loss  0.5305238366127014\n",
            "Epoch  1 Batch  11 / 525  Training Loss  0.5320760011672974\n",
            "Epoch  1 Batch  12 / 525  Training Loss  0.5120550990104675\n",
            "Epoch  1 Batch  13 / 525  Training Loss  0.4717279374599457\n",
            "Epoch  1 Batch  14 / 525  Training Loss  0.4447910785675049\n",
            "Epoch  1 Batch  15 / 525  Training Loss  0.46398457884788513\n",
            "Epoch  1 Batch  16 / 525  Training Loss  0.4412851333618164\n",
            "Epoch  1 Batch  17 / 525  Training Loss  0.39714211225509644\n",
            "Epoch  1 Batch  18 / 525  Training Loss  0.4193480610847473\n",
            "Epoch  1 Batch  19 / 525  Training Loss  0.5107942819595337\n",
            "Epoch  1 Batch  20 / 525  Training Loss  0.41491904854774475\n",
            "Epoch  1 Batch  21 / 525  Training Loss  0.3804040551185608\n",
            "Epoch  1 Batch  22 / 525  Training Loss  0.4049256443977356\n",
            "Epoch  1 Batch  23 / 525  Training Loss  0.3322363793849945\n",
            "Epoch  1 Batch  24 / 525  Training Loss  0.3890794813632965\n",
            "Epoch  1 Batch  25 / 525  Training Loss  0.385917603969574\n",
            "Epoch  1 Batch  26 / 525  Training Loss  0.38647112250328064\n",
            "Epoch  1 Batch  27 / 525  Training Loss  0.39445120096206665\n",
            "Epoch  1 Batch  28 / 525  Training Loss  0.38094767928123474\n",
            "Epoch  1 Batch  29 / 525  Training Loss  0.41183918714523315\n",
            "Epoch  1 Batch  30 / 525  Training Loss  0.3638196885585785\n",
            "Epoch  1 Batch  31 / 525  Training Loss  0.36522796750068665\n",
            "Epoch  1 Batch  32 / 525  Training Loss  0.360579252243042\n",
            "Epoch  1 Batch  33 / 525  Training Loss  0.3689565360546112\n",
            "Epoch  1 Batch  34 / 525  Training Loss  0.3656079173088074\n",
            "Epoch  1 Batch  35 / 525  Training Loss  0.37585338950157166\n",
            "Epoch  1 Batch  36 / 525  Training Loss  0.3801107108592987\n",
            "Epoch  1 Batch  37 / 525  Training Loss  0.4083203375339508\n",
            "Epoch  1 Batch  38 / 525  Training Loss  0.38078463077545166\n",
            "Epoch  1 Batch  39 / 525  Training Loss  0.3717058002948761\n",
            "Epoch  1 Batch  40 / 525  Training Loss  0.3756796717643738\n",
            "Epoch  1 Batch  41 / 525  Training Loss  0.36597198247909546\n",
            "Epoch  1 Batch  42 / 525  Training Loss  0.3645903170108795\n",
            "Epoch  1 Batch  43 / 525  Training Loss  0.3416530191898346\n",
            "Epoch  1 Batch  44 / 525  Training Loss  0.3725129961967468\n",
            "Epoch  1 Batch  45 / 525  Training Loss  0.32810667157173157\n",
            "Epoch  1 Batch  46 / 525  Training Loss  0.36303484439849854\n",
            "Epoch  1 Batch  47 / 525  Training Loss  0.34199169278144836\n",
            "Epoch  1 Batch  48 / 525  Training Loss  0.31505027413368225\n",
            "Epoch  1 Batch  49 / 525  Training Loss  0.3284359872341156\n",
            "Epoch  1 Batch  50 / 525  Training Loss  0.3561486303806305\n",
            "Epoch  1 Batch  51 / 525  Training Loss  0.3756445050239563\n",
            "Epoch  1 Batch  52 / 525  Training Loss  0.3328252136707306\n",
            "Epoch  1 Batch  53 / 525  Training Loss  0.32622411847114563\n",
            "Epoch  1 Batch  54 / 525  Training Loss  0.39394766092300415\n",
            "Epoch  1 Batch  55 / 525  Training Loss  0.3638344705104828\n",
            "Epoch  1 Batch  56 / 525  Training Loss  0.34817856550216675\n",
            "Epoch  1 Batch  57 / 525  Training Loss  0.324410617351532\n",
            "Epoch  1 Batch  58 / 525  Training Loss  0.34112271666526794\n",
            "Epoch  1 Batch  59 / 525  Training Loss  0.32946544885635376\n",
            "Epoch  1 Batch  60 / 525  Training Loss  0.35342928767204285\n",
            "Epoch  1 Batch  61 / 525  Training Loss  0.3542381227016449\n",
            "Epoch  1 Batch  62 / 525  Training Loss  0.32911157608032227\n",
            "Epoch  1 Batch  63 / 525  Training Loss  0.32694849371910095\n",
            "Epoch  1 Batch  64 / 525  Training Loss  0.32610026001930237\n",
            "Epoch  1 Batch  65 / 525  Training Loss  0.3327491879463196\n",
            "Epoch  1 Batch  66 / 525  Training Loss  0.35789093375205994\n",
            "Epoch  1 Batch  67 / 525  Training Loss  0.31983664631843567\n",
            "Epoch  1 Batch  68 / 525  Training Loss  0.3158903121948242\n",
            "Epoch  1 Batch  69 / 525  Training Loss  0.3144054412841797\n",
            "Epoch  1 Batch  70 / 525  Training Loss  0.3472578525543213\n",
            "Epoch  1 Batch  71 / 525  Training Loss  0.30654850602149963\n",
            "Epoch  1 Batch  72 / 525  Training Loss  0.3404318392276764\n",
            "Epoch  1 Batch  73 / 525  Training Loss  0.3295551836490631\n",
            "Epoch  1 Batch  74 / 525  Training Loss  0.32916513085365295\n",
            "Epoch  1 Batch  75 / 525  Training Loss  0.3184983432292938\n",
            "Epoch  1 Batch  76 / 525  Training Loss  0.31466853618621826\n",
            "Epoch  1 Batch  77 / 525  Training Loss  0.2936980128288269\n",
            "Epoch  1 Batch  78 / 525  Training Loss  0.309103399515152\n",
            "Epoch  1 Batch  79 / 525  Training Loss  0.337573766708374\n",
            "Epoch  1 Batch  80 / 525  Training Loss  0.3062345087528229\n",
            "Epoch  1 Batch  81 / 525  Training Loss  0.3172340989112854\n",
            "Epoch  1 Batch  82 / 525  Training Loss  0.3112139403820038\n",
            "Epoch  1 Batch  83 / 525  Training Loss  0.3159518241882324\n",
            "Epoch  1 Batch  84 / 525  Training Loss  0.3480989933013916\n",
            "Epoch  1 Batch  85 / 525  Training Loss  0.32451269030570984\n",
            "Epoch  1 Batch  86 / 525  Training Loss  0.3166816830635071\n",
            "Epoch  1 Batch  87 / 525  Training Loss  0.29028454422950745\n",
            "Epoch  1 Batch  88 / 525  Training Loss  0.3226679563522339\n",
            "Epoch  1 Batch  89 / 525  Training Loss  0.3328326344490051\n",
            "Epoch  1 Batch  90 / 525  Training Loss  0.2944919168949127\n",
            "Epoch  1 Batch  91 / 525  Training Loss  0.3095109164714813\n",
            "Epoch  1 Batch  92 / 525  Training Loss  0.3423771858215332\n",
            "Epoch  1 Batch  93 / 525  Training Loss  0.3221902847290039\n",
            "Epoch  1 Batch  94 / 525  Training Loss  0.3459990322589874\n",
            "Epoch  1 Batch  95 / 525  Training Loss  0.31391531229019165\n",
            "Epoch  1 Batch  96 / 525  Training Loss  0.29884010553359985\n",
            "Epoch  1 Batch  97 / 525  Training Loss  0.32584890723228455\n",
            "Epoch  1 Batch  98 / 525  Training Loss  0.2939828038215637\n",
            "Epoch  1 Batch  99 / 525  Training Loss  0.3523947596549988\n",
            "Epoch  1 Batch  100 / 525  Training Loss  0.2973329424858093\n",
            "Epoch  1 Batch  101 / 525  Training Loss  0.33482205867767334\n",
            "Epoch  1 Batch  102 / 525  Training Loss  0.3251810073852539\n",
            "Epoch  1 Batch  103 / 525  Training Loss  0.3121759295463562\n",
            "Epoch  1 Batch  104 / 525  Training Loss  0.32486313581466675\n",
            "Epoch  1 Batch  105 / 525  Training Loss  0.29095548391342163\n",
            "Epoch  1 Batch  106 / 525  Training Loss  0.3431091606616974\n",
            "Epoch  1 Batch  107 / 525  Training Loss  0.32392552495002747\n",
            "Epoch  1 Batch  108 / 525  Training Loss  0.30892008543014526\n",
            "Epoch  1 Batch  109 / 525  Training Loss  0.29958370327949524\n",
            "Epoch  1 Batch  110 / 525  Training Loss  0.29266613721847534\n",
            "Epoch  1 Batch  111 / 525  Training Loss  0.3120211064815521\n",
            "Epoch  1 Batch  112 / 525  Training Loss  0.30240482091903687\n",
            "Epoch  1 Batch  113 / 525  Training Loss  0.31984105706214905\n",
            "Epoch  1 Batch  114 / 525  Training Loss  0.32543283700942993\n",
            "Epoch  1 Batch  115 / 525  Training Loss  0.30873286724090576\n",
            "Epoch  1 Batch  116 / 525  Training Loss  0.27169254422187805\n",
            "Epoch  1 Batch  117 / 525  Training Loss  0.30079886317253113\n",
            "Epoch  1 Batch  118 / 525  Training Loss  0.3277963399887085\n",
            "Epoch  1 Batch  119 / 525  Training Loss  0.29463037848472595\n",
            "Epoch  1 Batch  120 / 525  Training Loss  0.2949734330177307\n",
            "Epoch  1 Batch  121 / 525  Training Loss  0.3180186450481415\n",
            "Epoch  1 Batch  122 / 525  Training Loss  0.29642996191978455\n",
            "Epoch  1 Batch  123 / 525  Training Loss  0.30725592374801636\n",
            "Epoch  1 Batch  124 / 525  Training Loss  0.2935803532600403\n",
            "Epoch  1 Batch  125 / 525  Training Loss  0.31607288122177124\n",
            "Epoch  1 Batch  126 / 525  Training Loss  0.2936965525150299\n",
            "Epoch  1 Batch  127 / 525  Training Loss  0.3037430942058563\n",
            "Epoch  1 Batch  128 / 525  Training Loss  0.35116952657699585\n",
            "Epoch  1 Batch  129 / 525  Training Loss  0.32102659344673157\n",
            "Epoch  1 Batch  130 / 525  Training Loss  0.31786981225013733\n",
            "Epoch  1 Batch  131 / 525  Training Loss  0.290176123380661\n",
            "Epoch  1 Batch  132 / 525  Training Loss  0.2821168005466461\n",
            "Epoch  1 Batch  133 / 525  Training Loss  0.277895987033844\n",
            "Epoch  1 Batch  134 / 525  Training Loss  0.34146374464035034\n",
            "Epoch  1 Batch  135 / 525  Training Loss  0.30300667881965637\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1 Batch  136 / 525  Training Loss  0.2719727158546448\n",
            "Epoch  1 Batch  137 / 525  Training Loss  0.31373536586761475\n",
            "Epoch  1 Batch  138 / 525  Training Loss  0.2898225784301758\n",
            "Epoch  1 Batch  139 / 525  Training Loss  0.29510587453842163\n",
            "Epoch  1 Batch  140 / 525  Training Loss  0.3216855227947235\n",
            "Epoch  1 Batch  141 / 525  Training Loss  0.28650885820388794\n",
            "Epoch  1 Batch  142 / 525  Training Loss  0.28655925393104553\n",
            "Epoch  1 Batch  143 / 525  Training Loss  0.29546767473220825\n",
            "Epoch  1 Batch  144 / 525  Training Loss  0.3155761659145355\n",
            "Epoch  1 Batch  145 / 525  Training Loss  0.2774558663368225\n",
            "Epoch  1 Batch  146 / 525  Training Loss  0.2530766427516937\n",
            "Epoch  1 Batch  147 / 525  Training Loss  0.2867032587528229\n",
            "Epoch  1 Batch  148 / 525  Training Loss  0.29210811853408813\n",
            "Epoch  1 Batch  149 / 525  Training Loss  0.2913128435611725\n",
            "Epoch  1 Batch  150 / 525  Training Loss  0.3010880649089813\n",
            "Epoch  1 Batch  151 / 525  Training Loss  0.29417484998703003\n",
            "Epoch  1 Batch  152 / 525  Training Loss  0.28037482500076294\n",
            "Epoch  1 Batch  153 / 525  Training Loss  0.3053673207759857\n",
            "Epoch  1 Batch  154 / 525  Training Loss  0.3011884093284607\n",
            "Epoch  1 Batch  155 / 525  Training Loss  0.30644577741622925\n",
            "Epoch  1 Batch  156 / 525  Training Loss  0.27211111783981323\n",
            "Epoch  1 Batch  157 / 525  Training Loss  0.3035954535007477\n",
            "Epoch  1 Batch  158 / 525  Training Loss  0.2983700633049011\n",
            "Epoch  1 Batch  159 / 525  Training Loss  0.30222058296203613\n",
            "Epoch  1 Batch  160 / 525  Training Loss  0.2889648377895355\n",
            "Epoch  1 Batch  161 / 525  Training Loss  0.28679728507995605\n",
            "Epoch  1 Batch  162 / 525  Training Loss  0.2916755676269531\n",
            "Epoch  1 Batch  163 / 525  Training Loss  0.3029780089855194\n",
            "Epoch  1 Batch  164 / 525  Training Loss  0.29582878947257996\n",
            "Epoch  1 Batch  165 / 525  Training Loss  0.2850971817970276\n",
            "Epoch  1 Batch  166 / 525  Training Loss  0.28813058137893677\n",
            "Epoch  1 Batch  167 / 525  Training Loss  0.29677683115005493\n",
            "Epoch  1 Batch  168 / 525  Training Loss  0.30935701727867126\n",
            "Epoch  1 Batch  169 / 525  Training Loss  0.3085509240627289\n",
            "Epoch  1 Batch  170 / 525  Training Loss  0.31999310851097107\n",
            "Epoch  1 Batch  171 / 525  Training Loss  0.30082398653030396\n",
            "Epoch  1 Batch  172 / 525  Training Loss  0.29533758759498596\n",
            "Epoch  1 Batch  173 / 525  Training Loss  0.28513503074645996\n",
            "Epoch  1 Batch  174 / 525  Training Loss  0.2882346212863922\n",
            "Epoch  1 Batch  175 / 525  Training Loss  0.2866949439048767\n",
            "Epoch  1 Batch  176 / 525  Training Loss  0.2700360119342804\n",
            "Epoch  1 Batch  177 / 525  Training Loss  0.27347278594970703\n",
            "Epoch  1 Batch  178 / 525  Training Loss  0.28566890954971313\n",
            "Epoch  1 Batch  179 / 525  Training Loss  0.3057630658149719\n",
            "Epoch  1 Batch  180 / 525  Training Loss  0.2797596752643585\n",
            "Epoch  1 Batch  181 / 525  Training Loss  0.2904520034790039\n",
            "Epoch  1 Batch  182 / 525  Training Loss  0.26318538188934326\n",
            "Epoch  1 Batch  183 / 525  Training Loss  0.2996026575565338\n",
            "Epoch  1 Batch  184 / 525  Training Loss  0.2924230098724365\n",
            "Epoch  1 Batch  185 / 525  Training Loss  0.30397966504096985\n",
            "Epoch  1 Batch  186 / 525  Training Loss  0.2719537317752838\n",
            "Epoch  1 Batch  187 / 525  Training Loss  0.30411669611930847\n",
            "Epoch  1 Batch  188 / 525  Training Loss  0.25281184911727905\n",
            "Epoch  1 Batch  189 / 525  Training Loss  0.270188570022583\n",
            "Epoch  1 Batch  190 / 525  Training Loss  0.2788229286670685\n",
            "Epoch  1 Batch  191 / 525  Training Loss  0.3020067811012268\n",
            "Epoch  1 Batch  192 / 525  Training Loss  0.29504331946372986\n",
            "Epoch  1 Batch  193 / 525  Training Loss  0.2950107157230377\n",
            "Epoch  1 Batch  194 / 525  Training Loss  0.2689358592033386\n",
            "Epoch  1 Batch  195 / 525  Training Loss  0.27819445729255676\n",
            "Epoch  1 Batch  196 / 525  Training Loss  0.2765686511993408\n",
            "Epoch  1 Batch  197 / 525  Training Loss  0.3167705833911896\n",
            "Epoch  1 Batch  198 / 525  Training Loss  0.281229704618454\n",
            "Epoch  1 Batch  199 / 525  Training Loss  0.31427234411239624\n",
            "Epoch  1 Batch  200 / 525  Training Loss  0.3085145354270935\n",
            "Epoch  1 Batch  201 / 525  Training Loss  0.27281564474105835\n",
            "Epoch  1 Batch  202 / 525  Training Loss  0.2876896262168884\n",
            "Epoch  1 Batch  203 / 525  Training Loss  0.3021862208843231\n",
            "Epoch  1 Batch  204 / 525  Training Loss  0.29779088497161865\n",
            "Epoch  1 Batch  205 / 525  Training Loss  0.2775495648384094\n",
            "Epoch  1 Batch  206 / 525  Training Loss  0.2698107659816742\n",
            "Epoch  1 Batch  207 / 525  Training Loss  0.2841809391975403\n",
            "Epoch  1 Batch  208 / 525  Training Loss  0.27631038427352905\n",
            "Epoch  1 Batch  209 / 525  Training Loss  0.29147860407829285\n",
            "Epoch  1 Batch  210 / 525  Training Loss  0.2976432144641876\n",
            "Epoch  1 Batch  211 / 525  Training Loss  0.29757723212242126\n",
            "Epoch  1 Batch  212 / 525  Training Loss  0.3054119050502777\n",
            "Epoch  1 Batch  213 / 525  Training Loss  0.2752886414527893\n",
            "Epoch  1 Batch  214 / 525  Training Loss  0.2715640664100647\n",
            "Epoch  1 Batch  215 / 525  Training Loss  0.2701021730899811\n",
            "Epoch  1 Batch  216 / 525  Training Loss  0.26097023487091064\n",
            "Epoch  1 Batch  217 / 525  Training Loss  0.29072055220603943\n",
            "Epoch  1 Batch  218 / 525  Training Loss  0.29328322410583496\n",
            "Epoch  1 Batch  219 / 525  Training Loss  0.311426043510437\n",
            "Epoch  1 Batch  220 / 525  Training Loss  0.29442721605300903\n",
            "Epoch  1 Batch  221 / 525  Training Loss  0.30282002687454224\n",
            "Epoch  1 Batch  222 / 525  Training Loss  0.2767135202884674\n",
            "Epoch  1 Batch  223 / 525  Training Loss  0.26427367329597473\n",
            "Epoch  1 Batch  224 / 525  Training Loss  0.2878747582435608\n",
            "Epoch  1 Batch  225 / 525  Training Loss  0.29488903284072876\n",
            "Epoch  1 Batch  226 / 525  Training Loss  0.2670171856880188\n",
            "Epoch  1 Batch  227 / 525  Training Loss  0.27400845289230347\n",
            "Epoch  1 Batch  228 / 525  Training Loss  0.2599324584007263\n",
            "Epoch  1 Batch  229 / 525  Training Loss  0.292429119348526\n",
            "Epoch  1 Batch  230 / 525  Training Loss  0.27045074105262756\n",
            "Epoch  1 Batch  231 / 525  Training Loss  0.2772060036659241\n",
            "Epoch  1 Batch  232 / 525  Training Loss  0.2733219265937805\n",
            "Epoch  1 Batch  233 / 525  Training Loss  0.28529971837997437\n",
            "Epoch  1 Batch  234 / 525  Training Loss  0.24561350047588348\n",
            "Epoch  1 Batch  235 / 525  Training Loss  0.2650578022003174\n",
            "Epoch  1 Batch  236 / 525  Training Loss  0.2844012379646301\n",
            "Epoch  1 Batch  237 / 525  Training Loss  0.28186020255088806\n",
            "Epoch  1 Batch  238 / 525  Training Loss  0.3143898546695709\n",
            "Epoch  1 Batch  239 / 525  Training Loss  0.2768042981624603\n",
            "Epoch  1 Batch  240 / 525  Training Loss  0.27757367491722107\n",
            "Epoch  1 Batch  241 / 525  Training Loss  0.268704891204834\n",
            "Epoch  1 Batch  242 / 525  Training Loss  0.2621120810508728\n",
            "Epoch  1 Batch  243 / 525  Training Loss  0.26590538024902344\n",
            "Epoch  1 Batch  244 / 525  Training Loss  0.2951620817184448\n",
            "Epoch  1 Batch  245 / 525  Training Loss  0.2836769223213196\n",
            "Epoch  1 Batch  246 / 525  Training Loss  0.2720966637134552\n",
            "Epoch  1 Batch  247 / 525  Training Loss  0.2752301096916199\n",
            "Epoch  1 Batch  248 / 525  Training Loss  0.2895951271057129\n",
            "Epoch  1 Batch  249 / 525  Training Loss  0.2972530722618103\n",
            "Epoch  1 Batch  250 / 525  Training Loss  0.2713068723678589\n",
            "Epoch  1 Batch  251 / 525  Training Loss  0.2818058431148529\n",
            "Epoch  1 Batch  252 / 525  Training Loss  0.29058364033699036\n",
            "Epoch  1 Batch  253 / 525  Training Loss  0.27369433641433716\n",
            "Epoch  1 Batch  254 / 525  Training Loss  0.2817991077899933\n",
            "Epoch  1 Batch  255 / 525  Training Loss  0.2774398922920227\n",
            "Epoch  1 Batch  256 / 525  Training Loss  0.281105101108551\n",
            "Epoch  1 Batch  257 / 525  Training Loss  0.2675979435443878\n",
            "Epoch  1 Batch  258 / 525  Training Loss  0.27549511194229126\n",
            "Epoch  1 Batch  259 / 525  Training Loss  0.2688561677932739\n",
            "Epoch  1 Batch  260 / 525  Training Loss  0.29553788900375366\n",
            "Epoch  1 Batch  261 / 525  Training Loss  0.2874663472175598\n",
            "Epoch  1 Batch  262 / 525  Training Loss  0.28122201561927795\n",
            "Epoch  1 Batch  263 / 525  Training Loss  0.2720203697681427\n",
            "Epoch  1 Batch  264 / 525  Training Loss  0.2714696526527405\n",
            "Epoch  1 Batch  265 / 525  Training Loss  0.26998743414878845\n",
            "Epoch  1 Batch  266 / 525  Training Loss  0.266112744808197\n",
            "Epoch  1 Batch  267 / 525  Training Loss  0.2916993200778961\n",
            "Epoch  1 Batch  268 / 525  Training Loss  0.2667810320854187\n",
            "Epoch  1 Batch  269 / 525  Training Loss  0.26344674825668335\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1 Batch  270 / 525  Training Loss  0.2920979857444763\n",
            "Epoch  1 Batch  271 / 525  Training Loss  0.24385924637317657\n",
            "Epoch  1 Batch  272 / 525  Training Loss  0.26793572306632996\n",
            "Epoch  1 Batch  273 / 525  Training Loss  0.2646312117576599\n",
            "Epoch  1 Batch  274 / 525  Training Loss  0.2912420630455017\n",
            "Epoch  1 Batch  275 / 525  Training Loss  0.2775667607784271\n",
            "Epoch  1 Batch  276 / 525  Training Loss  0.2592415511608124\n",
            "Epoch  1 Batch  277 / 525  Training Loss  0.28720465302467346\n",
            "Epoch  1 Batch  278 / 525  Training Loss  0.2552935481071472\n",
            "Epoch  1 Batch  279 / 525  Training Loss  0.24706678092479706\n",
            "Epoch  1 Batch  280 / 525  Training Loss  0.2836511731147766\n",
            "Epoch  1 Batch  281 / 525  Training Loss  0.27410146594047546\n",
            "Epoch  1 Batch  282 / 525  Training Loss  0.27358278632164\n",
            "Epoch  1 Batch  283 / 525  Training Loss  0.2818191647529602\n",
            "Epoch  1 Batch  284 / 525  Training Loss  0.2938045263290405\n",
            "Epoch  1 Batch  285 / 525  Training Loss  0.2891625761985779\n",
            "Epoch  1 Batch  286 / 525  Training Loss  0.2711791396141052\n",
            "Epoch  1 Batch  287 / 525  Training Loss  0.27771517634391785\n",
            "Epoch  1 Batch  288 / 525  Training Loss  0.2742965817451477\n",
            "Epoch  1 Batch  289 / 525  Training Loss  0.27014487981796265\n",
            "Epoch  1 Batch  290 / 525  Training Loss  0.2671037018299103\n",
            "Epoch  1 Batch  291 / 525  Training Loss  0.2721128463745117\n",
            "Epoch  1 Batch  292 / 525  Training Loss  0.25826165080070496\n",
            "Epoch  1 Batch  293 / 525  Training Loss  0.27756842970848083\n",
            "Epoch  1 Batch  294 / 525  Training Loss  0.2788792848587036\n",
            "Epoch  1 Batch  295 / 525  Training Loss  0.23803658783435822\n",
            "Epoch  1 Batch  296 / 525  Training Loss  0.2532424032688141\n",
            "Epoch  1 Batch  297 / 525  Training Loss  0.26516062021255493\n",
            "Epoch  1 Batch  298 / 525  Training Loss  0.25971901416778564\n",
            "Epoch  1 Batch  299 / 525  Training Loss  0.2465413510799408\n",
            "Epoch  1 Batch  300 / 525  Training Loss  0.27686065435409546\n",
            "Epoch  1 Batch  301 / 525  Training Loss  0.2622390687465668\n",
            "Epoch  1 Batch  302 / 525  Training Loss  0.29143765568733215\n",
            "Epoch  1 Batch  303 / 525  Training Loss  0.2729765772819519\n",
            "Epoch  1 Batch  304 / 525  Training Loss  0.2816150188446045\n",
            "Epoch  1 Batch  305 / 525  Training Loss  0.25233784317970276\n",
            "Epoch  1 Batch  306 / 525  Training Loss  0.2656633257865906\n",
            "Epoch  1 Batch  307 / 525  Training Loss  0.27673155069351196\n",
            "Epoch  1 Batch  308 / 525  Training Loss  0.26271286606788635\n",
            "Epoch  1 Batch  309 / 525  Training Loss  0.28079333901405334\n",
            "Epoch  1 Batch  310 / 525  Training Loss  0.25971269607543945\n",
            "Epoch  1 Batch  311 / 525  Training Loss  0.28045180439949036\n",
            "Epoch  1 Batch  312 / 525  Training Loss  0.28105852007865906\n",
            "Epoch  1 Batch  313 / 525  Training Loss  0.2678438425064087\n",
            "Epoch  1 Batch  314 / 525  Training Loss  0.27203822135925293\n",
            "Epoch  1 Batch  315 / 525  Training Loss  0.2773182690143585\n",
            "Epoch  1 Batch  316 / 525  Training Loss  0.2669885456562042\n",
            "Epoch  1 Batch  317 / 525  Training Loss  0.24996384978294373\n",
            "Epoch  1 Batch  318 / 525  Training Loss  0.27911388874053955\n",
            "Epoch  1 Batch  319 / 525  Training Loss  0.282400906085968\n",
            "Epoch  1 Batch  320 / 525  Training Loss  0.30304160714149475\n",
            "Epoch  1 Batch  321 / 525  Training Loss  0.2977500557899475\n",
            "Epoch  1 Batch  322 / 525  Training Loss  0.2667457163333893\n",
            "Epoch  1 Batch  323 / 525  Training Loss  0.25423547625541687\n",
            "Epoch  1 Batch  324 / 525  Training Loss  0.2479470670223236\n",
            "Epoch  1 Batch  325 / 525  Training Loss  0.2698192298412323\n",
            "Epoch  1 Batch  326 / 525  Training Loss  0.2553606629371643\n",
            "Epoch  1 Batch  327 / 525  Training Loss  0.28435951471328735\n",
            "Epoch  1 Batch  328 / 525  Training Loss  0.2764468789100647\n",
            "Epoch  1 Batch  329 / 525  Training Loss  0.27146071195602417\n",
            "Epoch  1 Batch  330 / 525  Training Loss  0.24732732772827148\n",
            "Epoch  1 Batch  331 / 525  Training Loss  0.24975936114788055\n",
            "Epoch  1 Batch  332 / 525  Training Loss  0.2980533242225647\n",
            "Epoch  1 Batch  333 / 525  Training Loss  0.2554386854171753\n",
            "Epoch  1 Batch  334 / 525  Training Loss  0.2466932088136673\n",
            "Epoch  1 Batch  335 / 525  Training Loss  0.26663073897361755\n",
            "Epoch  1 Batch  336 / 525  Training Loss  0.2548862397670746\n",
            "Epoch  1 Batch  337 / 525  Training Loss  0.26334038376808167\n",
            "Epoch  1 Batch  338 / 525  Training Loss  0.2557413578033447\n",
            "Epoch  1 Batch  339 / 525  Training Loss  0.2518864870071411\n",
            "Epoch  1 Batch  340 / 525  Training Loss  0.2566131055355072\n",
            "Epoch  1 Batch  341 / 525  Training Loss  0.28251200914382935\n",
            "Epoch  1 Batch  342 / 525  Training Loss  0.27973079681396484\n",
            "Epoch  1 Batch  343 / 525  Training Loss  0.2456245720386505\n",
            "Epoch  1 Batch  344 / 525  Training Loss  0.2757108509540558\n",
            "Epoch  1 Batch  345 / 525  Training Loss  0.2639407217502594\n",
            "Epoch  1 Batch  346 / 525  Training Loss  0.27314648032188416\n",
            "Epoch  1 Batch  347 / 525  Training Loss  0.2490050494670868\n",
            "Epoch  1 Batch  348 / 525  Training Loss  0.299061119556427\n",
            "Epoch  1 Batch  349 / 525  Training Loss  0.27894359827041626\n",
            "Epoch  1 Batch  350 / 525  Training Loss  0.25469598174095154\n",
            "Epoch  1 Batch  351 / 525  Training Loss  0.2583688497543335\n",
            "Epoch  1 Batch  352 / 525  Training Loss  0.2600436508655548\n",
            "Epoch  1 Batch  353 / 525  Training Loss  0.25848883390426636\n",
            "Epoch  1 Batch  354 / 525  Training Loss  0.2562512159347534\n",
            "Epoch  1 Batch  355 / 525  Training Loss  0.27010005712509155\n",
            "Epoch  1 Batch  356 / 525  Training Loss  0.2815012037754059\n",
            "Epoch  1 Batch  357 / 525  Training Loss  0.2300158441066742\n",
            "Epoch  1 Batch  358 / 525  Training Loss  0.26172423362731934\n",
            "Epoch  1 Batch  359 / 525  Training Loss  0.2763448655605316\n",
            "Epoch  1 Batch  360 / 525  Training Loss  0.2557087540626526\n",
            "Epoch  1 Batch  361 / 525  Training Loss  0.2638467848300934\n",
            "Epoch  1 Batch  362 / 525  Training Loss  0.2768407464027405\n",
            "Epoch  1 Batch  363 / 525  Training Loss  0.2547144293785095\n",
            "Epoch  1 Batch  364 / 525  Training Loss  0.3027802109718323\n",
            "Epoch  1 Batch  365 / 525  Training Loss  0.24480590224266052\n",
            "Epoch  1 Batch  366 / 525  Training Loss  0.24950973689556122\n",
            "Epoch  1 Batch  367 / 525  Training Loss  0.28406456112861633\n",
            "Epoch  1 Batch  368 / 525  Training Loss  0.2710192799568176\n",
            "Epoch  1 Batch  369 / 525  Training Loss  0.2797139585018158\n",
            "Epoch  1 Batch  370 / 525  Training Loss  0.276143878698349\n",
            "Epoch  1 Batch  371 / 525  Training Loss  0.2561764121055603\n",
            "Epoch  1 Batch  372 / 525  Training Loss  0.296164333820343\n",
            "Epoch  1 Batch  373 / 525  Training Loss  0.24005334079265594\n",
            "Epoch  1 Batch  374 / 525  Training Loss  0.26758846640586853\n",
            "Epoch  1 Batch  375 / 525  Training Loss  0.27815550565719604\n",
            "Epoch  1 Batch  376 / 525  Training Loss  0.26933738589286804\n",
            "Epoch  1 Batch  377 / 525  Training Loss  0.2516780495643616\n",
            "Epoch  1 Batch  378 / 525  Training Loss  0.25508323311805725\n",
            "Epoch  1 Batch  379 / 525  Training Loss  0.24972617626190186\n",
            "Epoch  1 Batch  380 / 525  Training Loss  0.2508646845817566\n",
            "Epoch  1 Batch  381 / 525  Training Loss  0.24634592235088348\n",
            "Epoch  1 Batch  382 / 525  Training Loss  0.26042553782463074\n",
            "Epoch  1 Batch  383 / 525  Training Loss  0.29158875346183777\n",
            "Epoch  1 Batch  384 / 525  Training Loss  0.2444150745868683\n",
            "Epoch  1 Batch  385 / 525  Training Loss  0.24714557826519012\n",
            "Epoch  1 Batch  386 / 525  Training Loss  0.25671106576919556\n",
            "Epoch  1 Batch  387 / 525  Training Loss  0.25092101097106934\n",
            "Epoch  1 Batch  388 / 525  Training Loss  0.22673299908638\n",
            "Epoch  1 Batch  389 / 525  Training Loss  0.2605471909046173\n",
            "Epoch  1 Batch  390 / 525  Training Loss  0.2693025469779968\n",
            "Epoch  1 Batch  391 / 525  Training Loss  0.2794814109802246\n",
            "Epoch  1 Batch  392 / 525  Training Loss  0.2585009038448334\n",
            "Epoch  1 Batch  393 / 525  Training Loss  0.25057345628738403\n",
            "Epoch  1 Batch  394 / 525  Training Loss  0.2679336965084076\n",
            "Epoch  1 Batch  395 / 525  Training Loss  0.2689020335674286\n",
            "Epoch  1 Batch  396 / 525  Training Loss  0.25795862078666687\n",
            "Epoch  1 Batch  397 / 525  Training Loss  0.27538949251174927\n",
            "Epoch  1 Batch  398 / 525  Training Loss  0.25536227226257324\n",
            "Epoch  1 Batch  399 / 525  Training Loss  0.27127769589424133\n",
            "Epoch  1 Batch  400 / 525  Training Loss  0.2653411030769348\n",
            "Epoch  1 Batch  401 / 525  Training Loss  0.234088733792305\n",
            "Epoch  1 Batch  402 / 525  Training Loss  0.24003854393959045\n",
            "Epoch  1 Batch  403 / 525  Training Loss  0.2590673565864563\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  1 Batch  404 / 525  Training Loss  0.24410276114940643\n",
            "Epoch  1 Batch  405 / 525  Training Loss  0.2533096671104431\n",
            "Epoch  1 Batch  406 / 525  Training Loss  0.2665836811065674\n",
            "Epoch  1 Batch  407 / 525  Training Loss  0.241398423910141\n",
            "Epoch  1 Batch  408 / 525  Training Loss  0.25727248191833496\n",
            "Epoch  1 Batch  409 / 525  Training Loss  0.25586479902267456\n",
            "Epoch  1 Batch  410 / 525  Training Loss  0.2836964726448059\n",
            "Epoch  1 Batch  411 / 525  Training Loss  0.26946505904197693\n",
            "Epoch  1 Batch  412 / 525  Training Loss  0.26142990589141846\n",
            "Epoch  1 Batch  413 / 525  Training Loss  0.23782499134540558\n",
            "Epoch  1 Batch  414 / 525  Training Loss  0.2433195561170578\n",
            "Epoch  1 Batch  415 / 525  Training Loss  0.2625615894794464\n",
            "Epoch  1 Batch  416 / 525  Training Loss  0.26705995202064514\n",
            "Epoch  1 Batch  417 / 525  Training Loss  0.22451825439929962\n",
            "Epoch  1 Batch  418 / 525  Training Loss  0.25361281633377075\n",
            "Epoch  1 Batch  419 / 525  Training Loss  0.2448377162218094\n",
            "Epoch  1 Batch  420 / 525  Training Loss  0.25528082251548767\n",
            "Epoch  1 Batch  421 / 525  Training Loss  0.256175696849823\n",
            "Epoch  1 Batch  422 / 525  Training Loss  0.2873747646808624\n",
            "Epoch  1 Batch  423 / 525  Training Loss  0.24071133136749268\n",
            "Epoch  1 Batch  424 / 525  Training Loss  0.26117172837257385\n",
            "Epoch  1 Batch  425 / 525  Training Loss  0.2368641346693039\n",
            "Epoch  1 Batch  426 / 525  Training Loss  0.2460629642009735\n",
            "Epoch  1 Batch  427 / 525  Training Loss  0.24667903780937195\n",
            "Epoch  1 Batch  428 / 525  Training Loss  0.2737719416618347\n",
            "Epoch  1 Batch  429 / 525  Training Loss  0.2443964183330536\n",
            "Epoch  1 Batch  430 / 525  Training Loss  0.26391464471817017\n",
            "Epoch  1 Batch  431 / 525  Training Loss  0.24607479572296143\n",
            "Epoch  1 Batch  432 / 525  Training Loss  0.2801748514175415\n",
            "Epoch  1 Batch  433 / 525  Training Loss  0.24852386116981506\n",
            "Epoch  1 Batch  434 / 525  Training Loss  0.2668958604335785\n",
            "Epoch  1 Batch  435 / 525  Training Loss  0.2678186893463135\n",
            "Epoch  1 Batch  436 / 525  Training Loss  0.2709241509437561\n",
            "Epoch  1 Batch  437 / 525  Training Loss  0.2540588080883026\n",
            "Epoch  1 Batch  438 / 525  Training Loss  0.24512982368469238\n",
            "Epoch  1 Batch  439 / 525  Training Loss  0.22875452041625977\n",
            "Epoch  1 Batch  440 / 525  Training Loss  0.26251059770584106\n",
            "Epoch  1 Batch  441 / 525  Training Loss  0.2934354543685913\n",
            "Epoch  1 Batch  442 / 525  Training Loss  0.24356791377067566\n",
            "Epoch  1 Batch  443 / 525  Training Loss  0.24811533093452454\n",
            "Epoch  1 Batch  444 / 525  Training Loss  0.25077271461486816\n",
            "Epoch  1 Batch  445 / 525  Training Loss  0.275209903717041\n",
            "Epoch  1 Batch  446 / 525  Training Loss  0.2463698387145996\n",
            "Epoch  1 Batch  447 / 525  Training Loss  0.25692498683929443\n",
            "Epoch  1 Batch  448 / 525  Training Loss  0.2779868245124817\n",
            "Epoch  1 Batch  449 / 525  Training Loss  0.24777428805828094\n",
            "Epoch  1 Batch  450 / 525  Training Loss  0.2252470701932907\n",
            "Epoch  1 Batch  451 / 525  Training Loss  0.24604812264442444\n",
            "Epoch  1 Batch  452 / 525  Training Loss  0.24052157998085022\n",
            "Epoch  1 Batch  453 / 525  Training Loss  0.24503569304943085\n",
            "Epoch  1 Batch  454 / 525  Training Loss  0.23891031742095947\n",
            "Epoch  1 Batch  455 / 525  Training Loss  0.2484547346830368\n",
            "Epoch  1 Batch  456 / 525  Training Loss  0.23119309544563293\n",
            "Epoch  1 Batch  457 / 525  Training Loss  0.24830317497253418\n",
            "Epoch  1 Batch  458 / 525  Training Loss  0.2496425360441208\n",
            "Epoch  1 Batch  459 / 525  Training Loss  0.26229384541511536\n",
            "Epoch  1 Batch  460 / 525  Training Loss  0.273140013217926\n",
            "Epoch  1 Batch  461 / 525  Training Loss  0.25337427854537964\n",
            "Epoch  1 Batch  462 / 525  Training Loss  0.2681472897529602\n",
            "Epoch  1 Batch  463 / 525  Training Loss  0.262856662273407\n",
            "Epoch  1 Batch  464 / 525  Training Loss  0.25017422437667847\n",
            "Epoch  1 Batch  465 / 525  Training Loss  0.2495482712984085\n",
            "Epoch  1 Batch  466 / 525  Training Loss  0.23355457186698914\n",
            "Epoch  1 Batch  467 / 525  Training Loss  0.22870469093322754\n",
            "Epoch  1 Batch  468 / 525  Training Loss  0.24825724959373474\n",
            "Epoch  1 Batch  469 / 525  Training Loss  0.23702359199523926\n",
            "Epoch  1 Batch  470 / 525  Training Loss  0.256704181432724\n",
            "Epoch  1 Batch  471 / 525  Training Loss  0.2718504071235657\n",
            "Epoch  1 Batch  472 / 525  Training Loss  0.2661987841129303\n",
            "Epoch  1 Batch  473 / 525  Training Loss  0.260886013507843\n",
            "Epoch  1 Batch  474 / 525  Training Loss  0.2310984879732132\n",
            "Epoch  1 Batch  475 / 525  Training Loss  0.260142982006073\n",
            "Epoch  1 Batch  476 / 525  Training Loss  0.21667948365211487\n",
            "Epoch  1 Batch  477 / 525  Training Loss  0.2633264660835266\n",
            "Epoch  1 Batch  478 / 525  Training Loss  0.23154227435588837\n",
            "Epoch  1 Batch  479 / 525  Training Loss  0.254075288772583\n",
            "Epoch  1 Batch  480 / 525  Training Loss  0.2608644366264343\n",
            "Epoch  1 Batch  481 / 525  Training Loss  0.27377161383628845\n",
            "Epoch  1 Batch  482 / 525  Training Loss  0.24697542190551758\n",
            "Epoch  1 Batch  483 / 525  Training Loss  0.26646295189857483\n",
            "Epoch  1 Batch  484 / 525  Training Loss  0.24847085773944855\n",
            "Epoch  1 Batch  485 / 525  Training Loss  0.24849839508533478\n",
            "Epoch  1 Batch  486 / 525  Training Loss  0.2502589821815491\n",
            "Epoch  1 Batch  487 / 525  Training Loss  0.2351454198360443\n",
            "Epoch  1 Batch  488 / 525  Training Loss  0.2586573362350464\n",
            "Epoch  1 Batch  489 / 525  Training Loss  0.24319970607757568\n",
            "Epoch  1 Batch  490 / 525  Training Loss  0.25678735971450806\n",
            "Epoch  1 Batch  491 / 525  Training Loss  0.25155049562454224\n",
            "Epoch  1 Batch  492 / 525  Training Loss  0.2565525770187378\n",
            "Epoch  1 Batch  493 / 525  Training Loss  0.28551483154296875\n",
            "Epoch  1 Batch  494 / 525  Training Loss  0.2308441400527954\n",
            "Epoch  1 Batch  495 / 525  Training Loss  0.24041247367858887\n",
            "Epoch  1 Batch  496 / 525  Training Loss  0.2517164647579193\n",
            "Epoch  1 Batch  497 / 525  Training Loss  0.26880592107772827\n",
            "Epoch  1 Batch  498 / 525  Training Loss  0.2516254186630249\n",
            "Epoch  1 Batch  499 / 525  Training Loss  0.258791983127594\n",
            "Epoch  1 Batch  500 / 525  Training Loss  0.2387508898973465\n",
            "Epoch  1 Batch  501 / 525  Training Loss  0.24481062591075897\n",
            "Epoch  1 Batch  502 / 525  Training Loss  0.24956949055194855\n",
            "Epoch  1 Batch  503 / 525  Training Loss  0.2776948809623718\n",
            "Epoch  1 Batch  504 / 525  Training Loss  0.24143218994140625\n",
            "Epoch  1 Batch  505 / 525  Training Loss  0.24078646302223206\n",
            "Epoch  1 Batch  506 / 525  Training Loss  0.2531755566596985\n",
            "Epoch  1 Batch  507 / 525  Training Loss  0.24743318557739258\n",
            "Epoch  1 Batch  508 / 525  Training Loss  0.2631572186946869\n",
            "Epoch  1 Batch  509 / 525  Training Loss  0.2471558302640915\n",
            "Epoch  1 Batch  510 / 525  Training Loss  0.23884379863739014\n",
            "Epoch  1 Batch  511 / 525  Training Loss  0.26638999581336975\n",
            "Epoch  1 Batch  512 / 525  Training Loss  0.24509215354919434\n",
            "Epoch  1 Batch  513 / 525  Training Loss  0.25054359436035156\n",
            "Epoch  1 Batch  514 / 525  Training Loss  0.23051956295967102\n",
            "Epoch  1 Batch  515 / 525  Training Loss  0.25747790932655334\n",
            "Epoch  1 Batch  516 / 525  Training Loss  0.23897850513458252\n",
            "Epoch  1 Batch  517 / 525  Training Loss  0.2636646330356598\n",
            "Epoch  1 Batch  518 / 525  Training Loss  0.24817518889904022\n",
            "Epoch  1 Batch  519 / 525  Training Loss  0.2596178650856018\n",
            "Epoch  1 Batch  520 / 525  Training Loss  0.25240084528923035\n",
            "Epoch  1 Batch  521 / 525  Training Loss  0.24037210643291473\n",
            "Epoch  1 Batch  522 / 525  Training Loss  0.24482285976409912\n",
            "Epoch  1 Batch  523 / 525  Training Loss  0.25292375683784485\n",
            "Epoch  1 Batch  524 / 525  Training Loss  0.2561146914958954\n",
            "   2    |    -    |   0.302144   |   5.05   \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 2\n",
            "Epoch  2 Batch  0 / 525  Training Loss  0.18082228302955627\n",
            "Epoch  2 Batch  1 / 525  Training Loss  0.21662326157093048\n",
            "Epoch  2 Batch  2 / 525  Training Loss  0.20850256085395813\n",
            "Epoch  2 Batch  3 / 525  Training Loss  0.20294365286827087\n",
            "Epoch  2 Batch  4 / 525  Training Loss  0.2251131534576416\n",
            "Epoch  2 Batch  5 / 525  Training Loss  0.1988411545753479\n",
            "Epoch  2 Batch  6 / 525  Training Loss  0.20803818106651306\n",
            "Epoch  2 Batch  7 / 525  Training Loss  0.19371218979358673\n",
            "Epoch  2 Batch  8 / 525  Training Loss  0.2024502456188202\n",
            "Epoch  2 Batch  9 / 525  Training Loss  0.20217137038707733\n",
            "Epoch  2 Batch  10 / 525  Training Loss  0.19379355013370514\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  2 Batch  11 / 525  Training Loss  0.20787546038627625\n",
            "Epoch  2 Batch  12 / 525  Training Loss  0.18649932742118835\n",
            "Epoch  2 Batch  13 / 525  Training Loss  0.20963601768016815\n",
            "Epoch  2 Batch  14 / 525  Training Loss  0.2054549902677536\n",
            "Epoch  2 Batch  15 / 525  Training Loss  0.21061749756336212\n",
            "Epoch  2 Batch  16 / 525  Training Loss  0.19125398993492126\n",
            "Epoch  2 Batch  17 / 525  Training Loss  0.19148819148540497\n",
            "Epoch  2 Batch  18 / 525  Training Loss  0.17958387732505798\n",
            "Epoch  2 Batch  19 / 525  Training Loss  0.19916266202926636\n",
            "Epoch  2 Batch  20 / 525  Training Loss  0.19189658761024475\n",
            "Epoch  2 Batch  21 / 525  Training Loss  0.19384440779685974\n",
            "Epoch  2 Batch  22 / 525  Training Loss  0.2044832408428192\n",
            "Epoch  2 Batch  23 / 525  Training Loss  0.1768709421157837\n",
            "Epoch  2 Batch  24 / 525  Training Loss  0.19536536931991577\n",
            "Epoch  2 Batch  25 / 525  Training Loss  0.1952991783618927\n",
            "Epoch  2 Batch  26 / 525  Training Loss  0.2051517516374588\n",
            "Epoch  2 Batch  27 / 525  Training Loss  0.20303216576576233\n",
            "Epoch  2 Batch  28 / 525  Training Loss  0.21326284110546112\n",
            "Epoch  2 Batch  29 / 525  Training Loss  0.20951147377490997\n",
            "Epoch  2 Batch  30 / 525  Training Loss  0.1853976547718048\n",
            "Epoch  2 Batch  31 / 525  Training Loss  0.20308899879455566\n",
            "Epoch  2 Batch  32 / 525  Training Loss  0.20689448714256287\n",
            "Epoch  2 Batch  33 / 525  Training Loss  0.19066691398620605\n",
            "Epoch  2 Batch  34 / 525  Training Loss  0.1966155618429184\n",
            "Epoch  2 Batch  35 / 525  Training Loss  0.19244752824306488\n",
            "Epoch  2 Batch  36 / 525  Training Loss  0.1901772916316986\n",
            "Epoch  2 Batch  37 / 525  Training Loss  0.2013441026210785\n",
            "Epoch  2 Batch  38 / 525  Training Loss  0.19222953915596008\n",
            "Epoch  2 Batch  39 / 525  Training Loss  0.2124365121126175\n",
            "Epoch  2 Batch  40 / 525  Training Loss  0.18224342167377472\n",
            "Epoch  2 Batch  41 / 525  Training Loss  0.2039937674999237\n",
            "Epoch  2 Batch  42 / 525  Training Loss  0.18582947552204132\n",
            "Epoch  2 Batch  43 / 525  Training Loss  0.1969902217388153\n",
            "Epoch  2 Batch  44 / 525  Training Loss  0.18798264861106873\n",
            "Epoch  2 Batch  45 / 525  Training Loss  0.19610907137393951\n",
            "Epoch  2 Batch  46 / 525  Training Loss  0.1868530660867691\n",
            "Epoch  2 Batch  47 / 525  Training Loss  0.19143398106098175\n",
            "Epoch  2 Batch  48 / 525  Training Loss  0.18634852766990662\n",
            "Epoch  2 Batch  49 / 525  Training Loss  0.1965717077255249\n",
            "Epoch  2 Batch  50 / 525  Training Loss  0.19807784259319305\n",
            "Epoch  2 Batch  51 / 525  Training Loss  0.1998596489429474\n",
            "Epoch  2 Batch  52 / 525  Training Loss  0.1615806519985199\n",
            "Epoch  2 Batch  53 / 525  Training Loss  0.18813320994377136\n",
            "Epoch  2 Batch  54 / 525  Training Loss  0.18597066402435303\n",
            "Epoch  2 Batch  55 / 525  Training Loss  0.18802452087402344\n",
            "Epoch  2 Batch  56 / 525  Training Loss  0.18401846289634705\n",
            "Epoch  2 Batch  57 / 525  Training Loss  0.1912630945444107\n",
            "Epoch  2 Batch  58 / 525  Training Loss  0.1939045488834381\n",
            "Epoch  2 Batch  59 / 525  Training Loss  0.200354665517807\n",
            "Epoch  2 Batch  60 / 525  Training Loss  0.20172400772571564\n",
            "Epoch  2 Batch  61 / 525  Training Loss  0.2111361026763916\n",
            "Epoch  2 Batch  62 / 525  Training Loss  0.1873578578233719\n",
            "Epoch  2 Batch  63 / 525  Training Loss  0.1994432508945465\n",
            "Epoch  2 Batch  64 / 525  Training Loss  0.1952732503414154\n",
            "Epoch  2 Batch  65 / 525  Training Loss  0.20184679329395294\n",
            "Epoch  2 Batch  66 / 525  Training Loss  0.18960002064704895\n",
            "Epoch  2 Batch  67 / 525  Training Loss  0.18279656767845154\n",
            "Epoch  2 Batch  68 / 525  Training Loss  0.18365058302879333\n",
            "Epoch  2 Batch  69 / 525  Training Loss  0.19821257889270782\n",
            "Epoch  2 Batch  70 / 525  Training Loss  0.17225401103496552\n",
            "Epoch  2 Batch  71 / 525  Training Loss  0.1772182434797287\n",
            "Epoch  2 Batch  72 / 525  Training Loss  0.20392227172851562\n",
            "Epoch  2 Batch  73 / 525  Training Loss  0.16835884749889374\n",
            "Epoch  2 Batch  74 / 525  Training Loss  0.16629134118556976\n",
            "Epoch  2 Batch  75 / 525  Training Loss  0.20222067832946777\n",
            "Epoch  2 Batch  76 / 525  Training Loss  0.1716327965259552\n",
            "Epoch  2 Batch  77 / 525  Training Loss  0.1993149369955063\n",
            "Epoch  2 Batch  78 / 525  Training Loss  0.17439308762550354\n",
            "Epoch  2 Batch  79 / 525  Training Loss  0.19502957165241241\n",
            "Epoch  2 Batch  80 / 525  Training Loss  0.20845218002796173\n",
            "Epoch  2 Batch  81 / 525  Training Loss  0.18984197080135345\n",
            "Epoch  2 Batch  82 / 525  Training Loss  0.1856280267238617\n",
            "Epoch  2 Batch  83 / 525  Training Loss  0.18010953068733215\n",
            "Epoch  2 Batch  84 / 525  Training Loss  0.1935284435749054\n",
            "Epoch  2 Batch  85 / 525  Training Loss  0.20451140403747559\n",
            "Epoch  2 Batch  86 / 525  Training Loss  0.1719183623790741\n",
            "Epoch  2 Batch  87 / 525  Training Loss  0.19034388661384583\n",
            "Epoch  2 Batch  88 / 525  Training Loss  0.17517492175102234\n",
            "Epoch  2 Batch  89 / 525  Training Loss  0.16115263104438782\n",
            "Epoch  2 Batch  90 / 525  Training Loss  0.19404330849647522\n",
            "Epoch  2 Batch  91 / 525  Training Loss  0.17258578538894653\n",
            "Epoch  2 Batch  92 / 525  Training Loss  0.18207351863384247\n",
            "Epoch  2 Batch  93 / 525  Training Loss  0.18145689368247986\n",
            "Epoch  2 Batch  94 / 525  Training Loss  0.1885453760623932\n",
            "Epoch  2 Batch  95 / 525  Training Loss  0.18508818745613098\n",
            "Epoch  2 Batch  96 / 525  Training Loss  0.1921963393688202\n",
            "Epoch  2 Batch  97 / 525  Training Loss  0.17629803717136383\n",
            "Epoch  2 Batch  98 / 525  Training Loss  0.17249223589897156\n",
            "Epoch  2 Batch  99 / 525  Training Loss  0.18303117156028748\n",
            "Epoch  2 Batch  100 / 525  Training Loss  0.16585996747016907\n",
            "Epoch  2 Batch  101 / 525  Training Loss  0.17960934340953827\n",
            "Epoch  2 Batch  102 / 525  Training Loss  0.17894574999809265\n",
            "Epoch  2 Batch  103 / 525  Training Loss  0.19096437096595764\n",
            "Epoch  2 Batch  104 / 525  Training Loss  0.20996765792369843\n",
            "Epoch  2 Batch  105 / 525  Training Loss  0.19170013070106506\n",
            "Epoch  2 Batch  106 / 525  Training Loss  0.1694948673248291\n",
            "Epoch  2 Batch  107 / 525  Training Loss  0.1978360116481781\n",
            "Epoch  2 Batch  108 / 525  Training Loss  0.16428251564502716\n",
            "Epoch  2 Batch  109 / 525  Training Loss  0.16919498145580292\n",
            "Epoch  2 Batch  110 / 525  Training Loss  0.1713045984506607\n",
            "Epoch  2 Batch  111 / 525  Training Loss  0.18059109151363373\n",
            "Epoch  2 Batch  112 / 525  Training Loss  0.1941324919462204\n",
            "Epoch  2 Batch  113 / 525  Training Loss  0.19387972354888916\n",
            "Epoch  2 Batch  114 / 525  Training Loss  0.20549234747886658\n",
            "Epoch  2 Batch  115 / 525  Training Loss  0.16661514341831207\n",
            "Epoch  2 Batch  116 / 525  Training Loss  0.16440121829509735\n",
            "Epoch  2 Batch  117 / 525  Training Loss  0.19601798057556152\n",
            "Epoch  2 Batch  118 / 525  Training Loss  0.167485311627388\n",
            "Epoch  2 Batch  119 / 525  Training Loss  0.17683528363704681\n",
            "Epoch  2 Batch  120 / 525  Training Loss  0.1737179309129715\n",
            "Epoch  2 Batch  121 / 525  Training Loss  0.17594635486602783\n",
            "Epoch  2 Batch  122 / 525  Training Loss  0.18090495467185974\n",
            "Epoch  2 Batch  123 / 525  Training Loss  0.1635783463716507\n",
            "Epoch  2 Batch  124 / 525  Training Loss  0.16557471454143524\n",
            "Epoch  2 Batch  125 / 525  Training Loss  0.19164803624153137\n",
            "Epoch  2 Batch  126 / 525  Training Loss  0.20482809841632843\n",
            "Epoch  2 Batch  127 / 525  Training Loss  0.18372401595115662\n",
            "Epoch  2 Batch  128 / 525  Training Loss  0.17801877856254578\n",
            "Epoch  2 Batch  129 / 525  Training Loss  0.19873884320259094\n",
            "Epoch  2 Batch  130 / 525  Training Loss  0.21563200652599335\n",
            "Epoch  2 Batch  131 / 525  Training Loss  0.16505379974842072\n",
            "Epoch  2 Batch  132 / 525  Training Loss  0.17936262488365173\n",
            "Epoch  2 Batch  133 / 525  Training Loss  0.19771547615528107\n",
            "Epoch  2 Batch  134 / 525  Training Loss  0.19718563556671143\n",
            "Epoch  2 Batch  135 / 525  Training Loss  0.17148920893669128\n",
            "Epoch  2 Batch  136 / 525  Training Loss  0.17254194617271423\n",
            "Epoch  2 Batch  137 / 525  Training Loss  0.18682727217674255\n",
            "Epoch  2 Batch  138 / 525  Training Loss  0.17171286046504974\n",
            "Epoch  2 Batch  139 / 525  Training Loss  0.1752137541770935\n",
            "Epoch  2 Batch  140 / 525  Training Loss  0.20443129539489746\n",
            "Epoch  2 Batch  141 / 525  Training Loss  0.17702409625053406\n",
            "Epoch  2 Batch  142 / 525  Training Loss  0.1826883852481842\n",
            "Epoch  2 Batch  143 / 525  Training Loss  0.1808648556470871\n",
            "Epoch  2 Batch  144 / 525  Training Loss  0.1786428987979889\n",
            "Epoch  2 Batch  145 / 525  Training Loss  0.1871197372674942\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  2 Batch  146 / 525  Training Loss  0.1694449484348297\n",
            "Epoch  2 Batch  147 / 525  Training Loss  0.17715156078338623\n",
            "Epoch  2 Batch  148 / 525  Training Loss  0.1825440675020218\n",
            "Epoch  2 Batch  149 / 525  Training Loss  0.16919897496700287\n",
            "Epoch  2 Batch  150 / 525  Training Loss  0.18085059523582458\n",
            "Epoch  2 Batch  151 / 525  Training Loss  0.1727488934993744\n",
            "Epoch  2 Batch  152 / 525  Training Loss  0.17518553137779236\n",
            "Epoch  2 Batch  153 / 525  Training Loss  0.18902727961540222\n",
            "Epoch  2 Batch  154 / 525  Training Loss  0.1762380450963974\n",
            "Epoch  2 Batch  155 / 525  Training Loss  0.16712026298046112\n",
            "Epoch  2 Batch  156 / 525  Training Loss  0.1930089145898819\n",
            "Epoch  2 Batch  157 / 525  Training Loss  0.19482944905757904\n",
            "Epoch  2 Batch  158 / 525  Training Loss  0.16257916390895844\n",
            "Epoch  2 Batch  159 / 525  Training Loss  0.18926545977592468\n",
            "Epoch  2 Batch  160 / 525  Training Loss  0.18335403501987457\n",
            "Epoch  2 Batch  161 / 525  Training Loss  0.1730390191078186\n",
            "Epoch  2 Batch  162 / 525  Training Loss  0.18037782609462738\n",
            "Epoch  2 Batch  163 / 525  Training Loss  0.18455538153648376\n",
            "Epoch  2 Batch  164 / 525  Training Loss  0.18314892053604126\n",
            "Epoch  2 Batch  165 / 525  Training Loss  0.1844879537820816\n",
            "Epoch  2 Batch  166 / 525  Training Loss  0.19011889398097992\n",
            "Epoch  2 Batch  167 / 525  Training Loss  0.201594740152359\n",
            "Epoch  2 Batch  168 / 525  Training Loss  0.18752804398536682\n",
            "Epoch  2 Batch  169 / 525  Training Loss  0.18724684417247772\n",
            "Epoch  2 Batch  170 / 525  Training Loss  0.1769750565290451\n",
            "Epoch  2 Batch  171 / 525  Training Loss  0.17857956886291504\n",
            "Epoch  2 Batch  172 / 525  Training Loss  0.19023172557353973\n",
            "Epoch  2 Batch  173 / 525  Training Loss  0.1763925999403\n",
            "Epoch  2 Batch  174 / 525  Training Loss  0.1919625848531723\n",
            "Epoch  2 Batch  175 / 525  Training Loss  0.19991309940814972\n",
            "Epoch  2 Batch  176 / 525  Training Loss  0.1656150370836258\n",
            "Epoch  2 Batch  177 / 525  Training Loss  0.17579683661460876\n",
            "Epoch  2 Batch  178 / 525  Training Loss  0.1837344914674759\n",
            "Epoch  2 Batch  179 / 525  Training Loss  0.18775004148483276\n",
            "Epoch  2 Batch  180 / 525  Training Loss  0.16581609845161438\n",
            "Epoch  2 Batch  181 / 525  Training Loss  0.18216654658317566\n",
            "Epoch  2 Batch  182 / 525  Training Loss  0.1802108734846115\n",
            "Epoch  2 Batch  183 / 525  Training Loss  0.15975356101989746\n",
            "Epoch  2 Batch  184 / 525  Training Loss  0.16065974533557892\n",
            "Epoch  2 Batch  185 / 525  Training Loss  0.18310901522636414\n",
            "Epoch  2 Batch  186 / 525  Training Loss  0.17368291318416595\n",
            "Epoch  2 Batch  187 / 525  Training Loss  0.1670342981815338\n",
            "Epoch  2 Batch  188 / 525  Training Loss  0.21710729598999023\n",
            "Epoch  2 Batch  189 / 525  Training Loss  0.1749754250049591\n",
            "Epoch  2 Batch  190 / 525  Training Loss  0.17911531031131744\n",
            "Epoch  2 Batch  191 / 525  Training Loss  0.17078624665737152\n",
            "Epoch  2 Batch  192 / 525  Training Loss  0.16076473891735077\n",
            "Epoch  2 Batch  193 / 525  Training Loss  0.16962508857250214\n",
            "Epoch  2 Batch  194 / 525  Training Loss  0.200720876455307\n",
            "Epoch  2 Batch  195 / 525  Training Loss  0.16614221036434174\n",
            "Epoch  2 Batch  196 / 525  Training Loss  0.1743128001689911\n",
            "Epoch  2 Batch  197 / 525  Training Loss  0.1700732409954071\n",
            "Epoch  2 Batch  198 / 525  Training Loss  0.1571243405342102\n",
            "Epoch  2 Batch  199 / 525  Training Loss  0.1811133325099945\n",
            "Epoch  2 Batch  200 / 525  Training Loss  0.16819895803928375\n",
            "Epoch  2 Batch  201 / 525  Training Loss  0.16265393793582916\n",
            "Epoch  2 Batch  202 / 525  Training Loss  0.16212719678878784\n",
            "Epoch  2 Batch  203 / 525  Training Loss  0.16427691280841827\n",
            "Epoch  2 Batch  204 / 525  Training Loss  0.16752073168754578\n",
            "Epoch  2 Batch  205 / 525  Training Loss  0.17728999257087708\n",
            "Epoch  2 Batch  206 / 525  Training Loss  0.17146672308444977\n",
            "Epoch  2 Batch  207 / 525  Training Loss  0.19508163630962372\n",
            "Epoch  2 Batch  208 / 525  Training Loss  0.1841663271188736\n",
            "Epoch  2 Batch  209 / 525  Training Loss  0.16764777898788452\n",
            "Epoch  2 Batch  210 / 525  Training Loss  0.17479708790779114\n",
            "Epoch  2 Batch  211 / 525  Training Loss  0.18053802847862244\n",
            "Epoch  2 Batch  212 / 525  Training Loss  0.185962975025177\n",
            "Epoch  2 Batch  213 / 525  Training Loss  0.1617259532213211\n",
            "Epoch  2 Batch  214 / 525  Training Loss  0.17997106909751892\n",
            "Epoch  2 Batch  215 / 525  Training Loss  0.17938359081745148\n",
            "Epoch  2 Batch  216 / 525  Training Loss  0.15429279208183289\n",
            "Epoch  2 Batch  217 / 525  Training Loss  0.17995937168598175\n",
            "Epoch  2 Batch  218 / 525  Training Loss  0.16767802834510803\n",
            "Epoch  2 Batch  219 / 525  Training Loss  0.1788817197084427\n",
            "Epoch  2 Batch  220 / 525  Training Loss  0.15789589285850525\n",
            "Epoch  2 Batch  221 / 525  Training Loss  0.16175314784049988\n",
            "Epoch  2 Batch  222 / 525  Training Loss  0.17214801907539368\n",
            "Epoch  2 Batch  223 / 525  Training Loss  0.16799017786979675\n",
            "Epoch  2 Batch  224 / 525  Training Loss  0.20652203261852264\n",
            "Epoch  2 Batch  225 / 525  Training Loss  0.179536834359169\n",
            "Epoch  2 Batch  226 / 525  Training Loss  0.17704859375953674\n",
            "Epoch  2 Batch  227 / 525  Training Loss  0.17735090851783752\n",
            "Epoch  2 Batch  228 / 525  Training Loss  0.1672450602054596\n",
            "Epoch  2 Batch  229 / 525  Training Loss  0.15963289141654968\n",
            "Epoch  2 Batch  230 / 525  Training Loss  0.20273089408874512\n",
            "Epoch  2 Batch  231 / 525  Training Loss  0.1656607836484909\n",
            "Epoch  2 Batch  232 / 525  Training Loss  0.18639305233955383\n",
            "Epoch  2 Batch  233 / 525  Training Loss  0.16361437737941742\n",
            "Epoch  2 Batch  234 / 525  Training Loss  0.1525745838880539\n",
            "Epoch  2 Batch  235 / 525  Training Loss  0.15475599467754364\n",
            "Epoch  2 Batch  236 / 525  Training Loss  0.1617797315120697\n",
            "Epoch  2 Batch  237 / 525  Training Loss  0.18178784847259521\n",
            "Epoch  2 Batch  238 / 525  Training Loss  0.14145055413246155\n",
            "Epoch  2 Batch  239 / 525  Training Loss  0.18015354871749878\n",
            "Epoch  2 Batch  240 / 525  Training Loss  0.1902695596218109\n",
            "Epoch  2 Batch  241 / 525  Training Loss  0.1648603081703186\n",
            "Epoch  2 Batch  242 / 525  Training Loss  0.16812525689601898\n",
            "Epoch  2 Batch  243 / 525  Training Loss  0.18048371374607086\n",
            "Epoch  2 Batch  244 / 525  Training Loss  0.17358314990997314\n",
            "Epoch  2 Batch  245 / 525  Training Loss  0.15445885062217712\n",
            "Epoch  2 Batch  246 / 525  Training Loss  0.15811040997505188\n",
            "Epoch  2 Batch  247 / 525  Training Loss  0.16139134764671326\n",
            "Epoch  2 Batch  248 / 525  Training Loss  0.17125582695007324\n",
            "Epoch  2 Batch  249 / 525  Training Loss  0.1689704805612564\n",
            "Epoch  2 Batch  250 / 525  Training Loss  0.1737956553697586\n",
            "Epoch  2 Batch  251 / 525  Training Loss  0.17747510969638824\n",
            "Epoch  2 Batch  252 / 525  Training Loss  0.18182753026485443\n",
            "Epoch  2 Batch  253 / 525  Training Loss  0.15417571365833282\n",
            "Epoch  2 Batch  254 / 525  Training Loss  0.18177030980587006\n",
            "Epoch  2 Batch  255 / 525  Training Loss  0.18841682374477386\n",
            "Epoch  2 Batch  256 / 525  Training Loss  0.16811083257198334\n",
            "Epoch  2 Batch  257 / 525  Training Loss  0.15676426887512207\n",
            "Epoch  2 Batch  258 / 525  Training Loss  0.16556426882743835\n",
            "Epoch  2 Batch  259 / 525  Training Loss  0.1911429911851883\n",
            "Epoch  2 Batch  260 / 525  Training Loss  0.15980468690395355\n",
            "Epoch  2 Batch  261 / 525  Training Loss  0.1707208752632141\n",
            "Epoch  2 Batch  262 / 525  Training Loss  0.1604584902524948\n",
            "Epoch  2 Batch  263 / 525  Training Loss  0.1622287631034851\n",
            "Epoch  2 Batch  264 / 525  Training Loss  0.151482492685318\n",
            "Epoch  2 Batch  265 / 525  Training Loss  0.17510227859020233\n",
            "Epoch  2 Batch  266 / 525  Training Loss  0.17837680876255035\n",
            "Epoch  2 Batch  267 / 525  Training Loss  0.18180124461650848\n",
            "Epoch  2 Batch  268 / 525  Training Loss  0.173223078250885\n",
            "Epoch  2 Batch  269 / 525  Training Loss  0.1673729568719864\n",
            "Epoch  2 Batch  270 / 525  Training Loss  0.16079309582710266\n",
            "Epoch  2 Batch  271 / 525  Training Loss  0.18015757203102112\n",
            "Epoch  2 Batch  272 / 525  Training Loss  0.13453692197799683\n",
            "Epoch  2 Batch  273 / 525  Training Loss  0.16678157448768616\n",
            "Epoch  2 Batch  274 / 525  Training Loss  0.18420253694057465\n",
            "Epoch  2 Batch  275 / 525  Training Loss  0.1594569832086563\n",
            "Epoch  2 Batch  276 / 525  Training Loss  0.17215348780155182\n",
            "Epoch  2 Batch  277 / 525  Training Loss  0.1596876084804535\n",
            "Epoch  2 Batch  278 / 525  Training Loss  0.15600284934043884\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  2 Batch  279 / 525  Training Loss  0.17702896893024445\n",
            "Epoch  2 Batch  280 / 525  Training Loss  0.16769373416900635\n",
            "Epoch  2 Batch  281 / 525  Training Loss  0.16081809997558594\n",
            "Epoch  2 Batch  282 / 525  Training Loss  0.16795091331005096\n",
            "Epoch  2 Batch  283 / 525  Training Loss  0.160251185297966\n",
            "Epoch  2 Batch  284 / 525  Training Loss  0.17294511198997498\n",
            "Epoch  2 Batch  285 / 525  Training Loss  0.17540836334228516\n",
            "Epoch  2 Batch  286 / 525  Training Loss  0.15933959186077118\n",
            "Epoch  2 Batch  287 / 525  Training Loss  0.14977219700813293\n",
            "Epoch  2 Batch  288 / 525  Training Loss  0.17512404918670654\n",
            "Epoch  2 Batch  289 / 525  Training Loss  0.16640092432498932\n",
            "Epoch  2 Batch  290 / 525  Training Loss  0.14667674899101257\n",
            "Epoch  2 Batch  291 / 525  Training Loss  0.15851256251335144\n",
            "Epoch  2 Batch  292 / 525  Training Loss  0.16640672087669373\n",
            "Epoch  2 Batch  293 / 525  Training Loss  0.16296608746051788\n",
            "Epoch  2 Batch  294 / 525  Training Loss  0.17825379967689514\n",
            "Epoch  2 Batch  295 / 525  Training Loss  0.17375408113002777\n",
            "Epoch  2 Batch  296 / 525  Training Loss  0.1674298495054245\n",
            "Epoch  2 Batch  297 / 525  Training Loss  0.14290812611579895\n",
            "Epoch  2 Batch  298 / 525  Training Loss  0.1907753348350525\n",
            "Epoch  2 Batch  299 / 525  Training Loss  0.17075003683567047\n",
            "Epoch  2 Batch  300 / 525  Training Loss  0.1765131950378418\n",
            "Epoch  2 Batch  301 / 525  Training Loss  0.16909675300121307\n",
            "Epoch  2 Batch  302 / 525  Training Loss  0.17481675744056702\n",
            "Epoch  2 Batch  303 / 525  Training Loss  0.16246971487998962\n",
            "Epoch  2 Batch  304 / 525  Training Loss  0.1495816707611084\n",
            "Epoch  2 Batch  305 / 525  Training Loss  0.1736612766981125\n",
            "Epoch  2 Batch  306 / 525  Training Loss  0.17253363132476807\n",
            "Epoch  2 Batch  307 / 525  Training Loss  0.17168675363063812\n",
            "Epoch  2 Batch  308 / 525  Training Loss  0.17518508434295654\n",
            "Epoch  2 Batch  309 / 525  Training Loss  0.1614377498626709\n",
            "Epoch  2 Batch  310 / 525  Training Loss  0.16218271851539612\n",
            "Epoch  2 Batch  311 / 525  Training Loss  0.17932549118995667\n",
            "Epoch  2 Batch  312 / 525  Training Loss  0.15463371574878693\n",
            "Epoch  2 Batch  313 / 525  Training Loss  0.16124491393566132\n",
            "Epoch  2 Batch  314 / 525  Training Loss  0.174264594912529\n",
            "Epoch  2 Batch  315 / 525  Training Loss  0.14934691786766052\n",
            "Epoch  2 Batch  316 / 525  Training Loss  0.15681712329387665\n",
            "Epoch  2 Batch  317 / 525  Training Loss  0.15988919138908386\n",
            "Epoch  2 Batch  318 / 525  Training Loss  0.16055385768413544\n",
            "Epoch  2 Batch  319 / 525  Training Loss  0.1522884964942932\n",
            "Epoch  2 Batch  320 / 525  Training Loss  0.18348810076713562\n",
            "Epoch  2 Batch  321 / 525  Training Loss  0.15097646415233612\n",
            "Epoch  2 Batch  322 / 525  Training Loss  0.17638838291168213\n",
            "Epoch  2 Batch  323 / 525  Training Loss  0.15999451279640198\n",
            "Epoch  2 Batch  324 / 525  Training Loss  0.1664525717496872\n",
            "Epoch  2 Batch  325 / 525  Training Loss  0.15913040935993195\n",
            "Epoch  2 Batch  326 / 525  Training Loss  0.1608520895242691\n",
            "Epoch  2 Batch  327 / 525  Training Loss  0.17068712413311005\n",
            "Epoch  2 Batch  328 / 525  Training Loss  0.16452667117118835\n",
            "Epoch  2 Batch  329 / 525  Training Loss  0.1678740680217743\n",
            "Epoch  2 Batch  330 / 525  Training Loss  0.16256017982959747\n",
            "Epoch  2 Batch  331 / 525  Training Loss  0.16750845313072205\n",
            "Epoch  2 Batch  332 / 525  Training Loss  0.15370073914527893\n",
            "Epoch  2 Batch  333 / 525  Training Loss  0.14915627241134644\n",
            "Epoch  2 Batch  334 / 525  Training Loss  0.1550961583852768\n",
            "Epoch  2 Batch  335 / 525  Training Loss  0.16775716841220856\n",
            "Epoch  2 Batch  336 / 525  Training Loss  0.14574918150901794\n",
            "Epoch  2 Batch  337 / 525  Training Loss  0.15965989232063293\n",
            "Epoch  2 Batch  338 / 525  Training Loss  0.18023668229579926\n",
            "Epoch  2 Batch  339 / 525  Training Loss  0.17398838698863983\n",
            "Epoch  2 Batch  340 / 525  Training Loss  0.1488570272922516\n",
            "Epoch  2 Batch  341 / 525  Training Loss  0.1473800539970398\n",
            "Epoch  2 Batch  342 / 525  Training Loss  0.169333353638649\n",
            "Epoch  2 Batch  343 / 525  Training Loss  0.17367374897003174\n",
            "Epoch  2 Batch  344 / 525  Training Loss  0.14639298617839813\n",
            "Epoch  2 Batch  345 / 525  Training Loss  0.16884088516235352\n",
            "Epoch  2 Batch  346 / 525  Training Loss  0.16351494193077087\n",
            "Epoch  2 Batch  347 / 525  Training Loss  0.18019148707389832\n",
            "Epoch  2 Batch  348 / 525  Training Loss  0.15775644779205322\n",
            "Epoch  2 Batch  349 / 525  Training Loss  0.16341064870357513\n",
            "Epoch  2 Batch  350 / 525  Training Loss  0.17401370406150818\n",
            "Epoch  2 Batch  351 / 525  Training Loss  0.1701781004667282\n",
            "Epoch  2 Batch  352 / 525  Training Loss  0.1689741015434265\n",
            "Epoch  2 Batch  353 / 525  Training Loss  0.15714731812477112\n",
            "Epoch  2 Batch  354 / 525  Training Loss  0.16452988982200623\n",
            "Epoch  2 Batch  355 / 525  Training Loss  0.16664937138557434\n",
            "Epoch  2 Batch  356 / 525  Training Loss  0.16639018058776855\n",
            "Epoch  2 Batch  357 / 525  Training Loss  0.15904918313026428\n",
            "Epoch  2 Batch  358 / 525  Training Loss  0.18028879165649414\n",
            "Epoch  2 Batch  359 / 525  Training Loss  0.1735735982656479\n",
            "Epoch  2 Batch  360 / 525  Training Loss  0.1738617867231369\n",
            "Epoch  2 Batch  361 / 525  Training Loss  0.14943304657936096\n",
            "Epoch  2 Batch  362 / 525  Training Loss  0.17854250967502594\n",
            "Epoch  2 Batch  363 / 525  Training Loss  0.18124054372310638\n",
            "Epoch  2 Batch  364 / 525  Training Loss  0.15560977160930634\n",
            "Epoch  2 Batch  365 / 525  Training Loss  0.16512760519981384\n",
            "Epoch  2 Batch  366 / 525  Training Loss  0.15061800181865692\n",
            "Epoch  2 Batch  367 / 525  Training Loss  0.1534411907196045\n",
            "Epoch  2 Batch  368 / 525  Training Loss  0.14275133609771729\n",
            "Epoch  2 Batch  369 / 525  Training Loss  0.16286399960517883\n",
            "Epoch  2 Batch  370 / 525  Training Loss  0.16915032267570496\n",
            "Epoch  2 Batch  371 / 525  Training Loss  0.15099456906318665\n",
            "Epoch  2 Batch  372 / 525  Training Loss  0.15291927754878998\n",
            "Epoch  2 Batch  373 / 525  Training Loss  0.1629360318183899\n",
            "Epoch  2 Batch  374 / 525  Training Loss  0.1622500866651535\n",
            "Epoch  2 Batch  375 / 525  Training Loss  0.1669217348098755\n",
            "Epoch  2 Batch  376 / 525  Training Loss  0.18519707024097443\n",
            "Epoch  2 Batch  377 / 525  Training Loss  0.14158707857131958\n",
            "Epoch  2 Batch  378 / 525  Training Loss  0.14922654628753662\n",
            "Epoch  2 Batch  379 / 525  Training Loss  0.1506611555814743\n",
            "Epoch  2 Batch  380 / 525  Training Loss  0.1621561497449875\n",
            "Epoch  2 Batch  381 / 525  Training Loss  0.14091555774211884\n",
            "Epoch  2 Batch  382 / 525  Training Loss  0.1579838991165161\n",
            "Epoch  2 Batch  383 / 525  Training Loss  0.15920376777648926\n",
            "Epoch  2 Batch  384 / 525  Training Loss  0.16061890125274658\n",
            "Epoch  2 Batch  385 / 525  Training Loss  0.1611836701631546\n",
            "Epoch  2 Batch  386 / 525  Training Loss  0.17186574637889862\n",
            "Epoch  2 Batch  387 / 525  Training Loss  0.17454975843429565\n",
            "Epoch  2 Batch  388 / 525  Training Loss  0.16057991981506348\n",
            "Epoch  2 Batch  389 / 525  Training Loss  0.14763040840625763\n",
            "Epoch  2 Batch  390 / 525  Training Loss  0.15031646192073822\n",
            "Epoch  2 Batch  391 / 525  Training Loss  0.17572012543678284\n",
            "Epoch  2 Batch  392 / 525  Training Loss  0.148969367146492\n",
            "Epoch  2 Batch  393 / 525  Training Loss  0.16172686219215393\n",
            "Epoch  2 Batch  394 / 525  Training Loss  0.18047283589839935\n",
            "Epoch  2 Batch  395 / 525  Training Loss  0.1720634251832962\n",
            "Epoch  2 Batch  396 / 525  Training Loss  0.17453935742378235\n",
            "Epoch  2 Batch  397 / 525  Training Loss  0.1721014529466629\n",
            "Epoch  2 Batch  398 / 525  Training Loss  0.18476180732250214\n",
            "Epoch  2 Batch  399 / 525  Training Loss  0.15855571627616882\n",
            "Epoch  2 Batch  400 / 525  Training Loss  0.1621403694152832\n",
            "Epoch  2 Batch  401 / 525  Training Loss  0.15923921763896942\n",
            "Epoch  2 Batch  402 / 525  Training Loss  0.14711667597293854\n",
            "Epoch  2 Batch  403 / 525  Training Loss  0.14970961213111877\n",
            "Epoch  2 Batch  404 / 525  Training Loss  0.16487380862236023\n",
            "Epoch  2 Batch  405 / 525  Training Loss  0.16532373428344727\n",
            "Epoch  2 Batch  406 / 525  Training Loss  0.15197250247001648\n",
            "Epoch  2 Batch  407 / 525  Training Loss  0.15225251019001007\n",
            "Epoch  2 Batch  408 / 525  Training Loss  0.15178798139095306\n",
            "Epoch  2 Batch  409 / 525  Training Loss  0.14976009726524353\n",
            "Epoch  2 Batch  410 / 525  Training Loss  0.14418165385723114\n",
            "Epoch  2 Batch  411 / 525  Training Loss  0.1488107442855835\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  2 Batch  412 / 525  Training Loss  0.15900857746601105\n",
            "Epoch  2 Batch  413 / 525  Training Loss  0.16019666194915771\n",
            "Epoch  2 Batch  414 / 525  Training Loss  0.15676537156105042\n",
            "Epoch  2 Batch  415 / 525  Training Loss  0.1663210093975067\n",
            "Epoch  2 Batch  416 / 525  Training Loss  0.1636417806148529\n",
            "Epoch  2 Batch  417 / 525  Training Loss  0.1665615290403366\n",
            "Epoch  2 Batch  418 / 525  Training Loss  0.15747804939746857\n",
            "Epoch  2 Batch  419 / 525  Training Loss  0.16463765501976013\n",
            "Epoch  2 Batch  420 / 525  Training Loss  0.1672653704881668\n",
            "Epoch  2 Batch  421 / 525  Training Loss  0.150955468416214\n",
            "Epoch  2 Batch  422 / 525  Training Loss  0.16586674749851227\n",
            "Epoch  2 Batch  423 / 525  Training Loss  0.18392959237098694\n",
            "Epoch  2 Batch  424 / 525  Training Loss  0.17684873938560486\n",
            "Epoch  2 Batch  425 / 525  Training Loss  0.1318950355052948\n",
            "Epoch  2 Batch  426 / 525  Training Loss  0.1691327840089798\n",
            "Epoch  2 Batch  427 / 525  Training Loss  0.13940542936325073\n",
            "Epoch  2 Batch  428 / 525  Training Loss  0.1516447812318802\n",
            "Epoch  2 Batch  429 / 525  Training Loss  0.16939057409763336\n",
            "Epoch  2 Batch  430 / 525  Training Loss  0.17143751680850983\n",
            "Epoch  2 Batch  431 / 525  Training Loss  0.16473522782325745\n",
            "Epoch  2 Batch  432 / 525  Training Loss  0.17335474491119385\n",
            "Epoch  2 Batch  433 / 525  Training Loss  0.1542324721813202\n",
            "Epoch  2 Batch  434 / 525  Training Loss  0.1459406465291977\n",
            "Epoch  2 Batch  435 / 525  Training Loss  0.17247331142425537\n",
            "Epoch  2 Batch  436 / 525  Training Loss  0.14573626220226288\n",
            "Epoch  2 Batch  437 / 525  Training Loss  0.1652582585811615\n",
            "Epoch  2 Batch  438 / 525  Training Loss  0.16439373791217804\n",
            "Epoch  2 Batch  439 / 525  Training Loss  0.1772119700908661\n",
            "Epoch  2 Batch  440 / 525  Training Loss  0.17207856476306915\n",
            "Epoch  2 Batch  441 / 525  Training Loss  0.1887931227684021\n",
            "Epoch  2 Batch  442 / 525  Training Loss  0.1592828929424286\n",
            "Epoch  2 Batch  443 / 525  Training Loss  0.1765480488538742\n",
            "Epoch  2 Batch  444 / 525  Training Loss  0.1480235755443573\n",
            "Epoch  2 Batch  445 / 525  Training Loss  0.16237734258174896\n",
            "Epoch  2 Batch  446 / 525  Training Loss  0.1654292792081833\n",
            "Epoch  2 Batch  447 / 525  Training Loss  0.15408697724342346\n",
            "Epoch  2 Batch  448 / 525  Training Loss  0.1657247543334961\n",
            "Epoch  2 Batch  449 / 525  Training Loss  0.14277616143226624\n",
            "Epoch  2 Batch  450 / 525  Training Loss  0.1756695806980133\n",
            "Epoch  2 Batch  451 / 525  Training Loss  0.16217628121376038\n",
            "Epoch  2 Batch  452 / 525  Training Loss  0.1538686454296112\n",
            "Epoch  2 Batch  453 / 525  Training Loss  0.15451866388320923\n",
            "Epoch  2 Batch  454 / 525  Training Loss  0.17141404747962952\n",
            "Epoch  2 Batch  455 / 525  Training Loss  0.16353102028369904\n",
            "Epoch  2 Batch  456 / 525  Training Loss  0.1646084040403366\n",
            "Epoch  2 Batch  457 / 525  Training Loss  0.16737328469753265\n",
            "Epoch  2 Batch  458 / 525  Training Loss  0.1716294139623642\n",
            "Epoch  2 Batch  459 / 525  Training Loss  0.13193626701831818\n",
            "Epoch  2 Batch  460 / 525  Training Loss  0.15341389179229736\n",
            "Epoch  2 Batch  461 / 525  Training Loss  0.15881173312664032\n",
            "Epoch  2 Batch  462 / 525  Training Loss  0.1506405472755432\n",
            "Epoch  2 Batch  463 / 525  Training Loss  0.15447521209716797\n",
            "Epoch  2 Batch  464 / 525  Training Loss  0.15202057361602783\n",
            "Epoch  2 Batch  465 / 525  Training Loss  0.1706240028142929\n",
            "Epoch  2 Batch  466 / 525  Training Loss  0.15893930196762085\n",
            "Epoch  2 Batch  467 / 525  Training Loss  0.13636384904384613\n",
            "Epoch  2 Batch  468 / 525  Training Loss  0.16625481843948364\n",
            "Epoch  2 Batch  469 / 525  Training Loss  0.15519949793815613\n",
            "Epoch  2 Batch  470 / 525  Training Loss  0.15282580256462097\n",
            "Epoch  2 Batch  471 / 525  Training Loss  0.18096938729286194\n",
            "Epoch  2 Batch  472 / 525  Training Loss  0.1483057290315628\n",
            "Epoch  2 Batch  473 / 525  Training Loss  0.15359434485435486\n",
            "Epoch  2 Batch  474 / 525  Training Loss  0.1515982449054718\n",
            "Epoch  2 Batch  475 / 525  Training Loss  0.16025306284427643\n",
            "Epoch  2 Batch  476 / 525  Training Loss  0.16061833500862122\n",
            "Epoch  2 Batch  477 / 525  Training Loss  0.15223965048789978\n",
            "Epoch  2 Batch  478 / 525  Training Loss  0.15149998664855957\n",
            "Epoch  2 Batch  479 / 525  Training Loss  0.15441516041755676\n",
            "Epoch  2 Batch  480 / 525  Training Loss  0.17388950288295746\n",
            "Epoch  2 Batch  481 / 525  Training Loss  0.16200897097587585\n",
            "Epoch  2 Batch  482 / 525  Training Loss  0.16844893991947174\n",
            "Epoch  2 Batch  483 / 525  Training Loss  0.14996862411499023\n",
            "Epoch  2 Batch  484 / 525  Training Loss  0.15437041223049164\n",
            "Epoch  2 Batch  485 / 525  Training Loss  0.15294817090034485\n",
            "Epoch  2 Batch  486 / 525  Training Loss  0.1636035442352295\n",
            "Epoch  2 Batch  487 / 525  Training Loss  0.15734410285949707\n",
            "Epoch  2 Batch  488 / 525  Training Loss  0.15735602378845215\n",
            "Epoch  2 Batch  489 / 525  Training Loss  0.15807171165943146\n",
            "Epoch  2 Batch  490 / 525  Training Loss  0.14761197566986084\n",
            "Epoch  2 Batch  491 / 525  Training Loss  0.16522324085235596\n",
            "Epoch  2 Batch  492 / 525  Training Loss  0.16201043128967285\n",
            "Epoch  2 Batch  493 / 525  Training Loss  0.14008750021457672\n",
            "Epoch  2 Batch  494 / 525  Training Loss  0.15700528025627136\n",
            "Epoch  2 Batch  495 / 525  Training Loss  0.14471349120140076\n",
            "Epoch  2 Batch  496 / 525  Training Loss  0.17425377666950226\n",
            "Epoch  2 Batch  497 / 525  Training Loss  0.15079531073570251\n",
            "Epoch  2 Batch  498 / 525  Training Loss  0.16002985835075378\n",
            "Epoch  2 Batch  499 / 525  Training Loss  0.14177019894123077\n",
            "Epoch  2 Batch  500 / 525  Training Loss  0.14954838156700134\n",
            "Epoch  2 Batch  501 / 525  Training Loss  0.14307871460914612\n",
            "Epoch  2 Batch  502 / 525  Training Loss  0.14425767958164215\n",
            "Epoch  2 Batch  503 / 525  Training Loss  0.14339222013950348\n",
            "Epoch  2 Batch  504 / 525  Training Loss  0.16344082355499268\n",
            "Epoch  2 Batch  505 / 525  Training Loss  0.17103342711925507\n",
            "Epoch  2 Batch  506 / 525  Training Loss  0.15359294414520264\n",
            "Epoch  2 Batch  507 / 525  Training Loss  0.18651838600635529\n",
            "Epoch  2 Batch  508 / 525  Training Loss  0.13272348046302795\n",
            "Epoch  2 Batch  509 / 525  Training Loss  0.1487966626882553\n",
            "Epoch  2 Batch  510 / 525  Training Loss  0.16406820714473724\n",
            "Epoch  2 Batch  511 / 525  Training Loss  0.16087478399276733\n",
            "Epoch  2 Batch  512 / 525  Training Loss  0.14618505537509918\n",
            "Epoch  2 Batch  513 / 525  Training Loss  0.16247588396072388\n",
            "Epoch  2 Batch  514 / 525  Training Loss  0.16035552322864532\n",
            "Epoch  2 Batch  515 / 525  Training Loss  0.16008244454860687\n",
            "Epoch  2 Batch  516 / 525  Training Loss  0.13041432201862335\n",
            "Epoch  2 Batch  517 / 525  Training Loss  0.1562880277633667\n",
            "Epoch  2 Batch  518 / 525  Training Loss  0.15427032113075256\n",
            "Epoch  2 Batch  519 / 525  Training Loss  0.14821290969848633\n",
            "Epoch  2 Batch  520 / 525  Training Loss  0.15946218371391296\n",
            "Epoch  2 Batch  521 / 525  Training Loss  0.15556089580059052\n",
            "Epoch  2 Batch  522 / 525  Training Loss  0.14139467477798462\n",
            "Epoch  2 Batch  523 / 525  Training Loss  0.13155320286750793\n",
            "Epoch  2 Batch  524 / 525  Training Loss  0.15846996009349823\n",
            "   3    |    -    |   0.172112   |   13.23  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 3\n",
            "Epoch  3 Batch  0 / 525  Training Loss  0.1587168276309967\n",
            "Epoch  3 Batch  1 / 525  Training Loss  0.1464119851589203\n",
            "Epoch  3 Batch  2 / 525  Training Loss  0.13964733481407166\n",
            "Epoch  3 Batch  3 / 525  Training Loss  0.1342432051897049\n",
            "Epoch  3 Batch  4 / 525  Training Loss  0.12877584993839264\n",
            "Epoch  3 Batch  5 / 525  Training Loss  0.11081083118915558\n",
            "Epoch  3 Batch  6 / 525  Training Loss  0.13650788366794586\n",
            "Epoch  3 Batch  7 / 525  Training Loss  0.11353492736816406\n",
            "Epoch  3 Batch  8 / 525  Training Loss  0.1191062480211258\n",
            "Epoch  3 Batch  9 / 525  Training Loss  0.15198102593421936\n",
            "Epoch  3 Batch  10 / 525  Training Loss  0.13642215728759766\n",
            "Epoch  3 Batch  11 / 525  Training Loss  0.1349727064371109\n",
            "Epoch  3 Batch  12 / 525  Training Loss  0.1395735740661621\n",
            "Epoch  3 Batch  13 / 525  Training Loss  0.1325373649597168\n",
            "Epoch  3 Batch  14 / 525  Training Loss  0.13352349400520325\n",
            "Epoch  3 Batch  15 / 525  Training Loss  0.13465510308742523\n",
            "Epoch  3 Batch  16 / 525  Training Loss  0.13864009082317352\n",
            "Epoch  3 Batch  17 / 525  Training Loss  0.1467781513929367\n",
            "Epoch  3 Batch  18 / 525  Training Loss  0.12627749145030975\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  3 Batch  19 / 525  Training Loss  0.13564470410346985\n",
            "Epoch  3 Batch  20 / 525  Training Loss  0.13512122631072998\n",
            "Epoch  3 Batch  21 / 525  Training Loss  0.13494865596294403\n",
            "Epoch  3 Batch  22 / 525  Training Loss  0.13656491041183472\n",
            "Epoch  3 Batch  23 / 525  Training Loss  0.1326439380645752\n",
            "Epoch  3 Batch  24 / 525  Training Loss  0.1390099823474884\n",
            "Epoch  3 Batch  25 / 525  Training Loss  0.13610012829303741\n",
            "Epoch  3 Batch  26 / 525  Training Loss  0.1456369012594223\n",
            "Epoch  3 Batch  27 / 525  Training Loss  0.12463051080703735\n",
            "Epoch  3 Batch  28 / 525  Training Loss  0.1479426771402359\n",
            "Epoch  3 Batch  29 / 525  Training Loss  0.15013372898101807\n",
            "Epoch  3 Batch  30 / 525  Training Loss  0.1385089010000229\n",
            "Epoch  3 Batch  31 / 525  Training Loss  0.13049903512001038\n",
            "Epoch  3 Batch  32 / 525  Training Loss  0.12553173303604126\n",
            "Epoch  3 Batch  33 / 525  Training Loss  0.13391704857349396\n",
            "Epoch  3 Batch  34 / 525  Training Loss  0.12138281762599945\n",
            "Epoch  3 Batch  35 / 525  Training Loss  0.15305596590042114\n",
            "Epoch  3 Batch  36 / 525  Training Loss  0.14516660571098328\n",
            "Epoch  3 Batch  37 / 525  Training Loss  0.14058417081832886\n",
            "Epoch  3 Batch  38 / 525  Training Loss  0.14206543564796448\n",
            "Epoch  3 Batch  39 / 525  Training Loss  0.1494246870279312\n",
            "Epoch  3 Batch  40 / 525  Training Loss  0.14436891674995422\n",
            "Epoch  3 Batch  41 / 525  Training Loss  0.1325596421957016\n",
            "Epoch  3 Batch  42 / 525  Training Loss  0.1284228265285492\n",
            "Epoch  3 Batch  43 / 525  Training Loss  0.1376999169588089\n",
            "Epoch  3 Batch  44 / 525  Training Loss  0.14803385734558105\n",
            "Epoch  3 Batch  45 / 525  Training Loss  0.13149471580982208\n",
            "Epoch  3 Batch  46 / 525  Training Loss  0.14506478607654572\n",
            "Epoch  3 Batch  47 / 525  Training Loss  0.14457054436206818\n",
            "Epoch  3 Batch  48 / 525  Training Loss  0.1160360723733902\n",
            "Epoch  3 Batch  49 / 525  Training Loss  0.16659380495548248\n",
            "Epoch  3 Batch  50 / 525  Training Loss  0.14683057367801666\n",
            "Epoch  3 Batch  51 / 525  Training Loss  0.1403086632490158\n",
            "Epoch  3 Batch  52 / 525  Training Loss  0.1519121378660202\n",
            "Epoch  3 Batch  53 / 525  Training Loss  0.1370970755815506\n",
            "Epoch  3 Batch  54 / 525  Training Loss  0.13435237109661102\n",
            "Epoch  3 Batch  55 / 525  Training Loss  0.1498812735080719\n",
            "Epoch  3 Batch  56 / 525  Training Loss  0.15148158371448517\n",
            "Epoch  3 Batch  57 / 525  Training Loss  0.13379797339439392\n",
            "Epoch  3 Batch  58 / 525  Training Loss  0.13902759552001953\n",
            "Epoch  3 Batch  59 / 525  Training Loss  0.13301816582679749\n",
            "Epoch  3 Batch  60 / 525  Training Loss  0.14541473984718323\n",
            "Epoch  3 Batch  61 / 525  Training Loss  0.14053627848625183\n",
            "Epoch  3 Batch  62 / 525  Training Loss  0.15818652510643005\n",
            "Epoch  3 Batch  63 / 525  Training Loss  0.1347144991159439\n",
            "Epoch  3 Batch  64 / 525  Training Loss  0.1401005983352661\n",
            "Epoch  3 Batch  65 / 525  Training Loss  0.11848195642232895\n",
            "Epoch  3 Batch  66 / 525  Training Loss  0.14443279802799225\n",
            "Epoch  3 Batch  67 / 525  Training Loss  0.13256710767745972\n",
            "Epoch  3 Batch  68 / 525  Training Loss  0.13200688362121582\n",
            "Epoch  3 Batch  69 / 525  Training Loss  0.1319461166858673\n",
            "Epoch  3 Batch  70 / 525  Training Loss  0.14076204597949982\n",
            "Epoch  3 Batch  71 / 525  Training Loss  0.12862837314605713\n",
            "Epoch  3 Batch  72 / 525  Training Loss  0.14892417192459106\n",
            "Epoch  3 Batch  73 / 525  Training Loss  0.1371946632862091\n",
            "Epoch  3 Batch  74 / 525  Training Loss  0.12360209226608276\n",
            "Epoch  3 Batch  75 / 525  Training Loss  0.13812978565692902\n",
            "Epoch  3 Batch  76 / 525  Training Loss  0.12340422719717026\n",
            "Epoch  3 Batch  77 / 525  Training Loss  0.13397082686424255\n",
            "Epoch  3 Batch  78 / 525  Training Loss  0.14042744040489197\n",
            "Epoch  3 Batch  79 / 525  Training Loss  0.15314549207687378\n",
            "Epoch  3 Batch  80 / 525  Training Loss  0.1300080418586731\n",
            "Epoch  3 Batch  81 / 525  Training Loss  0.13650944828987122\n",
            "Epoch  3 Batch  82 / 525  Training Loss  0.13493266701698303\n",
            "Epoch  3 Batch  83 / 525  Training Loss  0.15762151777744293\n",
            "Epoch  3 Batch  84 / 525  Training Loss  0.16103965044021606\n",
            "Epoch  3 Batch  85 / 525  Training Loss  0.1423238217830658\n",
            "Epoch  3 Batch  86 / 525  Training Loss  0.12346850335597992\n",
            "Epoch  3 Batch  87 / 525  Training Loss  0.13733527064323425\n",
            "Epoch  3 Batch  88 / 525  Training Loss  0.13552795350551605\n",
            "Epoch  3 Batch  89 / 525  Training Loss  0.13118606805801392\n",
            "Epoch  3 Batch  90 / 525  Training Loss  0.15065303444862366\n",
            "Epoch  3 Batch  91 / 525  Training Loss  0.13529065251350403\n",
            "Epoch  3 Batch  92 / 525  Training Loss  0.12961333990097046\n",
            "Epoch  3 Batch  93 / 525  Training Loss  0.13291850686073303\n",
            "Epoch  3 Batch  94 / 525  Training Loss  0.13230006396770477\n",
            "Epoch  3 Batch  95 / 525  Training Loss  0.13395684957504272\n",
            "Epoch  3 Batch  96 / 525  Training Loss  0.13741029798984528\n",
            "Epoch  3 Batch  97 / 525  Training Loss  0.15246281027793884\n",
            "Epoch  3 Batch  98 / 525  Training Loss  0.1559324413537979\n",
            "Epoch  3 Batch  99 / 525  Training Loss  0.12302696704864502\n",
            "Epoch  3 Batch  100 / 525  Training Loss  0.1531694531440735\n",
            "Epoch  3 Batch  101 / 525  Training Loss  0.13338078558444977\n",
            "Epoch  3 Batch  102 / 525  Training Loss  0.15437054634094238\n",
            "Epoch  3 Batch  103 / 525  Training Loss  0.15967333316802979\n",
            "Epoch  3 Batch  104 / 525  Training Loss  0.13749352097511292\n",
            "Epoch  3 Batch  105 / 525  Training Loss  0.1398002654314041\n",
            "Epoch  3 Batch  106 / 525  Training Loss  0.1507042944431305\n",
            "Epoch  3 Batch  107 / 525  Training Loss  0.15022507309913635\n",
            "Epoch  3 Batch  108 / 525  Training Loss  0.11910456418991089\n",
            "Epoch  3 Batch  109 / 525  Training Loss  0.1262854039669037\n",
            "Epoch  3 Batch  110 / 525  Training Loss  0.14508755505084991\n",
            "Epoch  3 Batch  111 / 525  Training Loss  0.14055410027503967\n",
            "Epoch  3 Batch  112 / 525  Training Loss  0.14750424027442932\n",
            "Epoch  3 Batch  113 / 525  Training Loss  0.14018288254737854\n",
            "Epoch  3 Batch  114 / 525  Training Loss  0.1405988484621048\n",
            "Epoch  3 Batch  115 / 525  Training Loss  0.1299787312746048\n",
            "Epoch  3 Batch  116 / 525  Training Loss  0.14373426139354706\n",
            "Epoch  3 Batch  117 / 525  Training Loss  0.14954873919487\n",
            "Epoch  3 Batch  118 / 525  Training Loss  0.1378786712884903\n",
            "Epoch  3 Batch  119 / 525  Training Loss  0.14526115357875824\n",
            "Epoch  3 Batch  120 / 525  Training Loss  0.1564069539308548\n",
            "Epoch  3 Batch  121 / 525  Training Loss  0.13767512142658234\n",
            "Epoch  3 Batch  122 / 525  Training Loss  0.15007953345775604\n",
            "Epoch  3 Batch  123 / 525  Training Loss  0.1633240282535553\n",
            "Epoch  3 Batch  124 / 525  Training Loss  0.14562232792377472\n",
            "Epoch  3 Batch  125 / 525  Training Loss  0.14415162801742554\n",
            "Epoch  3 Batch  126 / 525  Training Loss  0.14495162665843964\n",
            "Epoch  3 Batch  127 / 525  Training Loss  0.1301811933517456\n",
            "Epoch  3 Batch  128 / 525  Training Loss  0.12810935080051422\n",
            "Epoch  3 Batch  129 / 525  Training Loss  0.137950137257576\n",
            "Epoch  3 Batch  130 / 525  Training Loss  0.11707588285207748\n",
            "Epoch  3 Batch  131 / 525  Training Loss  0.14216828346252441\n",
            "Epoch  3 Batch  132 / 525  Training Loss  0.1354958713054657\n",
            "Epoch  3 Batch  133 / 525  Training Loss  0.12575969099998474\n",
            "Epoch  3 Batch  134 / 525  Training Loss  0.13736985623836517\n",
            "Epoch  3 Batch  135 / 525  Training Loss  0.14129383862018585\n",
            "Epoch  3 Batch  136 / 525  Training Loss  0.13439123332500458\n",
            "Epoch  3 Batch  137 / 525  Training Loss  0.1417331099510193\n",
            "Epoch  3 Batch  138 / 525  Training Loss  0.14181503653526306\n",
            "Epoch  3 Batch  139 / 525  Training Loss  0.12801575660705566\n",
            "Epoch  3 Batch  140 / 525  Training Loss  0.12456254661083221\n",
            "Epoch  3 Batch  141 / 525  Training Loss  0.1336219608783722\n",
            "Epoch  3 Batch  142 / 525  Training Loss  0.13701725006103516\n",
            "Epoch  3 Batch  143 / 525  Training Loss  0.12452371418476105\n",
            "Epoch  3 Batch  144 / 525  Training Loss  0.14303065836429596\n",
            "Epoch  3 Batch  145 / 525  Training Loss  0.13887764513492584\n",
            "Epoch  3 Batch  146 / 525  Training Loss  0.13947135210037231\n",
            "Epoch  3 Batch  147 / 525  Training Loss  0.1346028745174408\n",
            "Epoch  3 Batch  148 / 525  Training Loss  0.13308057188987732\n",
            "Epoch  3 Batch  149 / 525  Training Loss  0.16339720785617828\n",
            "Epoch  3 Batch  150 / 525  Training Loss  0.1431051343679428\n",
            "Epoch  3 Batch  151 / 525  Training Loss  0.13477398455142975\n",
            "Epoch  3 Batch  152 / 525  Training Loss  0.13073061406612396\n",
            "Epoch  3 Batch  153 / 525  Training Loss  0.13498914241790771\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  3 Batch  154 / 525  Training Loss  0.150716170668602\n",
            "Epoch  3 Batch  155 / 525  Training Loss  0.1350201666355133\n",
            "Epoch  3 Batch  156 / 525  Training Loss  0.1427476555109024\n",
            "Epoch  3 Batch  157 / 525  Training Loss  0.14861026406288147\n",
            "Epoch  3 Batch  158 / 525  Training Loss  0.12618771195411682\n",
            "Epoch  3 Batch  159 / 525  Training Loss  0.1242760419845581\n",
            "Epoch  3 Batch  160 / 525  Training Loss  0.1509673297405243\n",
            "Epoch  3 Batch  161 / 525  Training Loss  0.12400078773498535\n",
            "Epoch  3 Batch  162 / 525  Training Loss  0.12803997099399567\n",
            "Epoch  3 Batch  163 / 525  Training Loss  0.15367677807807922\n",
            "Epoch  3 Batch  164 / 525  Training Loss  0.12448761612176895\n",
            "Epoch  3 Batch  165 / 525  Training Loss  0.13250219821929932\n",
            "Epoch  3 Batch  166 / 525  Training Loss  0.13852642476558685\n",
            "Epoch  3 Batch  167 / 525  Training Loss  0.15218739211559296\n",
            "Epoch  3 Batch  168 / 525  Training Loss  0.13599933683872223\n",
            "Epoch  3 Batch  169 / 525  Training Loss  0.12131454050540924\n",
            "Epoch  3 Batch  170 / 525  Training Loss  0.1482851356267929\n",
            "Epoch  3 Batch  171 / 525  Training Loss  0.13757070899009705\n",
            "Epoch  3 Batch  172 / 525  Training Loss  0.13443692028522491\n",
            "Epoch  3 Batch  173 / 525  Training Loss  0.13261935114860535\n",
            "Epoch  3 Batch  174 / 525  Training Loss  0.12659312784671783\n",
            "Epoch  3 Batch  175 / 525  Training Loss  0.13357259333133698\n",
            "Epoch  3 Batch  176 / 525  Training Loss  0.12076374143362045\n",
            "Epoch  3 Batch  177 / 525  Training Loss  0.14038583636283875\n",
            "Epoch  3 Batch  178 / 525  Training Loss  0.12925389409065247\n",
            "Epoch  3 Batch  179 / 525  Training Loss  0.14027661085128784\n",
            "Epoch  3 Batch  180 / 525  Training Loss  0.13462446630001068\n",
            "Epoch  3 Batch  181 / 525  Training Loss  0.14451400935649872\n",
            "Epoch  3 Batch  182 / 525  Training Loss  0.12716160714626312\n",
            "Epoch  3 Batch  183 / 525  Training Loss  0.15345293283462524\n",
            "Epoch  3 Batch  184 / 525  Training Loss  0.11872056871652603\n",
            "Epoch  3 Batch  185 / 525  Training Loss  0.15467944741249084\n",
            "Epoch  3 Batch  186 / 525  Training Loss  0.14738169312477112\n",
            "Epoch  3 Batch  187 / 525  Training Loss  0.13445156812667847\n",
            "Epoch  3 Batch  188 / 525  Training Loss  0.1446746587753296\n",
            "Epoch  3 Batch  189 / 525  Training Loss  0.12120924890041351\n",
            "Epoch  3 Batch  190 / 525  Training Loss  0.14229457080364227\n",
            "Epoch  3 Batch  191 / 525  Training Loss  0.14562751352787018\n",
            "Epoch  3 Batch  192 / 525  Training Loss  0.14455990493297577\n",
            "Epoch  3 Batch  193 / 525  Training Loss  0.12152288854122162\n",
            "Epoch  3 Batch  194 / 525  Training Loss  0.11802039295434952\n",
            "Epoch  3 Batch  195 / 525  Training Loss  0.12878233194351196\n",
            "Epoch  3 Batch  196 / 525  Training Loss  0.12191621959209442\n",
            "Epoch  3 Batch  197 / 525  Training Loss  0.12354616820812225\n",
            "Epoch  3 Batch  198 / 525  Training Loss  0.15408791601657867\n",
            "Epoch  3 Batch  199 / 525  Training Loss  0.1256697177886963\n",
            "Epoch  3 Batch  200 / 525  Training Loss  0.14744287729263306\n",
            "Epoch  3 Batch  201 / 525  Training Loss  0.14906525611877441\n",
            "Epoch  3 Batch  202 / 525  Training Loss  0.13031592965126038\n",
            "Epoch  3 Batch  203 / 525  Training Loss  0.1403302550315857\n",
            "Epoch  3 Batch  204 / 525  Training Loss  0.1408042162656784\n",
            "Epoch  3 Batch  205 / 525  Training Loss  0.1443629115819931\n",
            "Epoch  3 Batch  206 / 525  Training Loss  0.13194069266319275\n",
            "Epoch  3 Batch  207 / 525  Training Loss  0.13902974128723145\n",
            "Epoch  3 Batch  208 / 525  Training Loss  0.12794426083564758\n",
            "Epoch  3 Batch  209 / 525  Training Loss  0.1505974531173706\n",
            "Epoch  3 Batch  210 / 525  Training Loss  0.14031127095222473\n",
            "Epoch  3 Batch  211 / 525  Training Loss  0.12039482593536377\n",
            "Epoch  3 Batch  212 / 525  Training Loss  0.1252443492412567\n",
            "Epoch  3 Batch  213 / 525  Training Loss  0.12682870030403137\n",
            "Epoch  3 Batch  214 / 525  Training Loss  0.1387435346841812\n",
            "Epoch  3 Batch  215 / 525  Training Loss  0.11585493385791779\n",
            "Epoch  3 Batch  216 / 525  Training Loss  0.14810091257095337\n",
            "Epoch  3 Batch  217 / 525  Training Loss  0.12665045261383057\n",
            "Epoch  3 Batch  218 / 525  Training Loss  0.1392868459224701\n",
            "Epoch  3 Batch  219 / 525  Training Loss  0.12966284155845642\n",
            "Epoch  3 Batch  220 / 525  Training Loss  0.16242489218711853\n",
            "Epoch  3 Batch  221 / 525  Training Loss  0.14590318500995636\n",
            "Epoch  3 Batch  222 / 525  Training Loss  0.12979881465435028\n",
            "Epoch  3 Batch  223 / 525  Training Loss  0.13575178384780884\n",
            "Epoch  3 Batch  224 / 525  Training Loss  0.12234201282262802\n",
            "Epoch  3 Batch  225 / 525  Training Loss  0.1260967254638672\n",
            "Epoch  3 Batch  226 / 525  Training Loss  0.1340682953596115\n",
            "Epoch  3 Batch  227 / 525  Training Loss  0.13607683777809143\n",
            "Epoch  3 Batch  228 / 525  Training Loss  0.12611326575279236\n",
            "Epoch  3 Batch  229 / 525  Training Loss  0.11364607512950897\n",
            "Epoch  3 Batch  230 / 525  Training Loss  0.14747779071331024\n",
            "Epoch  3 Batch  231 / 525  Training Loss  0.1330118179321289\n",
            "Epoch  3 Batch  232 / 525  Training Loss  0.1249064952135086\n",
            "Epoch  3 Batch  233 / 525  Training Loss  0.12215030193328857\n",
            "Epoch  3 Batch  234 / 525  Training Loss  0.14138799905776978\n",
            "Epoch  3 Batch  235 / 525  Training Loss  0.13318340480327606\n",
            "Epoch  3 Batch  236 / 525  Training Loss  0.13253140449523926\n",
            "Epoch  3 Batch  237 / 525  Training Loss  0.13359428942203522\n",
            "Epoch  3 Batch  238 / 525  Training Loss  0.1317397803068161\n",
            "Epoch  3 Batch  239 / 525  Training Loss  0.1309383362531662\n",
            "Epoch  3 Batch  240 / 525  Training Loss  0.13842080533504486\n",
            "Epoch  3 Batch  241 / 525  Training Loss  0.13153335452079773\n",
            "Epoch  3 Batch  242 / 525  Training Loss  0.12839609384536743\n",
            "Epoch  3 Batch  243 / 525  Training Loss  0.14224497973918915\n",
            "Epoch  3 Batch  244 / 525  Training Loss  0.1600295752286911\n",
            "Epoch  3 Batch  245 / 525  Training Loss  0.14650484919548035\n",
            "Epoch  3 Batch  246 / 525  Training Loss  0.12267573922872543\n",
            "Epoch  3 Batch  247 / 525  Training Loss  0.11988000571727753\n",
            "Epoch  3 Batch  248 / 525  Training Loss  0.14695729315280914\n",
            "Epoch  3 Batch  249 / 525  Training Loss  0.12268779426813126\n",
            "Epoch  3 Batch  250 / 525  Training Loss  0.1469481885433197\n",
            "Epoch  3 Batch  251 / 525  Training Loss  0.14364966750144958\n",
            "Epoch  3 Batch  252 / 525  Training Loss  0.11360397189855576\n",
            "Epoch  3 Batch  253 / 525  Training Loss  0.11284897476434708\n",
            "Epoch  3 Batch  254 / 525  Training Loss  0.14826765656471252\n",
            "Epoch  3 Batch  255 / 525  Training Loss  0.14439579844474792\n",
            "Epoch  3 Batch  256 / 525  Training Loss  0.11011415719985962\n",
            "Epoch  3 Batch  257 / 525  Training Loss  0.1222762018442154\n",
            "Epoch  3 Batch  258 / 525  Training Loss  0.17852553725242615\n",
            "Epoch  3 Batch  259 / 525  Training Loss  0.13609257340431213\n",
            "Epoch  3 Batch  260 / 525  Training Loss  0.13418474793434143\n",
            "Epoch  3 Batch  261 / 525  Training Loss  0.12890088558197021\n",
            "Epoch  3 Batch  262 / 525  Training Loss  0.15408484637737274\n",
            "Epoch  3 Batch  263 / 525  Training Loss  0.11059373617172241\n",
            "Epoch  3 Batch  264 / 525  Training Loss  0.1413394808769226\n",
            "Epoch  3 Batch  265 / 525  Training Loss  0.14217141270637512\n",
            "Epoch  3 Batch  266 / 525  Training Loss  0.1526786834001541\n",
            "Epoch  3 Batch  267 / 525  Training Loss  0.14795157313346863\n",
            "Epoch  3 Batch  268 / 525  Training Loss  0.1297178715467453\n",
            "Epoch  3 Batch  269 / 525  Training Loss  0.13581489026546478\n",
            "Epoch  3 Batch  270 / 525  Training Loss  0.1431257575750351\n",
            "Epoch  3 Batch  271 / 525  Training Loss  0.14973506331443787\n",
            "Epoch  3 Batch  272 / 525  Training Loss  0.14098745584487915\n",
            "Epoch  3 Batch  273 / 525  Training Loss  0.1447656899690628\n",
            "Epoch  3 Batch  274 / 525  Training Loss  0.1401883065700531\n",
            "Epoch  3 Batch  275 / 525  Training Loss  0.13882645964622498\n",
            "Epoch  3 Batch  276 / 525  Training Loss  0.1432546079158783\n",
            "Epoch  3 Batch  277 / 525  Training Loss  0.1318301260471344\n",
            "Epoch  3 Batch  278 / 525  Training Loss  0.12369491159915924\n",
            "Epoch  3 Batch  279 / 525  Training Loss  0.12500737607479095\n",
            "Epoch  3 Batch  280 / 525  Training Loss  0.12690329551696777\n",
            "Epoch  3 Batch  281 / 525  Training Loss  0.1353822648525238\n",
            "Epoch  3 Batch  282 / 525  Training Loss  0.16544702649116516\n",
            "Epoch  3 Batch  283 / 525  Training Loss  0.12271209061145782\n",
            "Epoch  3 Batch  284 / 525  Training Loss  0.14281782507896423\n",
            "Epoch  3 Batch  285 / 525  Training Loss  0.1259186714887619\n",
            "Epoch  3 Batch  286 / 525  Training Loss  0.11753436177968979\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  3 Batch  287 / 525  Training Loss  0.14210352301597595\n",
            "Epoch  3 Batch  288 / 525  Training Loss  0.12663277983665466\n",
            "Epoch  3 Batch  289 / 525  Training Loss  0.12627002596855164\n",
            "Epoch  3 Batch  290 / 525  Training Loss  0.11690797656774521\n",
            "Epoch  3 Batch  291 / 525  Training Loss  0.10923869907855988\n",
            "Epoch  3 Batch  292 / 525  Training Loss  0.14546212553977966\n",
            "Epoch  3 Batch  293 / 525  Training Loss  0.1436072438955307\n",
            "Epoch  3 Batch  294 / 525  Training Loss  0.1300172656774521\n",
            "Epoch  3 Batch  295 / 525  Training Loss  0.10317017883062363\n",
            "Epoch  3 Batch  296 / 525  Training Loss  0.1170339584350586\n",
            "Epoch  3 Batch  297 / 525  Training Loss  0.12398342788219452\n",
            "Epoch  3 Batch  298 / 525  Training Loss  0.14143893122673035\n",
            "Epoch  3 Batch  299 / 525  Training Loss  0.12512649595737457\n",
            "Epoch  3 Batch  300 / 525  Training Loss  0.13208310306072235\n",
            "Epoch  3 Batch  301 / 525  Training Loss  0.14767101407051086\n",
            "Epoch  3 Batch  302 / 525  Training Loss  0.13173142075538635\n",
            "Epoch  3 Batch  303 / 525  Training Loss  0.13957038521766663\n",
            "Epoch  3 Batch  304 / 525  Training Loss  0.12605629861354828\n",
            "Epoch  3 Batch  305 / 525  Training Loss  0.12172041088342667\n",
            "Epoch  3 Batch  306 / 525  Training Loss  0.14218702912330627\n",
            "Epoch  3 Batch  307 / 525  Training Loss  0.11627350747585297\n",
            "Epoch  3 Batch  308 / 525  Training Loss  0.12892670929431915\n",
            "Epoch  3 Batch  309 / 525  Training Loss  0.13791774213314056\n",
            "Epoch  3 Batch  310 / 525  Training Loss  0.13430675864219666\n",
            "Epoch  3 Batch  311 / 525  Training Loss  0.1278524249792099\n",
            "Epoch  3 Batch  312 / 525  Training Loss  0.11346189677715302\n",
            "Epoch  3 Batch  313 / 525  Training Loss  0.12849655747413635\n",
            "Epoch  3 Batch  314 / 525  Training Loss  0.11721204221248627\n",
            "Epoch  3 Batch  315 / 525  Training Loss  0.13496892154216766\n",
            "Epoch  3 Batch  316 / 525  Training Loss  0.12671014666557312\n",
            "Epoch  3 Batch  317 / 525  Training Loss  0.12572261691093445\n",
            "Epoch  3 Batch  318 / 525  Training Loss  0.12755565345287323\n",
            "Epoch  3 Batch  319 / 525  Training Loss  0.1417083442211151\n",
            "Epoch  3 Batch  320 / 525  Training Loss  0.1326911300420761\n",
            "Epoch  3 Batch  321 / 525  Training Loss  0.1380256861448288\n",
            "Epoch  3 Batch  322 / 525  Training Loss  0.11674849689006805\n",
            "Epoch  3 Batch  323 / 525  Training Loss  0.12893976271152496\n",
            "Epoch  3 Batch  324 / 525  Training Loss  0.12113585323095322\n",
            "Epoch  3 Batch  325 / 525  Training Loss  0.1325409710407257\n",
            "Epoch  3 Batch  326 / 525  Training Loss  0.13349905610084534\n",
            "Epoch  3 Batch  327 / 525  Training Loss  0.12088682502508163\n",
            "Epoch  3 Batch  328 / 525  Training Loss  0.13749061524868011\n",
            "Epoch  3 Batch  329 / 525  Training Loss  0.1166440024971962\n",
            "Epoch  3 Batch  330 / 525  Training Loss  0.12222020328044891\n",
            "Epoch  3 Batch  331 / 525  Training Loss  0.12656927108764648\n",
            "Epoch  3 Batch  332 / 525  Training Loss  0.11709240823984146\n",
            "Epoch  3 Batch  333 / 525  Training Loss  0.13888105750083923\n",
            "Epoch  3 Batch  334 / 525  Training Loss  0.13797405362129211\n",
            "Epoch  3 Batch  335 / 525  Training Loss  0.13844351470470428\n",
            "Epoch  3 Batch  336 / 525  Training Loss  0.1291801482439041\n",
            "Epoch  3 Batch  337 / 525  Training Loss  0.12770462036132812\n",
            "Epoch  3 Batch  338 / 525  Training Loss  0.15486037731170654\n",
            "Epoch  3 Batch  339 / 525  Training Loss  0.1551656872034073\n",
            "Epoch  3 Batch  340 / 525  Training Loss  0.13087224960327148\n",
            "Epoch  3 Batch  341 / 525  Training Loss  0.14065508544445038\n",
            "Epoch  3 Batch  342 / 525  Training Loss  0.13567788898944855\n",
            "Epoch  3 Batch  343 / 525  Training Loss  0.12883467972278595\n",
            "Epoch  3 Batch  344 / 525  Training Loss  0.156576007604599\n",
            "Epoch  3 Batch  345 / 525  Training Loss  0.1416482925415039\n",
            "Epoch  3 Batch  346 / 525  Training Loss  0.11777757108211517\n",
            "Epoch  3 Batch  347 / 525  Training Loss  0.14287033677101135\n",
            "Epoch  3 Batch  348 / 525  Training Loss  0.1251336634159088\n",
            "Epoch  3 Batch  349 / 525  Training Loss  0.13781215250492096\n",
            "Epoch  3 Batch  350 / 525  Training Loss  0.13189932703971863\n",
            "Epoch  3 Batch  351 / 525  Training Loss  0.12664476037025452\n",
            "Epoch  3 Batch  352 / 525  Training Loss  0.12191381305456161\n",
            "Epoch  3 Batch  353 / 525  Training Loss  0.11326879262924194\n",
            "Epoch  3 Batch  354 / 525  Training Loss  0.1368277370929718\n",
            "Epoch  3 Batch  355 / 525  Training Loss  0.12642545998096466\n",
            "Epoch  3 Batch  356 / 525  Training Loss  0.1269332319498062\n",
            "Epoch  3 Batch  357 / 525  Training Loss  0.13755711913108826\n",
            "Epoch  3 Batch  358 / 525  Training Loss  0.10682467371225357\n",
            "Epoch  3 Batch  359 / 525  Training Loss  0.13143134117126465\n",
            "Epoch  3 Batch  360 / 525  Training Loss  0.15147490799427032\n",
            "Epoch  3 Batch  361 / 525  Training Loss  0.13241633772850037\n",
            "Epoch  3 Batch  362 / 525  Training Loss  0.12034253776073456\n",
            "Epoch  3 Batch  363 / 525  Training Loss  0.14218071103096008\n",
            "Epoch  3 Batch  364 / 525  Training Loss  0.13810715079307556\n",
            "Epoch  3 Batch  365 / 525  Training Loss  0.14081493020057678\n",
            "Epoch  3 Batch  366 / 525  Training Loss  0.14604228734970093\n",
            "Epoch  3 Batch  367 / 525  Training Loss  0.14435255527496338\n",
            "Epoch  3 Batch  368 / 525  Training Loss  0.1269896924495697\n",
            "Epoch  3 Batch  369 / 525  Training Loss  0.12178070843219757\n",
            "Epoch  3 Batch  370 / 525  Training Loss  0.14942148327827454\n",
            "Epoch  3 Batch  371 / 525  Training Loss  0.11348799616098404\n",
            "Epoch  3 Batch  372 / 525  Training Loss  0.13573811948299408\n",
            "Epoch  3 Batch  373 / 525  Training Loss  0.1365678310394287\n",
            "Epoch  3 Batch  374 / 525  Training Loss  0.10445624589920044\n",
            "Epoch  3 Batch  375 / 525  Training Loss  0.11644570529460907\n",
            "Epoch  3 Batch  376 / 525  Training Loss  0.14135420322418213\n",
            "Epoch  3 Batch  377 / 525  Training Loss  0.14958341419696808\n",
            "Epoch  3 Batch  378 / 525  Training Loss  0.1256893277168274\n",
            "Epoch  3 Batch  379 / 525  Training Loss  0.16896919906139374\n",
            "Epoch  3 Batch  380 / 525  Training Loss  0.14021004736423492\n",
            "Epoch  3 Batch  381 / 525  Training Loss  0.14514519274234772\n",
            "Epoch  3 Batch  382 / 525  Training Loss  0.12039615958929062\n",
            "Epoch  3 Batch  383 / 525  Training Loss  0.1400536149740219\n",
            "Epoch  3 Batch  384 / 525  Training Loss  0.13276144862174988\n",
            "Epoch  3 Batch  385 / 525  Training Loss  0.14671310782432556\n",
            "Epoch  3 Batch  386 / 525  Training Loss  0.12236114591360092\n",
            "Epoch  3 Batch  387 / 525  Training Loss  0.15971998870372772\n",
            "Epoch  3 Batch  388 / 525  Training Loss  0.12420612573623657\n",
            "Epoch  3 Batch  389 / 525  Training Loss  0.10776539891958237\n",
            "Epoch  3 Batch  390 / 525  Training Loss  0.10115967690944672\n",
            "Epoch  3 Batch  391 / 525  Training Loss  0.12678226828575134\n",
            "Epoch  3 Batch  392 / 525  Training Loss  0.13203029334545135\n",
            "Epoch  3 Batch  393 / 525  Training Loss  0.1183343380689621\n",
            "Epoch  3 Batch  394 / 525  Training Loss  0.11555857956409454\n",
            "Epoch  3 Batch  395 / 525  Training Loss  0.11123637855052948\n",
            "Epoch  3 Batch  396 / 525  Training Loss  0.14291957020759583\n",
            "Epoch  3 Batch  397 / 525  Training Loss  0.1305769383907318\n",
            "Epoch  3 Batch  398 / 525  Training Loss  0.1351235806941986\n",
            "Epoch  3 Batch  399 / 525  Training Loss  0.13173949718475342\n",
            "Epoch  3 Batch  400 / 525  Training Loss  0.14347462356090546\n",
            "Epoch  3 Batch  401 / 525  Training Loss  0.13732783496379852\n",
            "Epoch  3 Batch  402 / 525  Training Loss  0.11873166263103485\n",
            "Epoch  3 Batch  403 / 525  Training Loss  0.1494646668434143\n",
            "Epoch  3 Batch  404 / 525  Training Loss  0.13844361901283264\n",
            "Epoch  3 Batch  405 / 525  Training Loss  0.14298461377620697\n",
            "Epoch  3 Batch  406 / 525  Training Loss  0.12829609215259552\n",
            "Epoch  3 Batch  407 / 525  Training Loss  0.13105669617652893\n",
            "Epoch  3 Batch  408 / 525  Training Loss  0.10164711624383926\n",
            "Epoch  3 Batch  409 / 525  Training Loss  0.09376682341098785\n",
            "Epoch  3 Batch  410 / 525  Training Loss  0.11710550636053085\n",
            "Epoch  3 Batch  411 / 525  Training Loss  0.12318792194128036\n",
            "Epoch  3 Batch  412 / 525  Training Loss  0.1455375701189041\n",
            "Epoch  3 Batch  413 / 525  Training Loss  0.11284246295690536\n",
            "Epoch  3 Batch  414 / 525  Training Loss  0.1281907856464386\n",
            "Epoch  3 Batch  415 / 525  Training Loss  0.11816409975290298\n",
            "Epoch  3 Batch  416 / 525  Training Loss  0.13946665823459625\n",
            "Epoch  3 Batch  417 / 525  Training Loss  0.1300235092639923\n",
            "Epoch  3 Batch  418 / 525  Training Loss  0.12450212240219116\n",
            "Epoch  3 Batch  419 / 525  Training Loss  0.1400233805179596\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  3 Batch  420 / 525  Training Loss  0.1378973424434662\n",
            "Epoch  3 Batch  421 / 525  Training Loss  0.1456570029258728\n",
            "Epoch  3 Batch  422 / 525  Training Loss  0.10747895389795303\n",
            "Epoch  3 Batch  423 / 525  Training Loss  0.11926422268152237\n",
            "Epoch  3 Batch  424 / 525  Training Loss  0.12131984531879425\n",
            "Epoch  3 Batch  425 / 525  Training Loss  0.13763363659381866\n",
            "Epoch  3 Batch  426 / 525  Training Loss  0.12082849442958832\n",
            "Epoch  3 Batch  427 / 525  Training Loss  0.1228710189461708\n",
            "Epoch  3 Batch  428 / 525  Training Loss  0.1401478797197342\n",
            "Epoch  3 Batch  429 / 525  Training Loss  0.12724117934703827\n",
            "Epoch  3 Batch  430 / 525  Training Loss  0.13770391047000885\n",
            "Epoch  3 Batch  431 / 525  Training Loss  0.15847936272621155\n",
            "Epoch  3 Batch  432 / 525  Training Loss  0.13835954666137695\n",
            "Epoch  3 Batch  433 / 525  Training Loss  0.12265612185001373\n",
            "Epoch  3 Batch  434 / 525  Training Loss  0.12806132435798645\n",
            "Epoch  3 Batch  435 / 525  Training Loss  0.15810571610927582\n",
            "Epoch  3 Batch  436 / 525  Training Loss  0.1283533275127411\n",
            "Epoch  3 Batch  437 / 525  Training Loss  0.1305357962846756\n",
            "Epoch  3 Batch  438 / 525  Training Loss  0.134820356965065\n",
            "Epoch  3 Batch  439 / 525  Training Loss  0.11524860560894012\n",
            "Epoch  3 Batch  440 / 525  Training Loss  0.12871089577674866\n",
            "Epoch  3 Batch  441 / 525  Training Loss  0.13186663389205933\n",
            "Epoch  3 Batch  442 / 525  Training Loss  0.11318807303905487\n",
            "Epoch  3 Batch  443 / 525  Training Loss  0.12824495136737823\n",
            "Epoch  3 Batch  444 / 525  Training Loss  0.12259366363286972\n",
            "Epoch  3 Batch  445 / 525  Training Loss  0.12010879814624786\n",
            "Epoch  3 Batch  446 / 525  Training Loss  0.1242288202047348\n",
            "Epoch  3 Batch  447 / 525  Training Loss  0.13318577408790588\n",
            "Epoch  3 Batch  448 / 525  Training Loss  0.14157547056674957\n",
            "Epoch  3 Batch  449 / 525  Training Loss  0.1261119842529297\n",
            "Epoch  3 Batch  450 / 525  Training Loss  0.1338665932416916\n",
            "Epoch  3 Batch  451 / 525  Training Loss  0.14139671623706818\n",
            "Epoch  3 Batch  452 / 525  Training Loss  0.15069209039211273\n",
            "Epoch  3 Batch  453 / 525  Training Loss  0.13643372058868408\n",
            "Epoch  3 Batch  454 / 525  Training Loss  0.14261481165885925\n",
            "Epoch  3 Batch  455 / 525  Training Loss  0.12062583863735199\n",
            "Epoch  3 Batch  456 / 525  Training Loss  0.1455727517604828\n",
            "Epoch  3 Batch  457 / 525  Training Loss  0.11041072756052017\n",
            "Epoch  3 Batch  458 / 525  Training Loss  0.11731404066085815\n",
            "Epoch  3 Batch  459 / 525  Training Loss  0.11794398725032806\n",
            "Epoch  3 Batch  460 / 525  Training Loss  0.1327255219221115\n",
            "Epoch  3 Batch  461 / 525  Training Loss  0.12905827164649963\n",
            "Epoch  3 Batch  462 / 525  Training Loss  0.1117347702383995\n",
            "Epoch  3 Batch  463 / 525  Training Loss  0.13911905884742737\n",
            "Epoch  3 Batch  464 / 525  Training Loss  0.12187319993972778\n",
            "Epoch  3 Batch  465 / 525  Training Loss  0.12421026080846786\n",
            "Epoch  3 Batch  466 / 525  Training Loss  0.12726211547851562\n",
            "Epoch  3 Batch  467 / 525  Training Loss  0.13348326086997986\n",
            "Epoch  3 Batch  468 / 525  Training Loss  0.12942102551460266\n",
            "Epoch  3 Batch  469 / 525  Training Loss  0.12225339561700821\n",
            "Epoch  3 Batch  470 / 525  Training Loss  0.1315169334411621\n",
            "Epoch  3 Batch  471 / 525  Training Loss  0.15820357203483582\n",
            "Epoch  3 Batch  472 / 525  Training Loss  0.14095275104045868\n",
            "Epoch  3 Batch  473 / 525  Training Loss  0.1289454996585846\n",
            "Epoch  3 Batch  474 / 525  Training Loss  0.12833502888679504\n",
            "Epoch  3 Batch  475 / 525  Training Loss  0.1425240933895111\n",
            "Epoch  3 Batch  476 / 525  Training Loss  0.1284419298171997\n",
            "Epoch  3 Batch  477 / 525  Training Loss  0.11877510696649551\n",
            "Epoch  3 Batch  478 / 525  Training Loss  0.1253500133752823\n",
            "Epoch  3 Batch  479 / 525  Training Loss  0.12166664749383926\n",
            "Epoch  3 Batch  480 / 525  Training Loss  0.12221727520227432\n",
            "Epoch  3 Batch  481 / 525  Training Loss  0.12300962209701538\n",
            "Epoch  3 Batch  482 / 525  Training Loss  0.11811646074056625\n",
            "Epoch  3 Batch  483 / 525  Training Loss  0.12299849092960358\n",
            "Epoch  3 Batch  484 / 525  Training Loss  0.14529570937156677\n",
            "Epoch  3 Batch  485 / 525  Training Loss  0.126670241355896\n",
            "Epoch  3 Batch  486 / 525  Training Loss  0.1001541018486023\n",
            "Epoch  3 Batch  487 / 525  Training Loss  0.13435783982276917\n",
            "Epoch  3 Batch  488 / 525  Training Loss  0.1279756724834442\n",
            "Epoch  3 Batch  489 / 525  Training Loss  0.12027943134307861\n",
            "Epoch  3 Batch  490 / 525  Training Loss  0.12210305035114288\n",
            "Epoch  3 Batch  491 / 525  Training Loss  0.14474736154079437\n",
            "Epoch  3 Batch  492 / 525  Training Loss  0.14942970871925354\n",
            "Epoch  3 Batch  493 / 525  Training Loss  0.1500990390777588\n",
            "Epoch  3 Batch  494 / 525  Training Loss  0.12407144159078598\n",
            "Epoch  3 Batch  495 / 525  Training Loss  0.13863550126552582\n",
            "Epoch  3 Batch  496 / 525  Training Loss  0.152476966381073\n",
            "Epoch  3 Batch  497 / 525  Training Loss  0.1106114387512207\n",
            "Epoch  3 Batch  498 / 525  Training Loss  0.12780293822288513\n",
            "Epoch  3 Batch  499 / 525  Training Loss  0.14027926325798035\n",
            "Epoch  3 Batch  500 / 525  Training Loss  0.1278056800365448\n",
            "Epoch  3 Batch  501 / 525  Training Loss  0.13561959564685822\n",
            "Epoch  3 Batch  502 / 525  Training Loss  0.12126223742961884\n",
            "Epoch  3 Batch  503 / 525  Training Loss  0.120393767952919\n",
            "Epoch  3 Batch  504 / 525  Training Loss  0.12966057658195496\n",
            "Epoch  3 Batch  505 / 525  Training Loss  0.15039031207561493\n",
            "Epoch  3 Batch  506 / 525  Training Loss  0.1304025948047638\n",
            "Epoch  3 Batch  507 / 525  Training Loss  0.1313392072916031\n",
            "Epoch  3 Batch  508 / 525  Training Loss  0.11746484041213989\n",
            "Epoch  3 Batch  509 / 525  Training Loss  0.1204894557595253\n",
            "Epoch  3 Batch  510 / 525  Training Loss  0.15299825370311737\n",
            "Epoch  3 Batch  511 / 525  Training Loss  0.13629566133022308\n",
            "Epoch  3 Batch  512 / 525  Training Loss  0.14417611062526703\n",
            "Epoch  3 Batch  513 / 525  Training Loss  0.13631872832775116\n",
            "Epoch  3 Batch  514 / 525  Training Loss  0.12279029935598373\n",
            "Epoch  3 Batch  515 / 525  Training Loss  0.14795807003974915\n",
            "Epoch  3 Batch  516 / 525  Training Loss  0.10820160061120987\n",
            "Epoch  3 Batch  517 / 525  Training Loss  0.1145070418715477\n",
            "Epoch  3 Batch  518 / 525  Training Loss  0.12059702724218369\n",
            "Epoch  3 Batch  519 / 525  Training Loss  0.1155177503824234\n",
            "Epoch  3 Batch  520 / 525  Training Loss  0.13683554530143738\n",
            "Epoch  3 Batch  521 / 525  Training Loss  0.11874910444021225\n",
            "Epoch  3 Batch  522 / 525  Training Loss  0.12188247591257095\n",
            "Epoch  3 Batch  523 / 525  Training Loss  0.11598128080368042\n",
            "Epoch  3 Batch  524 / 525  Training Loss  0.13714642822742462\n",
            "   4    |    -    |   0.133999   |   20.07  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 4\n",
            "Epoch  4 Batch  0 / 525  Training Loss  0.1042606383562088\n",
            "Epoch  4 Batch  1 / 525  Training Loss  0.10536960512399673\n",
            "Epoch  4 Batch  2 / 525  Training Loss  0.09238801151514053\n",
            "Epoch  4 Batch  3 / 525  Training Loss  0.10014550387859344\n",
            "Epoch  4 Batch  4 / 525  Training Loss  0.10632996261119843\n",
            "Epoch  4 Batch  5 / 525  Training Loss  0.10287152230739594\n",
            "Epoch  4 Batch  6 / 525  Training Loss  0.11378280073404312\n",
            "Epoch  4 Batch  7 / 525  Training Loss  0.10545792430639267\n",
            "Epoch  4 Batch  8 / 525  Training Loss  0.12029178440570831\n",
            "Epoch  4 Batch  9 / 525  Training Loss  0.11307696253061295\n",
            "Epoch  4 Batch  10 / 525  Training Loss  0.1351875513792038\n",
            "Epoch  4 Batch  11 / 525  Training Loss  0.11605601012706757\n",
            "Epoch  4 Batch  12 / 525  Training Loss  0.11122183501720428\n",
            "Epoch  4 Batch  13 / 525  Training Loss  0.12638023495674133\n",
            "Epoch  4 Batch  14 / 525  Training Loss  0.11613041162490845\n",
            "Epoch  4 Batch  15 / 525  Training Loss  0.10580191761255264\n",
            "Epoch  4 Batch  16 / 525  Training Loss  0.12117409706115723\n",
            "Epoch  4 Batch  17 / 525  Training Loss  0.11111085116863251\n",
            "Epoch  4 Batch  18 / 525  Training Loss  0.11019831895828247\n",
            "Epoch  4 Batch  19 / 525  Training Loss  0.13111627101898193\n",
            "Epoch  4 Batch  20 / 525  Training Loss  0.08627612143754959\n",
            "Epoch  4 Batch  21 / 525  Training Loss  0.10522544384002686\n",
            "Epoch  4 Batch  22 / 525  Training Loss  0.10401000827550888\n",
            "Epoch  4 Batch  23 / 525  Training Loss  0.10877867788076401\n",
            "Epoch  4 Batch  24 / 525  Training Loss  0.09954775869846344\n",
            "Epoch  4 Batch  25 / 525  Training Loss  0.09561000764369965\n",
            "Epoch  4 Batch  26 / 525  Training Loss  0.10341060161590576\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  4 Batch  27 / 525  Training Loss  0.10886546224355698\n",
            "Epoch  4 Batch  28 / 525  Training Loss  0.09337826073169708\n",
            "Epoch  4 Batch  29 / 525  Training Loss  0.11654652655124664\n",
            "Epoch  4 Batch  30 / 525  Training Loss  0.10849175602197647\n",
            "Epoch  4 Batch  31 / 525  Training Loss  0.11581264436244965\n",
            "Epoch  4 Batch  32 / 525  Training Loss  0.13458526134490967\n",
            "Epoch  4 Batch  33 / 525  Training Loss  0.11377420276403427\n",
            "Epoch  4 Batch  34 / 525  Training Loss  0.11002274602651596\n",
            "Epoch  4 Batch  35 / 525  Training Loss  0.13174499571323395\n",
            "Epoch  4 Batch  36 / 525  Training Loss  0.11650212854146957\n",
            "Epoch  4 Batch  37 / 525  Training Loss  0.1210184097290039\n",
            "Epoch  4 Batch  38 / 525  Training Loss  0.11120418459177017\n",
            "Epoch  4 Batch  39 / 525  Training Loss  0.13331608474254608\n",
            "Epoch  4 Batch  40 / 525  Training Loss  0.11639174073934555\n",
            "Epoch  4 Batch  41 / 525  Training Loss  0.11407165229320526\n",
            "Epoch  4 Batch  42 / 525  Training Loss  0.11260636150836945\n",
            "Epoch  4 Batch  43 / 525  Training Loss  0.11335141956806183\n",
            "Epoch  4 Batch  44 / 525  Training Loss  0.08474965393543243\n",
            "Epoch  4 Batch  45 / 525  Training Loss  0.1009812206029892\n",
            "Epoch  4 Batch  46 / 525  Training Loss  0.10525496304035187\n",
            "Epoch  4 Batch  47 / 525  Training Loss  0.1068606972694397\n",
            "Epoch  4 Batch  48 / 525  Training Loss  0.1297636777162552\n",
            "Epoch  4 Batch  49 / 525  Training Loss  0.10954760015010834\n",
            "Epoch  4 Batch  50 / 525  Training Loss  0.10690219700336456\n",
            "Epoch  4 Batch  51 / 525  Training Loss  0.10886641591787338\n",
            "Epoch  4 Batch  52 / 525  Training Loss  0.11807973682880402\n",
            "Epoch  4 Batch  53 / 525  Training Loss  0.11791150271892548\n",
            "Epoch  4 Batch  54 / 525  Training Loss  0.10997293889522552\n",
            "Epoch  4 Batch  55 / 525  Training Loss  0.11448341608047485\n",
            "Epoch  4 Batch  56 / 525  Training Loss  0.10327483713626862\n",
            "Epoch  4 Batch  57 / 525  Training Loss  0.10737274587154388\n",
            "Epoch  4 Batch  58 / 525  Training Loss  0.12858042120933533\n",
            "Epoch  4 Batch  59 / 525  Training Loss  0.10606668144464493\n",
            "Epoch  4 Batch  60 / 525  Training Loss  0.11903730779886246\n",
            "Epoch  4 Batch  61 / 525  Training Loss  0.11097747087478638\n",
            "Epoch  4 Batch  62 / 525  Training Loss  0.12114261090755463\n",
            "Epoch  4 Batch  63 / 525  Training Loss  0.10807593166828156\n",
            "Epoch  4 Batch  64 / 525  Training Loss  0.09816218912601471\n",
            "Epoch  4 Batch  65 / 525  Training Loss  0.10635896027088165\n",
            "Epoch  4 Batch  66 / 525  Training Loss  0.10214662551879883\n",
            "Epoch  4 Batch  67 / 525  Training Loss  0.11032499372959137\n",
            "Epoch  4 Batch  68 / 525  Training Loss  0.10933138430118561\n",
            "Epoch  4 Batch  69 / 525  Training Loss  0.09162050485610962\n",
            "Epoch  4 Batch  70 / 525  Training Loss  0.09631092846393585\n",
            "Epoch  4 Batch  71 / 525  Training Loss  0.11484837532043457\n",
            "Epoch  4 Batch  72 / 525  Training Loss  0.09851100295782089\n",
            "Epoch  4 Batch  73 / 525  Training Loss  0.11187281459569931\n",
            "Epoch  4 Batch  74 / 525  Training Loss  0.10681519657373428\n",
            "Epoch  4 Batch  75 / 525  Training Loss  0.10771988332271576\n",
            "Epoch  4 Batch  76 / 525  Training Loss  0.09954766184091568\n",
            "Epoch  4 Batch  77 / 525  Training Loss  0.10742813348770142\n",
            "Epoch  4 Batch  78 / 525  Training Loss  0.09993360191583633\n",
            "Epoch  4 Batch  79 / 525  Training Loss  0.1235901489853859\n",
            "Epoch  4 Batch  80 / 525  Training Loss  0.11884985864162445\n",
            "Epoch  4 Batch  81 / 525  Training Loss  0.11392547935247421\n",
            "Epoch  4 Batch  82 / 525  Training Loss  0.12018802016973495\n",
            "Epoch  4 Batch  83 / 525  Training Loss  0.10397171974182129\n",
            "Epoch  4 Batch  84 / 525  Training Loss  0.10851272195577621\n",
            "Epoch  4 Batch  85 / 525  Training Loss  0.1093658059835434\n",
            "Epoch  4 Batch  86 / 525  Training Loss  0.08927957713603973\n",
            "Epoch  4 Batch  87 / 525  Training Loss  0.0866374522447586\n",
            "Epoch  4 Batch  88 / 525  Training Loss  0.0958608016371727\n",
            "Epoch  4 Batch  89 / 525  Training Loss  0.10476011037826538\n",
            "Epoch  4 Batch  90 / 525  Training Loss  0.11058435589075089\n",
            "Epoch  4 Batch  91 / 525  Training Loss  0.1125873327255249\n",
            "Epoch  4 Batch  92 / 525  Training Loss  0.10938438028097153\n",
            "Epoch  4 Batch  93 / 525  Training Loss  0.09768396615982056\n",
            "Epoch  4 Batch  94 / 525  Training Loss  0.10918699204921722\n",
            "Epoch  4 Batch  95 / 525  Training Loss  0.1199875921010971\n",
            "Epoch  4 Batch  96 / 525  Training Loss  0.10644763708114624\n",
            "Epoch  4 Batch  97 / 525  Training Loss  0.11903689801692963\n",
            "Epoch  4 Batch  98 / 525  Training Loss  0.10476361215114594\n",
            "Epoch  4 Batch  99 / 525  Training Loss  0.11414569616317749\n",
            "Epoch  4 Batch  100 / 525  Training Loss  0.10953108221292496\n",
            "Epoch  4 Batch  101 / 525  Training Loss  0.10222315788269043\n",
            "Epoch  4 Batch  102 / 525  Training Loss  0.11378949880599976\n",
            "Epoch  4 Batch  103 / 525  Training Loss  0.10518456995487213\n",
            "Epoch  4 Batch  104 / 525  Training Loss  0.11664743721485138\n",
            "Epoch  4 Batch  105 / 525  Training Loss  0.11633334308862686\n",
            "Epoch  4 Batch  106 / 525  Training Loss  0.10822074115276337\n",
            "Epoch  4 Batch  107 / 525  Training Loss  0.1193871945142746\n",
            "Epoch  4 Batch  108 / 525  Training Loss  0.09226671606302261\n",
            "Epoch  4 Batch  109 / 525  Training Loss  0.12989407777786255\n",
            "Epoch  4 Batch  110 / 525  Training Loss  0.1064736470580101\n",
            "Epoch  4 Batch  111 / 525  Training Loss  0.11056675761938095\n",
            "Epoch  4 Batch  112 / 525  Training Loss  0.10172393172979355\n",
            "Epoch  4 Batch  113 / 525  Training Loss  0.09979881346225739\n",
            "Epoch  4 Batch  114 / 525  Training Loss  0.11662914603948593\n",
            "Epoch  4 Batch  115 / 525  Training Loss  0.11708091199398041\n",
            "Epoch  4 Batch  116 / 525  Training Loss  0.10676904022693634\n",
            "Epoch  4 Batch  117 / 525  Training Loss  0.09616367518901825\n",
            "Epoch  4 Batch  118 / 525  Training Loss  0.10755603015422821\n",
            "Epoch  4 Batch  119 / 525  Training Loss  0.12110459804534912\n",
            "Epoch  4 Batch  120 / 525  Training Loss  0.11910686641931534\n",
            "Epoch  4 Batch  121 / 525  Training Loss  0.10183122009038925\n",
            "Epoch  4 Batch  122 / 525  Training Loss  0.13733749091625214\n",
            "Epoch  4 Batch  123 / 525  Training Loss  0.098384790122509\n",
            "Epoch  4 Batch  124 / 525  Training Loss  0.11450295150279999\n",
            "Epoch  4 Batch  125 / 525  Training Loss  0.11271943897008896\n",
            "Epoch  4 Batch  126 / 525  Training Loss  0.10986153781414032\n",
            "Epoch  4 Batch  127 / 525  Training Loss  0.1258809119462967\n",
            "Epoch  4 Batch  128 / 525  Training Loss  0.12067748606204987\n",
            "Epoch  4 Batch  129 / 525  Training Loss  0.12956787645816803\n",
            "Epoch  4 Batch  130 / 525  Training Loss  0.12497774511575699\n",
            "Epoch  4 Batch  131 / 525  Training Loss  0.10018172115087509\n",
            "Epoch  4 Batch  132 / 525  Training Loss  0.09415942430496216\n",
            "Epoch  4 Batch  133 / 525  Training Loss  0.12744800746440887\n",
            "Epoch  4 Batch  134 / 525  Training Loss  0.10923083871603012\n",
            "Epoch  4 Batch  135 / 525  Training Loss  0.08701930195093155\n",
            "Epoch  4 Batch  136 / 525  Training Loss  0.10418295860290527\n",
            "Epoch  4 Batch  137 / 525  Training Loss  0.10050271451473236\n",
            "Epoch  4 Batch  138 / 525  Training Loss  0.10697157680988312\n",
            "Epoch  4 Batch  139 / 525  Training Loss  0.11089693009853363\n",
            "Epoch  4 Batch  140 / 525  Training Loss  0.10687267780303955\n",
            "Epoch  4 Batch  141 / 525  Training Loss  0.11417597532272339\n",
            "Epoch  4 Batch  142 / 525  Training Loss  0.10923315584659576\n",
            "Epoch  4 Batch  143 / 525  Training Loss  0.1118323802947998\n",
            "Epoch  4 Batch  144 / 525  Training Loss  0.10326936095952988\n",
            "Epoch  4 Batch  145 / 525  Training Loss  0.10115107148885727\n",
            "Epoch  4 Batch  146 / 525  Training Loss  0.09959998726844788\n",
            "Epoch  4 Batch  147 / 525  Training Loss  0.10360604524612427\n",
            "Epoch  4 Batch  148 / 525  Training Loss  0.1050952821969986\n",
            "Epoch  4 Batch  149 / 525  Training Loss  0.0921027809381485\n",
            "Epoch  4 Batch  150 / 525  Training Loss  0.11809976398944855\n",
            "Epoch  4 Batch  151 / 525  Training Loss  0.10514857620000839\n",
            "Epoch  4 Batch  152 / 525  Training Loss  0.10909762233495712\n",
            "Epoch  4 Batch  153 / 525  Training Loss  0.09970102459192276\n",
            "Epoch  4 Batch  154 / 525  Training Loss  0.10673234611749649\n",
            "Epoch  4 Batch  155 / 525  Training Loss  0.09268109500408173\n",
            "Epoch  4 Batch  156 / 525  Training Loss  0.09755264222621918\n",
            "Epoch  4 Batch  157 / 525  Training Loss  0.12052728980779648\n",
            "Epoch  4 Batch  158 / 525  Training Loss  0.09325648099184036\n",
            "Epoch  4 Batch  159 / 525  Training Loss  0.1141042485833168\n",
            "Epoch  4 Batch  160 / 525  Training Loss  0.11427164077758789\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  4 Batch  161 / 525  Training Loss  0.10678000748157501\n",
            "Epoch  4 Batch  162 / 525  Training Loss  0.09587978571653366\n",
            "Epoch  4 Batch  163 / 525  Training Loss  0.1146843433380127\n",
            "Epoch  4 Batch  164 / 525  Training Loss  0.13302095234394073\n",
            "Epoch  4 Batch  165 / 525  Training Loss  0.12246517837047577\n",
            "Epoch  4 Batch  166 / 525  Training Loss  0.11645475775003433\n",
            "Epoch  4 Batch  167 / 525  Training Loss  0.1085149273276329\n",
            "Epoch  4 Batch  168 / 525  Training Loss  0.10345464944839478\n",
            "Epoch  4 Batch  169 / 525  Training Loss  0.09676126390695572\n",
            "Epoch  4 Batch  170 / 525  Training Loss  0.12449042499065399\n",
            "Epoch  4 Batch  171 / 525  Training Loss  0.12467348575592041\n",
            "Epoch  4 Batch  172 / 525  Training Loss  0.09893684089183807\n",
            "Epoch  4 Batch  173 / 525  Training Loss  0.10869748890399933\n",
            "Epoch  4 Batch  174 / 525  Training Loss  0.1288130134344101\n",
            "Epoch  4 Batch  175 / 525  Training Loss  0.09909756481647491\n",
            "Epoch  4 Batch  176 / 525  Training Loss  0.1121252179145813\n",
            "Epoch  4 Batch  177 / 525  Training Loss  0.12143133580684662\n",
            "Epoch  4 Batch  178 / 525  Training Loss  0.1137818694114685\n",
            "Epoch  4 Batch  179 / 525  Training Loss  0.09732628613710403\n",
            "Epoch  4 Batch  180 / 525  Training Loss  0.10039981454610825\n",
            "Epoch  4 Batch  181 / 525  Training Loss  0.13326214253902435\n",
            "Epoch  4 Batch  182 / 525  Training Loss  0.12648847699165344\n",
            "Epoch  4 Batch  183 / 525  Training Loss  0.09644748270511627\n",
            "Epoch  4 Batch  184 / 525  Training Loss  0.10744534432888031\n",
            "Epoch  4 Batch  185 / 525  Training Loss  0.11042597144842148\n",
            "Epoch  4 Batch  186 / 525  Training Loss  0.10776899755001068\n",
            "Epoch  4 Batch  187 / 525  Training Loss  0.10312280803918839\n",
            "Epoch  4 Batch  188 / 525  Training Loss  0.11393823474645615\n",
            "Epoch  4 Batch  189 / 525  Training Loss  0.10752608627080917\n",
            "Epoch  4 Batch  190 / 525  Training Loss  0.11326022446155548\n",
            "Epoch  4 Batch  191 / 525  Training Loss  0.09947273135185242\n",
            "Epoch  4 Batch  192 / 525  Training Loss  0.12715588510036469\n",
            "Epoch  4 Batch  193 / 525  Training Loss  0.14222365617752075\n",
            "Epoch  4 Batch  194 / 525  Training Loss  0.1191963329911232\n",
            "Epoch  4 Batch  195 / 525  Training Loss  0.11684806644916534\n",
            "Epoch  4 Batch  196 / 525  Training Loss  0.08873699605464935\n",
            "Epoch  4 Batch  197 / 525  Training Loss  0.10337014496326447\n",
            "Epoch  4 Batch  198 / 525  Training Loss  0.12478718906641006\n",
            "Epoch  4 Batch  199 / 525  Training Loss  0.11849156767129898\n",
            "Epoch  4 Batch  200 / 525  Training Loss  0.12720085680484772\n",
            "Epoch  4 Batch  201 / 525  Training Loss  0.13295529782772064\n",
            "Epoch  4 Batch  202 / 525  Training Loss  0.11682061851024628\n",
            "Epoch  4 Batch  203 / 525  Training Loss  0.1266239732503891\n",
            "Epoch  4 Batch  204 / 525  Training Loss  0.13755257427692413\n",
            "Epoch  4 Batch  205 / 525  Training Loss  0.11827445030212402\n",
            "Epoch  4 Batch  206 / 525  Training Loss  0.12296118587255478\n",
            "Epoch  4 Batch  207 / 525  Training Loss  0.12488933652639389\n",
            "Epoch  4 Batch  208 / 525  Training Loss  0.1119675263762474\n",
            "Epoch  4 Batch  209 / 525  Training Loss  0.10485432296991348\n",
            "Epoch  4 Batch  210 / 525  Training Loss  0.10532548278570175\n",
            "Epoch  4 Batch  211 / 525  Training Loss  0.11021263897418976\n",
            "Epoch  4 Batch  212 / 525  Training Loss  0.10673820972442627\n",
            "Epoch  4 Batch  213 / 525  Training Loss  0.11145894229412079\n",
            "Epoch  4 Batch  214 / 525  Training Loss  0.11731600761413574\n",
            "Epoch  4 Batch  215 / 525  Training Loss  0.0953994169831276\n",
            "Epoch  4 Batch  216 / 525  Training Loss  0.10376914590597153\n",
            "Epoch  4 Batch  217 / 525  Training Loss  0.10488004982471466\n",
            "Epoch  4 Batch  218 / 525  Training Loss  0.1208251491189003\n",
            "Epoch  4 Batch  219 / 525  Training Loss  0.12559321522712708\n",
            "Epoch  4 Batch  220 / 525  Training Loss  0.12131229788064957\n",
            "Epoch  4 Batch  221 / 525  Training Loss  0.1301194429397583\n",
            "Epoch  4 Batch  222 / 525  Training Loss  0.11263592541217804\n",
            "Epoch  4 Batch  223 / 525  Training Loss  0.12164529412984848\n",
            "Epoch  4 Batch  224 / 525  Training Loss  0.11901494115591049\n",
            "Epoch  4 Batch  225 / 525  Training Loss  0.10729371011257172\n",
            "Epoch  4 Batch  226 / 525  Training Loss  0.10480199754238129\n",
            "Epoch  4 Batch  227 / 525  Training Loss  0.12615838646888733\n",
            "Epoch  4 Batch  228 / 525  Training Loss  0.09378832578659058\n",
            "Epoch  4 Batch  229 / 525  Training Loss  0.10920281708240509\n",
            "Epoch  4 Batch  230 / 525  Training Loss  0.09361469000577927\n",
            "Epoch  4 Batch  231 / 525  Training Loss  0.14445550739765167\n",
            "Epoch  4 Batch  232 / 525  Training Loss  0.11539103835821152\n",
            "Epoch  4 Batch  233 / 525  Training Loss  0.10882169008255005\n",
            "Epoch  4 Batch  234 / 525  Training Loss  0.12021539360284805\n",
            "Epoch  4 Batch  235 / 525  Training Loss  0.0980641096830368\n",
            "Epoch  4 Batch  236 / 525  Training Loss  0.10791198909282684\n",
            "Epoch  4 Batch  237 / 525  Training Loss  0.10945729166269302\n",
            "Epoch  4 Batch  238 / 525  Training Loss  0.11051198095083237\n",
            "Epoch  4 Batch  239 / 525  Training Loss  0.11568969488143921\n",
            "Epoch  4 Batch  240 / 525  Training Loss  0.1030610203742981\n",
            "Epoch  4 Batch  241 / 525  Training Loss  0.08347100019454956\n",
            "Epoch  4 Batch  242 / 525  Training Loss  0.12301851809024811\n",
            "Epoch  4 Batch  243 / 525  Training Loss  0.12012715637683868\n",
            "Epoch  4 Batch  244 / 525  Training Loss  0.1069803386926651\n",
            "Epoch  4 Batch  245 / 525  Training Loss  0.12218385934829712\n",
            "Epoch  4 Batch  246 / 525  Training Loss  0.10763311386108398\n",
            "Epoch  4 Batch  247 / 525  Training Loss  0.1154743880033493\n",
            "Epoch  4 Batch  248 / 525  Training Loss  0.11390872299671173\n",
            "Epoch  4 Batch  249 / 525  Training Loss  0.13207072019577026\n",
            "Epoch  4 Batch  250 / 525  Training Loss  0.09935793280601501\n",
            "Epoch  4 Batch  251 / 525  Training Loss  0.1109950989484787\n",
            "Epoch  4 Batch  252 / 525  Training Loss  0.1291276067495346\n",
            "Epoch  4 Batch  253 / 525  Training Loss  0.11675538122653961\n",
            "Epoch  4 Batch  254 / 525  Training Loss  0.09371550381183624\n",
            "Epoch  4 Batch  255 / 525  Training Loss  0.09078367054462433\n",
            "Epoch  4 Batch  256 / 525  Training Loss  0.10380186140537262\n",
            "Epoch  4 Batch  257 / 525  Training Loss  0.1174798384308815\n",
            "Epoch  4 Batch  258 / 525  Training Loss  0.12619028985500336\n",
            "Epoch  4 Batch  259 / 525  Training Loss  0.1169937252998352\n",
            "Epoch  4 Batch  260 / 525  Training Loss  0.112101711332798\n",
            "Epoch  4 Batch  261 / 525  Training Loss  0.137398824095726\n",
            "Epoch  4 Batch  262 / 525  Training Loss  0.13065654039382935\n",
            "Epoch  4 Batch  263 / 525  Training Loss  0.10891763120889664\n",
            "Epoch  4 Batch  264 / 525  Training Loss  0.12155661731958389\n",
            "Epoch  4 Batch  265 / 525  Training Loss  0.12035024166107178\n",
            "Epoch  4 Batch  266 / 525  Training Loss  0.10102267563343048\n",
            "Epoch  4 Batch  267 / 525  Training Loss  0.120303213596344\n",
            "Epoch  4 Batch  268 / 525  Training Loss  0.12806347012519836\n",
            "Epoch  4 Batch  269 / 525  Training Loss  0.1128421202301979\n",
            "Epoch  4 Batch  270 / 525  Training Loss  0.11554618179798126\n",
            "Epoch  4 Batch  271 / 525  Training Loss  0.10071692615747452\n",
            "Epoch  4 Batch  272 / 525  Training Loss  0.10056553035974503\n",
            "Epoch  4 Batch  273 / 525  Training Loss  0.09889329224824905\n",
            "Epoch  4 Batch  274 / 525  Training Loss  0.08003421127796173\n",
            "Epoch  4 Batch  275 / 525  Training Loss  0.10933895409107208\n",
            "Epoch  4 Batch  276 / 525  Training Loss  0.10522842407226562\n",
            "Epoch  4 Batch  277 / 525  Training Loss  0.13230279088020325\n",
            "Epoch  4 Batch  278 / 525  Training Loss  0.11345343291759491\n",
            "Epoch  4 Batch  279 / 525  Training Loss  0.11209814250469208\n",
            "Epoch  4 Batch  280 / 525  Training Loss  0.09182409942150116\n",
            "Epoch  4 Batch  281 / 525  Training Loss  0.13623574376106262\n",
            "Epoch  4 Batch  282 / 525  Training Loss  0.08812782168388367\n",
            "Epoch  4 Batch  283 / 525  Training Loss  0.10933849960565567\n",
            "Epoch  4 Batch  284 / 525  Training Loss  0.09912387281656265\n",
            "Epoch  4 Batch  285 / 525  Training Loss  0.09719236195087433\n",
            "Epoch  4 Batch  286 / 525  Training Loss  0.11211974918842316\n",
            "Epoch  4 Batch  287 / 525  Training Loss  0.09997566789388657\n",
            "Epoch  4 Batch  288 / 525  Training Loss  0.12467122077941895\n",
            "Epoch  4 Batch  289 / 525  Training Loss  0.1131925955414772\n",
            "Epoch  4 Batch  290 / 525  Training Loss  0.11123903095722198\n",
            "Epoch  4 Batch  291 / 525  Training Loss  0.10651467740535736\n",
            "Epoch  4 Batch  292 / 525  Training Loss  0.11104851961135864\n",
            "Epoch  4 Batch  293 / 525  Training Loss  0.1198946014046669\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  4 Batch  294 / 525  Training Loss  0.10119907557964325\n",
            "Epoch  4 Batch  295 / 525  Training Loss  0.11252494901418686\n",
            "Epoch  4 Batch  296 / 525  Training Loss  0.09421879798173904\n",
            "Epoch  4 Batch  297 / 525  Training Loss  0.09583000838756561\n",
            "Epoch  4 Batch  298 / 525  Training Loss  0.10645415633916855\n",
            "Epoch  4 Batch  299 / 525  Training Loss  0.12008150666952133\n",
            "Epoch  4 Batch  300 / 525  Training Loss  0.14062544703483582\n",
            "Epoch  4 Batch  301 / 525  Training Loss  0.10697852075099945\n",
            "Epoch  4 Batch  302 / 525  Training Loss  0.09546231478452682\n",
            "Epoch  4 Batch  303 / 525  Training Loss  0.12826953828334808\n",
            "Epoch  4 Batch  304 / 525  Training Loss  0.10582774877548218\n",
            "Epoch  4 Batch  305 / 525  Training Loss  0.09280914068222046\n",
            "Epoch  4 Batch  306 / 525  Training Loss  0.10375167429447174\n",
            "Epoch  4 Batch  307 / 525  Training Loss  0.11628935486078262\n",
            "Epoch  4 Batch  308 / 525  Training Loss  0.09675426036119461\n",
            "Epoch  4 Batch  309 / 525  Training Loss  0.09746523201465607\n",
            "Epoch  4 Batch  310 / 525  Training Loss  0.10799497365951538\n",
            "Epoch  4 Batch  311 / 525  Training Loss  0.1238517314195633\n",
            "Epoch  4 Batch  312 / 525  Training Loss  0.11431439965963364\n",
            "Epoch  4 Batch  313 / 525  Training Loss  0.11154703795909882\n",
            "Epoch  4 Batch  314 / 525  Training Loss  0.1110813170671463\n",
            "Epoch  4 Batch  315 / 525  Training Loss  0.10555420070886612\n",
            "Epoch  4 Batch  316 / 525  Training Loss  0.1076904758810997\n",
            "Epoch  4 Batch  317 / 525  Training Loss  0.10294155776500702\n",
            "Epoch  4 Batch  318 / 525  Training Loss  0.11576022207736969\n",
            "Epoch  4 Batch  319 / 525  Training Loss  0.11228501796722412\n",
            "Epoch  4 Batch  320 / 525  Training Loss  0.10363969951868057\n",
            "Epoch  4 Batch  321 / 525  Training Loss  0.08744117617607117\n",
            "Epoch  4 Batch  322 / 525  Training Loss  0.09941411763429642\n",
            "Epoch  4 Batch  323 / 525  Training Loss  0.1414598971605301\n",
            "Epoch  4 Batch  324 / 525  Training Loss  0.10679557174444199\n",
            "Epoch  4 Batch  325 / 525  Training Loss  0.12381166219711304\n",
            "Epoch  4 Batch  326 / 525  Training Loss  0.10128921270370483\n",
            "Epoch  4 Batch  327 / 525  Training Loss  0.09031051397323608\n",
            "Epoch  4 Batch  328 / 525  Training Loss  0.11953771114349365\n",
            "Epoch  4 Batch  329 / 525  Training Loss  0.10188601166009903\n",
            "Epoch  4 Batch  330 / 525  Training Loss  0.12159313261508942\n",
            "Epoch  4 Batch  331 / 525  Training Loss  0.11962193250656128\n",
            "Epoch  4 Batch  332 / 525  Training Loss  0.11162708699703217\n",
            "Epoch  4 Batch  333 / 525  Training Loss  0.09813600033521652\n",
            "Epoch  4 Batch  334 / 525  Training Loss  0.11125675588846207\n",
            "Epoch  4 Batch  335 / 525  Training Loss  0.12493772804737091\n",
            "Epoch  4 Batch  336 / 525  Training Loss  0.10967199504375458\n",
            "Epoch  4 Batch  337 / 525  Training Loss  0.09381027519702911\n",
            "Epoch  4 Batch  338 / 525  Training Loss  0.11277377605438232\n",
            "Epoch  4 Batch  339 / 525  Training Loss  0.10149222612380981\n",
            "Epoch  4 Batch  340 / 525  Training Loss  0.09677362442016602\n",
            "Epoch  4 Batch  341 / 525  Training Loss  0.10408005863428116\n",
            "Epoch  4 Batch  342 / 525  Training Loss  0.11004482209682465\n",
            "Epoch  4 Batch  343 / 525  Training Loss  0.11540772020816803\n",
            "Epoch  4 Batch  344 / 525  Training Loss  0.11775700747966766\n",
            "Epoch  4 Batch  345 / 525  Training Loss  0.11829236894845963\n",
            "Epoch  4 Batch  346 / 525  Training Loss  0.1139553040266037\n",
            "Epoch  4 Batch  347 / 525  Training Loss  0.1342470347881317\n",
            "Epoch  4 Batch  348 / 525  Training Loss  0.11038126796483994\n",
            "Epoch  4 Batch  349 / 525  Training Loss  0.11630960553884506\n",
            "Epoch  4 Batch  350 / 525  Training Loss  0.11639650166034698\n",
            "Epoch  4 Batch  351 / 525  Training Loss  0.10392846912145615\n",
            "Epoch  4 Batch  352 / 525  Training Loss  0.10552990436553955\n",
            "Epoch  4 Batch  353 / 525  Training Loss  0.09566397964954376\n",
            "Epoch  4 Batch  354 / 525  Training Loss  0.10610716044902802\n",
            "Epoch  4 Batch  355 / 525  Training Loss  0.12114538252353668\n",
            "Epoch  4 Batch  356 / 525  Training Loss  0.1002425104379654\n",
            "Epoch  4 Batch  357 / 525  Training Loss  0.09277813136577606\n",
            "Epoch  4 Batch  358 / 525  Training Loss  0.11669407784938812\n",
            "Epoch  4 Batch  359 / 525  Training Loss  0.11872217804193497\n",
            "Epoch  4 Batch  360 / 525  Training Loss  0.11467502266168594\n",
            "Epoch  4 Batch  361 / 525  Training Loss  0.12487628310918808\n",
            "Epoch  4 Batch  362 / 525  Training Loss  0.1195414811372757\n",
            "Epoch  4 Batch  363 / 525  Training Loss  0.11932174116373062\n",
            "Epoch  4 Batch  364 / 525  Training Loss  0.1131206750869751\n",
            "Epoch  4 Batch  365 / 525  Training Loss  0.14017599821090698\n",
            "Epoch  4 Batch  366 / 525  Training Loss  0.09609893709421158\n",
            "Epoch  4 Batch  367 / 525  Training Loss  0.10387279838323593\n",
            "Epoch  4 Batch  368 / 525  Training Loss  0.1412362903356552\n",
            "Epoch  4 Batch  369 / 525  Training Loss  0.12875080108642578\n",
            "Epoch  4 Batch  370 / 525  Training Loss  0.1124328151345253\n",
            "Epoch  4 Batch  371 / 525  Training Loss  0.10697980225086212\n",
            "Epoch  4 Batch  372 / 525  Training Loss  0.0981588214635849\n",
            "Epoch  4 Batch  373 / 525  Training Loss  0.09050498157739639\n",
            "Epoch  4 Batch  374 / 525  Training Loss  0.10429936647415161\n",
            "Epoch  4 Batch  375 / 525  Training Loss  0.09762503951787949\n",
            "Epoch  4 Batch  376 / 525  Training Loss  0.09247009456157684\n",
            "Epoch  4 Batch  377 / 525  Training Loss  0.10090024769306183\n",
            "Epoch  4 Batch  378 / 525  Training Loss  0.08896053582429886\n",
            "Epoch  4 Batch  379 / 525  Training Loss  0.1174851506948471\n",
            "Epoch  4 Batch  380 / 525  Training Loss  0.11744179576635361\n",
            "Epoch  4 Batch  381 / 525  Training Loss  0.10513027012348175\n",
            "Epoch  4 Batch  382 / 525  Training Loss  0.10138402134180069\n",
            "Epoch  4 Batch  383 / 525  Training Loss  0.11625619232654572\n",
            "Epoch  4 Batch  384 / 525  Training Loss  0.1095937117934227\n",
            "Epoch  4 Batch  385 / 525  Training Loss  0.11927525699138641\n",
            "Epoch  4 Batch  386 / 525  Training Loss  0.08973731100559235\n",
            "Epoch  4 Batch  387 / 525  Training Loss  0.10647805035114288\n",
            "Epoch  4 Batch  388 / 525  Training Loss  0.11209925264120102\n",
            "Epoch  4 Batch  389 / 525  Training Loss  0.11112730205059052\n",
            "Epoch  4 Batch  390 / 525  Training Loss  0.1026935800909996\n",
            "Epoch  4 Batch  391 / 525  Training Loss  0.09941461682319641\n",
            "Epoch  4 Batch  392 / 525  Training Loss  0.1300649642944336\n",
            "Epoch  4 Batch  393 / 525  Training Loss  0.1124718189239502\n",
            "Epoch  4 Batch  394 / 525  Training Loss  0.12038189172744751\n",
            "Epoch  4 Batch  395 / 525  Training Loss  0.10397789627313614\n",
            "Epoch  4 Batch  396 / 525  Training Loss  0.09921414405107498\n",
            "Epoch  4 Batch  397 / 525  Training Loss  0.09523765742778778\n",
            "Epoch  4 Batch  398 / 525  Training Loss  0.11663702875375748\n",
            "Epoch  4 Batch  399 / 525  Training Loss  0.11436215788125992\n",
            "Epoch  4 Batch  400 / 525  Training Loss  0.11299820244312286\n",
            "Epoch  4 Batch  401 / 525  Training Loss  0.10721717029809952\n",
            "Epoch  4 Batch  402 / 525  Training Loss  0.09150980412960052\n",
            "Epoch  4 Batch  403 / 525  Training Loss  0.10910762846469879\n",
            "Epoch  4 Batch  404 / 525  Training Loss  0.11131443828344345\n",
            "Epoch  4 Batch  405 / 525  Training Loss  0.11074040830135345\n",
            "Epoch  4 Batch  406 / 525  Training Loss  0.1267782747745514\n",
            "Epoch  4 Batch  407 / 525  Training Loss  0.10529857873916626\n",
            "Epoch  4 Batch  408 / 525  Training Loss  0.11026809364557266\n",
            "Epoch  4 Batch  409 / 525  Training Loss  0.10041578859090805\n",
            "Epoch  4 Batch  410 / 525  Training Loss  0.10055488348007202\n",
            "Epoch  4 Batch  411 / 525  Training Loss  0.10060682147741318\n",
            "Epoch  4 Batch  412 / 525  Training Loss  0.11460970342159271\n",
            "Epoch  4 Batch  413 / 525  Training Loss  0.09362418949604034\n",
            "Epoch  4 Batch  414 / 525  Training Loss  0.08894883096218109\n",
            "Epoch  4 Batch  415 / 525  Training Loss  0.10745514929294586\n",
            "Epoch  4 Batch  416 / 525  Training Loss  0.11122511327266693\n",
            "Epoch  4 Batch  417 / 525  Training Loss  0.12012200057506561\n",
            "Epoch  4 Batch  418 / 525  Training Loss  0.11621497571468353\n",
            "Epoch  4 Batch  419 / 525  Training Loss  0.10299873352050781\n",
            "Epoch  4 Batch  420 / 525  Training Loss  0.10322264581918716\n",
            "Epoch  4 Batch  421 / 525  Training Loss  0.11728677898645401\n",
            "Epoch  4 Batch  422 / 525  Training Loss  0.08725220710039139\n",
            "Epoch  4 Batch  423 / 525  Training Loss  0.09609098732471466\n",
            "Epoch  4 Batch  424 / 525  Training Loss  0.10546859353780746\n",
            "Epoch  4 Batch  425 / 525  Training Loss  0.10300525277853012\n",
            "Epoch  4 Batch  426 / 525  Training Loss  0.12237425148487091\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  4 Batch  427 / 525  Training Loss  0.1023990735411644\n",
            "Epoch  4 Batch  428 / 525  Training Loss  0.08442921936511993\n",
            "Epoch  4 Batch  429 / 525  Training Loss  0.10562791675329208\n",
            "Epoch  4 Batch  430 / 525  Training Loss  0.09856618195772171\n",
            "Epoch  4 Batch  431 / 525  Training Loss  0.09368561208248138\n",
            "Epoch  4 Batch  432 / 525  Training Loss  0.11383087933063507\n",
            "Epoch  4 Batch  433 / 525  Training Loss  0.11359940469264984\n",
            "Epoch  4 Batch  434 / 525  Training Loss  0.10662480443716049\n",
            "Epoch  4 Batch  435 / 525  Training Loss  0.10777218639850616\n",
            "Epoch  4 Batch  436 / 525  Training Loss  0.11207970231771469\n",
            "Epoch  4 Batch  437 / 525  Training Loss  0.10852662473917007\n",
            "Epoch  4 Batch  438 / 525  Training Loss  0.09720619022846222\n",
            "Epoch  4 Batch  439 / 525  Training Loss  0.10491319745779037\n",
            "Epoch  4 Batch  440 / 525  Training Loss  0.1148306354880333\n",
            "Epoch  4 Batch  441 / 525  Training Loss  0.11590715497732162\n",
            "Epoch  4 Batch  442 / 525  Training Loss  0.11129653453826904\n",
            "Epoch  4 Batch  443 / 525  Training Loss  0.10158544778823853\n",
            "Epoch  4 Batch  444 / 525  Training Loss  0.09510257095098495\n",
            "Epoch  4 Batch  445 / 525  Training Loss  0.12724409997463226\n",
            "Epoch  4 Batch  446 / 525  Training Loss  0.1074158102273941\n",
            "Epoch  4 Batch  447 / 525  Training Loss  0.12271088361740112\n",
            "Epoch  4 Batch  448 / 525  Training Loss  0.11369254440069199\n",
            "Epoch  4 Batch  449 / 525  Training Loss  0.11441825330257416\n",
            "Epoch  4 Batch  450 / 525  Training Loss  0.09506218880414963\n",
            "Epoch  4 Batch  451 / 525  Training Loss  0.09951633214950562\n",
            "Epoch  4 Batch  452 / 525  Training Loss  0.09825949370861053\n",
            "Epoch  4 Batch  453 / 525  Training Loss  0.11232896149158478\n",
            "Epoch  4 Batch  454 / 525  Training Loss  0.10392360389232635\n",
            "Epoch  4 Batch  455 / 525  Training Loss  0.10896531492471695\n",
            "Epoch  4 Batch  456 / 525  Training Loss  0.10671879351139069\n",
            "Epoch  4 Batch  457 / 525  Training Loss  0.09299848228693008\n",
            "Epoch  4 Batch  458 / 525  Training Loss  0.11832854896783829\n",
            "Epoch  4 Batch  459 / 525  Training Loss  0.10052946954965591\n",
            "Epoch  4 Batch  460 / 525  Training Loss  0.09973515570163727\n",
            "Epoch  4 Batch  461 / 525  Training Loss  0.11255602538585663\n",
            "Epoch  4 Batch  462 / 525  Training Loss  0.11221466213464737\n",
            "Epoch  4 Batch  463 / 525  Training Loss  0.09994931519031525\n",
            "Epoch  4 Batch  464 / 525  Training Loss  0.10196484625339508\n",
            "Epoch  4 Batch  465 / 525  Training Loss  0.1007913202047348\n",
            "Epoch  4 Batch  466 / 525  Training Loss  0.087396040558815\n",
            "Epoch  4 Batch  467 / 525  Training Loss  0.11448211967945099\n",
            "Epoch  4 Batch  468 / 525  Training Loss  0.10592895746231079\n",
            "Epoch  4 Batch  469 / 525  Training Loss  0.09616711735725403\n",
            "Epoch  4 Batch  470 / 525  Training Loss  0.11884450912475586\n",
            "Epoch  4 Batch  471 / 525  Training Loss  0.08881131559610367\n",
            "Epoch  4 Batch  472 / 525  Training Loss  0.12625309824943542\n",
            "Epoch  4 Batch  473 / 525  Training Loss  0.13038890063762665\n",
            "Epoch  4 Batch  474 / 525  Training Loss  0.10553111881017685\n",
            "Epoch  4 Batch  475 / 525  Training Loss  0.09257795661687851\n",
            "Epoch  4 Batch  476 / 525  Training Loss  0.11579658836126328\n",
            "Epoch  4 Batch  477 / 525  Training Loss  0.11383682489395142\n",
            "Epoch  4 Batch  478 / 525  Training Loss  0.1132955402135849\n",
            "Epoch  4 Batch  479 / 525  Training Loss  0.09496675431728363\n",
            "Epoch  4 Batch  480 / 525  Training Loss  0.09235114604234695\n",
            "Epoch  4 Batch  481 / 525  Training Loss  0.1014181599020958\n",
            "Epoch  4 Batch  482 / 525  Training Loss  0.0963771790266037\n",
            "Epoch  4 Batch  483 / 525  Training Loss  0.11736472696065903\n",
            "Epoch  4 Batch  484 / 525  Training Loss  0.09333823621273041\n",
            "Epoch  4 Batch  485 / 525  Training Loss  0.1129719614982605\n",
            "Epoch  4 Batch  486 / 525  Training Loss  0.10118796676397324\n",
            "Epoch  4 Batch  487 / 525  Training Loss  0.12587468326091766\n",
            "Epoch  4 Batch  488 / 525  Training Loss  0.10862050950527191\n",
            "Epoch  4 Batch  489 / 525  Training Loss  0.12008108198642731\n",
            "Epoch  4 Batch  490 / 525  Training Loss  0.0764714926481247\n",
            "Epoch  4 Batch  491 / 525  Training Loss  0.10139711946249008\n",
            "Epoch  4 Batch  492 / 525  Training Loss  0.10834042727947235\n",
            "Epoch  4 Batch  493 / 525  Training Loss  0.0999760776758194\n",
            "Epoch  4 Batch  494 / 525  Training Loss  0.10377571731805801\n",
            "Epoch  4 Batch  495 / 525  Training Loss  0.10823898017406464\n",
            "Epoch  4 Batch  496 / 525  Training Loss  0.08974091708660126\n",
            "Epoch  4 Batch  497 / 525  Training Loss  0.09995075315237045\n",
            "Epoch  4 Batch  498 / 525  Training Loss  0.12015392631292343\n",
            "Epoch  4 Batch  499 / 525  Training Loss  0.09379880130290985\n",
            "Epoch  4 Batch  500 / 525  Training Loss  0.10977156460285187\n",
            "Epoch  4 Batch  501 / 525  Training Loss  0.09853837639093399\n",
            "Epoch  4 Batch  502 / 525  Training Loss  0.08076829463243484\n",
            "Epoch  4 Batch  503 / 525  Training Loss  0.09620286524295807\n",
            "Epoch  4 Batch  504 / 525  Training Loss  0.09786541759967804\n",
            "Epoch  4 Batch  505 / 525  Training Loss  0.10500834137201309\n",
            "Epoch  4 Batch  506 / 525  Training Loss  0.08911154419183731\n",
            "Epoch  4 Batch  507 / 525  Training Loss  0.11906423419713974\n",
            "Epoch  4 Batch  508 / 525  Training Loss  0.11015912145376205\n",
            "Epoch  4 Batch  509 / 525  Training Loss  0.10855011641979218\n",
            "Epoch  4 Batch  510 / 525  Training Loss  0.10988923162221909\n",
            "Epoch  4 Batch  511 / 525  Training Loss  0.1072581559419632\n",
            "Epoch  4 Batch  512 / 525  Training Loss  0.10198184102773666\n",
            "Epoch  4 Batch  513 / 525  Training Loss  0.1061553955078125\n",
            "Epoch  4 Batch  514 / 525  Training Loss  0.11315419524908066\n",
            "Epoch  4 Batch  515 / 525  Training Loss  0.11642014980316162\n",
            "Epoch  4 Batch  516 / 525  Training Loss  0.10080637782812119\n",
            "Epoch  4 Batch  517 / 525  Training Loss  0.12173809856176376\n",
            "Epoch  4 Batch  518 / 525  Training Loss  0.11133785545825958\n",
            "Epoch  4 Batch  519 / 525  Training Loss  0.10330817848443985\n",
            "Epoch  4 Batch  520 / 525  Training Loss  0.11143467575311661\n",
            "Epoch  4 Batch  521 / 525  Training Loss  0.12247347831726074\n",
            "Epoch  4 Batch  522 / 525  Training Loss  0.11236003786325455\n",
            "Epoch  4 Batch  523 / 525  Training Loss  0.11017205566167831\n",
            "Epoch  4 Batch  524 / 525  Training Loss  0.10725601762533188\n",
            "   5    |    -    |   0.109538   |   26.16  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 5\n",
            "Epoch  5 Batch  0 / 525  Training Loss  0.08885563910007477\n",
            "Epoch  5 Batch  1 / 525  Training Loss  0.10056296736001968\n",
            "Epoch  5 Batch  2 / 525  Training Loss  0.08005498349666595\n",
            "Epoch  5 Batch  3 / 525  Training Loss  0.1054341048002243\n",
            "Epoch  5 Batch  4 / 525  Training Loss  0.08900581300258636\n",
            "Epoch  5 Batch  5 / 525  Training Loss  0.11346562206745148\n",
            "Epoch  5 Batch  6 / 525  Training Loss  0.07672182470560074\n",
            "Epoch  5 Batch  7 / 525  Training Loss  0.06743514537811279\n",
            "Epoch  5 Batch  8 / 525  Training Loss  0.1006769984960556\n",
            "Epoch  5 Batch  9 / 525  Training Loss  0.10772547870874405\n",
            "Epoch  5 Batch  10 / 525  Training Loss  0.07802993059158325\n",
            "Epoch  5 Batch  11 / 525  Training Loss  0.09294597804546356\n",
            "Epoch  5 Batch  12 / 525  Training Loss  0.09182822704315186\n",
            "Epoch  5 Batch  13 / 525  Training Loss  0.08470819890499115\n",
            "Epoch  5 Batch  14 / 525  Training Loss  0.08099730312824249\n",
            "Epoch  5 Batch  15 / 525  Training Loss  0.08230810612440109\n",
            "Epoch  5 Batch  16 / 525  Training Loss  0.0910320058465004\n",
            "Epoch  5 Batch  17 / 525  Training Loss  0.10032516717910767\n",
            "Epoch  5 Batch  18 / 525  Training Loss  0.0855911523103714\n",
            "Epoch  5 Batch  19 / 525  Training Loss  0.09288512170314789\n",
            "Epoch  5 Batch  20 / 525  Training Loss  0.09506095200777054\n",
            "Epoch  5 Batch  21 / 525  Training Loss  0.07496816664934158\n",
            "Epoch  5 Batch  22 / 525  Training Loss  0.09447725862264633\n",
            "Epoch  5 Batch  23 / 525  Training Loss  0.11053534597158432\n",
            "Epoch  5 Batch  24 / 525  Training Loss  0.10650993883609772\n",
            "Epoch  5 Batch  25 / 525  Training Loss  0.08565036952495575\n",
            "Epoch  5 Batch  26 / 525  Training Loss  0.08907078206539154\n",
            "Epoch  5 Batch  27 / 525  Training Loss  0.06969437748193741\n",
            "Epoch  5 Batch  28 / 525  Training Loss  0.08387832343578339\n",
            "Epoch  5 Batch  29 / 525  Training Loss  0.0779588371515274\n",
            "Epoch  5 Batch  30 / 525  Training Loss  0.07910121977329254\n",
            "Epoch  5 Batch  31 / 525  Training Loss  0.0994187593460083\n",
            "Epoch  5 Batch  32 / 525  Training Loss  0.09427124261856079\n",
            "Epoch  5 Batch  33 / 525  Training Loss  0.09316502511501312\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  5 Batch  34 / 525  Training Loss  0.09468907117843628\n",
            "Epoch  5 Batch  35 / 525  Training Loss  0.08733000606298447\n",
            "Epoch  5 Batch  36 / 525  Training Loss  0.1098460927605629\n",
            "Epoch  5 Batch  37 / 525  Training Loss  0.09888587146997452\n",
            "Epoch  5 Batch  38 / 525  Training Loss  0.09061704576015472\n",
            "Epoch  5 Batch  39 / 525  Training Loss  0.09007395803928375\n",
            "Epoch  5 Batch  40 / 525  Training Loss  0.09262941777706146\n",
            "Epoch  5 Batch  41 / 525  Training Loss  0.0905037671327591\n",
            "Epoch  5 Batch  42 / 525  Training Loss  0.07936473190784454\n",
            "Epoch  5 Batch  43 / 525  Training Loss  0.08248814940452576\n",
            "Epoch  5 Batch  44 / 525  Training Loss  0.08193963766098022\n",
            "Epoch  5 Batch  45 / 525  Training Loss  0.08045629411935806\n",
            "Epoch  5 Batch  46 / 525  Training Loss  0.07328714430332184\n",
            "Epoch  5 Batch  47 / 525  Training Loss  0.07798202335834503\n",
            "Epoch  5 Batch  48 / 525  Training Loss  0.11280231177806854\n",
            "Epoch  5 Batch  49 / 525  Training Loss  0.08817560970783234\n",
            "Epoch  5 Batch  50 / 525  Training Loss  0.08319234848022461\n",
            "Epoch  5 Batch  51 / 525  Training Loss  0.08615010976791382\n",
            "Epoch  5 Batch  52 / 525  Training Loss  0.07558067888021469\n",
            "Epoch  5 Batch  53 / 525  Training Loss  0.071040078997612\n",
            "Epoch  5 Batch  54 / 525  Training Loss  0.08182110637426376\n",
            "Epoch  5 Batch  55 / 525  Training Loss  0.09326004981994629\n",
            "Epoch  5 Batch  56 / 525  Training Loss  0.0886223167181015\n",
            "Epoch  5 Batch  57 / 525  Training Loss  0.09353046864271164\n",
            "Epoch  5 Batch  58 / 525  Training Loss  0.08547951281070709\n",
            "Epoch  5 Batch  59 / 525  Training Loss  0.12232507765293121\n",
            "Epoch  5 Batch  60 / 525  Training Loss  0.09048724174499512\n",
            "Epoch  5 Batch  61 / 525  Training Loss  0.08322644233703613\n",
            "Epoch  5 Batch  62 / 525  Training Loss  0.06816593557596207\n",
            "Epoch  5 Batch  63 / 525  Training Loss  0.08230958878993988\n",
            "Epoch  5 Batch  64 / 525  Training Loss  0.10172512382268906\n",
            "Epoch  5 Batch  65 / 525  Training Loss  0.10392143577337265\n",
            "Epoch  5 Batch  66 / 525  Training Loss  0.07950522750616074\n",
            "Epoch  5 Batch  67 / 525  Training Loss  0.09072975814342499\n",
            "Epoch  5 Batch  68 / 525  Training Loss  0.07356279343366623\n",
            "Epoch  5 Batch  69 / 525  Training Loss  0.08273009955883026\n",
            "Epoch  5 Batch  70 / 525  Training Loss  0.06469141691923141\n",
            "Epoch  5 Batch  71 / 525  Training Loss  0.07547742128372192\n",
            "Epoch  5 Batch  72 / 525  Training Loss  0.08502461016178131\n",
            "Epoch  5 Batch  73 / 525  Training Loss  0.07815425097942352\n",
            "Epoch  5 Batch  74 / 525  Training Loss  0.08631326258182526\n",
            "Epoch  5 Batch  75 / 525  Training Loss  0.08238038420677185\n",
            "Epoch  5 Batch  76 / 525  Training Loss  0.07653234899044037\n",
            "Epoch  5 Batch  77 / 525  Training Loss  0.0982016772031784\n",
            "Epoch  5 Batch  78 / 525  Training Loss  0.08639524132013321\n",
            "Epoch  5 Batch  79 / 525  Training Loss  0.08722509443759918\n",
            "Epoch  5 Batch  80 / 525  Training Loss  0.07954398542642593\n",
            "Epoch  5 Batch  81 / 525  Training Loss  0.08389388769865036\n",
            "Epoch  5 Batch  82 / 525  Training Loss  0.09471199661493301\n",
            "Epoch  5 Batch  83 / 525  Training Loss  0.08408385515213013\n",
            "Epoch  5 Batch  84 / 525  Training Loss  0.09978891909122467\n",
            "Epoch  5 Batch  85 / 525  Training Loss  0.09272237122058868\n",
            "Epoch  5 Batch  86 / 525  Training Loss  0.08413691818714142\n",
            "Epoch  5 Batch  87 / 525  Training Loss  0.05698511004447937\n",
            "Epoch  5 Batch  88 / 525  Training Loss  0.07575173676013947\n",
            "Epoch  5 Batch  89 / 525  Training Loss  0.09387640655040741\n",
            "Epoch  5 Batch  90 / 525  Training Loss  0.09091704338788986\n",
            "Epoch  5 Batch  91 / 525  Training Loss  0.1089213639497757\n",
            "Epoch  5 Batch  92 / 525  Training Loss  0.09721602499485016\n",
            "Epoch  5 Batch  93 / 525  Training Loss  0.08555175364017487\n",
            "Epoch  5 Batch  94 / 525  Training Loss  0.08221884071826935\n",
            "Epoch  5 Batch  95 / 525  Training Loss  0.09670688211917877\n",
            "Epoch  5 Batch  96 / 525  Training Loss  0.07351817190647125\n",
            "Epoch  5 Batch  97 / 525  Training Loss  0.08428069204092026\n",
            "Epoch  5 Batch  98 / 525  Training Loss  0.10605863481760025\n",
            "Epoch  5 Batch  99 / 525  Training Loss  0.11118175834417343\n",
            "Epoch  5 Batch  100 / 525  Training Loss  0.08043073117733002\n",
            "Epoch  5 Batch  101 / 525  Training Loss  0.07935240864753723\n",
            "Epoch  5 Batch  102 / 525  Training Loss  0.09267733991146088\n",
            "Epoch  5 Batch  103 / 525  Training Loss  0.08990070968866348\n",
            "Epoch  5 Batch  104 / 525  Training Loss  0.09872528910636902\n",
            "Epoch  5 Batch  105 / 525  Training Loss  0.10673630237579346\n",
            "Epoch  5 Batch  106 / 525  Training Loss  0.07655574381351471\n",
            "Epoch  5 Batch  107 / 525  Training Loss  0.0904240533709526\n",
            "Epoch  5 Batch  108 / 525  Training Loss  0.07488995790481567\n",
            "Epoch  5 Batch  109 / 525  Training Loss  0.08916351944208145\n",
            "Epoch  5 Batch  110 / 525  Training Loss  0.08863969147205353\n",
            "Epoch  5 Batch  111 / 525  Training Loss  0.10691394656896591\n",
            "Epoch  5 Batch  112 / 525  Training Loss  0.07456866651773453\n",
            "Epoch  5 Batch  113 / 525  Training Loss  0.11518724262714386\n",
            "Epoch  5 Batch  114 / 525  Training Loss  0.06890847533941269\n",
            "Epoch  5 Batch  115 / 525  Training Loss  0.09365526586771011\n",
            "Epoch  5 Batch  116 / 525  Training Loss  0.08579333871603012\n",
            "Epoch  5 Batch  117 / 525  Training Loss  0.09534803032875061\n",
            "Epoch  5 Batch  118 / 525  Training Loss  0.09535645693540573\n",
            "Epoch  5 Batch  119 / 525  Training Loss  0.09327972680330276\n",
            "Epoch  5 Batch  120 / 525  Training Loss  0.07942022383213043\n",
            "Epoch  5 Batch  121 / 525  Training Loss  0.09783688187599182\n",
            "Epoch  5 Batch  122 / 525  Training Loss  0.07509205490350723\n",
            "Epoch  5 Batch  123 / 525  Training Loss  0.10708098113536835\n",
            "Epoch  5 Batch  124 / 525  Training Loss  0.09794479608535767\n",
            "Epoch  5 Batch  125 / 525  Training Loss  0.10781644284725189\n",
            "Epoch  5 Batch  126 / 525  Training Loss  0.08257583528757095\n",
            "Epoch  5 Batch  127 / 525  Training Loss  0.06661015003919601\n",
            "Epoch  5 Batch  128 / 525  Training Loss  0.09906716644763947\n",
            "Epoch  5 Batch  129 / 525  Training Loss  0.09229502826929092\n",
            "Epoch  5 Batch  130 / 525  Training Loss  0.08732324093580246\n",
            "Epoch  5 Batch  131 / 525  Training Loss  0.08688916265964508\n",
            "Epoch  5 Batch  132 / 525  Training Loss  0.07400460541248322\n",
            "Epoch  5 Batch  133 / 525  Training Loss  0.10400811582803726\n",
            "Epoch  5 Batch  134 / 525  Training Loss  0.07945360243320465\n",
            "Epoch  5 Batch  135 / 525  Training Loss  0.08062984049320221\n",
            "Epoch  5 Batch  136 / 525  Training Loss  0.0788288339972496\n",
            "Epoch  5 Batch  137 / 525  Training Loss  0.09230847656726837\n",
            "Epoch  5 Batch  138 / 525  Training Loss  0.10220448672771454\n",
            "Epoch  5 Batch  139 / 525  Training Loss  0.10470519214868546\n",
            "Epoch  5 Batch  140 / 525  Training Loss  0.07774437963962555\n",
            "Epoch  5 Batch  141 / 525  Training Loss  0.07706393301486969\n",
            "Epoch  5 Batch  142 / 525  Training Loss  0.09247259050607681\n",
            "Epoch  5 Batch  143 / 525  Training Loss  0.08581160753965378\n",
            "Epoch  5 Batch  144 / 525  Training Loss  0.10044445842504501\n",
            "Epoch  5 Batch  145 / 525  Training Loss  0.07555347681045532\n",
            "Epoch  5 Batch  146 / 525  Training Loss  0.09104100614786148\n",
            "Epoch  5 Batch  147 / 525  Training Loss  0.08861245959997177\n",
            "Epoch  5 Batch  148 / 525  Training Loss  0.08648959547281265\n",
            "Epoch  5 Batch  149 / 525  Training Loss  0.0751451849937439\n",
            "Epoch  5 Batch  150 / 525  Training Loss  0.08504097163677216\n",
            "Epoch  5 Batch  151 / 525  Training Loss  0.11281232535839081\n",
            "Epoch  5 Batch  152 / 525  Training Loss  0.09854292124509811\n",
            "Epoch  5 Batch  153 / 525  Training Loss  0.08506663143634796\n",
            "Epoch  5 Batch  154 / 525  Training Loss  0.08762641251087189\n",
            "Epoch  5 Batch  155 / 525  Training Loss  0.07748465240001678\n",
            "Epoch  5 Batch  156 / 525  Training Loss  0.08722933381795883\n",
            "Epoch  5 Batch  157 / 525  Training Loss  0.08360156416893005\n",
            "Epoch  5 Batch  158 / 525  Training Loss  0.10282035171985626\n",
            "Epoch  5 Batch  159 / 525  Training Loss  0.07640610635280609\n",
            "Epoch  5 Batch  160 / 525  Training Loss  0.095472052693367\n",
            "Epoch  5 Batch  161 / 525  Training Loss  0.09299745410680771\n",
            "Epoch  5 Batch  162 / 525  Training Loss  0.09320776164531708\n",
            "Epoch  5 Batch  163 / 525  Training Loss  0.08369234949350357\n",
            "Epoch  5 Batch  164 / 525  Training Loss  0.08251814544200897\n",
            "Epoch  5 Batch  165 / 525  Training Loss  0.09579938650131226\n",
            "Epoch  5 Batch  166 / 525  Training Loss  0.09530549496412277\n",
            "Epoch  5 Batch  167 / 525  Training Loss  0.06752513349056244\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  5 Batch  168 / 525  Training Loss  0.11034269630908966\n",
            "Epoch  5 Batch  169 / 525  Training Loss  0.09385932981967926\n",
            "Epoch  5 Batch  170 / 525  Training Loss  0.08424799144268036\n",
            "Epoch  5 Batch  171 / 525  Training Loss  0.08864222466945648\n",
            "Epoch  5 Batch  172 / 525  Training Loss  0.08148663491010666\n",
            "Epoch  5 Batch  173 / 525  Training Loss  0.09486470371484756\n",
            "Epoch  5 Batch  174 / 525  Training Loss  0.09638726711273193\n",
            "Epoch  5 Batch  175 / 525  Training Loss  0.0862220972776413\n",
            "Epoch  5 Batch  176 / 525  Training Loss  0.10222715139389038\n",
            "Epoch  5 Batch  177 / 525  Training Loss  0.0941181480884552\n",
            "Epoch  5 Batch  178 / 525  Training Loss  0.09315988421440125\n",
            "Epoch  5 Batch  179 / 525  Training Loss  0.07913308590650558\n",
            "Epoch  5 Batch  180 / 525  Training Loss  0.0834578275680542\n",
            "Epoch  5 Batch  181 / 525  Training Loss  0.0879177451133728\n",
            "Epoch  5 Batch  182 / 525  Training Loss  0.0977829098701477\n",
            "Epoch  5 Batch  183 / 525  Training Loss  0.08690953254699707\n",
            "Epoch  5 Batch  184 / 525  Training Loss  0.11804598569869995\n",
            "Epoch  5 Batch  185 / 525  Training Loss  0.08758236467838287\n",
            "Epoch  5 Batch  186 / 525  Training Loss  0.08294330537319183\n",
            "Epoch  5 Batch  187 / 525  Training Loss  0.08734352141618729\n",
            "Epoch  5 Batch  188 / 525  Training Loss  0.11472245305776596\n",
            "Epoch  5 Batch  189 / 525  Training Loss  0.0867094025015831\n",
            "Epoch  5 Batch  190 / 525  Training Loss  0.08550631254911423\n",
            "Epoch  5 Batch  191 / 525  Training Loss  0.10100331157445908\n",
            "Epoch  5 Batch  192 / 525  Training Loss  0.1135457381606102\n",
            "Epoch  5 Batch  193 / 525  Training Loss  0.09186217933893204\n",
            "Epoch  5 Batch  194 / 525  Training Loss  0.08571857959032059\n",
            "Epoch  5 Batch  195 / 525  Training Loss  0.11011854559183121\n",
            "Epoch  5 Batch  196 / 525  Training Loss  0.08810754120349884\n",
            "Epoch  5 Batch  197 / 525  Training Loss  0.08401231467723846\n",
            "Epoch  5 Batch  198 / 525  Training Loss  0.0789298266172409\n",
            "Epoch  5 Batch  199 / 525  Training Loss  0.07475284487009048\n",
            "Epoch  5 Batch  200 / 525  Training Loss  0.10035371780395508\n",
            "Epoch  5 Batch  201 / 525  Training Loss  0.07625870406627655\n",
            "Epoch  5 Batch  202 / 525  Training Loss  0.10374822467565536\n",
            "Epoch  5 Batch  203 / 525  Training Loss  0.06649667769670486\n",
            "Epoch  5 Batch  204 / 525  Training Loss  0.09120742976665497\n",
            "Epoch  5 Batch  205 / 525  Training Loss  0.09973547607660294\n",
            "Epoch  5 Batch  206 / 525  Training Loss  0.10635950416326523\n",
            "Epoch  5 Batch  207 / 525  Training Loss  0.10270986706018448\n",
            "Epoch  5 Batch  208 / 525  Training Loss  0.1090206652879715\n",
            "Epoch  5 Batch  209 / 525  Training Loss  0.06098323315382004\n",
            "Epoch  5 Batch  210 / 525  Training Loss  0.08379006385803223\n",
            "Epoch  5 Batch  211 / 525  Training Loss  0.08540501445531845\n",
            "Epoch  5 Batch  212 / 525  Training Loss  0.0800873339176178\n",
            "Epoch  5 Batch  213 / 525  Training Loss  0.08168170601129532\n",
            "Epoch  5 Batch  214 / 525  Training Loss  0.08979935199022293\n",
            "Epoch  5 Batch  215 / 525  Training Loss  0.08321380615234375\n",
            "Epoch  5 Batch  216 / 525  Training Loss  0.09393270313739777\n",
            "Epoch  5 Batch  217 / 525  Training Loss  0.09027083218097687\n",
            "Epoch  5 Batch  218 / 525  Training Loss  0.08792465925216675\n",
            "Epoch  5 Batch  219 / 525  Training Loss  0.09135880321264267\n",
            "Epoch  5 Batch  220 / 525  Training Loss  0.0944448858499527\n",
            "Epoch  5 Batch  221 / 525  Training Loss  0.12585648894309998\n",
            "Epoch  5 Batch  222 / 525  Training Loss  0.0944618508219719\n",
            "Epoch  5 Batch  223 / 525  Training Loss  0.0836104303598404\n",
            "Epoch  5 Batch  224 / 525  Training Loss  0.08779918402433395\n",
            "Epoch  5 Batch  225 / 525  Training Loss  0.10350574553012848\n",
            "Epoch  5 Batch  226 / 525  Training Loss  0.08718103915452957\n",
            "Epoch  5 Batch  227 / 525  Training Loss  0.09523525834083557\n",
            "Epoch  5 Batch  228 / 525  Training Loss  0.08614744991064072\n",
            "Epoch  5 Batch  229 / 525  Training Loss  0.0998246967792511\n",
            "Epoch  5 Batch  230 / 525  Training Loss  0.09095732122659683\n",
            "Epoch  5 Batch  231 / 525  Training Loss  0.10470855236053467\n",
            "Epoch  5 Batch  232 / 525  Training Loss  0.10007300227880478\n",
            "Epoch  5 Batch  233 / 525  Training Loss  0.10067051649093628\n",
            "Epoch  5 Batch  234 / 525  Training Loss  0.07542584091424942\n",
            "Epoch  5 Batch  235 / 525  Training Loss  0.12069401890039444\n",
            "Epoch  5 Batch  236 / 525  Training Loss  0.08924262225627899\n",
            "Epoch  5 Batch  237 / 525  Training Loss  0.0896025225520134\n",
            "Epoch  5 Batch  238 / 525  Training Loss  0.10579810291528702\n",
            "Epoch  5 Batch  239 / 525  Training Loss  0.08801586925983429\n",
            "Epoch  5 Batch  240 / 525  Training Loss  0.061416782438755035\n",
            "Epoch  5 Batch  241 / 525  Training Loss  0.07491051405668259\n",
            "Epoch  5 Batch  242 / 525  Training Loss  0.11414225399494171\n",
            "Epoch  5 Batch  243 / 525  Training Loss  0.08401887863874435\n",
            "Epoch  5 Batch  244 / 525  Training Loss  0.08274619281291962\n",
            "Epoch  5 Batch  245 / 525  Training Loss  0.0852116122841835\n",
            "Epoch  5 Batch  246 / 525  Training Loss  0.0849357470870018\n",
            "Epoch  5 Batch  247 / 525  Training Loss  0.08422105014324188\n",
            "Epoch  5 Batch  248 / 525  Training Loss  0.08401481807231903\n",
            "Epoch  5 Batch  249 / 525  Training Loss  0.10361625999212265\n",
            "Epoch  5 Batch  250 / 525  Training Loss  0.09413452446460724\n",
            "Epoch  5 Batch  251 / 525  Training Loss  0.07031123340129852\n",
            "Epoch  5 Batch  252 / 525  Training Loss  0.10519315302371979\n",
            "Epoch  5 Batch  253 / 525  Training Loss  0.09519447386264801\n",
            "Epoch  5 Batch  254 / 525  Training Loss  0.083362877368927\n",
            "Epoch  5 Batch  255 / 525  Training Loss  0.09358061850070953\n",
            "Epoch  5 Batch  256 / 525  Training Loss  0.08721724897623062\n",
            "Epoch  5 Batch  257 / 525  Training Loss  0.1078333705663681\n",
            "Epoch  5 Batch  258 / 525  Training Loss  0.09889565408229828\n",
            "Epoch  5 Batch  259 / 525  Training Loss  0.09278266131877899\n",
            "Epoch  5 Batch  260 / 525  Training Loss  0.1023487001657486\n",
            "Epoch  5 Batch  261 / 525  Training Loss  0.09654717147350311\n",
            "Epoch  5 Batch  262 / 525  Training Loss  0.08633995056152344\n",
            "Epoch  5 Batch  263 / 525  Training Loss  0.0731891617178917\n",
            "Epoch  5 Batch  264 / 525  Training Loss  0.06501834839582443\n",
            "Epoch  5 Batch  265 / 525  Training Loss  0.09639240801334381\n",
            "Epoch  5 Batch  266 / 525  Training Loss  0.09417428821325302\n",
            "Epoch  5 Batch  267 / 525  Training Loss  0.07339083403348923\n",
            "Epoch  5 Batch  268 / 525  Training Loss  0.10018501430749893\n",
            "Epoch  5 Batch  269 / 525  Training Loss  0.08125246316194534\n",
            "Epoch  5 Batch  270 / 525  Training Loss  0.1012825146317482\n",
            "Epoch  5 Batch  271 / 525  Training Loss  0.08556222170591354\n",
            "Epoch  5 Batch  272 / 525  Training Loss  0.10531699657440186\n",
            "Epoch  5 Batch  273 / 525  Training Loss  0.0957241803407669\n",
            "Epoch  5 Batch  274 / 525  Training Loss  0.08805150538682938\n",
            "Epoch  5 Batch  275 / 525  Training Loss  0.0912884995341301\n",
            "Epoch  5 Batch  276 / 525  Training Loss  0.10537855327129364\n",
            "Epoch  5 Batch  277 / 525  Training Loss  0.07955419272184372\n",
            "Epoch  5 Batch  278 / 525  Training Loss  0.09061683714389801\n",
            "Epoch  5 Batch  279 / 525  Training Loss  0.09196456521749496\n",
            "Epoch  5 Batch  280 / 525  Training Loss  0.07301422953605652\n",
            "Epoch  5 Batch  281 / 525  Training Loss  0.06909900903701782\n",
            "Epoch  5 Batch  282 / 525  Training Loss  0.09540660679340363\n",
            "Epoch  5 Batch  283 / 525  Training Loss  0.095794178545475\n",
            "Epoch  5 Batch  284 / 525  Training Loss  0.08862073719501495\n",
            "Epoch  5 Batch  285 / 525  Training Loss  0.10688450187444687\n",
            "Epoch  5 Batch  286 / 525  Training Loss  0.0945865735411644\n",
            "Epoch  5 Batch  287 / 525  Training Loss  0.0817766785621643\n",
            "Epoch  5 Batch  288 / 525  Training Loss  0.09766825288534164\n",
            "Epoch  5 Batch  289 / 525  Training Loss  0.10252463817596436\n",
            "Epoch  5 Batch  290 / 525  Training Loss  0.07218438386917114\n",
            "Epoch  5 Batch  291 / 525  Training Loss  0.08580703288316727\n",
            "Epoch  5 Batch  292 / 525  Training Loss  0.09217742085456848\n",
            "Epoch  5 Batch  293 / 525  Training Loss  0.08841505646705627\n",
            "Epoch  5 Batch  294 / 525  Training Loss  0.0761934295296669\n",
            "Epoch  5 Batch  295 / 525  Training Loss  0.10107626765966415\n",
            "Epoch  5 Batch  296 / 525  Training Loss  0.09341812133789062\n",
            "Epoch  5 Batch  297 / 525  Training Loss  0.09689217805862427\n",
            "Epoch  5 Batch  298 / 525  Training Loss  0.11552609503269196\n",
            "Epoch  5 Batch  299 / 525  Training Loss  0.06984928995370865\n",
            "Epoch  5 Batch  300 / 525  Training Loss  0.09616845101118088\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  5 Batch  301 / 525  Training Loss  0.08384723216295242\n",
            "Epoch  5 Batch  302 / 525  Training Loss  0.08099978417158127\n",
            "Epoch  5 Batch  303 / 525  Training Loss  0.07641962915658951\n",
            "Epoch  5 Batch  304 / 525  Training Loss  0.09064022451639175\n",
            "Epoch  5 Batch  305 / 525  Training Loss  0.09981327503919601\n",
            "Epoch  5 Batch  306 / 525  Training Loss  0.08808162063360214\n",
            "Epoch  5 Batch  307 / 525  Training Loss  0.09356522560119629\n",
            "Epoch  5 Batch  308 / 525  Training Loss  0.09454961121082306\n",
            "Epoch  5 Batch  309 / 525  Training Loss  0.08994177728891373\n",
            "Epoch  5 Batch  310 / 525  Training Loss  0.10610765218734741\n",
            "Epoch  5 Batch  311 / 525  Training Loss  0.09023548662662506\n",
            "Epoch  5 Batch  312 / 525  Training Loss  0.06574799865484238\n",
            "Epoch  5 Batch  313 / 525  Training Loss  0.1031666025519371\n",
            "Epoch  5 Batch  314 / 525  Training Loss  0.07376839220523834\n",
            "Epoch  5 Batch  315 / 525  Training Loss  0.092330701649189\n",
            "Epoch  5 Batch  316 / 525  Training Loss  0.0732930600643158\n",
            "Epoch  5 Batch  317 / 525  Training Loss  0.0815231129527092\n",
            "Epoch  5 Batch  318 / 525  Training Loss  0.09537980705499649\n",
            "Epoch  5 Batch  319 / 525  Training Loss  0.09545224159955978\n",
            "Epoch  5 Batch  320 / 525  Training Loss  0.08258800208568573\n",
            "Epoch  5 Batch  321 / 525  Training Loss  0.07018992304801941\n",
            "Epoch  5 Batch  322 / 525  Training Loss  0.09256647527217865\n",
            "Epoch  5 Batch  323 / 525  Training Loss  0.08032412827014923\n",
            "Epoch  5 Batch  324 / 525  Training Loss  0.07322045415639877\n",
            "Epoch  5 Batch  325 / 525  Training Loss  0.08876625448465347\n",
            "Epoch  5 Batch  326 / 525  Training Loss  0.08998467028141022\n",
            "Epoch  5 Batch  327 / 525  Training Loss  0.11446844041347504\n",
            "Epoch  5 Batch  328 / 525  Training Loss  0.09455092996358871\n",
            "Epoch  5 Batch  329 / 525  Training Loss  0.07156442105770111\n",
            "Epoch  5 Batch  330 / 525  Training Loss  0.08711715042591095\n",
            "Epoch  5 Batch  331 / 525  Training Loss  0.0773731917142868\n",
            "Epoch  5 Batch  332 / 525  Training Loss  0.07536883652210236\n",
            "Epoch  5 Batch  333 / 525  Training Loss  0.08369626849889755\n",
            "Epoch  5 Batch  334 / 525  Training Loss  0.07732462882995605\n",
            "Epoch  5 Batch  335 / 525  Training Loss  0.0965287983417511\n",
            "Epoch  5 Batch  336 / 525  Training Loss  0.08581118285655975\n",
            "Epoch  5 Batch  337 / 525  Training Loss  0.05637132376432419\n",
            "Epoch  5 Batch  338 / 525  Training Loss  0.08299209922552109\n",
            "Epoch  5 Batch  339 / 525  Training Loss  0.08250094950199127\n",
            "Epoch  5 Batch  340 / 525  Training Loss  0.06840048730373383\n",
            "Epoch  5 Batch  341 / 525  Training Loss  0.09180203080177307\n",
            "Epoch  5 Batch  342 / 525  Training Loss  0.0890074297785759\n",
            "Epoch  5 Batch  343 / 525  Training Loss  0.08731075376272202\n",
            "Epoch  5 Batch  344 / 525  Training Loss  0.08995556086301804\n",
            "Epoch  5 Batch  345 / 525  Training Loss  0.07401420176029205\n",
            "Epoch  5 Batch  346 / 525  Training Loss  0.07656191289424896\n",
            "Epoch  5 Batch  347 / 525  Training Loss  0.09430798143148422\n",
            "Epoch  5 Batch  348 / 525  Training Loss  0.08811035007238388\n",
            "Epoch  5 Batch  349 / 525  Training Loss  0.09216639399528503\n",
            "Epoch  5 Batch  350 / 525  Training Loss  0.08464476466178894\n",
            "Epoch  5 Batch  351 / 525  Training Loss  0.08858160674571991\n",
            "Epoch  5 Batch  352 / 525  Training Loss  0.09758609533309937\n",
            "Epoch  5 Batch  353 / 525  Training Loss  0.0757659524679184\n",
            "Epoch  5 Batch  354 / 525  Training Loss  0.07880067080259323\n",
            "Epoch  5 Batch  355 / 525  Training Loss  0.07412002235651016\n",
            "Epoch  5 Batch  356 / 525  Training Loss  0.1027253121137619\n",
            "Epoch  5 Batch  357 / 525  Training Loss  0.10118462890386581\n",
            "Epoch  5 Batch  358 / 525  Training Loss  0.08393550664186478\n",
            "Epoch  5 Batch  359 / 525  Training Loss  0.09259911626577377\n",
            "Epoch  5 Batch  360 / 525  Training Loss  0.09384806454181671\n",
            "Epoch  5 Batch  361 / 525  Training Loss  0.10774600505828857\n",
            "Epoch  5 Batch  362 / 525  Training Loss  0.09223073720932007\n",
            "Epoch  5 Batch  363 / 525  Training Loss  0.09260259568691254\n",
            "Epoch  5 Batch  364 / 525  Training Loss  0.10111769288778305\n",
            "Epoch  5 Batch  365 / 525  Training Loss  0.06400218605995178\n",
            "Epoch  5 Batch  366 / 525  Training Loss  0.08752146363258362\n",
            "Epoch  5 Batch  367 / 525  Training Loss  0.10407643020153046\n",
            "Epoch  5 Batch  368 / 525  Training Loss  0.08784505724906921\n",
            "Epoch  5 Batch  369 / 525  Training Loss  0.1364758312702179\n",
            "Epoch  5 Batch  370 / 525  Training Loss  0.08086179196834564\n",
            "Epoch  5 Batch  371 / 525  Training Loss  0.09893733263015747\n",
            "Epoch  5 Batch  372 / 525  Training Loss  0.0996757298707962\n",
            "Epoch  5 Batch  373 / 525  Training Loss  0.11663933843374252\n",
            "Epoch  5 Batch  374 / 525  Training Loss  0.09412409365177155\n",
            "Epoch  5 Batch  375 / 525  Training Loss  0.09579214453697205\n",
            "Epoch  5 Batch  376 / 525  Training Loss  0.09525088965892792\n",
            "Epoch  5 Batch  377 / 525  Training Loss  0.0867529958486557\n",
            "Epoch  5 Batch  378 / 525  Training Loss  0.08198940008878708\n",
            "Epoch  5 Batch  379 / 525  Training Loss  0.102897509932518\n",
            "Epoch  5 Batch  380 / 525  Training Loss  0.09068843722343445\n",
            "Epoch  5 Batch  381 / 525  Training Loss  0.1003509908914566\n",
            "Epoch  5 Batch  382 / 525  Training Loss  0.09681045264005661\n",
            "Epoch  5 Batch  383 / 525  Training Loss  0.07952598482370377\n",
            "Epoch  5 Batch  384 / 525  Training Loss  0.0914929062128067\n",
            "Epoch  5 Batch  385 / 525  Training Loss  0.08222310990095139\n",
            "Epoch  5 Batch  386 / 525  Training Loss  0.09386126697063446\n",
            "Epoch  5 Batch  387 / 525  Training Loss  0.09630399197340012\n",
            "Epoch  5 Batch  388 / 525  Training Loss  0.07152444124221802\n",
            "Epoch  5 Batch  389 / 525  Training Loss  0.07005710899829865\n",
            "Epoch  5 Batch  390 / 525  Training Loss  0.06461890041828156\n",
            "Epoch  5 Batch  391 / 525  Training Loss  0.09586355090141296\n",
            "Epoch  5 Batch  392 / 525  Training Loss  0.08157622814178467\n",
            "Epoch  5 Batch  393 / 525  Training Loss  0.08534292876720428\n",
            "Epoch  5 Batch  394 / 525  Training Loss  0.09792111814022064\n",
            "Epoch  5 Batch  395 / 525  Training Loss  0.0705784484744072\n",
            "Epoch  5 Batch  396 / 525  Training Loss  0.07414913922548294\n",
            "Epoch  5 Batch  397 / 525  Training Loss  0.07728629559278488\n",
            "Epoch  5 Batch  398 / 525  Training Loss  0.08907680213451385\n",
            "Epoch  5 Batch  399 / 525  Training Loss  0.09916312992572784\n",
            "Epoch  5 Batch  400 / 525  Training Loss  0.09047738462686539\n",
            "Epoch  5 Batch  401 / 525  Training Loss  0.08011342585086823\n",
            "Epoch  5 Batch  402 / 525  Training Loss  0.09385571628808975\n",
            "Epoch  5 Batch  403 / 525  Training Loss  0.0834910050034523\n",
            "Epoch  5 Batch  404 / 525  Training Loss  0.0916265919804573\n",
            "Epoch  5 Batch  405 / 525  Training Loss  0.07550295442342758\n",
            "Epoch  5 Batch  406 / 525  Training Loss  0.08462652564048767\n",
            "Epoch  5 Batch  407 / 525  Training Loss  0.08447272330522537\n",
            "Epoch  5 Batch  408 / 525  Training Loss  0.10288602113723755\n",
            "Epoch  5 Batch  409 / 525  Training Loss  0.07810015976428986\n",
            "Epoch  5 Batch  410 / 525  Training Loss  0.07976540923118591\n",
            "Epoch  5 Batch  411 / 525  Training Loss  0.10060246288776398\n",
            "Epoch  5 Batch  412 / 525  Training Loss  0.08524440973997116\n",
            "Epoch  5 Batch  413 / 525  Training Loss  0.08066491782665253\n",
            "Epoch  5 Batch  414 / 525  Training Loss  0.09018861502408981\n",
            "Epoch  5 Batch  415 / 525  Training Loss  0.08487382531166077\n",
            "Epoch  5 Batch  416 / 525  Training Loss  0.07758928090333939\n",
            "Epoch  5 Batch  417 / 525  Training Loss  0.0940723568201065\n",
            "Epoch  5 Batch  418 / 525  Training Loss  0.11497534811496735\n",
            "Epoch  5 Batch  419 / 525  Training Loss  0.09649167954921722\n",
            "Epoch  5 Batch  420 / 525  Training Loss  0.08256354928016663\n",
            "Epoch  5 Batch  421 / 525  Training Loss  0.0693468302488327\n",
            "Epoch  5 Batch  422 / 525  Training Loss  0.08826198428869247\n",
            "Epoch  5 Batch  423 / 525  Training Loss  0.12109355628490448\n",
            "Epoch  5 Batch  424 / 525  Training Loss  0.0805874615907669\n",
            "Epoch  5 Batch  425 / 525  Training Loss  0.07556365430355072\n",
            "Epoch  5 Batch  426 / 525  Training Loss  0.09479144960641861\n",
            "Epoch  5 Batch  427 / 525  Training Loss  0.1012946143746376\n",
            "Epoch  5 Batch  428 / 525  Training Loss  0.07713621854782104\n",
            "Epoch  5 Batch  429 / 525  Training Loss  0.060134582221508026\n",
            "Epoch  5 Batch  430 / 525  Training Loss  0.09510870277881622\n",
            "Epoch  5 Batch  431 / 525  Training Loss  0.08781237155199051\n",
            "Epoch  5 Batch  432 / 525  Training Loss  0.07821868360042572\n",
            "Epoch  5 Batch  433 / 525  Training Loss  0.07933943718671799\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  5 Batch  434 / 525  Training Loss  0.08763327449560165\n",
            "Epoch  5 Batch  435 / 525  Training Loss  0.0942460149526596\n",
            "Epoch  5 Batch  436 / 525  Training Loss  0.10668037086725235\n",
            "Epoch  5 Batch  437 / 525  Training Loss  0.08449384570121765\n",
            "Epoch  5 Batch  438 / 525  Training Loss  0.09000501036643982\n",
            "Epoch  5 Batch  439 / 525  Training Loss  0.07671626657247543\n",
            "Epoch  5 Batch  440 / 525  Training Loss  0.08914349973201752\n",
            "Epoch  5 Batch  441 / 525  Training Loss  0.08125440031290054\n",
            "Epoch  5 Batch  442 / 525  Training Loss  0.06409428268671036\n",
            "Epoch  5 Batch  443 / 525  Training Loss  0.06688432395458221\n",
            "Epoch  5 Batch  444 / 525  Training Loss  0.07548811286687851\n",
            "Epoch  5 Batch  445 / 525  Training Loss  0.08551547676324844\n",
            "Epoch  5 Batch  446 / 525  Training Loss  0.07688641548156738\n",
            "Epoch  5 Batch  447 / 525  Training Loss  0.09574674069881439\n",
            "Epoch  5 Batch  448 / 525  Training Loss  0.10014859586954117\n",
            "Epoch  5 Batch  449 / 525  Training Loss  0.07116682082414627\n",
            "Epoch  5 Batch  450 / 525  Training Loss  0.07379955053329468\n",
            "Epoch  5 Batch  451 / 525  Training Loss  0.08470779657363892\n",
            "Epoch  5 Batch  452 / 525  Training Loss  0.07113880664110184\n",
            "Epoch  5 Batch  453 / 525  Training Loss  0.10725060850381851\n",
            "Epoch  5 Batch  454 / 525  Training Loss  0.09163092076778412\n",
            "Epoch  5 Batch  455 / 525  Training Loss  0.08508692681789398\n",
            "Epoch  5 Batch  456 / 525  Training Loss  0.09348775446414948\n",
            "Epoch  5 Batch  457 / 525  Training Loss  0.08430685847997665\n",
            "Epoch  5 Batch  458 / 525  Training Loss  0.07125373184680939\n",
            "Epoch  5 Batch  459 / 525  Training Loss  0.10400209575891495\n",
            "Epoch  5 Batch  460 / 525  Training Loss  0.08935800194740295\n",
            "Epoch  5 Batch  461 / 525  Training Loss  0.08109304308891296\n",
            "Epoch  5 Batch  462 / 525  Training Loss  0.09641577303409576\n",
            "Epoch  5 Batch  463 / 525  Training Loss  0.10258112102746964\n",
            "Epoch  5 Batch  464 / 525  Training Loss  0.0823078379034996\n",
            "Epoch  5 Batch  465 / 525  Training Loss  0.094994455575943\n",
            "Epoch  5 Batch  466 / 525  Training Loss  0.08153460919857025\n",
            "Epoch  5 Batch  467 / 525  Training Loss  0.08851861953735352\n",
            "Epoch  5 Batch  468 / 525  Training Loss  0.09997563064098358\n",
            "Epoch  5 Batch  469 / 525  Training Loss  0.0928792729973793\n",
            "Epoch  5 Batch  470 / 525  Training Loss  0.1130506619811058\n",
            "Epoch  5 Batch  471 / 525  Training Loss  0.09031395614147186\n",
            "Epoch  5 Batch  472 / 525  Training Loss  0.0929841697216034\n",
            "Epoch  5 Batch  473 / 525  Training Loss  0.08287133276462555\n",
            "Epoch  5 Batch  474 / 525  Training Loss  0.09119071066379547\n",
            "Epoch  5 Batch  475 / 525  Training Loss  0.0791555792093277\n",
            "Epoch  5 Batch  476 / 525  Training Loss  0.07806552201509476\n",
            "Epoch  5 Batch  477 / 525  Training Loss  0.08529267460107803\n",
            "Epoch  5 Batch  478 / 525  Training Loss  0.10058154165744781\n",
            "Epoch  5 Batch  479 / 525  Training Loss  0.11549849808216095\n",
            "Epoch  5 Batch  480 / 525  Training Loss  0.09124144166707993\n",
            "Epoch  5 Batch  481 / 525  Training Loss  0.08627857267856598\n",
            "Epoch  5 Batch  482 / 525  Training Loss  0.06522754579782486\n",
            "Epoch  5 Batch  483 / 525  Training Loss  0.0829768031835556\n",
            "Epoch  5 Batch  484 / 525  Training Loss  0.08634644001722336\n",
            "Epoch  5 Batch  485 / 525  Training Loss  0.09677119553089142\n",
            "Epoch  5 Batch  486 / 525  Training Loss  0.0962628647685051\n",
            "Epoch  5 Batch  487 / 525  Training Loss  0.07961685210466385\n",
            "Epoch  5 Batch  488 / 525  Training Loss  0.09037773311138153\n",
            "Epoch  5 Batch  489 / 525  Training Loss  0.09100373089313507\n",
            "Epoch  5 Batch  490 / 525  Training Loss  0.08427843451499939\n",
            "Epoch  5 Batch  491 / 525  Training Loss  0.09775872528553009\n",
            "Epoch  5 Batch  492 / 525  Training Loss  0.08391745388507843\n",
            "Epoch  5 Batch  493 / 525  Training Loss  0.11131958663463593\n",
            "Epoch  5 Batch  494 / 525  Training Loss  0.08720956742763519\n",
            "Epoch  5 Batch  495 / 525  Training Loss  0.08878160268068314\n",
            "Epoch  5 Batch  496 / 525  Training Loss  0.08806081861257553\n",
            "Epoch  5 Batch  497 / 525  Training Loss  0.09008054435253143\n",
            "Epoch  5 Batch  498 / 525  Training Loss  0.06968275457620621\n",
            "Epoch  5 Batch  499 / 525  Training Loss  0.09348145127296448\n",
            "Epoch  5 Batch  500 / 525  Training Loss  0.10295233875513077\n",
            "Epoch  5 Batch  501 / 525  Training Loss  0.08634965121746063\n",
            "Epoch  5 Batch  502 / 525  Training Loss  0.10759037733078003\n",
            "Epoch  5 Batch  503 / 525  Training Loss  0.09517167508602142\n",
            "Epoch  5 Batch  504 / 525  Training Loss  0.0681798979640007\n",
            "Epoch  5 Batch  505 / 525  Training Loss  0.08746309578418732\n",
            "Epoch  5 Batch  506 / 525  Training Loss  0.09207974374294281\n",
            "Epoch  5 Batch  507 / 525  Training Loss  0.09875060617923737\n",
            "Epoch  5 Batch  508 / 525  Training Loss  0.09046667069196701\n",
            "Epoch  5 Batch  509 / 525  Training Loss  0.06513339281082153\n",
            "Epoch  5 Batch  510 / 525  Training Loss  0.09584711492061615\n",
            "Epoch  5 Batch  511 / 525  Training Loss  0.12129372358322144\n",
            "Epoch  5 Batch  512 / 525  Training Loss  0.09011966735124588\n",
            "Epoch  5 Batch  513 / 525  Training Loss  0.11419080197811127\n",
            "Epoch  5 Batch  514 / 525  Training Loss  0.08525747060775757\n",
            "Epoch  5 Batch  515 / 525  Training Loss  0.09283988177776337\n",
            "Epoch  5 Batch  516 / 525  Training Loss  0.09107990562915802\n",
            "Epoch  5 Batch  517 / 525  Training Loss  0.07597456872463226\n",
            "Epoch  5 Batch  518 / 525  Training Loss  0.09359435737133026\n",
            "Epoch  5 Batch  519 / 525  Training Loss  0.09091071784496307\n",
            "Epoch  5 Batch  520 / 525  Training Loss  0.10571776330471039\n",
            "Epoch  5 Batch  521 / 525  Training Loss  0.08893205225467682\n",
            "Epoch  5 Batch  522 / 525  Training Loss  0.08308900147676468\n",
            "Epoch  5 Batch  523 / 525  Training Loss  0.08551757037639618\n",
            "Epoch  5 Batch  524 / 525  Training Loss  0.07977264374494553\n",
            "   6    |    -    |   0.089114   |   33.79  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 6\n",
            "Epoch  6 Batch  0 / 525  Training Loss  0.07621999084949493\n",
            "Epoch  6 Batch  1 / 525  Training Loss  0.06862793117761612\n",
            "Epoch  6 Batch  2 / 525  Training Loss  0.057099390774965286\n",
            "Epoch  6 Batch  3 / 525  Training Loss  0.07189112156629562\n",
            "Epoch  6 Batch  4 / 525  Training Loss  0.06301356852054596\n",
            "Epoch  6 Batch  5 / 525  Training Loss  0.06646980345249176\n",
            "Epoch  6 Batch  6 / 525  Training Loss  0.07007329165935516\n",
            "Epoch  6 Batch  7 / 525  Training Loss  0.07847173511981964\n",
            "Epoch  6 Batch  8 / 525  Training Loss  0.07294756919145584\n",
            "Epoch  6 Batch  9 / 525  Training Loss  0.06603378057479858\n",
            "Epoch  6 Batch  10 / 525  Training Loss  0.06525107473134995\n",
            "Epoch  6 Batch  11 / 525  Training Loss  0.06287147849798203\n",
            "Epoch  6 Batch  12 / 525  Training Loss  0.0667358785867691\n",
            "Epoch  6 Batch  13 / 525  Training Loss  0.07226993143558502\n",
            "Epoch  6 Batch  14 / 525  Training Loss  0.06922005861997604\n",
            "Epoch  6 Batch  15 / 525  Training Loss  0.05317171290516853\n",
            "Epoch  6 Batch  16 / 525  Training Loss  0.07374979555606842\n",
            "Epoch  6 Batch  17 / 525  Training Loss  0.06301020085811615\n",
            "Epoch  6 Batch  18 / 525  Training Loss  0.046878568828105927\n",
            "Epoch  6 Batch  19 / 525  Training Loss  0.04844192788004875\n",
            "Epoch  6 Batch  20 / 525  Training Loss  0.07403504848480225\n",
            "Epoch  6 Batch  21 / 525  Training Loss  0.073844775557518\n",
            "Epoch  6 Batch  22 / 525  Training Loss  0.054693661630153656\n",
            "Epoch  6 Batch  23 / 525  Training Loss  0.05391094833612442\n",
            "Epoch  6 Batch  24 / 525  Training Loss  0.05252901464700699\n",
            "Epoch  6 Batch  25 / 525  Training Loss  0.06288960576057434\n",
            "Epoch  6 Batch  26 / 525  Training Loss  0.06940669566392899\n",
            "Epoch  6 Batch  27 / 525  Training Loss  0.07248978316783905\n",
            "Epoch  6 Batch  28 / 525  Training Loss  0.07912161201238632\n",
            "Epoch  6 Batch  29 / 525  Training Loss  0.08349261432886124\n",
            "Epoch  6 Batch  30 / 525  Training Loss  0.06330084800720215\n",
            "Epoch  6 Batch  31 / 525  Training Loss  0.06074024364352226\n",
            "Epoch  6 Batch  32 / 525  Training Loss  0.04272613674402237\n",
            "Epoch  6 Batch  33 / 525  Training Loss  0.06745558977127075\n",
            "Epoch  6 Batch  34 / 525  Training Loss  0.061873842030763626\n",
            "Epoch  6 Batch  35 / 525  Training Loss  0.07392469048500061\n",
            "Epoch  6 Batch  36 / 525  Training Loss  0.08004190772771835\n",
            "Epoch  6 Batch  37 / 525  Training Loss  0.055782534182071686\n",
            "Epoch  6 Batch  38 / 525  Training Loss  0.07267240434885025\n",
            "Epoch  6 Batch  39 / 525  Training Loss  0.08369940519332886\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  6 Batch  40 / 525  Training Loss  0.07843252271413803\n",
            "Epoch  6 Batch  41 / 525  Training Loss  0.07905653864145279\n",
            "Epoch  6 Batch  42 / 525  Training Loss  0.05163903161883354\n",
            "Epoch  6 Batch  43 / 525  Training Loss  0.07836978137493134\n",
            "Epoch  6 Batch  44 / 525  Training Loss  0.055459123104810715\n",
            "Epoch  6 Batch  45 / 525  Training Loss  0.054717469960451126\n",
            "Epoch  6 Batch  46 / 525  Training Loss  0.04732671007514\n",
            "Epoch  6 Batch  47 / 525  Training Loss  0.054885197430849075\n",
            "Epoch  6 Batch  48 / 525  Training Loss  0.0867965966463089\n",
            "Epoch  6 Batch  49 / 525  Training Loss  0.08549954742193222\n",
            "Epoch  6 Batch  50 / 525  Training Loss  0.05805807188153267\n",
            "Epoch  6 Batch  51 / 525  Training Loss  0.0826835185289383\n",
            "Epoch  6 Batch  52 / 525  Training Loss  0.06753955036401749\n",
            "Epoch  6 Batch  53 / 525  Training Loss  0.0505407340824604\n",
            "Epoch  6 Batch  54 / 525  Training Loss  0.06717854738235474\n",
            "Epoch  6 Batch  55 / 525  Training Loss  0.05529177188873291\n",
            "Epoch  6 Batch  56 / 525  Training Loss  0.05070672184228897\n",
            "Epoch  6 Batch  57 / 525  Training Loss  0.05586780235171318\n",
            "Epoch  6 Batch  58 / 525  Training Loss  0.07470684498548508\n",
            "Epoch  6 Batch  59 / 525  Training Loss  0.0672488883137703\n",
            "Epoch  6 Batch  60 / 525  Training Loss  0.07445862144231796\n",
            "Epoch  6 Batch  61 / 525  Training Loss  0.05321076512336731\n",
            "Epoch  6 Batch  62 / 525  Training Loss  0.06642232090234756\n",
            "Epoch  6 Batch  63 / 525  Training Loss  0.07199865579605103\n",
            "Epoch  6 Batch  64 / 525  Training Loss  0.06649132072925568\n",
            "Epoch  6 Batch  65 / 525  Training Loss  0.07467290014028549\n",
            "Epoch  6 Batch  66 / 525  Training Loss  0.06449922174215317\n",
            "Epoch  6 Batch  67 / 525  Training Loss  0.06334472447633743\n",
            "Epoch  6 Batch  68 / 525  Training Loss  0.06092775613069534\n",
            "Epoch  6 Batch  69 / 525  Training Loss  0.08021662384271622\n",
            "Epoch  6 Batch  70 / 525  Training Loss  0.06744672358036041\n",
            "Epoch  6 Batch  71 / 525  Training Loss  0.05848129466176033\n",
            "Epoch  6 Batch  72 / 525  Training Loss  0.09694232046604156\n",
            "Epoch  6 Batch  73 / 525  Training Loss  0.08511120826005936\n",
            "Epoch  6 Batch  74 / 525  Training Loss  0.0726240798830986\n",
            "Epoch  6 Batch  75 / 525  Training Loss  0.07053742557764053\n",
            "Epoch  6 Batch  76 / 525  Training Loss  0.051481325179338455\n",
            "Epoch  6 Batch  77 / 525  Training Loss  0.074961818754673\n",
            "Epoch  6 Batch  78 / 525  Training Loss  0.05722351744771004\n",
            "Epoch  6 Batch  79 / 525  Training Loss  0.059510935097932816\n",
            "Epoch  6 Batch  80 / 525  Training Loss  0.06705556064844131\n",
            "Epoch  6 Batch  81 / 525  Training Loss  0.05764881521463394\n",
            "Epoch  6 Batch  82 / 525  Training Loss  0.06166726350784302\n",
            "Epoch  6 Batch  83 / 525  Training Loss  0.06677127629518509\n",
            "Epoch  6 Batch  84 / 525  Training Loss  0.07256744801998138\n",
            "Epoch  6 Batch  85 / 525  Training Loss  0.06157582998275757\n",
            "Epoch  6 Batch  86 / 525  Training Loss  0.08972080051898956\n",
            "Epoch  6 Batch  87 / 525  Training Loss  0.04952207952737808\n",
            "Epoch  6 Batch  88 / 525  Training Loss  0.06784180551767349\n",
            "Epoch  6 Batch  89 / 525  Training Loss  0.0838039368391037\n",
            "Epoch  6 Batch  90 / 525  Training Loss  0.06980571150779724\n",
            "Epoch  6 Batch  91 / 525  Training Loss  0.07720039784908295\n",
            "Epoch  6 Batch  92 / 525  Training Loss  0.0686018317937851\n",
            "Epoch  6 Batch  93 / 525  Training Loss  0.06298736482858658\n",
            "Epoch  6 Batch  94 / 525  Training Loss  0.06401794403791428\n",
            "Epoch  6 Batch  95 / 525  Training Loss  0.06701874732971191\n",
            "Epoch  6 Batch  96 / 525  Training Loss  0.04995531961321831\n",
            "Epoch  6 Batch  97 / 525  Training Loss  0.06238146871328354\n",
            "Epoch  6 Batch  98 / 525  Training Loss  0.050874657928943634\n",
            "Epoch  6 Batch  99 / 525  Training Loss  0.061809249222278595\n",
            "Epoch  6 Batch  100 / 525  Training Loss  0.05951201170682907\n",
            "Epoch  6 Batch  101 / 525  Training Loss  0.06295356899499893\n",
            "Epoch  6 Batch  102 / 525  Training Loss  0.07121467590332031\n",
            "Epoch  6 Batch  103 / 525  Training Loss  0.07546035945415497\n",
            "Epoch  6 Batch  104 / 525  Training Loss  0.04959580674767494\n",
            "Epoch  6 Batch  105 / 525  Training Loss  0.06516964733600616\n",
            "Epoch  6 Batch  106 / 525  Training Loss  0.05722872167825699\n",
            "Epoch  6 Batch  107 / 525  Training Loss  0.0735669881105423\n",
            "Epoch  6 Batch  108 / 525  Training Loss  0.054407186806201935\n",
            "Epoch  6 Batch  109 / 525  Training Loss  0.07257957756519318\n",
            "Epoch  6 Batch  110 / 525  Training Loss  0.08263614028692245\n",
            "Epoch  6 Batch  111 / 525  Training Loss  0.07753127813339233\n",
            "Epoch  6 Batch  112 / 525  Training Loss  0.08795775473117828\n",
            "Epoch  6 Batch  113 / 525  Training Loss  0.07848746329545975\n",
            "Epoch  6 Batch  114 / 525  Training Loss  0.06804761290550232\n",
            "Epoch  6 Batch  115 / 525  Training Loss  0.0849333181977272\n",
            "Epoch  6 Batch  116 / 525  Training Loss  0.06027562543749809\n",
            "Epoch  6 Batch  117 / 525  Training Loss  0.06899906694889069\n",
            "Epoch  6 Batch  118 / 525  Training Loss  0.043936505913734436\n",
            "Epoch  6 Batch  119 / 525  Training Loss  0.08222253620624542\n",
            "Epoch  6 Batch  120 / 525  Training Loss  0.08584360778331757\n",
            "Epoch  6 Batch  121 / 525  Training Loss  0.05298880860209465\n",
            "Epoch  6 Batch  122 / 525  Training Loss  0.06492637097835541\n",
            "Epoch  6 Batch  123 / 525  Training Loss  0.08905060589313507\n",
            "Epoch  6 Batch  124 / 525  Training Loss  0.06895998865365982\n",
            "Epoch  6 Batch  125 / 525  Training Loss  0.06461837887763977\n",
            "Epoch  6 Batch  126 / 525  Training Loss  0.08915799856185913\n",
            "Epoch  6 Batch  127 / 525  Training Loss  0.07342912256717682\n",
            "Epoch  6 Batch  128 / 525  Training Loss  0.08209738880395889\n",
            "Epoch  6 Batch  129 / 525  Training Loss  0.09274642914533615\n",
            "Epoch  6 Batch  130 / 525  Training Loss  0.07518371939659119\n",
            "Epoch  6 Batch  131 / 525  Training Loss  0.0709591656923294\n",
            "Epoch  6 Batch  132 / 525  Training Loss  0.07824622094631195\n",
            "Epoch  6 Batch  133 / 525  Training Loss  0.07118411362171173\n",
            "Epoch  6 Batch  134 / 525  Training Loss  0.07535450905561447\n",
            "Epoch  6 Batch  135 / 525  Training Loss  0.08866815268993378\n",
            "Epoch  6 Batch  136 / 525  Training Loss  0.10516532510519028\n",
            "Epoch  6 Batch  137 / 525  Training Loss  0.07333385199308395\n",
            "Epoch  6 Batch  138 / 525  Training Loss  0.0729757696390152\n",
            "Epoch  6 Batch  139 / 525  Training Loss  0.07624354958534241\n",
            "Epoch  6 Batch  140 / 525  Training Loss  0.05013807862997055\n",
            "Epoch  6 Batch  141 / 525  Training Loss  0.08461125940084457\n",
            "Epoch  6 Batch  142 / 525  Training Loss  0.05605731159448624\n",
            "Epoch  6 Batch  143 / 525  Training Loss  0.06330491602420807\n",
            "Epoch  6 Batch  144 / 525  Training Loss  0.06690152734518051\n",
            "Epoch  6 Batch  145 / 525  Training Loss  0.07832671701908112\n",
            "Epoch  6 Batch  146 / 525  Training Loss  0.07272499799728394\n",
            "Epoch  6 Batch  147 / 525  Training Loss  0.06465789675712585\n",
            "Epoch  6 Batch  148 / 525  Training Loss  0.05425813049077988\n",
            "Epoch  6 Batch  149 / 525  Training Loss  0.057701367884874344\n",
            "Epoch  6 Batch  150 / 525  Training Loss  0.07206512987613678\n",
            "Epoch  6 Batch  151 / 525  Training Loss  0.07076121866703033\n",
            "Epoch  6 Batch  152 / 525  Training Loss  0.07411407679319382\n",
            "Epoch  6 Batch  153 / 525  Training Loss  0.07568639516830444\n",
            "Epoch  6 Batch  154 / 525  Training Loss  0.06064428761601448\n",
            "Epoch  6 Batch  155 / 525  Training Loss  0.057828545570373535\n",
            "Epoch  6 Batch  156 / 525  Training Loss  0.07412044703960419\n",
            "Epoch  6 Batch  157 / 525  Training Loss  0.06721565127372742\n",
            "Epoch  6 Batch  158 / 525  Training Loss  0.0962517261505127\n",
            "Epoch  6 Batch  159 / 525  Training Loss  0.07354497164487839\n",
            "Epoch  6 Batch  160 / 525  Training Loss  0.06852396577596664\n",
            "Epoch  6 Batch  161 / 525  Training Loss  0.07425093650817871\n",
            "Epoch  6 Batch  162 / 525  Training Loss  0.07940997183322906\n",
            "Epoch  6 Batch  163 / 525  Training Loss  0.09634292870759964\n",
            "Epoch  6 Batch  164 / 525  Training Loss  0.07550979405641556\n",
            "Epoch  6 Batch  165 / 525  Training Loss  0.08336824923753738\n",
            "Epoch  6 Batch  166 / 525  Training Loss  0.07573864609003067\n",
            "Epoch  6 Batch  167 / 525  Training Loss  0.06465412676334381\n",
            "Epoch  6 Batch  168 / 525  Training Loss  0.05954139307141304\n",
            "Epoch  6 Batch  169 / 525  Training Loss  0.06340499967336655\n",
            "Epoch  6 Batch  170 / 525  Training Loss  0.07540135085582733\n",
            "Epoch  6 Batch  171 / 525  Training Loss  0.06871844828128815\n",
            "Epoch  6 Batch  172 / 525  Training Loss  0.07259975373744965\n",
            "Epoch  6 Batch  173 / 525  Training Loss  0.07087888568639755\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  6 Batch  174 / 525  Training Loss  0.0835535079240799\n",
            "Epoch  6 Batch  175 / 525  Training Loss  0.08012737333774567\n",
            "Epoch  6 Batch  176 / 525  Training Loss  0.07875216752290726\n",
            "Epoch  6 Batch  177 / 525  Training Loss  0.06862516701221466\n",
            "Epoch  6 Batch  178 / 525  Training Loss  0.05932101607322693\n",
            "Epoch  6 Batch  179 / 525  Training Loss  0.06287557631731033\n",
            "Epoch  6 Batch  180 / 525  Training Loss  0.052593596279621124\n",
            "Epoch  6 Batch  181 / 525  Training Loss  0.07168534398078918\n",
            "Epoch  6 Batch  182 / 525  Training Loss  0.0659041553735733\n",
            "Epoch  6 Batch  183 / 525  Training Loss  0.0741061419248581\n",
            "Epoch  6 Batch  184 / 525  Training Loss  0.09114792197942734\n",
            "Epoch  6 Batch  185 / 525  Training Loss  0.06298758089542389\n",
            "Epoch  6 Batch  186 / 525  Training Loss  0.07366364449262619\n",
            "Epoch  6 Batch  187 / 525  Training Loss  0.0569559708237648\n",
            "Epoch  6 Batch  188 / 525  Training Loss  0.06761203706264496\n",
            "Epoch  6 Batch  189 / 525  Training Loss  0.04276216775178909\n",
            "Epoch  6 Batch  190 / 525  Training Loss  0.07912389934062958\n",
            "Epoch  6 Batch  191 / 525  Training Loss  0.05490373447537422\n",
            "Epoch  6 Batch  192 / 525  Training Loss  0.07506310194730759\n",
            "Epoch  6 Batch  193 / 525  Training Loss  0.09298491477966309\n",
            "Epoch  6 Batch  194 / 525  Training Loss  0.08125273883342743\n",
            "Epoch  6 Batch  195 / 525  Training Loss  0.0708281621336937\n",
            "Epoch  6 Batch  196 / 525  Training Loss  0.05882631614804268\n",
            "Epoch  6 Batch  197 / 525  Training Loss  0.06997231394052505\n",
            "Epoch  6 Batch  198 / 525  Training Loss  0.07701275497674942\n",
            "Epoch  6 Batch  199 / 525  Training Loss  0.061022646725177765\n",
            "Epoch  6 Batch  200 / 525  Training Loss  0.07899640500545502\n",
            "Epoch  6 Batch  201 / 525  Training Loss  0.05850228667259216\n",
            "Epoch  6 Batch  202 / 525  Training Loss  0.07526656985282898\n",
            "Epoch  6 Batch  203 / 525  Training Loss  0.07698075473308563\n",
            "Epoch  6 Batch  204 / 525  Training Loss  0.07143743336200714\n",
            "Epoch  6 Batch  205 / 525  Training Loss  0.04490827023983002\n",
            "Epoch  6 Batch  206 / 525  Training Loss  0.07575484365224838\n",
            "Epoch  6 Batch  207 / 525  Training Loss  0.10720358788967133\n",
            "Epoch  6 Batch  208 / 525  Training Loss  0.0684158056974411\n",
            "Epoch  6 Batch  209 / 525  Training Loss  0.07107309997081757\n",
            "Epoch  6 Batch  210 / 525  Training Loss  0.06152762100100517\n",
            "Epoch  6 Batch  211 / 525  Training Loss  0.06982008367776871\n",
            "Epoch  6 Batch  212 / 525  Training Loss  0.06816153973340988\n",
            "Epoch  6 Batch  213 / 525  Training Loss  0.07088679820299149\n",
            "Epoch  6 Batch  214 / 525  Training Loss  0.06890960037708282\n",
            "Epoch  6 Batch  215 / 525  Training Loss  0.05314376950263977\n",
            "Epoch  6 Batch  216 / 525  Training Loss  0.060065578669309616\n",
            "Epoch  6 Batch  217 / 525  Training Loss  0.07615772634744644\n",
            "Epoch  6 Batch  218 / 525  Training Loss  0.06452900916337967\n",
            "Epoch  6 Batch  219 / 525  Training Loss  0.07920615375041962\n",
            "Epoch  6 Batch  220 / 525  Training Loss  0.05967200919985771\n",
            "Epoch  6 Batch  221 / 525  Training Loss  0.07486220449209213\n",
            "Epoch  6 Batch  222 / 525  Training Loss  0.07360365241765976\n",
            "Epoch  6 Batch  223 / 525  Training Loss  0.09200315177440643\n",
            "Epoch  6 Batch  224 / 525  Training Loss  0.0953737422823906\n",
            "Epoch  6 Batch  225 / 525  Training Loss  0.08166627585887909\n",
            "Epoch  6 Batch  226 / 525  Training Loss  0.06472963839769363\n",
            "Epoch  6 Batch  227 / 525  Training Loss  0.0648965910077095\n",
            "Epoch  6 Batch  228 / 525  Training Loss  0.04868602007627487\n",
            "Epoch  6 Batch  229 / 525  Training Loss  0.05872095376253128\n",
            "Epoch  6 Batch  230 / 525  Training Loss  0.0750519186258316\n",
            "Epoch  6 Batch  231 / 525  Training Loss  0.07010234892368317\n",
            "Epoch  6 Batch  232 / 525  Training Loss  0.07460401952266693\n",
            "Epoch  6 Batch  233 / 525  Training Loss  0.07728713750839233\n",
            "Epoch  6 Batch  234 / 525  Training Loss  0.04703540354967117\n",
            "Epoch  6 Batch  235 / 525  Training Loss  0.08891293406486511\n",
            "Epoch  6 Batch  236 / 525  Training Loss  0.06251496821641922\n",
            "Epoch  6 Batch  237 / 525  Training Loss  0.06548583507537842\n",
            "Epoch  6 Batch  238 / 525  Training Loss  0.055110447108745575\n",
            "Epoch  6 Batch  239 / 525  Training Loss  0.07857395708560944\n",
            "Epoch  6 Batch  240 / 525  Training Loss  0.07407482713460922\n",
            "Epoch  6 Batch  241 / 525  Training Loss  0.0727335512638092\n",
            "Epoch  6 Batch  242 / 525  Training Loss  0.07285679876804352\n",
            "Epoch  6 Batch  243 / 525  Training Loss  0.053677428513765335\n",
            "Epoch  6 Batch  244 / 525  Training Loss  0.06632907688617706\n",
            "Epoch  6 Batch  245 / 525  Training Loss  0.0760778859257698\n",
            "Epoch  6 Batch  246 / 525  Training Loss  0.08780936896800995\n",
            "Epoch  6 Batch  247 / 525  Training Loss  0.05937490612268448\n",
            "Epoch  6 Batch  248 / 525  Training Loss  0.07177720218896866\n",
            "Epoch  6 Batch  249 / 525  Training Loss  0.06375064700841904\n",
            "Epoch  6 Batch  250 / 525  Training Loss  0.07313371449708939\n",
            "Epoch  6 Batch  251 / 525  Training Loss  0.06669137626886368\n",
            "Epoch  6 Batch  252 / 525  Training Loss  0.0823131874203682\n",
            "Epoch  6 Batch  253 / 525  Training Loss  0.05216440558433533\n",
            "Epoch  6 Batch  254 / 525  Training Loss  0.06991516053676605\n",
            "Epoch  6 Batch  255 / 525  Training Loss  0.08965467661619186\n",
            "Epoch  6 Batch  256 / 525  Training Loss  0.08661554753780365\n",
            "Epoch  6 Batch  257 / 525  Training Loss  0.0693565234541893\n",
            "Epoch  6 Batch  258 / 525  Training Loss  0.06562010943889618\n",
            "Epoch  6 Batch  259 / 525  Training Loss  0.07815138250589371\n",
            "Epoch  6 Batch  260 / 525  Training Loss  0.06968162953853607\n",
            "Epoch  6 Batch  261 / 525  Training Loss  0.07244990020990372\n",
            "Epoch  6 Batch  262 / 525  Training Loss  0.06798055022954941\n",
            "Epoch  6 Batch  263 / 525  Training Loss  0.07016446441411972\n",
            "Epoch  6 Batch  264 / 525  Training Loss  0.07686954736709595\n",
            "Epoch  6 Batch  265 / 525  Training Loss  0.06642182916402817\n",
            "Epoch  6 Batch  266 / 525  Training Loss  0.059757113456726074\n",
            "Epoch  6 Batch  267 / 525  Training Loss  0.06352923810482025\n",
            "Epoch  6 Batch  268 / 525  Training Loss  0.07680206745862961\n",
            "Epoch  6 Batch  269 / 525  Training Loss  0.05576590821146965\n",
            "Epoch  6 Batch  270 / 525  Training Loss  0.10045146942138672\n",
            "Epoch  6 Batch  271 / 525  Training Loss  0.0849699154496193\n",
            "Epoch  6 Batch  272 / 525  Training Loss  0.0846690759062767\n",
            "Epoch  6 Batch  273 / 525  Training Loss  0.06384177505970001\n",
            "Epoch  6 Batch  274 / 525  Training Loss  0.06980191916227341\n",
            "Epoch  6 Batch  275 / 525  Training Loss  0.07334151118993759\n",
            "Epoch  6 Batch  276 / 525  Training Loss  0.07789783179759979\n",
            "Epoch  6 Batch  277 / 525  Training Loss  0.08382518589496613\n",
            "Epoch  6 Batch  278 / 525  Training Loss  0.0763627216219902\n",
            "Epoch  6 Batch  279 / 525  Training Loss  0.08252076059579849\n",
            "Epoch  6 Batch  280 / 525  Training Loss  0.08046045154333115\n",
            "Epoch  6 Batch  281 / 525  Training Loss  0.06285057961940765\n",
            "Epoch  6 Batch  282 / 525  Training Loss  0.0717347115278244\n",
            "Epoch  6 Batch  283 / 525  Training Loss  0.08067553490400314\n",
            "Epoch  6 Batch  284 / 525  Training Loss  0.07154814153909683\n",
            "Epoch  6 Batch  285 / 525  Training Loss  0.05937405303120613\n",
            "Epoch  6 Batch  286 / 525  Training Loss  0.07452861964702606\n",
            "Epoch  6 Batch  287 / 525  Training Loss  0.0672629252076149\n",
            "Epoch  6 Batch  288 / 525  Training Loss  0.0989806279540062\n",
            "Epoch  6 Batch  289 / 525  Training Loss  0.07381130754947662\n",
            "Epoch  6 Batch  290 / 525  Training Loss  0.10189597308635712\n",
            "Epoch  6 Batch  291 / 525  Training Loss  0.06715530157089233\n",
            "Epoch  6 Batch  292 / 525  Training Loss  0.06109176203608513\n",
            "Epoch  6 Batch  293 / 525  Training Loss  0.07551563531160355\n",
            "Epoch  6 Batch  294 / 525  Training Loss  0.08110944926738739\n",
            "Epoch  6 Batch  295 / 525  Training Loss  0.07092706114053726\n",
            "Epoch  6 Batch  296 / 525  Training Loss  0.059064753353595734\n",
            "Epoch  6 Batch  297 / 525  Training Loss  0.05613934248685837\n",
            "Epoch  6 Batch  298 / 525  Training Loss  0.05941205099225044\n",
            "Epoch  6 Batch  299 / 525  Training Loss  0.06082990765571594\n",
            "Epoch  6 Batch  300 / 525  Training Loss  0.05773582309484482\n",
            "Epoch  6 Batch  301 / 525  Training Loss  0.06419163942337036\n",
            "Epoch  6 Batch  302 / 525  Training Loss  0.056465815752744675\n",
            "Epoch  6 Batch  303 / 525  Training Loss  0.08204308897256851\n",
            "Epoch  6 Batch  304 / 525  Training Loss  0.05674230307340622\n",
            "Epoch  6 Batch  305 / 525  Training Loss  0.08179984986782074\n",
            "Epoch  6 Batch  306 / 525  Training Loss  0.06136631965637207\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  6 Batch  307 / 525  Training Loss  0.06083252280950546\n",
            "Epoch  6 Batch  308 / 525  Training Loss  0.06413070112466812\n",
            "Epoch  6 Batch  309 / 525  Training Loss  0.07066406309604645\n",
            "Epoch  6 Batch  310 / 525  Training Loss  0.07333371788263321\n",
            "Epoch  6 Batch  311 / 525  Training Loss  0.06789946556091309\n",
            "Epoch  6 Batch  312 / 525  Training Loss  0.07280398905277252\n",
            "Epoch  6 Batch  313 / 525  Training Loss  0.09158580750226974\n",
            "Epoch  6 Batch  314 / 525  Training Loss  0.06676403433084488\n",
            "Epoch  6 Batch  315 / 525  Training Loss  0.07773568481206894\n",
            "Epoch  6 Batch  316 / 525  Training Loss  0.088786780834198\n",
            "Epoch  6 Batch  317 / 525  Training Loss  0.09079267084598541\n",
            "Epoch  6 Batch  318 / 525  Training Loss  0.06835918873548508\n",
            "Epoch  6 Batch  319 / 525  Training Loss  0.061074983328580856\n",
            "Epoch  6 Batch  320 / 525  Training Loss  0.07799328118562698\n",
            "Epoch  6 Batch  321 / 525  Training Loss  0.07163403928279877\n",
            "Epoch  6 Batch  322 / 525  Training Loss  0.07291006296873093\n",
            "Epoch  6 Batch  323 / 525  Training Loss  0.06113613396883011\n",
            "Epoch  6 Batch  324 / 525  Training Loss  0.05955258011817932\n",
            "Epoch  6 Batch  325 / 525  Training Loss  0.07428450137376785\n",
            "Epoch  6 Batch  326 / 525  Training Loss  0.05761708691716194\n",
            "Epoch  6 Batch  327 / 525  Training Loss  0.08814545720815659\n",
            "Epoch  6 Batch  328 / 525  Training Loss  0.06168760731816292\n",
            "Epoch  6 Batch  329 / 525  Training Loss  0.0529596321284771\n",
            "Epoch  6 Batch  330 / 525  Training Loss  0.06701884418725967\n",
            "Epoch  6 Batch  331 / 525  Training Loss  0.06657285243272781\n",
            "Epoch  6 Batch  332 / 525  Training Loss  0.054385773837566376\n",
            "Epoch  6 Batch  333 / 525  Training Loss  0.057299256324768066\n",
            "Epoch  6 Batch  334 / 525  Training Loss  0.06887738406658173\n",
            "Epoch  6 Batch  335 / 525  Training Loss  0.07412657141685486\n",
            "Epoch  6 Batch  336 / 525  Training Loss  0.05499682575464249\n",
            "Epoch  6 Batch  337 / 525  Training Loss  0.05217130109667778\n",
            "Epoch  6 Batch  338 / 525  Training Loss  0.06994341313838959\n",
            "Epoch  6 Batch  339 / 525  Training Loss  0.0676000565290451\n",
            "Epoch  6 Batch  340 / 525  Training Loss  0.10112786293029785\n",
            "Epoch  6 Batch  341 / 525  Training Loss  0.06571300327777863\n",
            "Epoch  6 Batch  342 / 525  Training Loss  0.07061941176652908\n",
            "Epoch  6 Batch  343 / 525  Training Loss  0.07856402546167374\n",
            "Epoch  6 Batch  344 / 525  Training Loss  0.07798802852630615\n",
            "Epoch  6 Batch  345 / 525  Training Loss  0.07433616369962692\n",
            "Epoch  6 Batch  346 / 525  Training Loss  0.05880368873476982\n",
            "Epoch  6 Batch  347 / 525  Training Loss  0.09012281894683838\n",
            "Epoch  6 Batch  348 / 525  Training Loss  0.06542642414569855\n",
            "Epoch  6 Batch  349 / 525  Training Loss  0.062410932034254074\n",
            "Epoch  6 Batch  350 / 525  Training Loss  0.06517870724201202\n",
            "Epoch  6 Batch  351 / 525  Training Loss  0.05087189003825188\n",
            "Epoch  6 Batch  352 / 525  Training Loss  0.0668267235159874\n",
            "Epoch  6 Batch  353 / 525  Training Loss  0.0683380588889122\n",
            "Epoch  6 Batch  354 / 525  Training Loss  0.07519557327032089\n",
            "Epoch  6 Batch  355 / 525  Training Loss  0.07627969980239868\n",
            "Epoch  6 Batch  356 / 525  Training Loss  0.06349040567874908\n",
            "Epoch  6 Batch  357 / 525  Training Loss  0.0875898152589798\n",
            "Epoch  6 Batch  358 / 525  Training Loss  0.07745341956615448\n",
            "Epoch  6 Batch  359 / 525  Training Loss  0.06960967928171158\n",
            "Epoch  6 Batch  360 / 525  Training Loss  0.060628194361925125\n",
            "Epoch  6 Batch  361 / 525  Training Loss  0.06586281955242157\n",
            "Epoch  6 Batch  362 / 525  Training Loss  0.07700536400079727\n",
            "Epoch  6 Batch  363 / 525  Training Loss  0.06980939209461212\n",
            "Epoch  6 Batch  364 / 525  Training Loss  0.07850523293018341\n",
            "Epoch  6 Batch  365 / 525  Training Loss  0.07193856686353683\n",
            "Epoch  6 Batch  366 / 525  Training Loss  0.06941868364810944\n",
            "Epoch  6 Batch  367 / 525  Training Loss  0.0513879768550396\n",
            "Epoch  6 Batch  368 / 525  Training Loss  0.07872758060693741\n",
            "Epoch  6 Batch  369 / 525  Training Loss  0.06797001510858536\n",
            "Epoch  6 Batch  370 / 525  Training Loss  0.05913897603750229\n",
            "Epoch  6 Batch  371 / 525  Training Loss  0.0989483967423439\n",
            "Epoch  6 Batch  372 / 525  Training Loss  0.05588287115097046\n",
            "Epoch  6 Batch  373 / 525  Training Loss  0.062238793820142746\n",
            "Epoch  6 Batch  374 / 525  Training Loss  0.04496166855096817\n",
            "Epoch  6 Batch  375 / 525  Training Loss  0.04701876640319824\n",
            "Epoch  6 Batch  376 / 525  Training Loss  0.08345279842615128\n",
            "Epoch  6 Batch  377 / 525  Training Loss  0.0796898603439331\n",
            "Epoch  6 Batch  378 / 525  Training Loss  0.062069159001111984\n",
            "Epoch  6 Batch  379 / 525  Training Loss  0.06670889258384705\n",
            "Epoch  6 Batch  380 / 525  Training Loss  0.08608725666999817\n",
            "Epoch  6 Batch  381 / 525  Training Loss  0.08177734911441803\n",
            "Epoch  6 Batch  382 / 525  Training Loss  0.04827065020799637\n",
            "Epoch  6 Batch  383 / 525  Training Loss  0.07812915742397308\n",
            "Epoch  6 Batch  384 / 525  Training Loss  0.057871490716934204\n",
            "Epoch  6 Batch  385 / 525  Training Loss  0.06391771137714386\n",
            "Epoch  6 Batch  386 / 525  Training Loss  0.07284620404243469\n",
            "Epoch  6 Batch  387 / 525  Training Loss  0.07511958479881287\n",
            "Epoch  6 Batch  388 / 525  Training Loss  0.062437791377305984\n",
            "Epoch  6 Batch  389 / 525  Training Loss  0.06389819085597992\n",
            "Epoch  6 Batch  390 / 525  Training Loss  0.07624369114637375\n",
            "Epoch  6 Batch  391 / 525  Training Loss  0.06573639810085297\n",
            "Epoch  6 Batch  392 / 525  Training Loss  0.05937456339597702\n",
            "Epoch  6 Batch  393 / 525  Training Loss  0.05768679454922676\n",
            "Epoch  6 Batch  394 / 525  Training Loss  0.05550349876284599\n",
            "Epoch  6 Batch  395 / 525  Training Loss  0.058634884655475616\n",
            "Epoch  6 Batch  396 / 525  Training Loss  0.08520189672708511\n",
            "Epoch  6 Batch  397 / 525  Training Loss  0.06539495289325714\n",
            "Epoch  6 Batch  398 / 525  Training Loss  0.06816790252923965\n",
            "Epoch  6 Batch  399 / 525  Training Loss  0.038999494165182114\n",
            "Epoch  6 Batch  400 / 525  Training Loss  0.062399912625551224\n",
            "Epoch  6 Batch  401 / 525  Training Loss  0.07630465924739838\n",
            "Epoch  6 Batch  402 / 525  Training Loss  0.07234646379947662\n",
            "Epoch  6 Batch  403 / 525  Training Loss  0.08923469483852386\n",
            "Epoch  6 Batch  404 / 525  Training Loss  0.07141422480344772\n",
            "Epoch  6 Batch  405 / 525  Training Loss  0.08346996456384659\n",
            "Epoch  6 Batch  406 / 525  Training Loss  0.06959512084722519\n",
            "Epoch  6 Batch  407 / 525  Training Loss  0.07125313580036163\n",
            "Epoch  6 Batch  408 / 525  Training Loss  0.05424922704696655\n",
            "Epoch  6 Batch  409 / 525  Training Loss  0.05658503249287605\n",
            "Epoch  6 Batch  410 / 525  Training Loss  0.07327048480510712\n",
            "Epoch  6 Batch  411 / 525  Training Loss  0.07328683137893677\n",
            "Epoch  6 Batch  412 / 525  Training Loss  0.05940743535757065\n",
            "Epoch  6 Batch  413 / 525  Training Loss  0.07196736335754395\n",
            "Epoch  6 Batch  414 / 525  Training Loss  0.06362587213516235\n",
            "Epoch  6 Batch  415 / 525  Training Loss  0.07719174772500992\n",
            "Epoch  6 Batch  416 / 525  Training Loss  0.0723637193441391\n",
            "Epoch  6 Batch  417 / 525  Training Loss  0.05478174611926079\n",
            "Epoch  6 Batch  418 / 525  Training Loss  0.058052659034729004\n",
            "Epoch  6 Batch  419 / 525  Training Loss  0.07057717442512512\n",
            "Epoch  6 Batch  420 / 525  Training Loss  0.07450257986783981\n",
            "Epoch  6 Batch  421 / 525  Training Loss  0.06717938929796219\n",
            "Epoch  6 Batch  422 / 525  Training Loss  0.04650261998176575\n",
            "Epoch  6 Batch  423 / 525  Training Loss  0.06852908432483673\n",
            "Epoch  6 Batch  424 / 525  Training Loss  0.05112835019826889\n",
            "Epoch  6 Batch  425 / 525  Training Loss  0.049406006932258606\n",
            "Epoch  6 Batch  426 / 525  Training Loss  0.07554488629102707\n",
            "Epoch  6 Batch  427 / 525  Training Loss  0.07518215477466583\n",
            "Epoch  6 Batch  428 / 525  Training Loss  0.06874392181634903\n",
            "Epoch  6 Batch  429 / 525  Training Loss  0.07567279785871506\n",
            "Epoch  6 Batch  430 / 525  Training Loss  0.07113578915596008\n",
            "Epoch  6 Batch  431 / 525  Training Loss  0.07203356921672821\n",
            "Epoch  6 Batch  432 / 525  Training Loss  0.06254765391349792\n",
            "Epoch  6 Batch  433 / 525  Training Loss  0.06078747659921646\n",
            "Epoch  6 Batch  434 / 525  Training Loss  0.07784687727689743\n",
            "Epoch  6 Batch  435 / 525  Training Loss  0.058097779750823975\n",
            "Epoch  6 Batch  436 / 525  Training Loss  0.08305913209915161\n",
            "Epoch  6 Batch  437 / 525  Training Loss  0.06044634431600571\n",
            "Epoch  6 Batch  438 / 525  Training Loss  0.06396372616291046\n",
            "Epoch  6 Batch  439 / 525  Training Loss  0.0660882368683815\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  6 Batch  440 / 525  Training Loss  0.07282371819019318\n",
            "Epoch  6 Batch  441 / 525  Training Loss  0.06842182576656342\n",
            "Epoch  6 Batch  442 / 525  Training Loss  0.0688672810792923\n",
            "Epoch  6 Batch  443 / 525  Training Loss  0.07669036835432053\n",
            "Epoch  6 Batch  444 / 525  Training Loss  0.06507085263729095\n",
            "Epoch  6 Batch  445 / 525  Training Loss  0.07394766062498093\n",
            "Epoch  6 Batch  446 / 525  Training Loss  0.0788293182849884\n",
            "Epoch  6 Batch  447 / 525  Training Loss  0.06549539417028427\n",
            "Epoch  6 Batch  448 / 525  Training Loss  0.05701535940170288\n",
            "Epoch  6 Batch  449 / 525  Training Loss  0.04745805636048317\n",
            "Epoch  6 Batch  450 / 525  Training Loss  0.05931175500154495\n",
            "Epoch  6 Batch  451 / 525  Training Loss  0.06450550258159637\n",
            "Epoch  6 Batch  452 / 525  Training Loss  0.07018256187438965\n",
            "Epoch  6 Batch  453 / 525  Training Loss  0.06132838875055313\n",
            "Epoch  6 Batch  454 / 525  Training Loss  0.06550121307373047\n",
            "Epoch  6 Batch  455 / 525  Training Loss  0.06474136561155319\n",
            "Epoch  6 Batch  456 / 525  Training Loss  0.07958117127418518\n",
            "Epoch  6 Batch  457 / 525  Training Loss  0.06078110262751579\n",
            "Epoch  6 Batch  458 / 525  Training Loss  0.07497698813676834\n",
            "Epoch  6 Batch  459 / 525  Training Loss  0.08210156112909317\n",
            "Epoch  6 Batch  460 / 525  Training Loss  0.06288538873195648\n",
            "Epoch  6 Batch  461 / 525  Training Loss  0.0935155376791954\n",
            "Epoch  6 Batch  462 / 525  Training Loss  0.06858042627573013\n",
            "Epoch  6 Batch  463 / 525  Training Loss  0.0663655623793602\n",
            "Epoch  6 Batch  464 / 525  Training Loss  0.06289560347795486\n",
            "Epoch  6 Batch  465 / 525  Training Loss  0.0816391259431839\n",
            "Epoch  6 Batch  466 / 525  Training Loss  0.055079519748687744\n",
            "Epoch  6 Batch  467 / 525  Training Loss  0.053818415850400925\n",
            "Epoch  6 Batch  468 / 525  Training Loss  0.04065047204494476\n",
            "Epoch  6 Batch  469 / 525  Training Loss  0.07891615480184555\n",
            "Epoch  6 Batch  470 / 525  Training Loss  0.07619959861040115\n",
            "Epoch  6 Batch  471 / 525  Training Loss  0.08359064161777496\n",
            "Epoch  6 Batch  472 / 525  Training Loss  0.08557368069887161\n",
            "Epoch  6 Batch  473 / 525  Training Loss  0.06811275333166122\n",
            "Epoch  6 Batch  474 / 525  Training Loss  0.06614569574594498\n",
            "Epoch  6 Batch  475 / 525  Training Loss  0.05719194561243057\n",
            "Epoch  6 Batch  476 / 525  Training Loss  0.07741952687501907\n",
            "Epoch  6 Batch  477 / 525  Training Loss  0.08382397890090942\n",
            "Epoch  6 Batch  478 / 525  Training Loss  0.06285523623228073\n",
            "Epoch  6 Batch  479 / 525  Training Loss  0.08933883160352707\n",
            "Epoch  6 Batch  480 / 525  Training Loss  0.09343454986810684\n",
            "Epoch  6 Batch  481 / 525  Training Loss  0.05922175198793411\n",
            "Epoch  6 Batch  482 / 525  Training Loss  0.07203638553619385\n",
            "Epoch  6 Batch  483 / 525  Training Loss  0.06881282478570938\n",
            "Epoch  6 Batch  484 / 525  Training Loss  0.08269704133272171\n",
            "Epoch  6 Batch  485 / 525  Training Loss  0.07326797395944595\n",
            "Epoch  6 Batch  486 / 525  Training Loss  0.08133943378925323\n",
            "Epoch  6 Batch  487 / 525  Training Loss  0.06232964247465134\n",
            "Epoch  6 Batch  488 / 525  Training Loss  0.06618347018957138\n",
            "Epoch  6 Batch  489 / 525  Training Loss  0.0708145946264267\n",
            "Epoch  6 Batch  490 / 525  Training Loss  0.049547769129276276\n",
            "Epoch  6 Batch  491 / 525  Training Loss  0.08068451285362244\n",
            "Epoch  6 Batch  492 / 525  Training Loss  0.052929021418094635\n",
            "Epoch  6 Batch  493 / 525  Training Loss  0.06723305583000183\n",
            "Epoch  6 Batch  494 / 525  Training Loss  0.06260703504085541\n",
            "Epoch  6 Batch  495 / 525  Training Loss  0.0792999118566513\n",
            "Epoch  6 Batch  496 / 525  Training Loss  0.08003820478916168\n",
            "Epoch  6 Batch  497 / 525  Training Loss  0.05433201789855957\n",
            "Epoch  6 Batch  498 / 525  Training Loss  0.07373181730508804\n",
            "Epoch  6 Batch  499 / 525  Training Loss  0.06191138178110123\n",
            "Epoch  6 Batch  500 / 525  Training Loss  0.054122962057590485\n",
            "Epoch  6 Batch  501 / 525  Training Loss  0.08058001101016998\n",
            "Epoch  6 Batch  502 / 525  Training Loss  0.061848364770412445\n",
            "Epoch  6 Batch  503 / 525  Training Loss  0.05943244695663452\n",
            "Epoch  6 Batch  504 / 525  Training Loss  0.07015395909547806\n",
            "Epoch  6 Batch  505 / 525  Training Loss  0.057751379907131195\n",
            "Epoch  6 Batch  506 / 525  Training Loss  0.06254514306783676\n",
            "Epoch  6 Batch  507 / 525  Training Loss  0.05937187746167183\n",
            "Epoch  6 Batch  508 / 525  Training Loss  0.08678079396486282\n",
            "Epoch  6 Batch  509 / 525  Training Loss  0.07817833125591278\n",
            "Epoch  6 Batch  510 / 525  Training Loss  0.0863456279039383\n",
            "Epoch  6 Batch  511 / 525  Training Loss  0.06738545745611191\n",
            "Epoch  6 Batch  512 / 525  Training Loss  0.10152747482061386\n",
            "Epoch  6 Batch  513 / 525  Training Loss  0.08429960906505585\n",
            "Epoch  6 Batch  514 / 525  Training Loss  0.05709191411733627\n",
            "Epoch  6 Batch  515 / 525  Training Loss  0.06427587568759918\n",
            "Epoch  6 Batch  516 / 525  Training Loss  0.05006365105509758\n",
            "Epoch  6 Batch  517 / 525  Training Loss  0.06984676420688629\n",
            "Epoch  6 Batch  518 / 525  Training Loss  0.0645860880613327\n",
            "Epoch  6 Batch  519 / 525  Training Loss  0.07832411676645279\n",
            "Epoch  6 Batch  520 / 525  Training Loss  0.08148421347141266\n",
            "Epoch  6 Batch  521 / 525  Training Loss  0.05596589297056198\n",
            "Epoch  6 Batch  522 / 525  Training Loss  0.07055932283401489\n",
            "Epoch  6 Batch  523 / 525  Training Loss  0.05440710857510567\n",
            "Epoch  6 Batch  524 / 525  Training Loss  0.07650710642337799\n",
            "   7    |    -    |   0.069301   |   40.99  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 7\n",
            "Epoch  7 Batch  0 / 525  Training Loss  0.04526279494166374\n",
            "Epoch  7 Batch  1 / 525  Training Loss  0.05079842358827591\n",
            "Epoch  7 Batch  2 / 525  Training Loss  0.03666389733552933\n",
            "Epoch  7 Batch  3 / 525  Training Loss  0.048646677285432816\n",
            "Epoch  7 Batch  4 / 525  Training Loss  0.03883190080523491\n",
            "Epoch  7 Batch  5 / 525  Training Loss  0.047446560114622116\n",
            "Epoch  7 Batch  6 / 525  Training Loss  0.026947984471917152\n",
            "Epoch  7 Batch  7 / 525  Training Loss  0.050024937838315964\n",
            "Epoch  7 Batch  8 / 525  Training Loss  0.05699165537953377\n",
            "Epoch  7 Batch  9 / 525  Training Loss  0.04806750267744064\n",
            "Epoch  7 Batch  10 / 525  Training Loss  0.039260029792785645\n",
            "Epoch  7 Batch  11 / 525  Training Loss  0.04192021116614342\n",
            "Epoch  7 Batch  12 / 525  Training Loss  0.05988465994596481\n",
            "Epoch  7 Batch  13 / 525  Training Loss  0.04825027659535408\n",
            "Epoch  7 Batch  14 / 525  Training Loss  0.04270390421152115\n",
            "Epoch  7 Batch  15 / 525  Training Loss  0.045631468296051025\n",
            "Epoch  7 Batch  16 / 525  Training Loss  0.05027388781309128\n",
            "Epoch  7 Batch  17 / 525  Training Loss  0.03763671964406967\n",
            "Epoch  7 Batch  18 / 525  Training Loss  0.06044209003448486\n",
            "Epoch  7 Batch  19 / 525  Training Loss  0.046141013503074646\n",
            "Epoch  7 Batch  20 / 525  Training Loss  0.05737648531794548\n",
            "Epoch  7 Batch  21 / 525  Training Loss  0.04087330028414726\n",
            "Epoch  7 Batch  22 / 525  Training Loss  0.042913272976875305\n",
            "Epoch  7 Batch  23 / 525  Training Loss  0.03906339406967163\n",
            "Epoch  7 Batch  24 / 525  Training Loss  0.03936562314629555\n",
            "Epoch  7 Batch  25 / 525  Training Loss  0.06327028572559357\n",
            "Epoch  7 Batch  26 / 525  Training Loss  0.0502757653594017\n",
            "Epoch  7 Batch  27 / 525  Training Loss  0.04607855901122093\n",
            "Epoch  7 Batch  28 / 525  Training Loss  0.06087842583656311\n",
            "Epoch  7 Batch  29 / 525  Training Loss  0.0494394451379776\n",
            "Epoch  7 Batch  30 / 525  Training Loss  0.048805899918079376\n",
            "Epoch  7 Batch  31 / 525  Training Loss  0.05054439976811409\n",
            "Epoch  7 Batch  32 / 525  Training Loss  0.05173792317509651\n",
            "Epoch  7 Batch  33 / 525  Training Loss  0.043859176337718964\n",
            "Epoch  7 Batch  34 / 525  Training Loss  0.046811554580926895\n",
            "Epoch  7 Batch  35 / 525  Training Loss  0.06303642690181732\n",
            "Epoch  7 Batch  36 / 525  Training Loss  0.056232623755931854\n",
            "Epoch  7 Batch  37 / 525  Training Loss  0.05247228220105171\n",
            "Epoch  7 Batch  38 / 525  Training Loss  0.05976809933781624\n",
            "Epoch  7 Batch  39 / 525  Training Loss  0.03721906989812851\n",
            "Epoch  7 Batch  40 / 525  Training Loss  0.03272519260644913\n",
            "Epoch  7 Batch  41 / 525  Training Loss  0.04880349338054657\n",
            "Epoch  7 Batch  42 / 525  Training Loss  0.05572890117764473\n",
            "Epoch  7 Batch  43 / 525  Training Loss  0.057864852249622345\n",
            "Epoch  7 Batch  44 / 525  Training Loss  0.030882561579346657\n",
            "Epoch  7 Batch  45 / 525  Training Loss  0.04450366646051407\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  7 Batch  46 / 525  Training Loss  0.0378846600651741\n",
            "Epoch  7 Batch  47 / 525  Training Loss  0.049471333622932434\n",
            "Epoch  7 Batch  48 / 525  Training Loss  0.048889994621276855\n",
            "Epoch  7 Batch  49 / 525  Training Loss  0.05950240045785904\n",
            "Epoch  7 Batch  50 / 525  Training Loss  0.05410756915807724\n",
            "Epoch  7 Batch  51 / 525  Training Loss  0.07017147541046143\n",
            "Epoch  7 Batch  52 / 525  Training Loss  0.03496761992573738\n",
            "Epoch  7 Batch  53 / 525  Training Loss  0.05294520780444145\n",
            "Epoch  7 Batch  54 / 525  Training Loss  0.04655534774065018\n",
            "Epoch  7 Batch  55 / 525  Training Loss  0.06053342670202255\n",
            "Epoch  7 Batch  56 / 525  Training Loss  0.03498010337352753\n",
            "Epoch  7 Batch  57 / 525  Training Loss  0.05695534497499466\n",
            "Epoch  7 Batch  58 / 525  Training Loss  0.053984858095645905\n",
            "Epoch  7 Batch  59 / 525  Training Loss  0.06861277669668198\n",
            "Epoch  7 Batch  60 / 525  Training Loss  0.08356114476919174\n",
            "Epoch  7 Batch  61 / 525  Training Loss  0.08911366760730743\n",
            "Epoch  7 Batch  62 / 525  Training Loss  0.04679054766893387\n",
            "Epoch  7 Batch  63 / 525  Training Loss  0.053679775446653366\n",
            "Epoch  7 Batch  64 / 525  Training Loss  0.05098555609583855\n",
            "Epoch  7 Batch  65 / 525  Training Loss  0.050874292850494385\n",
            "Epoch  7 Batch  66 / 525  Training Loss  0.042116064578294754\n",
            "Epoch  7 Batch  67 / 525  Training Loss  0.06593068689107895\n",
            "Epoch  7 Batch  68 / 525  Training Loss  0.03552872687578201\n",
            "Epoch  7 Batch  69 / 525  Training Loss  0.03849741443991661\n",
            "Epoch  7 Batch  70 / 525  Training Loss  0.0513821542263031\n",
            "Epoch  7 Batch  71 / 525  Training Loss  0.04845505952835083\n",
            "Epoch  7 Batch  72 / 525  Training Loss  0.07445280998945236\n",
            "Epoch  7 Batch  73 / 525  Training Loss  0.0452522337436676\n",
            "Epoch  7 Batch  74 / 525  Training Loss  0.04911329597234726\n",
            "Epoch  7 Batch  75 / 525  Training Loss  0.049194104969501495\n",
            "Epoch  7 Batch  76 / 525  Training Loss  0.07005943357944489\n",
            "Epoch  7 Batch  77 / 525  Training Loss  0.06687000393867493\n",
            "Epoch  7 Batch  78 / 525  Training Loss  0.036897771060466766\n",
            "Epoch  7 Batch  79 / 525  Training Loss  0.05667489767074585\n",
            "Epoch  7 Batch  80 / 525  Training Loss  0.058533765375614166\n",
            "Epoch  7 Batch  81 / 525  Training Loss  0.0535113625228405\n",
            "Epoch  7 Batch  82 / 525  Training Loss  0.046517953276634216\n",
            "Epoch  7 Batch  83 / 525  Training Loss  0.05878456309437752\n",
            "Epoch  7 Batch  84 / 525  Training Loss  0.05450868606567383\n",
            "Epoch  7 Batch  85 / 525  Training Loss  0.05171802639961243\n",
            "Epoch  7 Batch  86 / 525  Training Loss  0.043387800455093384\n",
            "Epoch  7 Batch  87 / 525  Training Loss  0.04110532999038696\n",
            "Epoch  7 Batch  88 / 525  Training Loss  0.04111305996775627\n",
            "Epoch  7 Batch  89 / 525  Training Loss  0.032140813767910004\n",
            "Epoch  7 Batch  90 / 525  Training Loss  0.07772479951381683\n",
            "Epoch  7 Batch  91 / 525  Training Loss  0.05858499929308891\n",
            "Epoch  7 Batch  92 / 525  Training Loss  0.05662773177027702\n",
            "Epoch  7 Batch  93 / 525  Training Loss  0.04956461489200592\n",
            "Epoch  7 Batch  94 / 525  Training Loss  0.04505693167448044\n",
            "Epoch  7 Batch  95 / 525  Training Loss  0.06012153625488281\n",
            "Epoch  7 Batch  96 / 525  Training Loss  0.05696997046470642\n",
            "Epoch  7 Batch  97 / 525  Training Loss  0.04418233036994934\n",
            "Epoch  7 Batch  98 / 525  Training Loss  0.049024440348148346\n",
            "Epoch  7 Batch  99 / 525  Training Loss  0.05915650725364685\n",
            "Epoch  7 Batch  100 / 525  Training Loss  0.06230099871754646\n",
            "Epoch  7 Batch  101 / 525  Training Loss  0.05033252388238907\n",
            "Epoch  7 Batch  102 / 525  Training Loss  0.04852490499615669\n",
            "Epoch  7 Batch  103 / 525  Training Loss  0.05667508393526077\n",
            "Epoch  7 Batch  104 / 525  Training Loss  0.046382032334804535\n",
            "Epoch  7 Batch  105 / 525  Training Loss  0.04524881765246391\n",
            "Epoch  7 Batch  106 / 525  Training Loss  0.04803822562098503\n",
            "Epoch  7 Batch  107 / 525  Training Loss  0.04817325994372368\n",
            "Epoch  7 Batch  108 / 525  Training Loss  0.056649815291166306\n",
            "Epoch  7 Batch  109 / 525  Training Loss  0.061310458928346634\n",
            "Epoch  7 Batch  110 / 525  Training Loss  0.052652548998594284\n",
            "Epoch  7 Batch  111 / 525  Training Loss  0.044790804386138916\n",
            "Epoch  7 Batch  112 / 525  Training Loss  0.05569184571504593\n",
            "Epoch  7 Batch  113 / 525  Training Loss  0.04123248904943466\n",
            "Epoch  7 Batch  114 / 525  Training Loss  0.05460255220532417\n",
            "Epoch  7 Batch  115 / 525  Training Loss  0.0571235828101635\n",
            "Epoch  7 Batch  116 / 525  Training Loss  0.03807590901851654\n",
            "Epoch  7 Batch  117 / 525  Training Loss  0.05261922627687454\n",
            "Epoch  7 Batch  118 / 525  Training Loss  0.057488489896059036\n",
            "Epoch  7 Batch  119 / 525  Training Loss  0.05095094442367554\n",
            "Epoch  7 Batch  120 / 525  Training Loss  0.044790301471948624\n",
            "Epoch  7 Batch  121 / 525  Training Loss  0.044197678565979004\n",
            "Epoch  7 Batch  122 / 525  Training Loss  0.05306240916252136\n",
            "Epoch  7 Batch  123 / 525  Training Loss  0.044065747410058975\n",
            "Epoch  7 Batch  124 / 525  Training Loss  0.06048334762454033\n",
            "Epoch  7 Batch  125 / 525  Training Loss  0.037237100303173065\n",
            "Epoch  7 Batch  126 / 525  Training Loss  0.04539714381098747\n",
            "Epoch  7 Batch  127 / 525  Training Loss  0.04912625998258591\n",
            "Epoch  7 Batch  128 / 525  Training Loss  0.042749982327222824\n",
            "Epoch  7 Batch  129 / 525  Training Loss  0.04884747415781021\n",
            "Epoch  7 Batch  130 / 525  Training Loss  0.04942837730050087\n",
            "Epoch  7 Batch  131 / 525  Training Loss  0.05814623832702637\n",
            "Epoch  7 Batch  132 / 525  Training Loss  0.04253362864255905\n",
            "Epoch  7 Batch  133 / 525  Training Loss  0.07015691697597504\n",
            "Epoch  7 Batch  134 / 525  Training Loss  0.04597120359539986\n",
            "Epoch  7 Batch  135 / 525  Training Loss  0.051849134266376495\n",
            "Epoch  7 Batch  136 / 525  Training Loss  0.039924800395965576\n",
            "Epoch  7 Batch  137 / 525  Training Loss  0.048035405576229095\n",
            "Epoch  7 Batch  138 / 525  Training Loss  0.04665745422244072\n",
            "Epoch  7 Batch  139 / 525  Training Loss  0.04103922098875046\n",
            "Epoch  7 Batch  140 / 525  Training Loss  0.04691549390554428\n",
            "Epoch  7 Batch  141 / 525  Training Loss  0.05242256447672844\n",
            "Epoch  7 Batch  142 / 525  Training Loss  0.05744525045156479\n",
            "Epoch  7 Batch  143 / 525  Training Loss  0.052559398114681244\n",
            "Epoch  7 Batch  144 / 525  Training Loss  0.051797788590192795\n",
            "Epoch  7 Batch  145 / 525  Training Loss  0.0630790963768959\n",
            "Epoch  7 Batch  146 / 525  Training Loss  0.05032864958047867\n",
            "Epoch  7 Batch  147 / 525  Training Loss  0.05653339624404907\n",
            "Epoch  7 Batch  148 / 525  Training Loss  0.04421785846352577\n",
            "Epoch  7 Batch  149 / 525  Training Loss  0.035665545612573624\n",
            "Epoch  7 Batch  150 / 525  Training Loss  0.05397145077586174\n",
            "Epoch  7 Batch  151 / 525  Training Loss  0.0524735227227211\n",
            "Epoch  7 Batch  152 / 525  Training Loss  0.06031777337193489\n",
            "Epoch  7 Batch  153 / 525  Training Loss  0.04542843624949455\n",
            "Epoch  7 Batch  154 / 525  Training Loss  0.03753306716680527\n",
            "Epoch  7 Batch  155 / 525  Training Loss  0.06603638827800751\n",
            "Epoch  7 Batch  156 / 525  Training Loss  0.06323214620351791\n",
            "Epoch  7 Batch  157 / 525  Training Loss  0.05042116716504097\n",
            "Epoch  7 Batch  158 / 525  Training Loss  0.051771216094493866\n",
            "Epoch  7 Batch  159 / 525  Training Loss  0.05788131430745125\n",
            "Epoch  7 Batch  160 / 525  Training Loss  0.03907329961657524\n",
            "Epoch  7 Batch  161 / 525  Training Loss  0.07020975649356842\n",
            "Epoch  7 Batch  162 / 525  Training Loss  0.04959094151854515\n",
            "Epoch  7 Batch  163 / 525  Training Loss  0.039243824779987335\n",
            "Epoch  7 Batch  164 / 525  Training Loss  0.04963862895965576\n",
            "Epoch  7 Batch  165 / 525  Training Loss  0.05832536146044731\n",
            "Epoch  7 Batch  166 / 525  Training Loss  0.04525754600763321\n",
            "Epoch  7 Batch  167 / 525  Training Loss  0.05675000697374344\n",
            "Epoch  7 Batch  168 / 525  Training Loss  0.05591479688882828\n",
            "Epoch  7 Batch  169 / 525  Training Loss  0.05663597583770752\n",
            "Epoch  7 Batch  170 / 525  Training Loss  0.0428994745016098\n",
            "Epoch  7 Batch  171 / 525  Training Loss  0.04791964218020439\n",
            "Epoch  7 Batch  172 / 525  Training Loss  0.07796061038970947\n",
            "Epoch  7 Batch  173 / 525  Training Loss  0.05392787605524063\n",
            "Epoch  7 Batch  174 / 525  Training Loss  0.040570929646492004\n",
            "Epoch  7 Batch  175 / 525  Training Loss  0.041401028633117676\n",
            "Epoch  7 Batch  176 / 525  Training Loss  0.0521988570690155\n",
            "Epoch  7 Batch  177 / 525  Training Loss  0.06640155613422394\n",
            "Epoch  7 Batch  178 / 525  Training Loss  0.05626557022333145\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  7 Batch  179 / 525  Training Loss  0.05124887079000473\n",
            "Epoch  7 Batch  180 / 525  Training Loss  0.06248771399259567\n",
            "Epoch  7 Batch  181 / 525  Training Loss  0.03563200682401657\n",
            "Epoch  7 Batch  182 / 525  Training Loss  0.06521724909543991\n",
            "Epoch  7 Batch  183 / 525  Training Loss  0.04881129413843155\n",
            "Epoch  7 Batch  184 / 525  Training Loss  0.068431556224823\n",
            "Epoch  7 Batch  185 / 525  Training Loss  0.05876658484339714\n",
            "Epoch  7 Batch  186 / 525  Training Loss  0.05285501480102539\n",
            "Epoch  7 Batch  187 / 525  Training Loss  0.05453056842088699\n",
            "Epoch  7 Batch  188 / 525  Training Loss  0.06308876723051071\n",
            "Epoch  7 Batch  189 / 525  Training Loss  0.030628159642219543\n",
            "Epoch  7 Batch  190 / 525  Training Loss  0.05076019838452339\n",
            "Epoch  7 Batch  191 / 525  Training Loss  0.05917643383145332\n",
            "Epoch  7 Batch  192 / 525  Training Loss  0.05295313149690628\n",
            "Epoch  7 Batch  193 / 525  Training Loss  0.04635240510106087\n",
            "Epoch  7 Batch  194 / 525  Training Loss  0.05979541689157486\n",
            "Epoch  7 Batch  195 / 525  Training Loss  0.05875221639871597\n",
            "Epoch  7 Batch  196 / 525  Training Loss  0.049321673810482025\n",
            "Epoch  7 Batch  197 / 525  Training Loss  0.03246227651834488\n",
            "Epoch  7 Batch  198 / 525  Training Loss  0.05776696652173996\n",
            "Epoch  7 Batch  199 / 525  Training Loss  0.037533584982156754\n",
            "Epoch  7 Batch  200 / 525  Training Loss  0.056624412536621094\n",
            "Epoch  7 Batch  201 / 525  Training Loss  0.06185168772935867\n",
            "Epoch  7 Batch  202 / 525  Training Loss  0.07696133852005005\n",
            "Epoch  7 Batch  203 / 525  Training Loss  0.04775961488485336\n",
            "Epoch  7 Batch  204 / 525  Training Loss  0.04026886820793152\n",
            "Epoch  7 Batch  205 / 525  Training Loss  0.060433752834796906\n",
            "Epoch  7 Batch  206 / 525  Training Loss  0.04974566400051117\n",
            "Epoch  7 Batch  207 / 525  Training Loss  0.03983261063694954\n",
            "Epoch  7 Batch  208 / 525  Training Loss  0.06714732199907303\n",
            "Epoch  7 Batch  209 / 525  Training Loss  0.04982713982462883\n",
            "Epoch  7 Batch  210 / 525  Training Loss  0.058900535106658936\n",
            "Epoch  7 Batch  211 / 525  Training Loss  0.05226217955350876\n",
            "Epoch  7 Batch  212 / 525  Training Loss  0.030223924666643143\n",
            "Epoch  7 Batch  213 / 525  Training Loss  0.04565998539328575\n",
            "Epoch  7 Batch  214 / 525  Training Loss  0.05832063406705856\n",
            "Epoch  7 Batch  215 / 525  Training Loss  0.05754219368100166\n",
            "Epoch  7 Batch  216 / 525  Training Loss  0.06682474911212921\n",
            "Epoch  7 Batch  217 / 525  Training Loss  0.053462713956832886\n",
            "Epoch  7 Batch  218 / 525  Training Loss  0.06847815215587616\n",
            "Epoch  7 Batch  219 / 525  Training Loss  0.0487968847155571\n",
            "Epoch  7 Batch  220 / 525  Training Loss  0.06744492799043655\n",
            "Epoch  7 Batch  221 / 525  Training Loss  0.055763136595487595\n",
            "Epoch  7 Batch  222 / 525  Training Loss  0.05800781399011612\n",
            "Epoch  7 Batch  223 / 525  Training Loss  0.0568232424557209\n",
            "Epoch  7 Batch  224 / 525  Training Loss  0.04944422096014023\n",
            "Epoch  7 Batch  225 / 525  Training Loss  0.05109231919050217\n",
            "Epoch  7 Batch  226 / 525  Training Loss  0.034898366779088974\n",
            "Epoch  7 Batch  227 / 525  Training Loss  0.05918438360095024\n",
            "Epoch  7 Batch  228 / 525  Training Loss  0.037593774497509\n",
            "Epoch  7 Batch  229 / 525  Training Loss  0.06333669275045395\n",
            "Epoch  7 Batch  230 / 525  Training Loss  0.05366797372698784\n",
            "Epoch  7 Batch  231 / 525  Training Loss  0.06910286843776703\n",
            "Epoch  7 Batch  232 / 525  Training Loss  0.04315024986863136\n",
            "Epoch  7 Batch  233 / 525  Training Loss  0.03934738039970398\n",
            "Epoch  7 Batch  234 / 525  Training Loss  0.040762823075056076\n",
            "Epoch  7 Batch  235 / 525  Training Loss  0.07287897914648056\n",
            "Epoch  7 Batch  236 / 525  Training Loss  0.03763258457183838\n",
            "Epoch  7 Batch  237 / 525  Training Loss  0.07772620022296906\n",
            "Epoch  7 Batch  238 / 525  Training Loss  0.056737519800662994\n",
            "Epoch  7 Batch  239 / 525  Training Loss  0.05457230284810066\n",
            "Epoch  7 Batch  240 / 525  Training Loss  0.046708397567272186\n",
            "Epoch  7 Batch  241 / 525  Training Loss  0.04731055349111557\n",
            "Epoch  7 Batch  242 / 525  Training Loss  0.04682581126689911\n",
            "Epoch  7 Batch  243 / 525  Training Loss  0.05072721093893051\n",
            "Epoch  7 Batch  244 / 525  Training Loss  0.05373690277338028\n",
            "Epoch  7 Batch  245 / 525  Training Loss  0.06812039762735367\n",
            "Epoch  7 Batch  246 / 525  Training Loss  0.060338061302900314\n",
            "Epoch  7 Batch  247 / 525  Training Loss  0.04956347122788429\n",
            "Epoch  7 Batch  248 / 525  Training Loss  0.05583026260137558\n",
            "Epoch  7 Batch  249 / 525  Training Loss  0.03615684062242508\n",
            "Epoch  7 Batch  250 / 525  Training Loss  0.047245003283023834\n",
            "Epoch  7 Batch  251 / 525  Training Loss  0.04439404979348183\n",
            "Epoch  7 Batch  252 / 525  Training Loss  0.08192982524633408\n",
            "Epoch  7 Batch  253 / 525  Training Loss  0.042190439999103546\n",
            "Epoch  7 Batch  254 / 525  Training Loss  0.06732220947742462\n",
            "Epoch  7 Batch  255 / 525  Training Loss  0.05235527828335762\n",
            "Epoch  7 Batch  256 / 525  Training Loss  0.049316566437482834\n",
            "Epoch  7 Batch  257 / 525  Training Loss  0.05235019326210022\n",
            "Epoch  7 Batch  258 / 525  Training Loss  0.05650944635272026\n",
            "Epoch  7 Batch  259 / 525  Training Loss  0.05344945192337036\n",
            "Epoch  7 Batch  260 / 525  Training Loss  0.05047671124339104\n",
            "Epoch  7 Batch  261 / 525  Training Loss  0.048180270940065384\n",
            "Epoch  7 Batch  262 / 525  Training Loss  0.05163121223449707\n",
            "Epoch  7 Batch  263 / 525  Training Loss  0.04684942215681076\n",
            "Epoch  7 Batch  264 / 525  Training Loss  0.054750699549913406\n",
            "Epoch  7 Batch  265 / 525  Training Loss  0.06242366507649422\n",
            "Epoch  7 Batch  266 / 525  Training Loss  0.05030212551355362\n",
            "Epoch  7 Batch  267 / 525  Training Loss  0.05536998435854912\n",
            "Epoch  7 Batch  268 / 525  Training Loss  0.05173831060528755\n",
            "Epoch  7 Batch  269 / 525  Training Loss  0.047646619379520416\n",
            "Epoch  7 Batch  270 / 525  Training Loss  0.06569664180278778\n",
            "Epoch  7 Batch  271 / 525  Training Loss  0.044902365654706955\n",
            "Epoch  7 Batch  272 / 525  Training Loss  0.045987870544195175\n",
            "Epoch  7 Batch  273 / 525  Training Loss  0.03826598450541496\n",
            "Epoch  7 Batch  274 / 525  Training Loss  0.04742133244872093\n",
            "Epoch  7 Batch  275 / 525  Training Loss  0.05559869483113289\n",
            "Epoch  7 Batch  276 / 525  Training Loss  0.03604752942919731\n",
            "Epoch  7 Batch  277 / 525  Training Loss  0.04782601445913315\n",
            "Epoch  7 Batch  278 / 525  Training Loss  0.05796901509165764\n",
            "Epoch  7 Batch  279 / 525  Training Loss  0.06080471724271774\n",
            "Epoch  7 Batch  280 / 525  Training Loss  0.04123588651418686\n",
            "Epoch  7 Batch  281 / 525  Training Loss  0.04670225828886032\n",
            "Epoch  7 Batch  282 / 525  Training Loss  0.09163233637809753\n",
            "Epoch  7 Batch  283 / 525  Training Loss  0.03945845738053322\n",
            "Epoch  7 Batch  284 / 525  Training Loss  0.04615858942270279\n",
            "Epoch  7 Batch  285 / 525  Training Loss  0.06797090172767639\n",
            "Epoch  7 Batch  286 / 525  Training Loss  0.06619509309530258\n",
            "Epoch  7 Batch  287 / 525  Training Loss  0.05888976901769638\n",
            "Epoch  7 Batch  288 / 525  Training Loss  0.03397229313850403\n",
            "Epoch  7 Batch  289 / 525  Training Loss  0.0531499870121479\n",
            "Epoch  7 Batch  290 / 525  Training Loss  0.05006103962659836\n",
            "Epoch  7 Batch  291 / 525  Training Loss  0.05775734782218933\n",
            "Epoch  7 Batch  292 / 525  Training Loss  0.0628327876329422\n",
            "Epoch  7 Batch  293 / 525  Training Loss  0.049018509685993195\n",
            "Epoch  7 Batch  294 / 525  Training Loss  0.06477995216846466\n",
            "Epoch  7 Batch  295 / 525  Training Loss  0.042457643896341324\n",
            "Epoch  7 Batch  296 / 525  Training Loss  0.06055494397878647\n",
            "Epoch  7 Batch  297 / 525  Training Loss  0.06486858427524567\n",
            "Epoch  7 Batch  298 / 525  Training Loss  0.04665748029947281\n",
            "Epoch  7 Batch  299 / 525  Training Loss  0.037732820957899094\n",
            "Epoch  7 Batch  300 / 525  Training Loss  0.05605538561940193\n",
            "Epoch  7 Batch  301 / 525  Training Loss  0.04618070274591446\n",
            "Epoch  7 Batch  302 / 525  Training Loss  0.038573525846004486\n",
            "Epoch  7 Batch  303 / 525  Training Loss  0.04144870489835739\n",
            "Epoch  7 Batch  304 / 525  Training Loss  0.04690481722354889\n",
            "Epoch  7 Batch  305 / 525  Training Loss  0.04758830741047859\n",
            "Epoch  7 Batch  306 / 525  Training Loss  0.05270715430378914\n",
            "Epoch  7 Batch  307 / 525  Training Loss  0.05820939689874649\n",
            "Epoch  7 Batch  308 / 525  Training Loss  0.03178448975086212\n",
            "Epoch  7 Batch  309 / 525  Training Loss  0.03376345336437225\n",
            "Epoch  7 Batch  310 / 525  Training Loss  0.062441013753414154\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  7 Batch  311 / 525  Training Loss  0.04160129278898239\n",
            "Epoch  7 Batch  312 / 525  Training Loss  0.059457261115312576\n",
            "Epoch  7 Batch  313 / 525  Training Loss  0.06056946516036987\n",
            "Epoch  7 Batch  314 / 525  Training Loss  0.049087971448898315\n",
            "Epoch  7 Batch  315 / 525  Training Loss  0.03218787536025047\n",
            "Epoch  7 Batch  316 / 525  Training Loss  0.06364040076732635\n",
            "Epoch  7 Batch  317 / 525  Training Loss  0.06281227618455887\n",
            "Epoch  7 Batch  318 / 525  Training Loss  0.06840799748897552\n",
            "Epoch  7 Batch  319 / 525  Training Loss  0.0635213702917099\n",
            "Epoch  7 Batch  320 / 525  Training Loss  0.043884262442588806\n",
            "Epoch  7 Batch  321 / 525  Training Loss  0.038315724581480026\n",
            "Epoch  7 Batch  322 / 525  Training Loss  0.04266420006752014\n",
            "Epoch  7 Batch  323 / 525  Training Loss  0.035256050527095795\n",
            "Epoch  7 Batch  324 / 525  Training Loss  0.06789229810237885\n",
            "Epoch  7 Batch  325 / 525  Training Loss  0.05747497081756592\n",
            "Epoch  7 Batch  326 / 525  Training Loss  0.05988064408302307\n",
            "Epoch  7 Batch  327 / 525  Training Loss  0.05476786941289902\n",
            "Epoch  7 Batch  328 / 525  Training Loss  0.04928348585963249\n",
            "Epoch  7 Batch  329 / 525  Training Loss  0.049061212688684464\n",
            "Epoch  7 Batch  330 / 525  Training Loss  0.04660128057003021\n",
            "Epoch  7 Batch  331 / 525  Training Loss  0.047915998846292496\n",
            "Epoch  7 Batch  332 / 525  Training Loss  0.05248120427131653\n",
            "Epoch  7 Batch  333 / 525  Training Loss  0.04507971554994583\n",
            "Epoch  7 Batch  334 / 525  Training Loss  0.0699177160859108\n",
            "Epoch  7 Batch  335 / 525  Training Loss  0.037700626999139786\n",
            "Epoch  7 Batch  336 / 525  Training Loss  0.06543367356061935\n",
            "Epoch  7 Batch  337 / 525  Training Loss  0.05188300460577011\n",
            "Epoch  7 Batch  338 / 525  Training Loss  0.059047259390354156\n",
            "Epoch  7 Batch  339 / 525  Training Loss  0.04035148769617081\n",
            "Epoch  7 Batch  340 / 525  Training Loss  0.05173974484205246\n",
            "Epoch  7 Batch  341 / 525  Training Loss  0.05397742986679077\n",
            "Epoch  7 Batch  342 / 525  Training Loss  0.042681287974119186\n",
            "Epoch  7 Batch  343 / 525  Training Loss  0.06205420941114426\n",
            "Epoch  7 Batch  344 / 525  Training Loss  0.04997745528817177\n",
            "Epoch  7 Batch  345 / 525  Training Loss  0.0673743486404419\n",
            "Epoch  7 Batch  346 / 525  Training Loss  0.04473742097616196\n",
            "Epoch  7 Batch  347 / 525  Training Loss  0.04348323494195938\n",
            "Epoch  7 Batch  348 / 525  Training Loss  0.05302786827087402\n",
            "Epoch  7 Batch  349 / 525  Training Loss  0.06970439851284027\n",
            "Epoch  7 Batch  350 / 525  Training Loss  0.06572078168392181\n",
            "Epoch  7 Batch  351 / 525  Training Loss  0.05372999981045723\n",
            "Epoch  7 Batch  352 / 525  Training Loss  0.0431695319712162\n",
            "Epoch  7 Batch  353 / 525  Training Loss  0.05483512952923775\n",
            "Epoch  7 Batch  354 / 525  Training Loss  0.04443834722042084\n",
            "Epoch  7 Batch  355 / 525  Training Loss  0.04722660034894943\n",
            "Epoch  7 Batch  356 / 525  Training Loss  0.04948028177022934\n",
            "Epoch  7 Batch  357 / 525  Training Loss  0.040915459394454956\n",
            "Epoch  7 Batch  358 / 525  Training Loss  0.050607629120349884\n",
            "Epoch  7 Batch  359 / 525  Training Loss  0.06831902265548706\n",
            "Epoch  7 Batch  360 / 525  Training Loss  0.05844762176275253\n",
            "Epoch  7 Batch  361 / 525  Training Loss  0.05868162587285042\n",
            "Epoch  7 Batch  362 / 525  Training Loss  0.04053914174437523\n",
            "Epoch  7 Batch  363 / 525  Training Loss  0.0391530804336071\n",
            "Epoch  7 Batch  364 / 525  Training Loss  0.04575750604271889\n",
            "Epoch  7 Batch  365 / 525  Training Loss  0.041697241365909576\n",
            "Epoch  7 Batch  366 / 525  Training Loss  0.06804795563220978\n",
            "Epoch  7 Batch  367 / 525  Training Loss  0.05175270512700081\n",
            "Epoch  7 Batch  368 / 525  Training Loss  0.06822837889194489\n",
            "Epoch  7 Batch  369 / 525  Training Loss  0.046667762100696564\n",
            "Epoch  7 Batch  370 / 525  Training Loss  0.04753446578979492\n",
            "Epoch  7 Batch  371 / 525  Training Loss  0.05236240476369858\n",
            "Epoch  7 Batch  372 / 525  Training Loss  0.03792038559913635\n",
            "Epoch  7 Batch  373 / 525  Training Loss  0.07144743949174881\n",
            "Epoch  7 Batch  374 / 525  Training Loss  0.05059026926755905\n",
            "Epoch  7 Batch  375 / 525  Training Loss  0.03543398529291153\n",
            "Epoch  7 Batch  376 / 525  Training Loss  0.057733453810214996\n",
            "Epoch  7 Batch  377 / 525  Training Loss  0.05718831345438957\n",
            "Epoch  7 Batch  378 / 525  Training Loss  0.04091796278953552\n",
            "Epoch  7 Batch  379 / 525  Training Loss  0.06060515716671944\n",
            "Epoch  7 Batch  380 / 525  Training Loss  0.054466087371110916\n",
            "Epoch  7 Batch  381 / 525  Training Loss  0.0514068603515625\n",
            "Epoch  7 Batch  382 / 525  Training Loss  0.05623353272676468\n",
            "Epoch  7 Batch  383 / 525  Training Loss  0.058578409254550934\n",
            "Epoch  7 Batch  384 / 525  Training Loss  0.05165359377861023\n",
            "Epoch  7 Batch  385 / 525  Training Loss  0.06908127665519714\n",
            "Epoch  7 Batch  386 / 525  Training Loss  0.051752738654613495\n",
            "Epoch  7 Batch  387 / 525  Training Loss  0.0381479449570179\n",
            "Epoch  7 Batch  388 / 525  Training Loss  0.03859661519527435\n",
            "Epoch  7 Batch  389 / 525  Training Loss  0.04888971522450447\n",
            "Epoch  7 Batch  390 / 525  Training Loss  0.05275692418217659\n",
            "Epoch  7 Batch  391 / 525  Training Loss  0.05036984756588936\n",
            "Epoch  7 Batch  392 / 525  Training Loss  0.05703001096844673\n",
            "Epoch  7 Batch  393 / 525  Training Loss  0.047105446457862854\n",
            "Epoch  7 Batch  394 / 525  Training Loss  0.05886409431695938\n",
            "Epoch  7 Batch  395 / 525  Training Loss  0.04611293226480484\n",
            "Epoch  7 Batch  396 / 525  Training Loss  0.053244102746248245\n",
            "Epoch  7 Batch  397 / 525  Training Loss  0.06299187988042831\n",
            "Epoch  7 Batch  398 / 525  Training Loss  0.036470554769039154\n",
            "Epoch  7 Batch  399 / 525  Training Loss  0.052776992321014404\n",
            "Epoch  7 Batch  400 / 525  Training Loss  0.05796469375491142\n",
            "Epoch  7 Batch  401 / 525  Training Loss  0.04548465460538864\n",
            "Epoch  7 Batch  402 / 525  Training Loss  0.057970453053712845\n",
            "Epoch  7 Batch  403 / 525  Training Loss  0.049750156700611115\n",
            "Epoch  7 Batch  404 / 525  Training Loss  0.06132178753614426\n",
            "Epoch  7 Batch  405 / 525  Training Loss  0.06279122084379196\n",
            "Epoch  7 Batch  406 / 525  Training Loss  0.04654846340417862\n",
            "Epoch  7 Batch  407 / 525  Training Loss  0.0733022540807724\n",
            "Epoch  7 Batch  408 / 525  Training Loss  0.06154518574476242\n",
            "Epoch  7 Batch  409 / 525  Training Loss  0.04324272274971008\n",
            "Epoch  7 Batch  410 / 525  Training Loss  0.05451664328575134\n",
            "Epoch  7 Batch  411 / 525  Training Loss  0.0613849051296711\n",
            "Epoch  7 Batch  412 / 525  Training Loss  0.04323018342256546\n",
            "Epoch  7 Batch  413 / 525  Training Loss  0.060963667929172516\n",
            "Epoch  7 Batch  414 / 525  Training Loss  0.05695805698633194\n",
            "Epoch  7 Batch  415 / 525  Training Loss  0.0567646250128746\n",
            "Epoch  7 Batch  416 / 525  Training Loss  0.04406096413731575\n",
            "Epoch  7 Batch  417 / 525  Training Loss  0.059360139071941376\n",
            "Epoch  7 Batch  418 / 525  Training Loss  0.050854384899139404\n",
            "Epoch  7 Batch  419 / 525  Training Loss  0.04308883473277092\n",
            "Epoch  7 Batch  420 / 525  Training Loss  0.06947620958089828\n",
            "Epoch  7 Batch  421 / 525  Training Loss  0.03609854355454445\n",
            "Epoch  7 Batch  422 / 525  Training Loss  0.050483573228120804\n",
            "Epoch  7 Batch  423 / 525  Training Loss  0.04659014195203781\n",
            "Epoch  7 Batch  424 / 525  Training Loss  0.0603976771235466\n",
            "Epoch  7 Batch  425 / 525  Training Loss  0.05999334529042244\n",
            "Epoch  7 Batch  426 / 525  Training Loss  0.05268830806016922\n",
            "Epoch  7 Batch  427 / 525  Training Loss  0.04496205970644951\n",
            "Epoch  7 Batch  428 / 525  Training Loss  0.0314476378262043\n",
            "Epoch  7 Batch  429 / 525  Training Loss  0.05450490862131119\n",
            "Epoch  7 Batch  430 / 525  Training Loss  0.050171058624982834\n",
            "Epoch  7 Batch  431 / 525  Training Loss  0.05143073946237564\n",
            "Epoch  7 Batch  432 / 525  Training Loss  0.06809960305690765\n",
            "Epoch  7 Batch  433 / 525  Training Loss  0.06643019616603851\n",
            "Epoch  7 Batch  434 / 525  Training Loss  0.06535322964191437\n",
            "Epoch  7 Batch  435 / 525  Training Loss  0.07805712521076202\n",
            "Epoch  7 Batch  436 / 525  Training Loss  0.057253867387771606\n",
            "Epoch  7 Batch  437 / 525  Training Loss  0.05543820187449455\n",
            "Epoch  7 Batch  438 / 525  Training Loss  0.05370355769991875\n",
            "Epoch  7 Batch  439 / 525  Training Loss  0.06322446465492249\n",
            "Epoch  7 Batch  440 / 525  Training Loss  0.07509342581033707\n",
            "Epoch  7 Batch  441 / 525  Training Loss  0.061311013996601105\n",
            "Epoch  7 Batch  442 / 525  Training Loss  0.06151864677667618\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  7 Batch  443 / 525  Training Loss  0.04469285160303116\n",
            "Epoch  7 Batch  444 / 525  Training Loss  0.037904661148786545\n",
            "Epoch  7 Batch  445 / 525  Training Loss  0.06566768139600754\n",
            "Epoch  7 Batch  446 / 525  Training Loss  0.05892353132367134\n",
            "Epoch  7 Batch  447 / 525  Training Loss  0.06023753434419632\n",
            "Epoch  7 Batch  448 / 525  Training Loss  0.0737067461013794\n",
            "Epoch  7 Batch  449 / 525  Training Loss  0.04080154746770859\n",
            "Epoch  7 Batch  450 / 525  Training Loss  0.03879474103450775\n",
            "Epoch  7 Batch  451 / 525  Training Loss  0.058602314442396164\n",
            "Epoch  7 Batch  452 / 525  Training Loss  0.057195741683244705\n",
            "Epoch  7 Batch  453 / 525  Training Loss  0.06017494201660156\n",
            "Epoch  7 Batch  454 / 525  Training Loss  0.05730317905545235\n",
            "Epoch  7 Batch  455 / 525  Training Loss  0.0725470781326294\n",
            "Epoch  7 Batch  456 / 525  Training Loss  0.059097178280353546\n",
            "Epoch  7 Batch  457 / 525  Training Loss  0.04692515730857849\n",
            "Epoch  7 Batch  458 / 525  Training Loss  0.04739558696746826\n",
            "Epoch  7 Batch  459 / 525  Training Loss  0.050094764679670334\n",
            "Epoch  7 Batch  460 / 525  Training Loss  0.05477091670036316\n",
            "Epoch  7 Batch  461 / 525  Training Loss  0.0464584045112133\n",
            "Epoch  7 Batch  462 / 525  Training Loss  0.04900512099266052\n",
            "Epoch  7 Batch  463 / 525  Training Loss  0.06074214726686478\n",
            "Epoch  7 Batch  464 / 525  Training Loss  0.032912321388721466\n",
            "Epoch  7 Batch  465 / 525  Training Loss  0.05525202676653862\n",
            "Epoch  7 Batch  466 / 525  Training Loss  0.03964506834745407\n",
            "Epoch  7 Batch  467 / 525  Training Loss  0.06083863228559494\n",
            "Epoch  7 Batch  468 / 525  Training Loss  0.0470590814948082\n",
            "Epoch  7 Batch  469 / 525  Training Loss  0.06019044667482376\n",
            "Epoch  7 Batch  470 / 525  Training Loss  0.06869794428348541\n",
            "Epoch  7 Batch  471 / 525  Training Loss  0.03647974133491516\n",
            "Epoch  7 Batch  472 / 525  Training Loss  0.06359288096427917\n",
            "Epoch  7 Batch  473 / 525  Training Loss  0.05089141055941582\n",
            "Epoch  7 Batch  474 / 525  Training Loss  0.06338013708591461\n",
            "Epoch  7 Batch  475 / 525  Training Loss  0.05630839616060257\n",
            "Epoch  7 Batch  476 / 525  Training Loss  0.038431841880083084\n",
            "Epoch  7 Batch  477 / 525  Training Loss  0.05720745772123337\n",
            "Epoch  7 Batch  478 / 525  Training Loss  0.06784145534038544\n",
            "Epoch  7 Batch  479 / 525  Training Loss  0.04133806750178337\n",
            "Epoch  7 Batch  480 / 525  Training Loss  0.05163692310452461\n",
            "Epoch  7 Batch  481 / 525  Training Loss  0.05797558277845383\n",
            "Epoch  7 Batch  482 / 525  Training Loss  0.055367451161146164\n",
            "Epoch  7 Batch  483 / 525  Training Loss  0.05880775302648544\n",
            "Epoch  7 Batch  484 / 525  Training Loss  0.0638221949338913\n",
            "Epoch  7 Batch  485 / 525  Training Loss  0.057269103825092316\n",
            "Epoch  7 Batch  486 / 525  Training Loss  0.04943647235631943\n",
            "Epoch  7 Batch  487 / 525  Training Loss  0.06254126131534576\n",
            "Epoch  7 Batch  488 / 525  Training Loss  0.04622620716691017\n",
            "Epoch  7 Batch  489 / 525  Training Loss  0.06817108392715454\n",
            "Epoch  7 Batch  490 / 525  Training Loss  0.0393281988799572\n",
            "Epoch  7 Batch  491 / 525  Training Loss  0.052603818476200104\n",
            "Epoch  7 Batch  492 / 525  Training Loss  0.05338925123214722\n",
            "Epoch  7 Batch  493 / 525  Training Loss  0.0579429566860199\n",
            "Epoch  7 Batch  494 / 525  Training Loss  0.05168958380818367\n",
            "Epoch  7 Batch  495 / 525  Training Loss  0.04063529521226883\n",
            "Epoch  7 Batch  496 / 525  Training Loss  0.05550582334399223\n",
            "Epoch  7 Batch  497 / 525  Training Loss  0.041926056146621704\n",
            "Epoch  7 Batch  498 / 525  Training Loss  0.053421832621097565\n",
            "Epoch  7 Batch  499 / 525  Training Loss  0.04240737482905388\n",
            "Epoch  7 Batch  500 / 525  Training Loss  0.04292260855436325\n",
            "Epoch  7 Batch  501 / 525  Training Loss  0.04824354499578476\n",
            "Epoch  7 Batch  502 / 525  Training Loss  0.05237483233213425\n",
            "Epoch  7 Batch  503 / 525  Training Loss  0.0651199072599411\n",
            "Epoch  7 Batch  504 / 525  Training Loss  0.049576714634895325\n",
            "Epoch  7 Batch  505 / 525  Training Loss  0.0331871435046196\n",
            "Epoch  7 Batch  506 / 525  Training Loss  0.06506212800741196\n",
            "Epoch  7 Batch  507 / 525  Training Loss  0.04721231758594513\n",
            "Epoch  7 Batch  508 / 525  Training Loss  0.06838605552911758\n",
            "Epoch  7 Batch  509 / 525  Training Loss  0.06285671889781952\n",
            "Epoch  7 Batch  510 / 525  Training Loss  0.038615383207798004\n",
            "Epoch  7 Batch  511 / 525  Training Loss  0.04478011280298233\n",
            "Epoch  7 Batch  512 / 525  Training Loss  0.04305042698979378\n",
            "Epoch  7 Batch  513 / 525  Training Loss  0.03191152960062027\n",
            "Epoch  7 Batch  514 / 525  Training Loss  0.053678255528211594\n",
            "Epoch  7 Batch  515 / 525  Training Loss  0.06488846242427826\n",
            "Epoch  7 Batch  516 / 525  Training Loss  0.06087648123502731\n",
            "Epoch  7 Batch  517 / 525  Training Loss  0.060448963195085526\n",
            "Epoch  7 Batch  518 / 525  Training Loss  0.04634353145956993\n",
            "Epoch  7 Batch  519 / 525  Training Loss  0.03999420627951622\n",
            "Epoch  7 Batch  520 / 525  Training Loss  0.040669746696949005\n",
            "Epoch  7 Batch  521 / 525  Training Loss  0.05180548503994942\n",
            "Epoch  7 Batch  522 / 525  Training Loss  0.027976129204034805\n",
            "Epoch  7 Batch  523 / 525  Training Loss  0.039821382611989975\n",
            "Epoch  7 Batch  524 / 525  Training Loss  0.06488518416881561\n",
            "   8    |    -    |   0.052165   |   46.19  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 8\n",
            "Epoch  8 Batch  0 / 525  Training Loss  0.034123994410037994\n",
            "Epoch  8 Batch  1 / 525  Training Loss  0.05534040182828903\n",
            "Epoch  8 Batch  2 / 525  Training Loss  0.024906232953071594\n",
            "Epoch  8 Batch  3 / 525  Training Loss  0.0370725579559803\n",
            "Epoch  8 Batch  4 / 525  Training Loss  0.029387716203927994\n",
            "Epoch  8 Batch  5 / 525  Training Loss  0.03463008627295494\n",
            "Epoch  8 Batch  6 / 525  Training Loss  0.050585877150297165\n",
            "Epoch  8 Batch  7 / 525  Training Loss  0.031038209795951843\n",
            "Epoch  8 Batch  8 / 525  Training Loss  0.04253257066011429\n",
            "Epoch  8 Batch  9 / 525  Training Loss  0.03118855319917202\n",
            "Epoch  8 Batch  10 / 525  Training Loss  0.030668750405311584\n",
            "Epoch  8 Batch  11 / 525  Training Loss  0.03610521927475929\n",
            "Epoch  8 Batch  12 / 525  Training Loss  0.027134845033288002\n",
            "Epoch  8 Batch  13 / 525  Training Loss  0.03159954398870468\n",
            "Epoch  8 Batch  14 / 525  Training Loss  0.0409620963037014\n",
            "Epoch  8 Batch  15 / 525  Training Loss  0.04281739145517349\n",
            "Epoch  8 Batch  16 / 525  Training Loss  0.03164996951818466\n",
            "Epoch  8 Batch  17 / 525  Training Loss  0.03780967369675636\n",
            "Epoch  8 Batch  18 / 525  Training Loss  0.04156406968832016\n",
            "Epoch  8 Batch  19 / 525  Training Loss  0.024868091568350792\n",
            "Epoch  8 Batch  20 / 525  Training Loss  0.0381777361035347\n",
            "Epoch  8 Batch  21 / 525  Training Loss  0.03188534080982208\n",
            "Epoch  8 Batch  22 / 525  Training Loss  0.040083177387714386\n",
            "Epoch  8 Batch  23 / 525  Training Loss  0.044545333832502365\n",
            "Epoch  8 Batch  24 / 525  Training Loss  0.040371689945459366\n",
            "Epoch  8 Batch  25 / 525  Training Loss  0.04349324107170105\n",
            "Epoch  8 Batch  26 / 525  Training Loss  0.03403322398662567\n",
            "Epoch  8 Batch  27 / 525  Training Loss  0.04833443835377693\n",
            "Epoch  8 Batch  28 / 525  Training Loss  0.03648917004466057\n",
            "Epoch  8 Batch  29 / 525  Training Loss  0.02670878730714321\n",
            "Epoch  8 Batch  30 / 525  Training Loss  0.041128259152173996\n",
            "Epoch  8 Batch  31 / 525  Training Loss  0.03589252755045891\n",
            "Epoch  8 Batch  32 / 525  Training Loss  0.03734847158193588\n",
            "Epoch  8 Batch  33 / 525  Training Loss  0.02597132883965969\n",
            "Epoch  8 Batch  34 / 525  Training Loss  0.04514205455780029\n",
            "Epoch  8 Batch  35 / 525  Training Loss  0.056379448622465134\n",
            "Epoch  8 Batch  36 / 525  Training Loss  0.033335961401462555\n",
            "Epoch  8 Batch  37 / 525  Training Loss  0.03534318506717682\n",
            "Epoch  8 Batch  38 / 525  Training Loss  0.03069629892706871\n",
            "Epoch  8 Batch  39 / 525  Training Loss  0.03213753551244736\n",
            "Epoch  8 Batch  40 / 525  Training Loss  0.027450883761048317\n",
            "Epoch  8 Batch  41 / 525  Training Loss  0.024555113166570663\n",
            "Epoch  8 Batch  42 / 525  Training Loss  0.04757619649171829\n",
            "Epoch  8 Batch  43 / 525  Training Loss  0.03397021442651749\n",
            "Epoch  8 Batch  44 / 525  Training Loss  0.03065679408609867\n",
            "Epoch  8 Batch  45 / 525  Training Loss  0.031095674261450768\n",
            "Epoch  8 Batch  46 / 525  Training Loss  0.054107826203107834\n",
            "Epoch  8 Batch  47 / 525  Training Loss  0.04079237952828407\n",
            "Epoch  8 Batch  48 / 525  Training Loss  0.04689570888876915\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  8 Batch  49 / 525  Training Loss  0.0356835313141346\n",
            "Epoch  8 Batch  50 / 525  Training Loss  0.03413615748286247\n",
            "Epoch  8 Batch  51 / 525  Training Loss  0.029998207464814186\n",
            "Epoch  8 Batch  52 / 525  Training Loss  0.03621544688940048\n",
            "Epoch  8 Batch  53 / 525  Training Loss  0.038269903510808945\n",
            "Epoch  8 Batch  54 / 525  Training Loss  0.04469970613718033\n",
            "Epoch  8 Batch  55 / 525  Training Loss  0.038438014686107635\n",
            "Epoch  8 Batch  56 / 525  Training Loss  0.029223274439573288\n",
            "Epoch  8 Batch  57 / 525  Training Loss  0.03292703628540039\n",
            "Epoch  8 Batch  58 / 525  Training Loss  0.030757173895835876\n",
            "Epoch  8 Batch  59 / 525  Training Loss  0.025216495618224144\n",
            "Epoch  8 Batch  60 / 525  Training Loss  0.03124229982495308\n",
            "Epoch  8 Batch  61 / 525  Training Loss  0.03538224846124649\n",
            "Epoch  8 Batch  62 / 525  Training Loss  0.03040795400738716\n",
            "Epoch  8 Batch  63 / 525  Training Loss  0.024066485464572906\n",
            "Epoch  8 Batch  64 / 525  Training Loss  0.034164056181907654\n",
            "Epoch  8 Batch  65 / 525  Training Loss  0.030151981860399246\n",
            "Epoch  8 Batch  66 / 525  Training Loss  0.03574686124920845\n",
            "Epoch  8 Batch  67 / 525  Training Loss  0.03964032977819443\n",
            "Epoch  8 Batch  68 / 525  Training Loss  0.0237587783485651\n",
            "Epoch  8 Batch  69 / 525  Training Loss  0.026822561398148537\n",
            "Epoch  8 Batch  70 / 525  Training Loss  0.038756050169467926\n",
            "Epoch  8 Batch  71 / 525  Training Loss  0.033243484795093536\n",
            "Epoch  8 Batch  72 / 525  Training Loss  0.029200028628110886\n",
            "Epoch  8 Batch  73 / 525  Training Loss  0.03914947062730789\n",
            "Epoch  8 Batch  74 / 525  Training Loss  0.03064044378697872\n",
            "Epoch  8 Batch  75 / 525  Training Loss  0.045640211552381516\n",
            "Epoch  8 Batch  76 / 525  Training Loss  0.031265825033187866\n",
            "Epoch  8 Batch  77 / 525  Training Loss  0.027915537357330322\n",
            "Epoch  8 Batch  78 / 525  Training Loss  0.03588813170790672\n",
            "Epoch  8 Batch  79 / 525  Training Loss  0.028169166296720505\n",
            "Epoch  8 Batch  80 / 525  Training Loss  0.03173141926527023\n",
            "Epoch  8 Batch  81 / 525  Training Loss  0.030297404155135155\n",
            "Epoch  8 Batch  82 / 525  Training Loss  0.023198608309030533\n",
            "Epoch  8 Batch  83 / 525  Training Loss  0.024675283581018448\n",
            "Epoch  8 Batch  84 / 525  Training Loss  0.030233342200517654\n",
            "Epoch  8 Batch  85 / 525  Training Loss  0.03719890117645264\n",
            "Epoch  8 Batch  86 / 525  Training Loss  0.03811439871788025\n",
            "Epoch  8 Batch  87 / 525  Training Loss  0.038554541766643524\n",
            "Epoch  8 Batch  88 / 525  Training Loss  0.04276707023382187\n",
            "Epoch  8 Batch  89 / 525  Training Loss  0.032964885234832764\n",
            "Epoch  8 Batch  90 / 525  Training Loss  0.03556433692574501\n",
            "Epoch  8 Batch  91 / 525  Training Loss  0.036453280597925186\n",
            "Epoch  8 Batch  92 / 525  Training Loss  0.03775478154420853\n",
            "Epoch  8 Batch  93 / 525  Training Loss  0.018782859668135643\n",
            "Epoch  8 Batch  94 / 525  Training Loss  0.023935465142130852\n",
            "Epoch  8 Batch  95 / 525  Training Loss  0.04495930299162865\n",
            "Epoch  8 Batch  96 / 525  Training Loss  0.047335390001535416\n",
            "Epoch  8 Batch  97 / 525  Training Loss  0.04539777338504791\n",
            "Epoch  8 Batch  98 / 525  Training Loss  0.03436223417520523\n",
            "Epoch  8 Batch  99 / 525  Training Loss  0.05398702621459961\n",
            "Epoch  8 Batch  100 / 525  Training Loss  0.03955584764480591\n",
            "Epoch  8 Batch  101 / 525  Training Loss  0.04175261780619621\n",
            "Epoch  8 Batch  102 / 525  Training Loss  0.030720729380846024\n",
            "Epoch  8 Batch  103 / 525  Training Loss  0.051409728825092316\n",
            "Epoch  8 Batch  104 / 525  Training Loss  0.04768551141023636\n",
            "Epoch  8 Batch  105 / 525  Training Loss  0.048563916236162186\n",
            "Epoch  8 Batch  106 / 525  Training Loss  0.0363851897418499\n",
            "Epoch  8 Batch  107 / 525  Training Loss  0.0407438650727272\n",
            "Epoch  8 Batch  108 / 525  Training Loss  0.0408320315182209\n",
            "Epoch  8 Batch  109 / 525  Training Loss  0.049243561923503876\n",
            "Epoch  8 Batch  110 / 525  Training Loss  0.04166705161333084\n",
            "Epoch  8 Batch  111 / 525  Training Loss  0.0258383359760046\n",
            "Epoch  8 Batch  112 / 525  Training Loss  0.04642611742019653\n",
            "Epoch  8 Batch  113 / 525  Training Loss  0.04017697274684906\n",
            "Epoch  8 Batch  114 / 525  Training Loss  0.04336434230208397\n",
            "Epoch  8 Batch  115 / 525  Training Loss  0.04680442065000534\n",
            "Epoch  8 Batch  116 / 525  Training Loss  0.0304728951305151\n",
            "Epoch  8 Batch  117 / 525  Training Loss  0.026247218251228333\n",
            "Epoch  8 Batch  118 / 525  Training Loss  0.025117266923189163\n",
            "Epoch  8 Batch  119 / 525  Training Loss  0.02749471366405487\n",
            "Epoch  8 Batch  120 / 525  Training Loss  0.03231757879257202\n",
            "Epoch  8 Batch  121 / 525  Training Loss  0.032682206481695175\n",
            "Epoch  8 Batch  122 / 525  Training Loss  0.040825508534908295\n",
            "Epoch  8 Batch  123 / 525  Training Loss  0.02973272278904915\n",
            "Epoch  8 Batch  124 / 525  Training Loss  0.028908777981996536\n",
            "Epoch  8 Batch  125 / 525  Training Loss  0.033897507935762405\n",
            "Epoch  8 Batch  126 / 525  Training Loss  0.027179380878806114\n",
            "Epoch  8 Batch  127 / 525  Training Loss  0.028128569945693016\n",
            "Epoch  8 Batch  128 / 525  Training Loss  0.026476437225937843\n",
            "Epoch  8 Batch  129 / 525  Training Loss  0.03326694294810295\n",
            "Epoch  8 Batch  130 / 525  Training Loss  0.026354262605309486\n",
            "Epoch  8 Batch  131 / 525  Training Loss  0.035320915281772614\n",
            "Epoch  8 Batch  132 / 525  Training Loss  0.02649458311498165\n",
            "Epoch  8 Batch  133 / 525  Training Loss  0.04594668745994568\n",
            "Epoch  8 Batch  134 / 525  Training Loss  0.034404207020998\n",
            "Epoch  8 Batch  135 / 525  Training Loss  0.05116957426071167\n",
            "Epoch  8 Batch  136 / 525  Training Loss  0.03862742334604263\n",
            "Epoch  8 Batch  137 / 525  Training Loss  0.03832454979419708\n",
            "Epoch  8 Batch  138 / 525  Training Loss  0.054040729999542236\n",
            "Epoch  8 Batch  139 / 525  Training Loss  0.04239431768655777\n",
            "Epoch  8 Batch  140 / 525  Training Loss  0.030944520607590675\n",
            "Epoch  8 Batch  141 / 525  Training Loss  0.03412063792347908\n",
            "Epoch  8 Batch  142 / 525  Training Loss  0.026355663314461708\n",
            "Epoch  8 Batch  143 / 525  Training Loss  0.035090748220682144\n",
            "Epoch  8 Batch  144 / 525  Training Loss  0.03553765267133713\n",
            "Epoch  8 Batch  145 / 525  Training Loss  0.031861111521720886\n",
            "Epoch  8 Batch  146 / 525  Training Loss  0.029138687998056412\n",
            "Epoch  8 Batch  147 / 525  Training Loss  0.02993161603808403\n",
            "Epoch  8 Batch  148 / 525  Training Loss  0.04834192246198654\n",
            "Epoch  8 Batch  149 / 525  Training Loss  0.041851677000522614\n",
            "Epoch  8 Batch  150 / 525  Training Loss  0.036767683923244476\n",
            "Epoch  8 Batch  151 / 525  Training Loss  0.0497002974152565\n",
            "Epoch  8 Batch  152 / 525  Training Loss  0.045789238065481186\n",
            "Epoch  8 Batch  153 / 525  Training Loss  0.025888344272971153\n",
            "Epoch  8 Batch  154 / 525  Training Loss  0.035493403673172\n",
            "Epoch  8 Batch  155 / 525  Training Loss  0.05005793645977974\n",
            "Epoch  8 Batch  156 / 525  Training Loss  0.037617094814777374\n",
            "Epoch  8 Batch  157 / 525  Training Loss  0.04268025606870651\n",
            "Epoch  8 Batch  158 / 525  Training Loss  0.04107874631881714\n",
            "Epoch  8 Batch  159 / 525  Training Loss  0.041033003479242325\n",
            "Epoch  8 Batch  160 / 525  Training Loss  0.02883332036435604\n",
            "Epoch  8 Batch  161 / 525  Training Loss  0.031002694740891457\n",
            "Epoch  8 Batch  162 / 525  Training Loss  0.0427527092397213\n",
            "Epoch  8 Batch  163 / 525  Training Loss  0.04313189163804054\n",
            "Epoch  8 Batch  164 / 525  Training Loss  0.04407631233334541\n",
            "Epoch  8 Batch  165 / 525  Training Loss  0.02293551154434681\n",
            "Epoch  8 Batch  166 / 525  Training Loss  0.03721120208501816\n",
            "Epoch  8 Batch  167 / 525  Training Loss  0.03930263966321945\n",
            "Epoch  8 Batch  168 / 525  Training Loss  0.027932465076446533\n",
            "Epoch  8 Batch  169 / 525  Training Loss  0.03655216097831726\n",
            "Epoch  8 Batch  170 / 525  Training Loss  0.05807378888130188\n",
            "Epoch  8 Batch  171 / 525  Training Loss  0.04251288250088692\n",
            "Epoch  8 Batch  172 / 525  Training Loss  0.03917882591485977\n",
            "Epoch  8 Batch  173 / 525  Training Loss  0.03452668339014053\n",
            "Epoch  8 Batch  174 / 525  Training Loss  0.04662591218948364\n",
            "Epoch  8 Batch  175 / 525  Training Loss  0.03517152741551399\n",
            "Epoch  8 Batch  176 / 525  Training Loss  0.035532813519239426\n",
            "Epoch  8 Batch  177 / 525  Training Loss  0.06518325954675674\n",
            "Epoch  8 Batch  178 / 525  Training Loss  0.03862638771533966\n",
            "Epoch  8 Batch  179 / 525  Training Loss  0.025973323732614517\n",
            "Epoch  8 Batch  180 / 525  Training Loss  0.02097899839282036\n",
            "Epoch  8 Batch  181 / 525  Training Loss  0.039983831346035004\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  8 Batch  182 / 525  Training Loss  0.035933978855609894\n",
            "Epoch  8 Batch  183 / 525  Training Loss  0.03410268574953079\n",
            "Epoch  8 Batch  184 / 525  Training Loss  0.04179218038916588\n",
            "Epoch  8 Batch  185 / 525  Training Loss  0.04135945439338684\n",
            "Epoch  8 Batch  186 / 525  Training Loss  0.031157756224274635\n",
            "Epoch  8 Batch  187 / 525  Training Loss  0.04190254956483841\n",
            "Epoch  8 Batch  188 / 525  Training Loss  0.03392167016863823\n",
            "Epoch  8 Batch  189 / 525  Training Loss  0.044695593416690826\n",
            "Epoch  8 Batch  190 / 525  Training Loss  0.040901586413383484\n",
            "Epoch  8 Batch  191 / 525  Training Loss  0.029727041721343994\n",
            "Epoch  8 Batch  192 / 525  Training Loss  0.04274119436740875\n",
            "Epoch  8 Batch  193 / 525  Training Loss  0.04516109451651573\n",
            "Epoch  8 Batch  194 / 525  Training Loss  0.04390047490596771\n",
            "Epoch  8 Batch  195 / 525  Training Loss  0.025944018736481667\n",
            "Epoch  8 Batch  196 / 525  Training Loss  0.04871116206049919\n",
            "Epoch  8 Batch  197 / 525  Training Loss  0.046010904014110565\n",
            "Epoch  8 Batch  198 / 525  Training Loss  0.03481528162956238\n",
            "Epoch  8 Batch  199 / 525  Training Loss  0.044794950634241104\n",
            "Epoch  8 Batch  200 / 525  Training Loss  0.0358734056353569\n",
            "Epoch  8 Batch  201 / 525  Training Loss  0.05378492921590805\n",
            "Epoch  8 Batch  202 / 525  Training Loss  0.035807423293590546\n",
            "Epoch  8 Batch  203 / 525  Training Loss  0.04364051669836044\n",
            "Epoch  8 Batch  204 / 525  Training Loss  0.048111796379089355\n",
            "Epoch  8 Batch  205 / 525  Training Loss  0.04091067239642143\n",
            "Epoch  8 Batch  206 / 525  Training Loss  0.027978580445051193\n",
            "Epoch  8 Batch  207 / 525  Training Loss  0.03260393068194389\n",
            "Epoch  8 Batch  208 / 525  Training Loss  0.032410115003585815\n",
            "Epoch  8 Batch  209 / 525  Training Loss  0.04202895238995552\n",
            "Epoch  8 Batch  210 / 525  Training Loss  0.05272551625967026\n",
            "Epoch  8 Batch  211 / 525  Training Loss  0.026576291769742966\n",
            "Epoch  8 Batch  212 / 525  Training Loss  0.035618025809526443\n",
            "Epoch  8 Batch  213 / 525  Training Loss  0.042469654232263565\n",
            "Epoch  8 Batch  214 / 525  Training Loss  0.054147034883499146\n",
            "Epoch  8 Batch  215 / 525  Training Loss  0.03901296481490135\n",
            "Epoch  8 Batch  216 / 525  Training Loss  0.035854488611221313\n",
            "Epoch  8 Batch  217 / 525  Training Loss  0.04630977660417557\n",
            "Epoch  8 Batch  218 / 525  Training Loss  0.02728850580751896\n",
            "Epoch  8 Batch  219 / 525  Training Loss  0.06724026799201965\n",
            "Epoch  8 Batch  220 / 525  Training Loss  0.04891151189804077\n",
            "Epoch  8 Batch  221 / 525  Training Loss  0.04851142689585686\n",
            "Epoch  8 Batch  222 / 525  Training Loss  0.04013253375887871\n",
            "Epoch  8 Batch  223 / 525  Training Loss  0.04513921961188316\n",
            "Epoch  8 Batch  224 / 525  Training Loss  0.03616798669099808\n",
            "Epoch  8 Batch  225 / 525  Training Loss  0.036396004259586334\n",
            "Epoch  8 Batch  226 / 525  Training Loss  0.038046080619096756\n",
            "Epoch  8 Batch  227 / 525  Training Loss  0.04740496724843979\n",
            "Epoch  8 Batch  228 / 525  Training Loss  0.03823096305131912\n",
            "Epoch  8 Batch  229 / 525  Training Loss  0.020647255703806877\n",
            "Epoch  8 Batch  230 / 525  Training Loss  0.03860393911600113\n",
            "Epoch  8 Batch  231 / 525  Training Loss  0.034604720771312714\n",
            "Epoch  8 Batch  232 / 525  Training Loss  0.028773456811904907\n",
            "Epoch  8 Batch  233 / 525  Training Loss  0.03685483708977699\n",
            "Epoch  8 Batch  234 / 525  Training Loss  0.043263278901576996\n",
            "Epoch  8 Batch  235 / 525  Training Loss  0.05065687745809555\n",
            "Epoch  8 Batch  236 / 525  Training Loss  0.029215723276138306\n",
            "Epoch  8 Batch  237 / 525  Training Loss  0.043209366500377655\n",
            "Epoch  8 Batch  238 / 525  Training Loss  0.05421789735555649\n",
            "Epoch  8 Batch  239 / 525  Training Loss  0.044351886957883835\n",
            "Epoch  8 Batch  240 / 525  Training Loss  0.050358861684799194\n",
            "Epoch  8 Batch  241 / 525  Training Loss  0.030416404828429222\n",
            "Epoch  8 Batch  242 / 525  Training Loss  0.048873625695705414\n",
            "Epoch  8 Batch  243 / 525  Training Loss  0.03336906433105469\n",
            "Epoch  8 Batch  244 / 525  Training Loss  0.03616020083427429\n",
            "Epoch  8 Batch  245 / 525  Training Loss  0.03562251478433609\n",
            "Epoch  8 Batch  246 / 525  Training Loss  0.054051268845796585\n",
            "Epoch  8 Batch  247 / 525  Training Loss  0.027832936495542526\n",
            "Epoch  8 Batch  248 / 525  Training Loss  0.03145293518900871\n",
            "Epoch  8 Batch  249 / 525  Training Loss  0.031028831377625465\n",
            "Epoch  8 Batch  250 / 525  Training Loss  0.029338324442505836\n",
            "Epoch  8 Batch  251 / 525  Training Loss  0.03470504283905029\n",
            "Epoch  8 Batch  252 / 525  Training Loss  0.027947083115577698\n",
            "Epoch  8 Batch  253 / 525  Training Loss  0.044319070875644684\n",
            "Epoch  8 Batch  254 / 525  Training Loss  0.04268821328878403\n",
            "Epoch  8 Batch  255 / 525  Training Loss  0.02723941206932068\n",
            "Epoch  8 Batch  256 / 525  Training Loss  0.05427498742938042\n",
            "Epoch  8 Batch  257 / 525  Training Loss  0.039068207144737244\n",
            "Epoch  8 Batch  258 / 525  Training Loss  0.04540226235985756\n",
            "Epoch  8 Batch  259 / 525  Training Loss  0.039700768887996674\n",
            "Epoch  8 Batch  260 / 525  Training Loss  0.04278993234038353\n",
            "Epoch  8 Batch  261 / 525  Training Loss  0.03300010785460472\n",
            "Epoch  8 Batch  262 / 525  Training Loss  0.03619249910116196\n",
            "Epoch  8 Batch  263 / 525  Training Loss  0.0342111736536026\n",
            "Epoch  8 Batch  264 / 525  Training Loss  0.03974668309092522\n",
            "Epoch  8 Batch  265 / 525  Training Loss  0.023530717939138412\n",
            "Epoch  8 Batch  266 / 525  Training Loss  0.0436350479722023\n",
            "Epoch  8 Batch  267 / 525  Training Loss  0.017654234543442726\n",
            "Epoch  8 Batch  268 / 525  Training Loss  0.04739517718553543\n",
            "Epoch  8 Batch  269 / 525  Training Loss  0.028531651943922043\n",
            "Epoch  8 Batch  270 / 525  Training Loss  0.043233949691057205\n",
            "Epoch  8 Batch  271 / 525  Training Loss  0.041326310485601425\n",
            "Epoch  8 Batch  272 / 525  Training Loss  0.03683168813586235\n",
            "Epoch  8 Batch  273 / 525  Training Loss  0.044424496591091156\n",
            "Epoch  8 Batch  274 / 525  Training Loss  0.027195056900382042\n",
            "Epoch  8 Batch  275 / 525  Training Loss  0.041761744767427444\n",
            "Epoch  8 Batch  276 / 525  Training Loss  0.03510301560163498\n",
            "Epoch  8 Batch  277 / 525  Training Loss  0.04528680443763733\n",
            "Epoch  8 Batch  278 / 525  Training Loss  0.039568278938531876\n",
            "Epoch  8 Batch  279 / 525  Training Loss  0.04024970531463623\n",
            "Epoch  8 Batch  280 / 525  Training Loss  0.02283875271677971\n",
            "Epoch  8 Batch  281 / 525  Training Loss  0.02595578506588936\n",
            "Epoch  8 Batch  282 / 525  Training Loss  0.037616174668073654\n",
            "Epoch  8 Batch  283 / 525  Training Loss  0.03231202811002731\n",
            "Epoch  8 Batch  284 / 525  Training Loss  0.048873160034418106\n",
            "Epoch  8 Batch  285 / 525  Training Loss  0.04081325978040695\n",
            "Epoch  8 Batch  286 / 525  Training Loss  0.030948709696531296\n",
            "Epoch  8 Batch  287 / 525  Training Loss  0.042046040296554565\n",
            "Epoch  8 Batch  288 / 525  Training Loss  0.04589086398482323\n",
            "Epoch  8 Batch  289 / 525  Training Loss  0.0313265323638916\n",
            "Epoch  8 Batch  290 / 525  Training Loss  0.0417751744389534\n",
            "Epoch  8 Batch  291 / 525  Training Loss  0.03212512284517288\n",
            "Epoch  8 Batch  292 / 525  Training Loss  0.039001382887363434\n",
            "Epoch  8 Batch  293 / 525  Training Loss  0.040981072932481766\n",
            "Epoch  8 Batch  294 / 525  Training Loss  0.038062114268541336\n",
            "Epoch  8 Batch  295 / 525  Training Loss  0.04341616481542587\n",
            "Epoch  8 Batch  296 / 525  Training Loss  0.0381123423576355\n",
            "Epoch  8 Batch  297 / 525  Training Loss  0.027366865426301956\n",
            "Epoch  8 Batch  298 / 525  Training Loss  0.031105827540159225\n",
            "Epoch  8 Batch  299 / 525  Training Loss  0.04297363758087158\n",
            "Epoch  8 Batch  300 / 525  Training Loss  0.045855022966861725\n",
            "Epoch  8 Batch  301 / 525  Training Loss  0.053612738847732544\n",
            "Epoch  8 Batch  302 / 525  Training Loss  0.026706358417868614\n",
            "Epoch  8 Batch  303 / 525  Training Loss  0.0346275195479393\n",
            "Epoch  8 Batch  304 / 525  Training Loss  0.041861191391944885\n",
            "Epoch  8 Batch  305 / 525  Training Loss  0.029972095042467117\n",
            "Epoch  8 Batch  306 / 525  Training Loss  0.04930129647254944\n",
            "Epoch  8 Batch  307 / 525  Training Loss  0.031014492735266685\n",
            "Epoch  8 Batch  308 / 525  Training Loss  0.053218673914670944\n",
            "Epoch  8 Batch  309 / 525  Training Loss  0.04329882189631462\n",
            "Epoch  8 Batch  310 / 525  Training Loss  0.03571879491209984\n",
            "Epoch  8 Batch  311 / 525  Training Loss  0.04159624129533768\n",
            "Epoch  8 Batch  312 / 525  Training Loss  0.02981695532798767\n",
            "Epoch  8 Batch  313 / 525  Training Loss  0.0409574881196022\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  8 Batch  314 / 525  Training Loss  0.04390375688672066\n",
            "Epoch  8 Batch  315 / 525  Training Loss  0.03316844627261162\n",
            "Epoch  8 Batch  316 / 525  Training Loss  0.03570947423577309\n",
            "Epoch  8 Batch  317 / 525  Training Loss  0.02941906824707985\n",
            "Epoch  8 Batch  318 / 525  Training Loss  0.0438203290104866\n",
            "Epoch  8 Batch  319 / 525  Training Loss  0.03206745535135269\n",
            "Epoch  8 Batch  320 / 525  Training Loss  0.027757082134485245\n",
            "Epoch  8 Batch  321 / 525  Training Loss  0.031114790588617325\n",
            "Epoch  8 Batch  322 / 525  Training Loss  0.03353498503565788\n",
            "Epoch  8 Batch  323 / 525  Training Loss  0.0413910336792469\n",
            "Epoch  8 Batch  324 / 525  Training Loss  0.04052503779530525\n",
            "Epoch  8 Batch  325 / 525  Training Loss  0.037977077066898346\n",
            "Epoch  8 Batch  326 / 525  Training Loss  0.04588233307003975\n",
            "Epoch  8 Batch  327 / 525  Training Loss  0.02338121458888054\n",
            "Epoch  8 Batch  328 / 525  Training Loss  0.04917832091450691\n",
            "Epoch  8 Batch  329 / 525  Training Loss  0.028718149289488792\n",
            "Epoch  8 Batch  330 / 525  Training Loss  0.047859180718660355\n",
            "Epoch  8 Batch  331 / 525  Training Loss  0.03725776821374893\n",
            "Epoch  8 Batch  332 / 525  Training Loss  0.033129964023828506\n",
            "Epoch  8 Batch  333 / 525  Training Loss  0.07434157282114029\n",
            "Epoch  8 Batch  334 / 525  Training Loss  0.04051610082387924\n",
            "Epoch  8 Batch  335 / 525  Training Loss  0.04757338762283325\n",
            "Epoch  8 Batch  336 / 525  Training Loss  0.047292761504650116\n",
            "Epoch  8 Batch  337 / 525  Training Loss  0.030528614297509193\n",
            "Epoch  8 Batch  338 / 525  Training Loss  0.03906954079866409\n",
            "Epoch  8 Batch  339 / 525  Training Loss  0.058674149215221405\n",
            "Epoch  8 Batch  340 / 525  Training Loss  0.025503506883978844\n",
            "Epoch  8 Batch  341 / 525  Training Loss  0.03118060901761055\n",
            "Epoch  8 Batch  342 / 525  Training Loss  0.04452214390039444\n",
            "Epoch  8 Batch  343 / 525  Training Loss  0.05114899203181267\n",
            "Epoch  8 Batch  344 / 525  Training Loss  0.03669540211558342\n",
            "Epoch  8 Batch  345 / 525  Training Loss  0.03572753816843033\n",
            "Epoch  8 Batch  346 / 525  Training Loss  0.05209842324256897\n",
            "Epoch  8 Batch  347 / 525  Training Loss  0.032603878527879715\n",
            "Epoch  8 Batch  348 / 525  Training Loss  0.044264357537031174\n",
            "Epoch  8 Batch  349 / 525  Training Loss  0.034676652401685715\n",
            "Epoch  8 Batch  350 / 525  Training Loss  0.07598651945590973\n",
            "Epoch  8 Batch  351 / 525  Training Loss  0.06282403320074081\n",
            "Epoch  8 Batch  352 / 525  Training Loss  0.03056028112769127\n",
            "Epoch  8 Batch  353 / 525  Training Loss  0.022481979802250862\n",
            "Epoch  8 Batch  354 / 525  Training Loss  0.02483866736292839\n",
            "Epoch  8 Batch  355 / 525  Training Loss  0.05416150018572807\n",
            "Epoch  8 Batch  356 / 525  Training Loss  0.04967814311385155\n",
            "Epoch  8 Batch  357 / 525  Training Loss  0.048061661422252655\n",
            "Epoch  8 Batch  358 / 525  Training Loss  0.042547401040792465\n",
            "Epoch  8 Batch  359 / 525  Training Loss  0.034321825951337814\n",
            "Epoch  8 Batch  360 / 525  Training Loss  0.03915095701813698\n",
            "Epoch  8 Batch  361 / 525  Training Loss  0.03764965385198593\n",
            "Epoch  8 Batch  362 / 525  Training Loss  0.048495396971702576\n",
            "Epoch  8 Batch  363 / 525  Training Loss  0.034518346190452576\n",
            "Epoch  8 Batch  364 / 525  Training Loss  0.03960917890071869\n",
            "Epoch  8 Batch  365 / 525  Training Loss  0.05498112365603447\n",
            "Epoch  8 Batch  366 / 525  Training Loss  0.04367949813604355\n",
            "Epoch  8 Batch  367 / 525  Training Loss  0.03621915727853775\n",
            "Epoch  8 Batch  368 / 525  Training Loss  0.03814362734556198\n",
            "Epoch  8 Batch  369 / 525  Training Loss  0.034820251166820526\n",
            "Epoch  8 Batch  370 / 525  Training Loss  0.04884997010231018\n",
            "Epoch  8 Batch  371 / 525  Training Loss  0.031811393797397614\n",
            "Epoch  8 Batch  372 / 525  Training Loss  0.051331572234630585\n",
            "Epoch  8 Batch  373 / 525  Training Loss  0.04743967205286026\n",
            "Epoch  8 Batch  374 / 525  Training Loss  0.02851122058928013\n",
            "Epoch  8 Batch  375 / 525  Training Loss  0.05504788085818291\n",
            "Epoch  8 Batch  376 / 525  Training Loss  0.030099596828222275\n",
            "Epoch  8 Batch  377 / 525  Training Loss  0.02878602221608162\n",
            "Epoch  8 Batch  378 / 525  Training Loss  0.04743693023920059\n",
            "Epoch  8 Batch  379 / 525  Training Loss  0.0523812361061573\n",
            "Epoch  8 Batch  380 / 525  Training Loss  0.033485278487205505\n",
            "Epoch  8 Batch  381 / 525  Training Loss  0.026797721162438393\n",
            "Epoch  8 Batch  382 / 525  Training Loss  0.030404115095734596\n",
            "Epoch  8 Batch  383 / 525  Training Loss  0.04866599291563034\n",
            "Epoch  8 Batch  384 / 525  Training Loss  0.03677348047494888\n",
            "Epoch  8 Batch  385 / 525  Training Loss  0.04303058609366417\n",
            "Epoch  8 Batch  386 / 525  Training Loss  0.034823764115571976\n",
            "Epoch  8 Batch  387 / 525  Training Loss  0.03650698438286781\n",
            "Epoch  8 Batch  388 / 525  Training Loss  0.05281422287225723\n",
            "Epoch  8 Batch  389 / 525  Training Loss  0.032911110669374466\n",
            "Epoch  8 Batch  390 / 525  Training Loss  0.04869890958070755\n",
            "Epoch  8 Batch  391 / 525  Training Loss  0.04421227425336838\n",
            "Epoch  8 Batch  392 / 525  Training Loss  0.03236667066812515\n",
            "Epoch  8 Batch  393 / 525  Training Loss  0.0328403040766716\n",
            "Epoch  8 Batch  394 / 525  Training Loss  0.056988365948200226\n",
            "Epoch  8 Batch  395 / 525  Training Loss  0.02544981800019741\n",
            "Epoch  8 Batch  396 / 525  Training Loss  0.029668187722563744\n",
            "Epoch  8 Batch  397 / 525  Training Loss  0.0634278804063797\n",
            "Epoch  8 Batch  398 / 525  Training Loss  0.03336375951766968\n",
            "Epoch  8 Batch  399 / 525  Training Loss  0.03307671099901199\n",
            "Epoch  8 Batch  400 / 525  Training Loss  0.040319107472896576\n",
            "Epoch  8 Batch  401 / 525  Training Loss  0.04156852141022682\n",
            "Epoch  8 Batch  402 / 525  Training Loss  0.029363790526986122\n",
            "Epoch  8 Batch  403 / 525  Training Loss  0.04368669539690018\n",
            "Epoch  8 Batch  404 / 525  Training Loss  0.05375633388757706\n",
            "Epoch  8 Batch  405 / 525  Training Loss  0.03571794182062149\n",
            "Epoch  8 Batch  406 / 525  Training Loss  0.05955915525555611\n",
            "Epoch  8 Batch  407 / 525  Training Loss  0.04509323835372925\n",
            "Epoch  8 Batch  408 / 525  Training Loss  0.04028219357132912\n",
            "Epoch  8 Batch  409 / 525  Training Loss  0.03314047306776047\n",
            "Epoch  8 Batch  410 / 525  Training Loss  0.029585862532258034\n",
            "Epoch  8 Batch  411 / 525  Training Loss  0.03511027246713638\n",
            "Epoch  8 Batch  412 / 525  Training Loss  0.029516760259866714\n",
            "Epoch  8 Batch  413 / 525  Training Loss  0.031404733657836914\n",
            "Epoch  8 Batch  414 / 525  Training Loss  0.051715146750211716\n",
            "Epoch  8 Batch  415 / 525  Training Loss  0.048545967787504196\n",
            "Epoch  8 Batch  416 / 525  Training Loss  0.033119745552539825\n",
            "Epoch  8 Batch  417 / 525  Training Loss  0.03640458732843399\n",
            "Epoch  8 Batch  418 / 525  Training Loss  0.028171509504318237\n",
            "Epoch  8 Batch  419 / 525  Training Loss  0.04567410424351692\n",
            "Epoch  8 Batch  420 / 525  Training Loss  0.027350028976798058\n",
            "Epoch  8 Batch  421 / 525  Training Loss  0.039115168154239655\n",
            "Epoch  8 Batch  422 / 525  Training Loss  0.03783069923520088\n",
            "Epoch  8 Batch  423 / 525  Training Loss  0.039270199835300446\n",
            "Epoch  8 Batch  424 / 525  Training Loss  0.035672515630722046\n",
            "Epoch  8 Batch  425 / 525  Training Loss  0.041603196412324905\n",
            "Epoch  8 Batch  426 / 525  Training Loss  0.041805069893598557\n",
            "Epoch  8 Batch  427 / 525  Training Loss  0.0548073872923851\n",
            "Epoch  8 Batch  428 / 525  Training Loss  0.055554378777742386\n",
            "Epoch  8 Batch  429 / 525  Training Loss  0.036605361849069595\n",
            "Epoch  8 Batch  430 / 525  Training Loss  0.030670592561364174\n",
            "Epoch  8 Batch  431 / 525  Training Loss  0.04001235216856003\n",
            "Epoch  8 Batch  432 / 525  Training Loss  0.023953363299369812\n",
            "Epoch  8 Batch  433 / 525  Training Loss  0.06250675022602081\n",
            "Epoch  8 Batch  434 / 525  Training Loss  0.03346628695726395\n",
            "Epoch  8 Batch  435 / 525  Training Loss  0.05622214823961258\n",
            "Epoch  8 Batch  436 / 525  Training Loss  0.044371429830789566\n",
            "Epoch  8 Batch  437 / 525  Training Loss  0.02825058065354824\n",
            "Epoch  8 Batch  438 / 525  Training Loss  0.04672561213374138\n",
            "Epoch  8 Batch  439 / 525  Training Loss  0.0428115651011467\n",
            "Epoch  8 Batch  440 / 525  Training Loss  0.040894582867622375\n",
            "Epoch  8 Batch  441 / 525  Training Loss  0.03930849954485893\n",
            "Epoch  8 Batch  442 / 525  Training Loss  0.04186486452817917\n",
            "Epoch  8 Batch  443 / 525  Training Loss  0.03257609158754349\n",
            "Epoch  8 Batch  444 / 525  Training Loss  0.026044854894280434\n",
            "Epoch  8 Batch  445 / 525  Training Loss  0.039457954466342926\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  8 Batch  446 / 525  Training Loss  0.03309748321771622\n",
            "Epoch  8 Batch  447 / 525  Training Loss  0.03307165950536728\n",
            "Epoch  8 Batch  448 / 525  Training Loss  0.037545960396528244\n",
            "Epoch  8 Batch  449 / 525  Training Loss  0.024120602756738663\n",
            "Epoch  8 Batch  450 / 525  Training Loss  0.04387517273426056\n",
            "Epoch  8 Batch  451 / 525  Training Loss  0.02512463927268982\n",
            "Epoch  8 Batch  452 / 525  Training Loss  0.03074529767036438\n",
            "Epoch  8 Batch  453 / 525  Training Loss  0.04602454602718353\n",
            "Epoch  8 Batch  454 / 525  Training Loss  0.03385182097554207\n",
            "Epoch  8 Batch  455 / 525  Training Loss  0.03090738132596016\n",
            "Epoch  8 Batch  456 / 525  Training Loss  0.035439036786556244\n",
            "Epoch  8 Batch  457 / 525  Training Loss  0.026852544397115707\n",
            "Epoch  8 Batch  458 / 525  Training Loss  0.024561237543821335\n",
            "Epoch  8 Batch  459 / 525  Training Loss  0.028001228347420692\n",
            "Epoch  8 Batch  460 / 525  Training Loss  0.06143980100750923\n",
            "Epoch  8 Batch  461 / 525  Training Loss  0.042484890669584274\n",
            "Epoch  8 Batch  462 / 525  Training Loss  0.028325121849775314\n",
            "Epoch  8 Batch  463 / 525  Training Loss  0.025907209143042564\n",
            "Epoch  8 Batch  464 / 525  Training Loss  0.048468850553035736\n",
            "Epoch  8 Batch  465 / 525  Training Loss  0.07367614656686783\n",
            "Epoch  8 Batch  466 / 525  Training Loss  0.052265994250774384\n",
            "Epoch  8 Batch  467 / 525  Training Loss  0.02280583791434765\n",
            "Epoch  8 Batch  468 / 525  Training Loss  0.033783458173274994\n",
            "Epoch  8 Batch  469 / 525  Training Loss  0.03800947219133377\n",
            "Epoch  8 Batch  470 / 525  Training Loss  0.028416279703378677\n",
            "Epoch  8 Batch  471 / 525  Training Loss  0.05274407938122749\n",
            "Epoch  8 Batch  472 / 525  Training Loss  0.025100404396653175\n",
            "Epoch  8 Batch  473 / 525  Training Loss  0.033743251115083694\n",
            "Epoch  8 Batch  474 / 525  Training Loss  0.03934147208929062\n",
            "Epoch  8 Batch  475 / 525  Training Loss  0.05160055309534073\n",
            "Epoch  8 Batch  476 / 525  Training Loss  0.029463743790984154\n",
            "Epoch  8 Batch  477 / 525  Training Loss  0.046494826674461365\n",
            "Epoch  8 Batch  478 / 525  Training Loss  0.05945311859250069\n",
            "Epoch  8 Batch  479 / 525  Training Loss  0.040054481476545334\n",
            "Epoch  8 Batch  480 / 525  Training Loss  0.04176035150885582\n",
            "Epoch  8 Batch  481 / 525  Training Loss  0.04100548103451729\n",
            "Epoch  8 Batch  482 / 525  Training Loss  0.026549240574240685\n",
            "Epoch  8 Batch  483 / 525  Training Loss  0.041439495980739594\n",
            "Epoch  8 Batch  484 / 525  Training Loss  0.034036651253700256\n",
            "Epoch  8 Batch  485 / 525  Training Loss  0.053002990782260895\n",
            "Epoch  8 Batch  486 / 525  Training Loss  0.03262786194682121\n",
            "Epoch  8 Batch  487 / 525  Training Loss  0.038271475583314896\n",
            "Epoch  8 Batch  488 / 525  Training Loss  0.04417068883776665\n",
            "Epoch  8 Batch  489 / 525  Training Loss  0.046108923852443695\n",
            "Epoch  8 Batch  490 / 525  Training Loss  0.03125514090061188\n",
            "Epoch  8 Batch  491 / 525  Training Loss  0.037772733718156815\n",
            "Epoch  8 Batch  492 / 525  Training Loss  0.03385908529162407\n",
            "Epoch  8 Batch  493 / 525  Training Loss  0.03506074100732803\n",
            "Epoch  8 Batch  494 / 525  Training Loss  0.03086528740823269\n",
            "Epoch  8 Batch  495 / 525  Training Loss  0.03659399598836899\n",
            "Epoch  8 Batch  496 / 525  Training Loss  0.0438556931912899\n",
            "Epoch  8 Batch  497 / 525  Training Loss  0.043595679104328156\n",
            "Epoch  8 Batch  498 / 525  Training Loss  0.05417376756668091\n",
            "Epoch  8 Batch  499 / 525  Training Loss  0.03807486221194267\n",
            "Epoch  8 Batch  500 / 525  Training Loss  0.047626104205846786\n",
            "Epoch  8 Batch  501 / 525  Training Loss  0.03314982354640961\n",
            "Epoch  8 Batch  502 / 525  Training Loss  0.026123112067580223\n",
            "Epoch  8 Batch  503 / 525  Training Loss  0.05407450348138809\n",
            "Epoch  8 Batch  504 / 525  Training Loss  0.03144797682762146\n",
            "Epoch  8 Batch  505 / 525  Training Loss  0.03879065066576004\n",
            "Epoch  8 Batch  506 / 525  Training Loss  0.03510398417711258\n",
            "Epoch  8 Batch  507 / 525  Training Loss  0.039726369082927704\n",
            "Epoch  8 Batch  508 / 525  Training Loss  0.04231167584657669\n",
            "Epoch  8 Batch  509 / 525  Training Loss  0.04749001935124397\n",
            "Epoch  8 Batch  510 / 525  Training Loss  0.042786307632923126\n",
            "Epoch  8 Batch  511 / 525  Training Loss  0.03077651932835579\n",
            "Epoch  8 Batch  512 / 525  Training Loss  0.05157839134335518\n",
            "Epoch  8 Batch  513 / 525  Training Loss  0.036400336772203445\n",
            "Epoch  8 Batch  514 / 525  Training Loss  0.025634685531258583\n",
            "Epoch  8 Batch  515 / 525  Training Loss  0.03769266605377197\n",
            "Epoch  8 Batch  516 / 525  Training Loss  0.03420228883624077\n",
            "Epoch  8 Batch  517 / 525  Training Loss  0.035821862518787384\n",
            "Epoch  8 Batch  518 / 525  Training Loss  0.031267307698726654\n",
            "Epoch  8 Batch  519 / 525  Training Loss  0.054232049733400345\n",
            "Epoch  8 Batch  520 / 525  Training Loss  0.03266075998544693\n",
            "Epoch  8 Batch  521 / 525  Training Loss  0.03977590054273605\n",
            "Epoch  8 Batch  522 / 525  Training Loss  0.046024221926927567\n",
            "Epoch  8 Batch  523 / 525  Training Loss  0.037940025329589844\n",
            "Epoch  8 Batch  524 / 525  Training Loss  0.04327382892370224\n",
            "   9    |    -    |   0.038278   |   50.70  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 9\n",
            "Epoch  9 Batch  0 / 525  Training Loss  0.032461948692798615\n",
            "Epoch  9 Batch  1 / 525  Training Loss  0.02401277795433998\n",
            "Epoch  9 Batch  2 / 525  Training Loss  0.021847199648618698\n",
            "Epoch  9 Batch  3 / 525  Training Loss  0.019802231341600418\n",
            "Epoch  9 Batch  4 / 525  Training Loss  0.025091346353292465\n",
            "Epoch  9 Batch  5 / 525  Training Loss  0.02279815450310707\n",
            "Epoch  9 Batch  6 / 525  Training Loss  0.019465720281004906\n",
            "Epoch  9 Batch  7 / 525  Training Loss  0.018418874591588974\n",
            "Epoch  9 Batch  8 / 525  Training Loss  0.023693090304732323\n",
            "Epoch  9 Batch  9 / 525  Training Loss  0.02733541652560234\n",
            "Epoch  9 Batch  10 / 525  Training Loss  0.03180140256881714\n",
            "Epoch  9 Batch  11 / 525  Training Loss  0.02168552204966545\n",
            "Epoch  9 Batch  12 / 525  Training Loss  0.026498839259147644\n",
            "Epoch  9 Batch  13 / 525  Training Loss  0.025257404893636703\n",
            "Epoch  9 Batch  14 / 525  Training Loss  0.02051810547709465\n",
            "Epoch  9 Batch  15 / 525  Training Loss  0.02739143744111061\n",
            "Epoch  9 Batch  16 / 525  Training Loss  0.01867539808154106\n",
            "Epoch  9 Batch  17 / 525  Training Loss  0.019489463418722153\n",
            "Epoch  9 Batch  18 / 525  Training Loss  0.02671644650399685\n",
            "Epoch  9 Batch  19 / 525  Training Loss  0.023691019043326378\n",
            "Epoch  9 Batch  20 / 525  Training Loss  0.026835758239030838\n",
            "Epoch  9 Batch  21 / 525  Training Loss  0.02634010836482048\n",
            "Epoch  9 Batch  22 / 525  Training Loss  0.016100460663437843\n",
            "Epoch  9 Batch  23 / 525  Training Loss  0.010501493699848652\n",
            "Epoch  9 Batch  24 / 525  Training Loss  0.01754971221089363\n",
            "Epoch  9 Batch  25 / 525  Training Loss  0.028399625793099403\n",
            "Epoch  9 Batch  26 / 525  Training Loss  0.018466748297214508\n",
            "Epoch  9 Batch  27 / 525  Training Loss  0.01871751807630062\n",
            "Epoch  9 Batch  28 / 525  Training Loss  0.015028722584247589\n",
            "Epoch  9 Batch  29 / 525  Training Loss  0.020818645134568214\n",
            "Epoch  9 Batch  30 / 525  Training Loss  0.03372891992330551\n",
            "Epoch  9 Batch  31 / 525  Training Loss  0.021053066477179527\n",
            "Epoch  9 Batch  32 / 525  Training Loss  0.03483344241976738\n",
            "Epoch  9 Batch  33 / 525  Training Loss  0.0326957181096077\n",
            "Epoch  9 Batch  34 / 525  Training Loss  0.018697276711463928\n",
            "Epoch  9 Batch  35 / 525  Training Loss  0.020606083795428276\n",
            "Epoch  9 Batch  36 / 525  Training Loss  0.025533834472298622\n",
            "Epoch  9 Batch  37 / 525  Training Loss  0.012406847439706326\n",
            "Epoch  9 Batch  38 / 525  Training Loss  0.024598564952611923\n",
            "Epoch  9 Batch  39 / 525  Training Loss  0.023811455816030502\n",
            "Epoch  9 Batch  40 / 525  Training Loss  0.03004119172692299\n",
            "Epoch  9 Batch  41 / 525  Training Loss  0.02089075557887554\n",
            "Epoch  9 Batch  42 / 525  Training Loss  0.02374347485601902\n",
            "Epoch  9 Batch  43 / 525  Training Loss  0.024329308420419693\n",
            "Epoch  9 Batch  44 / 525  Training Loss  0.028951773419976234\n",
            "Epoch  9 Batch  45 / 525  Training Loss  0.024867752566933632\n",
            "Epoch  9 Batch  46 / 525  Training Loss  0.014876781031489372\n",
            "Epoch  9 Batch  47 / 525  Training Loss  0.017044909298419952\n",
            "Epoch  9 Batch  48 / 525  Training Loss  0.019692590460181236\n",
            "Epoch  9 Batch  49 / 525  Training Loss  0.04150528833270073\n",
            "Epoch  9 Batch  50 / 525  Training Loss  0.014048025012016296\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  9 Batch  51 / 525  Training Loss  0.025323521345853806\n",
            "Epoch  9 Batch  52 / 525  Training Loss  0.018910761922597885\n",
            "Epoch  9 Batch  53 / 525  Training Loss  0.02072550170123577\n",
            "Epoch  9 Batch  54 / 525  Training Loss  0.016248097643256187\n",
            "Epoch  9 Batch  55 / 525  Training Loss  0.027028515934944153\n",
            "Epoch  9 Batch  56 / 525  Training Loss  0.009709829464554787\n",
            "Epoch  9 Batch  57 / 525  Training Loss  0.023274170234799385\n",
            "Epoch  9 Batch  58 / 525  Training Loss  0.024432631209492683\n",
            "Epoch  9 Batch  59 / 525  Training Loss  0.021473772823810577\n",
            "Epoch  9 Batch  60 / 525  Training Loss  0.024245429784059525\n",
            "Epoch  9 Batch  61 / 525  Training Loss  0.029334882274270058\n",
            "Epoch  9 Batch  62 / 525  Training Loss  0.03138820827007294\n",
            "Epoch  9 Batch  63 / 525  Training Loss  0.021799851208925247\n",
            "Epoch  9 Batch  64 / 525  Training Loss  0.02169289067387581\n",
            "Epoch  9 Batch  65 / 525  Training Loss  0.030126160010695457\n",
            "Epoch  9 Batch  66 / 525  Training Loss  0.031428832560777664\n",
            "Epoch  9 Batch  67 / 525  Training Loss  0.020809996873140335\n",
            "Epoch  9 Batch  68 / 525  Training Loss  0.030722543597221375\n",
            "Epoch  9 Batch  69 / 525  Training Loss  0.024257933720946312\n",
            "Epoch  9 Batch  70 / 525  Training Loss  0.04200161248445511\n",
            "Epoch  9 Batch  71 / 525  Training Loss  0.023411374539136887\n",
            "Epoch  9 Batch  72 / 525  Training Loss  0.01829652115702629\n",
            "Epoch  9 Batch  73 / 525  Training Loss  0.023540187627077103\n",
            "Epoch  9 Batch  74 / 525  Training Loss  0.03663323447108269\n",
            "Epoch  9 Batch  75 / 525  Training Loss  0.0222214013338089\n",
            "Epoch  9 Batch  76 / 525  Training Loss  0.026026323437690735\n",
            "Epoch  9 Batch  77 / 525  Training Loss  0.03970630094408989\n",
            "Epoch  9 Batch  78 / 525  Training Loss  0.035048920661211014\n",
            "Epoch  9 Batch  79 / 525  Training Loss  0.03198256343603134\n",
            "Epoch  9 Batch  80 / 525  Training Loss  0.0194355808198452\n",
            "Epoch  9 Batch  81 / 525  Training Loss  0.019234303385019302\n",
            "Epoch  9 Batch  82 / 525  Training Loss  0.02773781679570675\n",
            "Epoch  9 Batch  83 / 525  Training Loss  0.025124642997980118\n",
            "Epoch  9 Batch  84 / 525  Training Loss  0.022945458069443703\n",
            "Epoch  9 Batch  85 / 525  Training Loss  0.03064664825797081\n",
            "Epoch  9 Batch  86 / 525  Training Loss  0.02789132669568062\n",
            "Epoch  9 Batch  87 / 525  Training Loss  0.02438800223171711\n",
            "Epoch  9 Batch  88 / 525  Training Loss  0.03125278279185295\n",
            "Epoch  9 Batch  89 / 525  Training Loss  0.03334038332104683\n",
            "Epoch  9 Batch  90 / 525  Training Loss  0.019074950367212296\n",
            "Epoch  9 Batch  91 / 525  Training Loss  0.03131107613444328\n",
            "Epoch  9 Batch  92 / 525  Training Loss  0.029466062784194946\n",
            "Epoch  9 Batch  93 / 525  Training Loss  0.024990100413560867\n",
            "Epoch  9 Batch  94 / 525  Training Loss  0.03292922303080559\n",
            "Epoch  9 Batch  95 / 525  Training Loss  0.02979438565671444\n",
            "Epoch  9 Batch  96 / 525  Training Loss  0.017954114824533463\n",
            "Epoch  9 Batch  97 / 525  Training Loss  0.023106049746274948\n",
            "Epoch  9 Batch  98 / 525  Training Loss  0.022786322981119156\n",
            "Epoch  9 Batch  99 / 525  Training Loss  0.03143545240163803\n",
            "Epoch  9 Batch  100 / 525  Training Loss  0.038580745458602905\n",
            "Epoch  9 Batch  101 / 525  Training Loss  0.027055859565734863\n",
            "Epoch  9 Batch  102 / 525  Training Loss  0.02576838992536068\n",
            "Epoch  9 Batch  103 / 525  Training Loss  0.0262103620916605\n",
            "Epoch  9 Batch  104 / 525  Training Loss  0.04383819177746773\n",
            "Epoch  9 Batch  105 / 525  Training Loss  0.022529374808073044\n",
            "Epoch  9 Batch  106 / 525  Training Loss  0.018284674733877182\n",
            "Epoch  9 Batch  107 / 525  Training Loss  0.037996016442775726\n",
            "Epoch  9 Batch  108 / 525  Training Loss  0.024462567642331123\n",
            "Epoch  9 Batch  109 / 525  Training Loss  0.01932125724852085\n",
            "Epoch  9 Batch  110 / 525  Training Loss  0.0274040549993515\n",
            "Epoch  9 Batch  111 / 525  Training Loss  0.029751013964414597\n",
            "Epoch  9 Batch  112 / 525  Training Loss  0.026693183928728104\n",
            "Epoch  9 Batch  113 / 525  Training Loss  0.045073267072439194\n",
            "Epoch  9 Batch  114 / 525  Training Loss  0.022079939022660255\n",
            "Epoch  9 Batch  115 / 525  Training Loss  0.02760847471654415\n",
            "Epoch  9 Batch  116 / 525  Training Loss  0.012545493431389332\n",
            "Epoch  9 Batch  117 / 525  Training Loss  0.021358733996748924\n",
            "Epoch  9 Batch  118 / 525  Training Loss  0.01924258843064308\n",
            "Epoch  9 Batch  119 / 525  Training Loss  0.02032635360956192\n",
            "Epoch  9 Batch  120 / 525  Training Loss  0.014544954523444176\n",
            "Epoch  9 Batch  121 / 525  Training Loss  0.019911259412765503\n",
            "Epoch  9 Batch  122 / 525  Training Loss  0.030589982867240906\n",
            "Epoch  9 Batch  123 / 525  Training Loss  0.019356619566679\n",
            "Epoch  9 Batch  124 / 525  Training Loss  0.03509759157896042\n",
            "Epoch  9 Batch  125 / 525  Training Loss  0.024634914472699165\n",
            "Epoch  9 Batch  126 / 525  Training Loss  0.04272541403770447\n",
            "Epoch  9 Batch  127 / 525  Training Loss  0.016769729554653168\n",
            "Epoch  9 Batch  128 / 525  Training Loss  0.02858232520520687\n",
            "Epoch  9 Batch  129 / 525  Training Loss  0.017864830791950226\n",
            "Epoch  9 Batch  130 / 525  Training Loss  0.034719280898571014\n",
            "Epoch  9 Batch  131 / 525  Training Loss  0.01803087629377842\n",
            "Epoch  9 Batch  132 / 525  Training Loss  0.018119236454367638\n",
            "Epoch  9 Batch  133 / 525  Training Loss  0.017539432272315025\n",
            "Epoch  9 Batch  134 / 525  Training Loss  0.028455663472414017\n",
            "Epoch  9 Batch  135 / 525  Training Loss  0.012725655920803547\n",
            "Epoch  9 Batch  136 / 525  Training Loss  0.031224340200424194\n",
            "Epoch  9 Batch  137 / 525  Training Loss  0.029599884524941444\n",
            "Epoch  9 Batch  138 / 525  Training Loss  0.02420664206147194\n",
            "Epoch  9 Batch  139 / 525  Training Loss  0.013588150031864643\n",
            "Epoch  9 Batch  140 / 525  Training Loss  0.01806630752980709\n",
            "Epoch  9 Batch  141 / 525  Training Loss  0.024121906608343124\n",
            "Epoch  9 Batch  142 / 525  Training Loss  0.026564091444015503\n",
            "Epoch  9 Batch  143 / 525  Training Loss  0.015403756871819496\n",
            "Epoch  9 Batch  144 / 525  Training Loss  0.023808153346180916\n",
            "Epoch  9 Batch  145 / 525  Training Loss  0.024880442768335342\n",
            "Epoch  9 Batch  146 / 525  Training Loss  0.02265515737235546\n",
            "Epoch  9 Batch  147 / 525  Training Loss  0.028987806290388107\n",
            "Epoch  9 Batch  148 / 525  Training Loss  0.034718118607997894\n",
            "Epoch  9 Batch  149 / 525  Training Loss  0.028256282210350037\n",
            "Epoch  9 Batch  150 / 525  Training Loss  0.04011572524905205\n",
            "Epoch  9 Batch  151 / 525  Training Loss  0.024799736216664314\n",
            "Epoch  9 Batch  152 / 525  Training Loss  0.01344434916973114\n",
            "Epoch  9 Batch  153 / 525  Training Loss  0.025293853133916855\n",
            "Epoch  9 Batch  154 / 525  Training Loss  0.03420267626643181\n",
            "Epoch  9 Batch  155 / 525  Training Loss  0.022878360003232956\n",
            "Epoch  9 Batch  156 / 525  Training Loss  0.031613945960998535\n",
            "Epoch  9 Batch  157 / 525  Training Loss  0.021815963089466095\n",
            "Epoch  9 Batch  158 / 525  Training Loss  0.03300530090928078\n",
            "Epoch  9 Batch  159 / 525  Training Loss  0.02379039116203785\n",
            "Epoch  9 Batch  160 / 525  Training Loss  0.01551876962184906\n",
            "Epoch  9 Batch  161 / 525  Training Loss  0.028569091111421585\n",
            "Epoch  9 Batch  162 / 525  Training Loss  0.026398926973342896\n",
            "Epoch  9 Batch  163 / 525  Training Loss  0.026080533862113953\n",
            "Epoch  9 Batch  164 / 525  Training Loss  0.025246744975447655\n",
            "Epoch  9 Batch  165 / 525  Training Loss  0.020458942279219627\n",
            "Epoch  9 Batch  166 / 525  Training Loss  0.02912788651883602\n",
            "Epoch  9 Batch  167 / 525  Training Loss  0.04968809336423874\n",
            "Epoch  9 Batch  168 / 525  Training Loss  0.04095042496919632\n",
            "Epoch  9 Batch  169 / 525  Training Loss  0.02630683407187462\n",
            "Epoch  9 Batch  170 / 525  Training Loss  0.030357304960489273\n",
            "Epoch  9 Batch  171 / 525  Training Loss  0.023707877844572067\n",
            "Epoch  9 Batch  172 / 525  Training Loss  0.027633050456643105\n",
            "Epoch  9 Batch  173 / 525  Training Loss  0.026403015479445457\n",
            "Epoch  9 Batch  174 / 525  Training Loss  0.018253881484270096\n",
            "Epoch  9 Batch  175 / 525  Training Loss  0.038121823221445084\n",
            "Epoch  9 Batch  176 / 525  Training Loss  0.031067276373505592\n",
            "Epoch  9 Batch  177 / 525  Training Loss  0.03210560232400894\n",
            "Epoch  9 Batch  178 / 525  Training Loss  0.01916400156915188\n",
            "Epoch  9 Batch  179 / 525  Training Loss  0.03069213405251503\n",
            "Epoch  9 Batch  180 / 525  Training Loss  0.030117768794298172\n",
            "Epoch  9 Batch  181 / 525  Training Loss  0.015625886619091034\n",
            "Epoch  9 Batch  182 / 525  Training Loss  0.033439286053180695\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  9 Batch  183 / 525  Training Loss  0.0302295945584774\n",
            "Epoch  9 Batch  184 / 525  Training Loss  0.020535480231046677\n",
            "Epoch  9 Batch  185 / 525  Training Loss  0.02342880703508854\n",
            "Epoch  9 Batch  186 / 525  Training Loss  0.0209204014390707\n",
            "Epoch  9 Batch  187 / 525  Training Loss  0.033521223813295364\n",
            "Epoch  9 Batch  188 / 525  Training Loss  0.013870659284293652\n",
            "Epoch  9 Batch  189 / 525  Training Loss  0.017031889408826828\n",
            "Epoch  9 Batch  190 / 525  Training Loss  0.026434028521180153\n",
            "Epoch  9 Batch  191 / 525  Training Loss  0.013491424731910229\n",
            "Epoch  9 Batch  192 / 525  Training Loss  0.0291572455316782\n",
            "Epoch  9 Batch  193 / 525  Training Loss  0.02753549814224243\n",
            "Epoch  9 Batch  194 / 525  Training Loss  0.02385423704981804\n",
            "Epoch  9 Batch  195 / 525  Training Loss  0.0260456595569849\n",
            "Epoch  9 Batch  196 / 525  Training Loss  0.03535313159227371\n",
            "Epoch  9 Batch  197 / 525  Training Loss  0.029870331287384033\n",
            "Epoch  9 Batch  198 / 525  Training Loss  0.028008710592985153\n",
            "Epoch  9 Batch  199 / 525  Training Loss  0.03125092014670372\n",
            "Epoch  9 Batch  200 / 525  Training Loss  0.02102956548333168\n",
            "Epoch  9 Batch  201 / 525  Training Loss  0.018833447247743607\n",
            "Epoch  9 Batch  202 / 525  Training Loss  0.028982603922486305\n",
            "Epoch  9 Batch  203 / 525  Training Loss  0.02728952094912529\n",
            "Epoch  9 Batch  204 / 525  Training Loss  0.030114445835351944\n",
            "Epoch  9 Batch  205 / 525  Training Loss  0.018263425678014755\n",
            "Epoch  9 Batch  206 / 525  Training Loss  0.037695273756980896\n",
            "Epoch  9 Batch  207 / 525  Training Loss  0.017150085419416428\n",
            "Epoch  9 Batch  208 / 525  Training Loss  0.0286612119525671\n",
            "Epoch  9 Batch  209 / 525  Training Loss  0.03877342492341995\n",
            "Epoch  9 Batch  210 / 525  Training Loss  0.03983193263411522\n",
            "Epoch  9 Batch  211 / 525  Training Loss  0.012444129213690758\n",
            "Epoch  9 Batch  212 / 525  Training Loss  0.025324132293462753\n",
            "Epoch  9 Batch  213 / 525  Training Loss  0.028333861380815506\n",
            "Epoch  9 Batch  214 / 525  Training Loss  0.03369846194982529\n",
            "Epoch  9 Batch  215 / 525  Training Loss  0.027043435722589493\n",
            "Epoch  9 Batch  216 / 525  Training Loss  0.036228276789188385\n",
            "Epoch  9 Batch  217 / 525  Training Loss  0.029158085584640503\n",
            "Epoch  9 Batch  218 / 525  Training Loss  0.023837240412831306\n",
            "Epoch  9 Batch  219 / 525  Training Loss  0.027331694960594177\n",
            "Epoch  9 Batch  220 / 525  Training Loss  0.033516593277454376\n",
            "Epoch  9 Batch  221 / 525  Training Loss  0.033636849373579025\n",
            "Epoch  9 Batch  222 / 525  Training Loss  0.027195235714316368\n",
            "Epoch  9 Batch  223 / 525  Training Loss  0.03178830072283745\n",
            "Epoch  9 Batch  224 / 525  Training Loss  0.03032946214079857\n",
            "Epoch  9 Batch  225 / 525  Training Loss  0.021685699000954628\n",
            "Epoch  9 Batch  226 / 525  Training Loss  0.024431562051177025\n",
            "Epoch  9 Batch  227 / 525  Training Loss  0.03520490601658821\n",
            "Epoch  9 Batch  228 / 525  Training Loss  0.02971118688583374\n",
            "Epoch  9 Batch  229 / 525  Training Loss  0.03115326724946499\n",
            "Epoch  9 Batch  230 / 525  Training Loss  0.025978490710258484\n",
            "Epoch  9 Batch  231 / 525  Training Loss  0.03177559748291969\n",
            "Epoch  9 Batch  232 / 525  Training Loss  0.03842714801430702\n",
            "Epoch  9 Batch  233 / 525  Training Loss  0.019262369722127914\n",
            "Epoch  9 Batch  234 / 525  Training Loss  0.017582125961780548\n",
            "Epoch  9 Batch  235 / 525  Training Loss  0.01285337470471859\n",
            "Epoch  9 Batch  236 / 525  Training Loss  0.03815537318587303\n",
            "Epoch  9 Batch  237 / 525  Training Loss  0.05098919942975044\n",
            "Epoch  9 Batch  238 / 525  Training Loss  0.04582676663994789\n",
            "Epoch  9 Batch  239 / 525  Training Loss  0.020727921277284622\n",
            "Epoch  9 Batch  240 / 525  Training Loss  0.03159359470009804\n",
            "Epoch  9 Batch  241 / 525  Training Loss  0.02953711710870266\n",
            "Epoch  9 Batch  242 / 525  Training Loss  0.026898005977272987\n",
            "Epoch  9 Batch  243 / 525  Training Loss  0.02539926767349243\n",
            "Epoch  9 Batch  244 / 525  Training Loss  0.027223724871873856\n",
            "Epoch  9 Batch  245 / 525  Training Loss  0.01993071660399437\n",
            "Epoch  9 Batch  246 / 525  Training Loss  0.02537977136671543\n",
            "Epoch  9 Batch  247 / 525  Training Loss  0.035135019570589066\n",
            "Epoch  9 Batch  248 / 525  Training Loss  0.028332408517599106\n",
            "Epoch  9 Batch  249 / 525  Training Loss  0.033332642167806625\n",
            "Epoch  9 Batch  250 / 525  Training Loss  0.01828354224562645\n",
            "Epoch  9 Batch  251 / 525  Training Loss  0.028606344014406204\n",
            "Epoch  9 Batch  252 / 525  Training Loss  0.02870747447013855\n",
            "Epoch  9 Batch  253 / 525  Training Loss  0.03159991279244423\n",
            "Epoch  9 Batch  254 / 525  Training Loss  0.02264784276485443\n",
            "Epoch  9 Batch  255 / 525  Training Loss  0.03239043056964874\n",
            "Epoch  9 Batch  256 / 525  Training Loss  0.012682740576565266\n",
            "Epoch  9 Batch  257 / 525  Training Loss  0.01796303316950798\n",
            "Epoch  9 Batch  258 / 525  Training Loss  0.018075697124004364\n",
            "Epoch  9 Batch  259 / 525  Training Loss  0.039616040885448456\n",
            "Epoch  9 Batch  260 / 525  Training Loss  0.040565215051174164\n",
            "Epoch  9 Batch  261 / 525  Training Loss  0.026336168870329857\n",
            "Epoch  9 Batch  262 / 525  Training Loss  0.037465330213308334\n",
            "Epoch  9 Batch  263 / 525  Training Loss  0.02648266591131687\n",
            "Epoch  9 Batch  264 / 525  Training Loss  0.03181658312678337\n",
            "Epoch  9 Batch  265 / 525  Training Loss  0.02219991385936737\n",
            "Epoch  9 Batch  266 / 525  Training Loss  0.016163406893610954\n",
            "Epoch  9 Batch  267 / 525  Training Loss  0.021646540611982346\n",
            "Epoch  9 Batch  268 / 525  Training Loss  0.03685862571001053\n",
            "Epoch  9 Batch  269 / 525  Training Loss  0.022549057379364967\n",
            "Epoch  9 Batch  270 / 525  Training Loss  0.024041863158345222\n",
            "Epoch  9 Batch  271 / 525  Training Loss  0.020132407546043396\n",
            "Epoch  9 Batch  272 / 525  Training Loss  0.027658632025122643\n",
            "Epoch  9 Batch  273 / 525  Training Loss  0.046905722469091415\n",
            "Epoch  9 Batch  274 / 525  Training Loss  0.03745255619287491\n",
            "Epoch  9 Batch  275 / 525  Training Loss  0.02389722876250744\n",
            "Epoch  9 Batch  276 / 525  Training Loss  0.031426168978214264\n",
            "Epoch  9 Batch  277 / 525  Training Loss  0.039565928280353546\n",
            "Epoch  9 Batch  278 / 525  Training Loss  0.03656182065606117\n",
            "Epoch  9 Batch  279 / 525  Training Loss  0.02249603345990181\n",
            "Epoch  9 Batch  280 / 525  Training Loss  0.014033444225788116\n",
            "Epoch  9 Batch  281 / 525  Training Loss  0.013289433903992176\n",
            "Epoch  9 Batch  282 / 525  Training Loss  0.029691826552152634\n",
            "Epoch  9 Batch  283 / 525  Training Loss  0.027721622958779335\n",
            "Epoch  9 Batch  284 / 525  Training Loss  0.020799249410629272\n",
            "Epoch  9 Batch  285 / 525  Training Loss  0.02984808012843132\n",
            "Epoch  9 Batch  286 / 525  Training Loss  0.02313065156340599\n",
            "Epoch  9 Batch  287 / 525  Training Loss  0.02741444669663906\n",
            "Epoch  9 Batch  288 / 525  Training Loss  0.02999786101281643\n",
            "Epoch  9 Batch  289 / 525  Training Loss  0.025499457493424416\n",
            "Epoch  9 Batch  290 / 525  Training Loss  0.027154680341482162\n",
            "Epoch  9 Batch  291 / 525  Training Loss  0.024767186492681503\n",
            "Epoch  9 Batch  292 / 525  Training Loss  0.019945982843637466\n",
            "Epoch  9 Batch  293 / 525  Training Loss  0.022431131452322006\n",
            "Epoch  9 Batch  294 / 525  Training Loss  0.03364409878849983\n",
            "Epoch  9 Batch  295 / 525  Training Loss  0.021604331210255623\n",
            "Epoch  9 Batch  296 / 525  Training Loss  0.027693379670381546\n",
            "Epoch  9 Batch  297 / 525  Training Loss  0.02232654020190239\n",
            "Epoch  9 Batch  298 / 525  Training Loss  0.01660393550992012\n",
            "Epoch  9 Batch  299 / 525  Training Loss  0.019249100238084793\n",
            "Epoch  9 Batch  300 / 525  Training Loss  0.030055757611989975\n",
            "Epoch  9 Batch  301 / 525  Training Loss  0.030865993350744247\n",
            "Epoch  9 Batch  302 / 525  Training Loss  0.021377751603722572\n",
            "Epoch  9 Batch  303 / 525  Training Loss  0.01834755204617977\n",
            "Epoch  9 Batch  304 / 525  Training Loss  0.02694288268685341\n",
            "Epoch  9 Batch  305 / 525  Training Loss  0.020983893424272537\n",
            "Epoch  9 Batch  306 / 525  Training Loss  0.044222332537174225\n",
            "Epoch  9 Batch  307 / 525  Training Loss  0.03183584287762642\n",
            "Epoch  9 Batch  308 / 525  Training Loss  0.01809205301105976\n",
            "Epoch  9 Batch  309 / 525  Training Loss  0.021998632699251175\n",
            "Epoch  9 Batch  310 / 525  Training Loss  0.02301034703850746\n",
            "Epoch  9 Batch  311 / 525  Training Loss  0.03428935259580612\n",
            "Epoch  9 Batch  312 / 525  Training Loss  0.03973395749926567\n",
            "Epoch  9 Batch  313 / 525  Training Loss  0.0241275355219841\n",
            "Epoch  9 Batch  314 / 525  Training Loss  0.01953718438744545\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  9 Batch  315 / 525  Training Loss  0.0359925702214241\n",
            "Epoch  9 Batch  316 / 525  Training Loss  0.025093089789152145\n",
            "Epoch  9 Batch  317 / 525  Training Loss  0.03157132863998413\n",
            "Epoch  9 Batch  318 / 525  Training Loss  0.01869467832148075\n",
            "Epoch  9 Batch  319 / 525  Training Loss  0.023813530802726746\n",
            "Epoch  9 Batch  320 / 525  Training Loss  0.026146123185753822\n",
            "Epoch  9 Batch  321 / 525  Training Loss  0.02065201848745346\n",
            "Epoch  9 Batch  322 / 525  Training Loss  0.03233721852302551\n",
            "Epoch  9 Batch  323 / 525  Training Loss  0.024717535823583603\n",
            "Epoch  9 Batch  324 / 525  Training Loss  0.032049186527729034\n",
            "Epoch  9 Batch  325 / 525  Training Loss  0.020360639318823814\n",
            "Epoch  9 Batch  326 / 525  Training Loss  0.021780002862215042\n",
            "Epoch  9 Batch  327 / 525  Training Loss  0.0418289378285408\n",
            "Epoch  9 Batch  328 / 525  Training Loss  0.0366797000169754\n",
            "Epoch  9 Batch  329 / 525  Training Loss  0.03170746937394142\n",
            "Epoch  9 Batch  330 / 525  Training Loss  0.029466405510902405\n",
            "Epoch  9 Batch  331 / 525  Training Loss  0.03984446823596954\n",
            "Epoch  9 Batch  332 / 525  Training Loss  0.031168390065431595\n",
            "Epoch  9 Batch  333 / 525  Training Loss  0.02635551430284977\n",
            "Epoch  9 Batch  334 / 525  Training Loss  0.02467777207493782\n",
            "Epoch  9 Batch  335 / 525  Training Loss  0.034598059952259064\n",
            "Epoch  9 Batch  336 / 525  Training Loss  0.03805064037442207\n",
            "Epoch  9 Batch  337 / 525  Training Loss  0.04225081950426102\n",
            "Epoch  9 Batch  338 / 525  Training Loss  0.030145544558763504\n",
            "Epoch  9 Batch  339 / 525  Training Loss  0.03513230010867119\n",
            "Epoch  9 Batch  340 / 525  Training Loss  0.040674224495887756\n",
            "Epoch  9 Batch  341 / 525  Training Loss  0.02951037324965\n",
            "Epoch  9 Batch  342 / 525  Training Loss  0.0496576651930809\n",
            "Epoch  9 Batch  343 / 525  Training Loss  0.020421013236045837\n",
            "Epoch  9 Batch  344 / 525  Training Loss  0.04016968235373497\n",
            "Epoch  9 Batch  345 / 525  Training Loss  0.025941699743270874\n",
            "Epoch  9 Batch  346 / 525  Training Loss  0.02027316205203533\n",
            "Epoch  9 Batch  347 / 525  Training Loss  0.03325039893388748\n",
            "Epoch  9 Batch  348 / 525  Training Loss  0.02241000346839428\n",
            "Epoch  9 Batch  349 / 525  Training Loss  0.015489724464714527\n",
            "Epoch  9 Batch  350 / 525  Training Loss  0.024827415123581886\n",
            "Epoch  9 Batch  351 / 525  Training Loss  0.036089394241571426\n",
            "Epoch  9 Batch  352 / 525  Training Loss  0.036345675587654114\n",
            "Epoch  9 Batch  353 / 525  Training Loss  0.04059023782610893\n",
            "Epoch  9 Batch  354 / 525  Training Loss  0.02578640542924404\n",
            "Epoch  9 Batch  355 / 525  Training Loss  0.029006164520978928\n",
            "Epoch  9 Batch  356 / 525  Training Loss  0.02771405503153801\n",
            "Epoch  9 Batch  357 / 525  Training Loss  0.023512480780482292\n",
            "Epoch  9 Batch  358 / 525  Training Loss  0.031001243740320206\n",
            "Epoch  9 Batch  359 / 525  Training Loss  0.03064623847603798\n",
            "Epoch  9 Batch  360 / 525  Training Loss  0.03926486149430275\n",
            "Epoch  9 Batch  361 / 525  Training Loss  0.02777950093150139\n",
            "Epoch  9 Batch  362 / 525  Training Loss  0.03703150898218155\n",
            "Epoch  9 Batch  363 / 525  Training Loss  0.02389414981007576\n",
            "Epoch  9 Batch  364 / 525  Training Loss  0.03237900137901306\n",
            "Epoch  9 Batch  365 / 525  Training Loss  0.02053198032081127\n",
            "Epoch  9 Batch  366 / 525  Training Loss  0.0349176861345768\n",
            "Epoch  9 Batch  367 / 525  Training Loss  0.020388800650835037\n",
            "Epoch  9 Batch  368 / 525  Training Loss  0.026774218305945396\n",
            "Epoch  9 Batch  369 / 525  Training Loss  0.0188530832529068\n",
            "Epoch  9 Batch  370 / 525  Training Loss  0.02579823136329651\n",
            "Epoch  9 Batch  371 / 525  Training Loss  0.024962445721030235\n",
            "Epoch  9 Batch  372 / 525  Training Loss  0.020243490114808083\n",
            "Epoch  9 Batch  373 / 525  Training Loss  0.04239024966955185\n",
            "Epoch  9 Batch  374 / 525  Training Loss  0.03292669728398323\n",
            "Epoch  9 Batch  375 / 525  Training Loss  0.05594106391072273\n",
            "Epoch  9 Batch  376 / 525  Training Loss  0.021327035501599312\n",
            "Epoch  9 Batch  377 / 525  Training Loss  0.029009085148572922\n",
            "Epoch  9 Batch  378 / 525  Training Loss  0.03471919149160385\n",
            "Epoch  9 Batch  379 / 525  Training Loss  0.03283035010099411\n",
            "Epoch  9 Batch  380 / 525  Training Loss  0.029428932815790176\n",
            "Epoch  9 Batch  381 / 525  Training Loss  0.033708881586790085\n",
            "Epoch  9 Batch  382 / 525  Training Loss  0.03299299627542496\n",
            "Epoch  9 Batch  383 / 525  Training Loss  0.024945905432105064\n",
            "Epoch  9 Batch  384 / 525  Training Loss  0.03193715959787369\n",
            "Epoch  9 Batch  385 / 525  Training Loss  0.026012849062681198\n",
            "Epoch  9 Batch  386 / 525  Training Loss  0.028516480699181557\n",
            "Epoch  9 Batch  387 / 525  Training Loss  0.029487628489732742\n",
            "Epoch  9 Batch  388 / 525  Training Loss  0.032581791281700134\n",
            "Epoch  9 Batch  389 / 525  Training Loss  0.031076878309249878\n",
            "Epoch  9 Batch  390 / 525  Training Loss  0.03240853548049927\n",
            "Epoch  9 Batch  391 / 525  Training Loss  0.030978938564658165\n",
            "Epoch  9 Batch  392 / 525  Training Loss  0.03162243962287903\n",
            "Epoch  9 Batch  393 / 525  Training Loss  0.04070655629038811\n",
            "Epoch  9 Batch  394 / 525  Training Loss  0.0321296751499176\n",
            "Epoch  9 Batch  395 / 525  Training Loss  0.018498620018363\n",
            "Epoch  9 Batch  396 / 525  Training Loss  0.026818176731467247\n",
            "Epoch  9 Batch  397 / 525  Training Loss  0.03103322722017765\n",
            "Epoch  9 Batch  398 / 525  Training Loss  0.029536347836256027\n",
            "Epoch  9 Batch  399 / 525  Training Loss  0.03228380158543587\n",
            "Epoch  9 Batch  400 / 525  Training Loss  0.01540357805788517\n",
            "Epoch  9 Batch  401 / 525  Training Loss  0.020218003541231155\n",
            "Epoch  9 Batch  402 / 525  Training Loss  0.03544912859797478\n",
            "Epoch  9 Batch  403 / 525  Training Loss  0.03495979681611061\n",
            "Epoch  9 Batch  404 / 525  Training Loss  0.02133321389555931\n",
            "Epoch  9 Batch  405 / 525  Training Loss  0.0365728922188282\n",
            "Epoch  9 Batch  406 / 525  Training Loss  0.025956477969884872\n",
            "Epoch  9 Batch  407 / 525  Training Loss  0.03545994311571121\n",
            "Epoch  9 Batch  408 / 525  Training Loss  0.035019323229789734\n",
            "Epoch  9 Batch  409 / 525  Training Loss  0.04252808541059494\n",
            "Epoch  9 Batch  410 / 525  Training Loss  0.03881128132343292\n",
            "Epoch  9 Batch  411 / 525  Training Loss  0.03311852365732193\n",
            "Epoch  9 Batch  412 / 525  Training Loss  0.019722122699022293\n",
            "Epoch  9 Batch  413 / 525  Training Loss  0.025052543729543686\n",
            "Epoch  9 Batch  414 / 525  Training Loss  0.014003601856529713\n",
            "Epoch  9 Batch  415 / 525  Training Loss  0.033433638513088226\n",
            "Epoch  9 Batch  416 / 525  Training Loss  0.027417127043008804\n",
            "Epoch  9 Batch  417 / 525  Training Loss  0.02975982055068016\n",
            "Epoch  9 Batch  418 / 525  Training Loss  0.02535315230488777\n",
            "Epoch  9 Batch  419 / 525  Training Loss  0.020807616412639618\n",
            "Epoch  9 Batch  420 / 525  Training Loss  0.04065554216504097\n",
            "Epoch  9 Batch  421 / 525  Training Loss  0.04046791419386864\n",
            "Epoch  9 Batch  422 / 525  Training Loss  0.034698717296123505\n",
            "Epoch  9 Batch  423 / 525  Training Loss  0.03935832902789116\n",
            "Epoch  9 Batch  424 / 525  Training Loss  0.050201933830976486\n",
            "Epoch  9 Batch  425 / 525  Training Loss  0.040935397148132324\n",
            "Epoch  9 Batch  426 / 525  Training Loss  0.034970588982105255\n",
            "Epoch  9 Batch  427 / 525  Training Loss  0.02657570317387581\n",
            "Epoch  9 Batch  428 / 525  Training Loss  0.0268801711499691\n",
            "Epoch  9 Batch  429 / 525  Training Loss  0.03548654913902283\n",
            "Epoch  9 Batch  430 / 525  Training Loss  0.04360238462686539\n",
            "Epoch  9 Batch  431 / 525  Training Loss  0.02777162194252014\n",
            "Epoch  9 Batch  432 / 525  Training Loss  0.039211444556713104\n",
            "Epoch  9 Batch  433 / 525  Training Loss  0.027421539649367332\n",
            "Epoch  9 Batch  434 / 525  Training Loss  0.026573384180665016\n",
            "Epoch  9 Batch  435 / 525  Training Loss  0.03703750669956207\n",
            "Epoch  9 Batch  436 / 525  Training Loss  0.02177002839744091\n",
            "Epoch  9 Batch  437 / 525  Training Loss  0.030072882771492004\n",
            "Epoch  9 Batch  438 / 525  Training Loss  0.016825053840875626\n",
            "Epoch  9 Batch  439 / 525  Training Loss  0.02376030571758747\n",
            "Epoch  9 Batch  440 / 525  Training Loss  0.027921834960579872\n",
            "Epoch  9 Batch  441 / 525  Training Loss  0.0623580701649189\n",
            "Epoch  9 Batch  442 / 525  Training Loss  0.021824441850185394\n",
            "Epoch  9 Batch  443 / 525  Training Loss  0.018990464508533478\n",
            "Epoch  9 Batch  444 / 525  Training Loss  0.024335872381925583\n",
            "Epoch  9 Batch  445 / 525  Training Loss  0.028942521661520004\n",
            "Epoch  9 Batch  446 / 525  Training Loss  0.030425632372498512\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  9 Batch  447 / 525  Training Loss  0.02973235584795475\n",
            "Epoch  9 Batch  448 / 525  Training Loss  0.04465429112315178\n",
            "Epoch  9 Batch  449 / 525  Training Loss  0.0431101955473423\n",
            "Epoch  9 Batch  450 / 525  Training Loss  0.045374106615781784\n",
            "Epoch  9 Batch  451 / 525  Training Loss  0.027966756373643875\n",
            "Epoch  9 Batch  452 / 525  Training Loss  0.02113940753042698\n",
            "Epoch  9 Batch  453 / 525  Training Loss  0.03091091476380825\n",
            "Epoch  9 Batch  454 / 525  Training Loss  0.01594788022339344\n",
            "Epoch  9 Batch  455 / 525  Training Loss  0.01784476265311241\n",
            "Epoch  9 Batch  456 / 525  Training Loss  0.02214028500020504\n",
            "Epoch  9 Batch  457 / 525  Training Loss  0.023008285090327263\n",
            "Epoch  9 Batch  458 / 525  Training Loss  0.026652097702026367\n",
            "Epoch  9 Batch  459 / 525  Training Loss  0.01791607216000557\n",
            "Epoch  9 Batch  460 / 525  Training Loss  0.029720276594161987\n",
            "Epoch  9 Batch  461 / 525  Training Loss  0.03511537238955498\n",
            "Epoch  9 Batch  462 / 525  Training Loss  0.02556808851659298\n",
            "Epoch  9 Batch  463 / 525  Training Loss  0.025293808430433273\n",
            "Epoch  9 Batch  464 / 525  Training Loss  0.03195111081004143\n",
            "Epoch  9 Batch  465 / 525  Training Loss  0.024268511682748795\n",
            "Epoch  9 Batch  466 / 525  Training Loss  0.021307867020368576\n",
            "Epoch  9 Batch  467 / 525  Training Loss  0.02065569907426834\n",
            "Epoch  9 Batch  468 / 525  Training Loss  0.015987159684300423\n",
            "Epoch  9 Batch  469 / 525  Training Loss  0.01858450099825859\n",
            "Epoch  9 Batch  470 / 525  Training Loss  0.0319402702152729\n",
            "Epoch  9 Batch  471 / 525  Training Loss  0.017907556146383286\n",
            "Epoch  9 Batch  472 / 525  Training Loss  0.02856813743710518\n",
            "Epoch  9 Batch  473 / 525  Training Loss  0.02608552575111389\n",
            "Epoch  9 Batch  474 / 525  Training Loss  0.022219156846404076\n",
            "Epoch  9 Batch  475 / 525  Training Loss  0.02280631847679615\n",
            "Epoch  9 Batch  476 / 525  Training Loss  0.027370307594537735\n",
            "Epoch  9 Batch  477 / 525  Training Loss  0.015978071838617325\n",
            "Epoch  9 Batch  478 / 525  Training Loss  0.027844000607728958\n",
            "Epoch  9 Batch  479 / 525  Training Loss  0.01684056594967842\n",
            "Epoch  9 Batch  480 / 525  Training Loss  0.038008421659469604\n",
            "Epoch  9 Batch  481 / 525  Training Loss  0.020121093839406967\n",
            "Epoch  9 Batch  482 / 525  Training Loss  0.026008233428001404\n",
            "Epoch  9 Batch  483 / 525  Training Loss  0.03789389878511429\n",
            "Epoch  9 Batch  484 / 525  Training Loss  0.02530660852789879\n",
            "Epoch  9 Batch  485 / 525  Training Loss  0.022333059459924698\n",
            "Epoch  9 Batch  486 / 525  Training Loss  0.02311120554804802\n",
            "Epoch  9 Batch  487 / 525  Training Loss  0.03203815221786499\n",
            "Epoch  9 Batch  488 / 525  Training Loss  0.032338134944438934\n",
            "Epoch  9 Batch  489 / 525  Training Loss  0.019768361002206802\n",
            "Epoch  9 Batch  490 / 525  Training Loss  0.024249223992228508\n",
            "Epoch  9 Batch  491 / 525  Training Loss  0.022338256239891052\n",
            "Epoch  9 Batch  492 / 525  Training Loss  0.026921529322862625\n",
            "Epoch  9 Batch  493 / 525  Training Loss  0.02441558800637722\n",
            "Epoch  9 Batch  494 / 525  Training Loss  0.02614707313477993\n",
            "Epoch  9 Batch  495 / 525  Training Loss  0.02679344452917576\n",
            "Epoch  9 Batch  496 / 525  Training Loss  0.01934826746582985\n",
            "Epoch  9 Batch  497 / 525  Training Loss  0.030266111716628075\n",
            "Epoch  9 Batch  498 / 525  Training Loss  0.023768840357661247\n",
            "Epoch  9 Batch  499 / 525  Training Loss  0.032629817724227905\n",
            "Epoch  9 Batch  500 / 525  Training Loss  0.029522573575377464\n",
            "Epoch  9 Batch  501 / 525  Training Loss  0.03046495094895363\n",
            "Epoch  9 Batch  502 / 525  Training Loss  0.02663363143801689\n",
            "Epoch  9 Batch  503 / 525  Training Loss  0.023606587201356888\n",
            "Epoch  9 Batch  504 / 525  Training Loss  0.042449913918972015\n",
            "Epoch  9 Batch  505 / 525  Training Loss  0.04546365141868591\n",
            "Epoch  9 Batch  506 / 525  Training Loss  0.02297079935669899\n",
            "Epoch  9 Batch  507 / 525  Training Loss  0.03507247939705849\n",
            "Epoch  9 Batch  508 / 525  Training Loss  0.025097647681832314\n",
            "Epoch  9 Batch  509 / 525  Training Loss  0.016514109447598457\n",
            "Epoch  9 Batch  510 / 525  Training Loss  0.0253564715385437\n",
            "Epoch  9 Batch  511 / 525  Training Loss  0.023501530289649963\n",
            "Epoch  9 Batch  512 / 525  Training Loss  0.029956495389342308\n",
            "Epoch  9 Batch  513 / 525  Training Loss  0.03142615407705307\n",
            "Epoch  9 Batch  514 / 525  Training Loss  0.015875287353992462\n",
            "Epoch  9 Batch  515 / 525  Training Loss  0.02177056297659874\n",
            "Epoch  9 Batch  516 / 525  Training Loss  0.01860317774116993\n",
            "Epoch  9 Batch  517 / 525  Training Loss  0.04163051396608353\n",
            "Epoch  9 Batch  518 / 525  Training Loss  0.019034691154956818\n",
            "Epoch  9 Batch  519 / 525  Training Loss  0.03129280358552933\n",
            "Epoch  9 Batch  520 / 525  Training Loss  0.029430031776428223\n",
            "Epoch  9 Batch  521 / 525  Training Loss  0.04002665728330612\n",
            "Epoch  9 Batch  522 / 525  Training Loss  0.01930243894457817\n",
            "Epoch  9 Batch  523 / 525  Training Loss  0.025449808686971664\n",
            "Epoch  9 Batch  524 / 525  Training Loss  0.02360817790031433\n",
            "  10    |    -    |   0.027283   |   53.77  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 10\n",
            "Epoch  10 Batch  0 / 525  Training Loss  0.012626975774765015\n",
            "Epoch  10 Batch  1 / 525  Training Loss  0.009749328717589378\n",
            "Epoch  10 Batch  2 / 525  Training Loss  0.022761719301342964\n",
            "Epoch  10 Batch  3 / 525  Training Loss  0.01613074727356434\n",
            "Epoch  10 Batch  4 / 525  Training Loss  0.018972067162394524\n",
            "Epoch  10 Batch  5 / 525  Training Loss  0.0159152764827013\n",
            "Epoch  10 Batch  6 / 525  Training Loss  0.01833789236843586\n",
            "Epoch  10 Batch  7 / 525  Training Loss  0.032157059758901596\n",
            "Epoch  10 Batch  8 / 525  Training Loss  0.006716281175613403\n",
            "Epoch  10 Batch  9 / 525  Training Loss  0.013408293947577477\n",
            "Epoch  10 Batch  10 / 525  Training Loss  0.0182548426091671\n",
            "Epoch  10 Batch  11 / 525  Training Loss  0.008415928110480309\n",
            "Epoch  10 Batch  12 / 525  Training Loss  0.0170096755027771\n",
            "Epoch  10 Batch  13 / 525  Training Loss  0.018288984894752502\n",
            "Epoch  10 Batch  14 / 525  Training Loss  0.024339286610484123\n",
            "Epoch  10 Batch  15 / 525  Training Loss  0.01410616934299469\n",
            "Epoch  10 Batch  16 / 525  Training Loss  0.016722653061151505\n",
            "Epoch  10 Batch  17 / 525  Training Loss  0.017295734956860542\n",
            "Epoch  10 Batch  18 / 525  Training Loss  0.018341854214668274\n",
            "Epoch  10 Batch  19 / 525  Training Loss  0.008936229161918163\n",
            "Epoch  10 Batch  20 / 525  Training Loss  0.01185727957636118\n",
            "Epoch  10 Batch  21 / 525  Training Loss  0.02715997025370598\n",
            "Epoch  10 Batch  22 / 525  Training Loss  0.018818896263837814\n",
            "Epoch  10 Batch  23 / 525  Training Loss  0.017079610377550125\n",
            "Epoch  10 Batch  24 / 525  Training Loss  0.01477026380598545\n",
            "Epoch  10 Batch  25 / 525  Training Loss  0.011791890487074852\n",
            "Epoch  10 Batch  26 / 525  Training Loss  0.03443513065576553\n",
            "Epoch  10 Batch  27 / 525  Training Loss  0.03613274544477463\n",
            "Epoch  10 Batch  28 / 525  Training Loss  0.012576143257319927\n",
            "Epoch  10 Batch  29 / 525  Training Loss  0.024303842335939407\n",
            "Epoch  10 Batch  30 / 525  Training Loss  0.016178179532289505\n",
            "Epoch  10 Batch  31 / 525  Training Loss  0.014929167926311493\n",
            "Epoch  10 Batch  32 / 525  Training Loss  0.019170206040143967\n",
            "Epoch  10 Batch  33 / 525  Training Loss  0.0255996435880661\n",
            "Epoch  10 Batch  34 / 525  Training Loss  0.031429875642061234\n",
            "Epoch  10 Batch  35 / 525  Training Loss  0.013608075678348541\n",
            "Epoch  10 Batch  36 / 525  Training Loss  0.008920742198824883\n",
            "Epoch  10 Batch  37 / 525  Training Loss  0.017917728051543236\n",
            "Epoch  10 Batch  38 / 525  Training Loss  0.015255110338330269\n",
            "Epoch  10 Batch  39 / 525  Training Loss  0.02401900663971901\n",
            "Epoch  10 Batch  40 / 525  Training Loss  0.016802335157990456\n",
            "Epoch  10 Batch  41 / 525  Training Loss  0.014050054363906384\n",
            "Epoch  10 Batch  42 / 525  Training Loss  0.012468171305954456\n",
            "Epoch  10 Batch  43 / 525  Training Loss  0.006761120166629553\n",
            "Epoch  10 Batch  44 / 525  Training Loss  0.021913476288318634\n",
            "Epoch  10 Batch  45 / 525  Training Loss  0.01711837761104107\n",
            "Epoch  10 Batch  46 / 525  Training Loss  0.0173124261200428\n",
            "Epoch  10 Batch  47 / 525  Training Loss  0.02015787735581398\n",
            "Epoch  10 Batch  48 / 525  Training Loss  0.017888899892568588\n",
            "Epoch  10 Batch  49 / 525  Training Loss  0.016992680728435516\n",
            "Epoch  10 Batch  50 / 525  Training Loss  0.017002839595079422\n",
            "Epoch  10 Batch  51 / 525  Training Loss  0.013958279974758625\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  10 Batch  52 / 525  Training Loss  0.025098633021116257\n",
            "Epoch  10 Batch  53 / 525  Training Loss  0.019476082175970078\n",
            "Epoch  10 Batch  54 / 525  Training Loss  0.027693290263414383\n",
            "Epoch  10 Batch  55 / 525  Training Loss  0.024642914533615112\n",
            "Epoch  10 Batch  56 / 525  Training Loss  0.020917855203151703\n",
            "Epoch  10 Batch  57 / 525  Training Loss  0.01967419870197773\n",
            "Epoch  10 Batch  58 / 525  Training Loss  0.018964596092700958\n",
            "Epoch  10 Batch  59 / 525  Training Loss  0.016162164509296417\n",
            "Epoch  10 Batch  60 / 525  Training Loss  0.014004603028297424\n",
            "Epoch  10 Batch  61 / 525  Training Loss  0.02521209418773651\n",
            "Epoch  10 Batch  62 / 525  Training Loss  0.03265642374753952\n",
            "Epoch  10 Batch  63 / 525  Training Loss  0.015346899628639221\n",
            "Epoch  10 Batch  64 / 525  Training Loss  0.01393824815750122\n",
            "Epoch  10 Batch  65 / 525  Training Loss  0.008849488571286201\n",
            "Epoch  10 Batch  66 / 525  Training Loss  0.02333063818514347\n",
            "Epoch  10 Batch  67 / 525  Training Loss  0.02238401398062706\n",
            "Epoch  10 Batch  68 / 525  Training Loss  0.016109216958284378\n",
            "Epoch  10 Batch  69 / 525  Training Loss  0.024906765669584274\n",
            "Epoch  10 Batch  70 / 525  Training Loss  0.01844889670610428\n",
            "Epoch  10 Batch  71 / 525  Training Loss  0.018586181104183197\n",
            "Epoch  10 Batch  72 / 525  Training Loss  0.01763189770281315\n",
            "Epoch  10 Batch  73 / 525  Training Loss  0.019391877576708794\n",
            "Epoch  10 Batch  74 / 525  Training Loss  0.01699564978480339\n",
            "Epoch  10 Batch  75 / 525  Training Loss  0.015879977494478226\n",
            "Epoch  10 Batch  76 / 525  Training Loss  0.012713683769106865\n",
            "Epoch  10 Batch  77 / 525  Training Loss  0.011926701292395592\n",
            "Epoch  10 Batch  78 / 525  Training Loss  0.02548981085419655\n",
            "Epoch  10 Batch  79 / 525  Training Loss  0.0289735309779644\n",
            "Epoch  10 Batch  80 / 525  Training Loss  0.0196976400911808\n",
            "Epoch  10 Batch  81 / 525  Training Loss  0.030254637822508812\n",
            "Epoch  10 Batch  82 / 525  Training Loss  0.016227182000875473\n",
            "Epoch  10 Batch  83 / 525  Training Loss  0.022759687155485153\n",
            "Epoch  10 Batch  84 / 525  Training Loss  0.018641825765371323\n",
            "Epoch  10 Batch  85 / 525  Training Loss  0.031218070536851883\n",
            "Epoch  10 Batch  86 / 525  Training Loss  0.016582682728767395\n",
            "Epoch  10 Batch  87 / 525  Training Loss  0.014797590672969818\n",
            "Epoch  10 Batch  88 / 525  Training Loss  0.01610916666686535\n",
            "Epoch  10 Batch  89 / 525  Training Loss  0.012220446951687336\n",
            "Epoch  10 Batch  90 / 525  Training Loss  0.018994754180312157\n",
            "Epoch  10 Batch  91 / 525  Training Loss  0.019933385774493217\n",
            "Epoch  10 Batch  92 / 525  Training Loss  0.013739174231886864\n",
            "Epoch  10 Batch  93 / 525  Training Loss  0.017274662852287292\n",
            "Epoch  10 Batch  94 / 525  Training Loss  0.029670724645256996\n",
            "Epoch  10 Batch  95 / 525  Training Loss  0.013431747443974018\n",
            "Epoch  10 Batch  96 / 525  Training Loss  0.01129141915589571\n",
            "Epoch  10 Batch  97 / 525  Training Loss  0.017250996083021164\n",
            "Epoch  10 Batch  98 / 525  Training Loss  0.012321569956839085\n",
            "Epoch  10 Batch  99 / 525  Training Loss  0.011153162457048893\n",
            "Epoch  10 Batch  100 / 525  Training Loss  0.016544176265597343\n",
            "Epoch  10 Batch  101 / 525  Training Loss  0.009665888734161854\n",
            "Epoch  10 Batch  102 / 525  Training Loss  0.026778778061270714\n",
            "Epoch  10 Batch  103 / 525  Training Loss  0.01523177046328783\n",
            "Epoch  10 Batch  104 / 525  Training Loss  0.014370600692927837\n",
            "Epoch  10 Batch  105 / 525  Training Loss  0.029189636930823326\n",
            "Epoch  10 Batch  106 / 525  Training Loss  0.027400504797697067\n",
            "Epoch  10 Batch  107 / 525  Training Loss  0.009592549875378609\n",
            "Epoch  10 Batch  108 / 525  Training Loss  0.008290800265967846\n",
            "Epoch  10 Batch  109 / 525  Training Loss  0.01465911976993084\n",
            "Epoch  10 Batch  110 / 525  Training Loss  0.00885225273668766\n",
            "Epoch  10 Batch  111 / 525  Training Loss  0.0237988643348217\n",
            "Epoch  10 Batch  112 / 525  Training Loss  0.01785716786980629\n",
            "Epoch  10 Batch  113 / 525  Training Loss  0.015206766314804554\n",
            "Epoch  10 Batch  114 / 525  Training Loss  0.016196833923459053\n",
            "Epoch  10 Batch  115 / 525  Training Loss  0.011909266002476215\n",
            "Epoch  10 Batch  116 / 525  Training Loss  0.014579737558960915\n",
            "Epoch  10 Batch  117 / 525  Training Loss  0.02164934016764164\n",
            "Epoch  10 Batch  118 / 525  Training Loss  0.016551394015550613\n",
            "Epoch  10 Batch  119 / 525  Training Loss  0.015287172980606556\n",
            "Epoch  10 Batch  120 / 525  Training Loss  0.008667020127177238\n",
            "Epoch  10 Batch  121 / 525  Training Loss  0.01115268375724554\n",
            "Epoch  10 Batch  122 / 525  Training Loss  0.018042555078864098\n",
            "Epoch  10 Batch  123 / 525  Training Loss  0.016852881759405136\n",
            "Epoch  10 Batch  124 / 525  Training Loss  0.022489719092845917\n",
            "Epoch  10 Batch  125 / 525  Training Loss  0.018199225887656212\n",
            "Epoch  10 Batch  126 / 525  Training Loss  0.018153134733438492\n",
            "Epoch  10 Batch  127 / 525  Training Loss  0.011478251777589321\n",
            "Epoch  10 Batch  128 / 525  Training Loss  0.011682430282235146\n",
            "Epoch  10 Batch  129 / 525  Training Loss  0.015115129761397839\n",
            "Epoch  10 Batch  130 / 525  Training Loss  0.01735091023147106\n",
            "Epoch  10 Batch  131 / 525  Training Loss  0.014843298122286797\n",
            "Epoch  10 Batch  132 / 525  Training Loss  0.013283279724419117\n",
            "Epoch  10 Batch  133 / 525  Training Loss  0.0106662567704916\n",
            "Epoch  10 Batch  134 / 525  Training Loss  0.011238589882850647\n",
            "Epoch  10 Batch  135 / 525  Training Loss  0.015250411815941334\n",
            "Epoch  10 Batch  136 / 525  Training Loss  0.010366007685661316\n",
            "Epoch  10 Batch  137 / 525  Training Loss  0.007213671691715717\n",
            "Epoch  10 Batch  138 / 525  Training Loss  0.01523312646895647\n",
            "Epoch  10 Batch  139 / 525  Training Loss  0.012872548773884773\n",
            "Epoch  10 Batch  140 / 525  Training Loss  0.017795581370592117\n",
            "Epoch  10 Batch  141 / 525  Training Loss  0.011184357106685638\n",
            "Epoch  10 Batch  142 / 525  Training Loss  0.01340841967612505\n",
            "Epoch  10 Batch  143 / 525  Training Loss  0.009205212816596031\n",
            "Epoch  10 Batch  144 / 525  Training Loss  0.011708389036357403\n",
            "Epoch  10 Batch  145 / 525  Training Loss  0.023114945739507675\n",
            "Epoch  10 Batch  146 / 525  Training Loss  0.014928089454770088\n",
            "Epoch  10 Batch  147 / 525  Training Loss  0.019293878227472305\n",
            "Epoch  10 Batch  148 / 525  Training Loss  0.010656153783202171\n",
            "Epoch  10 Batch  149 / 525  Training Loss  0.022967714816331863\n",
            "Epoch  10 Batch  150 / 525  Training Loss  0.013120504096150398\n",
            "Epoch  10 Batch  151 / 525  Training Loss  0.01917087472975254\n",
            "Epoch  10 Batch  152 / 525  Training Loss  0.020393893122673035\n",
            "Epoch  10 Batch  153 / 525  Training Loss  0.021091515198349953\n",
            "Epoch  10 Batch  154 / 525  Training Loss  0.01194362249225378\n",
            "Epoch  10 Batch  155 / 525  Training Loss  0.028342977166175842\n",
            "Epoch  10 Batch  156 / 525  Training Loss  0.01921473816037178\n",
            "Epoch  10 Batch  157 / 525  Training Loss  0.023853937163949013\n",
            "Epoch  10 Batch  158 / 525  Training Loss  0.015320047736167908\n",
            "Epoch  10 Batch  159 / 525  Training Loss  0.01615085080265999\n",
            "Epoch  10 Batch  160 / 525  Training Loss  0.03549295663833618\n",
            "Epoch  10 Batch  161 / 525  Training Loss  0.024070415645837784\n",
            "Epoch  10 Batch  162 / 525  Training Loss  0.025642771273851395\n",
            "Epoch  10 Batch  163 / 525  Training Loss  0.011505169793963432\n",
            "Epoch  10 Batch  164 / 525  Training Loss  0.02045021392405033\n",
            "Epoch  10 Batch  165 / 525  Training Loss  0.025637084618210793\n",
            "Epoch  10 Batch  166 / 525  Training Loss  0.0068184612318873405\n",
            "Epoch  10 Batch  167 / 525  Training Loss  0.015148432925343513\n",
            "Epoch  10 Batch  168 / 525  Training Loss  0.016489818692207336\n",
            "Epoch  10 Batch  169 / 525  Training Loss  0.014155343174934387\n",
            "Epoch  10 Batch  170 / 525  Training Loss  0.01454364974051714\n",
            "Epoch  10 Batch  171 / 525  Training Loss  0.020809035748243332\n",
            "Epoch  10 Batch  172 / 525  Training Loss  0.01960732415318489\n",
            "Epoch  10 Batch  173 / 525  Training Loss  0.012016509659588337\n",
            "Epoch  10 Batch  174 / 525  Training Loss  0.018824834376573563\n",
            "Epoch  10 Batch  175 / 525  Training Loss  0.019522489979863167\n",
            "Epoch  10 Batch  176 / 525  Training Loss  0.011505543254315853\n",
            "Epoch  10 Batch  177 / 525  Training Loss  0.01566564291715622\n",
            "Epoch  10 Batch  178 / 525  Training Loss  0.011747810058295727\n",
            "Epoch  10 Batch  179 / 525  Training Loss  0.0179712176322937\n",
            "Epoch  10 Batch  180 / 525  Training Loss  0.010507673025131226\n",
            "Epoch  10 Batch  181 / 525  Training Loss  0.015098017640411854\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  10 Batch  182 / 525  Training Loss  0.007567903958261013\n",
            "Epoch  10 Batch  183 / 525  Training Loss  0.01642775721848011\n",
            "Epoch  10 Batch  184 / 525  Training Loss  0.017988091334700584\n",
            "Epoch  10 Batch  185 / 525  Training Loss  0.011083131656050682\n",
            "Epoch  10 Batch  186 / 525  Training Loss  0.01882285438477993\n",
            "Epoch  10 Batch  187 / 525  Training Loss  0.022230369970202446\n",
            "Epoch  10 Batch  188 / 525  Training Loss  0.013216746039688587\n",
            "Epoch  10 Batch  189 / 525  Training Loss  0.01412565540522337\n",
            "Epoch  10 Batch  190 / 525  Training Loss  0.01718682423233986\n",
            "Epoch  10 Batch  191 / 525  Training Loss  0.018771104514598846\n",
            "Epoch  10 Batch  192 / 525  Training Loss  0.02994895540177822\n",
            "Epoch  10 Batch  193 / 525  Training Loss  0.026959067210555077\n",
            "Epoch  10 Batch  194 / 525  Training Loss  0.014334100298583508\n",
            "Epoch  10 Batch  195 / 525  Training Loss  0.015414896421134472\n",
            "Epoch  10 Batch  196 / 525  Training Loss  0.03234395012259483\n",
            "Epoch  10 Batch  197 / 525  Training Loss  0.013251455500721931\n",
            "Epoch  10 Batch  198 / 525  Training Loss  0.012826736085116863\n",
            "Epoch  10 Batch  199 / 525  Training Loss  0.018411070108413696\n",
            "Epoch  10 Batch  200 / 525  Training Loss  0.012735197320580482\n",
            "Epoch  10 Batch  201 / 525  Training Loss  0.012366903945803642\n",
            "Epoch  10 Batch  202 / 525  Training Loss  0.012951423414051533\n",
            "Epoch  10 Batch  203 / 525  Training Loss  0.012571382336318493\n",
            "Epoch  10 Batch  204 / 525  Training Loss  0.014055120758712292\n",
            "Epoch  10 Batch  205 / 525  Training Loss  0.010222015902400017\n",
            "Epoch  10 Batch  206 / 525  Training Loss  0.012211253866553307\n",
            "Epoch  10 Batch  207 / 525  Training Loss  0.014729892835021019\n",
            "Epoch  10 Batch  208 / 525  Training Loss  0.017683595418930054\n",
            "Epoch  10 Batch  209 / 525  Training Loss  0.006856914609670639\n",
            "Epoch  10 Batch  210 / 525  Training Loss  0.016913078725337982\n",
            "Epoch  10 Batch  211 / 525  Training Loss  0.018100658431649208\n",
            "Epoch  10 Batch  212 / 525  Training Loss  0.027517398819327354\n",
            "Epoch  10 Batch  213 / 525  Training Loss  0.02416105940937996\n",
            "Epoch  10 Batch  214 / 525  Training Loss  0.009291565045714378\n",
            "Epoch  10 Batch  215 / 525  Training Loss  0.0131097212433815\n",
            "Epoch  10 Batch  216 / 525  Training Loss  0.008571920916438103\n",
            "Epoch  10 Batch  217 / 525  Training Loss  0.026542875915765762\n",
            "Epoch  10 Batch  218 / 525  Training Loss  0.011850939132273197\n",
            "Epoch  10 Batch  219 / 525  Training Loss  0.020661337301135063\n",
            "Epoch  10 Batch  220 / 525  Training Loss  0.013506079092621803\n",
            "Epoch  10 Batch  221 / 525  Training Loss  0.011189077980816364\n",
            "Epoch  10 Batch  222 / 525  Training Loss  0.018739307299256325\n",
            "Epoch  10 Batch  223 / 525  Training Loss  0.019858770072460175\n",
            "Epoch  10 Batch  224 / 525  Training Loss  0.01509820856153965\n",
            "Epoch  10 Batch  225 / 525  Training Loss  0.01612010970711708\n",
            "Epoch  10 Batch  226 / 525  Training Loss  0.012897106818854809\n",
            "Epoch  10 Batch  227 / 525  Training Loss  0.0079707195982337\n",
            "Epoch  10 Batch  228 / 525  Training Loss  0.017953645437955856\n",
            "Epoch  10 Batch  229 / 525  Training Loss  0.015960896387696266\n",
            "Epoch  10 Batch  230 / 525  Training Loss  0.014732034876942635\n",
            "Epoch  10 Batch  231 / 525  Training Loss  0.012304590083658695\n",
            "Epoch  10 Batch  232 / 525  Training Loss  0.015033585019409657\n",
            "Epoch  10 Batch  233 / 525  Training Loss  0.019633349031209946\n",
            "Epoch  10 Batch  234 / 525  Training Loss  0.019043434411287308\n",
            "Epoch  10 Batch  235 / 525  Training Loss  0.016004126518964767\n",
            "Epoch  10 Batch  236 / 525  Training Loss  0.014308805577456951\n",
            "Epoch  10 Batch  237 / 525  Training Loss  0.012473078444600105\n",
            "Epoch  10 Batch  238 / 525  Training Loss  0.0169075895100832\n",
            "Epoch  10 Batch  239 / 525  Training Loss  0.016817472875118256\n",
            "Epoch  10 Batch  240 / 525  Training Loss  0.009778791107237339\n",
            "Epoch  10 Batch  241 / 525  Training Loss  0.009990710765123367\n",
            "Epoch  10 Batch  242 / 525  Training Loss  0.01110103540122509\n",
            "Epoch  10 Batch  243 / 525  Training Loss  0.017471794039011\n",
            "Epoch  10 Batch  244 / 525  Training Loss  0.015999287366867065\n",
            "Epoch  10 Batch  245 / 525  Training Loss  0.025385523214936256\n",
            "Epoch  10 Batch  246 / 525  Training Loss  0.02125556766986847\n",
            "Epoch  10 Batch  247 / 525  Training Loss  0.015019367448985577\n",
            "Epoch  10 Batch  248 / 525  Training Loss  0.01255511213093996\n",
            "Epoch  10 Batch  249 / 525  Training Loss  0.020921368151903152\n",
            "Epoch  10 Batch  250 / 525  Training Loss  0.020957376807928085\n",
            "Epoch  10 Batch  251 / 525  Training Loss  0.02964284084737301\n",
            "Epoch  10 Batch  252 / 525  Training Loss  0.015010938048362732\n",
            "Epoch  10 Batch  253 / 525  Training Loss  0.01739526353776455\n",
            "Epoch  10 Batch  254 / 525  Training Loss  0.019166627898812294\n",
            "Epoch  10 Batch  255 / 525  Training Loss  0.016967058181762695\n",
            "Epoch  10 Batch  256 / 525  Training Loss  0.02494971826672554\n",
            "Epoch  10 Batch  257 / 525  Training Loss  0.014156637713313103\n",
            "Epoch  10 Batch  258 / 525  Training Loss  0.011621246114373207\n",
            "Epoch  10 Batch  259 / 525  Training Loss  0.011420820839703083\n",
            "Epoch  10 Batch  260 / 525  Training Loss  0.025627240538597107\n",
            "Epoch  10 Batch  261 / 525  Training Loss  0.022178569808602333\n",
            "Epoch  10 Batch  262 / 525  Training Loss  0.02342449501156807\n",
            "Epoch  10 Batch  263 / 525  Training Loss  0.013689545914530754\n",
            "Epoch  10 Batch  264 / 525  Training Loss  0.024393223226070404\n",
            "Epoch  10 Batch  265 / 525  Training Loss  0.019040778279304504\n",
            "Epoch  10 Batch  266 / 525  Training Loss  0.022436603903770447\n",
            "Epoch  10 Batch  267 / 525  Training Loss  0.018073908984661102\n",
            "Epoch  10 Batch  268 / 525  Training Loss  0.025144854560494423\n",
            "Epoch  10 Batch  269 / 525  Training Loss  0.019714485853910446\n",
            "Epoch  10 Batch  270 / 525  Training Loss  0.023219259455800056\n",
            "Epoch  10 Batch  271 / 525  Training Loss  0.015421740710735321\n",
            "Epoch  10 Batch  272 / 525  Training Loss  0.013832688331604004\n",
            "Epoch  10 Batch  273 / 525  Training Loss  0.012185782194137573\n",
            "Epoch  10 Batch  274 / 525  Training Loss  0.019366437569260597\n",
            "Epoch  10 Batch  275 / 525  Training Loss  0.019377822056412697\n",
            "Epoch  10 Batch  276 / 525  Training Loss  0.011619482189416885\n",
            "Epoch  10 Batch  277 / 525  Training Loss  0.024945642799139023\n",
            "Epoch  10 Batch  278 / 525  Training Loss  0.022179096937179565\n",
            "Epoch  10 Batch  279 / 525  Training Loss  0.012045304290950298\n",
            "Epoch  10 Batch  280 / 525  Training Loss  0.023136891424655914\n",
            "Epoch  10 Batch  281 / 525  Training Loss  0.02197011187672615\n",
            "Epoch  10 Batch  282 / 525  Training Loss  0.029209185391664505\n",
            "Epoch  10 Batch  283 / 525  Training Loss  0.01239621452987194\n",
            "Epoch  10 Batch  284 / 525  Training Loss  0.022252250462770462\n",
            "Epoch  10 Batch  285 / 525  Training Loss  0.02286912314593792\n",
            "Epoch  10 Batch  286 / 525  Training Loss  0.02897513471543789\n",
            "Epoch  10 Batch  287 / 525  Training Loss  0.012890389189124107\n",
            "Epoch  10 Batch  288 / 525  Training Loss  0.02269016206264496\n",
            "Epoch  10 Batch  289 / 525  Training Loss  0.021454233676195145\n",
            "Epoch  10 Batch  290 / 525  Training Loss  0.026813605800271034\n",
            "Epoch  10 Batch  291 / 525  Training Loss  0.025283262133598328\n",
            "Epoch  10 Batch  292 / 525  Training Loss  0.02210434526205063\n",
            "Epoch  10 Batch  293 / 525  Training Loss  0.019766857847571373\n",
            "Epoch  10 Batch  294 / 525  Training Loss  0.012281100265681744\n",
            "Epoch  10 Batch  295 / 525  Training Loss  0.018739888444542885\n",
            "Epoch  10 Batch  296 / 525  Training Loss  0.02219853177666664\n",
            "Epoch  10 Batch  297 / 525  Training Loss  0.018845753744244576\n",
            "Epoch  10 Batch  298 / 525  Training Loss  0.021165970712900162\n",
            "Epoch  10 Batch  299 / 525  Training Loss  0.028813133016228676\n",
            "Epoch  10 Batch  300 / 525  Training Loss  0.020082643255591393\n",
            "Epoch  10 Batch  301 / 525  Training Loss  0.0174531452357769\n",
            "Epoch  10 Batch  302 / 525  Training Loss  0.014101387932896614\n",
            "Epoch  10 Batch  303 / 525  Training Loss  0.01958910934627056\n",
            "Epoch  10 Batch  304 / 525  Training Loss  0.023188753053545952\n",
            "Epoch  10 Batch  305 / 525  Training Loss  0.027458786964416504\n",
            "Epoch  10 Batch  306 / 525  Training Loss  0.021527552977204323\n",
            "Epoch  10 Batch  307 / 525  Training Loss  0.022282954305410385\n",
            "Epoch  10 Batch  308 / 525  Training Loss  0.01591428741812706\n",
            "Epoch  10 Batch  309 / 525  Training Loss  0.021015118807554245\n",
            "Epoch  10 Batch  310 / 525  Training Loss  0.031356506049633026\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  10 Batch  311 / 525  Training Loss  0.035111404955387115\n",
            "Epoch  10 Batch  312 / 525  Training Loss  0.014167060144245625\n",
            "Epoch  10 Batch  313 / 525  Training Loss  0.042323604226112366\n",
            "Epoch  10 Batch  314 / 525  Training Loss  0.011408677324652672\n",
            "Epoch  10 Batch  315 / 525  Training Loss  0.027137398719787598\n",
            "Epoch  10 Batch  316 / 525  Training Loss  0.008880439214408398\n",
            "Epoch  10 Batch  317 / 525  Training Loss  0.014949575066566467\n",
            "Epoch  10 Batch  318 / 525  Training Loss  0.03746037930250168\n",
            "Epoch  10 Batch  319 / 525  Training Loss  0.028905799612402916\n",
            "Epoch  10 Batch  320 / 525  Training Loss  0.02580433525145054\n",
            "Epoch  10 Batch  321 / 525  Training Loss  0.01665988191962242\n",
            "Epoch  10 Batch  322 / 525  Training Loss  0.012444569729268551\n",
            "Epoch  10 Batch  323 / 525  Training Loss  0.01531123835593462\n",
            "Epoch  10 Batch  324 / 525  Training Loss  0.02274906635284424\n",
            "Epoch  10 Batch  325 / 525  Training Loss  0.019431743770837784\n",
            "Epoch  10 Batch  326 / 525  Training Loss  0.014706750400364399\n",
            "Epoch  10 Batch  327 / 525  Training Loss  0.02406979724764824\n",
            "Epoch  10 Batch  328 / 525  Training Loss  0.01907050982117653\n",
            "Epoch  10 Batch  329 / 525  Training Loss  0.010044415481388569\n",
            "Epoch  10 Batch  330 / 525  Training Loss  0.017845461145043373\n",
            "Epoch  10 Batch  331 / 525  Training Loss  0.014099185355007648\n",
            "Epoch  10 Batch  332 / 525  Training Loss  0.015933480113744736\n",
            "Epoch  10 Batch  333 / 525  Training Loss  0.013924072496592999\n",
            "Epoch  10 Batch  334 / 525  Training Loss  0.01064012385904789\n",
            "Epoch  10 Batch  335 / 525  Training Loss  0.009485142305493355\n",
            "Epoch  10 Batch  336 / 525  Training Loss  0.022524308413267136\n",
            "Epoch  10 Batch  337 / 525  Training Loss  0.02059926651418209\n",
            "Epoch  10 Batch  338 / 525  Training Loss  0.01746865175664425\n",
            "Epoch  10 Batch  339 / 525  Training Loss  0.021677186712622643\n",
            "Epoch  10 Batch  340 / 525  Training Loss  0.02040240727365017\n",
            "Epoch  10 Batch  341 / 525  Training Loss  0.022986911237239838\n",
            "Epoch  10 Batch  342 / 525  Training Loss  0.027655605226755142\n",
            "Epoch  10 Batch  343 / 525  Training Loss  0.030016833916306496\n",
            "Epoch  10 Batch  344 / 525  Training Loss  0.006541850510984659\n",
            "Epoch  10 Batch  345 / 525  Training Loss  0.023091202601790428\n",
            "Epoch  10 Batch  346 / 525  Training Loss  0.01930372603237629\n",
            "Epoch  10 Batch  347 / 525  Training Loss  0.012204298749566078\n",
            "Epoch  10 Batch  348 / 525  Training Loss  0.014347205869853497\n",
            "Epoch  10 Batch  349 / 525  Training Loss  0.014847049489617348\n",
            "Epoch  10 Batch  350 / 525  Training Loss  0.022011801600456238\n",
            "Epoch  10 Batch  351 / 525  Training Loss  0.027859896421432495\n",
            "Epoch  10 Batch  352 / 525  Training Loss  0.02541704848408699\n",
            "Epoch  10 Batch  353 / 525  Training Loss  0.03670624643564224\n",
            "Epoch  10 Batch  354 / 525  Training Loss  0.030555162578821182\n",
            "Epoch  10 Batch  355 / 525  Training Loss  0.01902981847524643\n",
            "Epoch  10 Batch  356 / 525  Training Loss  0.016767416149377823\n",
            "Epoch  10 Batch  357 / 525  Training Loss  0.014963152818381786\n",
            "Epoch  10 Batch  358 / 525  Training Loss  0.018052082508802414\n",
            "Epoch  10 Batch  359 / 525  Training Loss  0.02397264540195465\n",
            "Epoch  10 Batch  360 / 525  Training Loss  0.011688463389873505\n",
            "Epoch  10 Batch  361 / 525  Training Loss  0.01922995038330555\n",
            "Epoch  10 Batch  362 / 525  Training Loss  0.013761681504547596\n",
            "Epoch  10 Batch  363 / 525  Training Loss  0.02666475810110569\n",
            "Epoch  10 Batch  364 / 525  Training Loss  0.015532794408500195\n",
            "Epoch  10 Batch  365 / 525  Training Loss  0.03031006082892418\n",
            "Epoch  10 Batch  366 / 525  Training Loss  0.03614077717065811\n",
            "Epoch  10 Batch  367 / 525  Training Loss  0.019557854160666466\n",
            "Epoch  10 Batch  368 / 525  Training Loss  0.017779819667339325\n",
            "Epoch  10 Batch  369 / 525  Training Loss  0.02289111539721489\n",
            "Epoch  10 Batch  370 / 525  Training Loss  0.019303683191537857\n",
            "Epoch  10 Batch  371 / 525  Training Loss  0.012646043673157692\n",
            "Epoch  10 Batch  372 / 525  Training Loss  0.013499009422957897\n",
            "Epoch  10 Batch  373 / 525  Training Loss  0.01395629346370697\n",
            "Epoch  10 Batch  374 / 525  Training Loss  0.014981865882873535\n",
            "Epoch  10 Batch  375 / 525  Training Loss  0.014982247725129128\n",
            "Epoch  10 Batch  376 / 525  Training Loss  0.01459728367626667\n",
            "Epoch  10 Batch  377 / 525  Training Loss  0.02805059589445591\n",
            "Epoch  10 Batch  378 / 525  Training Loss  0.020146258175373077\n",
            "Epoch  10 Batch  379 / 525  Training Loss  0.016820808872580528\n",
            "Epoch  10 Batch  380 / 525  Training Loss  0.015001127496361732\n",
            "Epoch  10 Batch  381 / 525  Training Loss  0.026979470625519753\n",
            "Epoch  10 Batch  382 / 525  Training Loss  0.020991770550608635\n",
            "Epoch  10 Batch  383 / 525  Training Loss  0.029579918831586838\n",
            "Epoch  10 Batch  384 / 525  Training Loss  0.015936078503727913\n",
            "Epoch  10 Batch  385 / 525  Training Loss  0.022612249478697777\n",
            "Epoch  10 Batch  386 / 525  Training Loss  0.009668443351984024\n",
            "Epoch  10 Batch  387 / 525  Training Loss  0.01993352547287941\n",
            "Epoch  10 Batch  388 / 525  Training Loss  0.017672862857580185\n",
            "Epoch  10 Batch  389 / 525  Training Loss  0.009800365194678307\n",
            "Epoch  10 Batch  390 / 525  Training Loss  0.013717589899897575\n",
            "Epoch  10 Batch  391 / 525  Training Loss  0.013599616475403309\n",
            "Epoch  10 Batch  392 / 525  Training Loss  0.013382216915488243\n",
            "Epoch  10 Batch  393 / 525  Training Loss  0.017367636784911156\n",
            "Epoch  10 Batch  394 / 525  Training Loss  0.014335444197058678\n",
            "Epoch  10 Batch  395 / 525  Training Loss  0.014088697731494904\n",
            "Epoch  10 Batch  396 / 525  Training Loss  0.016867659986019135\n",
            "Epoch  10 Batch  397 / 525  Training Loss  0.017653267830610275\n",
            "Epoch  10 Batch  398 / 525  Training Loss  0.022739330306649208\n",
            "Epoch  10 Batch  399 / 525  Training Loss  0.01500922255218029\n",
            "Epoch  10 Batch  400 / 525  Training Loss  0.020916201174259186\n",
            "Epoch  10 Batch  401 / 525  Training Loss  0.00933309830725193\n",
            "Epoch  10 Batch  402 / 525  Training Loss  0.013754725456237793\n",
            "Epoch  10 Batch  403 / 525  Training Loss  0.030238250270485878\n",
            "Epoch  10 Batch  404 / 525  Training Loss  0.012942750938236713\n",
            "Epoch  10 Batch  405 / 525  Training Loss  0.029024625197052956\n",
            "Epoch  10 Batch  406 / 525  Training Loss  0.022445429116487503\n",
            "Epoch  10 Batch  407 / 525  Training Loss  0.02711237035691738\n",
            "Epoch  10 Batch  408 / 525  Training Loss  0.018148530274629593\n",
            "Epoch  10 Batch  409 / 525  Training Loss  0.014215904287993908\n",
            "Epoch  10 Batch  410 / 525  Training Loss  0.015037457458674908\n",
            "Epoch  10 Batch  411 / 525  Training Loss  0.015254586935043335\n",
            "Epoch  10 Batch  412 / 525  Training Loss  0.017851773649454117\n",
            "Epoch  10 Batch  413 / 525  Training Loss  0.019069798290729523\n",
            "Epoch  10 Batch  414 / 525  Training Loss  0.017193591222167015\n",
            "Epoch  10 Batch  415 / 525  Training Loss  0.023595690727233887\n",
            "Epoch  10 Batch  416 / 525  Training Loss  0.022223662585020065\n",
            "Epoch  10 Batch  417 / 525  Training Loss  0.04126539081335068\n",
            "Epoch  10 Batch  418 / 525  Training Loss  0.015255513601005077\n",
            "Epoch  10 Batch  419 / 525  Training Loss  0.03364857658743858\n",
            "Epoch  10 Batch  420 / 525  Training Loss  0.010845759883522987\n",
            "Epoch  10 Batch  421 / 525  Training Loss  0.03163143992424011\n",
            "Epoch  10 Batch  422 / 525  Training Loss  0.02229246124625206\n",
            "Epoch  10 Batch  423 / 525  Training Loss  0.019908804446458817\n",
            "Epoch  10 Batch  424 / 525  Training Loss  0.020475704222917557\n",
            "Epoch  10 Batch  425 / 525  Training Loss  0.0288368072360754\n",
            "Epoch  10 Batch  426 / 525  Training Loss  0.014494771137833595\n",
            "Epoch  10 Batch  427 / 525  Training Loss  0.018160199746489525\n",
            "Epoch  10 Batch  428 / 525  Training Loss  0.016115080565214157\n",
            "Epoch  10 Batch  429 / 525  Training Loss  0.01901666447520256\n",
            "Epoch  10 Batch  430 / 525  Training Loss  0.015214070677757263\n",
            "Epoch  10 Batch  431 / 525  Training Loss  0.013991864398121834\n",
            "Epoch  10 Batch  432 / 525  Training Loss  0.015258456580340862\n",
            "Epoch  10 Batch  433 / 525  Training Loss  0.02229059860110283\n",
            "Epoch  10 Batch  434 / 525  Training Loss  0.010482830926775932\n",
            "Epoch  10 Batch  435 / 525  Training Loss  0.01997116580605507\n",
            "Epoch  10 Batch  436 / 525  Training Loss  0.023871390148997307\n",
            "Epoch  10 Batch  437 / 525  Training Loss  0.020235184580087662\n",
            "Epoch  10 Batch  438 / 525  Training Loss  0.037791166454553604\n",
            "Epoch  10 Batch  439 / 525  Training Loss  0.031245669350028038\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  10 Batch  440 / 525  Training Loss  0.013015983626246452\n",
            "Epoch  10 Batch  441 / 525  Training Loss  0.023136984556913376\n",
            "Epoch  10 Batch  442 / 525  Training Loss  0.02118351310491562\n",
            "Epoch  10 Batch  443 / 525  Training Loss  0.02280304953455925\n",
            "Epoch  10 Batch  444 / 525  Training Loss  0.029070893302559853\n",
            "Epoch  10 Batch  445 / 525  Training Loss  0.01934635639190674\n",
            "Epoch  10 Batch  446 / 525  Training Loss  0.015255628153681755\n",
            "Epoch  10 Batch  447 / 525  Training Loss  0.03602566197514534\n",
            "Epoch  10 Batch  448 / 525  Training Loss  0.03056306578218937\n",
            "Epoch  10 Batch  449 / 525  Training Loss  0.01644621230661869\n",
            "Epoch  10 Batch  450 / 525  Training Loss  0.016136961057782173\n",
            "Epoch  10 Batch  451 / 525  Training Loss  0.01499887090176344\n",
            "Epoch  10 Batch  452 / 525  Training Loss  0.01624136045575142\n",
            "Epoch  10 Batch  453 / 525  Training Loss  0.016155194491147995\n",
            "Epoch  10 Batch  454 / 525  Training Loss  0.020279856398701668\n",
            "Epoch  10 Batch  455 / 525  Training Loss  0.02504592016339302\n",
            "Epoch  10 Batch  456 / 525  Training Loss  0.022275174036622047\n",
            "Epoch  10 Batch  457 / 525  Training Loss  0.013058662414550781\n",
            "Epoch  10 Batch  458 / 525  Training Loss  0.01893887296319008\n",
            "Epoch  10 Batch  459 / 525  Training Loss  0.0234672874212265\n",
            "Epoch  10 Batch  460 / 525  Training Loss  0.024916237220168114\n",
            "Epoch  10 Batch  461 / 525  Training Loss  0.02105867490172386\n",
            "Epoch  10 Batch  462 / 525  Training Loss  0.02289782464504242\n",
            "Epoch  10 Batch  463 / 525  Training Loss  0.035569120198488235\n",
            "Epoch  10 Batch  464 / 525  Training Loss  0.011895591393113136\n",
            "Epoch  10 Batch  465 / 525  Training Loss  0.012733285315334797\n",
            "Epoch  10 Batch  466 / 525  Training Loss  0.02302653342485428\n",
            "Epoch  10 Batch  467 / 525  Training Loss  0.021011782810091972\n",
            "Epoch  10 Batch  468 / 525  Training Loss  0.018599506467580795\n",
            "Epoch  10 Batch  469 / 525  Training Loss  0.018824396654963493\n",
            "Epoch  10 Batch  470 / 525  Training Loss  0.006810593418776989\n",
            "Epoch  10 Batch  471 / 525  Training Loss  0.016451139003038406\n",
            "Epoch  10 Batch  472 / 525  Training Loss  0.010342178866267204\n",
            "Epoch  10 Batch  473 / 525  Training Loss  0.008942130953073502\n",
            "Epoch  10 Batch  474 / 525  Training Loss  0.02111111395061016\n",
            "Epoch  10 Batch  475 / 525  Training Loss  0.02778957411646843\n",
            "Epoch  10 Batch  476 / 525  Training Loss  0.01626114919781685\n",
            "Epoch  10 Batch  477 / 525  Training Loss  0.02327406220138073\n",
            "Epoch  10 Batch  478 / 525  Training Loss  0.011809295043349266\n",
            "Epoch  10 Batch  479 / 525  Training Loss  0.012156883254647255\n",
            "Epoch  10 Batch  480 / 525  Training Loss  0.017936766147613525\n",
            "Epoch  10 Batch  481 / 525  Training Loss  0.007769822143018246\n",
            "Epoch  10 Batch  482 / 525  Training Loss  0.017604876309633255\n",
            "Epoch  10 Batch  483 / 525  Training Loss  0.018385935574769974\n",
            "Epoch  10 Batch  484 / 525  Training Loss  0.025105440989136696\n",
            "Epoch  10 Batch  485 / 525  Training Loss  0.023809175938367844\n",
            "Epoch  10 Batch  486 / 525  Training Loss  0.020294124260544777\n",
            "Epoch  10 Batch  487 / 525  Training Loss  0.019709689542651176\n",
            "Epoch  10 Batch  488 / 525  Training Loss  0.016152679920196533\n",
            "Epoch  10 Batch  489 / 525  Training Loss  0.017996709793806076\n",
            "Epoch  10 Batch  490 / 525  Training Loss  0.014731364324688911\n",
            "Epoch  10 Batch  491 / 525  Training Loss  0.01467030681669712\n",
            "Epoch  10 Batch  492 / 525  Training Loss  0.020496195182204247\n",
            "Epoch  10 Batch  493 / 525  Training Loss  0.016743268817663193\n",
            "Epoch  10 Batch  494 / 525  Training Loss  0.018676508218050003\n",
            "Epoch  10 Batch  495 / 525  Training Loss  0.018424808979034424\n",
            "Epoch  10 Batch  496 / 525  Training Loss  0.011676494963467121\n",
            "Epoch  10 Batch  497 / 525  Training Loss  0.018484337255358696\n",
            "Epoch  10 Batch  498 / 525  Training Loss  0.018639367073774338\n",
            "Epoch  10 Batch  499 / 525  Training Loss  0.021132994443178177\n",
            "Epoch  10 Batch  500 / 525  Training Loss  0.03127896040678024\n",
            "Epoch  10 Batch  501 / 525  Training Loss  0.03970484808087349\n",
            "Epoch  10 Batch  502 / 525  Training Loss  0.030340995639562607\n",
            "Epoch  10 Batch  503 / 525  Training Loss  0.017500733956694603\n",
            "Epoch  10 Batch  504 / 525  Training Loss  0.03951765224337578\n",
            "Epoch  10 Batch  505 / 525  Training Loss  0.015737384557724\n",
            "Epoch  10 Batch  506 / 525  Training Loss  0.016660112887620926\n",
            "Epoch  10 Batch  507 / 525  Training Loss  0.01644003763794899\n",
            "Epoch  10 Batch  508 / 525  Training Loss  0.014245639555156231\n",
            "Epoch  10 Batch  509 / 525  Training Loss  0.0070714266039431095\n",
            "Epoch  10 Batch  510 / 525  Training Loss  0.02057127095758915\n",
            "Epoch  10 Batch  511 / 525  Training Loss  0.01730281487107277\n",
            "Epoch  10 Batch  512 / 525  Training Loss  0.01170247234404087\n",
            "Epoch  10 Batch  513 / 525  Training Loss  0.014112383127212524\n",
            "Epoch  10 Batch  514 / 525  Training Loss  0.029767226427793503\n",
            "Epoch  10 Batch  515 / 525  Training Loss  0.028508245944976807\n",
            "Epoch  10 Batch  516 / 525  Training Loss  0.01886596903204918\n",
            "Epoch  10 Batch  517 / 525  Training Loss  0.024830756708979607\n",
            "Epoch  10 Batch  518 / 525  Training Loss  0.01621074229478836\n",
            "Epoch  10 Batch  519 / 525  Training Loss  0.024141883477568626\n",
            "Epoch  10 Batch  520 / 525  Training Loss  0.02230406366288662\n",
            "Epoch  10 Batch  521 / 525  Training Loss  0.039400894194841385\n",
            "Epoch  10 Batch  522 / 525  Training Loss  0.021537791937589645\n",
            "Epoch  10 Batch  523 / 525  Training Loss  0.015756363049149513\n",
            "Epoch  10 Batch  524 / 525  Training Loss  0.01855551451444626\n",
            "  11    |    -    |   0.018560   |   58.14  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 11\n",
            "Epoch  11 Batch  0 / 525  Training Loss  0.013258901424705982\n",
            "Epoch  11 Batch  1 / 525  Training Loss  0.009294356219470501\n",
            "Epoch  11 Batch  2 / 525  Training Loss  0.010155152529478073\n",
            "Epoch  11 Batch  3 / 525  Training Loss  0.011257578618824482\n",
            "Epoch  11 Batch  4 / 525  Training Loss  0.012826047837734222\n",
            "Epoch  11 Batch  5 / 525  Training Loss  0.0162114966660738\n",
            "Epoch  11 Batch  6 / 525  Training Loss  0.006276089698076248\n",
            "Epoch  11 Batch  7 / 525  Training Loss  0.013005164451897144\n",
            "Epoch  11 Batch  8 / 525  Training Loss  0.0073258234187960625\n",
            "Epoch  11 Batch  9 / 525  Training Loss  0.010254322551190853\n",
            "Epoch  11 Batch  10 / 525  Training Loss  0.00661372859030962\n",
            "Epoch  11 Batch  11 / 525  Training Loss  0.012039977125823498\n",
            "Epoch  11 Batch  12 / 525  Training Loss  0.01052683126181364\n",
            "Epoch  11 Batch  13 / 525  Training Loss  0.006678134202957153\n",
            "Epoch  11 Batch  14 / 525  Training Loss  0.00845362525433302\n",
            "Epoch  11 Batch  15 / 525  Training Loss  0.011487193405628204\n",
            "Epoch  11 Batch  16 / 525  Training Loss  0.01113745104521513\n",
            "Epoch  11 Batch  17 / 525  Training Loss  0.009997271001338959\n",
            "Epoch  11 Batch  18 / 525  Training Loss  0.00835081934928894\n",
            "Epoch  11 Batch  19 / 525  Training Loss  0.009504454210400581\n",
            "Epoch  11 Batch  20 / 525  Training Loss  0.010293431580066681\n",
            "Epoch  11 Batch  21 / 525  Training Loss  0.005416323896497488\n",
            "Epoch  11 Batch  22 / 525  Training Loss  0.009747790172696114\n",
            "Epoch  11 Batch  23 / 525  Training Loss  0.00935368426144123\n",
            "Epoch  11 Batch  24 / 525  Training Loss  0.010777311399579048\n",
            "Epoch  11 Batch  25 / 525  Training Loss  0.009406614117324352\n",
            "Epoch  11 Batch  26 / 525  Training Loss  0.005678688641637564\n",
            "Epoch  11 Batch  27 / 525  Training Loss  0.007053860928863287\n",
            "Epoch  11 Batch  28 / 525  Training Loss  0.004950389731675386\n",
            "Epoch  11 Batch  29 / 525  Training Loss  0.013304084539413452\n",
            "Epoch  11 Batch  30 / 525  Training Loss  0.016013210639357567\n",
            "Epoch  11 Batch  31 / 525  Training Loss  0.007313658948987722\n",
            "Epoch  11 Batch  32 / 525  Training Loss  0.0044110920280218124\n",
            "Epoch  11 Batch  33 / 525  Training Loss  0.008331661112606525\n",
            "Epoch  11 Batch  34 / 525  Training Loss  0.018994266167283058\n",
            "Epoch  11 Batch  35 / 525  Training Loss  0.014433520846068859\n",
            "Epoch  11 Batch  36 / 525  Training Loss  0.02193727158010006\n",
            "Epoch  11 Batch  37 / 525  Training Loss  0.013535124249756336\n",
            "Epoch  11 Batch  38 / 525  Training Loss  0.010865343734622002\n",
            "Epoch  11 Batch  39 / 525  Training Loss  0.013011259026825428\n",
            "Epoch  11 Batch  40 / 525  Training Loss  0.00825485773384571\n",
            "Epoch  11 Batch  41 / 525  Training Loss  0.009013242088258266\n",
            "Epoch  11 Batch  42 / 525  Training Loss  0.011491465382277966\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  11 Batch  43 / 525  Training Loss  0.013809017837047577\n",
            "Epoch  11 Batch  44 / 525  Training Loss  0.005849617533385754\n",
            "Epoch  11 Batch  45 / 525  Training Loss  0.011504929512739182\n",
            "Epoch  11 Batch  46 / 525  Training Loss  0.01210060529410839\n",
            "Epoch  11 Batch  47 / 525  Training Loss  0.00690790731459856\n",
            "Epoch  11 Batch  48 / 525  Training Loss  0.011269345879554749\n",
            "Epoch  11 Batch  49 / 525  Training Loss  0.009546227753162384\n",
            "Epoch  11 Batch  50 / 525  Training Loss  0.010705779306590557\n",
            "Epoch  11 Batch  51 / 525  Training Loss  0.011304431594908237\n",
            "Epoch  11 Batch  52 / 525  Training Loss  0.009033756330609322\n",
            "Epoch  11 Batch  53 / 525  Training Loss  0.0054141562432050705\n",
            "Epoch  11 Batch  54 / 525  Training Loss  0.014601016417145729\n",
            "Epoch  11 Batch  55 / 525  Training Loss  0.005714929196983576\n",
            "Epoch  11 Batch  56 / 525  Training Loss  0.01016618125140667\n",
            "Epoch  11 Batch  57 / 525  Training Loss  0.012181403115391731\n",
            "Epoch  11 Batch  58 / 525  Training Loss  0.013592255301773548\n",
            "Epoch  11 Batch  59 / 525  Training Loss  0.014045159332454205\n",
            "Epoch  11 Batch  60 / 525  Training Loss  0.012083514593541622\n",
            "Epoch  11 Batch  61 / 525  Training Loss  0.013246980495750904\n",
            "Epoch  11 Batch  62 / 525  Training Loss  0.00743580749258399\n",
            "Epoch  11 Batch  63 / 525  Training Loss  0.011144988238811493\n",
            "Epoch  11 Batch  64 / 525  Training Loss  0.02811616286635399\n",
            "Epoch  11 Batch  65 / 525  Training Loss  0.018914412707090378\n",
            "Epoch  11 Batch  66 / 525  Training Loss  0.022840959951281548\n",
            "Epoch  11 Batch  67 / 525  Training Loss  0.013512089848518372\n",
            "Epoch  11 Batch  68 / 525  Training Loss  0.008080845698714256\n",
            "Epoch  11 Batch  69 / 525  Training Loss  0.0062617352232337\n",
            "Epoch  11 Batch  70 / 525  Training Loss  0.00971430353820324\n",
            "Epoch  11 Batch  71 / 525  Training Loss  0.012930047698318958\n",
            "Epoch  11 Batch  72 / 525  Training Loss  0.006341634783893824\n",
            "Epoch  11 Batch  73 / 525  Training Loss  0.005438283551484346\n",
            "Epoch  11 Batch  74 / 525  Training Loss  0.004297291394323111\n",
            "Epoch  11 Batch  75 / 525  Training Loss  0.009187746793031693\n",
            "Epoch  11 Batch  76 / 525  Training Loss  0.008905249647796154\n",
            "Epoch  11 Batch  77 / 525  Training Loss  0.03899182379245758\n",
            "Epoch  11 Batch  78 / 525  Training Loss  0.005235439166426659\n",
            "Epoch  11 Batch  79 / 525  Training Loss  0.02535426616668701\n",
            "Epoch  11 Batch  80 / 525  Training Loss  0.02240782603621483\n",
            "Epoch  11 Batch  81 / 525  Training Loss  0.02120785415172577\n",
            "Epoch  11 Batch  82 / 525  Training Loss  0.010243093594908714\n",
            "Epoch  11 Batch  83 / 525  Training Loss  0.006719308439642191\n",
            "Epoch  11 Batch  84 / 525  Training Loss  0.005053413100540638\n",
            "Epoch  11 Batch  85 / 525  Training Loss  0.004759601317346096\n",
            "Epoch  11 Batch  86 / 525  Training Loss  0.00608777767047286\n",
            "Epoch  11 Batch  87 / 525  Training Loss  0.006103837862610817\n",
            "Epoch  11 Batch  88 / 525  Training Loss  0.014263423159718513\n",
            "Epoch  11 Batch  89 / 525  Training Loss  0.0037808497436344624\n",
            "Epoch  11 Batch  90 / 525  Training Loss  0.009547222405672073\n",
            "Epoch  11 Batch  91 / 525  Training Loss  0.010629622265696526\n",
            "Epoch  11 Batch  92 / 525  Training Loss  0.0069690546952188015\n",
            "Epoch  11 Batch  93 / 525  Training Loss  0.005954986438155174\n",
            "Epoch  11 Batch  94 / 525  Training Loss  0.007117005996406078\n",
            "Epoch  11 Batch  95 / 525  Training Loss  0.006803973112255335\n",
            "Epoch  11 Batch  96 / 525  Training Loss  0.004657009616494179\n",
            "Epoch  11 Batch  97 / 525  Training Loss  0.007077159825712442\n",
            "Epoch  11 Batch  98 / 525  Training Loss  0.014126164838671684\n",
            "Epoch  11 Batch  99 / 525  Training Loss  0.008755686692893505\n",
            "Epoch  11 Batch  100 / 525  Training Loss  0.01502574235200882\n",
            "Epoch  11 Batch  101 / 525  Training Loss  0.01765521802008152\n",
            "Epoch  11 Batch  102 / 525  Training Loss  0.010156821459531784\n",
            "Epoch  11 Batch  103 / 525  Training Loss  0.012444245629012585\n",
            "Epoch  11 Batch  104 / 525  Training Loss  0.012021169997751713\n",
            "Epoch  11 Batch  105 / 525  Training Loss  0.02915448509156704\n",
            "Epoch  11 Batch  106 / 525  Training Loss  0.017766103148460388\n",
            "Epoch  11 Batch  107 / 525  Training Loss  0.0206805057823658\n",
            "Epoch  11 Batch  108 / 525  Training Loss  0.007337423972785473\n",
            "Epoch  11 Batch  109 / 525  Training Loss  0.01934024691581726\n",
            "Epoch  11 Batch  110 / 525  Training Loss  0.020043034106492996\n",
            "Epoch  11 Batch  111 / 525  Training Loss  0.018622126430273056\n",
            "Epoch  11 Batch  112 / 525  Training Loss  0.014988288283348083\n",
            "Epoch  11 Batch  113 / 525  Training Loss  0.0074036261066794395\n",
            "Epoch  11 Batch  114 / 525  Training Loss  0.006676440127193928\n",
            "Epoch  11 Batch  115 / 525  Training Loss  0.012163151986896992\n",
            "Epoch  11 Batch  116 / 525  Training Loss  0.009281326085329056\n",
            "Epoch  11 Batch  117 / 525  Training Loss  0.013431638479232788\n",
            "Epoch  11 Batch  118 / 525  Training Loss  0.006736361421644688\n",
            "Epoch  11 Batch  119 / 525  Training Loss  0.010294092819094658\n",
            "Epoch  11 Batch  120 / 525  Training Loss  0.023652035742998123\n",
            "Epoch  11 Batch  121 / 525  Training Loss  0.006085291039198637\n",
            "Epoch  11 Batch  122 / 525  Training Loss  0.006135804113000631\n",
            "Epoch  11 Batch  123 / 525  Training Loss  0.00875968486070633\n",
            "Epoch  11 Batch  124 / 525  Training Loss  0.009667051956057549\n",
            "Epoch  11 Batch  125 / 525  Training Loss  0.013359785079956055\n",
            "Epoch  11 Batch  126 / 525  Training Loss  0.023894768208265305\n",
            "Epoch  11 Batch  127 / 525  Training Loss  0.006970430724322796\n",
            "Epoch  11 Batch  128 / 525  Training Loss  0.01183097343891859\n",
            "Epoch  11 Batch  129 / 525  Training Loss  0.009579162113368511\n",
            "Epoch  11 Batch  130 / 525  Training Loss  0.009952275082468987\n",
            "Epoch  11 Batch  131 / 525  Training Loss  0.010732890106737614\n",
            "Epoch  11 Batch  132 / 525  Training Loss  0.009918296709656715\n",
            "Epoch  11 Batch  133 / 525  Training Loss  0.005153785925358534\n",
            "Epoch  11 Batch  134 / 525  Training Loss  0.008758338168263435\n",
            "Epoch  11 Batch  135 / 525  Training Loss  0.018102195113897324\n",
            "Epoch  11 Batch  136 / 525  Training Loss  0.02134537696838379\n",
            "Epoch  11 Batch  137 / 525  Training Loss  0.004404214210808277\n",
            "Epoch  11 Batch  138 / 525  Training Loss  0.00906924344599247\n",
            "Epoch  11 Batch  139 / 525  Training Loss  0.005074366461485624\n",
            "Epoch  11 Batch  140 / 525  Training Loss  0.005960569251328707\n",
            "Epoch  11 Batch  141 / 525  Training Loss  0.006246945820748806\n",
            "Epoch  11 Batch  142 / 525  Training Loss  0.006723769940435886\n",
            "Epoch  11 Batch  143 / 525  Training Loss  0.009264344349503517\n",
            "Epoch  11 Batch  144 / 525  Training Loss  0.008139622397720814\n",
            "Epoch  11 Batch  145 / 525  Training Loss  0.01831219531595707\n",
            "Epoch  11 Batch  146 / 525  Training Loss  0.006120367906987667\n",
            "Epoch  11 Batch  147 / 525  Training Loss  0.013275409117341042\n",
            "Epoch  11 Batch  148 / 525  Training Loss  0.005101159680634737\n",
            "Epoch  11 Batch  149 / 525  Training Loss  0.00920152384787798\n",
            "Epoch  11 Batch  150 / 525  Training Loss  0.01669873669743538\n",
            "Epoch  11 Batch  151 / 525  Training Loss  0.006842124275863171\n",
            "Epoch  11 Batch  152 / 525  Training Loss  0.004923941567540169\n",
            "Epoch  11 Batch  153 / 525  Training Loss  0.013112550601363182\n",
            "Epoch  11 Batch  154 / 525  Training Loss  0.010078441351652145\n",
            "Epoch  11 Batch  155 / 525  Training Loss  0.00967331975698471\n",
            "Epoch  11 Batch  156 / 525  Training Loss  0.009294072166085243\n",
            "Epoch  11 Batch  157 / 525  Training Loss  0.007309799548238516\n",
            "Epoch  11 Batch  158 / 525  Training Loss  0.007262189872562885\n",
            "Epoch  11 Batch  159 / 525  Training Loss  0.008985372260212898\n",
            "Epoch  11 Batch  160 / 525  Training Loss  0.01559836883097887\n",
            "Epoch  11 Batch  161 / 525  Training Loss  0.009896088391542435\n",
            "Epoch  11 Batch  162 / 525  Training Loss  0.004833610728383064\n",
            "Epoch  11 Batch  163 / 525  Training Loss  0.004383475985378027\n",
            "Epoch  11 Batch  164 / 525  Training Loss  0.015851449221372604\n",
            "Epoch  11 Batch  165 / 525  Training Loss  0.007645372301340103\n",
            "Epoch  11 Batch  166 / 525  Training Loss  0.015102380886673927\n",
            "Epoch  11 Batch  167 / 525  Training Loss  0.004115733318030834\n",
            "Epoch  11 Batch  168 / 525  Training Loss  0.01261623203754425\n",
            "Epoch  11 Batch  169 / 525  Training Loss  0.008123273029923439\n",
            "Epoch  11 Batch  170 / 525  Training Loss  0.01841805689036846\n",
            "Epoch  11 Batch  171 / 525  Training Loss  0.005717755760997534\n",
            "Epoch  11 Batch  172 / 525  Training Loss  0.01703510992228985\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  11 Batch  173 / 525  Training Loss  0.011539148166775703\n",
            "Epoch  11 Batch  174 / 525  Training Loss  0.008450067602097988\n",
            "Epoch  11 Batch  175 / 525  Training Loss  0.007639259099960327\n",
            "Epoch  11 Batch  176 / 525  Training Loss  0.005137932021170855\n",
            "Epoch  11 Batch  177 / 525  Training Loss  0.0078008146956563\n",
            "Epoch  11 Batch  178 / 525  Training Loss  0.010643289424479008\n",
            "Epoch  11 Batch  179 / 525  Training Loss  0.005509652197360992\n",
            "Epoch  11 Batch  180 / 525  Training Loss  0.010942451655864716\n",
            "Epoch  11 Batch  181 / 525  Training Loss  0.005316474940627813\n",
            "Epoch  11 Batch  182 / 525  Training Loss  0.012023557908833027\n",
            "Epoch  11 Batch  183 / 525  Training Loss  0.0065317219123244286\n",
            "Epoch  11 Batch  184 / 525  Training Loss  0.011699218302965164\n",
            "Epoch  11 Batch  185 / 525  Training Loss  0.009196878410875797\n",
            "Epoch  11 Batch  186 / 525  Training Loss  0.006683179177343845\n",
            "Epoch  11 Batch  187 / 525  Training Loss  0.00831020437180996\n",
            "Epoch  11 Batch  188 / 525  Training Loss  0.004951159469783306\n",
            "Epoch  11 Batch  189 / 525  Training Loss  0.009832555428147316\n",
            "Epoch  11 Batch  190 / 525  Training Loss  0.006720378994941711\n",
            "Epoch  11 Batch  191 / 525  Training Loss  0.006688346620649099\n",
            "Epoch  11 Batch  192 / 525  Training Loss  0.007185281254351139\n",
            "Epoch  11 Batch  193 / 525  Training Loss  0.007894973270595074\n",
            "Epoch  11 Batch  194 / 525  Training Loss  0.010604159906506538\n",
            "Epoch  11 Batch  195 / 525  Training Loss  0.007234180811792612\n",
            "Epoch  11 Batch  196 / 525  Training Loss  0.009969521313905716\n",
            "Epoch  11 Batch  197 / 525  Training Loss  0.012603744864463806\n",
            "Epoch  11 Batch  198 / 525  Training Loss  0.002486326266080141\n",
            "Epoch  11 Batch  199 / 525  Training Loss  0.012326916679739952\n",
            "Epoch  11 Batch  200 / 525  Training Loss  0.004899488762021065\n",
            "Epoch  11 Batch  201 / 525  Training Loss  0.010087622329592705\n",
            "Epoch  11 Batch  202 / 525  Training Loss  0.005810804199427366\n",
            "Epoch  11 Batch  203 / 525  Training Loss  0.004977560136467218\n",
            "Epoch  11 Batch  204 / 525  Training Loss  0.015287728980183601\n",
            "Epoch  11 Batch  205 / 525  Training Loss  0.00837131030857563\n",
            "Epoch  11 Batch  206 / 525  Training Loss  0.010231010615825653\n",
            "Epoch  11 Batch  207 / 525  Training Loss  0.01134617067873478\n",
            "Epoch  11 Batch  208 / 525  Training Loss  0.009311830624938011\n",
            "Epoch  11 Batch  209 / 525  Training Loss  0.00874939002096653\n",
            "Epoch  11 Batch  210 / 525  Training Loss  0.011153317987918854\n",
            "Epoch  11 Batch  211 / 525  Training Loss  0.014011519961059093\n",
            "Epoch  11 Batch  212 / 525  Training Loss  0.0196051187813282\n",
            "Epoch  11 Batch  213 / 525  Training Loss  0.012899723835289478\n",
            "Epoch  11 Batch  214 / 525  Training Loss  0.011659597977995872\n",
            "Epoch  11 Batch  215 / 525  Training Loss  0.005611327011138201\n",
            "Epoch  11 Batch  216 / 525  Training Loss  0.006010222248733044\n",
            "Epoch  11 Batch  217 / 525  Training Loss  0.006203827448189259\n",
            "Epoch  11 Batch  218 / 525  Training Loss  0.0070234015583992004\n",
            "Epoch  11 Batch  219 / 525  Training Loss  0.021587388589978218\n",
            "Epoch  11 Batch  220 / 525  Training Loss  0.015625348314642906\n",
            "Epoch  11 Batch  221 / 525  Training Loss  0.013201199471950531\n",
            "Epoch  11 Batch  222 / 525  Training Loss  0.009114225395023823\n",
            "Epoch  11 Batch  223 / 525  Training Loss  0.01111797709017992\n",
            "Epoch  11 Batch  224 / 525  Training Loss  0.004866785369813442\n",
            "Epoch  11 Batch  225 / 525  Training Loss  0.008677667006850243\n",
            "Epoch  11 Batch  226 / 525  Training Loss  0.019991297274827957\n",
            "Epoch  11 Batch  227 / 525  Training Loss  0.01749427802860737\n",
            "Epoch  11 Batch  228 / 525  Training Loss  0.00791517086327076\n",
            "Epoch  11 Batch  229 / 525  Training Loss  0.014869233593344688\n",
            "Epoch  11 Batch  230 / 525  Training Loss  0.006381201092153788\n",
            "Epoch  11 Batch  231 / 525  Training Loss  0.008206131868064404\n",
            "Epoch  11 Batch  232 / 525  Training Loss  0.012729267589747906\n",
            "Epoch  11 Batch  233 / 525  Training Loss  0.009318124502897263\n",
            "Epoch  11 Batch  234 / 525  Training Loss  0.01336926780641079\n",
            "Epoch  11 Batch  235 / 525  Training Loss  0.010513908229768276\n",
            "Epoch  11 Batch  236 / 525  Training Loss  0.011230653151869774\n",
            "Epoch  11 Batch  237 / 525  Training Loss  0.007372955791652203\n",
            "Epoch  11 Batch  238 / 525  Training Loss  0.006719645112752914\n",
            "Epoch  11 Batch  239 / 525  Training Loss  0.0028827751521021128\n",
            "Epoch  11 Batch  240 / 525  Training Loss  0.00911751203238964\n",
            "Epoch  11 Batch  241 / 525  Training Loss  0.00999794714152813\n",
            "Epoch  11 Batch  242 / 525  Training Loss  0.018128033727407455\n",
            "Epoch  11 Batch  243 / 525  Training Loss  0.008085107430815697\n",
            "Epoch  11 Batch  244 / 525  Training Loss  0.009414894506335258\n",
            "Epoch  11 Batch  245 / 525  Training Loss  0.008948747999966145\n",
            "Epoch  11 Batch  246 / 525  Training Loss  0.007645945064723492\n",
            "Epoch  11 Batch  247 / 525  Training Loss  0.007406533695757389\n",
            "Epoch  11 Batch  248 / 525  Training Loss  0.01745481975376606\n",
            "Epoch  11 Batch  249 / 525  Training Loss  0.02032739482820034\n",
            "Epoch  11 Batch  250 / 525  Training Loss  0.008304419927299023\n",
            "Epoch  11 Batch  251 / 525  Training Loss  0.015929384157061577\n",
            "Epoch  11 Batch  252 / 525  Training Loss  0.02100813202559948\n",
            "Epoch  11 Batch  253 / 525  Training Loss  0.01245990302413702\n",
            "Epoch  11 Batch  254 / 525  Training Loss  0.014345352537930012\n",
            "Epoch  11 Batch  255 / 525  Training Loss  0.010295714251697063\n",
            "Epoch  11 Batch  256 / 525  Training Loss  0.009547887369990349\n",
            "Epoch  11 Batch  257 / 525  Training Loss  0.016279131174087524\n",
            "Epoch  11 Batch  258 / 525  Training Loss  0.0041697160340845585\n",
            "Epoch  11 Batch  259 / 525  Training Loss  0.013224269263446331\n",
            "Epoch  11 Batch  260 / 525  Training Loss  0.0113133629783988\n",
            "Epoch  11 Batch  261 / 525  Training Loss  0.007454524748027325\n",
            "Epoch  11 Batch  262 / 525  Training Loss  0.006540105678141117\n",
            "Epoch  11 Batch  263 / 525  Training Loss  0.011152506805956364\n",
            "Epoch  11 Batch  264 / 525  Training Loss  0.009562927298247814\n",
            "Epoch  11 Batch  265 / 525  Training Loss  0.014322933740913868\n",
            "Epoch  11 Batch  266 / 525  Training Loss  0.016430538147687912\n",
            "Epoch  11 Batch  267 / 525  Training Loss  0.011561469174921513\n",
            "Epoch  11 Batch  268 / 525  Training Loss  0.011183788068592548\n",
            "Epoch  11 Batch  269 / 525  Training Loss  0.010777455754578114\n",
            "Epoch  11 Batch  270 / 525  Training Loss  0.007321788463741541\n",
            "Epoch  11 Batch  271 / 525  Training Loss  0.010198880918323994\n",
            "Epoch  11 Batch  272 / 525  Training Loss  0.012840516865253448\n",
            "Epoch  11 Batch  273 / 525  Training Loss  0.007579682860523462\n",
            "Epoch  11 Batch  274 / 525  Training Loss  0.007001715246587992\n",
            "Epoch  11 Batch  275 / 525  Training Loss  0.009770759381353855\n",
            "Epoch  11 Batch  276 / 525  Training Loss  0.012345405295491219\n",
            "Epoch  11 Batch  277 / 525  Training Loss  0.014216860756278038\n",
            "Epoch  11 Batch  278 / 525  Training Loss  0.005686257965862751\n",
            "Epoch  11 Batch  279 / 525  Training Loss  0.01691875047981739\n",
            "Epoch  11 Batch  280 / 525  Training Loss  0.005097865127027035\n",
            "Epoch  11 Batch  281 / 525  Training Loss  0.009874858893454075\n",
            "Epoch  11 Batch  282 / 525  Training Loss  0.013530261814594269\n",
            "Epoch  11 Batch  283 / 525  Training Loss  0.013628462329506874\n",
            "Epoch  11 Batch  284 / 525  Training Loss  0.007035070098936558\n",
            "Epoch  11 Batch  285 / 525  Training Loss  0.013853894546627998\n",
            "Epoch  11 Batch  286 / 525  Training Loss  0.011777829378843307\n",
            "Epoch  11 Batch  287 / 525  Training Loss  0.00871090404689312\n",
            "Epoch  11 Batch  288 / 525  Training Loss  0.014823121950030327\n",
            "Epoch  11 Batch  289 / 525  Training Loss  0.0057546962052583694\n",
            "Epoch  11 Batch  290 / 525  Training Loss  0.02439439669251442\n",
            "Epoch  11 Batch  291 / 525  Training Loss  0.020155390724539757\n",
            "Epoch  11 Batch  292 / 525  Training Loss  0.009464019909501076\n",
            "Epoch  11 Batch  293 / 525  Training Loss  0.015331797301769257\n",
            "Epoch  11 Batch  294 / 525  Training Loss  0.014853064902126789\n",
            "Epoch  11 Batch  295 / 525  Training Loss  0.011897874996066093\n",
            "Epoch  11 Batch  296 / 525  Training Loss  0.008415086194872856\n",
            "Epoch  11 Batch  297 / 525  Training Loss  0.011663656681776047\n",
            "Epoch  11 Batch  298 / 525  Training Loss  0.00925679225474596\n",
            "Epoch  11 Batch  299 / 525  Training Loss  0.00793173722922802\n",
            "Epoch  11 Batch  300 / 525  Training Loss  0.012991215102374554\n",
            "Epoch  11 Batch  301 / 525  Training Loss  0.0087305698543787\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  11 Batch  302 / 525  Training Loss  0.011810907162725925\n",
            "Epoch  11 Batch  303 / 525  Training Loss  0.01176492590457201\n",
            "Epoch  11 Batch  304 / 525  Training Loss  0.012954835779964924\n",
            "Epoch  11 Batch  305 / 525  Training Loss  0.019275443628430367\n",
            "Epoch  11 Batch  306 / 525  Training Loss  0.007713410072028637\n",
            "Epoch  11 Batch  307 / 525  Training Loss  0.02620181441307068\n",
            "Epoch  11 Batch  308 / 525  Training Loss  0.01915924623608589\n",
            "Epoch  11 Batch  309 / 525  Training Loss  0.009398506954312325\n",
            "Epoch  11 Batch  310 / 525  Training Loss  0.013561452738940716\n",
            "Epoch  11 Batch  311 / 525  Training Loss  0.009470602497458458\n",
            "Epoch  11 Batch  312 / 525  Training Loss  0.01858195662498474\n",
            "Epoch  11 Batch  313 / 525  Training Loss  0.013057926669716835\n",
            "Epoch  11 Batch  314 / 525  Training Loss  0.015460041351616383\n",
            "Epoch  11 Batch  315 / 525  Training Loss  0.0095834881067276\n",
            "Epoch  11 Batch  316 / 525  Training Loss  0.018057988956570625\n",
            "Epoch  11 Batch  317 / 525  Training Loss  0.012589564546942711\n",
            "Epoch  11 Batch  318 / 525  Training Loss  0.01672457903623581\n",
            "Epoch  11 Batch  319 / 525  Training Loss  0.011412030085921288\n",
            "Epoch  11 Batch  320 / 525  Training Loss  0.019976967945694923\n",
            "Epoch  11 Batch  321 / 525  Training Loss  0.01486146729439497\n",
            "Epoch  11 Batch  322 / 525  Training Loss  0.009516274556517601\n",
            "Epoch  11 Batch  323 / 525  Training Loss  0.006169440690428019\n",
            "Epoch  11 Batch  324 / 525  Training Loss  0.01695866510272026\n",
            "Epoch  11 Batch  325 / 525  Training Loss  0.011846712790429592\n",
            "Epoch  11 Batch  326 / 525  Training Loss  0.010594825260341167\n",
            "Epoch  11 Batch  327 / 525  Training Loss  0.004524753428995609\n",
            "Epoch  11 Batch  328 / 525  Training Loss  0.007835447788238525\n",
            "Epoch  11 Batch  329 / 525  Training Loss  0.007528689689934254\n",
            "Epoch  11 Batch  330 / 525  Training Loss  0.01280977576971054\n",
            "Epoch  11 Batch  331 / 525  Training Loss  0.01297801174223423\n",
            "Epoch  11 Batch  332 / 525  Training Loss  0.009850675240159035\n",
            "Epoch  11 Batch  333 / 525  Training Loss  0.02132928930222988\n",
            "Epoch  11 Batch  334 / 525  Training Loss  0.01576787605881691\n",
            "Epoch  11 Batch  335 / 525  Training Loss  0.01048901304602623\n",
            "Epoch  11 Batch  336 / 525  Training Loss  0.008893834426999092\n",
            "Epoch  11 Batch  337 / 525  Training Loss  0.01073532272130251\n",
            "Epoch  11 Batch  338 / 525  Training Loss  0.009325176477432251\n",
            "Epoch  11 Batch  339 / 525  Training Loss  0.005694364197552204\n",
            "Epoch  11 Batch  340 / 525  Training Loss  0.005257961805909872\n",
            "Epoch  11 Batch  341 / 525  Training Loss  0.010179178789258003\n",
            "Epoch  11 Batch  342 / 525  Training Loss  0.021634483709931374\n",
            "Epoch  11 Batch  343 / 525  Training Loss  0.016522468999028206\n",
            "Epoch  11 Batch  344 / 525  Training Loss  0.012105749920010567\n",
            "Epoch  11 Batch  345 / 525  Training Loss  0.004730266984552145\n",
            "Epoch  11 Batch  346 / 525  Training Loss  0.014452708885073662\n",
            "Epoch  11 Batch  347 / 525  Training Loss  0.01804114505648613\n",
            "Epoch  11 Batch  348 / 525  Training Loss  0.011916249059140682\n",
            "Epoch  11 Batch  349 / 525  Training Loss  0.014901804737746716\n",
            "Epoch  11 Batch  350 / 525  Training Loss  0.011250344105064869\n",
            "Epoch  11 Batch  351 / 525  Training Loss  0.008041182532906532\n",
            "Epoch  11 Batch  352 / 525  Training Loss  0.014046852476894855\n",
            "Epoch  11 Batch  353 / 525  Training Loss  0.010743162594735622\n",
            "Epoch  11 Batch  354 / 525  Training Loss  0.007321344222873449\n",
            "Epoch  11 Batch  355 / 525  Training Loss  0.020073192194104195\n",
            "Epoch  11 Batch  356 / 525  Training Loss  0.006956385914236307\n",
            "Epoch  11 Batch  357 / 525  Training Loss  0.022880975157022476\n",
            "Epoch  11 Batch  358 / 525  Training Loss  0.011456058360636234\n",
            "Epoch  11 Batch  359 / 525  Training Loss  0.021176958456635475\n",
            "Epoch  11 Batch  360 / 525  Training Loss  0.007817758247256279\n",
            "Epoch  11 Batch  361 / 525  Training Loss  0.014328976161777973\n",
            "Epoch  11 Batch  362 / 525  Training Loss  0.011513822712004185\n",
            "Epoch  11 Batch  363 / 525  Training Loss  0.005795831326395273\n",
            "Epoch  11 Batch  364 / 525  Training Loss  0.010006261989474297\n",
            "Epoch  11 Batch  365 / 525  Training Loss  0.023591522127389908\n",
            "Epoch  11 Batch  366 / 525  Training Loss  0.007887233980000019\n",
            "Epoch  11 Batch  367 / 525  Training Loss  0.012167232111096382\n",
            "Epoch  11 Batch  368 / 525  Training Loss  0.014632867649197578\n",
            "Epoch  11 Batch  369 / 525  Training Loss  0.005756276194006205\n",
            "Epoch  11 Batch  370 / 525  Training Loss  0.015019334852695465\n",
            "Epoch  11 Batch  371 / 525  Training Loss  0.01328902505338192\n",
            "Epoch  11 Batch  372 / 525  Training Loss  0.0051873899064958096\n",
            "Epoch  11 Batch  373 / 525  Training Loss  0.019286416471004486\n",
            "Epoch  11 Batch  374 / 525  Training Loss  0.013516852632164955\n",
            "Epoch  11 Batch  375 / 525  Training Loss  0.010475472547113895\n",
            "Epoch  11 Batch  376 / 525  Training Loss  0.01351109892129898\n",
            "Epoch  11 Batch  377 / 525  Training Loss  0.005725750233978033\n",
            "Epoch  11 Batch  378 / 525  Training Loss  0.01762617938220501\n",
            "Epoch  11 Batch  379 / 525  Training Loss  0.004262811504304409\n",
            "Epoch  11 Batch  380 / 525  Training Loss  0.02254418656229973\n",
            "Epoch  11 Batch  381 / 525  Training Loss  0.01518846582621336\n",
            "Epoch  11 Batch  382 / 525  Training Loss  0.019222993403673172\n",
            "Epoch  11 Batch  383 / 525  Training Loss  0.009380199015140533\n",
            "Epoch  11 Batch  384 / 525  Training Loss  0.013512656092643738\n",
            "Epoch  11 Batch  385 / 525  Training Loss  0.017461301758885384\n",
            "Epoch  11 Batch  386 / 525  Training Loss  0.011255363002419472\n",
            "Epoch  11 Batch  387 / 525  Training Loss  0.009825164452195168\n",
            "Epoch  11 Batch  388 / 525  Training Loss  0.019428947940468788\n",
            "Epoch  11 Batch  389 / 525  Training Loss  0.022907186299562454\n",
            "Epoch  11 Batch  390 / 525  Training Loss  0.009473598562180996\n",
            "Epoch  11 Batch  391 / 525  Training Loss  0.010777389630675316\n",
            "Epoch  11 Batch  392 / 525  Training Loss  0.007617820054292679\n",
            "Epoch  11 Batch  393 / 525  Training Loss  0.010209867730736732\n",
            "Epoch  11 Batch  394 / 525  Training Loss  0.016309943050146103\n",
            "Epoch  11 Batch  395 / 525  Training Loss  0.007148814387619495\n",
            "Epoch  11 Batch  396 / 525  Training Loss  0.019833048805594444\n",
            "Epoch  11 Batch  397 / 525  Training Loss  0.008369524963200092\n",
            "Epoch  11 Batch  398 / 525  Training Loss  0.014667125418782234\n",
            "Epoch  11 Batch  399 / 525  Training Loss  0.020802559331059456\n",
            "Epoch  11 Batch  400 / 525  Training Loss  0.013455581851303577\n",
            "Epoch  11 Batch  401 / 525  Training Loss  0.012884353287518024\n",
            "Epoch  11 Batch  402 / 525  Training Loss  0.009239159524440765\n",
            "Epoch  11 Batch  403 / 525  Training Loss  0.009347307495772839\n",
            "Epoch  11 Batch  404 / 525  Training Loss  0.01083127036690712\n",
            "Epoch  11 Batch  405 / 525  Training Loss  0.014447115361690521\n",
            "Epoch  11 Batch  406 / 525  Training Loss  0.014638399705290794\n",
            "Epoch  11 Batch  407 / 525  Training Loss  0.03290441632270813\n",
            "Epoch  11 Batch  408 / 525  Training Loss  0.025696029886603355\n",
            "Epoch  11 Batch  409 / 525  Training Loss  0.01502789556980133\n",
            "Epoch  11 Batch  410 / 525  Training Loss  0.018846964463591576\n",
            "Epoch  11 Batch  411 / 525  Training Loss  0.008133573457598686\n",
            "Epoch  11 Batch  412 / 525  Training Loss  0.02439733035862446\n",
            "Epoch  11 Batch  413 / 525  Training Loss  0.0421636700630188\n",
            "Epoch  11 Batch  414 / 525  Training Loss  0.014167701825499535\n",
            "Epoch  11 Batch  415 / 525  Training Loss  0.010463561862707138\n",
            "Epoch  11 Batch  416 / 525  Training Loss  0.01083198469132185\n",
            "Epoch  11 Batch  417 / 525  Training Loss  0.0140871312469244\n",
            "Epoch  11 Batch  418 / 525  Training Loss  0.009092931635677814\n",
            "Epoch  11 Batch  419 / 525  Training Loss  0.00898929312825203\n",
            "Epoch  11 Batch  420 / 525  Training Loss  0.012859635055065155\n",
            "Epoch  11 Batch  421 / 525  Training Loss  0.015743914991617203\n",
            "Epoch  11 Batch  422 / 525  Training Loss  0.013775354251265526\n",
            "Epoch  11 Batch  423 / 525  Training Loss  0.014696645550429821\n",
            "Epoch  11 Batch  424 / 525  Training Loss  0.006237691268324852\n",
            "Epoch  11 Batch  425 / 525  Training Loss  0.014509981498122215\n",
            "Epoch  11 Batch  426 / 525  Training Loss  0.010250534862279892\n",
            "Epoch  11 Batch  427 / 525  Training Loss  0.005965153221040964\n",
            "Epoch  11 Batch  428 / 525  Training Loss  0.01364871859550476\n",
            "Epoch  11 Batch  429 / 525  Training Loss  0.012831578962504864\n",
            "Epoch  11 Batch  430 / 525  Training Loss  0.00739945936948061\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  11 Batch  431 / 525  Training Loss  0.01517525129020214\n",
            "Epoch  11 Batch  432 / 525  Training Loss  0.010677303187549114\n",
            "Epoch  11 Batch  433 / 525  Training Loss  0.005941770970821381\n",
            "Epoch  11 Batch  434 / 525  Training Loss  0.007863729260861874\n",
            "Epoch  11 Batch  435 / 525  Training Loss  0.014247926883399487\n",
            "Epoch  11 Batch  436 / 525  Training Loss  0.017898138612508774\n",
            "Epoch  11 Batch  437 / 525  Training Loss  0.011844521388411522\n",
            "Epoch  11 Batch  438 / 525  Training Loss  0.010827678255736828\n",
            "Epoch  11 Batch  439 / 525  Training Loss  0.005058246664702892\n",
            "Epoch  11 Batch  440 / 525  Training Loss  0.019069749861955643\n",
            "Epoch  11 Batch  441 / 525  Training Loss  0.013542267493903637\n",
            "Epoch  11 Batch  442 / 525  Training Loss  0.010200250893831253\n",
            "Epoch  11 Batch  443 / 525  Training Loss  0.011378467082977295\n",
            "Epoch  11 Batch  444 / 525  Training Loss  0.021651137620210648\n",
            "Epoch  11 Batch  445 / 525  Training Loss  0.02095867693424225\n",
            "Epoch  11 Batch  446 / 525  Training Loss  0.015771804377436638\n",
            "Epoch  11 Batch  447 / 525  Training Loss  0.013134809210896492\n",
            "Epoch  11 Batch  448 / 525  Training Loss  0.011450210586190224\n",
            "Epoch  11 Batch  449 / 525  Training Loss  0.0166371650993824\n",
            "Epoch  11 Batch  450 / 525  Training Loss  0.013755028136074543\n",
            "Epoch  11 Batch  451 / 525  Training Loss  0.012690240517258644\n",
            "Epoch  11 Batch  452 / 525  Training Loss  0.01187662873417139\n",
            "Epoch  11 Batch  453 / 525  Training Loss  0.020194020122289658\n",
            "Epoch  11 Batch  454 / 525  Training Loss  0.013487713411450386\n",
            "Epoch  11 Batch  455 / 525  Training Loss  0.005061347968876362\n",
            "Epoch  11 Batch  456 / 525  Training Loss  0.00975311454385519\n",
            "Epoch  11 Batch  457 / 525  Training Loss  0.00603765482082963\n",
            "Epoch  11 Batch  458 / 525  Training Loss  0.021916862577199936\n",
            "Epoch  11 Batch  459 / 525  Training Loss  0.021966202184557915\n",
            "Epoch  11 Batch  460 / 525  Training Loss  0.010976111516356468\n",
            "Epoch  11 Batch  461 / 525  Training Loss  0.006881976965814829\n",
            "Epoch  11 Batch  462 / 525  Training Loss  0.00778724392876029\n",
            "Epoch  11 Batch  463 / 525  Training Loss  0.016406942158937454\n",
            "Epoch  11 Batch  464 / 525  Training Loss  0.012403788045048714\n",
            "Epoch  11 Batch  465 / 525  Training Loss  0.021364660933613777\n",
            "Epoch  11 Batch  466 / 525  Training Loss  0.008003425784409046\n",
            "Epoch  11 Batch  467 / 525  Training Loss  0.013646120205521584\n",
            "Epoch  11 Batch  468 / 525  Training Loss  0.01600080356001854\n",
            "Epoch  11 Batch  469 / 525  Training Loss  0.01466621458530426\n",
            "Epoch  11 Batch  470 / 525  Training Loss  0.007265507243573666\n",
            "Epoch  11 Batch  471 / 525  Training Loss  0.01677265390753746\n",
            "Epoch  11 Batch  472 / 525  Training Loss  0.009530751034617424\n",
            "Epoch  11 Batch  473 / 525  Training Loss  0.015039989724755287\n",
            "Epoch  11 Batch  474 / 525  Training Loss  0.015718568116426468\n",
            "Epoch  11 Batch  475 / 525  Training Loss  0.01853252574801445\n",
            "Epoch  11 Batch  476 / 525  Training Loss  0.019363023340702057\n",
            "Epoch  11 Batch  477 / 525  Training Loss  0.008006389252841473\n",
            "Epoch  11 Batch  478 / 525  Training Loss  0.011105669662356377\n",
            "Epoch  11 Batch  479 / 525  Training Loss  0.014319075271487236\n",
            "Epoch  11 Batch  480 / 525  Training Loss  0.01040746457874775\n",
            "Epoch  11 Batch  481 / 525  Training Loss  0.014872057363390923\n",
            "Epoch  11 Batch  482 / 525  Training Loss  0.012771164067089558\n",
            "Epoch  11 Batch  483 / 525  Training Loss  0.025188054889440536\n",
            "Epoch  11 Batch  484 / 525  Training Loss  0.005866818595677614\n",
            "Epoch  11 Batch  485 / 525  Training Loss  0.013532070443034172\n",
            "Epoch  11 Batch  486 / 525  Training Loss  0.009423218667507172\n",
            "Epoch  11 Batch  487 / 525  Training Loss  0.019802136346697807\n",
            "Epoch  11 Batch  488 / 525  Training Loss  0.011207718402147293\n",
            "Epoch  11 Batch  489 / 525  Training Loss  0.011441050097346306\n",
            "Epoch  11 Batch  490 / 525  Training Loss  0.007901637814939022\n",
            "Epoch  11 Batch  491 / 525  Training Loss  0.018908638507127762\n",
            "Epoch  11 Batch  492 / 525  Training Loss  0.010202323086559772\n",
            "Epoch  11 Batch  493 / 525  Training Loss  0.019776402041316032\n",
            "Epoch  11 Batch  494 / 525  Training Loss  0.014764430932700634\n",
            "Epoch  11 Batch  495 / 525  Training Loss  0.01438023429363966\n",
            "Epoch  11 Batch  496 / 525  Training Loss  0.01085115410387516\n",
            "Epoch  11 Batch  497 / 525  Training Loss  0.014974355697631836\n",
            "Epoch  11 Batch  498 / 525  Training Loss  0.023022398352622986\n",
            "Epoch  11 Batch  499 / 525  Training Loss  0.029240652918815613\n",
            "Epoch  11 Batch  500 / 525  Training Loss  0.01410644967108965\n",
            "Epoch  11 Batch  501 / 525  Training Loss  0.012037857435643673\n",
            "Epoch  11 Batch  502 / 525  Training Loss  0.01621728017926216\n",
            "Epoch  11 Batch  503 / 525  Training Loss  0.01502664852887392\n",
            "Epoch  11 Batch  504 / 525  Training Loss  0.008121132850646973\n",
            "Epoch  11 Batch  505 / 525  Training Loss  0.019816434010863304\n",
            "Epoch  11 Batch  506 / 525  Training Loss  0.013054062612354755\n",
            "Epoch  11 Batch  507 / 525  Training Loss  0.014021920040249825\n",
            "Epoch  11 Batch  508 / 525  Training Loss  0.020676590502262115\n",
            "Epoch  11 Batch  509 / 525  Training Loss  0.02621161937713623\n",
            "Epoch  11 Batch  510 / 525  Training Loss  0.02551237866282463\n",
            "Epoch  11 Batch  511 / 525  Training Loss  0.01453797984868288\n",
            "Epoch  11 Batch  512 / 525  Training Loss  0.01422475092113018\n",
            "Epoch  11 Batch  513 / 525  Training Loss  0.011078565381467342\n",
            "Epoch  11 Batch  514 / 525  Training Loss  0.009491151198744774\n",
            "Epoch  11 Batch  515 / 525  Training Loss  0.012875348329544067\n",
            "Epoch  11 Batch  516 / 525  Training Loss  0.013235333375632763\n",
            "Epoch  11 Batch  517 / 525  Training Loss  0.01831788569688797\n",
            "Epoch  11 Batch  518 / 525  Training Loss  0.018086882308125496\n",
            "Epoch  11 Batch  519 / 525  Training Loss  0.013945767655968666\n",
            "Epoch  11 Batch  520 / 525  Training Loss  0.012931203469634056\n",
            "Epoch  11 Batch  521 / 525  Training Loss  0.010195175185799599\n",
            "Epoch  11 Batch  522 / 525  Training Loss  0.00921074952930212\n",
            "Epoch  11 Batch  523 / 525  Training Loss  0.014775523915886879\n",
            "Epoch  11 Batch  524 / 525  Training Loss  0.00795163307338953\n",
            "  12    |    -    |   0.011982   |   59.88  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 12\n",
            "Epoch  12 Batch  0 / 525  Training Loss  0.0029785202350467443\n",
            "Epoch  12 Batch  1 / 525  Training Loss  0.004168312065303326\n",
            "Epoch  12 Batch  2 / 525  Training Loss  0.0029774957802146673\n",
            "Epoch  12 Batch  3 / 525  Training Loss  0.007526657544076443\n",
            "Epoch  12 Batch  4 / 525  Training Loss  0.010122345760464668\n",
            "Epoch  12 Batch  5 / 525  Training Loss  0.00563741335645318\n",
            "Epoch  12 Batch  6 / 525  Training Loss  0.006547941826283932\n",
            "Epoch  12 Batch  7 / 525  Training Loss  0.00853384006768465\n",
            "Epoch  12 Batch  8 / 525  Training Loss  0.004104523453861475\n",
            "Epoch  12 Batch  9 / 525  Training Loss  0.008765196427702904\n",
            "Epoch  12 Batch  10 / 525  Training Loss  0.005392076447606087\n",
            "Epoch  12 Batch  11 / 525  Training Loss  0.0029549081809818745\n",
            "Epoch  12 Batch  12 / 525  Training Loss  0.006335997022688389\n",
            "Epoch  12 Batch  13 / 525  Training Loss  0.01077411137521267\n",
            "Epoch  12 Batch  14 / 525  Training Loss  0.00531753757968545\n",
            "Epoch  12 Batch  15 / 525  Training Loss  0.008523164317011833\n",
            "Epoch  12 Batch  16 / 525  Training Loss  0.009055333212018013\n",
            "Epoch  12 Batch  17 / 525  Training Loss  0.01087199617177248\n",
            "Epoch  12 Batch  18 / 525  Training Loss  0.0036234103608876467\n",
            "Epoch  12 Batch  19 / 525  Training Loss  0.00914011336863041\n",
            "Epoch  12 Batch  20 / 525  Training Loss  0.012819911353290081\n",
            "Epoch  12 Batch  21 / 525  Training Loss  0.01387258805334568\n",
            "Epoch  12 Batch  22 / 525  Training Loss  0.008296666666865349\n",
            "Epoch  12 Batch  23 / 525  Training Loss  0.006899893283843994\n",
            "Epoch  12 Batch  24 / 525  Training Loss  0.024296222254633904\n",
            "Epoch  12 Batch  25 / 525  Training Loss  0.0062429956160485744\n",
            "Epoch  12 Batch  26 / 525  Training Loss  0.010540524497628212\n",
            "Epoch  12 Batch  27 / 525  Training Loss  0.010545858182013035\n",
            "Epoch  12 Batch  28 / 525  Training Loss  0.006065854802727699\n",
            "Epoch  12 Batch  29 / 525  Training Loss  0.004989742301404476\n",
            "Epoch  12 Batch  30 / 525  Training Loss  0.004905040841549635\n",
            "Epoch  12 Batch  31 / 525  Training Loss  0.0033806574065238237\n",
            "Epoch  12 Batch  32 / 525  Training Loss  0.00526924105361104\n",
            "Epoch  12 Batch  33 / 525  Training Loss  0.004696747753769159\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  12 Batch  34 / 525  Training Loss  0.005489612463861704\n",
            "Epoch  12 Batch  35 / 525  Training Loss  0.007562694139778614\n",
            "Epoch  12 Batch  36 / 525  Training Loss  0.003819109173491597\n",
            "Epoch  12 Batch  37 / 525  Training Loss  0.007631073240190744\n",
            "Epoch  12 Batch  38 / 525  Training Loss  0.002377818338572979\n",
            "Epoch  12 Batch  39 / 525  Training Loss  0.004177448805421591\n",
            "Epoch  12 Batch  40 / 525  Training Loss  0.008087878115475178\n",
            "Epoch  12 Batch  41 / 525  Training Loss  0.008967282250523567\n",
            "Epoch  12 Batch  42 / 525  Training Loss  0.009668023325502872\n",
            "Epoch  12 Batch  43 / 525  Training Loss  0.013258447870612144\n",
            "Epoch  12 Batch  44 / 525  Training Loss  0.007170808967202902\n",
            "Epoch  12 Batch  45 / 525  Training Loss  0.00787634588778019\n",
            "Epoch  12 Batch  46 / 525  Training Loss  0.005400140769779682\n",
            "Epoch  12 Batch  47 / 525  Training Loss  0.006217072252184153\n",
            "Epoch  12 Batch  48 / 525  Training Loss  0.00790125411003828\n",
            "Epoch  12 Batch  49 / 525  Training Loss  0.007627869490534067\n",
            "Epoch  12 Batch  50 / 525  Training Loss  0.009385280311107635\n",
            "Epoch  12 Batch  51 / 525  Training Loss  0.004817637614905834\n",
            "Epoch  12 Batch  52 / 525  Training Loss  0.003921265713870525\n",
            "Epoch  12 Batch  53 / 525  Training Loss  0.009987537749111652\n",
            "Epoch  12 Batch  54 / 525  Training Loss  0.004899794701486826\n",
            "Epoch  12 Batch  55 / 525  Training Loss  0.005633861757814884\n",
            "Epoch  12 Batch  56 / 525  Training Loss  0.008306686766445637\n",
            "Epoch  12 Batch  57 / 525  Training Loss  0.007868692278862\n",
            "Epoch  12 Batch  58 / 525  Training Loss  0.007342210970818996\n",
            "Epoch  12 Batch  59 / 525  Training Loss  0.0041874973103404045\n",
            "Epoch  12 Batch  60 / 525  Training Loss  0.013694684021174908\n",
            "Epoch  12 Batch  61 / 525  Training Loss  0.01072015706449747\n",
            "Epoch  12 Batch  62 / 525  Training Loss  0.0071090878918766975\n",
            "Epoch  12 Batch  63 / 525  Training Loss  0.006173581816256046\n",
            "Epoch  12 Batch  64 / 525  Training Loss  0.006476539187133312\n",
            "Epoch  12 Batch  65 / 525  Training Loss  0.00648881122469902\n",
            "Epoch  12 Batch  66 / 525  Training Loss  0.005480102263391018\n",
            "Epoch  12 Batch  67 / 525  Training Loss  0.007152440492063761\n",
            "Epoch  12 Batch  68 / 525  Training Loss  0.005169665440917015\n",
            "Epoch  12 Batch  69 / 525  Training Loss  0.0076295011676847935\n",
            "Epoch  12 Batch  70 / 525  Training Loss  0.003500006627291441\n",
            "Epoch  12 Batch  71 / 525  Training Loss  0.003131888108327985\n",
            "Epoch  12 Batch  72 / 525  Training Loss  0.00473505724221468\n",
            "Epoch  12 Batch  73 / 525  Training Loss  0.01043024007230997\n",
            "Epoch  12 Batch  74 / 525  Training Loss  0.003726975992321968\n",
            "Epoch  12 Batch  75 / 525  Training Loss  0.0031715822406113148\n",
            "Epoch  12 Batch  76 / 525  Training Loss  0.011904096230864525\n",
            "Epoch  12 Batch  77 / 525  Training Loss  0.008681594394147396\n",
            "Epoch  12 Batch  78 / 525  Training Loss  0.0057848235592246056\n",
            "Epoch  12 Batch  79 / 525  Training Loss  0.0034259515814483166\n",
            "Epoch  12 Batch  80 / 525  Training Loss  0.004152005538344383\n",
            "Epoch  12 Batch  81 / 525  Training Loss  0.007974511943757534\n",
            "Epoch  12 Batch  82 / 525  Training Loss  0.0104470644146204\n",
            "Epoch  12 Batch  83 / 525  Training Loss  0.003508035559207201\n",
            "Epoch  12 Batch  84 / 525  Training Loss  0.006325867958366871\n",
            "Epoch  12 Batch  85 / 525  Training Loss  0.011480150744318962\n",
            "Epoch  12 Batch  86 / 525  Training Loss  0.006894601974636316\n",
            "Epoch  12 Batch  87 / 525  Training Loss  0.012462954968214035\n",
            "Epoch  12 Batch  88 / 525  Training Loss  0.013083177618682384\n",
            "Epoch  12 Batch  89 / 525  Training Loss  0.011258305981755257\n",
            "Epoch  12 Batch  90 / 525  Training Loss  0.008552595973014832\n",
            "Epoch  12 Batch  91 / 525  Training Loss  0.006543177179992199\n",
            "Epoch  12 Batch  92 / 525  Training Loss  0.011102522723376751\n",
            "Epoch  12 Batch  93 / 525  Training Loss  0.004768030252307653\n",
            "Epoch  12 Batch  94 / 525  Training Loss  0.013895896263420582\n",
            "Epoch  12 Batch  95 / 525  Training Loss  0.007634148932993412\n",
            "Epoch  12 Batch  96 / 525  Training Loss  0.012543268501758575\n",
            "Epoch  12 Batch  97 / 525  Training Loss  0.007623973302543163\n",
            "Epoch  12 Batch  98 / 525  Training Loss  0.009068837389349937\n",
            "Epoch  12 Batch  99 / 525  Training Loss  0.009134259074926376\n",
            "Epoch  12 Batch  100 / 525  Training Loss  0.005578203126788139\n",
            "Epoch  12 Batch  101 / 525  Training Loss  0.00557909719645977\n",
            "Epoch  12 Batch  102 / 525  Training Loss  0.006692452821880579\n",
            "Epoch  12 Batch  103 / 525  Training Loss  0.010023960843682289\n",
            "Epoch  12 Batch  104 / 525  Training Loss  0.004882791079580784\n",
            "Epoch  12 Batch  105 / 525  Training Loss  0.00496362429112196\n",
            "Epoch  12 Batch  106 / 525  Training Loss  0.005733106750994921\n",
            "Epoch  12 Batch  107 / 525  Training Loss  0.011952775530517101\n",
            "Epoch  12 Batch  108 / 525  Training Loss  0.003918474540114403\n",
            "Epoch  12 Batch  109 / 525  Training Loss  0.00688142841681838\n",
            "Epoch  12 Batch  110 / 525  Training Loss  0.004583211150020361\n",
            "Epoch  12 Batch  111 / 525  Training Loss  0.005168945994228125\n",
            "Epoch  12 Batch  112 / 525  Training Loss  0.005594893358647823\n",
            "Epoch  12 Batch  113 / 525  Training Loss  0.01107102632522583\n",
            "Epoch  12 Batch  114 / 525  Training Loss  0.007656174711883068\n",
            "Epoch  12 Batch  115 / 525  Training Loss  0.005503291264176369\n",
            "Epoch  12 Batch  116 / 525  Training Loss  0.008906785398721695\n",
            "Epoch  12 Batch  117 / 525  Training Loss  0.008042680099606514\n",
            "Epoch  12 Batch  118 / 525  Training Loss  0.004499941132962704\n",
            "Epoch  12 Batch  119 / 525  Training Loss  0.011663956567645073\n",
            "Epoch  12 Batch  120 / 525  Training Loss  0.004806508310139179\n",
            "Epoch  12 Batch  121 / 525  Training Loss  0.00503193773329258\n",
            "Epoch  12 Batch  122 / 525  Training Loss  0.006826078984886408\n",
            "Epoch  12 Batch  123 / 525  Training Loss  0.006918344646692276\n",
            "Epoch  12 Batch  124 / 525  Training Loss  0.004737274721264839\n",
            "Epoch  12 Batch  125 / 525  Training Loss  0.01910991594195366\n",
            "Epoch  12 Batch  126 / 525  Training Loss  0.006657144520431757\n",
            "Epoch  12 Batch  127 / 525  Training Loss  0.006968854460865259\n",
            "Epoch  12 Batch  128 / 525  Training Loss  0.004947796929627657\n",
            "Epoch  12 Batch  129 / 525  Training Loss  0.002841748995706439\n",
            "Epoch  12 Batch  130 / 525  Training Loss  0.011222705245018005\n",
            "Epoch  12 Batch  131 / 525  Training Loss  0.004791986662894487\n",
            "Epoch  12 Batch  132 / 525  Training Loss  0.003738198895007372\n",
            "Epoch  12 Batch  133 / 525  Training Loss  0.0033556583803147078\n",
            "Epoch  12 Batch  134 / 525  Training Loss  0.0074407621286809444\n",
            "Epoch  12 Batch  135 / 525  Training Loss  0.0025887941010296345\n",
            "Epoch  12 Batch  136 / 525  Training Loss  0.0030139542650431395\n",
            "Epoch  12 Batch  137 / 525  Training Loss  0.009016631171107292\n",
            "Epoch  12 Batch  138 / 525  Training Loss  0.012108273804187775\n",
            "Epoch  12 Batch  139 / 525  Training Loss  0.0047759003937244415\n",
            "Epoch  12 Batch  140 / 525  Training Loss  0.01743299886584282\n",
            "Epoch  12 Batch  141 / 525  Training Loss  0.009576622396707535\n",
            "Epoch  12 Batch  142 / 525  Training Loss  0.024298958480358124\n",
            "Epoch  12 Batch  143 / 525  Training Loss  0.0060219657607376575\n",
            "Epoch  12 Batch  144 / 525  Training Loss  0.005371750332415104\n",
            "Epoch  12 Batch  145 / 525  Training Loss  0.009492246434092522\n",
            "Epoch  12 Batch  146 / 525  Training Loss  0.009151611477136612\n",
            "Epoch  12 Batch  147 / 525  Training Loss  0.003191496478393674\n",
            "Epoch  12 Batch  148 / 525  Training Loss  0.007360158953815699\n",
            "Epoch  12 Batch  149 / 525  Training Loss  0.00643728394061327\n",
            "Epoch  12 Batch  150 / 525  Training Loss  0.011604007333517075\n",
            "Epoch  12 Batch  151 / 525  Training Loss  0.0040314337238669395\n",
            "Epoch  12 Batch  152 / 525  Training Loss  0.006290326360613108\n",
            "Epoch  12 Batch  153 / 525  Training Loss  0.011952743865549564\n",
            "Epoch  12 Batch  154 / 525  Training Loss  0.012055329978466034\n",
            "Epoch  12 Batch  155 / 525  Training Loss  0.003994298167526722\n",
            "Epoch  12 Batch  156 / 525  Training Loss  0.011934650130569935\n",
            "Epoch  12 Batch  157 / 525  Training Loss  0.004412578884512186\n",
            "Epoch  12 Batch  158 / 525  Training Loss  0.009007363580167294\n",
            "Epoch  12 Batch  159 / 525  Training Loss  0.008343325927853584\n",
            "Epoch  12 Batch  160 / 525  Training Loss  0.00473517831414938\n",
            "Epoch  12 Batch  161 / 525  Training Loss  0.003775989171117544\n",
            "Epoch  12 Batch  162 / 525  Training Loss  0.005079591181129217\n",
            "Epoch  12 Batch  163 / 525  Training Loss  0.0102658336982131\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  12 Batch  164 / 525  Training Loss  0.00816663820296526\n",
            "Epoch  12 Batch  165 / 525  Training Loss  0.005349761340767145\n",
            "Epoch  12 Batch  166 / 525  Training Loss  0.005628176033496857\n",
            "Epoch  12 Batch  167 / 525  Training Loss  0.004967543762177229\n",
            "Epoch  12 Batch  168 / 525  Training Loss  0.010459345765411854\n",
            "Epoch  12 Batch  169 / 525  Training Loss  0.0070524089969694614\n",
            "Epoch  12 Batch  170 / 525  Training Loss  0.012306811287999153\n",
            "Epoch  12 Batch  171 / 525  Training Loss  0.004858545958995819\n",
            "Epoch  12 Batch  172 / 525  Training Loss  0.00422266311943531\n",
            "Epoch  12 Batch  173 / 525  Training Loss  0.0033564369659870863\n",
            "Epoch  12 Batch  174 / 525  Training Loss  0.0044136373326182365\n",
            "Epoch  12 Batch  175 / 525  Training Loss  0.011830726638436317\n",
            "Epoch  12 Batch  176 / 525  Training Loss  0.0016145575791597366\n",
            "Epoch  12 Batch  177 / 525  Training Loss  0.007289988454431295\n",
            "Epoch  12 Batch  178 / 525  Training Loss  0.0068238042294979095\n",
            "Epoch  12 Batch  179 / 525  Training Loss  0.009462110698223114\n",
            "Epoch  12 Batch  180 / 525  Training Loss  0.006122105289250612\n",
            "Epoch  12 Batch  181 / 525  Training Loss  0.008609535172581673\n",
            "Epoch  12 Batch  182 / 525  Training Loss  0.009030641987919807\n",
            "Epoch  12 Batch  183 / 525  Training Loss  0.009444505907595158\n",
            "Epoch  12 Batch  184 / 525  Training Loss  0.005751037038862705\n",
            "Epoch  12 Batch  185 / 525  Training Loss  0.023550700396299362\n",
            "Epoch  12 Batch  186 / 525  Training Loss  0.004665600135922432\n",
            "Epoch  12 Batch  187 / 525  Training Loss  0.007900895550847054\n",
            "Epoch  12 Batch  188 / 525  Training Loss  0.010721144266426563\n",
            "Epoch  12 Batch  189 / 525  Training Loss  0.005614742636680603\n",
            "Epoch  12 Batch  190 / 525  Training Loss  0.0033046393655240536\n",
            "Epoch  12 Batch  191 / 525  Training Loss  0.002991200890392065\n",
            "Epoch  12 Batch  192 / 525  Training Loss  0.004220533650368452\n",
            "Epoch  12 Batch  193 / 525  Training Loss  0.004076747689396143\n",
            "Epoch  12 Batch  194 / 525  Training Loss  0.008563566952943802\n",
            "Epoch  12 Batch  195 / 525  Training Loss  0.008990188129246235\n",
            "Epoch  12 Batch  196 / 525  Training Loss  0.007024525664746761\n",
            "Epoch  12 Batch  197 / 525  Training Loss  0.013818157836794853\n",
            "Epoch  12 Batch  198 / 525  Training Loss  0.0048716203309595585\n",
            "Epoch  12 Batch  199 / 525  Training Loss  0.0075261881574988365\n",
            "Epoch  12 Batch  200 / 525  Training Loss  0.004963219165802002\n",
            "Epoch  12 Batch  201 / 525  Training Loss  0.003959782421588898\n",
            "Epoch  12 Batch  202 / 525  Training Loss  0.006985320243984461\n",
            "Epoch  12 Batch  203 / 525  Training Loss  0.01975451596081257\n",
            "Epoch  12 Batch  204 / 525  Training Loss  0.004229063168168068\n",
            "Epoch  12 Batch  205 / 525  Training Loss  0.004558605141937733\n",
            "Epoch  12 Batch  206 / 525  Training Loss  0.004645645618438721\n",
            "Epoch  12 Batch  207 / 525  Training Loss  0.0030919527634978294\n",
            "Epoch  12 Batch  208 / 525  Training Loss  0.006883329711854458\n",
            "Epoch  12 Batch  209 / 525  Training Loss  0.005129693541675806\n",
            "Epoch  12 Batch  210 / 525  Training Loss  0.012697207741439342\n",
            "Epoch  12 Batch  211 / 525  Training Loss  0.0032658218406140804\n",
            "Epoch  12 Batch  212 / 525  Training Loss  0.015200652182102203\n",
            "Epoch  12 Batch  213 / 525  Training Loss  0.009167180396616459\n",
            "Epoch  12 Batch  214 / 525  Training Loss  0.00823531486093998\n",
            "Epoch  12 Batch  215 / 525  Training Loss  0.005794314667582512\n",
            "Epoch  12 Batch  216 / 525  Training Loss  0.008279641158878803\n",
            "Epoch  12 Batch  217 / 525  Training Loss  0.010701407678425312\n",
            "Epoch  12 Batch  218 / 525  Training Loss  0.005146941635757685\n",
            "Epoch  12 Batch  219 / 525  Training Loss  0.011012429371476173\n",
            "Epoch  12 Batch  220 / 525  Training Loss  0.00965743139386177\n",
            "Epoch  12 Batch  221 / 525  Training Loss  0.005823751445859671\n",
            "Epoch  12 Batch  222 / 525  Training Loss  0.004826545249670744\n",
            "Epoch  12 Batch  223 / 525  Training Loss  0.0037506811786442995\n",
            "Epoch  12 Batch  224 / 525  Training Loss  0.004596597980707884\n",
            "Epoch  12 Batch  225 / 525  Training Loss  0.00942993350327015\n",
            "Epoch  12 Batch  226 / 525  Training Loss  0.028017649427056313\n",
            "Epoch  12 Batch  227 / 525  Training Loss  0.012362961657345295\n",
            "Epoch  12 Batch  228 / 525  Training Loss  0.005330023355782032\n",
            "Epoch  12 Batch  229 / 525  Training Loss  0.015160498209297657\n",
            "Epoch  12 Batch  230 / 525  Training Loss  0.00541294738650322\n",
            "Epoch  12 Batch  231 / 525  Training Loss  0.008765952661633492\n",
            "Epoch  12 Batch  232 / 525  Training Loss  0.01093992218375206\n",
            "Epoch  12 Batch  233 / 525  Training Loss  0.015135998837649822\n",
            "Epoch  12 Batch  234 / 525  Training Loss  0.005383793730288744\n",
            "Epoch  12 Batch  235 / 525  Training Loss  0.005711530800908804\n",
            "Epoch  12 Batch  236 / 525  Training Loss  0.013113275170326233\n",
            "Epoch  12 Batch  237 / 525  Training Loss  0.006455676164478064\n",
            "Epoch  12 Batch  238 / 525  Training Loss  0.0070198229514062405\n",
            "Epoch  12 Batch  239 / 525  Training Loss  0.004247843753546476\n",
            "Epoch  12 Batch  240 / 525  Training Loss  0.005659198854118586\n",
            "Epoch  12 Batch  241 / 525  Training Loss  0.004943304229527712\n",
            "Epoch  12 Batch  242 / 525  Training Loss  0.015564242377877235\n",
            "Epoch  12 Batch  243 / 525  Training Loss  0.019936976954340935\n",
            "Epoch  12 Batch  244 / 525  Training Loss  0.031224718317389488\n",
            "Epoch  12 Batch  245 / 525  Training Loss  0.007639446761459112\n",
            "Epoch  12 Batch  246 / 525  Training Loss  0.0077195921912789345\n",
            "Epoch  12 Batch  247 / 525  Training Loss  0.007501831743866205\n",
            "Epoch  12 Batch  248 / 525  Training Loss  0.00763900950551033\n",
            "Epoch  12 Batch  249 / 525  Training Loss  0.0069615826942026615\n",
            "Epoch  12 Batch  250 / 525  Training Loss  0.005801171995699406\n",
            "Epoch  12 Batch  251 / 525  Training Loss  0.00619963463395834\n",
            "Epoch  12 Batch  252 / 525  Training Loss  0.008187574334442616\n",
            "Epoch  12 Batch  253 / 525  Training Loss  0.0030199300963431597\n",
            "Epoch  12 Batch  254 / 525  Training Loss  0.009401796385645866\n",
            "Epoch  12 Batch  255 / 525  Training Loss  0.0036115734837949276\n",
            "Epoch  12 Batch  256 / 525  Training Loss  0.00610602181404829\n",
            "Epoch  12 Batch  257 / 525  Training Loss  0.005364221055060625\n",
            "Epoch  12 Batch  258 / 525  Training Loss  0.010102692060172558\n",
            "Epoch  12 Batch  259 / 525  Training Loss  0.009978100657463074\n",
            "Epoch  12 Batch  260 / 525  Training Loss  0.007720620837062597\n",
            "Epoch  12 Batch  261 / 525  Training Loss  0.005230114795267582\n",
            "Epoch  12 Batch  262 / 525  Training Loss  0.004704535007476807\n",
            "Epoch  12 Batch  263 / 525  Training Loss  0.01649477519094944\n",
            "Epoch  12 Batch  264 / 525  Training Loss  0.005091998726129532\n",
            "Epoch  12 Batch  265 / 525  Training Loss  0.004473517183214426\n",
            "Epoch  12 Batch  266 / 525  Training Loss  0.0030569129157811403\n",
            "Epoch  12 Batch  267 / 525  Training Loss  0.005709056276828051\n",
            "Epoch  12 Batch  268 / 525  Training Loss  0.004819197114557028\n",
            "Epoch  12 Batch  269 / 525  Training Loss  0.005756398197263479\n",
            "Epoch  12 Batch  270 / 525  Training Loss  0.006076616235077381\n",
            "Epoch  12 Batch  271 / 525  Training Loss  0.007121149450540543\n",
            "Epoch  12 Batch  272 / 525  Training Loss  0.00620304374024272\n",
            "Epoch  12 Batch  273 / 525  Training Loss  0.00608451385051012\n",
            "Epoch  12 Batch  274 / 525  Training Loss  0.007823126390576363\n",
            "Epoch  12 Batch  275 / 525  Training Loss  0.005143835674971342\n",
            "Epoch  12 Batch  276 / 525  Training Loss  0.004012807738035917\n",
            "Epoch  12 Batch  277 / 525  Training Loss  0.0075956666842103004\n",
            "Epoch  12 Batch  278 / 525  Training Loss  0.00836123339831829\n",
            "Epoch  12 Batch  279 / 525  Training Loss  0.009811919182538986\n",
            "Epoch  12 Batch  280 / 525  Training Loss  0.013010555878281593\n",
            "Epoch  12 Batch  281 / 525  Training Loss  0.007210421375930309\n",
            "Epoch  12 Batch  282 / 525  Training Loss  0.00865623913705349\n",
            "Epoch  12 Batch  283 / 525  Training Loss  0.005929230712354183\n",
            "Epoch  12 Batch  284 / 525  Training Loss  0.0038660974241793156\n",
            "Epoch  12 Batch  285 / 525  Training Loss  0.004410553723573685\n",
            "Epoch  12 Batch  286 / 525  Training Loss  0.011999035254120827\n",
            "Epoch  12 Batch  287 / 525  Training Loss  0.0040214778855443\n",
            "Epoch  12 Batch  288 / 525  Training Loss  0.008010895922780037\n",
            "Epoch  12 Batch  289 / 525  Training Loss  0.018733566626906395\n",
            "Epoch  12 Batch  290 / 525  Training Loss  0.005147878546267748\n",
            "Epoch  12 Batch  291 / 525  Training Loss  0.0061332653276622295\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  12 Batch  292 / 525  Training Loss  0.007998871617019176\n",
            "Epoch  12 Batch  293 / 525  Training Loss  0.00315664941444993\n",
            "Epoch  12 Batch  294 / 525  Training Loss  0.009280730970203876\n",
            "Epoch  12 Batch  295 / 525  Training Loss  0.010474343784153461\n",
            "Epoch  12 Batch  296 / 525  Training Loss  0.006448051426559687\n",
            "Epoch  12 Batch  297 / 525  Training Loss  0.0060354601591825485\n",
            "Epoch  12 Batch  298 / 525  Training Loss  0.004272031597793102\n",
            "Epoch  12 Batch  299 / 525  Training Loss  0.01008525863289833\n",
            "Epoch  12 Batch  300 / 525  Training Loss  0.004666185472160578\n",
            "Epoch  12 Batch  301 / 525  Training Loss  0.013118232600390911\n",
            "Epoch  12 Batch  302 / 525  Training Loss  0.007770378142595291\n",
            "Epoch  12 Batch  303 / 525  Training Loss  0.004717306233942509\n",
            "Epoch  12 Batch  304 / 525  Training Loss  0.007001889403909445\n",
            "Epoch  12 Batch  305 / 525  Training Loss  0.004419566132128239\n",
            "Epoch  12 Batch  306 / 525  Training Loss  0.01788792386651039\n",
            "Epoch  12 Batch  307 / 525  Training Loss  0.0029752380214631557\n",
            "Epoch  12 Batch  308 / 525  Training Loss  0.0022981122601777315\n",
            "Epoch  12 Batch  309 / 525  Training Loss  0.011804113164544106\n",
            "Epoch  12 Batch  310 / 525  Training Loss  0.0050785779021680355\n",
            "Epoch  12 Batch  311 / 525  Training Loss  0.006295177154242992\n",
            "Epoch  12 Batch  312 / 525  Training Loss  0.010332437232136726\n",
            "Epoch  12 Batch  313 / 525  Training Loss  0.007441314868628979\n",
            "Epoch  12 Batch  314 / 525  Training Loss  0.004664133302867413\n",
            "Epoch  12 Batch  315 / 525  Training Loss  0.010186344385147095\n",
            "Epoch  12 Batch  316 / 525  Training Loss  0.004380938597023487\n",
            "Epoch  12 Batch  317 / 525  Training Loss  0.0064152502454817295\n",
            "Epoch  12 Batch  318 / 525  Training Loss  0.0034846649505198\n",
            "Epoch  12 Batch  319 / 525  Training Loss  0.008072157390415668\n",
            "Epoch  12 Batch  320 / 525  Training Loss  0.006234161555767059\n",
            "Epoch  12 Batch  321 / 525  Training Loss  0.005718614906072617\n",
            "Epoch  12 Batch  322 / 525  Training Loss  0.0065984902903437614\n",
            "Epoch  12 Batch  323 / 525  Training Loss  0.006837162189185619\n",
            "Epoch  12 Batch  324 / 525  Training Loss  0.006005118601024151\n",
            "Epoch  12 Batch  325 / 525  Training Loss  0.010537861846387386\n",
            "Epoch  12 Batch  326 / 525  Training Loss  0.015725398436188698\n",
            "Epoch  12 Batch  327 / 525  Training Loss  0.01805220916867256\n",
            "Epoch  12 Batch  328 / 525  Training Loss  0.009172177873551846\n",
            "Epoch  12 Batch  329 / 525  Training Loss  0.008275347761809826\n",
            "Epoch  12 Batch  330 / 525  Training Loss  0.011912086978554726\n",
            "Epoch  12 Batch  331 / 525  Training Loss  0.00916394405066967\n",
            "Epoch  12 Batch  332 / 525  Training Loss  0.015225263312458992\n",
            "Epoch  12 Batch  333 / 525  Training Loss  0.0041191717609763145\n",
            "Epoch  12 Batch  334 / 525  Training Loss  0.014595657587051392\n",
            "Epoch  12 Batch  335 / 525  Training Loss  0.00817149505019188\n",
            "Epoch  12 Batch  336 / 525  Training Loss  0.0074960337951779366\n",
            "Epoch  12 Batch  337 / 525  Training Loss  0.0070982398465275764\n",
            "Epoch  12 Batch  338 / 525  Training Loss  0.006896115839481354\n",
            "Epoch  12 Batch  339 / 525  Training Loss  0.008115170523524284\n",
            "Epoch  12 Batch  340 / 525  Training Loss  0.011486219242215157\n",
            "Epoch  12 Batch  341 / 525  Training Loss  0.0031359661370515823\n",
            "Epoch  12 Batch  342 / 525  Training Loss  0.006452404893934727\n",
            "Epoch  12 Batch  343 / 525  Training Loss  0.01212781947106123\n",
            "Epoch  12 Batch  344 / 525  Training Loss  0.007883353158831596\n",
            "Epoch  12 Batch  345 / 525  Training Loss  0.005309416446834803\n",
            "Epoch  12 Batch  346 / 525  Training Loss  0.003652028040960431\n",
            "Epoch  12 Batch  347 / 525  Training Loss  0.009302064776420593\n",
            "Epoch  12 Batch  348 / 525  Training Loss  0.010498909279704094\n",
            "Epoch  12 Batch  349 / 525  Training Loss  0.012837084010243416\n",
            "Epoch  12 Batch  350 / 525  Training Loss  0.009816320613026619\n",
            "Epoch  12 Batch  351 / 525  Training Loss  0.005589581094682217\n",
            "Epoch  12 Batch  352 / 525  Training Loss  0.003086684737354517\n",
            "Epoch  12 Batch  353 / 525  Training Loss  0.021021734923124313\n",
            "Epoch  12 Batch  354 / 525  Training Loss  0.02316443994641304\n",
            "Epoch  12 Batch  355 / 525  Training Loss  0.014257708564400673\n",
            "Epoch  12 Batch  356 / 525  Training Loss  0.01081882044672966\n",
            "Epoch  12 Batch  357 / 525  Training Loss  0.004089165013283491\n",
            "Epoch  12 Batch  358 / 525  Training Loss  0.01709074154496193\n",
            "Epoch  12 Batch  359 / 525  Training Loss  0.00618342962116003\n",
            "Epoch  12 Batch  360 / 525  Training Loss  0.007862198166549206\n",
            "Epoch  12 Batch  361 / 525  Training Loss  0.009858833625912666\n",
            "Epoch  12 Batch  362 / 525  Training Loss  0.00705978786572814\n",
            "Epoch  12 Batch  363 / 525  Training Loss  0.01066178921610117\n",
            "Epoch  12 Batch  364 / 525  Training Loss  0.006929104682058096\n",
            "Epoch  12 Batch  365 / 525  Training Loss  0.003813011571764946\n",
            "Epoch  12 Batch  366 / 525  Training Loss  0.011357222683727741\n",
            "Epoch  12 Batch  367 / 525  Training Loss  0.009966407902538776\n",
            "Epoch  12 Batch  368 / 525  Training Loss  0.010551787912845612\n",
            "Epoch  12 Batch  369 / 525  Training Loss  0.010889440774917603\n",
            "Epoch  12 Batch  370 / 525  Training Loss  0.005158301908522844\n",
            "Epoch  12 Batch  371 / 525  Training Loss  0.004032259341329336\n",
            "Epoch  12 Batch  372 / 525  Training Loss  0.0069778175093233585\n",
            "Epoch  12 Batch  373 / 525  Training Loss  0.003889328334480524\n",
            "Epoch  12 Batch  374 / 525  Training Loss  0.02381523698568344\n",
            "Epoch  12 Batch  375 / 525  Training Loss  0.0049625723622739315\n",
            "Epoch  12 Batch  376 / 525  Training Loss  0.017094029113650322\n",
            "Epoch  12 Batch  377 / 525  Training Loss  0.01866140030324459\n",
            "Epoch  12 Batch  378 / 525  Training Loss  0.01375317107886076\n",
            "Epoch  12 Batch  379 / 525  Training Loss  0.008528925478458405\n",
            "Epoch  12 Batch  380 / 525  Training Loss  0.014788158237934113\n",
            "Epoch  12 Batch  381 / 525  Training Loss  0.0055475723929703236\n",
            "Epoch  12 Batch  382 / 525  Training Loss  0.0045784548856318\n",
            "Epoch  12 Batch  383 / 525  Training Loss  0.012663054279983044\n",
            "Epoch  12 Batch  384 / 525  Training Loss  0.009487862698733807\n",
            "Epoch  12 Batch  385 / 525  Training Loss  0.006738494150340557\n",
            "Epoch  12 Batch  386 / 525  Training Loss  0.0066245729103684425\n",
            "Epoch  12 Batch  387 / 525  Training Loss  0.012978081591427326\n",
            "Epoch  12 Batch  388 / 525  Training Loss  0.004930232185870409\n",
            "Epoch  12 Batch  389 / 525  Training Loss  0.006857029162347317\n",
            "Epoch  12 Batch  390 / 525  Training Loss  0.011135418899357319\n",
            "Epoch  12 Batch  391 / 525  Training Loss  0.0031068117823451757\n",
            "Epoch  12 Batch  392 / 525  Training Loss  0.0036959226708859205\n",
            "Epoch  12 Batch  393 / 525  Training Loss  0.006363643798977137\n",
            "Epoch  12 Batch  394 / 525  Training Loss  0.0068834396079182625\n",
            "Epoch  12 Batch  395 / 525  Training Loss  0.007634109817445278\n",
            "Epoch  12 Batch  396 / 525  Training Loss  0.022179197520017624\n",
            "Epoch  12 Batch  397 / 525  Training Loss  0.00977399293333292\n",
            "Epoch  12 Batch  398 / 525  Training Loss  0.010014868341386318\n",
            "Epoch  12 Batch  399 / 525  Training Loss  0.013762682676315308\n",
            "Epoch  12 Batch  400 / 525  Training Loss  0.024561777710914612\n",
            "Epoch  12 Batch  401 / 525  Training Loss  0.005767827853560448\n",
            "Epoch  12 Batch  402 / 525  Training Loss  0.00899259652942419\n",
            "Epoch  12 Batch  403 / 525  Training Loss  0.005696686450392008\n",
            "Epoch  12 Batch  404 / 525  Training Loss  0.007099139038473368\n",
            "Epoch  12 Batch  405 / 525  Training Loss  0.009353796020150185\n",
            "Epoch  12 Batch  406 / 525  Training Loss  0.01563987508416176\n",
            "Epoch  12 Batch  407 / 525  Training Loss  0.021182123571634293\n",
            "Epoch  12 Batch  408 / 525  Training Loss  0.002184107666835189\n",
            "Epoch  12 Batch  409 / 525  Training Loss  0.006844266317784786\n",
            "Epoch  12 Batch  410 / 525  Training Loss  0.003993768244981766\n",
            "Epoch  12 Batch  411 / 525  Training Loss  0.00834202952682972\n",
            "Epoch  12 Batch  412 / 525  Training Loss  0.009611183777451515\n",
            "Epoch  12 Batch  413 / 525  Training Loss  0.005539447069168091\n",
            "Epoch  12 Batch  414 / 525  Training Loss  0.0067271823063492775\n",
            "Epoch  12 Batch  415 / 525  Training Loss  0.00519768288359046\n",
            "Epoch  12 Batch  416 / 525  Training Loss  0.006848891731351614\n",
            "Epoch  12 Batch  417 / 525  Training Loss  0.006822997238487005\n",
            "Epoch  12 Batch  418 / 525  Training Loss  0.005909629166126251\n",
            "Epoch  12 Batch  419 / 525  Training Loss  0.009007373824715614\n",
            "Epoch  12 Batch  420 / 525  Training Loss  0.012851771898567677\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  12 Batch  421 / 525  Training Loss  0.009939624927937984\n",
            "Epoch  12 Batch  422 / 525  Training Loss  0.02503359690308571\n",
            "Epoch  12 Batch  423 / 525  Training Loss  0.010056142695248127\n",
            "Epoch  12 Batch  424 / 525  Training Loss  0.00806369073688984\n",
            "Epoch  12 Batch  425 / 525  Training Loss  0.003908342681825161\n",
            "Epoch  12 Batch  426 / 525  Training Loss  0.007578056305646896\n",
            "Epoch  12 Batch  427 / 525  Training Loss  0.004971861839294434\n",
            "Epoch  12 Batch  428 / 525  Training Loss  0.004851596895605326\n",
            "Epoch  12 Batch  429 / 525  Training Loss  0.008299685083329678\n",
            "Epoch  12 Batch  430 / 525  Training Loss  0.01370104681700468\n",
            "Epoch  12 Batch  431 / 525  Training Loss  0.013043532148003578\n",
            "Epoch  12 Batch  432 / 525  Training Loss  0.005716107785701752\n",
            "Epoch  12 Batch  433 / 525  Training Loss  0.010454121977090836\n",
            "Epoch  12 Batch  434 / 525  Training Loss  0.0032813686411827803\n",
            "Epoch  12 Batch  435 / 525  Training Loss  0.005244845524430275\n",
            "Epoch  12 Batch  436 / 525  Training Loss  0.00454335194081068\n",
            "Epoch  12 Batch  437 / 525  Training Loss  0.0030583753250539303\n",
            "Epoch  12 Batch  438 / 525  Training Loss  0.017730584368109703\n",
            "Epoch  12 Batch  439 / 525  Training Loss  0.016197245568037033\n",
            "Epoch  12 Batch  440 / 525  Training Loss  0.009741436690092087\n",
            "Epoch  12 Batch  441 / 525  Training Loss  0.007437096443027258\n",
            "Epoch  12 Batch  442 / 525  Training Loss  0.004622706212103367\n",
            "Epoch  12 Batch  443 / 525  Training Loss  0.0043954369612038136\n",
            "Epoch  12 Batch  444 / 525  Training Loss  0.011838250793516636\n",
            "Epoch  12 Batch  445 / 525  Training Loss  0.00851151067763567\n",
            "Epoch  12 Batch  446 / 525  Training Loss  0.005421020090579987\n",
            "Epoch  12 Batch  447 / 525  Training Loss  0.006429161876440048\n",
            "Epoch  12 Batch  448 / 525  Training Loss  0.013729900121688843\n",
            "Epoch  12 Batch  449 / 525  Training Loss  0.005311360117048025\n",
            "Epoch  12 Batch  450 / 525  Training Loss  0.013030852191150188\n",
            "Epoch  12 Batch  451 / 525  Training Loss  0.025578398257493973\n",
            "Epoch  12 Batch  452 / 525  Training Loss  0.0032434421591460705\n",
            "Epoch  12 Batch  453 / 525  Training Loss  0.016028540208935738\n",
            "Epoch  12 Batch  454 / 525  Training Loss  0.010571882128715515\n",
            "Epoch  12 Batch  455 / 525  Training Loss  0.010621463879942894\n",
            "Epoch  12 Batch  456 / 525  Training Loss  0.007289194501936436\n",
            "Epoch  12 Batch  457 / 525  Training Loss  0.019479701295495033\n",
            "Epoch  12 Batch  458 / 525  Training Loss  0.008780762553215027\n",
            "Epoch  12 Batch  459 / 525  Training Loss  0.003506498411297798\n",
            "Epoch  12 Batch  460 / 525  Training Loss  0.007219740189611912\n",
            "Epoch  12 Batch  461 / 525  Training Loss  0.010135343298316002\n",
            "Epoch  12 Batch  462 / 525  Training Loss  0.003174601122736931\n",
            "Epoch  12 Batch  463 / 525  Training Loss  0.011450392194092274\n",
            "Epoch  12 Batch  464 / 525  Training Loss  0.010959908366203308\n",
            "Epoch  12 Batch  465 / 525  Training Loss  0.009068797342479229\n",
            "Epoch  12 Batch  466 / 525  Training Loss  0.017568515613675117\n",
            "Epoch  12 Batch  467 / 525  Training Loss  0.013019450008869171\n",
            "Epoch  12 Batch  468 / 525  Training Loss  0.004463876597583294\n",
            "Epoch  12 Batch  469 / 525  Training Loss  0.012616132386028767\n",
            "Epoch  12 Batch  470 / 525  Training Loss  0.009265313856303692\n",
            "Epoch  12 Batch  471 / 525  Training Loss  0.007233174983412027\n",
            "Epoch  12 Batch  472 / 525  Training Loss  0.020810356363654137\n",
            "Epoch  12 Batch  473 / 525  Training Loss  0.01767442561686039\n",
            "Epoch  12 Batch  474 / 525  Training Loss  0.004595477133989334\n",
            "Epoch  12 Batch  475 / 525  Training Loss  0.012206868268549442\n",
            "Epoch  12 Batch  476 / 525  Training Loss  0.003517144825309515\n",
            "Epoch  12 Batch  477 / 525  Training Loss  0.013827046379446983\n",
            "Epoch  12 Batch  478 / 525  Training Loss  0.01733672246336937\n",
            "Epoch  12 Batch  479 / 525  Training Loss  0.012155434116721153\n",
            "Epoch  12 Batch  480 / 525  Training Loss  0.006925337016582489\n",
            "Epoch  12 Batch  481 / 525  Training Loss  0.011774159967899323\n",
            "Epoch  12 Batch  482 / 525  Training Loss  0.009577418677508831\n",
            "Epoch  12 Batch  483 / 525  Training Loss  0.0132228909060359\n",
            "Epoch  12 Batch  484 / 525  Training Loss  0.013198399916291237\n",
            "Epoch  12 Batch  485 / 525  Training Loss  0.01956748589873314\n",
            "Epoch  12 Batch  486 / 525  Training Loss  0.00548290042206645\n",
            "Epoch  12 Batch  487 / 525  Training Loss  0.01439319271594286\n",
            "Epoch  12 Batch  488 / 525  Training Loss  0.004492250736802816\n",
            "Epoch  12 Batch  489 / 525  Training Loss  0.008467811159789562\n",
            "Epoch  12 Batch  490 / 525  Training Loss  0.006239843554794788\n",
            "Epoch  12 Batch  491 / 525  Training Loss  0.00493312394246459\n",
            "Epoch  12 Batch  492 / 525  Training Loss  0.005417509935796261\n",
            "Epoch  12 Batch  493 / 525  Training Loss  0.006705784238874912\n",
            "Epoch  12 Batch  494 / 525  Training Loss  0.0034766378812491894\n",
            "Epoch  12 Batch  495 / 525  Training Loss  0.008316419087350368\n",
            "Epoch  12 Batch  496 / 525  Training Loss  0.0040527647361159325\n",
            "Epoch  12 Batch  497 / 525  Training Loss  0.0057483939453959465\n",
            "Epoch  12 Batch  498 / 525  Training Loss  0.007015937473624945\n",
            "Epoch  12 Batch  499 / 525  Training Loss  0.011121029034256935\n",
            "Epoch  12 Batch  500 / 525  Training Loss  0.00966669898480177\n",
            "Epoch  12 Batch  501 / 525  Training Loss  0.011226414702832699\n",
            "Epoch  12 Batch  502 / 525  Training Loss  0.006700551602989435\n",
            "Epoch  12 Batch  503 / 525  Training Loss  0.007284379098564386\n",
            "Epoch  12 Batch  504 / 525  Training Loss  0.007552056107670069\n",
            "Epoch  12 Batch  505 / 525  Training Loss  0.010210651904344559\n",
            "Epoch  12 Batch  506 / 525  Training Loss  0.014465687796473503\n",
            "Epoch  12 Batch  507 / 525  Training Loss  0.010435839183628559\n",
            "Epoch  12 Batch  508 / 525  Training Loss  0.006933396216481924\n",
            "Epoch  12 Batch  509 / 525  Training Loss  0.004746674094349146\n",
            "Epoch  12 Batch  510 / 525  Training Loss  0.004969837609678507\n",
            "Epoch  12 Batch  511 / 525  Training Loss  0.010189024731516838\n",
            "Epoch  12 Batch  512 / 525  Training Loss  0.006523117423057556\n",
            "Epoch  12 Batch  513 / 525  Training Loss  0.0045485240407288074\n",
            "Epoch  12 Batch  514 / 525  Training Loss  0.008313534781336784\n",
            "Epoch  12 Batch  515 / 525  Training Loss  0.0026021606754511595\n",
            "Epoch  12 Batch  516 / 525  Training Loss  0.009116031229496002\n",
            "Epoch  12 Batch  517 / 525  Training Loss  0.009145276620984077\n",
            "Epoch  12 Batch  518 / 525  Training Loss  0.012638243846595287\n",
            "Epoch  12 Batch  519 / 525  Training Loss  0.012658944353461266\n",
            "Epoch  12 Batch  520 / 525  Training Loss  0.00738974055275321\n",
            "Epoch  12 Batch  521 / 525  Training Loss  0.004867092706263065\n",
            "Epoch  12 Batch  522 / 525  Training Loss  0.004039658233523369\n",
            "Epoch  12 Batch  523 / 525  Training Loss  0.0065565877594053745\n",
            "Epoch  12 Batch  524 / 525  Training Loss  0.01668485999107361\n",
            "  13    |    -    |   0.008290   |   61.27  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 13\n",
            "Epoch  13 Batch  0 / 525  Training Loss  0.002562724519520998\n",
            "Epoch  13 Batch  1 / 525  Training Loss  0.007662501186132431\n",
            "Epoch  13 Batch  2 / 525  Training Loss  0.014887225814163685\n",
            "Epoch  13 Batch  3 / 525  Training Loss  0.011516796424984932\n",
            "Epoch  13 Batch  4 / 525  Training Loss  0.009218926541507244\n",
            "Epoch  13 Batch  5 / 525  Training Loss  0.006132494658231735\n",
            "Epoch  13 Batch  6 / 525  Training Loss  0.0021030157804489136\n",
            "Epoch  13 Batch  7 / 525  Training Loss  0.004777568392455578\n",
            "Epoch  13 Batch  8 / 525  Training Loss  0.003226970788091421\n",
            "Epoch  13 Batch  9 / 525  Training Loss  0.005168000236153603\n",
            "Epoch  13 Batch  10 / 525  Training Loss  0.002393722301349044\n",
            "Epoch  13 Batch  11 / 525  Training Loss  0.0019257351523265243\n",
            "Epoch  13 Batch  12 / 525  Training Loss  0.010094838216900826\n",
            "Epoch  13 Batch  13 / 525  Training Loss  0.0032696290872991085\n",
            "Epoch  13 Batch  14 / 525  Training Loss  0.0019428975647315383\n",
            "Epoch  13 Batch  15 / 525  Training Loss  0.003203062806278467\n",
            "Epoch  13 Batch  16 / 525  Training Loss  0.004487989936023951\n",
            "Epoch  13 Batch  17 / 525  Training Loss  0.006266341544687748\n",
            "Epoch  13 Batch  18 / 525  Training Loss  0.004096849821507931\n",
            "Epoch  13 Batch  19 / 525  Training Loss  0.0055938721634447575\n",
            "Epoch  13 Batch  20 / 525  Training Loss  0.002746608341112733\n",
            "Epoch  13 Batch  21 / 525  Training Loss  0.0023319553583860397\n",
            "Epoch  13 Batch  22 / 525  Training Loss  0.004521755035966635\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  13 Batch  23 / 525  Training Loss  0.0035231788642704487\n",
            "Epoch  13 Batch  24 / 525  Training Loss  0.0038386532105505466\n",
            "Epoch  13 Batch  25 / 525  Training Loss  0.009249313734471798\n",
            "Epoch  13 Batch  26 / 525  Training Loss  0.00662710377946496\n",
            "Epoch  13 Batch  27 / 525  Training Loss  0.0028782961890101433\n",
            "Epoch  13 Batch  28 / 525  Training Loss  0.0022449621465057135\n",
            "Epoch  13 Batch  29 / 525  Training Loss  0.002784923417493701\n",
            "Epoch  13 Batch  30 / 525  Training Loss  0.003821897553279996\n",
            "Epoch  13 Batch  31 / 525  Training Loss  0.001967616844922304\n",
            "Epoch  13 Batch  32 / 525  Training Loss  0.003543476341292262\n",
            "Epoch  13 Batch  33 / 525  Training Loss  0.0046386257745325565\n",
            "Epoch  13 Batch  34 / 525  Training Loss  0.0032752621918916702\n",
            "Epoch  13 Batch  35 / 525  Training Loss  0.002645042259246111\n",
            "Epoch  13 Batch  36 / 525  Training Loss  0.0035314522683620453\n",
            "Epoch  13 Batch  37 / 525  Training Loss  0.005045129451900721\n",
            "Epoch  13 Batch  38 / 525  Training Loss  0.004143309313803911\n",
            "Epoch  13 Batch  39 / 525  Training Loss  0.0016418356681242585\n",
            "Epoch  13 Batch  40 / 525  Training Loss  0.005703463219106197\n",
            "Epoch  13 Batch  41 / 525  Training Loss  0.002145153935998678\n",
            "Epoch  13 Batch  42 / 525  Training Loss  0.026371944695711136\n",
            "Epoch  13 Batch  43 / 525  Training Loss  0.0038263413589447737\n",
            "Epoch  13 Batch  44 / 525  Training Loss  0.003235690761357546\n",
            "Epoch  13 Batch  45 / 525  Training Loss  0.012804456055164337\n",
            "Epoch  13 Batch  46 / 525  Training Loss  0.00233229948207736\n",
            "Epoch  13 Batch  47 / 525  Training Loss  0.003263689111918211\n",
            "Epoch  13 Batch  48 / 525  Training Loss  0.0033691588323563337\n",
            "Epoch  13 Batch  49 / 525  Training Loss  0.0020510456524789333\n",
            "Epoch  13 Batch  50 / 525  Training Loss  0.0020964150317013264\n",
            "Epoch  13 Batch  51 / 525  Training Loss  0.0022597801871597767\n",
            "Epoch  13 Batch  52 / 525  Training Loss  0.0035135969519615173\n",
            "Epoch  13 Batch  53 / 525  Training Loss  0.0058412193320691586\n",
            "Epoch  13 Batch  54 / 525  Training Loss  0.0030619881581515074\n",
            "Epoch  13 Batch  55 / 525  Training Loss  0.003398546949028969\n",
            "Epoch  13 Batch  56 / 525  Training Loss  0.0034323795698583126\n",
            "Epoch  13 Batch  57 / 525  Training Loss  0.004207273945212364\n",
            "Epoch  13 Batch  58 / 525  Training Loss  0.00366964447312057\n",
            "Epoch  13 Batch  59 / 525  Training Loss  0.00244829966686666\n",
            "Epoch  13 Batch  60 / 525  Training Loss  0.0025740428827703\n",
            "Epoch  13 Batch  61 / 525  Training Loss  0.0032479502260684967\n",
            "Epoch  13 Batch  62 / 525  Training Loss  0.0013767866184934974\n",
            "Epoch  13 Batch  63 / 525  Training Loss  0.003219243139028549\n",
            "Epoch  13 Batch  64 / 525  Training Loss  0.002689423505216837\n",
            "Epoch  13 Batch  65 / 525  Training Loss  0.0016726957401260734\n",
            "Epoch  13 Batch  66 / 525  Training Loss  0.0023895129561424255\n",
            "Epoch  13 Batch  67 / 525  Training Loss  0.004145556129515171\n",
            "Epoch  13 Batch  68 / 525  Training Loss  0.0051751215942204\n",
            "Epoch  13 Batch  69 / 525  Training Loss  0.002268622163683176\n",
            "Epoch  13 Batch  70 / 525  Training Loss  0.008082786574959755\n",
            "Epoch  13 Batch  71 / 525  Training Loss  0.003215016331523657\n",
            "Epoch  13 Batch  72 / 525  Training Loss  0.003620584961026907\n",
            "Epoch  13 Batch  73 / 525  Training Loss  0.0033744201064109802\n",
            "Epoch  13 Batch  74 / 525  Training Loss  0.001742732711136341\n",
            "Epoch  13 Batch  75 / 525  Training Loss  0.006745836231857538\n",
            "Epoch  13 Batch  76 / 525  Training Loss  0.0033350703306496143\n",
            "Epoch  13 Batch  77 / 525  Training Loss  0.004185494966804981\n",
            "Epoch  13 Batch  78 / 525  Training Loss  0.006492448505014181\n",
            "Epoch  13 Batch  79 / 525  Training Loss  0.009122627787292004\n",
            "Epoch  13 Batch  80 / 525  Training Loss  0.004469508770853281\n",
            "Epoch  13 Batch  81 / 525  Training Loss  0.0018459692364558578\n",
            "Epoch  13 Batch  82 / 525  Training Loss  0.004692484624683857\n",
            "Epoch  13 Batch  83 / 525  Training Loss  0.002218297217041254\n",
            "Epoch  13 Batch  84 / 525  Training Loss  0.0024888128973543644\n",
            "Epoch  13 Batch  85 / 525  Training Loss  0.0017926886212080717\n",
            "Epoch  13 Batch  86 / 525  Training Loss  0.003874456975609064\n",
            "Epoch  13 Batch  87 / 525  Training Loss  0.0024355181958526373\n",
            "Epoch  13 Batch  88 / 525  Training Loss  0.002955352421849966\n",
            "Epoch  13 Batch  89 / 525  Training Loss  0.002677305368706584\n",
            "Epoch  13 Batch  90 / 525  Training Loss  0.0030671153217554092\n",
            "Epoch  13 Batch  91 / 525  Training Loss  0.006085170898586512\n",
            "Epoch  13 Batch  92 / 525  Training Loss  0.003365491284057498\n",
            "Epoch  13 Batch  93 / 525  Training Loss  0.007161363959312439\n",
            "Epoch  13 Batch  94 / 525  Training Loss  0.0017176687251776457\n",
            "Epoch  13 Batch  95 / 525  Training Loss  0.0065515972673892975\n",
            "Epoch  13 Batch  96 / 525  Training Loss  0.004017486236989498\n",
            "Epoch  13 Batch  97 / 525  Training Loss  0.001915600849315524\n",
            "Epoch  13 Batch  98 / 525  Training Loss  0.002731314627453685\n",
            "Epoch  13 Batch  99 / 525  Training Loss  0.008492018096148968\n",
            "Epoch  13 Batch  100 / 525  Training Loss  0.0038716390263289213\n",
            "Epoch  13 Batch  101 / 525  Training Loss  0.00815280620008707\n",
            "Epoch  13 Batch  102 / 525  Training Loss  0.0013360201846808195\n",
            "Epoch  13 Batch  103 / 525  Training Loss  0.005675601772964001\n",
            "Epoch  13 Batch  104 / 525  Training Loss  0.0033046253956854343\n",
            "Epoch  13 Batch  105 / 525  Training Loss  0.004059202969074249\n",
            "Epoch  13 Batch  106 / 525  Training Loss  0.0028056867886334658\n",
            "Epoch  13 Batch  107 / 525  Training Loss  0.0033038719557225704\n",
            "Epoch  13 Batch  108 / 525  Training Loss  0.0032672963570803404\n",
            "Epoch  13 Batch  109 / 525  Training Loss  0.00294686877168715\n",
            "Epoch  13 Batch  110 / 525  Training Loss  0.0012270143488422036\n",
            "Epoch  13 Batch  111 / 525  Training Loss  0.0034666359424591064\n",
            "Epoch  13 Batch  112 / 525  Training Loss  0.00347544951364398\n",
            "Epoch  13 Batch  113 / 525  Training Loss  0.001708500785753131\n",
            "Epoch  13 Batch  114 / 525  Training Loss  0.006586247123777866\n",
            "Epoch  13 Batch  115 / 525  Training Loss  0.00606573699042201\n",
            "Epoch  13 Batch  116 / 525  Training Loss  0.0047620078548789024\n",
            "Epoch  13 Batch  117 / 525  Training Loss  0.0018620456103235483\n",
            "Epoch  13 Batch  118 / 525  Training Loss  0.007059373892843723\n",
            "Epoch  13 Batch  119 / 525  Training Loss  0.0023744343779981136\n",
            "Epoch  13 Batch  120 / 525  Training Loss  0.0028294972144067287\n",
            "Epoch  13 Batch  121 / 525  Training Loss  0.002933396492153406\n",
            "Epoch  13 Batch  122 / 525  Training Loss  0.008004995994269848\n",
            "Epoch  13 Batch  123 / 525  Training Loss  0.004678306169807911\n",
            "Epoch  13 Batch  124 / 525  Training Loss  0.0063115679658949375\n",
            "Epoch  13 Batch  125 / 525  Training Loss  0.0033880076371133327\n",
            "Epoch  13 Batch  126 / 525  Training Loss  0.0028750733472406864\n",
            "Epoch  13 Batch  127 / 525  Training Loss  0.007993875071406364\n",
            "Epoch  13 Batch  128 / 525  Training Loss  0.010101424530148506\n",
            "Epoch  13 Batch  129 / 525  Training Loss  0.018065309152007103\n",
            "Epoch  13 Batch  130 / 525  Training Loss  0.00481226434931159\n",
            "Epoch  13 Batch  131 / 525  Training Loss  0.002935986500233412\n",
            "Epoch  13 Batch  132 / 525  Training Loss  0.002716677263379097\n",
            "Epoch  13 Batch  133 / 525  Training Loss  0.0026656140107661486\n",
            "Epoch  13 Batch  134 / 525  Training Loss  0.003662362229079008\n",
            "Epoch  13 Batch  135 / 525  Training Loss  0.0026030135340988636\n",
            "Epoch  13 Batch  136 / 525  Training Loss  0.004905242007225752\n",
            "Epoch  13 Batch  137 / 525  Training Loss  0.0027989011723548174\n",
            "Epoch  13 Batch  138 / 525  Training Loss  0.0022814192343503237\n",
            "Epoch  13 Batch  139 / 525  Training Loss  0.003784365253522992\n",
            "Epoch  13 Batch  140 / 525  Training Loss  0.004114547744393349\n",
            "Epoch  13 Batch  141 / 525  Training Loss  0.0015929335495457053\n",
            "Epoch  13 Batch  142 / 525  Training Loss  0.0029424303211271763\n",
            "Epoch  13 Batch  143 / 525  Training Loss  0.0032944839913398027\n",
            "Epoch  13 Batch  144 / 525  Training Loss  0.0021504699252545834\n",
            "Epoch  13 Batch  145 / 525  Training Loss  0.00892026349902153\n",
            "Epoch  13 Batch  146 / 525  Training Loss  0.0032663848251104355\n",
            "Epoch  13 Batch  147 / 525  Training Loss  0.0026506136637181044\n",
            "Epoch  13 Batch  148 / 525  Training Loss  0.003040105104446411\n",
            "Epoch  13 Batch  149 / 525  Training Loss  0.006963692605495453\n",
            "Epoch  13 Batch  150 / 525  Training Loss  0.0034602568484842777\n",
            "Epoch  13 Batch  151 / 525  Training Loss  0.005238907411694527\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  13 Batch  152 / 525  Training Loss  0.009193988516926765\n",
            "Epoch  13 Batch  153 / 525  Training Loss  0.007637397851794958\n",
            "Epoch  13 Batch  154 / 525  Training Loss  0.006552467588335276\n",
            "Epoch  13 Batch  155 / 525  Training Loss  0.010670996271073818\n",
            "Epoch  13 Batch  156 / 525  Training Loss  0.004759394563734531\n",
            "Epoch  13 Batch  157 / 525  Training Loss  0.0052238954231143\n",
            "Epoch  13 Batch  158 / 525  Training Loss  0.002335313241928816\n",
            "Epoch  13 Batch  159 / 525  Training Loss  0.0016486123204231262\n",
            "Epoch  13 Batch  160 / 525  Training Loss  0.003061349969357252\n",
            "Epoch  13 Batch  161 / 525  Training Loss  0.0030660689808428288\n",
            "Epoch  13 Batch  162 / 525  Training Loss  0.001982804387807846\n",
            "Epoch  13 Batch  163 / 525  Training Loss  0.0036054744850844145\n",
            "Epoch  13 Batch  164 / 525  Training Loss  0.0021539945155382156\n",
            "Epoch  13 Batch  165 / 525  Training Loss  0.0024610143154859543\n",
            "Epoch  13 Batch  166 / 525  Training Loss  0.003513652365654707\n",
            "Epoch  13 Batch  167 / 525  Training Loss  0.0029573787469416857\n",
            "Epoch  13 Batch  168 / 525  Training Loss  0.005515504162758589\n",
            "Epoch  13 Batch  169 / 525  Training Loss  0.00420809630304575\n",
            "Epoch  13 Batch  170 / 525  Training Loss  0.007033342961221933\n",
            "Epoch  13 Batch  171 / 525  Training Loss  0.00287790410220623\n",
            "Epoch  13 Batch  172 / 525  Training Loss  0.004297338891774416\n",
            "Epoch  13 Batch  173 / 525  Training Loss  0.004038998857140541\n",
            "Epoch  13 Batch  174 / 525  Training Loss  0.0022180036175996065\n",
            "Epoch  13 Batch  175 / 525  Training Loss  0.005489158444106579\n",
            "Epoch  13 Batch  176 / 525  Training Loss  0.003513817908242345\n",
            "Epoch  13 Batch  177 / 525  Training Loss  0.0012047672644257545\n",
            "Epoch  13 Batch  178 / 525  Training Loss  0.005925224628299475\n",
            "Epoch  13 Batch  179 / 525  Training Loss  0.002053263131529093\n",
            "Epoch  13 Batch  180 / 525  Training Loss  0.0018613983411341906\n",
            "Epoch  13 Batch  181 / 525  Training Loss  0.0016760139260441065\n",
            "Epoch  13 Batch  182 / 525  Training Loss  0.003777236444875598\n",
            "Epoch  13 Batch  183 / 525  Training Loss  0.00525363814085722\n",
            "Epoch  13 Batch  184 / 525  Training Loss  0.002561132423579693\n",
            "Epoch  13 Batch  185 / 525  Training Loss  0.0075708492659032345\n",
            "Epoch  13 Batch  186 / 525  Training Loss  0.0018261189106851816\n",
            "Epoch  13 Batch  187 / 525  Training Loss  0.003639381378889084\n",
            "Epoch  13 Batch  188 / 525  Training Loss  0.0014410812873393297\n",
            "Epoch  13 Batch  189 / 525  Training Loss  0.003932634834200144\n",
            "Epoch  13 Batch  190 / 525  Training Loss  0.007081320974975824\n",
            "Epoch  13 Batch  191 / 525  Training Loss  0.0029654281679540873\n",
            "Epoch  13 Batch  192 / 525  Training Loss  0.0028360846918076277\n",
            "Epoch  13 Batch  193 / 525  Training Loss  0.001127238618209958\n",
            "Epoch  13 Batch  194 / 525  Training Loss  0.001163919223472476\n",
            "Epoch  13 Batch  195 / 525  Training Loss  0.0030952647794038057\n",
            "Epoch  13 Batch  196 / 525  Training Loss  0.0008892094483599067\n",
            "Epoch  13 Batch  197 / 525  Training Loss  0.003314101602882147\n",
            "Epoch  13 Batch  198 / 525  Training Loss  0.0014371916186064482\n",
            "Epoch  13 Batch  199 / 525  Training Loss  0.0023712259717285633\n",
            "Epoch  13 Batch  200 / 525  Training Loss  0.0033346109557896852\n",
            "Epoch  13 Batch  201 / 525  Training Loss  0.0014419991057366133\n",
            "Epoch  13 Batch  202 / 525  Training Loss  0.0028443154878914356\n",
            "Epoch  13 Batch  203 / 525  Training Loss  0.002246369142085314\n",
            "Epoch  13 Batch  204 / 525  Training Loss  0.004527886398136616\n",
            "Epoch  13 Batch  205 / 525  Training Loss  0.0017062319675460458\n",
            "Epoch  13 Batch  206 / 525  Training Loss  0.0034461270552128553\n",
            "Epoch  13 Batch  207 / 525  Training Loss  0.002303879475221038\n",
            "Epoch  13 Batch  208 / 525  Training Loss  0.004732334055006504\n",
            "Epoch  13 Batch  209 / 525  Training Loss  0.009406276047229767\n",
            "Epoch  13 Batch  210 / 525  Training Loss  0.004353536292910576\n",
            "Epoch  13 Batch  211 / 525  Training Loss  0.0016813637921586633\n",
            "Epoch  13 Batch  212 / 525  Training Loss  0.002310196403414011\n",
            "Epoch  13 Batch  213 / 525  Training Loss  0.0034900172613561153\n",
            "Epoch  13 Batch  214 / 525  Training Loss  0.0037570272106677294\n",
            "Epoch  13 Batch  215 / 525  Training Loss  0.0037633117754012346\n",
            "Epoch  13 Batch  216 / 525  Training Loss  0.003813589457422495\n",
            "Epoch  13 Batch  217 / 525  Training Loss  0.0022192581091076136\n",
            "Epoch  13 Batch  218 / 525  Training Loss  0.003435607999563217\n",
            "Epoch  13 Batch  219 / 525  Training Loss  0.002101779216900468\n",
            "Epoch  13 Batch  220 / 525  Training Loss  0.00452521163970232\n",
            "Epoch  13 Batch  221 / 525  Training Loss  0.0028163844253867865\n",
            "Epoch  13 Batch  222 / 525  Training Loss  0.0031515727750957012\n",
            "Epoch  13 Batch  223 / 525  Training Loss  0.0018431823700666428\n",
            "Epoch  13 Batch  224 / 525  Training Loss  0.0028467292431741953\n",
            "Epoch  13 Batch  225 / 525  Training Loss  0.0011955222580581903\n",
            "Epoch  13 Batch  226 / 525  Training Loss  0.008197823539376259\n",
            "Epoch  13 Batch  227 / 525  Training Loss  0.002283448353409767\n",
            "Epoch  13 Batch  228 / 525  Training Loss  0.001757571822963655\n",
            "Epoch  13 Batch  229 / 525  Training Loss  0.0017030693124979734\n",
            "Epoch  13 Batch  230 / 525  Training Loss  0.003918589558452368\n",
            "Epoch  13 Batch  231 / 525  Training Loss  0.0017370044952258468\n",
            "Epoch  13 Batch  232 / 525  Training Loss  0.007412297185510397\n",
            "Epoch  13 Batch  233 / 525  Training Loss  0.0032957643270492554\n",
            "Epoch  13 Batch  234 / 525  Training Loss  0.005303443409502506\n",
            "Epoch  13 Batch  235 / 525  Training Loss  0.009422337636351585\n",
            "Epoch  13 Batch  236 / 525  Training Loss  0.007116538472473621\n",
            "Epoch  13 Batch  237 / 525  Training Loss  0.009118816815316677\n",
            "Epoch  13 Batch  238 / 525  Training Loss  0.0026122869458049536\n",
            "Epoch  13 Batch  239 / 525  Training Loss  0.005713649094104767\n",
            "Epoch  13 Batch  240 / 525  Training Loss  0.00968590285629034\n",
            "Epoch  13 Batch  241 / 525  Training Loss  0.005208044312894344\n",
            "Epoch  13 Batch  242 / 525  Training Loss  0.003140666289255023\n",
            "Epoch  13 Batch  243 / 525  Training Loss  0.00450520496815443\n",
            "Epoch  13 Batch  244 / 525  Training Loss  0.002813037484884262\n",
            "Epoch  13 Batch  245 / 525  Training Loss  0.0014598366105929017\n",
            "Epoch  13 Batch  246 / 525  Training Loss  0.0016745298635214567\n",
            "Epoch  13 Batch  247 / 525  Training Loss  0.0025974989403039217\n",
            "Epoch  13 Batch  248 / 525  Training Loss  0.012181989848613739\n",
            "Epoch  13 Batch  249 / 525  Training Loss  0.014154775068163872\n",
            "Epoch  13 Batch  250 / 525  Training Loss  0.0027909495402127504\n",
            "Epoch  13 Batch  251 / 525  Training Loss  0.005408949218690395\n",
            "Epoch  13 Batch  252 / 525  Training Loss  0.002075491938740015\n",
            "Epoch  13 Batch  253 / 525  Training Loss  0.006475784815847874\n",
            "Epoch  13 Batch  254 / 525  Training Loss  0.005362826399505138\n",
            "Epoch  13 Batch  255 / 525  Training Loss  0.01169334352016449\n",
            "Epoch  13 Batch  256 / 525  Training Loss  0.01001917663961649\n",
            "Epoch  13 Batch  257 / 525  Training Loss  0.005611498840153217\n",
            "Epoch  13 Batch  258 / 525  Training Loss  0.00462624616920948\n",
            "Epoch  13 Batch  259 / 525  Training Loss  0.0036640893667936325\n",
            "Epoch  13 Batch  260 / 525  Training Loss  0.004301450215280056\n",
            "Epoch  13 Batch  261 / 525  Training Loss  0.003564354032278061\n",
            "Epoch  13 Batch  262 / 525  Training Loss  0.003191243391484022\n",
            "Epoch  13 Batch  263 / 525  Training Loss  0.004871551413089037\n",
            "Epoch  13 Batch  264 / 525  Training Loss  0.006355551071465015\n",
            "Epoch  13 Batch  265 / 525  Training Loss  0.0021262969821691513\n",
            "Epoch  13 Batch  266 / 525  Training Loss  0.0025586087722331285\n",
            "Epoch  13 Batch  267 / 525  Training Loss  0.0019698168616741896\n",
            "Epoch  13 Batch  268 / 525  Training Loss  0.0019241671543568373\n",
            "Epoch  13 Batch  269 / 525  Training Loss  0.002588474890217185\n",
            "Epoch  13 Batch  270 / 525  Training Loss  0.003571557579562068\n",
            "Epoch  13 Batch  271 / 525  Training Loss  0.0026910286396741867\n",
            "Epoch  13 Batch  272 / 525  Training Loss  0.004969689063727856\n",
            "Epoch  13 Batch  273 / 525  Training Loss  0.00357941840775311\n",
            "Epoch  13 Batch  274 / 525  Training Loss  0.00372026558034122\n",
            "Epoch  13 Batch  275 / 525  Training Loss  0.00898820161819458\n",
            "Epoch  13 Batch  276 / 525  Training Loss  0.001877626171335578\n",
            "Epoch  13 Batch  277 / 525  Training Loss  0.00549177173525095\n",
            "Epoch  13 Batch  278 / 525  Training Loss  0.0034311525523662567\n",
            "Epoch  13 Batch  279 / 525  Training Loss  0.00441494258120656\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  13 Batch  280 / 525  Training Loss  0.0039013312198221684\n",
            "Epoch  13 Batch  281 / 525  Training Loss  0.005844832397997379\n",
            "Epoch  13 Batch  282 / 525  Training Loss  0.0036558485589921474\n",
            "Epoch  13 Batch  283 / 525  Training Loss  0.0038219052366912365\n",
            "Epoch  13 Batch  284 / 525  Training Loss  0.0023537047673016787\n",
            "Epoch  13 Batch  285 / 525  Training Loss  0.005363885313272476\n",
            "Epoch  13 Batch  286 / 525  Training Loss  0.0057853879407048225\n",
            "Epoch  13 Batch  287 / 525  Training Loss  0.00421521533280611\n",
            "Epoch  13 Batch  288 / 525  Training Loss  0.00403206329792738\n",
            "Epoch  13 Batch  289 / 525  Training Loss  0.003375678788870573\n",
            "Epoch  13 Batch  290 / 525  Training Loss  0.002492982894182205\n",
            "Epoch  13 Batch  291 / 525  Training Loss  0.003247293410822749\n",
            "Epoch  13 Batch  292 / 525  Training Loss  0.01277129165828228\n",
            "Epoch  13 Batch  293 / 525  Training Loss  0.0035777047742158175\n",
            "Epoch  13 Batch  294 / 525  Training Loss  0.004087249748408794\n",
            "Epoch  13 Batch  295 / 525  Training Loss  0.002375780139118433\n",
            "Epoch  13 Batch  296 / 525  Training Loss  0.002483054529875517\n",
            "Epoch  13 Batch  297 / 525  Training Loss  0.0038253325037658215\n",
            "Epoch  13 Batch  298 / 525  Training Loss  0.0029076035134494305\n",
            "Epoch  13 Batch  299 / 525  Training Loss  0.002404147991910577\n",
            "Epoch  13 Batch  300 / 525  Training Loss  0.009257528930902481\n",
            "Epoch  13 Batch  301 / 525  Training Loss  0.0034954852890223265\n",
            "Epoch  13 Batch  302 / 525  Training Loss  0.002835349878296256\n",
            "Epoch  13 Batch  303 / 525  Training Loss  0.003562029916793108\n",
            "Epoch  13 Batch  304 / 525  Training Loss  0.0022351727820932865\n",
            "Epoch  13 Batch  305 / 525  Training Loss  0.0037073721177875996\n",
            "Epoch  13 Batch  306 / 525  Training Loss  0.0034334014635533094\n",
            "Epoch  13 Batch  307 / 525  Training Loss  0.0032356269657611847\n",
            "Epoch  13 Batch  308 / 525  Training Loss  0.005664239637553692\n",
            "Epoch  13 Batch  309 / 525  Training Loss  0.01097802259027958\n",
            "Epoch  13 Batch  310 / 525  Training Loss  0.0021877570543438196\n",
            "Epoch  13 Batch  311 / 525  Training Loss  0.002981926081702113\n",
            "Epoch  13 Batch  312 / 525  Training Loss  0.005588965956121683\n",
            "Epoch  13 Batch  313 / 525  Training Loss  0.0023175657261162996\n",
            "Epoch  13 Batch  314 / 525  Training Loss  0.0032955645583570004\n",
            "Epoch  13 Batch  315 / 525  Training Loss  0.00217258557677269\n",
            "Epoch  13 Batch  316 / 525  Training Loss  0.004515615291893482\n",
            "Epoch  13 Batch  317 / 525  Training Loss  0.008934278972446918\n",
            "Epoch  13 Batch  318 / 525  Training Loss  0.0025697334203869104\n",
            "Epoch  13 Batch  319 / 525  Training Loss  0.0050844294019043446\n",
            "Epoch  13 Batch  320 / 525  Training Loss  0.002866871654987335\n",
            "Epoch  13 Batch  321 / 525  Training Loss  0.006510662846267223\n",
            "Epoch  13 Batch  322 / 525  Training Loss  0.002886256668716669\n",
            "Epoch  13 Batch  323 / 525  Training Loss  0.003927404992282391\n",
            "Epoch  13 Batch  324 / 525  Training Loss  0.002056388882920146\n",
            "Epoch  13 Batch  325 / 525  Training Loss  0.004908471368253231\n",
            "Epoch  13 Batch  326 / 525  Training Loss  0.008518078364431858\n",
            "Epoch  13 Batch  327 / 525  Training Loss  0.0028621216770261526\n",
            "Epoch  13 Batch  328 / 525  Training Loss  0.0020837034098803997\n",
            "Epoch  13 Batch  329 / 525  Training Loss  0.0079946368932724\n",
            "Epoch  13 Batch  330 / 525  Training Loss  0.005029062740504742\n",
            "Epoch  13 Batch  331 / 525  Training Loss  0.0020340450573712587\n",
            "Epoch  13 Batch  332 / 525  Training Loss  0.00311525771394372\n",
            "Epoch  13 Batch  333 / 525  Training Loss  0.0020504635758697987\n",
            "Epoch  13 Batch  334 / 525  Training Loss  0.007393392734229565\n",
            "Epoch  13 Batch  335 / 525  Training Loss  0.001850274158641696\n",
            "Epoch  13 Batch  336 / 525  Training Loss  0.0024756193161010742\n",
            "Epoch  13 Batch  337 / 525  Training Loss  0.01541068870574236\n",
            "Epoch  13 Batch  338 / 525  Training Loss  0.0036312032025307417\n",
            "Epoch  13 Batch  339 / 525  Training Loss  0.005482190288603306\n",
            "Epoch  13 Batch  340 / 525  Training Loss  0.00470649404451251\n",
            "Epoch  13 Batch  341 / 525  Training Loss  0.004306023009121418\n",
            "Epoch  13 Batch  342 / 525  Training Loss  0.0021715881302952766\n",
            "Epoch  13 Batch  343 / 525  Training Loss  0.006498407572507858\n",
            "Epoch  13 Batch  344 / 525  Training Loss  0.0016247474122792482\n",
            "Epoch  13 Batch  345 / 525  Training Loss  0.0025208296719938517\n",
            "Epoch  13 Batch  346 / 525  Training Loss  0.0062801418825984\n",
            "Epoch  13 Batch  347 / 525  Training Loss  0.0052125109359622\n",
            "Epoch  13 Batch  348 / 525  Training Loss  0.00765229482203722\n",
            "Epoch  13 Batch  349 / 525  Training Loss  0.00451501552015543\n",
            "Epoch  13 Batch  350 / 525  Training Loss  0.005399801768362522\n",
            "Epoch  13 Batch  351 / 525  Training Loss  0.002609566319733858\n",
            "Epoch  13 Batch  352 / 525  Training Loss  0.0029182415455579758\n",
            "Epoch  13 Batch  353 / 525  Training Loss  0.0074524120427668095\n",
            "Epoch  13 Batch  354 / 525  Training Loss  0.0014756073942407966\n",
            "Epoch  13 Batch  355 / 525  Training Loss  0.00473844213411212\n",
            "Epoch  13 Batch  356 / 525  Training Loss  0.0019814237020909786\n",
            "Epoch  13 Batch  357 / 525  Training Loss  0.003694164101034403\n",
            "Epoch  13 Batch  358 / 525  Training Loss  0.0026154653169214725\n",
            "Epoch  13 Batch  359 / 525  Training Loss  0.0045169321820139885\n",
            "Epoch  13 Batch  360 / 525  Training Loss  0.0025814431719481945\n",
            "Epoch  13 Batch  361 / 525  Training Loss  0.005568061023950577\n",
            "Epoch  13 Batch  362 / 525  Training Loss  0.003599823685362935\n",
            "Epoch  13 Batch  363 / 525  Training Loss  0.004438426345586777\n",
            "Epoch  13 Batch  364 / 525  Training Loss  0.002231067046523094\n",
            "Epoch  13 Batch  365 / 525  Training Loss  0.005066666752099991\n",
            "Epoch  13 Batch  366 / 525  Training Loss  0.002821560250595212\n",
            "Epoch  13 Batch  367 / 525  Training Loss  0.0040761255659163\n",
            "Epoch  13 Batch  368 / 525  Training Loss  0.003022662131115794\n",
            "Epoch  13 Batch  369 / 525  Training Loss  0.017790891230106354\n",
            "Epoch  13 Batch  370 / 525  Training Loss  0.004973426461219788\n",
            "Epoch  13 Batch  371 / 525  Training Loss  0.0025022837799042463\n",
            "Epoch  13 Batch  372 / 525  Training Loss  0.010903341695666313\n",
            "Epoch  13 Batch  373 / 525  Training Loss  0.004850069526582956\n",
            "Epoch  13 Batch  374 / 525  Training Loss  0.010211531072854996\n",
            "Epoch  13 Batch  375 / 525  Training Loss  0.0018053122330456972\n",
            "Epoch  13 Batch  376 / 525  Training Loss  0.00433653499931097\n",
            "Epoch  13 Batch  377 / 525  Training Loss  0.0041394541040062904\n",
            "Epoch  13 Batch  378 / 525  Training Loss  0.006629686802625656\n",
            "Epoch  13 Batch  379 / 525  Training Loss  0.003461414948105812\n",
            "Epoch  13 Batch  380 / 525  Training Loss  0.004186796024441719\n",
            "Epoch  13 Batch  381 / 525  Training Loss  0.002627108944579959\n",
            "Epoch  13 Batch  382 / 525  Training Loss  0.005129560828208923\n",
            "Epoch  13 Batch  383 / 525  Training Loss  0.01173868216574192\n",
            "Epoch  13 Batch  384 / 525  Training Loss  0.006757849361747503\n",
            "Epoch  13 Batch  385 / 525  Training Loss  0.01347927562892437\n",
            "Epoch  13 Batch  386 / 525  Training Loss  0.004295616876333952\n",
            "Epoch  13 Batch  387 / 525  Training Loss  0.009867768734693527\n",
            "Epoch  13 Batch  388 / 525  Training Loss  0.0028978902846574783\n",
            "Epoch  13 Batch  389 / 525  Training Loss  0.0031285881996154785\n",
            "Epoch  13 Batch  390 / 525  Training Loss  0.007575224153697491\n",
            "Epoch  13 Batch  391 / 525  Training Loss  0.002473203232511878\n",
            "Epoch  13 Batch  392 / 525  Training Loss  0.005288975778967142\n",
            "Epoch  13 Batch  393 / 525  Training Loss  0.003258502809330821\n",
            "Epoch  13 Batch  394 / 525  Training Loss  0.004056369885802269\n",
            "Epoch  13 Batch  395 / 525  Training Loss  0.0012136166915297508\n",
            "Epoch  13 Batch  396 / 525  Training Loss  0.0051277573220431805\n",
            "Epoch  13 Batch  397 / 525  Training Loss  0.008866934105753899\n",
            "Epoch  13 Batch  398 / 525  Training Loss  0.004842186812311411\n",
            "Epoch  13 Batch  399 / 525  Training Loss  0.011806944385170937\n",
            "Epoch  13 Batch  400 / 525  Training Loss  0.0043500917963683605\n",
            "Epoch  13 Batch  401 / 525  Training Loss  0.0024308464489877224\n",
            "Epoch  13 Batch  402 / 525  Training Loss  0.0016174458432942629\n",
            "Epoch  13 Batch  403 / 525  Training Loss  0.007739211432635784\n",
            "Epoch  13 Batch  404 / 525  Training Loss  0.00460051279515028\n",
            "Epoch  13 Batch  405 / 525  Training Loss  0.008698130026459694\n",
            "Epoch  13 Batch  406 / 525  Training Loss  0.0041090985760092735\n",
            "Epoch  13 Batch  407 / 525  Training Loss  0.002610405907034874\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  13 Batch  408 / 525  Training Loss  0.007424152456223965\n",
            "Epoch  13 Batch  409 / 525  Training Loss  0.006254028528928757\n",
            "Epoch  13 Batch  410 / 525  Training Loss  0.0048623280599713326\n",
            "Epoch  13 Batch  411 / 525  Training Loss  0.0025508878752589226\n",
            "Epoch  13 Batch  412 / 525  Training Loss  0.0027277888730168343\n",
            "Epoch  13 Batch  413 / 525  Training Loss  0.008766070939600468\n",
            "Epoch  13 Batch  414 / 525  Training Loss  0.005593751557171345\n",
            "Epoch  13 Batch  415 / 525  Training Loss  0.004103402607142925\n",
            "Epoch  13 Batch  416 / 525  Training Loss  0.00330118159763515\n",
            "Epoch  13 Batch  417 / 525  Training Loss  0.003270573914051056\n",
            "Epoch  13 Batch  418 / 525  Training Loss  0.004359481390565634\n",
            "Epoch  13 Batch  419 / 525  Training Loss  0.0012765501160174608\n",
            "Epoch  13 Batch  420 / 525  Training Loss  0.00706120952963829\n",
            "Epoch  13 Batch  421 / 525  Training Loss  0.0030644882936030626\n",
            "Epoch  13 Batch  422 / 525  Training Loss  0.0068444847129285336\n",
            "Epoch  13 Batch  423 / 525  Training Loss  0.0029579666443169117\n",
            "Epoch  13 Batch  424 / 525  Training Loss  0.003999144304543734\n",
            "Epoch  13 Batch  425 / 525  Training Loss  0.001524000079371035\n",
            "Epoch  13 Batch  426 / 525  Training Loss  0.0027738623321056366\n",
            "Epoch  13 Batch  427 / 525  Training Loss  0.0016038164030760527\n",
            "Epoch  13 Batch  428 / 525  Training Loss  0.005732367746531963\n",
            "Epoch  13 Batch  429 / 525  Training Loss  0.003434595186263323\n",
            "Epoch  13 Batch  430 / 525  Training Loss  0.003793166484683752\n",
            "Epoch  13 Batch  431 / 525  Training Loss  0.004000937100499868\n",
            "Epoch  13 Batch  432 / 525  Training Loss  0.0016877815360203385\n",
            "Epoch  13 Batch  433 / 525  Training Loss  0.006526238285005093\n",
            "Epoch  13 Batch  434 / 525  Training Loss  0.00571055430918932\n",
            "Epoch  13 Batch  435 / 525  Training Loss  0.004335931036621332\n",
            "Epoch  13 Batch  436 / 525  Training Loss  0.002743543591350317\n",
            "Epoch  13 Batch  437 / 525  Training Loss  0.004257718101143837\n",
            "Epoch  13 Batch  438 / 525  Training Loss  0.015873389318585396\n",
            "Epoch  13 Batch  439 / 525  Training Loss  0.0031799282878637314\n",
            "Epoch  13 Batch  440 / 525  Training Loss  0.002566967159509659\n",
            "Epoch  13 Batch  441 / 525  Training Loss  0.0050875963643193245\n",
            "Epoch  13 Batch  442 / 525  Training Loss  0.004237677436321974\n",
            "Epoch  13 Batch  443 / 525  Training Loss  0.0031649074517190456\n",
            "Epoch  13 Batch  444 / 525  Training Loss  0.003977256361395121\n",
            "Epoch  13 Batch  445 / 525  Training Loss  0.0010221732081845403\n",
            "Epoch  13 Batch  446 / 525  Training Loss  0.0016870662802830338\n",
            "Epoch  13 Batch  447 / 525  Training Loss  0.0029946845024824142\n",
            "Epoch  13 Batch  448 / 525  Training Loss  0.0021709550637751818\n",
            "Epoch  13 Batch  449 / 525  Training Loss  0.0037785531021654606\n",
            "Epoch  13 Batch  450 / 525  Training Loss  0.004242416005581617\n",
            "Epoch  13 Batch  451 / 525  Training Loss  0.001595879322849214\n",
            "Epoch  13 Batch  452 / 525  Training Loss  0.003504533786326647\n",
            "Epoch  13 Batch  453 / 525  Training Loss  0.003716500476002693\n",
            "Epoch  13 Batch  454 / 525  Training Loss  0.003397644031792879\n",
            "Epoch  13 Batch  455 / 525  Training Loss  0.003488793270662427\n",
            "Epoch  13 Batch  456 / 525  Training Loss  0.004249968566000462\n",
            "Epoch  13 Batch  457 / 525  Training Loss  0.002404466038569808\n",
            "Epoch  13 Batch  458 / 525  Training Loss  0.006902221590280533\n",
            "Epoch  13 Batch  459 / 525  Training Loss  0.004873401485383511\n",
            "Epoch  13 Batch  460 / 525  Training Loss  0.011913713999092579\n",
            "Epoch  13 Batch  461 / 525  Training Loss  0.0026524867862462997\n",
            "Epoch  13 Batch  462 / 525  Training Loss  0.002645601751282811\n",
            "Epoch  13 Batch  463 / 525  Training Loss  0.0036096975672990084\n",
            "Epoch  13 Batch  464 / 525  Training Loss  0.002252044389024377\n",
            "Epoch  13 Batch  465 / 525  Training Loss  0.005377768538892269\n",
            "Epoch  13 Batch  466 / 525  Training Loss  0.003665124299004674\n",
            "Epoch  13 Batch  467 / 525  Training Loss  0.005465561058372259\n",
            "Epoch  13 Batch  468 / 525  Training Loss  0.002001016167923808\n",
            "Epoch  13 Batch  469 / 525  Training Loss  0.006322421133518219\n",
            "Epoch  13 Batch  470 / 525  Training Loss  0.003626655787229538\n",
            "Epoch  13 Batch  471 / 525  Training Loss  0.0019485473167151213\n",
            "Epoch  13 Batch  472 / 525  Training Loss  0.008331716991961002\n",
            "Epoch  13 Batch  473 / 525  Training Loss  0.0019799168221652508\n",
            "Epoch  13 Batch  474 / 525  Training Loss  0.0036225528456270695\n",
            "Epoch  13 Batch  475 / 525  Training Loss  0.005278590135276318\n",
            "Epoch  13 Batch  476 / 525  Training Loss  0.0027238817419856787\n",
            "Epoch  13 Batch  477 / 525  Training Loss  0.002823497634381056\n",
            "Epoch  13 Batch  478 / 525  Training Loss  0.0018042873125523329\n",
            "Epoch  13 Batch  479 / 525  Training Loss  0.004031510092318058\n",
            "Epoch  13 Batch  480 / 525  Training Loss  0.0014973729848861694\n",
            "Epoch  13 Batch  481 / 525  Training Loss  0.0032072707545012236\n",
            "Epoch  13 Batch  482 / 525  Training Loss  0.0035714316181838512\n",
            "Epoch  13 Batch  483 / 525  Training Loss  0.005307183600962162\n",
            "Epoch  13 Batch  484 / 525  Training Loss  0.009380260482430458\n",
            "Epoch  13 Batch  485 / 525  Training Loss  0.002230701269581914\n",
            "Epoch  13 Batch  486 / 525  Training Loss  0.0013128897408023477\n",
            "Epoch  13 Batch  487 / 525  Training Loss  0.008241998963057995\n",
            "Epoch  13 Batch  488 / 525  Training Loss  0.013747932389378548\n",
            "Epoch  13 Batch  489 / 525  Training Loss  0.004267859272658825\n",
            "Epoch  13 Batch  490 / 525  Training Loss  0.003011370077729225\n",
            "Epoch  13 Batch  491 / 525  Training Loss  0.005577687174081802\n",
            "Epoch  13 Batch  492 / 525  Training Loss  0.003163690911605954\n",
            "Epoch  13 Batch  493 / 525  Training Loss  0.005142534151673317\n",
            "Epoch  13 Batch  494 / 525  Training Loss  0.010707241483032703\n",
            "Epoch  13 Batch  495 / 525  Training Loss  0.003406072035431862\n",
            "Epoch  13 Batch  496 / 525  Training Loss  0.0033706079702824354\n",
            "Epoch  13 Batch  497 / 525  Training Loss  0.007092247251421213\n",
            "Epoch  13 Batch  498 / 525  Training Loss  0.004320692270994186\n",
            "Epoch  13 Batch  499 / 525  Training Loss  0.008649094961583614\n",
            "Epoch  13 Batch  500 / 525  Training Loss  0.002437298186123371\n",
            "Epoch  13 Batch  501 / 525  Training Loss  0.003394570667296648\n",
            "Epoch  13 Batch  502 / 525  Training Loss  0.008845901116728783\n",
            "Epoch  13 Batch  503 / 525  Training Loss  0.004308169707655907\n",
            "Epoch  13 Batch  504 / 525  Training Loss  0.005154164042323828\n",
            "Epoch  13 Batch  505 / 525  Training Loss  0.005188886076211929\n",
            "Epoch  13 Batch  506 / 525  Training Loss  0.0026128278113901615\n",
            "Epoch  13 Batch  507 / 525  Training Loss  0.0023865108378231525\n",
            "Epoch  13 Batch  508 / 525  Training Loss  0.0026622391305863857\n",
            "Epoch  13 Batch  509 / 525  Training Loss  0.003979000262916088\n",
            "Epoch  13 Batch  510 / 525  Training Loss  0.0014449470909312367\n",
            "Epoch  13 Batch  511 / 525  Training Loss  0.003423946676775813\n",
            "Epoch  13 Batch  512 / 525  Training Loss  0.0018882497679442167\n",
            "Epoch  13 Batch  513 / 525  Training Loss  0.003419596701860428\n",
            "Epoch  13 Batch  514 / 525  Training Loss  0.004430626053363085\n",
            "Epoch  13 Batch  515 / 525  Training Loss  0.00342707522213459\n",
            "Epoch  13 Batch  516 / 525  Training Loss  0.00168613123241812\n",
            "Epoch  13 Batch  517 / 525  Training Loss  0.009363392367959023\n",
            "Epoch  13 Batch  518 / 525  Training Loss  0.00480105634778738\n",
            "Epoch  13 Batch  519 / 525  Training Loss  0.004902314394712448\n",
            "Epoch  13 Batch  520 / 525  Training Loss  0.002834153128787875\n",
            "Epoch  13 Batch  521 / 525  Training Loss  0.002180702518671751\n",
            "Epoch  13 Batch  522 / 525  Training Loss  0.002551992889493704\n",
            "Epoch  13 Batch  523 / 525  Training Loss  0.0034572489093989134\n",
            "Epoch  13 Batch  524 / 525  Training Loss  0.0028334083035588264\n",
            "  14    |    -    |   0.004362   |   62.73  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 14\n",
            "Epoch  14 Batch  0 / 525  Training Loss  0.0013611472677439451\n",
            "Epoch  14 Batch  1 / 525  Training Loss  0.0013602112885564566\n",
            "Epoch  14 Batch  2 / 525  Training Loss  0.002212467836216092\n",
            "Epoch  14 Batch  3 / 525  Training Loss  0.004728225059807301\n",
            "Epoch  14 Batch  4 / 525  Training Loss  0.0023752909619361162\n",
            "Epoch  14 Batch  5 / 525  Training Loss  0.0031400423031300306\n",
            "Epoch  14 Batch  6 / 525  Training Loss  0.0009149981779046357\n",
            "Epoch  14 Batch  7 / 525  Training Loss  0.0016678590327501297\n",
            "Epoch  14 Batch  8 / 525  Training Loss  0.0026364619843661785\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  14 Batch  9 / 525  Training Loss  0.0011689031962305307\n",
            "Epoch  14 Batch  10 / 525  Training Loss  0.0006863786256872118\n",
            "Epoch  14 Batch  11 / 525  Training Loss  0.0008966461755335331\n",
            "Epoch  14 Batch  12 / 525  Training Loss  0.0013747927732765675\n",
            "Epoch  14 Batch  13 / 525  Training Loss  0.0013135634362697601\n",
            "Epoch  14 Batch  14 / 525  Training Loss  0.0010633822530508041\n",
            "Epoch  14 Batch  15 / 525  Training Loss  0.0016827418003231287\n",
            "Epoch  14 Batch  16 / 525  Training Loss  0.0010156033094972372\n",
            "Epoch  14 Batch  17 / 525  Training Loss  0.001789440750144422\n",
            "Epoch  14 Batch  18 / 525  Training Loss  0.0024162004701793194\n",
            "Epoch  14 Batch  19 / 525  Training Loss  0.0015626417007297277\n",
            "Epoch  14 Batch  20 / 525  Training Loss  0.0015386685263365507\n",
            "Epoch  14 Batch  21 / 525  Training Loss  0.0035905768163502216\n",
            "Epoch  14 Batch  22 / 525  Training Loss  0.004116024821996689\n",
            "Epoch  14 Batch  23 / 525  Training Loss  0.001047060126438737\n",
            "Epoch  14 Batch  24 / 525  Training Loss  0.0027564927004277706\n",
            "Epoch  14 Batch  25 / 525  Training Loss  0.001500310143455863\n",
            "Epoch  14 Batch  26 / 525  Training Loss  0.0037197619676589966\n",
            "Epoch  14 Batch  27 / 525  Training Loss  0.001622185343876481\n",
            "Epoch  14 Batch  28 / 525  Training Loss  0.0017290916293859482\n",
            "Epoch  14 Batch  29 / 525  Training Loss  0.0015725663397461176\n",
            "Epoch  14 Batch  30 / 525  Training Loss  0.0023574938531965017\n",
            "Epoch  14 Batch  31 / 525  Training Loss  0.0012049437500536442\n",
            "Epoch  14 Batch  32 / 525  Training Loss  0.0013554948382079601\n",
            "Epoch  14 Batch  33 / 525  Training Loss  0.002648375928401947\n",
            "Epoch  14 Batch  34 / 525  Training Loss  0.0025268830358982086\n",
            "Epoch  14 Batch  35 / 525  Training Loss  0.0008320838096551597\n",
            "Epoch  14 Batch  36 / 525  Training Loss  0.0016524192178621888\n",
            "Epoch  14 Batch  37 / 525  Training Loss  0.001727517694234848\n",
            "Epoch  14 Batch  38 / 525  Training Loss  0.007423431612551212\n",
            "Epoch  14 Batch  39 / 525  Training Loss  0.0015193864237517118\n",
            "Epoch  14 Batch  40 / 525  Training Loss  0.0069761513732373714\n",
            "Epoch  14 Batch  41 / 525  Training Loss  0.0048963213339447975\n",
            "Epoch  14 Batch  42 / 525  Training Loss  0.001523484941571951\n",
            "Epoch  14 Batch  43 / 525  Training Loss  0.004949011839926243\n",
            "Epoch  14 Batch  44 / 525  Training Loss  0.001545473001897335\n",
            "Epoch  14 Batch  45 / 525  Training Loss  0.0012566018849611282\n",
            "Epoch  14 Batch  46 / 525  Training Loss  0.0017137229442596436\n",
            "Epoch  14 Batch  47 / 525  Training Loss  0.0011725655058398843\n",
            "Epoch  14 Batch  48 / 525  Training Loss  0.008492570370435715\n",
            "Epoch  14 Batch  49 / 525  Training Loss  0.0011628108331933618\n",
            "Epoch  14 Batch  50 / 525  Training Loss  0.0009362121345475316\n",
            "Epoch  14 Batch  51 / 525  Training Loss  0.0014226922066882253\n",
            "Epoch  14 Batch  52 / 525  Training Loss  0.0024510123766958714\n",
            "Epoch  14 Batch  53 / 525  Training Loss  0.0015224522212520242\n",
            "Epoch  14 Batch  54 / 525  Training Loss  0.006753350142389536\n",
            "Epoch  14 Batch  55 / 525  Training Loss  0.0025860746391117573\n",
            "Epoch  14 Batch  56 / 525  Training Loss  0.0030204199720174074\n",
            "Epoch  14 Batch  57 / 525  Training Loss  0.0007742487941868603\n",
            "Epoch  14 Batch  58 / 525  Training Loss  0.0012115854769945145\n",
            "Epoch  14 Batch  59 / 525  Training Loss  0.004209556151181459\n",
            "Epoch  14 Batch  60 / 525  Training Loss  0.0015185524243861437\n",
            "Epoch  14 Batch  61 / 525  Training Loss  0.003399039851501584\n",
            "Epoch  14 Batch  62 / 525  Training Loss  0.0007731355726718903\n",
            "Epoch  14 Batch  63 / 525  Training Loss  0.0010961821535602212\n",
            "Epoch  14 Batch  64 / 525  Training Loss  0.0033315825276076794\n",
            "Epoch  14 Batch  65 / 525  Training Loss  0.0032384865917265415\n",
            "Epoch  14 Batch  66 / 525  Training Loss  0.000724431243725121\n",
            "Epoch  14 Batch  67 / 525  Training Loss  0.0024592599365860224\n",
            "Epoch  14 Batch  68 / 525  Training Loss  0.0010554587934166193\n",
            "Epoch  14 Batch  69 / 525  Training Loss  0.00192027329467237\n",
            "Epoch  14 Batch  70 / 525  Training Loss  0.0027452579233795404\n",
            "Epoch  14 Batch  71 / 525  Training Loss  0.0024056262336671352\n",
            "Epoch  14 Batch  72 / 525  Training Loss  0.0020200074650347233\n",
            "Epoch  14 Batch  73 / 525  Training Loss  0.00199533486738801\n",
            "Epoch  14 Batch  74 / 525  Training Loss  0.0012639565393328667\n",
            "Epoch  14 Batch  75 / 525  Training Loss  0.0014334778534248471\n",
            "Epoch  14 Batch  76 / 525  Training Loss  0.0017685085767880082\n",
            "Epoch  14 Batch  77 / 525  Training Loss  0.001264789141714573\n",
            "Epoch  14 Batch  78 / 525  Training Loss  0.0020623926538974047\n",
            "Epoch  14 Batch  79 / 525  Training Loss  0.001276352908462286\n",
            "Epoch  14 Batch  80 / 525  Training Loss  0.0009374541114084423\n",
            "Epoch  14 Batch  81 / 525  Training Loss  0.005183605011552572\n",
            "Epoch  14 Batch  82 / 525  Training Loss  0.020574456080794334\n",
            "Epoch  14 Batch  83 / 525  Training Loss  0.004335511941462755\n",
            "Epoch  14 Batch  84 / 525  Training Loss  0.0015005016466602683\n",
            "Epoch  14 Batch  85 / 525  Training Loss  0.0023790502455085516\n",
            "Epoch  14 Batch  86 / 525  Training Loss  0.0020371084101498127\n",
            "Epoch  14 Batch  87 / 525  Training Loss  0.0029920863453298807\n",
            "Epoch  14 Batch  88 / 525  Training Loss  0.0019481390481814742\n",
            "Epoch  14 Batch  89 / 525  Training Loss  0.0009420196292921901\n",
            "Epoch  14 Batch  90 / 525  Training Loss  0.005434870719909668\n",
            "Epoch  14 Batch  91 / 525  Training Loss  0.0014013889012858272\n",
            "Epoch  14 Batch  92 / 525  Training Loss  0.0020654741674661636\n",
            "Epoch  14 Batch  93 / 525  Training Loss  0.0022431733086705208\n",
            "Epoch  14 Batch  94 / 525  Training Loss  0.0014383109519258142\n",
            "Epoch  14 Batch  95 / 525  Training Loss  0.0012961619067937136\n",
            "Epoch  14 Batch  96 / 525  Training Loss  0.00470044557005167\n",
            "Epoch  14 Batch  97 / 525  Training Loss  0.001590131432749331\n",
            "Epoch  14 Batch  98 / 525  Training Loss  0.0018648620462045074\n",
            "Epoch  14 Batch  99 / 525  Training Loss  0.01066133938729763\n",
            "Epoch  14 Batch  100 / 525  Training Loss  0.0009295960189774632\n",
            "Epoch  14 Batch  101 / 525  Training Loss  0.0008547391626052558\n",
            "Epoch  14 Batch  102 / 525  Training Loss  0.0011178625281900167\n",
            "Epoch  14 Batch  103 / 525  Training Loss  0.0012950525851920247\n",
            "Epoch  14 Batch  104 / 525  Training Loss  0.0028609675355255604\n",
            "Epoch  14 Batch  105 / 525  Training Loss  0.0009354121284559369\n",
            "Epoch  14 Batch  106 / 525  Training Loss  0.0013837700244039297\n",
            "Epoch  14 Batch  107 / 525  Training Loss  0.0015519122825935483\n",
            "Epoch  14 Batch  108 / 525  Training Loss  0.0006305655115284026\n",
            "Epoch  14 Batch  109 / 525  Training Loss  0.002137398812919855\n",
            "Epoch  14 Batch  110 / 525  Training Loss  0.0005583907477557659\n",
            "Epoch  14 Batch  111 / 525  Training Loss  0.0008687393856234848\n",
            "Epoch  14 Batch  112 / 525  Training Loss  0.004099370911717415\n",
            "Epoch  14 Batch  113 / 525  Training Loss  0.0021543989423662424\n",
            "Epoch  14 Batch  114 / 525  Training Loss  0.002483909949660301\n",
            "Epoch  14 Batch  115 / 525  Training Loss  0.0007405771757476032\n",
            "Epoch  14 Batch  116 / 525  Training Loss  0.0007687186589464545\n",
            "Epoch  14 Batch  117 / 525  Training Loss  0.001484856242313981\n",
            "Epoch  14 Batch  118 / 525  Training Loss  0.0010514587629586458\n",
            "Epoch  14 Batch  119 / 525  Training Loss  0.0012641337234526873\n",
            "Epoch  14 Batch  120 / 525  Training Loss  0.0013466000091284513\n",
            "Epoch  14 Batch  121 / 525  Training Loss  0.008214562200009823\n",
            "Epoch  14 Batch  122 / 525  Training Loss  0.0014619558351114392\n",
            "Epoch  14 Batch  123 / 525  Training Loss  0.0012977465521544218\n",
            "Epoch  14 Batch  124 / 525  Training Loss  0.0008812706219032407\n",
            "Epoch  14 Batch  125 / 525  Training Loss  0.0018926674965769053\n",
            "Epoch  14 Batch  126 / 525  Training Loss  0.0022246413864195347\n",
            "Epoch  14 Batch  127 / 525  Training Loss  0.001723868539556861\n",
            "Epoch  14 Batch  128 / 525  Training Loss  0.0010600622044876218\n",
            "Epoch  14 Batch  129 / 525  Training Loss  0.0009478164138272405\n",
            "Epoch  14 Batch  130 / 525  Training Loss  0.0014260240131989121\n",
            "Epoch  14 Batch  131 / 525  Training Loss  0.001071221544407308\n",
            "Epoch  14 Batch  132 / 525  Training Loss  0.0012298490619286895\n",
            "Epoch  14 Batch  133 / 525  Training Loss  0.0012414198135957122\n",
            "Epoch  14 Batch  134 / 525  Training Loss  0.0010294641833752394\n",
            "Epoch  14 Batch  135 / 525  Training Loss  0.0021642481442540884\n",
            "Epoch  14 Batch  136 / 525  Training Loss  0.0007362061878666282\n",
            "Epoch  14 Batch  137 / 525  Training Loss  0.0011468701995909214\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  14 Batch  138 / 525  Training Loss  0.0019067550310865045\n",
            "Epoch  14 Batch  139 / 525  Training Loss  0.0011443719267845154\n",
            "Epoch  14 Batch  140 / 525  Training Loss  0.0022262590937316418\n",
            "Epoch  14 Batch  141 / 525  Training Loss  0.0034456364810466766\n",
            "Epoch  14 Batch  142 / 525  Training Loss  0.0012034613173455\n",
            "Epoch  14 Batch  143 / 525  Training Loss  0.0011452898615971208\n",
            "Epoch  14 Batch  144 / 525  Training Loss  0.0032092779874801636\n",
            "Epoch  14 Batch  145 / 525  Training Loss  0.0010314605897292495\n",
            "Epoch  14 Batch  146 / 525  Training Loss  0.0010723115410655737\n",
            "Epoch  14 Batch  147 / 525  Training Loss  0.0010147407883778214\n",
            "Epoch  14 Batch  148 / 525  Training Loss  0.0014997145626693964\n",
            "Epoch  14 Batch  149 / 525  Training Loss  0.001795662334188819\n",
            "Epoch  14 Batch  150 / 525  Training Loss  0.0019316805992275476\n",
            "Epoch  14 Batch  151 / 525  Training Loss  0.001186663517728448\n",
            "Epoch  14 Batch  152 / 525  Training Loss  0.003639217931777239\n",
            "Epoch  14 Batch  153 / 525  Training Loss  0.0005123828887008131\n",
            "Epoch  14 Batch  154 / 525  Training Loss  0.0029740273021161556\n",
            "Epoch  14 Batch  155 / 525  Training Loss  0.0012108429800719023\n",
            "Epoch  14 Batch  156 / 525  Training Loss  0.0012077824212610722\n",
            "Epoch  14 Batch  157 / 525  Training Loss  0.002979852491989732\n",
            "Epoch  14 Batch  158 / 525  Training Loss  0.0009742116671986878\n",
            "Epoch  14 Batch  159 / 525  Training Loss  0.0014138843398541212\n",
            "Epoch  14 Batch  160 / 525  Training Loss  0.002949835965409875\n",
            "Epoch  14 Batch  161 / 525  Training Loss  0.001500263111665845\n",
            "Epoch  14 Batch  162 / 525  Training Loss  0.0017170322826132178\n",
            "Epoch  14 Batch  163 / 525  Training Loss  0.0010347372153773904\n",
            "Epoch  14 Batch  164 / 525  Training Loss  0.0014233060646802187\n",
            "Epoch  14 Batch  165 / 525  Training Loss  0.00044474255992099643\n",
            "Epoch  14 Batch  166 / 525  Training Loss  0.0013383696787059307\n",
            "Epoch  14 Batch  167 / 525  Training Loss  0.0010252520442008972\n",
            "Epoch  14 Batch  168 / 525  Training Loss  0.004627952352166176\n",
            "Epoch  14 Batch  169 / 525  Training Loss  0.0007034171721898019\n",
            "Epoch  14 Batch  170 / 525  Training Loss  0.0012954416451975703\n",
            "Epoch  14 Batch  171 / 525  Training Loss  0.004117975011467934\n",
            "Epoch  14 Batch  172 / 525  Training Loss  0.0010002475464716554\n",
            "Epoch  14 Batch  173 / 525  Training Loss  0.0006883595488034189\n",
            "Epoch  14 Batch  174 / 525  Training Loss  0.0032533779740333557\n",
            "Epoch  14 Batch  175 / 525  Training Loss  0.0006694586481899023\n",
            "Epoch  14 Batch  176 / 525  Training Loss  0.002102916594594717\n",
            "Epoch  14 Batch  177 / 525  Training Loss  0.0021336725912988186\n",
            "Epoch  14 Batch  178 / 525  Training Loss  0.0009844861924648285\n",
            "Epoch  14 Batch  179 / 525  Training Loss  0.0017403170932084322\n",
            "Epoch  14 Batch  180 / 525  Training Loss  0.000642679980956018\n",
            "Epoch  14 Batch  181 / 525  Training Loss  0.000579500338062644\n",
            "Epoch  14 Batch  182 / 525  Training Loss  0.0017635596450418234\n",
            "Epoch  14 Batch  183 / 525  Training Loss  0.0023494160268455744\n",
            "Epoch  14 Batch  184 / 525  Training Loss  0.003429191652685404\n",
            "Epoch  14 Batch  185 / 525  Training Loss  0.0007527429843321443\n",
            "Epoch  14 Batch  186 / 525  Training Loss  0.0010703494772315025\n",
            "Epoch  14 Batch  187 / 525  Training Loss  0.0026341588236391544\n",
            "Epoch  14 Batch  188 / 525  Training Loss  0.0018761062528938055\n",
            "Epoch  14 Batch  189 / 525  Training Loss  0.0004888219991698861\n",
            "Epoch  14 Batch  190 / 525  Training Loss  0.0012926385970786214\n",
            "Epoch  14 Batch  191 / 525  Training Loss  0.0018243960803374648\n",
            "Epoch  14 Batch  192 / 525  Training Loss  0.003771565156057477\n",
            "Epoch  14 Batch  193 / 525  Training Loss  0.0016089724376797676\n",
            "Epoch  14 Batch  194 / 525  Training Loss  0.0013732184888795018\n",
            "Epoch  14 Batch  195 / 525  Training Loss  0.0009641284123063087\n",
            "Epoch  14 Batch  196 / 525  Training Loss  0.0012289704754948616\n",
            "Epoch  14 Batch  197 / 525  Training Loss  0.0019627560395747423\n",
            "Epoch  14 Batch  198 / 525  Training Loss  0.0013929316774010658\n",
            "Epoch  14 Batch  199 / 525  Training Loss  0.0036072328221052885\n",
            "Epoch  14 Batch  200 / 525  Training Loss  0.0008863309631124139\n",
            "Epoch  14 Batch  201 / 525  Training Loss  0.0013999457005411386\n",
            "Epoch  14 Batch  202 / 525  Training Loss  0.002184505108743906\n",
            "Epoch  14 Batch  203 / 525  Training Loss  0.0013901882339268923\n",
            "Epoch  14 Batch  204 / 525  Training Loss  0.004813441075384617\n",
            "Epoch  14 Batch  205 / 525  Training Loss  0.002221033675596118\n",
            "Epoch  14 Batch  206 / 525  Training Loss  0.0013394022826105356\n",
            "Epoch  14 Batch  207 / 525  Training Loss  0.001376359025016427\n",
            "Epoch  14 Batch  208 / 525  Training Loss  0.0006556808948516846\n",
            "Epoch  14 Batch  209 / 525  Training Loss  0.0017244697082787752\n",
            "Epoch  14 Batch  210 / 525  Training Loss  0.0011639940785244107\n",
            "Epoch  14 Batch  211 / 525  Training Loss  0.0034965728409588337\n",
            "Epoch  14 Batch  212 / 525  Training Loss  0.0029056044295430183\n",
            "Epoch  14 Batch  213 / 525  Training Loss  0.0017767999088391662\n",
            "Epoch  14 Batch  214 / 525  Training Loss  0.014275516383349895\n",
            "Epoch  14 Batch  215 / 525  Training Loss  0.0014878560323268175\n",
            "Epoch  14 Batch  216 / 525  Training Loss  0.002392533700913191\n",
            "Epoch  14 Batch  217 / 525  Training Loss  0.0031951195560395718\n",
            "Epoch  14 Batch  218 / 525  Training Loss  0.001977696316316724\n",
            "Epoch  14 Batch  219 / 525  Training Loss  0.0029118217062205076\n",
            "Epoch  14 Batch  220 / 525  Training Loss  0.001718893414363265\n",
            "Epoch  14 Batch  221 / 525  Training Loss  0.0022734515368938446\n",
            "Epoch  14 Batch  222 / 525  Training Loss  0.00103598996065557\n",
            "Epoch  14 Batch  223 / 525  Training Loss  0.002400037832558155\n",
            "Epoch  14 Batch  224 / 525  Training Loss  0.002723424229770899\n",
            "Epoch  14 Batch  225 / 525  Training Loss  0.002548995427787304\n",
            "Epoch  14 Batch  226 / 525  Training Loss  0.0007109881262294948\n",
            "Epoch  14 Batch  227 / 525  Training Loss  0.003254024777561426\n",
            "Epoch  14 Batch  228 / 525  Training Loss  0.0021762591786682606\n",
            "Epoch  14 Batch  229 / 525  Training Loss  0.002051746239885688\n",
            "Epoch  14 Batch  230 / 525  Training Loss  0.0057953898794949055\n",
            "Epoch  14 Batch  231 / 525  Training Loss  0.002792294602841139\n",
            "Epoch  14 Batch  232 / 525  Training Loss  0.001922704977914691\n",
            "Epoch  14 Batch  233 / 525  Training Loss  0.0006715700728818774\n",
            "Epoch  14 Batch  234 / 525  Training Loss  0.0014196863630786538\n",
            "Epoch  14 Batch  235 / 525  Training Loss  0.0009085489436984062\n",
            "Epoch  14 Batch  236 / 525  Training Loss  0.0017034709453582764\n",
            "Epoch  14 Batch  237 / 525  Training Loss  0.001133312238380313\n",
            "Epoch  14 Batch  238 / 525  Training Loss  0.004150525666773319\n",
            "Epoch  14 Batch  239 / 525  Training Loss  0.0020483715925365686\n",
            "Epoch  14 Batch  240 / 525  Training Loss  0.0008562365546822548\n",
            "Epoch  14 Batch  241 / 525  Training Loss  0.0009028037311509252\n",
            "Epoch  14 Batch  242 / 525  Training Loss  0.0013595456257462502\n",
            "Epoch  14 Batch  243 / 525  Training Loss  0.0014380097854882479\n",
            "Epoch  14 Batch  244 / 525  Training Loss  0.0027013712096959352\n",
            "Epoch  14 Batch  245 / 525  Training Loss  0.001348008867353201\n",
            "Epoch  14 Batch  246 / 525  Training Loss  0.001134297694079578\n",
            "Epoch  14 Batch  247 / 525  Training Loss  0.0011837768834084272\n",
            "Epoch  14 Batch  248 / 525  Training Loss  0.001318309223279357\n",
            "Epoch  14 Batch  249 / 525  Training Loss  0.001253766124136746\n",
            "Epoch  14 Batch  250 / 525  Training Loss  0.0007067205151543021\n",
            "Epoch  14 Batch  251 / 525  Training Loss  0.002290930598974228\n",
            "Epoch  14 Batch  252 / 525  Training Loss  0.0011265098582953215\n",
            "Epoch  14 Batch  253 / 525  Training Loss  0.0014285813085734844\n",
            "Epoch  14 Batch  254 / 525  Training Loss  0.002214676234871149\n",
            "Epoch  14 Batch  255 / 525  Training Loss  0.0010861508781090379\n",
            "Epoch  14 Batch  256 / 525  Training Loss  0.0016981095541268587\n",
            "Epoch  14 Batch  257 / 525  Training Loss  0.0011097388342022896\n",
            "Epoch  14 Batch  258 / 525  Training Loss  0.001095608458854258\n",
            "Epoch  14 Batch  259 / 525  Training Loss  0.0014764905208721757\n",
            "Epoch  14 Batch  260 / 525  Training Loss  0.003344662021845579\n",
            "Epoch  14 Batch  261 / 525  Training Loss  0.0019336484838277102\n",
            "Epoch  14 Batch  262 / 525  Training Loss  0.0009194104932248592\n",
            "Epoch  14 Batch  263 / 525  Training Loss  0.0012199773918837309\n",
            "Epoch  14 Batch  264 / 525  Training Loss  0.0015198023756965995\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  14 Batch  265 / 525  Training Loss  0.0005241670296527445\n",
            "Epoch  14 Batch  266 / 525  Training Loss  0.0012697696220129728\n",
            "Epoch  14 Batch  267 / 525  Training Loss  0.002223412739112973\n",
            "Epoch  14 Batch  268 / 525  Training Loss  0.0012129675596952438\n",
            "Epoch  14 Batch  269 / 525  Training Loss  0.0014674313133582473\n",
            "Epoch  14 Batch  270 / 525  Training Loss  0.001235300675034523\n",
            "Epoch  14 Batch  271 / 525  Training Loss  0.0027228761464357376\n",
            "Epoch  14 Batch  272 / 525  Training Loss  0.0012846505269408226\n",
            "Epoch  14 Batch  273 / 525  Training Loss  0.0010169639717787504\n",
            "Epoch  14 Batch  274 / 525  Training Loss  0.0013505273964256048\n",
            "Epoch  14 Batch  275 / 525  Training Loss  0.0013177059590816498\n",
            "Epoch  14 Batch  276 / 525  Training Loss  0.0018609855324029922\n",
            "Epoch  14 Batch  277 / 525  Training Loss  0.0008676268043927848\n",
            "Epoch  14 Batch  278 / 525  Training Loss  0.008017993532121181\n",
            "Epoch  14 Batch  279 / 525  Training Loss  0.0030238882172852755\n",
            "Epoch  14 Batch  280 / 525  Training Loss  0.0010513250017538667\n",
            "Epoch  14 Batch  281 / 525  Training Loss  0.0011805433314293623\n",
            "Epoch  14 Batch  282 / 525  Training Loss  0.005134359002113342\n",
            "Epoch  14 Batch  283 / 525  Training Loss  0.001752393669448793\n",
            "Epoch  14 Batch  284 / 525  Training Loss  0.004158425144851208\n",
            "Epoch  14 Batch  285 / 525  Training Loss  0.004237485118210316\n",
            "Epoch  14 Batch  286 / 525  Training Loss  0.008883561007678509\n",
            "Epoch  14 Batch  287 / 525  Training Loss  0.002237228211015463\n",
            "Epoch  14 Batch  288 / 525  Training Loss  0.0008164833998307586\n",
            "Epoch  14 Batch  289 / 525  Training Loss  0.0005412337486632168\n",
            "Epoch  14 Batch  290 / 525  Training Loss  0.0005106874159537256\n",
            "Epoch  14 Batch  291 / 525  Training Loss  0.0024211453273892403\n",
            "Epoch  14 Batch  292 / 525  Training Loss  0.0028244543354958296\n",
            "Epoch  14 Batch  293 / 525  Training Loss  0.0011041013058274984\n",
            "Epoch  14 Batch  294 / 525  Training Loss  0.003018233459442854\n",
            "Epoch  14 Batch  295 / 525  Training Loss  0.0033942193258553743\n",
            "Epoch  14 Batch  296 / 525  Training Loss  0.0011459593661129475\n",
            "Epoch  14 Batch  297 / 525  Training Loss  0.006091618910431862\n",
            "Epoch  14 Batch  298 / 525  Training Loss  0.0031078869942575693\n",
            "Epoch  14 Batch  299 / 525  Training Loss  0.0023377190809696913\n",
            "Epoch  14 Batch  300 / 525  Training Loss  0.0010320271831005812\n",
            "Epoch  14 Batch  301 / 525  Training Loss  0.0014721204061061144\n",
            "Epoch  14 Batch  302 / 525  Training Loss  0.0012649573618546128\n",
            "Epoch  14 Batch  303 / 525  Training Loss  0.003180823056027293\n",
            "Epoch  14 Batch  304 / 525  Training Loss  0.0017077293014153838\n",
            "Epoch  14 Batch  305 / 525  Training Loss  0.0032483735121786594\n",
            "Epoch  14 Batch  306 / 525  Training Loss  0.00209330883808434\n",
            "Epoch  14 Batch  307 / 525  Training Loss  0.001158239901997149\n",
            "Epoch  14 Batch  308 / 525  Training Loss  0.0011017729993909597\n",
            "Epoch  14 Batch  309 / 525  Training Loss  0.002623403910547495\n",
            "Epoch  14 Batch  310 / 525  Training Loss  0.0007591125322505832\n",
            "Epoch  14 Batch  311 / 525  Training Loss  0.003212863579392433\n",
            "Epoch  14 Batch  312 / 525  Training Loss  0.00793984904885292\n",
            "Epoch  14 Batch  313 / 525  Training Loss  0.001043691416271031\n",
            "Epoch  14 Batch  314 / 525  Training Loss  0.0013593758922070265\n",
            "Epoch  14 Batch  315 / 525  Training Loss  0.0007395430584438145\n",
            "Epoch  14 Batch  316 / 525  Training Loss  0.0019148228457197547\n",
            "Epoch  14 Batch  317 / 525  Training Loss  0.002077113138511777\n",
            "Epoch  14 Batch  318 / 525  Training Loss  0.0027830428443849087\n",
            "Epoch  14 Batch  319 / 525  Training Loss  0.009552331641316414\n",
            "Epoch  14 Batch  320 / 525  Training Loss  0.003479460719972849\n",
            "Epoch  14 Batch  321 / 525  Training Loss  0.0019985437393188477\n",
            "Epoch  14 Batch  322 / 525  Training Loss  0.0013633056078106165\n",
            "Epoch  14 Batch  323 / 525  Training Loss  0.002505615819245577\n",
            "Epoch  14 Batch  324 / 525  Training Loss  0.0025768850464373827\n",
            "Epoch  14 Batch  325 / 525  Training Loss  0.004470845218747854\n",
            "Epoch  14 Batch  326 / 525  Training Loss  0.0019849040545523167\n",
            "Epoch  14 Batch  327 / 525  Training Loss  0.0013640624238178134\n",
            "Epoch  14 Batch  328 / 525  Training Loss  0.0032493837643414736\n",
            "Epoch  14 Batch  329 / 525  Training Loss  0.0021018267143517733\n",
            "Epoch  14 Batch  330 / 525  Training Loss  0.0016516577452421188\n",
            "Epoch  14 Batch  331 / 525  Training Loss  0.0018274271860718727\n",
            "Epoch  14 Batch  332 / 525  Training Loss  0.0015899321297183633\n",
            "Epoch  14 Batch  333 / 525  Training Loss  0.0016975949984043837\n",
            "Epoch  14 Batch  334 / 525  Training Loss  0.0024491569492965937\n",
            "Epoch  14 Batch  335 / 525  Training Loss  0.0027480414137244225\n",
            "Epoch  14 Batch  336 / 525  Training Loss  0.002904526423662901\n",
            "Epoch  14 Batch  337 / 525  Training Loss  0.002317331498488784\n",
            "Epoch  14 Batch  338 / 525  Training Loss  0.0008184470352716744\n",
            "Epoch  14 Batch  339 / 525  Training Loss  0.003003699705004692\n",
            "Epoch  14 Batch  340 / 525  Training Loss  0.0015240961220115423\n",
            "Epoch  14 Batch  341 / 525  Training Loss  0.004079182166606188\n",
            "Epoch  14 Batch  342 / 525  Training Loss  0.001182448584586382\n",
            "Epoch  14 Batch  343 / 525  Training Loss  0.004407787229865789\n",
            "Epoch  14 Batch  344 / 525  Training Loss  0.002611869014799595\n",
            "Epoch  14 Batch  345 / 525  Training Loss  0.0018742510583251715\n",
            "Epoch  14 Batch  346 / 525  Training Loss  0.0027112674433737993\n",
            "Epoch  14 Batch  347 / 525  Training Loss  0.00191678071860224\n",
            "Epoch  14 Batch  348 / 525  Training Loss  0.0012411928037181497\n",
            "Epoch  14 Batch  349 / 525  Training Loss  0.0014432877069339156\n",
            "Epoch  14 Batch  350 / 525  Training Loss  0.002369966823607683\n",
            "Epoch  14 Batch  351 / 525  Training Loss  0.001153938821516931\n",
            "Epoch  14 Batch  352 / 525  Training Loss  0.0023669067304581404\n",
            "Epoch  14 Batch  353 / 525  Training Loss  0.0016550871077924967\n",
            "Epoch  14 Batch  354 / 525  Training Loss  0.0011299879988655448\n",
            "Epoch  14 Batch  355 / 525  Training Loss  0.001218166435137391\n",
            "Epoch  14 Batch  356 / 525  Training Loss  0.0023308098316192627\n",
            "Epoch  14 Batch  357 / 525  Training Loss  0.0012530161766335368\n",
            "Epoch  14 Batch  358 / 525  Training Loss  0.0007762267487123609\n",
            "Epoch  14 Batch  359 / 525  Training Loss  0.0013508604606613517\n",
            "Epoch  14 Batch  360 / 525  Training Loss  0.002476170426234603\n",
            "Epoch  14 Batch  361 / 525  Training Loss  0.0010880156187340617\n",
            "Epoch  14 Batch  362 / 525  Training Loss  0.001022731768898666\n",
            "Epoch  14 Batch  363 / 525  Training Loss  0.0006067000795155764\n",
            "Epoch  14 Batch  364 / 525  Training Loss  0.0017326390370726585\n",
            "Epoch  14 Batch  365 / 525  Training Loss  0.002073040697723627\n",
            "Epoch  14 Batch  366 / 525  Training Loss  0.0008996390970423818\n",
            "Epoch  14 Batch  367 / 525  Training Loss  0.0020876675844192505\n",
            "Epoch  14 Batch  368 / 525  Training Loss  0.002204506890848279\n",
            "Epoch  14 Batch  369 / 525  Training Loss  0.0008063853019848466\n",
            "Epoch  14 Batch  370 / 525  Training Loss  0.001102584763430059\n",
            "Epoch  14 Batch  371 / 525  Training Loss  0.0015693653840571642\n",
            "Epoch  14 Batch  372 / 525  Training Loss  0.0014184393221512437\n",
            "Epoch  14 Batch  373 / 525  Training Loss  0.004128224216401577\n",
            "Epoch  14 Batch  374 / 525  Training Loss  0.0033256628084927797\n",
            "Epoch  14 Batch  375 / 525  Training Loss  0.0011794602032750845\n",
            "Epoch  14 Batch  376 / 525  Training Loss  0.0018625024240463972\n",
            "Epoch  14 Batch  377 / 525  Training Loss  0.0024753573816269636\n",
            "Epoch  14 Batch  378 / 525  Training Loss  0.0012098526349291205\n",
            "Epoch  14 Batch  379 / 525  Training Loss  0.002354671014472842\n",
            "Epoch  14 Batch  380 / 525  Training Loss  0.002807851415127516\n",
            "Epoch  14 Batch  381 / 525  Training Loss  0.0030747209675610065\n",
            "Epoch  14 Batch  382 / 525  Training Loss  0.0025631634052842855\n",
            "Epoch  14 Batch  383 / 525  Training Loss  0.0007006347877904773\n",
            "Epoch  14 Batch  384 / 525  Training Loss  0.001269627595320344\n",
            "Epoch  14 Batch  385 / 525  Training Loss  0.0011424385011196136\n",
            "Epoch  14 Batch  386 / 525  Training Loss  0.003904257668182254\n",
            "Epoch  14 Batch  387 / 525  Training Loss  0.0013420319883152843\n",
            "Epoch  14 Batch  388 / 525  Training Loss  0.0015482831513509154\n",
            "Epoch  14 Batch  389 / 525  Training Loss  0.000908123329281807\n",
            "Epoch  14 Batch  390 / 525  Training Loss  0.0022399674635380507\n",
            "Epoch  14 Batch  391 / 525  Training Loss  0.01223683264106512\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  14 Batch  392 / 525  Training Loss  0.0014281593030318618\n",
            "Epoch  14 Batch  393 / 525  Training Loss  0.0011675420682877302\n",
            "Epoch  14 Batch  394 / 525  Training Loss  0.0036921524442732334\n",
            "Epoch  14 Batch  395 / 525  Training Loss  0.0012688824208453298\n",
            "Epoch  14 Batch  396 / 525  Training Loss  0.001243441365659237\n",
            "Epoch  14 Batch  397 / 525  Training Loss  0.007634577341377735\n",
            "Epoch  14 Batch  398 / 525  Training Loss  0.003037289483472705\n",
            "Epoch  14 Batch  399 / 525  Training Loss  0.002867828356102109\n",
            "Epoch  14 Batch  400 / 525  Training Loss  0.013795435428619385\n",
            "Epoch  14 Batch  401 / 525  Training Loss  0.0027190917171537876\n",
            "Epoch  14 Batch  402 / 525  Training Loss  0.001340601360425353\n",
            "Epoch  14 Batch  403 / 525  Training Loss  0.0015174000291153789\n",
            "Epoch  14 Batch  404 / 525  Training Loss  0.0012132101692259312\n",
            "Epoch  14 Batch  405 / 525  Training Loss  0.0007092211162671447\n",
            "Epoch  14 Batch  406 / 525  Training Loss  0.0009079249575734138\n",
            "Epoch  14 Batch  407 / 525  Training Loss  0.0016698210965842009\n",
            "Epoch  14 Batch  408 / 525  Training Loss  0.0007234472432173789\n",
            "Epoch  14 Batch  409 / 525  Training Loss  0.0013618323719128966\n",
            "Epoch  14 Batch  410 / 525  Training Loss  0.001525940140709281\n",
            "Epoch  14 Batch  411 / 525  Training Loss  0.0014737251913174987\n",
            "Epoch  14 Batch  412 / 525  Training Loss  0.0010483551304787397\n",
            "Epoch  14 Batch  413 / 525  Training Loss  0.0016767369816079736\n",
            "Epoch  14 Batch  414 / 525  Training Loss  0.0008669007802382112\n",
            "Epoch  14 Batch  415 / 525  Training Loss  0.0041696177795529366\n",
            "Epoch  14 Batch  416 / 525  Training Loss  0.001839285483583808\n",
            "Epoch  14 Batch  417 / 525  Training Loss  0.0010808274382725358\n",
            "Epoch  14 Batch  418 / 525  Training Loss  0.001205378444865346\n",
            "Epoch  14 Batch  419 / 525  Training Loss  0.005520857870578766\n",
            "Epoch  14 Batch  420 / 525  Training Loss  0.005059418734163046\n",
            "Epoch  14 Batch  421 / 525  Training Loss  0.0014953558566048741\n",
            "Epoch  14 Batch  422 / 525  Training Loss  0.007874397560954094\n",
            "Epoch  14 Batch  423 / 525  Training Loss  0.003050323110073805\n",
            "Epoch  14 Batch  424 / 525  Training Loss  0.002469342900440097\n",
            "Epoch  14 Batch  425 / 525  Training Loss  0.0023076380603015423\n",
            "Epoch  14 Batch  426 / 525  Training Loss  0.0026229219511151314\n",
            "Epoch  14 Batch  427 / 525  Training Loss  0.0014518348034471273\n",
            "Epoch  14 Batch  428 / 525  Training Loss  0.001642392249777913\n",
            "Epoch  14 Batch  429 / 525  Training Loss  0.0019514696905389428\n",
            "Epoch  14 Batch  430 / 525  Training Loss  0.003207404864951968\n",
            "Epoch  14 Batch  431 / 525  Training Loss  0.0011418869253247976\n",
            "Epoch  14 Batch  432 / 525  Training Loss  0.0006182571523822844\n",
            "Epoch  14 Batch  433 / 525  Training Loss  0.0013644505525007844\n",
            "Epoch  14 Batch  434 / 525  Training Loss  0.0013530992437154055\n",
            "Epoch  14 Batch  435 / 525  Training Loss  0.010937188751995564\n",
            "Epoch  14 Batch  436 / 525  Training Loss  0.0008858476066961884\n",
            "Epoch  14 Batch  437 / 525  Training Loss  0.002807576907798648\n",
            "Epoch  14 Batch  438 / 525  Training Loss  0.0011417826171964407\n",
            "Epoch  14 Batch  439 / 525  Training Loss  0.0013833293924108148\n",
            "Epoch  14 Batch  440 / 525  Training Loss  0.0013045725645497441\n",
            "Epoch  14 Batch  441 / 525  Training Loss  0.003431233810260892\n",
            "Epoch  14 Batch  442 / 525  Training Loss  0.002139248186722398\n",
            "Epoch  14 Batch  443 / 525  Training Loss  0.001029644743539393\n",
            "Epoch  14 Batch  444 / 525  Training Loss  0.0009347576415166259\n",
            "Epoch  14 Batch  445 / 525  Training Loss  0.0050009796395897865\n",
            "Epoch  14 Batch  446 / 525  Training Loss  0.002752018626779318\n",
            "Epoch  14 Batch  447 / 525  Training Loss  0.0008911003242246807\n",
            "Epoch  14 Batch  448 / 525  Training Loss  0.0023813103325664997\n",
            "Epoch  14 Batch  449 / 525  Training Loss  0.005519442725926638\n",
            "Epoch  14 Batch  450 / 525  Training Loss  0.0009696866618469357\n",
            "Epoch  14 Batch  451 / 525  Training Loss  0.0007670168997719884\n",
            "Epoch  14 Batch  452 / 525  Training Loss  0.005464540328830481\n",
            "Epoch  14 Batch  453 / 525  Training Loss  0.007308598607778549\n",
            "Epoch  14 Batch  454 / 525  Training Loss  0.001712640980258584\n",
            "Epoch  14 Batch  455 / 525  Training Loss  0.0016854737186804414\n",
            "Epoch  14 Batch  456 / 525  Training Loss  0.0014715857105329633\n",
            "Epoch  14 Batch  457 / 525  Training Loss  0.0008893135818652809\n",
            "Epoch  14 Batch  458 / 525  Training Loss  0.002515299478545785\n",
            "Epoch  14 Batch  459 / 525  Training Loss  0.0016733038937672973\n",
            "Epoch  14 Batch  460 / 525  Training Loss  0.0017199169378727674\n",
            "Epoch  14 Batch  461 / 525  Training Loss  0.0008365726098418236\n",
            "Epoch  14 Batch  462 / 525  Training Loss  0.00138622778467834\n",
            "Epoch  14 Batch  463 / 525  Training Loss  0.0008530603954568505\n",
            "Epoch  14 Batch  464 / 525  Training Loss  0.0009259477374143898\n",
            "Epoch  14 Batch  465 / 525  Training Loss  0.0010960327927023172\n",
            "Epoch  14 Batch  466 / 525  Training Loss  0.003656376153230667\n",
            "Epoch  14 Batch  467 / 525  Training Loss  0.0011715155560523272\n",
            "Epoch  14 Batch  468 / 525  Training Loss  0.002535546664148569\n",
            "Epoch  14 Batch  469 / 525  Training Loss  0.0011575609678402543\n",
            "Epoch  14 Batch  470 / 525  Training Loss  0.0024275784380733967\n",
            "Epoch  14 Batch  471 / 525  Training Loss  0.0008252851548604667\n",
            "Epoch  14 Batch  472 / 525  Training Loss  0.0017441741656512022\n",
            "Epoch  14 Batch  473 / 525  Training Loss  0.001422568573616445\n",
            "Epoch  14 Batch  474 / 525  Training Loss  0.0015096670249477029\n",
            "Epoch  14 Batch  475 / 525  Training Loss  0.002035600831732154\n",
            "Epoch  14 Batch  476 / 525  Training Loss  0.0019303520675748587\n",
            "Epoch  14 Batch  477 / 525  Training Loss  0.001275672228075564\n",
            "Epoch  14 Batch  478 / 525  Training Loss  0.0008229499799199402\n",
            "Epoch  14 Batch  479 / 525  Training Loss  0.0017277009319514036\n",
            "Epoch  14 Batch  480 / 525  Training Loss  0.004287746734917164\n",
            "Epoch  14 Batch  481 / 525  Training Loss  0.002937130630016327\n",
            "Epoch  14 Batch  482 / 525  Training Loss  0.000859201536513865\n",
            "Epoch  14 Batch  483 / 525  Training Loss  0.0013066448736935854\n",
            "Epoch  14 Batch  484 / 525  Training Loss  0.006610876880586147\n",
            "Epoch  14 Batch  485 / 525  Training Loss  0.0017092194175347686\n",
            "Epoch  14 Batch  486 / 525  Training Loss  0.002109625143930316\n",
            "Epoch  14 Batch  487 / 525  Training Loss  0.0011943267891183496\n",
            "Epoch  14 Batch  488 / 525  Training Loss  0.005104512441903353\n",
            "Epoch  14 Batch  489 / 525  Training Loss  0.0023672087118029594\n",
            "Epoch  14 Batch  490 / 525  Training Loss  0.0016528826672583818\n",
            "Epoch  14 Batch  491 / 525  Training Loss  0.0022004060447216034\n",
            "Epoch  14 Batch  492 / 525  Training Loss  0.004263151902705431\n",
            "Epoch  14 Batch  493 / 525  Training Loss  0.0018493530806154013\n",
            "Epoch  14 Batch  494 / 525  Training Loss  0.006139444652944803\n",
            "Epoch  14 Batch  495 / 525  Training Loss  0.0016012785490602255\n",
            "Epoch  14 Batch  496 / 525  Training Loss  0.0018203153740614653\n",
            "Epoch  14 Batch  497 / 525  Training Loss  0.0019973821472376585\n",
            "Epoch  14 Batch  498 / 525  Training Loss  0.0029421187937259674\n",
            "Epoch  14 Batch  499 / 525  Training Loss  0.004234996158629656\n",
            "Epoch  14 Batch  500 / 525  Training Loss  0.002890777075663209\n",
            "Epoch  14 Batch  501 / 525  Training Loss  0.0035376916639506817\n",
            "Epoch  14 Batch  502 / 525  Training Loss  0.005049572326242924\n",
            "Epoch  14 Batch  503 / 525  Training Loss  0.004940931685268879\n",
            "Epoch  14 Batch  504 / 525  Training Loss  0.00037867846549488604\n",
            "Epoch  14 Batch  505 / 525  Training Loss  0.004347094334661961\n",
            "Epoch  14 Batch  506 / 525  Training Loss  0.002127565909177065\n",
            "Epoch  14 Batch  507 / 525  Training Loss  0.0012567087542265654\n",
            "Epoch  14 Batch  508 / 525  Training Loss  0.0027409973554313183\n",
            "Epoch  14 Batch  509 / 525  Training Loss  0.00044706938206218183\n",
            "Epoch  14 Batch  510 / 525  Training Loss  0.002260672627016902\n",
            "Epoch  14 Batch  511 / 525  Training Loss  0.00046440918231382966\n",
            "Epoch  14 Batch  512 / 525  Training Loss  0.003851730842143297\n",
            "Epoch  14 Batch  513 / 525  Training Loss  0.00482149189338088\n",
            "Epoch  14 Batch  514 / 525  Training Loss  0.0022981001529842615\n",
            "Epoch  14 Batch  515 / 525  Training Loss  0.0037515468429774046\n",
            "Epoch  14 Batch  516 / 525  Training Loss  0.00121984351426363\n",
            "Epoch  14 Batch  517 / 525  Training Loss  0.002380571560934186\n",
            "Epoch  14 Batch  518 / 525  Training Loss  0.002711579902097583\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  14 Batch  519 / 525  Training Loss  0.0025807309430092573\n",
            "Epoch  14 Batch  520 / 525  Training Loss  0.0005197225837036967\n",
            "Epoch  14 Batch  521 / 525  Training Loss  0.004621315281838179\n",
            "Epoch  14 Batch  522 / 525  Training Loss  0.001221620012074709\n",
            "Epoch  14 Batch  523 / 525  Training Loss  0.0014916073996573687\n",
            "Epoch  14 Batch  524 / 525  Training Loss  0.0019795328844338655\n",
            "  15    |    -    |   0.002230   |   63.12  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 15\n",
            "Epoch  15 Batch  0 / 525  Training Loss  0.0011351227294653654\n",
            "Epoch  15 Batch  1 / 525  Training Loss  0.011279372498393059\n",
            "Epoch  15 Batch  2 / 525  Training Loss  0.0018335838103666902\n",
            "Epoch  15 Batch  3 / 525  Training Loss  0.0006427791668102145\n",
            "Epoch  15 Batch  4 / 525  Training Loss  0.001103344839066267\n",
            "Epoch  15 Batch  5 / 525  Training Loss  0.0010212833294644952\n",
            "Epoch  15 Batch  6 / 525  Training Loss  0.0007569295703433454\n",
            "Epoch  15 Batch  7 / 525  Training Loss  0.0016927749384194613\n",
            "Epoch  15 Batch  8 / 525  Training Loss  0.0011032882612198591\n",
            "Epoch  15 Batch  9 / 525  Training Loss  0.0006729571614414454\n",
            "Epoch  15 Batch  10 / 525  Training Loss  0.0006795983645133674\n",
            "Epoch  15 Batch  11 / 525  Training Loss  0.0009513080003671348\n",
            "Epoch  15 Batch  12 / 525  Training Loss  0.001485079643316567\n",
            "Epoch  15 Batch  13 / 525  Training Loss  0.001129430253058672\n",
            "Epoch  15 Batch  14 / 525  Training Loss  0.0005785009125247598\n",
            "Epoch  15 Batch  15 / 525  Training Loss  0.0018839891999959946\n",
            "Epoch  15 Batch  16 / 525  Training Loss  0.0015037679113447666\n",
            "Epoch  15 Batch  17 / 525  Training Loss  0.0005580763099715114\n",
            "Epoch  15 Batch  18 / 525  Training Loss  0.0006681539816781878\n",
            "Epoch  15 Batch  19 / 525  Training Loss  0.0016309277852997184\n",
            "Epoch  15 Batch  20 / 525  Training Loss  0.0008801940712146461\n",
            "Epoch  15 Batch  21 / 525  Training Loss  0.0007064179517328739\n",
            "Epoch  15 Batch  22 / 525  Training Loss  0.0018061923328787088\n",
            "Epoch  15 Batch  23 / 525  Training Loss  0.0004434586444403976\n",
            "Epoch  15 Batch  24 / 525  Training Loss  0.0011948072351515293\n",
            "Epoch  15 Batch  25 / 525  Training Loss  0.0008968740585260093\n",
            "Epoch  15 Batch  26 / 525  Training Loss  0.002177127869799733\n",
            "Epoch  15 Batch  27 / 525  Training Loss  0.0010086196707561612\n",
            "Epoch  15 Batch  28 / 525  Training Loss  0.005774063058197498\n",
            "Epoch  15 Batch  29 / 525  Training Loss  0.0008709730464033782\n",
            "Epoch  15 Batch  30 / 525  Training Loss  0.0006259513320401311\n",
            "Epoch  15 Batch  31 / 525  Training Loss  0.0005544660380110145\n",
            "Epoch  15 Batch  32 / 525  Training Loss  0.0008777575567364693\n",
            "Epoch  15 Batch  33 / 525  Training Loss  0.001834179274737835\n",
            "Epoch  15 Batch  34 / 525  Training Loss  0.0014403719687834382\n",
            "Epoch  15 Batch  35 / 525  Training Loss  0.0003978692402597517\n",
            "Epoch  15 Batch  36 / 525  Training Loss  0.000802766066044569\n",
            "Epoch  15 Batch  37 / 525  Training Loss  0.0007988107390701771\n",
            "Epoch  15 Batch  38 / 525  Training Loss  0.0010372127871960402\n",
            "Epoch  15 Batch  39 / 525  Training Loss  0.0018142780754715204\n",
            "Epoch  15 Batch  40 / 525  Training Loss  0.002333485521376133\n",
            "Epoch  15 Batch  41 / 525  Training Loss  0.0009177920292131603\n",
            "Epoch  15 Batch  42 / 525  Training Loss  0.0011796760372817516\n",
            "Epoch  15 Batch  43 / 525  Training Loss  0.00113392504863441\n",
            "Epoch  15 Batch  44 / 525  Training Loss  0.0008571865037083626\n",
            "Epoch  15 Batch  45 / 525  Training Loss  0.00038250713259913027\n",
            "Epoch  15 Batch  46 / 525  Training Loss  0.0013802882749587297\n",
            "Epoch  15 Batch  47 / 525  Training Loss  0.0007558157085441053\n",
            "Epoch  15 Batch  48 / 525  Training Loss  0.00038891626172699034\n",
            "Epoch  15 Batch  49 / 525  Training Loss  0.006015117280185223\n",
            "Epoch  15 Batch  50 / 525  Training Loss  0.0006328397430479527\n",
            "Epoch  15 Batch  51 / 525  Training Loss  0.001352788181975484\n",
            "Epoch  15 Batch  52 / 525  Training Loss  0.0007412934792228043\n",
            "Epoch  15 Batch  53 / 525  Training Loss  0.0005844687111675739\n",
            "Epoch  15 Batch  54 / 525  Training Loss  0.0004979899385944009\n",
            "Epoch  15 Batch  55 / 525  Training Loss  0.0006084268097765744\n",
            "Epoch  15 Batch  56 / 525  Training Loss  0.0007297100964933634\n",
            "Epoch  15 Batch  57 / 525  Training Loss  0.0012223093071952462\n",
            "Epoch  15 Batch  58 / 525  Training Loss  0.0006518766167573631\n",
            "Epoch  15 Batch  59 / 525  Training Loss  0.000921906903386116\n",
            "Epoch  15 Batch  60 / 525  Training Loss  0.0007423165952786803\n",
            "Epoch  15 Batch  61 / 525  Training Loss  0.0006845806492492557\n",
            "Epoch  15 Batch  62 / 525  Training Loss  0.0005930053885094821\n",
            "Epoch  15 Batch  63 / 525  Training Loss  0.00042551878141239285\n",
            "Epoch  15 Batch  64 / 525  Training Loss  0.00100458855740726\n",
            "Epoch  15 Batch  65 / 525  Training Loss  0.0005686382646672428\n",
            "Epoch  15 Batch  66 / 525  Training Loss  0.0014884117990732193\n",
            "Epoch  15 Batch  67 / 525  Training Loss  0.0007379939779639244\n",
            "Epoch  15 Batch  68 / 525  Training Loss  0.0006823483854532242\n",
            "Epoch  15 Batch  69 / 525  Training Loss  0.00045915250666439533\n",
            "Epoch  15 Batch  70 / 525  Training Loss  0.0006812344072386622\n",
            "Epoch  15 Batch  71 / 525  Training Loss  0.0016212789341807365\n",
            "Epoch  15 Batch  72 / 525  Training Loss  0.0004726777842734009\n",
            "Epoch  15 Batch  73 / 525  Training Loss  0.0006266082637012005\n",
            "Epoch  15 Batch  74 / 525  Training Loss  0.002929826034232974\n",
            "Epoch  15 Batch  75 / 525  Training Loss  0.0036285698879510164\n",
            "Epoch  15 Batch  76 / 525  Training Loss  0.00044568791054189205\n",
            "Epoch  15 Batch  77 / 525  Training Loss  0.0005180861917324364\n",
            "Epoch  15 Batch  78 / 525  Training Loss  0.0024973100516945124\n",
            "Epoch  15 Batch  79 / 525  Training Loss  0.000302227825159207\n",
            "Epoch  15 Batch  80 / 525  Training Loss  0.0009626072132959962\n",
            "Epoch  15 Batch  81 / 525  Training Loss  0.0010568952420726418\n",
            "Epoch  15 Batch  82 / 525  Training Loss  0.0023543110582977533\n",
            "Epoch  15 Batch  83 / 525  Training Loss  0.0005901608965359628\n",
            "Epoch  15 Batch  84 / 525  Training Loss  0.0011311732232570648\n",
            "Epoch  15 Batch  85 / 525  Training Loss  0.0008106412133201957\n",
            "Epoch  15 Batch  86 / 525  Training Loss  0.0013656839728355408\n",
            "Epoch  15 Batch  87 / 525  Training Loss  0.0005701495101675391\n",
            "Epoch  15 Batch  88 / 525  Training Loss  0.0025817113928496838\n",
            "Epoch  15 Batch  89 / 525  Training Loss  0.000636192737147212\n",
            "Epoch  15 Batch  90 / 525  Training Loss  0.0014936591032892466\n",
            "Epoch  15 Batch  91 / 525  Training Loss  0.0006812039646320045\n",
            "Epoch  15 Batch  92 / 525  Training Loss  0.0005238321609795094\n",
            "Epoch  15 Batch  93 / 525  Training Loss  0.0067136650905013084\n",
            "Epoch  15 Batch  94 / 525  Training Loss  0.0008624889887869358\n",
            "Epoch  15 Batch  95 / 525  Training Loss  0.00040973187424242496\n",
            "Epoch  15 Batch  96 / 525  Training Loss  0.001944269984960556\n",
            "Epoch  15 Batch  97 / 525  Training Loss  0.0010434308787807822\n",
            "Epoch  15 Batch  98 / 525  Training Loss  0.000724832178093493\n",
            "Epoch  15 Batch  99 / 525  Training Loss  0.002073816955089569\n",
            "Epoch  15 Batch  100 / 525  Training Loss  0.001343625015579164\n",
            "Epoch  15 Batch  101 / 525  Training Loss  0.0007511122385039926\n",
            "Epoch  15 Batch  102 / 525  Training Loss  0.0011247387155890465\n",
            "Epoch  15 Batch  103 / 525  Training Loss  0.001068352023139596\n",
            "Epoch  15 Batch  104 / 525  Training Loss  0.000995647395029664\n",
            "Epoch  15 Batch  105 / 525  Training Loss  0.0008546028984710574\n",
            "Epoch  15 Batch  106 / 525  Training Loss  0.0004658571560867131\n",
            "Epoch  15 Batch  107 / 525  Training Loss  0.0010440254118293524\n",
            "Epoch  15 Batch  108 / 525  Training Loss  0.0007466016104444861\n",
            "Epoch  15 Batch  109 / 525  Training Loss  0.00040608184644952416\n",
            "Epoch  15 Batch  110 / 525  Training Loss  0.0004265386669430882\n",
            "Epoch  15 Batch  111 / 525  Training Loss  0.0014234110713005066\n",
            "Epoch  15 Batch  112 / 525  Training Loss  0.0006359792896546423\n",
            "Epoch  15 Batch  113 / 525  Training Loss  0.0005769883864559233\n",
            "Epoch  15 Batch  114 / 525  Training Loss  0.0009562823688611388\n",
            "Epoch  15 Batch  115 / 525  Training Loss  0.0013882111525163054\n",
            "Epoch  15 Batch  116 / 525  Training Loss  0.0006760290707461536\n",
            "Epoch  15 Batch  117 / 525  Training Loss  0.0005486257141456008\n",
            "Epoch  15 Batch  118 / 525  Training Loss  0.002127988962456584\n",
            "Epoch  15 Batch  119 / 525  Training Loss  0.001021584146656096\n",
            "Epoch  15 Batch  120 / 525  Training Loss  0.0006132926791906357\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  15 Batch  121 / 525  Training Loss  0.0006686112028546631\n",
            "Epoch  15 Batch  122 / 525  Training Loss  0.000982609810307622\n",
            "Epoch  15 Batch  123 / 525  Training Loss  0.001438983716070652\n",
            "Epoch  15 Batch  124 / 525  Training Loss  0.0005877800285816193\n",
            "Epoch  15 Batch  125 / 525  Training Loss  0.0013333207461982965\n",
            "Epoch  15 Batch  126 / 525  Training Loss  0.0007735131657682359\n",
            "Epoch  15 Batch  127 / 525  Training Loss  0.0009682457894086838\n",
            "Epoch  15 Batch  128 / 525  Training Loss  0.000598774931859225\n",
            "Epoch  15 Batch  129 / 525  Training Loss  0.0004400731122586876\n",
            "Epoch  15 Batch  130 / 525  Training Loss  0.0005439183441922069\n",
            "Epoch  15 Batch  131 / 525  Training Loss  0.0008479530806653202\n",
            "Epoch  15 Batch  132 / 525  Training Loss  0.0007843624916858971\n",
            "Epoch  15 Batch  133 / 525  Training Loss  0.00040202122181653976\n",
            "Epoch  15 Batch  134 / 525  Training Loss  0.0010028401156887412\n",
            "Epoch  15 Batch  135 / 525  Training Loss  0.0005059788236394525\n",
            "Epoch  15 Batch  136 / 525  Training Loss  0.0018890012288466096\n",
            "Epoch  15 Batch  137 / 525  Training Loss  0.0011896960204467177\n",
            "Epoch  15 Batch  138 / 525  Training Loss  0.0010696484241634607\n",
            "Epoch  15 Batch  139 / 525  Training Loss  0.0010335410479456186\n",
            "Epoch  15 Batch  140 / 525  Training Loss  0.0014166543260216713\n",
            "Epoch  15 Batch  141 / 525  Training Loss  0.00034996558679267764\n",
            "Epoch  15 Batch  142 / 525  Training Loss  0.003128855023533106\n",
            "Epoch  15 Batch  143 / 525  Training Loss  0.007741869892925024\n",
            "Epoch  15 Batch  144 / 525  Training Loss  0.0007948529673740268\n",
            "Epoch  15 Batch  145 / 525  Training Loss  0.003366206306964159\n",
            "Epoch  15 Batch  146 / 525  Training Loss  0.009355067275464535\n",
            "Epoch  15 Batch  147 / 525  Training Loss  0.0007918343180790544\n",
            "Epoch  15 Batch  148 / 525  Training Loss  0.0003427756892051548\n",
            "Epoch  15 Batch  149 / 525  Training Loss  0.0008736327290534973\n",
            "Epoch  15 Batch  150 / 525  Training Loss  0.006605642382055521\n",
            "Epoch  15 Batch  151 / 525  Training Loss  0.003985053393989801\n",
            "Epoch  15 Batch  152 / 525  Training Loss  0.0013828647788614035\n",
            "Epoch  15 Batch  153 / 525  Training Loss  0.0013851646799594164\n",
            "Epoch  15 Batch  154 / 525  Training Loss  0.0030773738399147987\n",
            "Epoch  15 Batch  155 / 525  Training Loss  0.002342685591429472\n",
            "Epoch  15 Batch  156 / 525  Training Loss  0.0006023623282089829\n",
            "Epoch  15 Batch  157 / 525  Training Loss  0.005330313462764025\n",
            "Epoch  15 Batch  158 / 525  Training Loss  0.0016411112155765295\n",
            "Epoch  15 Batch  159 / 525  Training Loss  0.0009690515580587089\n",
            "Epoch  15 Batch  160 / 525  Training Loss  0.001112802536226809\n",
            "Epoch  15 Batch  161 / 525  Training Loss  0.0014423767570406199\n",
            "Epoch  15 Batch  162 / 525  Training Loss  0.0006301973480731249\n",
            "Epoch  15 Batch  163 / 525  Training Loss  0.0006668404093943536\n",
            "Epoch  15 Batch  164 / 525  Training Loss  0.0009730790625326335\n",
            "Epoch  15 Batch  165 / 525  Training Loss  0.0009265501284971833\n",
            "Epoch  15 Batch  166 / 525  Training Loss  0.0006807355675846338\n",
            "Epoch  15 Batch  167 / 525  Training Loss  0.0003403381851967424\n",
            "Epoch  15 Batch  168 / 525  Training Loss  0.0022748331539332867\n",
            "Epoch  15 Batch  169 / 525  Training Loss  0.003925567492842674\n",
            "Epoch  15 Batch  170 / 525  Training Loss  0.0006976921577006578\n",
            "Epoch  15 Batch  171 / 525  Training Loss  0.0010057786712422967\n",
            "Epoch  15 Batch  172 / 525  Training Loss  0.001313521177507937\n",
            "Epoch  15 Batch  173 / 525  Training Loss  0.000732257089111954\n",
            "Epoch  15 Batch  174 / 525  Training Loss  0.0009097393485717475\n",
            "Epoch  15 Batch  175 / 525  Training Loss  0.0006003376329317689\n",
            "Epoch  15 Batch  176 / 525  Training Loss  0.0006421307334676385\n",
            "Epoch  15 Batch  177 / 525  Training Loss  0.00136485043913126\n",
            "Epoch  15 Batch  178 / 525  Training Loss  0.0015778631204739213\n",
            "Epoch  15 Batch  179 / 525  Training Loss  0.002334600780159235\n",
            "Epoch  15 Batch  180 / 525  Training Loss  0.0003484993358142674\n",
            "Epoch  15 Batch  181 / 525  Training Loss  0.0009184879017993808\n",
            "Epoch  15 Batch  182 / 525  Training Loss  0.001485537737607956\n",
            "Epoch  15 Batch  183 / 525  Training Loss  0.000549580785445869\n",
            "Epoch  15 Batch  184 / 525  Training Loss  0.0012426829198375344\n",
            "Epoch  15 Batch  185 / 525  Training Loss  0.0003490484377834946\n",
            "Epoch  15 Batch  186 / 525  Training Loss  0.0013008968671783805\n",
            "Epoch  15 Batch  187 / 525  Training Loss  0.0004457042959984392\n",
            "Epoch  15 Batch  188 / 525  Training Loss  0.0030806069262325764\n",
            "Epoch  15 Batch  189 / 525  Training Loss  0.0010370570234954357\n",
            "Epoch  15 Batch  190 / 525  Training Loss  0.0008451663888990879\n",
            "Epoch  15 Batch  191 / 525  Training Loss  0.0005752042052336037\n",
            "Epoch  15 Batch  192 / 525  Training Loss  0.0018402636051177979\n",
            "Epoch  15 Batch  193 / 525  Training Loss  0.0010161956306546926\n",
            "Epoch  15 Batch  194 / 525  Training Loss  0.0003445851616561413\n",
            "Epoch  15 Batch  195 / 525  Training Loss  0.00048189269728027284\n",
            "Epoch  15 Batch  196 / 525  Training Loss  0.000949650420807302\n",
            "Epoch  15 Batch  197 / 525  Training Loss  0.0010142455575987697\n",
            "Epoch  15 Batch  198 / 525  Training Loss  0.0017542330315336585\n",
            "Epoch  15 Batch  199 / 525  Training Loss  0.0023315935395658016\n",
            "Epoch  15 Batch  200 / 525  Training Loss  0.0006628782139159739\n",
            "Epoch  15 Batch  201 / 525  Training Loss  0.0009106228244490921\n",
            "Epoch  15 Batch  202 / 525  Training Loss  0.001130049815401435\n",
            "Epoch  15 Batch  203 / 525  Training Loss  0.0012568613747134805\n",
            "Epoch  15 Batch  204 / 525  Training Loss  0.0013183753471821547\n",
            "Epoch  15 Batch  205 / 525  Training Loss  0.001963849877938628\n",
            "Epoch  15 Batch  206 / 525  Training Loss  0.001275881426408887\n",
            "Epoch  15 Batch  207 / 525  Training Loss  0.002079008612781763\n",
            "Epoch  15 Batch  208 / 525  Training Loss  0.0008870981400832534\n",
            "Epoch  15 Batch  209 / 525  Training Loss  0.0003950797545257956\n",
            "Epoch  15 Batch  210 / 525  Training Loss  0.0007688654586672783\n",
            "Epoch  15 Batch  211 / 525  Training Loss  0.0009232120355591178\n",
            "Epoch  15 Batch  212 / 525  Training Loss  0.0008525284938514233\n",
            "Epoch  15 Batch  213 / 525  Training Loss  0.0008275892469100654\n",
            "Epoch  15 Batch  214 / 525  Training Loss  0.0027632717974483967\n",
            "Epoch  15 Batch  215 / 525  Training Loss  0.0013615147909149528\n",
            "Epoch  15 Batch  216 / 525  Training Loss  0.003201643703505397\n",
            "Epoch  15 Batch  217 / 525  Training Loss  0.0012139050522819161\n",
            "Epoch  15 Batch  218 / 525  Training Loss  0.0010467746760696173\n",
            "Epoch  15 Batch  219 / 525  Training Loss  0.0010541852097958326\n",
            "Epoch  15 Batch  220 / 525  Training Loss  0.00034250126918777823\n",
            "Epoch  15 Batch  221 / 525  Training Loss  0.001307077007368207\n",
            "Epoch  15 Batch  222 / 525  Training Loss  0.0008515717345289886\n",
            "Epoch  15 Batch  223 / 525  Training Loss  0.0008371264557354152\n",
            "Epoch  15 Batch  224 / 525  Training Loss  0.0008466375293210149\n",
            "Epoch  15 Batch  225 / 525  Training Loss  0.0013778428547084332\n",
            "Epoch  15 Batch  226 / 525  Training Loss  0.00048126192996278405\n",
            "Epoch  15 Batch  227 / 525  Training Loss  0.0009119122987613082\n",
            "Epoch  15 Batch  228 / 525  Training Loss  0.0009921346791088581\n",
            "Epoch  15 Batch  229 / 525  Training Loss  0.0007767042843624949\n",
            "Epoch  15 Batch  230 / 525  Training Loss  0.0017506026197224855\n",
            "Epoch  15 Batch  231 / 525  Training Loss  0.000376509124180302\n",
            "Epoch  15 Batch  232 / 525  Training Loss  0.0009650156134739518\n",
            "Epoch  15 Batch  233 / 525  Training Loss  0.0004694586677942425\n",
            "Epoch  15 Batch  234 / 525  Training Loss  0.0011473477352410555\n",
            "Epoch  15 Batch  235 / 525  Training Loss  0.0007532767485827208\n",
            "Epoch  15 Batch  236 / 525  Training Loss  0.005939079448580742\n",
            "Epoch  15 Batch  237 / 525  Training Loss  0.001000824267975986\n",
            "Epoch  15 Batch  238 / 525  Training Loss  0.0008668795344419777\n",
            "Epoch  15 Batch  239 / 525  Training Loss  0.0006710534216836095\n",
            "Epoch  15 Batch  240 / 525  Training Loss  0.0006087393267080188\n",
            "Epoch  15 Batch  241 / 525  Training Loss  0.0020624417811632156\n",
            "Epoch  15 Batch  242 / 525  Training Loss  0.0007143054972402751\n",
            "Epoch  15 Batch  243 / 525  Training Loss  0.0005476886872202158\n",
            "Epoch  15 Batch  244 / 525  Training Loss  0.0009243290987797081\n",
            "Epoch  15 Batch  245 / 525  Training Loss  0.000653581868391484\n",
            "Epoch  15 Batch  246 / 525  Training Loss  0.0003805049054790288\n",
            "Epoch  15 Batch  247 / 525  Training Loss  0.000334885495249182\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  15 Batch  248 / 525  Training Loss  0.0009576118318364024\n",
            "Epoch  15 Batch  249 / 525  Training Loss  0.0007214820943772793\n",
            "Epoch  15 Batch  250 / 525  Training Loss  0.0012247251579537988\n",
            "Epoch  15 Batch  251 / 525  Training Loss  0.0006676343618892133\n",
            "Epoch  15 Batch  252 / 525  Training Loss  0.0004959589568898082\n",
            "Epoch  15 Batch  253 / 525  Training Loss  0.0009421558934263885\n",
            "Epoch  15 Batch  254 / 525  Training Loss  0.0031837481074035168\n",
            "Epoch  15 Batch  255 / 525  Training Loss  0.0012923846952617168\n",
            "Epoch  15 Batch  256 / 525  Training Loss  0.0005526263266801834\n",
            "Epoch  15 Batch  257 / 525  Training Loss  0.0006601603236049414\n",
            "Epoch  15 Batch  258 / 525  Training Loss  0.005136528052389622\n",
            "Epoch  15 Batch  259 / 525  Training Loss  0.0004985231207683682\n",
            "Epoch  15 Batch  260 / 525  Training Loss  0.0004983152030035853\n",
            "Epoch  15 Batch  261 / 525  Training Loss  0.000714421272277832\n",
            "Epoch  15 Batch  262 / 525  Training Loss  0.001032465137541294\n",
            "Epoch  15 Batch  263 / 525  Training Loss  0.001676690997555852\n",
            "Epoch  15 Batch  264 / 525  Training Loss  0.018651457503437996\n",
            "Epoch  15 Batch  265 / 525  Training Loss  0.0006628212286159396\n",
            "Epoch  15 Batch  266 / 525  Training Loss  0.002449254272505641\n",
            "Epoch  15 Batch  267 / 525  Training Loss  0.0033465835731476545\n",
            "Epoch  15 Batch  268 / 525  Training Loss  0.0023607900366187096\n",
            "Epoch  15 Batch  269 / 525  Training Loss  0.0021791637409478426\n",
            "Epoch  15 Batch  270 / 525  Training Loss  0.0008915721555240452\n",
            "Epoch  15 Batch  271 / 525  Training Loss  0.012696350924670696\n",
            "Epoch  15 Batch  272 / 525  Training Loss  0.00038141157710924745\n",
            "Epoch  15 Batch  273 / 525  Training Loss  0.0007030392880551517\n",
            "Epoch  15 Batch  274 / 525  Training Loss  0.0025392367970198393\n",
            "Epoch  15 Batch  275 / 525  Training Loss  0.00043880901648662984\n",
            "Epoch  15 Batch  276 / 525  Training Loss  0.0008001299574971199\n",
            "Epoch  15 Batch  277 / 525  Training Loss  0.0022445006761699915\n",
            "Epoch  15 Batch  278 / 525  Training Loss  0.0013673562789335847\n",
            "Epoch  15 Batch  279 / 525  Training Loss  0.0005897195660509169\n",
            "Epoch  15 Batch  280 / 525  Training Loss  0.0015252218581736088\n",
            "Epoch  15 Batch  281 / 525  Training Loss  0.004753940738737583\n",
            "Epoch  15 Batch  282 / 525  Training Loss  0.00170994084328413\n",
            "Epoch  15 Batch  283 / 525  Training Loss  0.0006465894402936101\n",
            "Epoch  15 Batch  284 / 525  Training Loss  0.0014920582761988044\n",
            "Epoch  15 Batch  285 / 525  Training Loss  0.0011214372934773564\n",
            "Epoch  15 Batch  286 / 525  Training Loss  0.0010599395027384162\n",
            "Epoch  15 Batch  287 / 525  Training Loss  0.001731370808556676\n",
            "Epoch  15 Batch  288 / 525  Training Loss  0.0005185866029933095\n",
            "Epoch  15 Batch  289 / 525  Training Loss  0.0013592237373813987\n",
            "Epoch  15 Batch  290 / 525  Training Loss  0.0009560653707012534\n",
            "Epoch  15 Batch  291 / 525  Training Loss  0.0005923488060943782\n",
            "Epoch  15 Batch  292 / 525  Training Loss  0.0006501617608591914\n",
            "Epoch  15 Batch  293 / 525  Training Loss  0.0008823842508718371\n",
            "Epoch  15 Batch  294 / 525  Training Loss  0.0006763191777281463\n",
            "Epoch  15 Batch  295 / 525  Training Loss  0.0007515148026868701\n",
            "Epoch  15 Batch  296 / 525  Training Loss  0.0007463921792805195\n",
            "Epoch  15 Batch  297 / 525  Training Loss  0.0010711255017668009\n",
            "Epoch  15 Batch  298 / 525  Training Loss  0.011714329943060875\n",
            "Epoch  15 Batch  299 / 525  Training Loss  0.0007926758262328804\n",
            "Epoch  15 Batch  300 / 525  Training Loss  0.0007311694789677858\n",
            "Epoch  15 Batch  301 / 525  Training Loss  0.0006937452126294374\n",
            "Epoch  15 Batch  302 / 525  Training Loss  0.0006316325743682683\n",
            "Epoch  15 Batch  303 / 525  Training Loss  0.004009701777249575\n",
            "Epoch  15 Batch  304 / 525  Training Loss  0.004014636389911175\n",
            "Epoch  15 Batch  305 / 525  Training Loss  0.0011078979587182403\n",
            "Epoch  15 Batch  306 / 525  Training Loss  0.00038463619421236217\n",
            "Epoch  15 Batch  307 / 525  Training Loss  0.0007091466104611754\n",
            "Epoch  15 Batch  308 / 525  Training Loss  0.00047113088658079505\n",
            "Epoch  15 Batch  309 / 525  Training Loss  0.0007770321681164205\n",
            "Epoch  15 Batch  310 / 525  Training Loss  0.001193258329294622\n",
            "Epoch  15 Batch  311 / 525  Training Loss  0.00105515425093472\n",
            "Epoch  15 Batch  312 / 525  Training Loss  0.000616885197814554\n",
            "Epoch  15 Batch  313 / 525  Training Loss  0.0036299037747085094\n",
            "Epoch  15 Batch  314 / 525  Training Loss  0.001181026455014944\n",
            "Epoch  15 Batch  315 / 525  Training Loss  0.0011813787277787924\n",
            "Epoch  15 Batch  316 / 525  Training Loss  0.0007620148826390505\n",
            "Epoch  15 Batch  317 / 525  Training Loss  0.000697650364600122\n",
            "Epoch  15 Batch  318 / 525  Training Loss  0.0005851181922480464\n",
            "Epoch  15 Batch  319 / 525  Training Loss  0.0004606192815117538\n",
            "Epoch  15 Batch  320 / 525  Training Loss  0.0008342606015503407\n",
            "Epoch  15 Batch  321 / 525  Training Loss  0.0003584566875360906\n",
            "Epoch  15 Batch  322 / 525  Training Loss  0.0007758074207231402\n",
            "Epoch  15 Batch  323 / 525  Training Loss  0.0007681429269723594\n",
            "Epoch  15 Batch  324 / 525  Training Loss  0.00044414159492589533\n",
            "Epoch  15 Batch  325 / 525  Training Loss  0.002116886666044593\n",
            "Epoch  15 Batch  326 / 525  Training Loss  0.0015115720452740788\n",
            "Epoch  15 Batch  327 / 525  Training Loss  0.009186296723783016\n",
            "Epoch  15 Batch  328 / 525  Training Loss  0.0005187818896956742\n",
            "Epoch  15 Batch  329 / 525  Training Loss  0.0013709263876080513\n",
            "Epoch  15 Batch  330 / 525  Training Loss  0.0010039100889116526\n",
            "Epoch  15 Batch  331 / 525  Training Loss  0.001568867126479745\n",
            "Epoch  15 Batch  332 / 525  Training Loss  0.0006400156999006867\n",
            "Epoch  15 Batch  333 / 525  Training Loss  0.0007272512884810567\n",
            "Epoch  15 Batch  334 / 525  Training Loss  0.0007585949497297406\n",
            "Epoch  15 Batch  335 / 525  Training Loss  0.0011264189379289746\n",
            "Epoch  15 Batch  336 / 525  Training Loss  0.0012620750349014997\n",
            "Epoch  15 Batch  337 / 525  Training Loss  0.0017374707385897636\n",
            "Epoch  15 Batch  338 / 525  Training Loss  0.0004516163025982678\n",
            "Epoch  15 Batch  339 / 525  Training Loss  0.0008671876275911927\n",
            "Epoch  15 Batch  340 / 525  Training Loss  0.0008894329657778144\n",
            "Epoch  15 Batch  341 / 525  Training Loss  0.0011594381649047136\n",
            "Epoch  15 Batch  342 / 525  Training Loss  0.001187443034723401\n",
            "Epoch  15 Batch  343 / 525  Training Loss  0.0010740841971710324\n",
            "Epoch  15 Batch  344 / 525  Training Loss  0.0005776143516413867\n",
            "Epoch  15 Batch  345 / 525  Training Loss  0.0009286454878747463\n",
            "Epoch  15 Batch  346 / 525  Training Loss  0.0011249470990151167\n",
            "Epoch  15 Batch  347 / 525  Training Loss  0.0004481387441046536\n",
            "Epoch  15 Batch  348 / 525  Training Loss  0.0008916432270780206\n",
            "Epoch  15 Batch  349 / 525  Training Loss  0.000987398438155651\n",
            "Epoch  15 Batch  350 / 525  Training Loss  0.0008084623259492218\n",
            "Epoch  15 Batch  351 / 525  Training Loss  0.001008000923320651\n",
            "Epoch  15 Batch  352 / 525  Training Loss  0.0007829608512111008\n",
            "Epoch  15 Batch  353 / 525  Training Loss  0.0009680294315330684\n",
            "Epoch  15 Batch  354 / 525  Training Loss  0.0005246782675385475\n",
            "Epoch  15 Batch  355 / 525  Training Loss  0.0005692377453669906\n",
            "Epoch  15 Batch  356 / 525  Training Loss  0.000609682232607156\n",
            "Epoch  15 Batch  357 / 525  Training Loss  0.0005003584083169699\n",
            "Epoch  15 Batch  358 / 525  Training Loss  0.0010445741936564445\n",
            "Epoch  15 Batch  359 / 525  Training Loss  0.00029317825101315975\n",
            "Epoch  15 Batch  360 / 525  Training Loss  0.0007576656644232571\n",
            "Epoch  15 Batch  361 / 525  Training Loss  0.0008397201308980584\n",
            "Epoch  15 Batch  362 / 525  Training Loss  0.0008946227608248591\n",
            "Epoch  15 Batch  363 / 525  Training Loss  0.001213656272739172\n",
            "Epoch  15 Batch  364 / 525  Training Loss  0.00039835451752878726\n",
            "Epoch  15 Batch  365 / 525  Training Loss  0.0004482509975787252\n",
            "Epoch  15 Batch  366 / 525  Training Loss  0.000769413192756474\n",
            "Epoch  15 Batch  367 / 525  Training Loss  0.0016783999744802713\n",
            "Epoch  15 Batch  368 / 525  Training Loss  0.0005207384820096195\n",
            "Epoch  15 Batch  369 / 525  Training Loss  0.0009046999621205032\n",
            "Epoch  15 Batch  370 / 525  Training Loss  0.0015120673924684525\n",
            "Epoch  15 Batch  371 / 525  Training Loss  0.0008091864874586463\n",
            "Epoch  15 Batch  372 / 525  Training Loss  0.0005819363286718726\n",
            "Epoch  15 Batch  373 / 525  Training Loss  0.00038279517320916057\n",
            "Epoch  15 Batch  374 / 525  Training Loss  0.0005680164904333651\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  15 Batch  375 / 525  Training Loss  0.0008116622339002788\n",
            "Epoch  15 Batch  376 / 525  Training Loss  0.0016296453541144729\n",
            "Epoch  15 Batch  377 / 525  Training Loss  0.0009913534158840775\n",
            "Epoch  15 Batch  378 / 525  Training Loss  0.0008698751917108893\n",
            "Epoch  15 Batch  379 / 525  Training Loss  0.0020012115128338337\n",
            "Epoch  15 Batch  380 / 525  Training Loss  0.0005200446466915309\n",
            "Epoch  15 Batch  381 / 525  Training Loss  0.00024369455059058964\n",
            "Epoch  15 Batch  382 / 525  Training Loss  0.001082096016034484\n",
            "Epoch  15 Batch  383 / 525  Training Loss  0.0005773286684416234\n",
            "Epoch  15 Batch  384 / 525  Training Loss  0.000687164138071239\n",
            "Epoch  15 Batch  385 / 525  Training Loss  0.0009787598391994834\n",
            "Epoch  15 Batch  386 / 525  Training Loss  0.0015213327715173364\n",
            "Epoch  15 Batch  387 / 525  Training Loss  0.0006096914876252413\n",
            "Epoch  15 Batch  388 / 525  Training Loss  0.0010435397271066904\n",
            "Epoch  15 Batch  389 / 525  Training Loss  0.002931206487119198\n",
            "Epoch  15 Batch  390 / 525  Training Loss  0.0024021239951252937\n",
            "Epoch  15 Batch  391 / 525  Training Loss  0.00065591000020504\n",
            "Epoch  15 Batch  392 / 525  Training Loss  0.0007853644201532006\n",
            "Epoch  15 Batch  393 / 525  Training Loss  0.0007376902503892779\n",
            "Epoch  15 Batch  394 / 525  Training Loss  0.0004222152638249099\n",
            "Epoch  15 Batch  395 / 525  Training Loss  0.004191880114376545\n",
            "Epoch  15 Batch  396 / 525  Training Loss  0.0010991276940330863\n",
            "Epoch  15 Batch  397 / 525  Training Loss  0.0005888623418286443\n",
            "Epoch  15 Batch  398 / 525  Training Loss  0.0021087925415486097\n",
            "Epoch  15 Batch  399 / 525  Training Loss  0.001577015733346343\n",
            "Epoch  15 Batch  400 / 525  Training Loss  0.0010288066696375608\n",
            "Epoch  15 Batch  401 / 525  Training Loss  0.0008039519307203591\n",
            "Epoch  15 Batch  402 / 525  Training Loss  0.0013435202417895198\n",
            "Epoch  15 Batch  403 / 525  Training Loss  0.0011335208546370268\n",
            "Epoch  15 Batch  404 / 525  Training Loss  0.0006723683327436447\n",
            "Epoch  15 Batch  405 / 525  Training Loss  0.0010274001397192478\n",
            "Epoch  15 Batch  406 / 525  Training Loss  0.007671870291233063\n",
            "Epoch  15 Batch  407 / 525  Training Loss  0.0011381341610103846\n",
            "Epoch  15 Batch  408 / 525  Training Loss  0.001184424851089716\n",
            "Epoch  15 Batch  409 / 525  Training Loss  0.0022169938310980797\n",
            "Epoch  15 Batch  410 / 525  Training Loss  0.008720353245735168\n",
            "Epoch  15 Batch  411 / 525  Training Loss  0.0004193252825643867\n",
            "Epoch  15 Batch  412 / 525  Training Loss  0.0006095488788560033\n",
            "Epoch  15 Batch  413 / 525  Training Loss  0.0016332445666193962\n",
            "Epoch  15 Batch  414 / 525  Training Loss  0.0026619727723300457\n",
            "Epoch  15 Batch  415 / 525  Training Loss  0.0005266476073302329\n",
            "Epoch  15 Batch  416 / 525  Training Loss  0.0009946702048182487\n",
            "Epoch  15 Batch  417 / 525  Training Loss  0.0012787969317287207\n",
            "Epoch  15 Batch  418 / 525  Training Loss  0.0005211656098254025\n",
            "Epoch  15 Batch  419 / 525  Training Loss  0.00037114592851139605\n",
            "Epoch  15 Batch  420 / 525  Training Loss  0.0004938792553730309\n",
            "Epoch  15 Batch  421 / 525  Training Loss  0.0019125740509480238\n",
            "Epoch  15 Batch  422 / 525  Training Loss  0.0007241858984343708\n",
            "Epoch  15 Batch  423 / 525  Training Loss  0.0006534105050377548\n",
            "Epoch  15 Batch  424 / 525  Training Loss  0.0005593941314145923\n",
            "Epoch  15 Batch  425 / 525  Training Loss  0.0010319818975403905\n",
            "Epoch  15 Batch  426 / 525  Training Loss  0.000730230298358947\n",
            "Epoch  15 Batch  427 / 525  Training Loss  0.0003184978850185871\n",
            "Epoch  15 Batch  428 / 525  Training Loss  0.0004941171500831842\n",
            "Epoch  15 Batch  429 / 525  Training Loss  0.0007883213693276048\n",
            "Epoch  15 Batch  430 / 525  Training Loss  0.0005830948357470334\n",
            "Epoch  15 Batch  431 / 525  Training Loss  0.0008000630186870694\n",
            "Epoch  15 Batch  432 / 525  Training Loss  0.0008657018770463765\n",
            "Epoch  15 Batch  433 / 525  Training Loss  0.0011048878077417612\n",
            "Epoch  15 Batch  434 / 525  Training Loss  0.0009626192040741444\n",
            "Epoch  15 Batch  435 / 525  Training Loss  0.0006990344263613224\n",
            "Epoch  15 Batch  436 / 525  Training Loss  0.0005514637450687587\n",
            "Epoch  15 Batch  437 / 525  Training Loss  0.0008388692513108253\n",
            "Epoch  15 Batch  438 / 525  Training Loss  0.0004131066089030355\n",
            "Epoch  15 Batch  439 / 525  Training Loss  0.0003920527524314821\n",
            "Epoch  15 Batch  440 / 525  Training Loss  0.000777744222432375\n",
            "Epoch  15 Batch  441 / 525  Training Loss  0.0004993473994545639\n",
            "Epoch  15 Batch  442 / 525  Training Loss  0.00047245994210243225\n",
            "Epoch  15 Batch  443 / 525  Training Loss  0.0007719006389379501\n",
            "Epoch  15 Batch  444 / 525  Training Loss  0.0011454093037173152\n",
            "Epoch  15 Batch  445 / 525  Training Loss  0.0006013690144754946\n",
            "Epoch  15 Batch  446 / 525  Training Loss  0.0005911864573135972\n",
            "Epoch  15 Batch  447 / 525  Training Loss  0.00027225393569096923\n",
            "Epoch  15 Batch  448 / 525  Training Loss  0.000922232517041266\n",
            "Epoch  15 Batch  449 / 525  Training Loss  0.0008526953752152622\n",
            "Epoch  15 Batch  450 / 525  Training Loss  0.0015945334453135729\n",
            "Epoch  15 Batch  451 / 525  Training Loss  0.0011550390627235174\n",
            "Epoch  15 Batch  452 / 525  Training Loss  0.0012646166142076254\n",
            "Epoch  15 Batch  453 / 525  Training Loss  0.0005256776930764318\n",
            "Epoch  15 Batch  454 / 525  Training Loss  0.00042279832996428013\n",
            "Epoch  15 Batch  455 / 525  Training Loss  0.000643848383333534\n",
            "Epoch  15 Batch  456 / 525  Training Loss  0.00038356840377673507\n",
            "Epoch  15 Batch  457 / 525  Training Loss  0.0011116553796455264\n",
            "Epoch  15 Batch  458 / 525  Training Loss  0.0003716136852744967\n",
            "Epoch  15 Batch  459 / 525  Training Loss  0.00033022830029949546\n",
            "Epoch  15 Batch  460 / 525  Training Loss  0.0004384214407764375\n",
            "Epoch  15 Batch  461 / 525  Training Loss  0.0006987765082158148\n",
            "Epoch  15 Batch  462 / 525  Training Loss  0.0004890187992714345\n",
            "Epoch  15 Batch  463 / 525  Training Loss  0.000563140376470983\n",
            "Epoch  15 Batch  464 / 525  Training Loss  0.0007707359036430717\n",
            "Epoch  15 Batch  465 / 525  Training Loss  0.0003029770159628242\n",
            "Epoch  15 Batch  466 / 525  Training Loss  0.0011312433052808046\n",
            "Epoch  15 Batch  467 / 525  Training Loss  0.00046582837239839137\n",
            "Epoch  15 Batch  468 / 525  Training Loss  0.0004890575655736029\n",
            "Epoch  15 Batch  469 / 525  Training Loss  0.0004296090919524431\n",
            "Epoch  15 Batch  470 / 525  Training Loss  0.00048798276111483574\n",
            "Epoch  15 Batch  471 / 525  Training Loss  0.00122284225653857\n",
            "Epoch  15 Batch  472 / 525  Training Loss  0.001040630042552948\n",
            "Epoch  15 Batch  473 / 525  Training Loss  0.008551479317247868\n",
            "Epoch  15 Batch  474 / 525  Training Loss  0.0006451405351981521\n",
            "Epoch  15 Batch  475 / 525  Training Loss  0.0025729616172611713\n",
            "Epoch  15 Batch  476 / 525  Training Loss  0.0008100189152173698\n",
            "Epoch  15 Batch  477 / 525  Training Loss  0.0011952845379710197\n",
            "Epoch  15 Batch  478 / 525  Training Loss  0.005457159131765366\n",
            "Epoch  15 Batch  479 / 525  Training Loss  0.0005063419230282307\n",
            "Epoch  15 Batch  480 / 525  Training Loss  0.0012272554449737072\n",
            "Epoch  15 Batch  481 / 525  Training Loss  0.0011461542453616858\n",
            "Epoch  15 Batch  482 / 525  Training Loss  0.0007712176884524524\n",
            "Epoch  15 Batch  483 / 525  Training Loss  0.003839869052171707\n",
            "Epoch  15 Batch  484 / 525  Training Loss  0.001868593506515026\n",
            "Epoch  15 Batch  485 / 525  Training Loss  0.0007585081038996577\n",
            "Epoch  15 Batch  486 / 525  Training Loss  0.0008643442997708917\n",
            "Epoch  15 Batch  487 / 525  Training Loss  0.0014362034853547812\n",
            "Epoch  15 Batch  488 / 525  Training Loss  0.0006904631154611707\n",
            "Epoch  15 Batch  489 / 525  Training Loss  0.00033846060978248715\n",
            "Epoch  15 Batch  490 / 525  Training Loss  0.001360636786557734\n",
            "Epoch  15 Batch  491 / 525  Training Loss  0.0007651745690964162\n",
            "Epoch  15 Batch  492 / 525  Training Loss  0.0007149977027438581\n",
            "Epoch  15 Batch  493 / 525  Training Loss  0.0008442261023446918\n",
            "Epoch  15 Batch  494 / 525  Training Loss  0.0015333987539634109\n",
            "Epoch  15 Batch  495 / 525  Training Loss  0.0009870084468275309\n",
            "Epoch  15 Batch  496 / 525  Training Loss  0.0031646874267607927\n",
            "Epoch  15 Batch  497 / 525  Training Loss  0.0005962472641840577\n",
            "Epoch  15 Batch  498 / 525  Training Loss  0.0011038979282602668\n",
            "Epoch  15 Batch  499 / 525  Training Loss  0.001130363205447793\n",
            "Epoch  15 Batch  500 / 525  Training Loss  0.000823371869046241\n",
            "Epoch  15 Batch  501 / 525  Training Loss  0.0019314701203256845\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  15 Batch  502 / 525  Training Loss  0.009482532739639282\n",
            "Epoch  15 Batch  503 / 525  Training Loss  0.0017759480979293585\n",
            "Epoch  15 Batch  504 / 525  Training Loss  0.0019056741148233414\n",
            "Epoch  15 Batch  505 / 525  Training Loss  0.0005243298946879804\n",
            "Epoch  15 Batch  506 / 525  Training Loss  0.0010008432436734438\n",
            "Epoch  15 Batch  507 / 525  Training Loss  0.0012670085998252034\n",
            "Epoch  15 Batch  508 / 525  Training Loss  0.00092939508613199\n",
            "Epoch  15 Batch  509 / 525  Training Loss  0.0008479886455461383\n",
            "Epoch  15 Batch  510 / 525  Training Loss  0.002535514533519745\n",
            "Epoch  15 Batch  511 / 525  Training Loss  0.002970126923173666\n",
            "Epoch  15 Batch  512 / 525  Training Loss  0.000987526262179017\n",
            "Epoch  15 Batch  513 / 525  Training Loss  0.0025040896143764257\n",
            "Epoch  15 Batch  514 / 525  Training Loss  0.0005839003715664148\n",
            "Epoch  15 Batch  515 / 525  Training Loss  0.0008004253031685948\n",
            "Epoch  15 Batch  516 / 525  Training Loss  0.0006115384167060256\n",
            "Epoch  15 Batch  517 / 525  Training Loss  0.002392454771324992\n",
            "Epoch  15 Batch  518 / 525  Training Loss  0.001206928980536759\n",
            "Epoch  15 Batch  519 / 525  Training Loss  0.0008862416143529117\n",
            "Epoch  15 Batch  520 / 525  Training Loss  0.0007310053915716708\n",
            "Epoch  15 Batch  521 / 525  Training Loss  0.0005853892071172595\n",
            "Epoch  15 Batch  522 / 525  Training Loss  0.0012187226675450802\n",
            "Epoch  15 Batch  523 / 525  Training Loss  0.0025083241052925587\n",
            "Epoch  15 Batch  524 / 525  Training Loss  0.000501960632391274\n",
            "  16    |    -    |   0.001341   |   63.59  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 16\n",
            "Epoch  16 Batch  0 / 525  Training Loss  0.012649612501263618\n",
            "Epoch  16 Batch  1 / 525  Training Loss  0.0007052439032122493\n",
            "Epoch  16 Batch  2 / 525  Training Loss  0.0010184812126681209\n",
            "Epoch  16 Batch  3 / 525  Training Loss  0.0009070113301277161\n",
            "Epoch  16 Batch  4 / 525  Training Loss  0.0005753031000494957\n",
            "Epoch  16 Batch  5 / 525  Training Loss  0.0006398169207386672\n",
            "Epoch  16 Batch  6 / 525  Training Loss  0.0005993141094222665\n",
            "Epoch  16 Batch  7 / 525  Training Loss  0.0004553467151708901\n",
            "Epoch  16 Batch  8 / 525  Training Loss  0.0005719335749745369\n",
            "Epoch  16 Batch  9 / 525  Training Loss  0.0005026516155339777\n",
            "Epoch  16 Batch  10 / 525  Training Loss  0.0003428921045269817\n",
            "Epoch  16 Batch  11 / 525  Training Loss  0.0005183383473195136\n",
            "Epoch  16 Batch  12 / 525  Training Loss  0.0006992233102209866\n",
            "Epoch  16 Batch  13 / 525  Training Loss  0.0007301281439140439\n",
            "Epoch  16 Batch  14 / 525  Training Loss  0.0009743844857439399\n",
            "Epoch  16 Batch  15 / 525  Training Loss  0.006565807852894068\n",
            "Epoch  16 Batch  16 / 525  Training Loss  0.0004842875059694052\n",
            "Epoch  16 Batch  17 / 525  Training Loss  0.00037617029738612473\n",
            "Epoch  16 Batch  18 / 525  Training Loss  0.0005800193757750094\n",
            "Epoch  16 Batch  19 / 525  Training Loss  0.0008496628142893314\n",
            "Epoch  16 Batch  20 / 525  Training Loss  0.00039365451084449887\n",
            "Epoch  16 Batch  21 / 525  Training Loss  0.0024310899898409843\n",
            "Epoch  16 Batch  22 / 525  Training Loss  0.006518244743347168\n",
            "Epoch  16 Batch  23 / 525  Training Loss  0.0005830758018419147\n",
            "Epoch  16 Batch  24 / 525  Training Loss  0.0008486703736707568\n",
            "Epoch  16 Batch  25 / 525  Training Loss  0.0008261579205282032\n",
            "Epoch  16 Batch  26 / 525  Training Loss  0.00033124859328381717\n",
            "Epoch  16 Batch  27 / 525  Training Loss  0.001913873478770256\n",
            "Epoch  16 Batch  28 / 525  Training Loss  0.0002899145765695721\n",
            "Epoch  16 Batch  29 / 525  Training Loss  0.00048468238674104214\n",
            "Epoch  16 Batch  30 / 525  Training Loss  0.0006261103553697467\n",
            "Epoch  16 Batch  31 / 525  Training Loss  0.0004310531076043844\n",
            "Epoch  16 Batch  32 / 525  Training Loss  0.00028023257618770003\n",
            "Epoch  16 Batch  33 / 525  Training Loss  0.0005382620729506016\n",
            "Epoch  16 Batch  34 / 525  Training Loss  0.00044642825378105044\n",
            "Epoch  16 Batch  35 / 525  Training Loss  0.0003983860951848328\n",
            "Epoch  16 Batch  36 / 525  Training Loss  0.0005303059588186443\n",
            "Epoch  16 Batch  37 / 525  Training Loss  0.0005764930392615497\n",
            "Epoch  16 Batch  38 / 525  Training Loss  0.00037857290590181947\n",
            "Epoch  16 Batch  39 / 525  Training Loss  0.0005290069384500384\n",
            "Epoch  16 Batch  40 / 525  Training Loss  0.0012673098826780915\n",
            "Epoch  16 Batch  41 / 525  Training Loss  0.0004842901253141463\n",
            "Epoch  16 Batch  42 / 525  Training Loss  0.0003101134207099676\n",
            "Epoch  16 Batch  43 / 525  Training Loss  0.000502386421430856\n",
            "Epoch  16 Batch  44 / 525  Training Loss  0.0003947142977267504\n",
            "Epoch  16 Batch  45 / 525  Training Loss  0.0005351126892492175\n",
            "Epoch  16 Batch  46 / 525  Training Loss  0.0006569300312548876\n",
            "Epoch  16 Batch  47 / 525  Training Loss  0.0011189423967152834\n",
            "Epoch  16 Batch  48 / 525  Training Loss  0.0010180281242355704\n",
            "Epoch  16 Batch  49 / 525  Training Loss  0.000532051082700491\n",
            "Epoch  16 Batch  50 / 525  Training Loss  0.00036187871592119336\n",
            "Epoch  16 Batch  51 / 525  Training Loss  0.00040452260873280466\n",
            "Epoch  16 Batch  52 / 525  Training Loss  0.0005807496490888298\n",
            "Epoch  16 Batch  53 / 525  Training Loss  0.000470777420559898\n",
            "Epoch  16 Batch  54 / 525  Training Loss  0.0004567071737255901\n",
            "Epoch  16 Batch  55 / 525  Training Loss  0.0009141263435594738\n",
            "Epoch  16 Batch  56 / 525  Training Loss  0.0003831428475677967\n",
            "Epoch  16 Batch  57 / 525  Training Loss  0.0006911948439665139\n",
            "Epoch  16 Batch  58 / 525  Training Loss  0.0005046158912591636\n",
            "Epoch  16 Batch  59 / 525  Training Loss  0.00042067532194778323\n",
            "Epoch  16 Batch  60 / 525  Training Loss  0.0004106861597392708\n",
            "Epoch  16 Batch  61 / 525  Training Loss  0.000489048077724874\n",
            "Epoch  16 Batch  62 / 525  Training Loss  0.0002895171637646854\n",
            "Epoch  16 Batch  63 / 525  Training Loss  0.0007690227357670665\n",
            "Epoch  16 Batch  64 / 525  Training Loss  0.0002671190013643354\n",
            "Epoch  16 Batch  65 / 525  Training Loss  0.0003480635932646692\n",
            "Epoch  16 Batch  66 / 525  Training Loss  0.0006170537672005594\n",
            "Epoch  16 Batch  67 / 525  Training Loss  0.00025911472039297223\n",
            "Epoch  16 Batch  68 / 525  Training Loss  0.00026709813391789794\n",
            "Epoch  16 Batch  69 / 525  Training Loss  0.0004893123987130821\n",
            "Epoch  16 Batch  70 / 525  Training Loss  0.0003292970359325409\n",
            "Epoch  16 Batch  71 / 525  Training Loss  0.00035894804750569165\n",
            "Epoch  16 Batch  72 / 525  Training Loss  0.0010282767470926046\n",
            "Epoch  16 Batch  73 / 525  Training Loss  0.00041478307684883475\n",
            "Epoch  16 Batch  74 / 525  Training Loss  0.0005688329692929983\n",
            "Epoch  16 Batch  75 / 525  Training Loss  0.00026688631623983383\n",
            "Epoch  16 Batch  76 / 525  Training Loss  0.0004519941285252571\n",
            "Epoch  16 Batch  77 / 525  Training Loss  0.0002439511736156419\n",
            "Epoch  16 Batch  78 / 525  Training Loss  0.0005257630837149918\n",
            "Epoch  16 Batch  79 / 525  Training Loss  0.0004926773253828287\n",
            "Epoch  16 Batch  80 / 525  Training Loss  0.00034545559901744127\n",
            "Epoch  16 Batch  81 / 525  Training Loss  0.00033893651561811566\n",
            "Epoch  16 Batch  82 / 525  Training Loss  0.00042917748214676976\n",
            "Epoch  16 Batch  83 / 525  Training Loss  0.00043226536945439875\n",
            "Epoch  16 Batch  84 / 525  Training Loss  0.0003877549897879362\n",
            "Epoch  16 Batch  85 / 525  Training Loss  0.0014901752583682537\n",
            "Epoch  16 Batch  86 / 525  Training Loss  0.0014539594994857907\n",
            "Epoch  16 Batch  87 / 525  Training Loss  0.0005350153078325093\n",
            "Epoch  16 Batch  88 / 525  Training Loss  0.00046350067714229226\n",
            "Epoch  16 Batch  89 / 525  Training Loss  0.0010095685720443726\n",
            "Epoch  16 Batch  90 / 525  Training Loss  0.00040901536704041064\n",
            "Epoch  16 Batch  91 / 525  Training Loss  0.000426355138188228\n",
            "Epoch  16 Batch  92 / 525  Training Loss  0.0009093255503103137\n",
            "Epoch  16 Batch  93 / 525  Training Loss  0.00024141697213053703\n",
            "Epoch  16 Batch  94 / 525  Training Loss  0.0014139350969344378\n",
            "Epoch  16 Batch  95 / 525  Training Loss  0.0007207871531136334\n",
            "Epoch  16 Batch  96 / 525  Training Loss  0.000413485657190904\n",
            "Epoch  16 Batch  97 / 525  Training Loss  0.0003161771164741367\n",
            "Epoch  16 Batch  98 / 525  Training Loss  0.00042463879799470305\n",
            "Epoch  16 Batch  99 / 525  Training Loss  0.0005250497488304973\n",
            "Epoch  16 Batch  100 / 525  Training Loss  0.0003699646331369877\n",
            "Epoch  16 Batch  101 / 525  Training Loss  0.00042996712727472186\n",
            "Epoch  16 Batch  102 / 525  Training Loss  0.0006173567962832749\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  16 Batch  103 / 525  Training Loss  0.0002904366992879659\n",
            "Epoch  16 Batch  104 / 525  Training Loss  0.000711737317033112\n",
            "Epoch  16 Batch  105 / 525  Training Loss  0.0002538158150855452\n",
            "Epoch  16 Batch  106 / 525  Training Loss  0.0005190639640204608\n",
            "Epoch  16 Batch  107 / 525  Training Loss  0.0003362144925631583\n",
            "Epoch  16 Batch  108 / 525  Training Loss  0.0020751822739839554\n",
            "Epoch  16 Batch  109 / 525  Training Loss  0.000481089431559667\n",
            "Epoch  16 Batch  110 / 525  Training Loss  0.00022860139142721891\n",
            "Epoch  16 Batch  111 / 525  Training Loss  0.0003486338828224689\n",
            "Epoch  16 Batch  112 / 525  Training Loss  0.00028259417740628123\n",
            "Epoch  16 Batch  113 / 525  Training Loss  0.0005016042850911617\n",
            "Epoch  16 Batch  114 / 525  Training Loss  0.0002667057851795107\n",
            "Epoch  16 Batch  115 / 525  Training Loss  0.0018116889987140894\n",
            "Epoch  16 Batch  116 / 525  Training Loss  0.00367225194349885\n",
            "Epoch  16 Batch  117 / 525  Training Loss  0.0005404391558840871\n",
            "Epoch  16 Batch  118 / 525  Training Loss  0.0005466148140840232\n",
            "Epoch  16 Batch  119 / 525  Training Loss  0.0005479366518557072\n",
            "Epoch  16 Batch  120 / 525  Training Loss  0.0003937554429285228\n",
            "Epoch  16 Batch  121 / 525  Training Loss  0.0002557191182859242\n",
            "Epoch  16 Batch  122 / 525  Training Loss  0.000624613487161696\n",
            "Epoch  16 Batch  123 / 525  Training Loss  0.00026639894349500537\n",
            "Epoch  16 Batch  124 / 525  Training Loss  0.00036267368705011904\n",
            "Epoch  16 Batch  125 / 525  Training Loss  0.0005622554454021156\n",
            "Epoch  16 Batch  126 / 525  Training Loss  0.0003726838913280517\n",
            "Epoch  16 Batch  127 / 525  Training Loss  0.00033679994521662593\n",
            "Epoch  16 Batch  128 / 525  Training Loss  0.00033806212013587356\n",
            "Epoch  16 Batch  129 / 525  Training Loss  0.00042436906369403005\n",
            "Epoch  16 Batch  130 / 525  Training Loss  0.000582419685088098\n",
            "Epoch  16 Batch  131 / 525  Training Loss  0.00040134129812940955\n",
            "Epoch  16 Batch  132 / 525  Training Loss  0.0005034242058172822\n",
            "Epoch  16 Batch  133 / 525  Training Loss  0.0005081983981654048\n",
            "Epoch  16 Batch  134 / 525  Training Loss  0.0005685362848453224\n",
            "Epoch  16 Batch  135 / 525  Training Loss  0.0004741410375572741\n",
            "Epoch  16 Batch  136 / 525  Training Loss  0.0002916147350333631\n",
            "Epoch  16 Batch  137 / 525  Training Loss  0.0016431051772087812\n",
            "Epoch  16 Batch  138 / 525  Training Loss  0.000613247393630445\n",
            "Epoch  16 Batch  139 / 525  Training Loss  0.0005634803092107177\n",
            "Epoch  16 Batch  140 / 525  Training Loss  0.00020191734074614942\n",
            "Epoch  16 Batch  141 / 525  Training Loss  0.0006639045896008611\n",
            "Epoch  16 Batch  142 / 525  Training Loss  0.0006564114009961486\n",
            "Epoch  16 Batch  143 / 525  Training Loss  0.006651288364082575\n",
            "Epoch  16 Batch  144 / 525  Training Loss  0.0005931959603913128\n",
            "Epoch  16 Batch  145 / 525  Training Loss  0.0006051267264410853\n",
            "Epoch  16 Batch  146 / 525  Training Loss  0.0005861253594048321\n",
            "Epoch  16 Batch  147 / 525  Training Loss  0.0008094877703115344\n",
            "Epoch  16 Batch  148 / 525  Training Loss  0.0021331426687538624\n",
            "Epoch  16 Batch  149 / 525  Training Loss  0.0016122082015499473\n",
            "Epoch  16 Batch  150 / 525  Training Loss  0.0006048822542652488\n",
            "Epoch  16 Batch  151 / 525  Training Loss  0.00031094177393242717\n",
            "Epoch  16 Batch  152 / 525  Training Loss  0.0015729404985904694\n",
            "Epoch  16 Batch  153 / 525  Training Loss  0.001071497448720038\n",
            "Epoch  16 Batch  154 / 525  Training Loss  0.0005716056330129504\n",
            "Epoch  16 Batch  155 / 525  Training Loss  0.0005047435406595469\n",
            "Epoch  16 Batch  156 / 525  Training Loss  0.0004859615582972765\n",
            "Epoch  16 Batch  157 / 525  Training Loss  0.0016638028901070356\n",
            "Epoch  16 Batch  158 / 525  Training Loss  0.0005506562301889062\n",
            "Epoch  16 Batch  159 / 525  Training Loss  0.0005377331981435418\n",
            "Epoch  16 Batch  160 / 525  Training Loss  0.0005037743248976767\n",
            "Epoch  16 Batch  161 / 525  Training Loss  0.0003128332900814712\n",
            "Epoch  16 Batch  162 / 525  Training Loss  0.0009078377042897046\n",
            "Epoch  16 Batch  163 / 525  Training Loss  0.00045107415644451976\n",
            "Epoch  16 Batch  164 / 525  Training Loss  0.0005465779104270041\n",
            "Epoch  16 Batch  165 / 525  Training Loss  0.0004566300194710493\n",
            "Epoch  16 Batch  166 / 525  Training Loss  0.0002872545155696571\n",
            "Epoch  16 Batch  167 / 525  Training Loss  0.0004139253869652748\n",
            "Epoch  16 Batch  168 / 525  Training Loss  0.0003588767140172422\n",
            "Epoch  16 Batch  169 / 525  Training Loss  0.0003300416865386069\n",
            "Epoch  16 Batch  170 / 525  Training Loss  0.0037702955305576324\n",
            "Epoch  16 Batch  171 / 525  Training Loss  0.00142741110175848\n",
            "Epoch  16 Batch  172 / 525  Training Loss  0.00040595472091808915\n",
            "Epoch  16 Batch  173 / 525  Training Loss  0.0007330745575018227\n",
            "Epoch  16 Batch  174 / 525  Training Loss  0.0005067231832072139\n",
            "Epoch  16 Batch  175 / 525  Training Loss  0.0005149539210833609\n",
            "Epoch  16 Batch  176 / 525  Training Loss  0.0004278516862541437\n",
            "Epoch  16 Batch  177 / 525  Training Loss  0.0003241639642510563\n",
            "Epoch  16 Batch  178 / 525  Training Loss  0.0008937112870626152\n",
            "Epoch  16 Batch  179 / 525  Training Loss  0.0006484479526989162\n",
            "Epoch  16 Batch  180 / 525  Training Loss  0.0007520841318182647\n",
            "Epoch  16 Batch  181 / 525  Training Loss  0.000787325669080019\n",
            "Epoch  16 Batch  182 / 525  Training Loss  0.000436950649600476\n",
            "Epoch  16 Batch  183 / 525  Training Loss  0.000941406178753823\n",
            "Epoch  16 Batch  184 / 525  Training Loss  0.0006381060229614377\n",
            "Epoch  16 Batch  185 / 525  Training Loss  0.0007452367572113872\n",
            "Epoch  16 Batch  186 / 525  Training Loss  0.0003725616552401334\n",
            "Epoch  16 Batch  187 / 525  Training Loss  0.0004861135093960911\n",
            "Epoch  16 Batch  188 / 525  Training Loss  0.00046214071335271\n",
            "Epoch  16 Batch  189 / 525  Training Loss  0.0006659438367933035\n",
            "Epoch  16 Batch  190 / 525  Training Loss  0.0006414519157260656\n",
            "Epoch  16 Batch  191 / 525  Training Loss  0.0028274357318878174\n",
            "Epoch  16 Batch  192 / 525  Training Loss  0.0006526661454699934\n",
            "Epoch  16 Batch  193 / 525  Training Loss  0.0007892629946582019\n",
            "Epoch  16 Batch  194 / 525  Training Loss  0.0005918710958212614\n",
            "Epoch  16 Batch  195 / 525  Training Loss  0.00047676629037596285\n",
            "Epoch  16 Batch  196 / 525  Training Loss  0.006256347056478262\n",
            "Epoch  16 Batch  197 / 525  Training Loss  0.0016473321011289954\n",
            "Epoch  16 Batch  198 / 525  Training Loss  0.0003737043880391866\n",
            "Epoch  16 Batch  199 / 525  Training Loss  0.000428312283474952\n",
            "Epoch  16 Batch  200 / 525  Training Loss  0.0023952540941536427\n",
            "Epoch  16 Batch  201 / 525  Training Loss  0.0002461584226693958\n",
            "Epoch  16 Batch  202 / 525  Training Loss  0.0021106142085045576\n",
            "Epoch  16 Batch  203 / 525  Training Loss  0.0008450258756056428\n",
            "Epoch  16 Batch  204 / 525  Training Loss  0.0004917251644656062\n",
            "Epoch  16 Batch  205 / 525  Training Loss  0.0008262906922027469\n",
            "Epoch  16 Batch  206 / 525  Training Loss  0.0008640674059279263\n",
            "Epoch  16 Batch  207 / 525  Training Loss  0.0005453212070278823\n",
            "Epoch  16 Batch  208 / 525  Training Loss  0.0010212314082309604\n",
            "Epoch  16 Batch  209 / 525  Training Loss  0.0009476726991124451\n",
            "Epoch  16 Batch  210 / 525  Training Loss  0.0002775589528027922\n",
            "Epoch  16 Batch  211 / 525  Training Loss  0.0018932657549157739\n",
            "Epoch  16 Batch  212 / 525  Training Loss  0.0005339759518392384\n",
            "Epoch  16 Batch  213 / 525  Training Loss  0.0003584139922168106\n",
            "Epoch  16 Batch  214 / 525  Training Loss  0.0004684966115746647\n",
            "Epoch  16 Batch  215 / 525  Training Loss  0.0005908909952268004\n",
            "Epoch  16 Batch  216 / 525  Training Loss  0.0004475695895962417\n",
            "Epoch  16 Batch  217 / 525  Training Loss  0.0005611733649857342\n",
            "Epoch  16 Batch  218 / 525  Training Loss  0.0006023591267876327\n",
            "Epoch  16 Batch  219 / 525  Training Loss  0.0004172971530351788\n",
            "Epoch  16 Batch  220 / 525  Training Loss  0.0008968853508122265\n",
            "Epoch  16 Batch  221 / 525  Training Loss  0.001297477399930358\n",
            "Epoch  16 Batch  222 / 525  Training Loss  0.0007911071879789233\n",
            "Epoch  16 Batch  223 / 525  Training Loss  0.0004149672167841345\n",
            "Epoch  16 Batch  224 / 525  Training Loss  0.0006077085272409022\n",
            "Epoch  16 Batch  225 / 525  Training Loss  0.0015834526857361197\n",
            "Epoch  16 Batch  226 / 525  Training Loss  0.0007633856730535626\n",
            "Epoch  16 Batch  227 / 525  Training Loss  0.0009193401783704758\n",
            "Epoch  16 Batch  228 / 525  Training Loss  0.0008020910318009555\n",
            "Epoch  16 Batch  229 / 525  Training Loss  0.0005063496646471322\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  16 Batch  230 / 525  Training Loss  0.00042971340008080006\n",
            "Epoch  16 Batch  231 / 525  Training Loss  0.0002779193746391684\n",
            "Epoch  16 Batch  232 / 525  Training Loss  0.000631630711723119\n",
            "Epoch  16 Batch  233 / 525  Training Loss  0.000516422267537564\n",
            "Epoch  16 Batch  234 / 525  Training Loss  0.00045481137931346893\n",
            "Epoch  16 Batch  235 / 525  Training Loss  0.001568106235936284\n",
            "Epoch  16 Batch  236 / 525  Training Loss  0.0017489080782979727\n",
            "Epoch  16 Batch  237 / 525  Training Loss  0.0004329271614551544\n",
            "Epoch  16 Batch  238 / 525  Training Loss  0.0012473388342186809\n",
            "Epoch  16 Batch  239 / 525  Training Loss  0.0033567938953638077\n",
            "Epoch  16 Batch  240 / 525  Training Loss  0.0018141188193112612\n",
            "Epoch  16 Batch  241 / 525  Training Loss  0.0007290566572919488\n",
            "Epoch  16 Batch  242 / 525  Training Loss  0.0008081095293164253\n",
            "Epoch  16 Batch  243 / 525  Training Loss  0.0003556123119778931\n",
            "Epoch  16 Batch  244 / 525  Training Loss  0.0006983147468417883\n",
            "Epoch  16 Batch  245 / 525  Training Loss  0.0005761695210821927\n",
            "Epoch  16 Batch  246 / 525  Training Loss  0.0032919305376708508\n",
            "Epoch  16 Batch  247 / 525  Training Loss  0.0009854539530351758\n",
            "Epoch  16 Batch  248 / 525  Training Loss  0.002853213343769312\n",
            "Epoch  16 Batch  249 / 525  Training Loss  0.0008097389945760369\n",
            "Epoch  16 Batch  250 / 525  Training Loss  0.000790438789408654\n",
            "Epoch  16 Batch  251 / 525  Training Loss  0.00045173329999670386\n",
            "Epoch  16 Batch  252 / 525  Training Loss  0.0010391816031187773\n",
            "Epoch  16 Batch  253 / 525  Training Loss  0.0003786220622714609\n",
            "Epoch  16 Batch  254 / 525  Training Loss  0.0008364531095139682\n",
            "Epoch  16 Batch  255 / 525  Training Loss  0.0008151796646416187\n",
            "Epoch  16 Batch  256 / 525  Training Loss  0.005425496958196163\n",
            "Epoch  16 Batch  257 / 525  Training Loss  0.0003744809073396027\n",
            "Epoch  16 Batch  258 / 525  Training Loss  0.0008585726609453559\n",
            "Epoch  16 Batch  259 / 525  Training Loss  0.0010927118128165603\n",
            "Epoch  16 Batch  260 / 525  Training Loss  0.000461007934063673\n",
            "Epoch  16 Batch  261 / 525  Training Loss  0.000634826545137912\n",
            "Epoch  16 Batch  262 / 525  Training Loss  0.000535433238837868\n",
            "Epoch  16 Batch  263 / 525  Training Loss  0.0009610548731870949\n",
            "Epoch  16 Batch  264 / 525  Training Loss  0.0003489017835818231\n",
            "Epoch  16 Batch  265 / 525  Training Loss  0.0004658569523598999\n",
            "Epoch  16 Batch  266 / 525  Training Loss  0.00044693759991787374\n",
            "Epoch  16 Batch  267 / 525  Training Loss  0.0007469427655451\n",
            "Epoch  16 Batch  268 / 525  Training Loss  0.0006082142353989184\n",
            "Epoch  16 Batch  269 / 525  Training Loss  0.003814753843471408\n",
            "Epoch  16 Batch  270 / 525  Training Loss  0.0007117738714441657\n",
            "Epoch  16 Batch  271 / 525  Training Loss  0.0009719701483845711\n",
            "Epoch  16 Batch  272 / 525  Training Loss  0.0006669430295005441\n",
            "Epoch  16 Batch  273 / 525  Training Loss  0.0007351620588451624\n",
            "Epoch  16 Batch  274 / 525  Training Loss  0.0006928221555426717\n",
            "Epoch  16 Batch  275 / 525  Training Loss  0.000705462007317692\n",
            "Epoch  16 Batch  276 / 525  Training Loss  0.0009937246795743704\n",
            "Epoch  16 Batch  277 / 525  Training Loss  0.0004359953454695642\n",
            "Epoch  16 Batch  278 / 525  Training Loss  0.0018791717011481524\n",
            "Epoch  16 Batch  279 / 525  Training Loss  0.00026943645207211375\n",
            "Epoch  16 Batch  280 / 525  Training Loss  0.00042453495552763343\n",
            "Epoch  16 Batch  281 / 525  Training Loss  0.00040126527892425656\n",
            "Epoch  16 Batch  282 / 525  Training Loss  0.001213461859151721\n",
            "Epoch  16 Batch  283 / 525  Training Loss  0.0005581136792898178\n",
            "Epoch  16 Batch  284 / 525  Training Loss  0.0004175486392341554\n",
            "Epoch  16 Batch  285 / 525  Training Loss  0.0012370930053293705\n",
            "Epoch  16 Batch  286 / 525  Training Loss  0.0003994256258010864\n",
            "Epoch  16 Batch  287 / 525  Training Loss  0.00044751184759661555\n",
            "Epoch  16 Batch  288 / 525  Training Loss  0.0010478042531758547\n",
            "Epoch  16 Batch  289 / 525  Training Loss  0.0005796808982267976\n",
            "Epoch  16 Batch  290 / 525  Training Loss  0.0006091538816690445\n",
            "Epoch  16 Batch  291 / 525  Training Loss  0.0016779478173702955\n",
            "Epoch  16 Batch  292 / 525  Training Loss  0.0005013320478610694\n",
            "Epoch  16 Batch  293 / 525  Training Loss  0.0004159136151429266\n",
            "Epoch  16 Batch  294 / 525  Training Loss  0.001976029947400093\n",
            "Epoch  16 Batch  295 / 525  Training Loss  0.0006783377612009645\n",
            "Epoch  16 Batch  296 / 525  Training Loss  0.0004046425747219473\n",
            "Epoch  16 Batch  297 / 525  Training Loss  0.000387073028832674\n",
            "Epoch  16 Batch  298 / 525  Training Loss  0.0006423685699701309\n",
            "Epoch  16 Batch  299 / 525  Training Loss  0.0007684949669055641\n",
            "Epoch  16 Batch  300 / 525  Training Loss  0.0001935998152475804\n",
            "Epoch  16 Batch  301 / 525  Training Loss  0.0003750741889234632\n",
            "Epoch  16 Batch  302 / 525  Training Loss  0.0003065642376895994\n",
            "Epoch  16 Batch  303 / 525  Training Loss  0.0003774842480197549\n",
            "Epoch  16 Batch  304 / 525  Training Loss  0.00042261515045538545\n",
            "Epoch  16 Batch  305 / 525  Training Loss  0.0004319853615015745\n",
            "Epoch  16 Batch  306 / 525  Training Loss  0.00032260824809782207\n",
            "Epoch  16 Batch  307 / 525  Training Loss  0.0004151565080974251\n",
            "Epoch  16 Batch  308 / 525  Training Loss  0.0004369928501546383\n",
            "Epoch  16 Batch  309 / 525  Training Loss  0.0007338453433476388\n",
            "Epoch  16 Batch  310 / 525  Training Loss  0.00040281953988596797\n",
            "Epoch  16 Batch  311 / 525  Training Loss  0.0004996919305995107\n",
            "Epoch  16 Batch  312 / 525  Training Loss  0.0004189695173408836\n",
            "Epoch  16 Batch  313 / 525  Training Loss  0.0008096579695120454\n",
            "Epoch  16 Batch  314 / 525  Training Loss  0.00046886177733540535\n",
            "Epoch  16 Batch  315 / 525  Training Loss  0.0003393758670426905\n",
            "Epoch  16 Batch  316 / 525  Training Loss  0.00039846813888289034\n",
            "Epoch  16 Batch  317 / 525  Training Loss  0.00022689970501232892\n",
            "Epoch  16 Batch  318 / 525  Training Loss  0.0004921670770272613\n",
            "Epoch  16 Batch  319 / 525  Training Loss  0.0004658953403122723\n",
            "Epoch  16 Batch  320 / 525  Training Loss  0.0006768601597286761\n",
            "Epoch  16 Batch  321 / 525  Training Loss  0.0010130906011909246\n",
            "Epoch  16 Batch  322 / 525  Training Loss  0.0002423000696580857\n",
            "Epoch  16 Batch  323 / 525  Training Loss  0.0011572502553462982\n",
            "Epoch  16 Batch  324 / 525  Training Loss  0.00048319631605409086\n",
            "Epoch  16 Batch  325 / 525  Training Loss  0.0004484595265239477\n",
            "Epoch  16 Batch  326 / 525  Training Loss  0.0006452130619436502\n",
            "Epoch  16 Batch  327 / 525  Training Loss  0.0003753722703550011\n",
            "Epoch  16 Batch  328 / 525  Training Loss  0.0005313258152455091\n",
            "Epoch  16 Batch  329 / 525  Training Loss  0.0007267934852279723\n",
            "Epoch  16 Batch  330 / 525  Training Loss  0.0011826857225969434\n",
            "Epoch  16 Batch  331 / 525  Training Loss  0.0006129060056991875\n",
            "Epoch  16 Batch  332 / 525  Training Loss  0.0007530392613261938\n",
            "Epoch  16 Batch  333 / 525  Training Loss  0.0002827224670909345\n",
            "Epoch  16 Batch  334 / 525  Training Loss  0.000547717500012368\n",
            "Epoch  16 Batch  335 / 525  Training Loss  0.00030970145598985255\n",
            "Epoch  16 Batch  336 / 525  Training Loss  0.0007175288046710193\n",
            "Epoch  16 Batch  337 / 525  Training Loss  0.000553219928406179\n",
            "Epoch  16 Batch  338 / 525  Training Loss  0.0003773859643843025\n",
            "Epoch  16 Batch  339 / 525  Training Loss  0.0004631319607142359\n",
            "Epoch  16 Batch  340 / 525  Training Loss  0.0013652513734996319\n",
            "Epoch  16 Batch  341 / 525  Training Loss  0.0006396478856913745\n",
            "Epoch  16 Batch  342 / 525  Training Loss  0.0013554701581597328\n",
            "Epoch  16 Batch  343 / 525  Training Loss  0.0006330047035589814\n",
            "Epoch  16 Batch  344 / 525  Training Loss  0.00025351456133648753\n",
            "Epoch  16 Batch  345 / 525  Training Loss  0.0005329596460796893\n",
            "Epoch  16 Batch  346 / 525  Training Loss  0.0003825595777016133\n",
            "Epoch  16 Batch  347 / 525  Training Loss  0.0002801588852889836\n",
            "Epoch  16 Batch  348 / 525  Training Loss  0.0015646654646843672\n",
            "Epoch  16 Batch  349 / 525  Training Loss  0.001170346513390541\n",
            "Epoch  16 Batch  350 / 525  Training Loss  0.0008791472646407783\n",
            "Epoch  16 Batch  351 / 525  Training Loss  0.00043468648800626397\n",
            "Epoch  16 Batch  352 / 525  Training Loss  0.0009146673837676644\n",
            "Epoch  16 Batch  353 / 525  Training Loss  0.0007385846693068743\n",
            "Epoch  16 Batch  354 / 525  Training Loss  0.009161075577139854\n",
            "Epoch  16 Batch  355 / 525  Training Loss  0.002881549298763275\n",
            "Epoch  16 Batch  356 / 525  Training Loss  0.003794446587562561\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  16 Batch  357 / 525  Training Loss  0.0014805425889790058\n",
            "Epoch  16 Batch  358 / 525  Training Loss  0.0009369843755848706\n",
            "Epoch  16 Batch  359 / 525  Training Loss  0.0010580375092104077\n",
            "Epoch  16 Batch  360 / 525  Training Loss  0.0027415347285568714\n",
            "Epoch  16 Batch  361 / 525  Training Loss  0.0005002813995815814\n",
            "Epoch  16 Batch  362 / 525  Training Loss  0.0005943445721641183\n",
            "Epoch  16 Batch  363 / 525  Training Loss  0.0005685407668352127\n",
            "Epoch  16 Batch  364 / 525  Training Loss  0.0002603805623948574\n",
            "Epoch  16 Batch  365 / 525  Training Loss  0.0007493032608181238\n",
            "Epoch  16 Batch  366 / 525  Training Loss  0.0012388855684548616\n",
            "Epoch  16 Batch  367 / 525  Training Loss  0.0005329501582309604\n",
            "Epoch  16 Batch  368 / 525  Training Loss  0.0005688761011697352\n",
            "Epoch  16 Batch  369 / 525  Training Loss  0.00042905163718387485\n",
            "Epoch  16 Batch  370 / 525  Training Loss  0.0005614262772724032\n",
            "Epoch  16 Batch  371 / 525  Training Loss  0.0007267845212481916\n",
            "Epoch  16 Batch  372 / 525  Training Loss  0.0007260775892063975\n",
            "Epoch  16 Batch  373 / 525  Training Loss  0.0008825788390822709\n",
            "Epoch  16 Batch  374 / 525  Training Loss  0.00030313944444060326\n",
            "Epoch  16 Batch  375 / 525  Training Loss  0.0003017967683263123\n",
            "Epoch  16 Batch  376 / 525  Training Loss  0.00046268789446912706\n",
            "Epoch  16 Batch  377 / 525  Training Loss  0.0004794680862687528\n",
            "Epoch  16 Batch  378 / 525  Training Loss  0.00027961484738625586\n",
            "Epoch  16 Batch  379 / 525  Training Loss  0.0005321566131897271\n",
            "Epoch  16 Batch  380 / 525  Training Loss  0.0013630238827317953\n",
            "Epoch  16 Batch  381 / 525  Training Loss  0.00046141204074956477\n",
            "Epoch  16 Batch  382 / 525  Training Loss  0.0006298368680290878\n",
            "Epoch  16 Batch  383 / 525  Training Loss  0.00035571487387642264\n",
            "Epoch  16 Batch  384 / 525  Training Loss  0.001372089609503746\n",
            "Epoch  16 Batch  385 / 525  Training Loss  0.016370464116334915\n",
            "Epoch  16 Batch  386 / 525  Training Loss  0.000444884120952338\n",
            "Epoch  16 Batch  387 / 525  Training Loss  0.0004490692517720163\n",
            "Epoch  16 Batch  388 / 525  Training Loss  0.0005229053786024451\n",
            "Epoch  16 Batch  389 / 525  Training Loss  0.0003813156799878925\n",
            "Epoch  16 Batch  390 / 525  Training Loss  0.0004231658240314573\n",
            "Epoch  16 Batch  391 / 525  Training Loss  0.0006171924178488553\n",
            "Epoch  16 Batch  392 / 525  Training Loss  0.0033330130390822887\n",
            "Epoch  16 Batch  393 / 525  Training Loss  0.0003731280448846519\n",
            "Epoch  16 Batch  394 / 525  Training Loss  0.0003770499024540186\n",
            "Epoch  16 Batch  395 / 525  Training Loss  0.0005086925812065601\n",
            "Epoch  16 Batch  396 / 525  Training Loss  0.0004096878692507744\n",
            "Epoch  16 Batch  397 / 525  Training Loss  0.0003243630926590413\n",
            "Epoch  16 Batch  398 / 525  Training Loss  0.0004480811767280102\n",
            "Epoch  16 Batch  399 / 525  Training Loss  0.0004242284339852631\n",
            "Epoch  16 Batch  400 / 525  Training Loss  0.0022704661823809147\n",
            "Epoch  16 Batch  401 / 525  Training Loss  0.0006043987814337015\n",
            "Epoch  16 Batch  402 / 525  Training Loss  0.0005884405691176653\n",
            "Epoch  16 Batch  403 / 525  Training Loss  0.000434430839959532\n",
            "Epoch  16 Batch  404 / 525  Training Loss  0.00040508032543584704\n",
            "Epoch  16 Batch  405 / 525  Training Loss  0.0012030595680698752\n",
            "Epoch  16 Batch  406 / 525  Training Loss  0.00045564869651570916\n",
            "Epoch  16 Batch  407 / 525  Training Loss  0.0005061873234808445\n",
            "Epoch  16 Batch  408 / 525  Training Loss  0.0003274155897088349\n",
            "Epoch  16 Batch  409 / 525  Training Loss  0.0006532050902023911\n",
            "Epoch  16 Batch  410 / 525  Training Loss  0.0004662374558392912\n",
            "Epoch  16 Batch  411 / 525  Training Loss  0.00035276944981887937\n",
            "Epoch  16 Batch  412 / 525  Training Loss  0.000671213143505156\n",
            "Epoch  16 Batch  413 / 525  Training Loss  0.0002255027211504057\n",
            "Epoch  16 Batch  414 / 525  Training Loss  0.00047945184633135796\n",
            "Epoch  16 Batch  415 / 525  Training Loss  0.00037011425592936575\n",
            "Epoch  16 Batch  416 / 525  Training Loss  0.0007006449159234762\n",
            "Epoch  16 Batch  417 / 525  Training Loss  0.005210136063396931\n",
            "Epoch  16 Batch  418 / 525  Training Loss  0.00048074586084112525\n",
            "Epoch  16 Batch  419 / 525  Training Loss  0.0005881169927306473\n",
            "Epoch  16 Batch  420 / 525  Training Loss  0.0004485949466470629\n",
            "Epoch  16 Batch  421 / 525  Training Loss  0.00040820991853252053\n",
            "Epoch  16 Batch  422 / 525  Training Loss  0.0005213426775299013\n",
            "Epoch  16 Batch  423 / 525  Training Loss  0.0004172106273472309\n",
            "Epoch  16 Batch  424 / 525  Training Loss  0.0009571417467668653\n",
            "Epoch  16 Batch  425 / 525  Training Loss  0.00048518957919441164\n",
            "Epoch  16 Batch  426 / 525  Training Loss  0.0003900449082721025\n",
            "Epoch  16 Batch  427 / 525  Training Loss  0.00040508914389647543\n",
            "Epoch  16 Batch  428 / 525  Training Loss  0.00039187935180962086\n",
            "Epoch  16 Batch  429 / 525  Training Loss  0.0006295900093391538\n",
            "Epoch  16 Batch  430 / 525  Training Loss  0.000369152519851923\n",
            "Epoch  16 Batch  431 / 525  Training Loss  0.00047019575140438974\n",
            "Epoch  16 Batch  432 / 525  Training Loss  0.0003059339360333979\n",
            "Epoch  16 Batch  433 / 525  Training Loss  0.0009128071251325309\n",
            "Epoch  16 Batch  434 / 525  Training Loss  0.0006569676334038377\n",
            "Epoch  16 Batch  435 / 525  Training Loss  0.00031609664438292384\n",
            "Epoch  16 Batch  436 / 525  Training Loss  0.0006588209653273225\n",
            "Epoch  16 Batch  437 / 525  Training Loss  0.0002583075547590852\n",
            "Epoch  16 Batch  438 / 525  Training Loss  0.0010481519857421517\n",
            "Epoch  16 Batch  439 / 525  Training Loss  0.00043558483594097197\n",
            "Epoch  16 Batch  440 / 525  Training Loss  0.0001693100930424407\n",
            "Epoch  16 Batch  441 / 525  Training Loss  0.00048170657828450203\n",
            "Epoch  16 Batch  442 / 525  Training Loss  0.00033924292074516416\n",
            "Epoch  16 Batch  443 / 525  Training Loss  0.00036139137228019536\n",
            "Epoch  16 Batch  444 / 525  Training Loss  0.0004417121526785195\n",
            "Epoch  16 Batch  445 / 525  Training Loss  0.00037819083081558347\n",
            "Epoch  16 Batch  446 / 525  Training Loss  0.0005531033384613693\n",
            "Epoch  16 Batch  447 / 525  Training Loss  0.00018382254347670823\n",
            "Epoch  16 Batch  448 / 525  Training Loss  0.00015541991160716861\n",
            "Epoch  16 Batch  449 / 525  Training Loss  0.0006426070467568934\n",
            "Epoch  16 Batch  450 / 525  Training Loss  0.0004694330564234406\n",
            "Epoch  16 Batch  451 / 525  Training Loss  0.0004544799157883972\n",
            "Epoch  16 Batch  452 / 525  Training Loss  0.000439146242570132\n",
            "Epoch  16 Batch  453 / 525  Training Loss  0.00029700688901357353\n",
            "Epoch  16 Batch  454 / 525  Training Loss  0.00047968869330361485\n",
            "Epoch  16 Batch  455 / 525  Training Loss  0.0005042903940193355\n",
            "Epoch  16 Batch  456 / 525  Training Loss  0.00014132607611827552\n",
            "Epoch  16 Batch  457 / 525  Training Loss  0.0007498052436858416\n",
            "Epoch  16 Batch  458 / 525  Training Loss  0.00033087501651607454\n",
            "Epoch  16 Batch  459 / 525  Training Loss  0.00035900360671803355\n",
            "Epoch  16 Batch  460 / 525  Training Loss  0.0004264528979547322\n",
            "Epoch  16 Batch  461 / 525  Training Loss  0.00032218178967013955\n",
            "Epoch  16 Batch  462 / 525  Training Loss  0.00037755974335595965\n",
            "Epoch  16 Batch  463 / 525  Training Loss  0.0003889066574629396\n",
            "Epoch  16 Batch  464 / 525  Training Loss  0.0005000907694920897\n",
            "Epoch  16 Batch  465 / 525  Training Loss  0.00037681590765714645\n",
            "Epoch  16 Batch  466 / 525  Training Loss  0.0002764997771009803\n",
            "Epoch  16 Batch  467 / 525  Training Loss  0.0004345639608800411\n",
            "Epoch  16 Batch  468 / 525  Training Loss  0.00045729088014923036\n",
            "Epoch  16 Batch  469 / 525  Training Loss  0.00023031712044030428\n",
            "Epoch  16 Batch  470 / 525  Training Loss  0.0007965088007040322\n",
            "Epoch  16 Batch  471 / 525  Training Loss  0.000533340557012707\n",
            "Epoch  16 Batch  472 / 525  Training Loss  0.00042224262142553926\n",
            "Epoch  16 Batch  473 / 525  Training Loss  0.0006214866880327463\n",
            "Epoch  16 Batch  474 / 525  Training Loss  0.0006138562457635999\n",
            "Epoch  16 Batch  475 / 525  Training Loss  0.0006062290631234646\n",
            "Epoch  16 Batch  476 / 525  Training Loss  0.0005755904130637646\n",
            "Epoch  16 Batch  477 / 525  Training Loss  0.0004806705401279032\n",
            "Epoch  16 Batch  478 / 525  Training Loss  0.0009854671079665422\n",
            "Epoch  16 Batch  479 / 525  Training Loss  0.0003138075699098408\n",
            "Epoch  16 Batch  480 / 525  Training Loss  0.00019522222282830626\n",
            "Epoch  16 Batch  481 / 525  Training Loss  0.00019617746875155717\n",
            "Epoch  16 Batch  482 / 525  Training Loss  0.0005356463370844722\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  16 Batch  483 / 525  Training Loss  0.00024245183158200234\n",
            "Epoch  16 Batch  484 / 525  Training Loss  0.0004072663723491132\n",
            "Epoch  16 Batch  485 / 525  Training Loss  0.0010906143579632044\n",
            "Epoch  16 Batch  486 / 525  Training Loss  0.00019205860735382885\n",
            "Epoch  16 Batch  487 / 525  Training Loss  0.0003314686182420701\n",
            "Epoch  16 Batch  488 / 525  Training Loss  0.0008257267763838172\n",
            "Epoch  16 Batch  489 / 525  Training Loss  0.00026263805921189487\n",
            "Epoch  16 Batch  490 / 525  Training Loss  0.00043390877544879913\n",
            "Epoch  16 Batch  491 / 525  Training Loss  0.00034214917104691267\n",
            "Epoch  16 Batch  492 / 525  Training Loss  0.00033108473871834576\n",
            "Epoch  16 Batch  493 / 525  Training Loss  0.00024151685647666454\n",
            "Epoch  16 Batch  494 / 525  Training Loss  0.0010607510339468718\n",
            "Epoch  16 Batch  495 / 525  Training Loss  0.0005293762078508735\n",
            "Epoch  16 Batch  496 / 525  Training Loss  0.00024238720652647316\n",
            "Epoch  16 Batch  497 / 525  Training Loss  0.0005326888058334589\n",
            "Epoch  16 Batch  498 / 525  Training Loss  0.0003613342996686697\n",
            "Epoch  16 Batch  499 / 525  Training Loss  0.000281021639239043\n",
            "Epoch  16 Batch  500 / 525  Training Loss  0.0005488411989063025\n",
            "Epoch  16 Batch  501 / 525  Training Loss  0.0004203198477625847\n",
            "Epoch  16 Batch  502 / 525  Training Loss  0.0009769193129613996\n",
            "Epoch  16 Batch  503 / 525  Training Loss  0.0003414613311178982\n",
            "Epoch  16 Batch  504 / 525  Training Loss  0.0005545967142097652\n",
            "Epoch  16 Batch  505 / 525  Training Loss  0.0016309653874486685\n",
            "Epoch  16 Batch  506 / 525  Training Loss  0.00024519258295185864\n",
            "Epoch  16 Batch  507 / 525  Training Loss  0.0005721014458686113\n",
            "Epoch  16 Batch  508 / 525  Training Loss  0.00030722643714398146\n",
            "Epoch  16 Batch  509 / 525  Training Loss  0.0002829161530826241\n",
            "Epoch  16 Batch  510 / 525  Training Loss  0.0005331902066245675\n",
            "Epoch  16 Batch  511 / 525  Training Loss  0.00029685007757507265\n",
            "Epoch  16 Batch  512 / 525  Training Loss  0.00036415638169273734\n",
            "Epoch  16 Batch  513 / 525  Training Loss  0.0005607798811979592\n",
            "Epoch  16 Batch  514 / 525  Training Loss  0.0005722111673094332\n",
            "Epoch  16 Batch  515 / 525  Training Loss  0.00026274187257513404\n",
            "Epoch  16 Batch  516 / 525  Training Loss  0.00036600217572413385\n",
            "Epoch  16 Batch  517 / 525  Training Loss  0.0003137296880595386\n",
            "Epoch  16 Batch  518 / 525  Training Loss  0.00045584808685816824\n",
            "Epoch  16 Batch  519 / 525  Training Loss  0.0009150521946139634\n",
            "Epoch  16 Batch  520 / 525  Training Loss  0.0007062564836815\n",
            "Epoch  16 Batch  521 / 525  Training Loss  0.00046746167936362326\n",
            "Epoch  16 Batch  522 / 525  Training Loss  0.0003044733894057572\n",
            "Epoch  16 Batch  523 / 525  Training Loss  0.0003780687693506479\n",
            "Epoch  16 Batch  524 / 525  Training Loss  0.00029929401353001595\n",
            "  17    |    -    |   0.000799   |   63.85  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 17\n",
            "Epoch  17 Batch  0 / 525  Training Loss  0.00024946779012680054\n",
            "Epoch  17 Batch  1 / 525  Training Loss  0.00023441606026608497\n",
            "Epoch  17 Batch  2 / 525  Training Loss  0.00021264274255372584\n",
            "Epoch  17 Batch  3 / 525  Training Loss  0.0003333047789055854\n",
            "Epoch  17 Batch  4 / 525  Training Loss  0.0001660911802900955\n",
            "Epoch  17 Batch  5 / 525  Training Loss  0.00021198097965680063\n",
            "Epoch  17 Batch  6 / 525  Training Loss  0.00015703123062849045\n",
            "Epoch  17 Batch  7 / 525  Training Loss  0.00025609906879253685\n",
            "Epoch  17 Batch  8 / 525  Training Loss  0.00018392328638583422\n",
            "Epoch  17 Batch  9 / 525  Training Loss  0.00043260297388769686\n",
            "Epoch  17 Batch  10 / 525  Training Loss  0.00023623897868674248\n",
            "Epoch  17 Batch  11 / 525  Training Loss  0.0004288788477424532\n",
            "Epoch  17 Batch  12 / 525  Training Loss  0.0002848038566298783\n",
            "Epoch  17 Batch  13 / 525  Training Loss  0.0001998512598220259\n",
            "Epoch  17 Batch  14 / 525  Training Loss  0.00020735737052746117\n",
            "Epoch  17 Batch  15 / 525  Training Loss  0.0004032255965285003\n",
            "Epoch  17 Batch  16 / 525  Training Loss  0.0002015500795096159\n",
            "Epoch  17 Batch  17 / 525  Training Loss  0.00027686377870850265\n",
            "Epoch  17 Batch  18 / 525  Training Loss  0.00020499902893789113\n",
            "Epoch  17 Batch  19 / 525  Training Loss  0.00022370352235157043\n",
            "Epoch  17 Batch  20 / 525  Training Loss  0.00034167038393206894\n",
            "Epoch  17 Batch  21 / 525  Training Loss  0.0003875631373375654\n",
            "Epoch  17 Batch  22 / 525  Training Loss  0.00022942782379686832\n",
            "Epoch  17 Batch  23 / 525  Training Loss  0.00041125057032331824\n",
            "Epoch  17 Batch  24 / 525  Training Loss  0.00020871339074801654\n",
            "Epoch  17 Batch  25 / 525  Training Loss  0.0002871844917535782\n",
            "Epoch  17 Batch  26 / 525  Training Loss  0.00037489106762222946\n",
            "Epoch  17 Batch  27 / 525  Training Loss  0.005289291962981224\n",
            "Epoch  17 Batch  28 / 525  Training Loss  0.00015489885117858648\n",
            "Epoch  17 Batch  29 / 525  Training Loss  0.000516305270139128\n",
            "Epoch  17 Batch  30 / 525  Training Loss  0.0003864358877763152\n",
            "Epoch  17 Batch  31 / 525  Training Loss  0.00048419073573313653\n",
            "Epoch  17 Batch  32 / 525  Training Loss  0.00035686162300407887\n",
            "Epoch  17 Batch  33 / 525  Training Loss  0.0014054389903321862\n",
            "Epoch  17 Batch  34 / 525  Training Loss  0.00028481578920036554\n",
            "Epoch  17 Batch  35 / 525  Training Loss  0.0007055504247546196\n",
            "Epoch  17 Batch  36 / 525  Training Loss  0.00028470088727772236\n",
            "Epoch  17 Batch  37 / 525  Training Loss  0.0001582202094141394\n",
            "Epoch  17 Batch  38 / 525  Training Loss  0.0007474658195860684\n",
            "Epoch  17 Batch  39 / 525  Training Loss  0.00028381714946590364\n",
            "Epoch  17 Batch  40 / 525  Training Loss  0.0003461902670096606\n",
            "Epoch  17 Batch  41 / 525  Training Loss  0.0002871998876798898\n",
            "Epoch  17 Batch  42 / 525  Training Loss  0.0003303003322798759\n",
            "Epoch  17 Batch  43 / 525  Training Loss  0.00017634322284720838\n",
            "Epoch  17 Batch  44 / 525  Training Loss  0.00042471670894883573\n",
            "Epoch  17 Batch  45 / 525  Training Loss  0.0002816107589751482\n",
            "Epoch  17 Batch  46 / 525  Training Loss  0.000280097039649263\n",
            "Epoch  17 Batch  47 / 525  Training Loss  0.0002614404074847698\n",
            "Epoch  17 Batch  48 / 525  Training Loss  0.0001784020132618025\n",
            "Epoch  17 Batch  49 / 525  Training Loss  0.00022067427926231176\n",
            "Epoch  17 Batch  50 / 525  Training Loss  0.000345557025866583\n",
            "Epoch  17 Batch  51 / 525  Training Loss  0.00022868378437124193\n",
            "Epoch  17 Batch  52 / 525  Training Loss  0.0005180169246159494\n",
            "Epoch  17 Batch  53 / 525  Training Loss  0.00029382319189608097\n",
            "Epoch  17 Batch  54 / 525  Training Loss  0.00016248984320554882\n",
            "Epoch  17 Batch  55 / 525  Training Loss  0.0005096473032608628\n",
            "Epoch  17 Batch  56 / 525  Training Loss  0.000299617211567238\n",
            "Epoch  17 Batch  57 / 525  Training Loss  0.0002452744811307639\n",
            "Epoch  17 Batch  58 / 525  Training Loss  0.00019923222134821117\n",
            "Epoch  17 Batch  59 / 525  Training Loss  0.00015042992890812457\n",
            "Epoch  17 Batch  60 / 525  Training Loss  0.00022473177523352206\n",
            "Epoch  17 Batch  61 / 525  Training Loss  0.00022001851175446063\n",
            "Epoch  17 Batch  62 / 525  Training Loss  0.00021670592832379043\n",
            "Epoch  17 Batch  63 / 525  Training Loss  0.0002784857642836869\n",
            "Epoch  17 Batch  64 / 525  Training Loss  0.0002597644634079188\n",
            "Epoch  17 Batch  65 / 525  Training Loss  0.00014827310224063694\n",
            "Epoch  17 Batch  66 / 525  Training Loss  0.0003008073545061052\n",
            "Epoch  17 Batch  67 / 525  Training Loss  0.00022944333613850176\n",
            "Epoch  17 Batch  68 / 525  Training Loss  0.0002797118213493377\n",
            "Epoch  17 Batch  69 / 525  Training Loss  0.00023786192468833178\n",
            "Epoch  17 Batch  70 / 525  Training Loss  0.0003898340801242739\n",
            "Epoch  17 Batch  71 / 525  Training Loss  0.00035702186869457364\n",
            "Epoch  17 Batch  72 / 525  Training Loss  0.0002536996908020228\n",
            "Epoch  17 Batch  73 / 525  Training Loss  0.0002169002837035805\n",
            "Epoch  17 Batch  74 / 525  Training Loss  0.0003200224309694022\n",
            "Epoch  17 Batch  75 / 525  Training Loss  0.0006817533285357058\n",
            "Epoch  17 Batch  76 / 525  Training Loss  0.0002702772035263479\n",
            "Epoch  17 Batch  77 / 525  Training Loss  0.00021870974160265177\n",
            "Epoch  17 Batch  78 / 525  Training Loss  0.0002724542864598334\n",
            "Epoch  17 Batch  79 / 525  Training Loss  0.0011069747852161527\n",
            "Epoch  17 Batch  80 / 525  Training Loss  0.00023590870841871947\n",
            "Epoch  17 Batch  81 / 525  Training Loss  0.00019470334518700838\n",
            "Epoch  17 Batch  82 / 525  Training Loss  0.0002443888515699655\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  17 Batch  83 / 525  Training Loss  0.00024331598251592368\n",
            "Epoch  17 Batch  84 / 525  Training Loss  0.00019143489771522582\n",
            "Epoch  17 Batch  85 / 525  Training Loss  0.00020079631940461695\n",
            "Epoch  17 Batch  86 / 525  Training Loss  0.00025339971762150526\n",
            "Epoch  17 Batch  87 / 525  Training Loss  0.00021915070828981698\n",
            "Epoch  17 Batch  88 / 525  Training Loss  0.00011810774594778195\n",
            "Epoch  17 Batch  89 / 525  Training Loss  0.0003700281376950443\n",
            "Epoch  17 Batch  90 / 525  Training Loss  0.0002773464366327971\n",
            "Epoch  17 Batch  91 / 525  Training Loss  0.0002527188917156309\n",
            "Epoch  17 Batch  92 / 525  Training Loss  0.00016671509365551174\n",
            "Epoch  17 Batch  93 / 525  Training Loss  0.00016059327754192054\n",
            "Epoch  17 Batch  94 / 525  Training Loss  0.00018161337357014418\n",
            "Epoch  17 Batch  95 / 525  Training Loss  0.0002487705205567181\n",
            "Epoch  17 Batch  96 / 525  Training Loss  0.00013996477355249226\n",
            "Epoch  17 Batch  97 / 525  Training Loss  0.00014130753697827458\n",
            "Epoch  17 Batch  98 / 525  Training Loss  0.0003096048894803971\n",
            "Epoch  17 Batch  99 / 525  Training Loss  0.0002126918116118759\n",
            "Epoch  17 Batch  100 / 525  Training Loss  0.0002615355188027024\n",
            "Epoch  17 Batch  101 / 525  Training Loss  0.00036318268394097686\n",
            "Epoch  17 Batch  102 / 525  Training Loss  0.0003025094629265368\n",
            "Epoch  17 Batch  103 / 525  Training Loss  0.0001954461185960099\n",
            "Epoch  17 Batch  104 / 525  Training Loss  0.00025898119201883674\n",
            "Epoch  17 Batch  105 / 525  Training Loss  0.00020314120047260076\n",
            "Epoch  17 Batch  106 / 525  Training Loss  0.000160634852363728\n",
            "Epoch  17 Batch  107 / 525  Training Loss  0.0002631261886563152\n",
            "Epoch  17 Batch  108 / 525  Training Loss  0.0002272912533953786\n",
            "Epoch  17 Batch  109 / 525  Training Loss  0.00040071303374134004\n",
            "Epoch  17 Batch  110 / 525  Training Loss  0.00013817427679896355\n",
            "Epoch  17 Batch  111 / 525  Training Loss  0.000194599837413989\n",
            "Epoch  17 Batch  112 / 525  Training Loss  0.00023700264864601195\n",
            "Epoch  17 Batch  113 / 525  Training Loss  0.00021524869953282177\n",
            "Epoch  17 Batch  114 / 525  Training Loss  0.00018910327344201505\n",
            "Epoch  17 Batch  115 / 525  Training Loss  0.0002522712165955454\n",
            "Epoch  17 Batch  116 / 525  Training Loss  0.00019680746481753886\n",
            "Epoch  17 Batch  117 / 525  Training Loss  0.00042820628732442856\n",
            "Epoch  17 Batch  118 / 525  Training Loss  0.00048166810302063823\n",
            "Epoch  17 Batch  119 / 525  Training Loss  0.0002075118973152712\n",
            "Epoch  17 Batch  120 / 525  Training Loss  0.0002879426465369761\n",
            "Epoch  17 Batch  121 / 525  Training Loss  0.00016682464047335088\n",
            "Epoch  17 Batch  122 / 525  Training Loss  0.00023345497902482748\n",
            "Epoch  17 Batch  123 / 525  Training Loss  0.00027822854463011026\n",
            "Epoch  17 Batch  124 / 525  Training Loss  0.00022368537611328065\n",
            "Epoch  17 Batch  125 / 525  Training Loss  0.00022660540707875043\n",
            "Epoch  17 Batch  126 / 525  Training Loss  0.000765792210586369\n",
            "Epoch  17 Batch  127 / 525  Training Loss  0.00014271536201704293\n",
            "Epoch  17 Batch  128 / 525  Training Loss  0.0001397513842675835\n",
            "Epoch  17 Batch  129 / 525  Training Loss  0.0002077228855341673\n",
            "Epoch  17 Batch  130 / 525  Training Loss  0.00017620314611122012\n",
            "Epoch  17 Batch  131 / 525  Training Loss  0.00017059755919035524\n",
            "Epoch  17 Batch  132 / 525  Training Loss  0.000246464362135157\n",
            "Epoch  17 Batch  133 / 525  Training Loss  0.00023356673773378134\n",
            "Epoch  17 Batch  134 / 525  Training Loss  0.00022049827384762466\n",
            "Epoch  17 Batch  135 / 525  Training Loss  0.0003214399912394583\n",
            "Epoch  17 Batch  136 / 525  Training Loss  0.00019540863286238164\n",
            "Epoch  17 Batch  137 / 525  Training Loss  0.0003252628375776112\n",
            "Epoch  17 Batch  138 / 525  Training Loss  0.00017114661750383675\n",
            "Epoch  17 Batch  139 / 525  Training Loss  0.00037873987457714975\n",
            "Epoch  17 Batch  140 / 525  Training Loss  0.00016248472093138844\n",
            "Epoch  17 Batch  141 / 525  Training Loss  0.00014441189705394208\n",
            "Epoch  17 Batch  142 / 525  Training Loss  0.0003323913551867008\n",
            "Epoch  17 Batch  143 / 525  Training Loss  0.00023411170695908368\n",
            "Epoch  17 Batch  144 / 525  Training Loss  0.00023380779020953923\n",
            "Epoch  17 Batch  145 / 525  Training Loss  0.0002466827572789043\n",
            "Epoch  17 Batch  146 / 525  Training Loss  0.0002392603491898626\n",
            "Epoch  17 Batch  147 / 525  Training Loss  0.00010673407086869702\n",
            "Epoch  17 Batch  148 / 525  Training Loss  0.00011756408639485016\n",
            "Epoch  17 Batch  149 / 525  Training Loss  0.00017352323629893363\n",
            "Epoch  17 Batch  150 / 525  Training Loss  0.00017064728308469057\n",
            "Epoch  17 Batch  151 / 525  Training Loss  0.00012593413703143597\n",
            "Epoch  17 Batch  152 / 525  Training Loss  0.0002578942512627691\n",
            "Epoch  17 Batch  153 / 525  Training Loss  0.0011936583323404193\n",
            "Epoch  17 Batch  154 / 525  Training Loss  0.001960428897291422\n",
            "Epoch  17 Batch  155 / 525  Training Loss  0.00028154189931228757\n",
            "Epoch  17 Batch  156 / 525  Training Loss  0.005338381975889206\n",
            "Epoch  17 Batch  157 / 525  Training Loss  0.00033069844357669353\n",
            "Epoch  17 Batch  158 / 525  Training Loss  0.0002983099257107824\n",
            "Epoch  17 Batch  159 / 525  Training Loss  0.0002803053648676723\n",
            "Epoch  17 Batch  160 / 525  Training Loss  0.0005295992596074939\n",
            "Epoch  17 Batch  161 / 525  Training Loss  0.0002942561695817858\n",
            "Epoch  17 Batch  162 / 525  Training Loss  0.00019338693527970463\n",
            "Epoch  17 Batch  163 / 525  Training Loss  0.0002580858126748353\n",
            "Epoch  17 Batch  164 / 525  Training Loss  0.00018599805480334908\n",
            "Epoch  17 Batch  165 / 525  Training Loss  0.0005056356312707067\n",
            "Epoch  17 Batch  166 / 525  Training Loss  0.0007184050045907497\n",
            "Epoch  17 Batch  167 / 525  Training Loss  0.00023512511688750237\n",
            "Epoch  17 Batch  168 / 525  Training Loss  0.00016842232435010374\n",
            "Epoch  17 Batch  169 / 525  Training Loss  0.00041926285484805703\n",
            "Epoch  17 Batch  170 / 525  Training Loss  0.0002318783663213253\n",
            "Epoch  17 Batch  171 / 525  Training Loss  0.0007758603896945715\n",
            "Epoch  17 Batch  172 / 525  Training Loss  0.0005591291119344532\n",
            "Epoch  17 Batch  173 / 525  Training Loss  0.000461919087683782\n",
            "Epoch  17 Batch  174 / 525  Training Loss  0.00038076250348240137\n",
            "Epoch  17 Batch  175 / 525  Training Loss  0.000769848411437124\n",
            "Epoch  17 Batch  176 / 525  Training Loss  0.00031981695792637765\n",
            "Epoch  17 Batch  177 / 525  Training Loss  0.0002494302170816809\n",
            "Epoch  17 Batch  178 / 525  Training Loss  0.0001918225607369095\n",
            "Epoch  17 Batch  179 / 525  Training Loss  0.00027852016501128674\n",
            "Epoch  17 Batch  180 / 525  Training Loss  0.00037639174843207\n",
            "Epoch  17 Batch  181 / 525  Training Loss  0.0003517156874295324\n",
            "Epoch  17 Batch  182 / 525  Training Loss  0.0003955218999180943\n",
            "Epoch  17 Batch  183 / 525  Training Loss  0.0003219093196094036\n",
            "Epoch  17 Batch  184 / 525  Training Loss  0.0003483143518678844\n",
            "Epoch  17 Batch  185 / 525  Training Loss  0.00018846732564270496\n",
            "Epoch  17 Batch  186 / 525  Training Loss  0.0006923741893842816\n",
            "Epoch  17 Batch  187 / 525  Training Loss  0.00032602506689727306\n",
            "Epoch  17 Batch  188 / 525  Training Loss  0.0002270636905450374\n",
            "Epoch  17 Batch  189 / 525  Training Loss  0.0001703122688923031\n",
            "Epoch  17 Batch  190 / 525  Training Loss  0.00031368827330879867\n",
            "Epoch  17 Batch  191 / 525  Training Loss  0.0003244851832278073\n",
            "Epoch  17 Batch  192 / 525  Training Loss  0.0005841708043590188\n",
            "Epoch  17 Batch  193 / 525  Training Loss  0.00021694258612114936\n",
            "Epoch  17 Batch  194 / 525  Training Loss  0.0008577856933698058\n",
            "Epoch  17 Batch  195 / 525  Training Loss  0.00018323872063774616\n",
            "Epoch  17 Batch  196 / 525  Training Loss  0.0002642979961819947\n",
            "Epoch  17 Batch  197 / 525  Training Loss  0.00027117907302454114\n",
            "Epoch  17 Batch  198 / 525  Training Loss  0.00019675488874781877\n",
            "Epoch  17 Batch  199 / 525  Training Loss  0.00027223429060541093\n",
            "Epoch  17 Batch  200 / 525  Training Loss  0.0004239647532813251\n",
            "Epoch  17 Batch  201 / 525  Training Loss  0.000190834078239277\n",
            "Epoch  17 Batch  202 / 525  Training Loss  0.0003280219971202314\n",
            "Epoch  17 Batch  203 / 525  Training Loss  0.00034420978045091033\n",
            "Epoch  17 Batch  204 / 525  Training Loss  0.0006094990530982614\n",
            "Epoch  17 Batch  205 / 525  Training Loss  0.0002614153490867466\n",
            "Epoch  17 Batch  206 / 525  Training Loss  0.0002541400899644941\n",
            "Epoch  17 Batch  207 / 525  Training Loss  0.0002069308393402025\n",
            "Epoch  17 Batch  208 / 525  Training Loss  0.0011585417669266462\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  17 Batch  209 / 525  Training Loss  0.0001720657164696604\n",
            "Epoch  17 Batch  210 / 525  Training Loss  0.0003030633379239589\n",
            "Epoch  17 Batch  211 / 525  Training Loss  0.0002101171703543514\n",
            "Epoch  17 Batch  212 / 525  Training Loss  0.00020176540419925004\n",
            "Epoch  17 Batch  213 / 525  Training Loss  0.00033318321220576763\n",
            "Epoch  17 Batch  214 / 525  Training Loss  0.00020720096654258668\n",
            "Epoch  17 Batch  215 / 525  Training Loss  0.00013796830899082124\n",
            "Epoch  17 Batch  216 / 525  Training Loss  0.00018712533346842974\n",
            "Epoch  17 Batch  217 / 525  Training Loss  0.0001808748784242198\n",
            "Epoch  17 Batch  218 / 525  Training Loss  0.0003497660218272358\n",
            "Epoch  17 Batch  219 / 525  Training Loss  0.00018152562552131712\n",
            "Epoch  17 Batch  220 / 525  Training Loss  0.00017368359840475023\n",
            "Epoch  17 Batch  221 / 525  Training Loss  0.0001947136188391596\n",
            "Epoch  17 Batch  222 / 525  Training Loss  0.00040911673568189144\n",
            "Epoch  17 Batch  223 / 525  Training Loss  0.0005406409036368132\n",
            "Epoch  17 Batch  224 / 525  Training Loss  0.0002591660595498979\n",
            "Epoch  17 Batch  225 / 525  Training Loss  0.0008604326285421848\n",
            "Epoch  17 Batch  226 / 525  Training Loss  0.00025183806428685784\n",
            "Epoch  17 Batch  227 / 525  Training Loss  0.0002403595281066373\n",
            "Epoch  17 Batch  228 / 525  Training Loss  0.00024235481396317482\n",
            "Epoch  17 Batch  229 / 525  Training Loss  0.00025120432837866247\n",
            "Epoch  17 Batch  230 / 525  Training Loss  0.0002918285899795592\n",
            "Epoch  17 Batch  231 / 525  Training Loss  0.0002248501405119896\n",
            "Epoch  17 Batch  232 / 525  Training Loss  0.00015564780915156007\n",
            "Epoch  17 Batch  233 / 525  Training Loss  0.0003817653632722795\n",
            "Epoch  17 Batch  234 / 525  Training Loss  0.0001961731759365648\n",
            "Epoch  17 Batch  235 / 525  Training Loss  0.0003256106865592301\n",
            "Epoch  17 Batch  236 / 525  Training Loss  0.00030946717015467584\n",
            "Epoch  17 Batch  237 / 525  Training Loss  0.00038066395791247487\n",
            "Epoch  17 Batch  238 / 525  Training Loss  0.0003167896647937596\n",
            "Epoch  17 Batch  239 / 525  Training Loss  0.000241678164456971\n",
            "Epoch  17 Batch  240 / 525  Training Loss  0.005716377403587103\n",
            "Epoch  17 Batch  241 / 525  Training Loss  0.00030241828062571585\n",
            "Epoch  17 Batch  242 / 525  Training Loss  0.00029703852487728\n",
            "Epoch  17 Batch  243 / 525  Training Loss  0.0015011252835392952\n",
            "Epoch  17 Batch  244 / 525  Training Loss  0.0003529042878653854\n",
            "Epoch  17 Batch  245 / 525  Training Loss  0.00030114222317934036\n",
            "Epoch  17 Batch  246 / 525  Training Loss  0.0007811346440576017\n",
            "Epoch  17 Batch  247 / 525  Training Loss  0.0015466667246073484\n",
            "Epoch  17 Batch  248 / 525  Training Loss  0.00022528201225213706\n",
            "Epoch  17 Batch  249 / 525  Training Loss  0.00045779667561873794\n",
            "Epoch  17 Batch  250 / 525  Training Loss  0.00024121103342622519\n",
            "Epoch  17 Batch  251 / 525  Training Loss  0.00046250285231508315\n",
            "Epoch  17 Batch  252 / 525  Training Loss  0.00039772066520527005\n",
            "Epoch  17 Batch  253 / 525  Training Loss  0.0002497385139577091\n",
            "Epoch  17 Batch  254 / 525  Training Loss  0.000343920080922544\n",
            "Epoch  17 Batch  255 / 525  Training Loss  0.000397541094571352\n",
            "Epoch  17 Batch  256 / 525  Training Loss  0.00020050672173965722\n",
            "Epoch  17 Batch  257 / 525  Training Loss  0.00026655112742446363\n",
            "Epoch  17 Batch  258 / 525  Training Loss  0.0002364714164286852\n",
            "Epoch  17 Batch  259 / 525  Training Loss  0.0001278662821277976\n",
            "Epoch  17 Batch  260 / 525  Training Loss  0.00027317024068906903\n",
            "Epoch  17 Batch  261 / 525  Training Loss  0.0004040373023599386\n",
            "Epoch  17 Batch  262 / 525  Training Loss  0.00037775651435367763\n",
            "Epoch  17 Batch  263 / 525  Training Loss  0.00026068236911669374\n",
            "Epoch  17 Batch  264 / 525  Training Loss  0.0002128763881046325\n",
            "Epoch  17 Batch  265 / 525  Training Loss  0.00025145814288407564\n",
            "Epoch  17 Batch  266 / 525  Training Loss  0.0006797694368287921\n",
            "Epoch  17 Batch  267 / 525  Training Loss  0.0003143725043628365\n",
            "Epoch  17 Batch  268 / 525  Training Loss  0.0003766843001358211\n",
            "Epoch  17 Batch  269 / 525  Training Loss  0.001931617734953761\n",
            "Epoch  17 Batch  270 / 525  Training Loss  0.0003585471713449806\n",
            "Epoch  17 Batch  271 / 525  Training Loss  0.0003075385757256299\n",
            "Epoch  17 Batch  272 / 525  Training Loss  0.0002639444137457758\n",
            "Epoch  17 Batch  273 / 525  Training Loss  0.0005000008386559784\n",
            "Epoch  17 Batch  274 / 525  Training Loss  0.0006816049572080374\n",
            "Epoch  17 Batch  275 / 525  Training Loss  0.00034210990997962654\n",
            "Epoch  17 Batch  276 / 525  Training Loss  0.0005413158214651048\n",
            "Epoch  17 Batch  277 / 525  Training Loss  0.0002630238304845989\n",
            "Epoch  17 Batch  278 / 525  Training Loss  0.00023436806804966182\n",
            "Epoch  17 Batch  279 / 525  Training Loss  0.0003536455042194575\n",
            "Epoch  17 Batch  280 / 525  Training Loss  0.00019840896129608154\n",
            "Epoch  17 Batch  281 / 525  Training Loss  0.00038004369707778096\n",
            "Epoch  17 Batch  282 / 525  Training Loss  0.00020032217435073107\n",
            "Epoch  17 Batch  283 / 525  Training Loss  0.0002246565418317914\n",
            "Epoch  17 Batch  284 / 525  Training Loss  0.0003824271843768656\n",
            "Epoch  17 Batch  285 / 525  Training Loss  0.00021745625417679548\n",
            "Epoch  17 Batch  286 / 525  Training Loss  0.0001806784566724673\n",
            "Epoch  17 Batch  287 / 525  Training Loss  0.00017138673865702003\n",
            "Epoch  17 Batch  288 / 525  Training Loss  0.0002058813552139327\n",
            "Epoch  17 Batch  289 / 525  Training Loss  0.000281407090369612\n",
            "Epoch  17 Batch  290 / 525  Training Loss  0.00014447394642047584\n",
            "Epoch  17 Batch  291 / 525  Training Loss  0.00023069728922564536\n",
            "Epoch  17 Batch  292 / 525  Training Loss  0.0005129251512698829\n",
            "Epoch  17 Batch  293 / 525  Training Loss  0.0003339198010507971\n",
            "Epoch  17 Batch  294 / 525  Training Loss  0.00034623773535713553\n",
            "Epoch  17 Batch  295 / 525  Training Loss  0.00016261462587863207\n",
            "Epoch  17 Batch  296 / 525  Training Loss  0.000359899626346305\n",
            "Epoch  17 Batch  297 / 525  Training Loss  0.00012094739213353023\n",
            "Epoch  17 Batch  298 / 525  Training Loss  0.00023714100825600326\n",
            "Epoch  17 Batch  299 / 525  Training Loss  0.0005201221792958677\n",
            "Epoch  17 Batch  300 / 525  Training Loss  0.00022093291045166552\n",
            "Epoch  17 Batch  301 / 525  Training Loss  0.00015416803944390267\n",
            "Epoch  17 Batch  302 / 525  Training Loss  0.0008301442721858621\n",
            "Epoch  17 Batch  303 / 525  Training Loss  0.000601807376369834\n",
            "Epoch  17 Batch  304 / 525  Training Loss  0.00021103373728692532\n",
            "Epoch  17 Batch  305 / 525  Training Loss  0.0003243802348151803\n",
            "Epoch  17 Batch  306 / 525  Training Loss  0.00016758826677687466\n",
            "Epoch  17 Batch  307 / 525  Training Loss  0.0002011026517720893\n",
            "Epoch  17 Batch  308 / 525  Training Loss  0.00026935042114928365\n",
            "Epoch  17 Batch  309 / 525  Training Loss  0.0002534663653932512\n",
            "Epoch  17 Batch  310 / 525  Training Loss  0.00037497826269827783\n",
            "Epoch  17 Batch  311 / 525  Training Loss  0.00029689676011912525\n",
            "Epoch  17 Batch  312 / 525  Training Loss  0.0001843824575189501\n",
            "Epoch  17 Batch  313 / 525  Training Loss  0.00022154227190185338\n",
            "Epoch  17 Batch  314 / 525  Training Loss  0.00024306029081344604\n",
            "Epoch  17 Batch  315 / 525  Training Loss  0.00021269272838253528\n",
            "Epoch  17 Batch  316 / 525  Training Loss  0.00031418108846992254\n",
            "Epoch  17 Batch  317 / 525  Training Loss  0.0002819335204549134\n",
            "Epoch  17 Batch  318 / 525  Training Loss  0.0004053554148413241\n",
            "Epoch  17 Batch  319 / 525  Training Loss  0.00019127041741739959\n",
            "Epoch  17 Batch  320 / 525  Training Loss  0.00028328440384939313\n",
            "Epoch  17 Batch  321 / 525  Training Loss  0.00023899684310890734\n",
            "Epoch  17 Batch  322 / 525  Training Loss  0.0003847201878670603\n",
            "Epoch  17 Batch  323 / 525  Training Loss  0.0002888356102630496\n",
            "Epoch  17 Batch  324 / 525  Training Loss  0.0002147289487766102\n",
            "Epoch  17 Batch  325 / 525  Training Loss  0.0001422852510586381\n",
            "Epoch  17 Batch  326 / 525  Training Loss  0.0002035034412983805\n",
            "Epoch  17 Batch  327 / 525  Training Loss  0.0005884787533432245\n",
            "Epoch  17 Batch  328 / 525  Training Loss  0.00033966489718295634\n",
            "Epoch  17 Batch  329 / 525  Training Loss  0.00024258813937194645\n",
            "Epoch  17 Batch  330 / 525  Training Loss  0.00028362584998831153\n",
            "Epoch  17 Batch  331 / 525  Training Loss  0.00021764915436506271\n",
            "Epoch  17 Batch  332 / 525  Training Loss  0.00023641044390387833\n",
            "Epoch  17 Batch  333 / 525  Training Loss  0.0002184687473345548\n",
            "Epoch  17 Batch  334 / 525  Training Loss  0.0019950715359300375\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  17 Batch  335 / 525  Training Loss  0.0002510994381736964\n",
            "Epoch  17 Batch  336 / 525  Training Loss  0.0002544264425523579\n",
            "Epoch  17 Batch  337 / 525  Training Loss  0.0002148551429854706\n",
            "Epoch  17 Batch  338 / 525  Training Loss  0.00029689137591049075\n",
            "Epoch  17 Batch  339 / 525  Training Loss  0.00017013898468576372\n",
            "Epoch  17 Batch  340 / 525  Training Loss  0.00014652246318291873\n",
            "Epoch  17 Batch  341 / 525  Training Loss  0.0005570431239902973\n",
            "Epoch  17 Batch  342 / 525  Training Loss  0.0003376103122718632\n",
            "Epoch  17 Batch  343 / 525  Training Loss  0.0005234507843852043\n",
            "Epoch  17 Batch  344 / 525  Training Loss  0.00047861546045169234\n",
            "Epoch  17 Batch  345 / 525  Training Loss  0.00018497974087949842\n",
            "Epoch  17 Batch  346 / 525  Training Loss  0.00020812665752600878\n",
            "Epoch  17 Batch  347 / 525  Training Loss  0.0002301247586729005\n",
            "Epoch  17 Batch  348 / 525  Training Loss  0.0001814293209463358\n",
            "Epoch  17 Batch  349 / 525  Training Loss  0.0002696900046430528\n",
            "Epoch  17 Batch  350 / 525  Training Loss  0.00016242233687080443\n",
            "Epoch  17 Batch  351 / 525  Training Loss  0.00029309216188266873\n",
            "Epoch  17 Batch  352 / 525  Training Loss  0.0003136905434075743\n",
            "Epoch  17 Batch  353 / 525  Training Loss  0.00030206370865926147\n",
            "Epoch  17 Batch  354 / 525  Training Loss  0.00016211059119086713\n",
            "Epoch  17 Batch  355 / 525  Training Loss  0.0002702059573493898\n",
            "Epoch  17 Batch  356 / 525  Training Loss  0.00016642766422592103\n",
            "Epoch  17 Batch  357 / 525  Training Loss  0.0004591820761561394\n",
            "Epoch  17 Batch  358 / 525  Training Loss  0.0003280803211964667\n",
            "Epoch  17 Batch  359 / 525  Training Loss  0.00023465126287192106\n",
            "Epoch  17 Batch  360 / 525  Training Loss  0.00019564645481295884\n",
            "Epoch  17 Batch  361 / 525  Training Loss  0.00026518123922869563\n",
            "Epoch  17 Batch  362 / 525  Training Loss  0.00026245080516673625\n",
            "Epoch  17 Batch  363 / 525  Training Loss  0.0002550087810959667\n",
            "Epoch  17 Batch  364 / 525  Training Loss  0.00026677746791392565\n",
            "Epoch  17 Batch  365 / 525  Training Loss  0.00018795518553815782\n",
            "Epoch  17 Batch  366 / 525  Training Loss  0.00023053698532748967\n",
            "Epoch  17 Batch  367 / 525  Training Loss  0.0002443075063638389\n",
            "Epoch  17 Batch  368 / 525  Training Loss  0.00038477699854411185\n",
            "Epoch  17 Batch  369 / 525  Training Loss  0.0002745968522503972\n",
            "Epoch  17 Batch  370 / 525  Training Loss  0.00016340830188710243\n",
            "Epoch  17 Batch  371 / 525  Training Loss  0.00031545350793749094\n",
            "Epoch  17 Batch  372 / 525  Training Loss  0.00023781748313922435\n",
            "Epoch  17 Batch  373 / 525  Training Loss  0.00027845543809235096\n",
            "Epoch  17 Batch  374 / 525  Training Loss  0.00024440803099423647\n",
            "Epoch  17 Batch  375 / 525  Training Loss  0.00020338762260507792\n",
            "Epoch  17 Batch  376 / 525  Training Loss  0.0002326567191630602\n",
            "Epoch  17 Batch  377 / 525  Training Loss  0.0003461645683273673\n",
            "Epoch  17 Batch  378 / 525  Training Loss  0.00015906037879176438\n",
            "Epoch  17 Batch  379 / 525  Training Loss  0.00018941091548185796\n",
            "Epoch  17 Batch  380 / 525  Training Loss  0.00017459769151173532\n",
            "Epoch  17 Batch  381 / 525  Training Loss  0.00028427812503650784\n",
            "Epoch  17 Batch  382 / 525  Training Loss  0.0001438766048522666\n",
            "Epoch  17 Batch  383 / 525  Training Loss  0.0003746141155716032\n",
            "Epoch  17 Batch  384 / 525  Training Loss  0.00012510806845966727\n",
            "Epoch  17 Batch  385 / 525  Training Loss  0.00016844581114128232\n",
            "Epoch  17 Batch  386 / 525  Training Loss  0.0002384334511589259\n",
            "Epoch  17 Batch  387 / 525  Training Loss  0.00014704326167702675\n",
            "Epoch  17 Batch  388 / 525  Training Loss  0.00019179150694981217\n",
            "Epoch  17 Batch  389 / 525  Training Loss  0.00024301020312123\n",
            "Epoch  17 Batch  390 / 525  Training Loss  0.0002915447112172842\n",
            "Epoch  17 Batch  391 / 525  Training Loss  0.0003127238596789539\n",
            "Epoch  17 Batch  392 / 525  Training Loss  0.0005627934588119388\n",
            "Epoch  17 Batch  393 / 525  Training Loss  0.00026007494307123125\n",
            "Epoch  17 Batch  394 / 525  Training Loss  0.0001689894706942141\n",
            "Epoch  17 Batch  395 / 525  Training Loss  0.0001947256678249687\n",
            "Epoch  17 Batch  396 / 525  Training Loss  0.00024613237474113703\n",
            "Epoch  17 Batch  397 / 525  Training Loss  0.0002049332979368046\n",
            "Epoch  17 Batch  398 / 525  Training Loss  0.0002481548872310668\n",
            "Epoch  17 Batch  399 / 525  Training Loss  0.00014774795272387564\n",
            "Epoch  17 Batch  400 / 525  Training Loss  0.0007019407348707318\n",
            "Epoch  17 Batch  401 / 525  Training Loss  0.0005860415403731167\n",
            "Epoch  17 Batch  402 / 525  Training Loss  0.0013990463921800256\n",
            "Epoch  17 Batch  403 / 525  Training Loss  0.00032594194635748863\n",
            "Epoch  17 Batch  404 / 525  Training Loss  0.00011289581016171724\n",
            "Epoch  17 Batch  405 / 525  Training Loss  0.0003835739626083523\n",
            "Epoch  17 Batch  406 / 525  Training Loss  0.00025088118854910135\n",
            "Epoch  17 Batch  407 / 525  Training Loss  0.00026446999981999397\n",
            "Epoch  17 Batch  408 / 525  Training Loss  0.00015518265718128532\n",
            "Epoch  17 Batch  409 / 525  Training Loss  0.0003246744745410979\n",
            "Epoch  17 Batch  410 / 525  Training Loss  0.00034747825702652335\n",
            "Epoch  17 Batch  411 / 525  Training Loss  0.00018190992705058306\n",
            "Epoch  17 Batch  412 / 525  Training Loss  0.0001733481913106516\n",
            "Epoch  17 Batch  413 / 525  Training Loss  0.00018442366854287684\n",
            "Epoch  17 Batch  414 / 525  Training Loss  0.00019923729996662587\n",
            "Epoch  17 Batch  415 / 525  Training Loss  0.0037772185169160366\n",
            "Epoch  17 Batch  416 / 525  Training Loss  0.0002663553168531507\n",
            "Epoch  17 Batch  417 / 525  Training Loss  0.0030310912989079952\n",
            "Epoch  17 Batch  418 / 525  Training Loss  0.0003163976070936769\n",
            "Epoch  17 Batch  419 / 525  Training Loss  0.0005517367972061038\n",
            "Epoch  17 Batch  420 / 525  Training Loss  0.00017846164701040834\n",
            "Epoch  17 Batch  421 / 525  Training Loss  0.000343284395057708\n",
            "Epoch  17 Batch  422 / 525  Training Loss  0.00032278610160574317\n",
            "Epoch  17 Batch  423 / 525  Training Loss  0.0005124961026012897\n",
            "Epoch  17 Batch  424 / 525  Training Loss  0.00018636805179994553\n",
            "Epoch  17 Batch  425 / 525  Training Loss  0.0005145942559465766\n",
            "Epoch  17 Batch  426 / 525  Training Loss  0.00033207907108590007\n",
            "Epoch  17 Batch  427 / 525  Training Loss  0.00038153683999553323\n",
            "Epoch  17 Batch  428 / 525  Training Loss  0.00015843129949644208\n",
            "Epoch  17 Batch  429 / 525  Training Loss  0.0003472192329354584\n",
            "Epoch  17 Batch  430 / 525  Training Loss  0.00019097374752163887\n",
            "Epoch  17 Batch  431 / 525  Training Loss  0.00041643064469099045\n",
            "Epoch  17 Batch  432 / 525  Training Loss  0.00019291506032459438\n",
            "Epoch  17 Batch  433 / 525  Training Loss  0.00035540375392884016\n",
            "Epoch  17 Batch  434 / 525  Training Loss  0.008889680728316307\n",
            "Epoch  17 Batch  435 / 525  Training Loss  0.0004221726267132908\n",
            "Epoch  17 Batch  436 / 525  Training Loss  0.0005587346968241036\n",
            "Epoch  17 Batch  437 / 525  Training Loss  0.0002690889814402908\n",
            "Epoch  17 Batch  438 / 525  Training Loss  0.0002745302044786513\n",
            "Epoch  17 Batch  439 / 525  Training Loss  0.000936454045586288\n",
            "Epoch  17 Batch  440 / 525  Training Loss  0.0008198914001695812\n",
            "Epoch  17 Batch  441 / 525  Training Loss  0.00023006091942079365\n",
            "Epoch  17 Batch  442 / 525  Training Loss  0.0009148825774900615\n",
            "Epoch  17 Batch  443 / 525  Training Loss  0.0005147016490809619\n",
            "Epoch  17 Batch  444 / 525  Training Loss  0.00018708051356952637\n",
            "Epoch  17 Batch  445 / 525  Training Loss  0.0007230204646475613\n",
            "Epoch  17 Batch  446 / 525  Training Loss  0.0002558184787631035\n",
            "Epoch  17 Batch  447 / 525  Training Loss  0.0006533332052640617\n",
            "Epoch  17 Batch  448 / 525  Training Loss  0.0003028081264346838\n",
            "Epoch  17 Batch  449 / 525  Training Loss  0.00019273333600722253\n",
            "Epoch  17 Batch  450 / 525  Training Loss  0.0003789349866565317\n",
            "Epoch  17 Batch  451 / 525  Training Loss  0.0002049337635980919\n",
            "Epoch  17 Batch  452 / 525  Training Loss  0.0002146410261048004\n",
            "Epoch  17 Batch  453 / 525  Training Loss  0.0002587701892480254\n",
            "Epoch  17 Batch  454 / 525  Training Loss  0.00022116643958725035\n",
            "Epoch  17 Batch  455 / 525  Training Loss  0.0003526773944031447\n",
            "Epoch  17 Batch  456 / 525  Training Loss  0.00024563874467276037\n",
            "Epoch  17 Batch  457 / 525  Training Loss  0.00023305288050323725\n",
            "Epoch  17 Batch  458 / 525  Training Loss  0.0010392216499894857\n",
            "Epoch  17 Batch  459 / 525  Training Loss  0.000228435528697446\n",
            "Epoch  17 Batch  460 / 525  Training Loss  0.0003448250936344266\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  17 Batch  461 / 525  Training Loss  0.0015089072985574603\n",
            "Epoch  17 Batch  462 / 525  Training Loss  0.0003940777969546616\n",
            "Epoch  17 Batch  463 / 525  Training Loss  0.0003340712864883244\n",
            "Epoch  17 Batch  464 / 525  Training Loss  0.01587008871138096\n",
            "Epoch  17 Batch  465 / 525  Training Loss  0.005667579360306263\n",
            "Epoch  17 Batch  466 / 525  Training Loss  0.0004921140498481691\n",
            "Epoch  17 Batch  467 / 525  Training Loss  0.00035083427792415023\n",
            "Epoch  17 Batch  468 / 525  Training Loss  0.000318357371725142\n",
            "Epoch  17 Batch  469 / 525  Training Loss  0.0020171687938272953\n",
            "Epoch  17 Batch  470 / 525  Training Loss  0.000267495313892141\n",
            "Epoch  17 Batch  471 / 525  Training Loss  0.00037424787296913564\n",
            "Epoch  17 Batch  472 / 525  Training Loss  0.0002839159278664738\n",
            "Epoch  17 Batch  473 / 525  Training Loss  0.0003660637594293803\n",
            "Epoch  17 Batch  474 / 525  Training Loss  0.0003621279902290553\n",
            "Epoch  17 Batch  475 / 525  Training Loss  0.00019546682597137988\n",
            "Epoch  17 Batch  476 / 525  Training Loss  0.0003514422569423914\n",
            "Epoch  17 Batch  477 / 525  Training Loss  0.00039787436253391206\n",
            "Epoch  17 Batch  478 / 525  Training Loss  0.0004882798530161381\n",
            "Epoch  17 Batch  479 / 525  Training Loss  0.0002200705057475716\n",
            "Epoch  17 Batch  480 / 525  Training Loss  0.00023887798306532204\n",
            "Epoch  17 Batch  481 / 525  Training Loss  0.00036833068588748574\n",
            "Epoch  17 Batch  482 / 525  Training Loss  0.0002531266654841602\n",
            "Epoch  17 Batch  483 / 525  Training Loss  0.0004771072999574244\n",
            "Epoch  17 Batch  484 / 525  Training Loss  0.00027856140513904393\n",
            "Epoch  17 Batch  485 / 525  Training Loss  0.00037913554115220904\n",
            "Epoch  17 Batch  486 / 525  Training Loss  0.0003455596452113241\n",
            "Epoch  17 Batch  487 / 525  Training Loss  0.0002065185981336981\n",
            "Epoch  17 Batch  488 / 525  Training Loss  0.0019167910795658827\n",
            "Epoch  17 Batch  489 / 525  Training Loss  0.00022590928710997105\n",
            "Epoch  17 Batch  490 / 525  Training Loss  0.00014925928553566337\n",
            "Epoch  17 Batch  491 / 525  Training Loss  0.0003247784043196589\n",
            "Epoch  17 Batch  492 / 525  Training Loss  0.0008457837393507361\n",
            "Epoch  17 Batch  493 / 525  Training Loss  0.0004782074538525194\n",
            "Epoch  17 Batch  494 / 525  Training Loss  0.001093235332518816\n",
            "Epoch  17 Batch  495 / 525  Training Loss  0.00031647697323933244\n",
            "Epoch  17 Batch  496 / 525  Training Loss  0.0002079940604744479\n",
            "Epoch  17 Batch  497 / 525  Training Loss  0.00029725467902608216\n",
            "Epoch  17 Batch  498 / 525  Training Loss  0.0006146501400507987\n",
            "Epoch  17 Batch  499 / 525  Training Loss  0.0002794128959067166\n",
            "Epoch  17 Batch  500 / 525  Training Loss  0.0001720782893244177\n",
            "Epoch  17 Batch  501 / 525  Training Loss  0.0002004578709602356\n",
            "Epoch  17 Batch  502 / 525  Training Loss  0.00024035005480982363\n",
            "Epoch  17 Batch  503 / 525  Training Loss  0.002005850663408637\n",
            "Epoch  17 Batch  504 / 525  Training Loss  0.000713951769284904\n",
            "Epoch  17 Batch  505 / 525  Training Loss  0.0002546632313169539\n",
            "Epoch  17 Batch  506 / 525  Training Loss  0.0007048991392366588\n",
            "Epoch  17 Batch  507 / 525  Training Loss  0.00022021090262569487\n",
            "Epoch  17 Batch  508 / 525  Training Loss  0.00031492748530581594\n",
            "Epoch  17 Batch  509 / 525  Training Loss  0.00034743623109534383\n",
            "Epoch  17 Batch  510 / 525  Training Loss  0.0002929295296780765\n",
            "Epoch  17 Batch  511 / 525  Training Loss  0.00027107252390123904\n",
            "Epoch  17 Batch  512 / 525  Training Loss  0.0002670569811016321\n",
            "Epoch  17 Batch  513 / 525  Training Loss  0.0033454156946390867\n",
            "Epoch  17 Batch  514 / 525  Training Loss  0.00041345233330503106\n",
            "Epoch  17 Batch  515 / 525  Training Loss  0.00033533803070895374\n",
            "Epoch  17 Batch  516 / 525  Training Loss  0.0005672668339684606\n",
            "Epoch  17 Batch  517 / 525  Training Loss  0.0002686747757252306\n",
            "Epoch  17 Batch  518 / 525  Training Loss  0.0004674030060414225\n",
            "Epoch  17 Batch  519 / 525  Training Loss  0.00018462256412021816\n",
            "Epoch  17 Batch  520 / 525  Training Loss  0.00013057157048024237\n",
            "Epoch  17 Batch  521 / 525  Training Loss  0.0002709951368160546\n",
            "Epoch  17 Batch  522 / 525  Training Loss  0.0005754551966674626\n",
            "Epoch  17 Batch  523 / 525  Training Loss  0.0002575895341578871\n",
            "Epoch  17 Batch  524 / 525  Training Loss  0.0001892966974992305\n",
            "  18    |    -    |   0.000442   |   64.38  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 18\n",
            "Epoch  18 Batch  0 / 525  Training Loss  0.002260535955429077\n",
            "Epoch  18 Batch  1 / 525  Training Loss  0.00031073548598214984\n",
            "Epoch  18 Batch  2 / 525  Training Loss  0.001486432971432805\n",
            "Epoch  18 Batch  3 / 525  Training Loss  0.0002952706126961857\n",
            "Epoch  18 Batch  4 / 525  Training Loss  0.000656861171592027\n",
            "Epoch  18 Batch  5 / 525  Training Loss  0.00040555946179665625\n",
            "Epoch  18 Batch  6 / 525  Training Loss  0.00017723911150824279\n",
            "Epoch  18 Batch  7 / 525  Training Loss  0.00012686994159594178\n",
            "Epoch  18 Batch  8 / 525  Training Loss  0.0002513067447580397\n",
            "Epoch  18 Batch  9 / 525  Training Loss  0.00018411381461191922\n",
            "Epoch  18 Batch  10 / 525  Training Loss  0.0002876797516364604\n",
            "Epoch  18 Batch  11 / 525  Training Loss  0.00017236906569451094\n",
            "Epoch  18 Batch  12 / 525  Training Loss  0.00027998938458040357\n",
            "Epoch  18 Batch  13 / 525  Training Loss  0.0005349681014195085\n",
            "Epoch  18 Batch  14 / 525  Training Loss  0.00020004673569928855\n",
            "Epoch  18 Batch  15 / 525  Training Loss  0.0002783199888654053\n",
            "Epoch  18 Batch  16 / 525  Training Loss  0.00015734814223833382\n",
            "Epoch  18 Batch  17 / 525  Training Loss  0.0001965475094038993\n",
            "Epoch  18 Batch  18 / 525  Training Loss  0.000390418223105371\n",
            "Epoch  18 Batch  19 / 525  Training Loss  0.0005765736568719149\n",
            "Epoch  18 Batch  20 / 525  Training Loss  0.00031170170404948294\n",
            "Epoch  18 Batch  21 / 525  Training Loss  0.00011965073645114899\n",
            "Epoch  18 Batch  22 / 525  Training Loss  0.0002430292806820944\n",
            "Epoch  18 Batch  23 / 525  Training Loss  0.0002802669769152999\n",
            "Epoch  18 Batch  24 / 525  Training Loss  0.00032450747676193714\n",
            "Epoch  18 Batch  25 / 525  Training Loss  0.0002965117746498436\n",
            "Epoch  18 Batch  26 / 525  Training Loss  0.00040509767131879926\n",
            "Epoch  18 Batch  27 / 525  Training Loss  0.00011414395703468472\n",
            "Epoch  18 Batch  28 / 525  Training Loss  0.0006575435982085764\n",
            "Epoch  18 Batch  29 / 525  Training Loss  0.0003129112592432648\n",
            "Epoch  18 Batch  30 / 525  Training Loss  0.00010480557102710009\n",
            "Epoch  18 Batch  31 / 525  Training Loss  0.00012618144683074206\n",
            "Epoch  18 Batch  32 / 525  Training Loss  0.00010372636461397633\n",
            "Epoch  18 Batch  33 / 525  Training Loss  0.0009284038096666336\n",
            "Epoch  18 Batch  34 / 525  Training Loss  0.00025148512213490903\n",
            "Epoch  18 Batch  35 / 525  Training Loss  0.00012183326907688752\n",
            "Epoch  18 Batch  36 / 525  Training Loss  0.0003880638687405735\n",
            "Epoch  18 Batch  37 / 525  Training Loss  0.0002667840162757784\n",
            "Epoch  18 Batch  38 / 525  Training Loss  0.00022483101929537952\n",
            "Epoch  18 Batch  39 / 525  Training Loss  0.00016342094750143588\n",
            "Epoch  18 Batch  40 / 525  Training Loss  0.0003129966789856553\n",
            "Epoch  18 Batch  41 / 525  Training Loss  0.0002067635505227372\n",
            "Epoch  18 Batch  42 / 525  Training Loss  0.00027928611962124705\n",
            "Epoch  18 Batch  43 / 525  Training Loss  0.00031827783095650375\n",
            "Epoch  18 Batch  44 / 525  Training Loss  0.0001994110207306221\n",
            "Epoch  18 Batch  45 / 525  Training Loss  0.00013141871022526175\n",
            "Epoch  18 Batch  46 / 525  Training Loss  0.0006358114769682288\n",
            "Epoch  18 Batch  47 / 525  Training Loss  0.0001193068892462179\n",
            "Epoch  18 Batch  48 / 525  Training Loss  0.00016849476378411055\n",
            "Epoch  18 Batch  49 / 525  Training Loss  0.00022176762286107987\n",
            "Epoch  18 Batch  50 / 525  Training Loss  0.00010179334640270099\n",
            "Epoch  18 Batch  51 / 525  Training Loss  0.00022503698710352182\n",
            "Epoch  18 Batch  52 / 525  Training Loss  0.0001620441908016801\n",
            "Epoch  18 Batch  53 / 525  Training Loss  0.00020265698549337685\n",
            "Epoch  18 Batch  54 / 525  Training Loss  0.00018092014943249524\n",
            "Epoch  18 Batch  55 / 525  Training Loss  0.00043219621875323355\n",
            "Epoch  18 Batch  56 / 525  Training Loss  0.0017944099381566048\n",
            "Epoch  18 Batch  57 / 525  Training Loss  0.00028844113694503903\n",
            "Epoch  18 Batch  58 / 525  Training Loss  0.0003850939974654466\n",
            "Epoch  18 Batch  59 / 525  Training Loss  0.0002309115807292983\n",
            "Epoch  18 Batch  60 / 525  Training Loss  0.00045926039456389844\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  18 Batch  61 / 525  Training Loss  0.00019485501979943365\n",
            "Epoch  18 Batch  62 / 525  Training Loss  0.0002359220088692382\n",
            "Epoch  18 Batch  63 / 525  Training Loss  0.0001098475549952127\n",
            "Epoch  18 Batch  64 / 525  Training Loss  0.00016850803513079882\n",
            "Epoch  18 Batch  65 / 525  Training Loss  0.0003100283211097121\n",
            "Epoch  18 Batch  66 / 525  Training Loss  0.0003028736391570419\n",
            "Epoch  18 Batch  67 / 525  Training Loss  0.00016345788026228547\n",
            "Epoch  18 Batch  68 / 525  Training Loss  0.0002094433584716171\n",
            "Epoch  18 Batch  69 / 525  Training Loss  0.0001305017212871462\n",
            "Epoch  18 Batch  70 / 525  Training Loss  0.00031141206272877753\n",
            "Epoch  18 Batch  71 / 525  Training Loss  0.00020127210882492363\n",
            "Epoch  18 Batch  72 / 525  Training Loss  0.00021329731680452824\n",
            "Epoch  18 Batch  73 / 525  Training Loss  0.0004053129814565182\n",
            "Epoch  18 Batch  74 / 525  Training Loss  0.00013529861462302506\n",
            "Epoch  18 Batch  75 / 525  Training Loss  0.00011796333274105564\n",
            "Epoch  18 Batch  76 / 525  Training Loss  0.00022246161825023592\n",
            "Epoch  18 Batch  77 / 525  Training Loss  0.0001429075055057183\n",
            "Epoch  18 Batch  78 / 525  Training Loss  0.00022277476091403514\n",
            "Epoch  18 Batch  79 / 525  Training Loss  0.00026155891828238964\n",
            "Epoch  18 Batch  80 / 525  Training Loss  0.0003126001975033432\n",
            "Epoch  18 Batch  81 / 525  Training Loss  0.00018330788589082658\n",
            "Epoch  18 Batch  82 / 525  Training Loss  0.00020108590251766145\n",
            "Epoch  18 Batch  83 / 525  Training Loss  0.00042670746915973723\n",
            "Epoch  18 Batch  84 / 525  Training Loss  0.00013391548418439925\n",
            "Epoch  18 Batch  85 / 525  Training Loss  0.00019196627545170486\n",
            "Epoch  18 Batch  86 / 525  Training Loss  0.0003098738961853087\n",
            "Epoch  18 Batch  87 / 525  Training Loss  0.0001900582865346223\n",
            "Epoch  18 Batch  88 / 525  Training Loss  0.00030774640617892146\n",
            "Epoch  18 Batch  89 / 525  Training Loss  0.00019452226115390658\n",
            "Epoch  18 Batch  90 / 525  Training Loss  0.00024533012765459716\n",
            "Epoch  18 Batch  91 / 525  Training Loss  0.00018425275629851967\n",
            "Epoch  18 Batch  92 / 525  Training Loss  0.00017553607176523656\n",
            "Epoch  18 Batch  93 / 525  Training Loss  0.0001532440510345623\n",
            "Epoch  18 Batch  94 / 525  Training Loss  0.00015223995433188975\n",
            "Epoch  18 Batch  95 / 525  Training Loss  0.0001986338320421055\n",
            "Epoch  18 Batch  96 / 525  Training Loss  0.00020145400776527822\n",
            "Epoch  18 Batch  97 / 525  Training Loss  0.00026008792337961495\n",
            "Epoch  18 Batch  98 / 525  Training Loss  0.00014976086094975471\n",
            "Epoch  18 Batch  99 / 525  Training Loss  0.00014738700701855123\n",
            "Epoch  18 Batch  100 / 525  Training Loss  0.0002455320209264755\n",
            "Epoch  18 Batch  101 / 525  Training Loss  0.0003615509776864201\n",
            "Epoch  18 Batch  102 / 525  Training Loss  0.00023338280152529478\n",
            "Epoch  18 Batch  103 / 525  Training Loss  0.00017340296471957117\n",
            "Epoch  18 Batch  104 / 525  Training Loss  8.227858052123338e-05\n",
            "Epoch  18 Batch  105 / 525  Training Loss  0.0001285262987948954\n",
            "Epoch  18 Batch  106 / 525  Training Loss  0.0014791630674153566\n",
            "Epoch  18 Batch  107 / 525  Training Loss  0.0001598855305928737\n",
            "Epoch  18 Batch  108 / 525  Training Loss  0.00016825005877763033\n",
            "Epoch  18 Batch  109 / 525  Training Loss  0.00021735811606049538\n",
            "Epoch  18 Batch  110 / 525  Training Loss  0.00028658652445301414\n",
            "Epoch  18 Batch  111 / 525  Training Loss  0.00017134507652372122\n",
            "Epoch  18 Batch  112 / 525  Training Loss  0.00022297739633359015\n",
            "Epoch  18 Batch  113 / 525  Training Loss  0.0002093644143315032\n",
            "Epoch  18 Batch  114 / 525  Training Loss  0.00012476539995986968\n",
            "Epoch  18 Batch  115 / 525  Training Loss  0.00017958300304599106\n",
            "Epoch  18 Batch  116 / 525  Training Loss  0.0002803244278766215\n",
            "Epoch  18 Batch  117 / 525  Training Loss  0.00020406022667884827\n",
            "Epoch  18 Batch  118 / 525  Training Loss  0.000307370035443455\n",
            "Epoch  18 Batch  119 / 525  Training Loss  0.00017697240400593728\n",
            "Epoch  18 Batch  120 / 525  Training Loss  0.00025243888376280665\n",
            "Epoch  18 Batch  121 / 525  Training Loss  0.00030283693922683597\n",
            "Epoch  18 Batch  122 / 525  Training Loss  0.0002294206788064912\n",
            "Epoch  18 Batch  123 / 525  Training Loss  0.00021026030299253762\n",
            "Epoch  18 Batch  124 / 525  Training Loss  0.00015942027675919235\n",
            "Epoch  18 Batch  125 / 525  Training Loss  0.00020845912513323128\n",
            "Epoch  18 Batch  126 / 525  Training Loss  0.00019236902880948037\n",
            "Epoch  18 Batch  127 / 525  Training Loss  0.00024352094624191523\n",
            "Epoch  18 Batch  128 / 525  Training Loss  0.00025081419153138995\n",
            "Epoch  18 Batch  129 / 525  Training Loss  0.0002488779427949339\n",
            "Epoch  18 Batch  130 / 525  Training Loss  0.00014830045984126627\n",
            "Epoch  18 Batch  131 / 525  Training Loss  0.00022571549925487489\n",
            "Epoch  18 Batch  132 / 525  Training Loss  0.00023509950551670045\n",
            "Epoch  18 Batch  133 / 525  Training Loss  0.00013579786173067987\n",
            "Epoch  18 Batch  134 / 525  Training Loss  0.00017451087478548288\n",
            "Epoch  18 Batch  135 / 525  Training Loss  0.0002253050624858588\n",
            "Epoch  18 Batch  136 / 525  Training Loss  0.0005649955128319561\n",
            "Epoch  18 Batch  137 / 525  Training Loss  0.00016726389003451914\n",
            "Epoch  18 Batch  138 / 525  Training Loss  0.00020873546600341797\n",
            "Epoch  18 Batch  139 / 525  Training Loss  0.00031548881088383496\n",
            "Epoch  18 Batch  140 / 525  Training Loss  0.0002465059806127101\n",
            "Epoch  18 Batch  141 / 525  Training Loss  0.00039905947051011026\n",
            "Epoch  18 Batch  142 / 525  Training Loss  0.00018208110122941434\n",
            "Epoch  18 Batch  143 / 525  Training Loss  0.00016416818834841251\n",
            "Epoch  18 Batch  144 / 525  Training Loss  0.00047736638225615025\n",
            "Epoch  18 Batch  145 / 525  Training Loss  0.00012659486674237996\n",
            "Epoch  18 Batch  146 / 525  Training Loss  0.00016056046297308058\n",
            "Epoch  18 Batch  147 / 525  Training Loss  0.00020963202405255288\n",
            "Epoch  18 Batch  148 / 525  Training Loss  0.0002499106922186911\n",
            "Epoch  18 Batch  149 / 525  Training Loss  0.00018416174862068146\n",
            "Epoch  18 Batch  150 / 525  Training Loss  0.00018269287829753011\n",
            "Epoch  18 Batch  151 / 525  Training Loss  0.0001223898580064997\n",
            "Epoch  18 Batch  152 / 525  Training Loss  0.00012132222764194012\n",
            "Epoch  18 Batch  153 / 525  Training Loss  0.00036366278072819114\n",
            "Epoch  18 Batch  154 / 525  Training Loss  0.000745593395549804\n",
            "Epoch  18 Batch  155 / 525  Training Loss  0.0002304227091372013\n",
            "Epoch  18 Batch  156 / 525  Training Loss  0.0001976109779207036\n",
            "Epoch  18 Batch  157 / 525  Training Loss  0.0003881246957462281\n",
            "Epoch  18 Batch  158 / 525  Training Loss  0.00030831521144136786\n",
            "Epoch  18 Batch  159 / 525  Training Loss  0.00019048650574404746\n",
            "Epoch  18 Batch  160 / 525  Training Loss  0.0001733577810227871\n",
            "Epoch  18 Batch  161 / 525  Training Loss  0.0001997989311348647\n",
            "Epoch  18 Batch  162 / 525  Training Loss  0.00014411963638849556\n",
            "Epoch  18 Batch  163 / 525  Training Loss  0.00015868863556534052\n",
            "Epoch  18 Batch  164 / 525  Training Loss  0.00016089169366750866\n",
            "Epoch  18 Batch  165 / 525  Training Loss  0.0001488740526838228\n",
            "Epoch  18 Batch  166 / 525  Training Loss  0.000210418802453205\n",
            "Epoch  18 Batch  167 / 525  Training Loss  0.0003020733129233122\n",
            "Epoch  18 Batch  168 / 525  Training Loss  0.00017267587827518582\n",
            "Epoch  18 Batch  169 / 525  Training Loss  0.0002827474963851273\n",
            "Epoch  18 Batch  170 / 525  Training Loss  0.00011984218144789338\n",
            "Epoch  18 Batch  171 / 525  Training Loss  0.0001799355959519744\n",
            "Epoch  18 Batch  172 / 525  Training Loss  0.00019096076721325517\n",
            "Epoch  18 Batch  173 / 525  Training Loss  0.00033016197266988456\n",
            "Epoch  18 Batch  174 / 525  Training Loss  0.00012462600716389716\n",
            "Epoch  18 Batch  175 / 525  Training Loss  0.0006679708021692932\n",
            "Epoch  18 Batch  176 / 525  Training Loss  0.00020332347776275128\n",
            "Epoch  18 Batch  177 / 525  Training Loss  0.00015549318050034344\n",
            "Epoch  18 Batch  178 / 525  Training Loss  0.0001668315235292539\n",
            "Epoch  18 Batch  179 / 525  Training Loss  0.0001269752101507038\n",
            "Epoch  18 Batch  180 / 525  Training Loss  0.00011617627751547843\n",
            "Epoch  18 Batch  181 / 525  Training Loss  0.00014136722893454134\n",
            "Epoch  18 Batch  182 / 525  Training Loss  0.0001945597177837044\n",
            "Epoch  18 Batch  183 / 525  Training Loss  0.00012600226909853518\n",
            "Epoch  18 Batch  184 / 525  Training Loss  0.00014411305892281234\n",
            "Epoch  18 Batch  185 / 525  Training Loss  0.00011684947094181553\n",
            "Epoch  18 Batch  186 / 525  Training Loss  0.0001380082976538688\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  18 Batch  187 / 525  Training Loss  0.00011238841398153454\n",
            "Epoch  18 Batch  188 / 525  Training Loss  0.00020780236809514463\n",
            "Epoch  18 Batch  189 / 525  Training Loss  0.00013497451436705887\n",
            "Epoch  18 Batch  190 / 525  Training Loss  0.00020863364625256509\n",
            "Epoch  18 Batch  191 / 525  Training Loss  0.0001400568289682269\n",
            "Epoch  18 Batch  192 / 525  Training Loss  0.00020151461649220437\n",
            "Epoch  18 Batch  193 / 525  Training Loss  0.0001963870890904218\n",
            "Epoch  18 Batch  194 / 525  Training Loss  0.00015048662316985428\n",
            "Epoch  18 Batch  195 / 525  Training Loss  0.0003173018922097981\n",
            "Epoch  18 Batch  196 / 525  Training Loss  0.00034280671388842165\n",
            "Epoch  18 Batch  197 / 525  Training Loss  0.0001478824415244162\n",
            "Epoch  18 Batch  198 / 525  Training Loss  0.0002254282298963517\n",
            "Epoch  18 Batch  199 / 525  Training Loss  0.00023977315868251026\n",
            "Epoch  18 Batch  200 / 525  Training Loss  0.00024020744604058564\n",
            "Epoch  18 Batch  201 / 525  Training Loss  0.0005962705472484231\n",
            "Epoch  18 Batch  202 / 525  Training Loss  0.00019485049415379763\n",
            "Epoch  18 Batch  203 / 525  Training Loss  0.0002847628784365952\n",
            "Epoch  18 Batch  204 / 525  Training Loss  0.00016650083125568926\n",
            "Epoch  18 Batch  205 / 525  Training Loss  0.00020278319425415248\n",
            "Epoch  18 Batch  206 / 525  Training Loss  0.00020958056848030537\n",
            "Epoch  18 Batch  207 / 525  Training Loss  9.82792116701603e-05\n",
            "Epoch  18 Batch  208 / 525  Training Loss  0.00015096327115315944\n",
            "Epoch  18 Batch  209 / 525  Training Loss  0.00015589961549267173\n",
            "Epoch  18 Batch  210 / 525  Training Loss  0.00020298904564697295\n",
            "Epoch  18 Batch  211 / 525  Training Loss  0.001285115140490234\n",
            "Epoch  18 Batch  212 / 525  Training Loss  0.0001928299170685932\n",
            "Epoch  18 Batch  213 / 525  Training Loss  0.00017019113874994218\n",
            "Epoch  18 Batch  214 / 525  Training Loss  0.00016428256640210748\n",
            "Epoch  18 Batch  215 / 525  Training Loss  0.00019549697753973305\n",
            "Epoch  18 Batch  216 / 525  Training Loss  0.000215163963730447\n",
            "Epoch  18 Batch  217 / 525  Training Loss  0.0008510990883223712\n",
            "Epoch  18 Batch  218 / 525  Training Loss  0.00019074628653470427\n",
            "Epoch  18 Batch  219 / 525  Training Loss  0.000431325810495764\n",
            "Epoch  18 Batch  220 / 525  Training Loss  0.00019118163618259132\n",
            "Epoch  18 Batch  221 / 525  Training Loss  0.00020204258908051997\n",
            "Epoch  18 Batch  222 / 525  Training Loss  0.00025835761334747076\n",
            "Epoch  18 Batch  223 / 525  Training Loss  0.00018521019956097007\n",
            "Epoch  18 Batch  224 / 525  Training Loss  0.00017598499835003167\n",
            "Epoch  18 Batch  225 / 525  Training Loss  0.00019671677728183568\n",
            "Epoch  18 Batch  226 / 525  Training Loss  0.00010587713768472895\n",
            "Epoch  18 Batch  227 / 525  Training Loss  0.00016172119649127126\n",
            "Epoch  18 Batch  228 / 525  Training Loss  0.0002286751987412572\n",
            "Epoch  18 Batch  229 / 525  Training Loss  0.0001559502852614969\n",
            "Epoch  18 Batch  230 / 525  Training Loss  0.00023273662372957915\n",
            "Epoch  18 Batch  231 / 525  Training Loss  0.00013587684952653944\n",
            "Epoch  18 Batch  232 / 525  Training Loss  0.0006114706629887223\n",
            "Epoch  18 Batch  233 / 525  Training Loss  0.0004693334340117872\n",
            "Epoch  18 Batch  234 / 525  Training Loss  0.0002533856313675642\n",
            "Epoch  18 Batch  235 / 525  Training Loss  0.0001721586158964783\n",
            "Epoch  18 Batch  236 / 525  Training Loss  0.0003000090946443379\n",
            "Epoch  18 Batch  237 / 525  Training Loss  0.00018249248387292027\n",
            "Epoch  18 Batch  238 / 525  Training Loss  0.00020714322454296052\n",
            "Epoch  18 Batch  239 / 525  Training Loss  0.000213854102184996\n",
            "Epoch  18 Batch  240 / 525  Training Loss  0.0003185765235684812\n",
            "Epoch  18 Batch  241 / 525  Training Loss  0.00021067840862087905\n",
            "Epoch  18 Batch  242 / 525  Training Loss  0.00011526417074492201\n",
            "Epoch  18 Batch  243 / 525  Training Loss  0.00011927945161005482\n",
            "Epoch  18 Batch  244 / 525  Training Loss  0.00022966996766626835\n",
            "Epoch  18 Batch  245 / 525  Training Loss  0.00016690490883775055\n",
            "Epoch  18 Batch  246 / 525  Training Loss  0.00012926622002851218\n",
            "Epoch  18 Batch  247 / 525  Training Loss  0.00023414278985001147\n",
            "Epoch  18 Batch  248 / 525  Training Loss  0.00020730476535391062\n",
            "Epoch  18 Batch  249 / 525  Training Loss  0.00012253329623490572\n",
            "Epoch  18 Batch  250 / 525  Training Loss  0.0002565579197835177\n",
            "Epoch  18 Batch  251 / 525  Training Loss  0.00019736324611585587\n",
            "Epoch  18 Batch  252 / 525  Training Loss  0.00010263272270094603\n",
            "Epoch  18 Batch  253 / 525  Training Loss  0.00016172262257896364\n",
            "Epoch  18 Batch  254 / 525  Training Loss  0.00015544774942100048\n",
            "Epoch  18 Batch  255 / 525  Training Loss  0.0002460263203829527\n",
            "Epoch  18 Batch  256 / 525  Training Loss  0.00012370440526865423\n",
            "Epoch  18 Batch  257 / 525  Training Loss  0.0001879538904177025\n",
            "Epoch  18 Batch  258 / 525  Training Loss  0.00014999642735347152\n",
            "Epoch  18 Batch  259 / 525  Training Loss  0.00018434117373544723\n",
            "Epoch  18 Batch  260 / 525  Training Loss  0.00016639591194689274\n",
            "Epoch  18 Batch  261 / 525  Training Loss  0.0002936434175353497\n",
            "Epoch  18 Batch  262 / 525  Training Loss  0.00017196964472532272\n",
            "Epoch  18 Batch  263 / 525  Training Loss  0.00020911372848786414\n",
            "Epoch  18 Batch  264 / 525  Training Loss  0.00024720028159208596\n",
            "Epoch  18 Batch  265 / 525  Training Loss  0.00012896784755866975\n",
            "Epoch  18 Batch  266 / 525  Training Loss  0.0001929636491695419\n",
            "Epoch  18 Batch  267 / 525  Training Loss  0.00020763759675901383\n",
            "Epoch  18 Batch  268 / 525  Training Loss  0.00027308642165735364\n",
            "Epoch  18 Batch  269 / 525  Training Loss  0.00011072810593759641\n",
            "Epoch  18 Batch  270 / 525  Training Loss  0.00010522786760702729\n",
            "Epoch  18 Batch  271 / 525  Training Loss  0.0001924983225762844\n",
            "Epoch  18 Batch  272 / 525  Training Loss  0.00015812255151104182\n",
            "Epoch  18 Batch  273 / 525  Training Loss  0.0002672215923666954\n",
            "Epoch  18 Batch  274 / 525  Training Loss  0.00012500557932071388\n",
            "Epoch  18 Batch  275 / 525  Training Loss  0.00011604302562773228\n",
            "Epoch  18 Batch  276 / 525  Training Loss  0.0001325974299106747\n",
            "Epoch  18 Batch  277 / 525  Training Loss  0.00013859190221410245\n",
            "Epoch  18 Batch  278 / 525  Training Loss  0.00016355214756913483\n",
            "Epoch  18 Batch  279 / 525  Training Loss  0.0001488906709710136\n",
            "Epoch  18 Batch  280 / 525  Training Loss  0.00047064200043678284\n",
            "Epoch  18 Batch  281 / 525  Training Loss  0.0003157824103254825\n",
            "Epoch  18 Batch  282 / 525  Training Loss  0.00017762767674867064\n",
            "Epoch  18 Batch  283 / 525  Training Loss  0.00018708666902966797\n",
            "Epoch  18 Batch  284 / 525  Training Loss  0.0002904919092543423\n",
            "Epoch  18 Batch  285 / 525  Training Loss  0.00014580962306354195\n",
            "Epoch  18 Batch  286 / 525  Training Loss  0.003692189697176218\n",
            "Epoch  18 Batch  287 / 525  Training Loss  0.0004993362817913294\n",
            "Epoch  18 Batch  288 / 525  Training Loss  0.0002658757730387151\n",
            "Epoch  18 Batch  289 / 525  Training Loss  0.00019035718287341297\n",
            "Epoch  18 Batch  290 / 525  Training Loss  0.0002636323042679578\n",
            "Epoch  18 Batch  291 / 525  Training Loss  0.00028980622300878167\n",
            "Epoch  18 Batch  292 / 525  Training Loss  0.00019974834867753088\n",
            "Epoch  18 Batch  293 / 525  Training Loss  0.00021995420684106648\n",
            "Epoch  18 Batch  294 / 525  Training Loss  0.00014838336210232228\n",
            "Epoch  18 Batch  295 / 525  Training Loss  0.00018974814156536013\n",
            "Epoch  18 Batch  296 / 525  Training Loss  0.002095230156555772\n",
            "Epoch  18 Batch  297 / 525  Training Loss  0.0002617335121612996\n",
            "Epoch  18 Batch  298 / 525  Training Loss  0.0001626387529540807\n",
            "Epoch  18 Batch  299 / 525  Training Loss  0.00014001266390550882\n",
            "Epoch  18 Batch  300 / 525  Training Loss  0.00018483919848222286\n",
            "Epoch  18 Batch  301 / 525  Training Loss  0.00019259782857261598\n",
            "Epoch  18 Batch  302 / 525  Training Loss  0.00018127451767213643\n",
            "Epoch  18 Batch  303 / 525  Training Loss  0.00011546585301402956\n",
            "Epoch  18 Batch  304 / 525  Training Loss  0.0003142281493637711\n",
            "Epoch  18 Batch  305 / 525  Training Loss  0.00013579410733655095\n",
            "Epoch  18 Batch  306 / 525  Training Loss  0.00033483150764368474\n",
            "Epoch  18 Batch  307 / 525  Training Loss  0.0001762897736625746\n",
            "Epoch  18 Batch  308 / 525  Training Loss  0.00012717094796244055\n",
            "Epoch  18 Batch  309 / 525  Training Loss  0.00025751610519364476\n",
            "Epoch  18 Batch  310 / 525  Training Loss  9.574601426720619e-05\n",
            "Epoch  18 Batch  311 / 525  Training Loss  0.00017488838057033718\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  18 Batch  312 / 525  Training Loss  0.00026734493440017104\n",
            "Epoch  18 Batch  313 / 525  Training Loss  0.00024860951816663146\n",
            "Epoch  18 Batch  314 / 525  Training Loss  7.289864879567176e-05\n",
            "Epoch  18 Batch  315 / 525  Training Loss  0.0002250144025310874\n",
            "Epoch  18 Batch  316 / 525  Training Loss  0.00017025216948240995\n",
            "Epoch  18 Batch  317 / 525  Training Loss  0.00014918073429726064\n",
            "Epoch  18 Batch  318 / 525  Training Loss  0.0001291125954594463\n",
            "Epoch  18 Batch  319 / 525  Training Loss  0.00013103600940667093\n",
            "Epoch  18 Batch  320 / 525  Training Loss  0.00013369649241212755\n",
            "Epoch  18 Batch  321 / 525  Training Loss  0.00020827171101700515\n",
            "Epoch  18 Batch  322 / 525  Training Loss  0.00014539049880113453\n",
            "Epoch  18 Batch  323 / 525  Training Loss  0.00012598153261933476\n",
            "Epoch  18 Batch  324 / 525  Training Loss  0.0002046212030109018\n",
            "Epoch  18 Batch  325 / 525  Training Loss  0.0001398819003952667\n",
            "Epoch  18 Batch  326 / 525  Training Loss  0.00021885966998524964\n",
            "Epoch  18 Batch  327 / 525  Training Loss  0.00018563977209851146\n",
            "Epoch  18 Batch  328 / 525  Training Loss  0.00015913350216578692\n",
            "Epoch  18 Batch  329 / 525  Training Loss  0.00010989239672198892\n",
            "Epoch  18 Batch  330 / 525  Training Loss  0.00016535540635231882\n",
            "Epoch  18 Batch  331 / 525  Training Loss  0.0001471532741561532\n",
            "Epoch  18 Batch  332 / 525  Training Loss  0.00010893253784161061\n",
            "Epoch  18 Batch  333 / 525  Training Loss  0.00013283231237437576\n",
            "Epoch  18 Batch  334 / 525  Training Loss  0.00014327093958854675\n",
            "Epoch  18 Batch  335 / 525  Training Loss  0.00014462365652434528\n",
            "Epoch  18 Batch  336 / 525  Training Loss  0.00013386746286414564\n",
            "Epoch  18 Batch  337 / 525  Training Loss  0.00022528837143909186\n",
            "Epoch  18 Batch  338 / 525  Training Loss  0.0001658055407460779\n",
            "Epoch  18 Batch  339 / 525  Training Loss  0.00013268867041915655\n",
            "Epoch  18 Batch  340 / 525  Training Loss  0.0003959568275604397\n",
            "Epoch  18 Batch  341 / 525  Training Loss  0.00020206319459248334\n",
            "Epoch  18 Batch  342 / 525  Training Loss  0.00010748715430963784\n",
            "Epoch  18 Batch  343 / 525  Training Loss  0.00012126219371566549\n",
            "Epoch  18 Batch  344 / 525  Training Loss  0.00019015064754057676\n",
            "Epoch  18 Batch  345 / 525  Training Loss  0.00019563280511647463\n",
            "Epoch  18 Batch  346 / 525  Training Loss  0.00032880803337320685\n",
            "Epoch  18 Batch  347 / 525  Training Loss  0.00022029543470125645\n",
            "Epoch  18 Batch  348 / 525  Training Loss  0.00023579338449053466\n",
            "Epoch  18 Batch  349 / 525  Training Loss  0.0003412316436879337\n",
            "Epoch  18 Batch  350 / 525  Training Loss  0.000268589414190501\n",
            "Epoch  18 Batch  351 / 525  Training Loss  0.0003189055132679641\n",
            "Epoch  18 Batch  352 / 525  Training Loss  0.0006874859100207686\n",
            "Epoch  18 Batch  353 / 525  Training Loss  0.0001245842140633613\n",
            "Epoch  18 Batch  354 / 525  Training Loss  0.00018964892660733312\n",
            "Epoch  18 Batch  355 / 525  Training Loss  0.0002635470300447196\n",
            "Epoch  18 Batch  356 / 525  Training Loss  0.00019752811931539327\n",
            "Epoch  18 Batch  357 / 525  Training Loss  0.00011876689677592367\n",
            "Epoch  18 Batch  358 / 525  Training Loss  0.0001550396700622514\n",
            "Epoch  18 Batch  359 / 525  Training Loss  0.00015493039973080158\n",
            "Epoch  18 Batch  360 / 525  Training Loss  0.000331249728333205\n",
            "Epoch  18 Batch  361 / 525  Training Loss  0.00018757855286821723\n",
            "Epoch  18 Batch  362 / 525  Training Loss  0.00012046065967297181\n",
            "Epoch  18 Batch  363 / 525  Training Loss  9.705050615593791e-05\n",
            "Epoch  18 Batch  364 / 525  Training Loss  0.00014298662426881492\n",
            "Epoch  18 Batch  365 / 525  Training Loss  0.0002212009421782568\n",
            "Epoch  18 Batch  366 / 525  Training Loss  0.0002719420299399644\n",
            "Epoch  18 Batch  367 / 525  Training Loss  0.00020542051061056554\n",
            "Epoch  18 Batch  368 / 525  Training Loss  0.00010761970042949542\n",
            "Epoch  18 Batch  369 / 525  Training Loss  0.00018627391546033323\n",
            "Epoch  18 Batch  370 / 525  Training Loss  0.00011212405661353841\n",
            "Epoch  18 Batch  371 / 525  Training Loss  0.0001530276203993708\n",
            "Epoch  18 Batch  372 / 525  Training Loss  0.0002435192873235792\n",
            "Epoch  18 Batch  373 / 525  Training Loss  0.0002038949605775997\n",
            "Epoch  18 Batch  374 / 525  Training Loss  0.00015292971511371434\n",
            "Epoch  18 Batch  375 / 525  Training Loss  9.524188499199226e-05\n",
            "Epoch  18 Batch  376 / 525  Training Loss  0.0002151649387087673\n",
            "Epoch  18 Batch  377 / 525  Training Loss  0.00014659040607511997\n",
            "Epoch  18 Batch  378 / 525  Training Loss  0.00017195139662362635\n",
            "Epoch  18 Batch  379 / 525  Training Loss  0.00036249798722565174\n",
            "Epoch  18 Batch  380 / 525  Training Loss  0.0001397073792759329\n",
            "Epoch  18 Batch  381 / 525  Training Loss  0.006699798163026571\n",
            "Epoch  18 Batch  382 / 525  Training Loss  0.0002639180747792125\n",
            "Epoch  18 Batch  383 / 525  Training Loss  0.008627848699688911\n",
            "Epoch  18 Batch  384 / 525  Training Loss  0.00032537669176235795\n",
            "Epoch  18 Batch  385 / 525  Training Loss  0.00022195240308064967\n",
            "Epoch  18 Batch  386 / 525  Training Loss  0.014438122510910034\n",
            "Epoch  18 Batch  387 / 525  Training Loss  0.0001841362682171166\n",
            "Epoch  18 Batch  388 / 525  Training Loss  0.00014607243065256625\n",
            "Epoch  18 Batch  389 / 525  Training Loss  0.000828361720778048\n",
            "Epoch  18 Batch  390 / 525  Training Loss  0.00023402264923788607\n",
            "Epoch  18 Batch  391 / 525  Training Loss  0.0004866484086960554\n",
            "Epoch  18 Batch  392 / 525  Training Loss  0.0002599015715532005\n",
            "Epoch  18 Batch  393 / 525  Training Loss  0.00015333748888224363\n",
            "Epoch  18 Batch  394 / 525  Training Loss  0.0003158697800245136\n",
            "Epoch  18 Batch  395 / 525  Training Loss  0.00038102170219644904\n",
            "Epoch  18 Batch  396 / 525  Training Loss  0.0006883427267894149\n",
            "Epoch  18 Batch  397 / 525  Training Loss  0.00016976171173155308\n",
            "Epoch  18 Batch  398 / 525  Training Loss  0.0002627137873787433\n",
            "Epoch  18 Batch  399 / 525  Training Loss  0.0001527521962998435\n",
            "Epoch  18 Batch  400 / 525  Training Loss  0.0002907029411289841\n",
            "Epoch  18 Batch  401 / 525  Training Loss  0.0002433054323773831\n",
            "Epoch  18 Batch  402 / 525  Training Loss  0.00023808176047168672\n",
            "Epoch  18 Batch  403 / 525  Training Loss  0.00022687790624331683\n",
            "Epoch  18 Batch  404 / 525  Training Loss  0.0003330705512780696\n",
            "Epoch  18 Batch  405 / 525  Training Loss  0.00027318362845107913\n",
            "Epoch  18 Batch  406 / 525  Training Loss  0.00020487906294874847\n",
            "Epoch  18 Batch  407 / 525  Training Loss  0.00012490393419284374\n",
            "Epoch  18 Batch  408 / 525  Training Loss  0.00023612317454535514\n",
            "Epoch  18 Batch  409 / 525  Training Loss  0.0003023759927600622\n",
            "Epoch  18 Batch  410 / 525  Training Loss  0.0006440679426304996\n",
            "Epoch  18 Batch  411 / 525  Training Loss  0.0004708372871391475\n",
            "Epoch  18 Batch  412 / 525  Training Loss  0.0002159122668672353\n",
            "Epoch  18 Batch  413 / 525  Training Loss  0.00024010094057302922\n",
            "Epoch  18 Batch  414 / 525  Training Loss  0.0016258656978607178\n",
            "Epoch  18 Batch  415 / 525  Training Loss  0.00039273445145227015\n",
            "Epoch  18 Batch  416 / 525  Training Loss  0.0001532865280751139\n",
            "Epoch  18 Batch  417 / 525  Training Loss  0.0006450984510593116\n",
            "Epoch  18 Batch  418 / 525  Training Loss  0.0031796018593013287\n",
            "Epoch  18 Batch  419 / 525  Training Loss  0.0002218948648078367\n",
            "Epoch  18 Batch  420 / 525  Training Loss  0.00032769422978162766\n",
            "Epoch  18 Batch  421 / 525  Training Loss  0.0002751937136054039\n",
            "Epoch  18 Batch  422 / 525  Training Loss  0.0008523240685462952\n",
            "Epoch  18 Batch  423 / 525  Training Loss  0.00012749966117553413\n",
            "Epoch  18 Batch  424 / 525  Training Loss  0.0001185848523164168\n",
            "Epoch  18 Batch  425 / 525  Training Loss  0.00030204112408682704\n",
            "Epoch  18 Batch  426 / 525  Training Loss  0.00027519912691786885\n",
            "Epoch  18 Batch  427 / 525  Training Loss  0.00015227377298288047\n",
            "Epoch  18 Batch  428 / 525  Training Loss  0.00038657907862216234\n",
            "Epoch  18 Batch  429 / 525  Training Loss  0.00025516582536511123\n",
            "Epoch  18 Batch  430 / 525  Training Loss  0.00022961749345995486\n",
            "Epoch  18 Batch  431 / 525  Training Loss  0.00027001299895346165\n",
            "Epoch  18 Batch  432 / 525  Training Loss  0.00026523516862653196\n",
            "Epoch  18 Batch  433 / 525  Training Loss  0.0002550081699155271\n",
            "Epoch  18 Batch  434 / 525  Training Loss  0.0003902438620571047\n",
            "Epoch  18 Batch  435 / 525  Training Loss  0.0001871833810582757\n",
            "Epoch  18 Batch  436 / 525  Training Loss  0.0005051658372394741\n",
            "Epoch  18 Batch  437 / 525  Training Loss  0.00026335494476370513\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  18 Batch  438 / 525  Training Loss  0.0001985272392630577\n",
            "Epoch  18 Batch  439 / 525  Training Loss  0.0005604330799542367\n",
            "Epoch  18 Batch  440 / 525  Training Loss  0.0002162056480301544\n",
            "Epoch  18 Batch  441 / 525  Training Loss  0.00024249145644716918\n",
            "Epoch  18 Batch  442 / 525  Training Loss  0.00542875099927187\n",
            "Epoch  18 Batch  443 / 525  Training Loss  0.00020434614270925522\n",
            "Epoch  18 Batch  444 / 525  Training Loss  0.0002911945921368897\n",
            "Epoch  18 Batch  445 / 525  Training Loss  0.00034832587698474526\n",
            "Epoch  18 Batch  446 / 525  Training Loss  0.0003403279697522521\n",
            "Epoch  18 Batch  447 / 525  Training Loss  0.0002130010398104787\n",
            "Epoch  18 Batch  448 / 525  Training Loss  0.00013086482067592442\n",
            "Epoch  18 Batch  449 / 525  Training Loss  0.00019820663146674633\n",
            "Epoch  18 Batch  450 / 525  Training Loss  0.00022077152971178293\n",
            "Epoch  18 Batch  451 / 525  Training Loss  0.00022468258976005018\n",
            "Epoch  18 Batch  452 / 525  Training Loss  0.0002991044020745903\n",
            "Epoch  18 Batch  453 / 525  Training Loss  0.00014253091649152339\n",
            "Epoch  18 Batch  454 / 525  Training Loss  0.0001878665789263323\n",
            "Epoch  18 Batch  455 / 525  Training Loss  0.0001854764122981578\n",
            "Epoch  18 Batch  456 / 525  Training Loss  0.00026021638768725097\n",
            "Epoch  18 Batch  457 / 525  Training Loss  0.0002260178152937442\n",
            "Epoch  18 Batch  458 / 525  Training Loss  0.00021341131650842726\n",
            "Epoch  18 Batch  459 / 525  Training Loss  0.0025579133071005344\n",
            "Epoch  18 Batch  460 / 525  Training Loss  0.00014726426161359996\n",
            "Epoch  18 Batch  461 / 525  Training Loss  0.0001951600715983659\n",
            "Epoch  18 Batch  462 / 525  Training Loss  0.00024608534295111895\n",
            "Epoch  18 Batch  463 / 525  Training Loss  0.00014946186274755746\n",
            "Epoch  18 Batch  464 / 525  Training Loss  0.00025578669738024473\n",
            "Epoch  18 Batch  465 / 525  Training Loss  0.00011467134027043357\n",
            "Epoch  18 Batch  466 / 525  Training Loss  0.0001968595606740564\n",
            "Epoch  18 Batch  467 / 525  Training Loss  0.00020530699111986905\n",
            "Epoch  18 Batch  468 / 525  Training Loss  0.0002531851059757173\n",
            "Epoch  18 Batch  469 / 525  Training Loss  0.00024377524096053094\n",
            "Epoch  18 Batch  470 / 525  Training Loss  0.00047154229832813144\n",
            "Epoch  18 Batch  471 / 525  Training Loss  0.000167927224538289\n",
            "Epoch  18 Batch  472 / 525  Training Loss  0.00044428525143302977\n",
            "Epoch  18 Batch  473 / 525  Training Loss  0.00032522232504561543\n",
            "Epoch  18 Batch  474 / 525  Training Loss  0.00017353930161334574\n",
            "Epoch  18 Batch  475 / 525  Training Loss  0.00020591456268448383\n",
            "Epoch  18 Batch  476 / 525  Training Loss  0.00031380722066387534\n",
            "Epoch  18 Batch  477 / 525  Training Loss  0.00018858860130421817\n",
            "Epoch  18 Batch  478 / 525  Training Loss  0.00017970477347262204\n",
            "Epoch  18 Batch  479 / 525  Training Loss  0.00023085596330929548\n",
            "Epoch  18 Batch  480 / 525  Training Loss  0.00020375827443785965\n",
            "Epoch  18 Batch  481 / 525  Training Loss  0.00016908723046071827\n",
            "Epoch  18 Batch  482 / 525  Training Loss  0.0003205997054465115\n",
            "Epoch  18 Batch  483 / 525  Training Loss  9.291110472986475e-05\n",
            "Epoch  18 Batch  484 / 525  Training Loss  0.00023737855372019112\n",
            "Epoch  18 Batch  485 / 525  Training Loss  0.00030231307027861476\n",
            "Epoch  18 Batch  486 / 525  Training Loss  0.00026248948415741324\n",
            "Epoch  18 Batch  487 / 525  Training Loss  0.0002296808670507744\n",
            "Epoch  18 Batch  488 / 525  Training Loss  0.00023448668071068823\n",
            "Epoch  18 Batch  489 / 525  Training Loss  0.0001455509482184425\n",
            "Epoch  18 Batch  490 / 525  Training Loss  0.0001756486890371889\n",
            "Epoch  18 Batch  491 / 525  Training Loss  0.00020825539832003415\n",
            "Epoch  18 Batch  492 / 525  Training Loss  0.0003077978326473385\n",
            "Epoch  18 Batch  493 / 525  Training Loss  0.00012135424185544252\n",
            "Epoch  18 Batch  494 / 525  Training Loss  0.00012852376676164567\n",
            "Epoch  18 Batch  495 / 525  Training Loss  0.00017805950483307242\n",
            "Epoch  18 Batch  496 / 525  Training Loss  0.0002701578487176448\n",
            "Epoch  18 Batch  497 / 525  Training Loss  0.0001348985533695668\n",
            "Epoch  18 Batch  498 / 525  Training Loss  0.00018701390945352614\n",
            "Epoch  18 Batch  499 / 525  Training Loss  0.00013106786354910582\n",
            "Epoch  18 Batch  500 / 525  Training Loss  0.0003369340265635401\n",
            "Epoch  18 Batch  501 / 525  Training Loss  0.00019628174777608365\n",
            "Epoch  18 Batch  502 / 525  Training Loss  0.0001650019985390827\n",
            "Epoch  18 Batch  503 / 525  Training Loss  0.0001768840302247554\n",
            "Epoch  18 Batch  504 / 525  Training Loss  0.00019643800624180585\n",
            "Epoch  18 Batch  505 / 525  Training Loss  0.00023020342632662505\n",
            "Epoch  18 Batch  506 / 525  Training Loss  0.00013245202717371285\n",
            "Epoch  18 Batch  507 / 525  Training Loss  0.00014251664106268436\n",
            "Epoch  18 Batch  508 / 525  Training Loss  0.00014320464106276631\n",
            "Epoch  18 Batch  509 / 525  Training Loss  0.00025303271831944585\n",
            "Epoch  18 Batch  510 / 525  Training Loss  0.00032302617910318077\n",
            "Epoch  18 Batch  511 / 525  Training Loss  0.00013435959408525378\n",
            "Epoch  18 Batch  512 / 525  Training Loss  0.00023590491036884487\n",
            "Epoch  18 Batch  513 / 525  Training Loss  0.0002034558419836685\n",
            "Epoch  18 Batch  514 / 525  Training Loss  0.0002565725881140679\n",
            "Epoch  18 Batch  515 / 525  Training Loss  0.00017803351511247456\n",
            "Epoch  18 Batch  516 / 525  Training Loss  0.000209920690394938\n",
            "Epoch  18 Batch  517 / 525  Training Loss  0.00011825569526990876\n",
            "Epoch  18 Batch  518 / 525  Training Loss  0.00021246956021059304\n",
            "Epoch  18 Batch  519 / 525  Training Loss  0.000188347403309308\n",
            "Epoch  18 Batch  520 / 525  Training Loss  9.728869190439582e-05\n",
            "Epoch  18 Batch  521 / 525  Training Loss  0.00018022616859525442\n",
            "Epoch  18 Batch  522 / 525  Training Loss  0.0001379211462335661\n",
            "Epoch  18 Batch  523 / 525  Training Loss  9.717205102788284e-05\n",
            "Epoch  18 Batch  524 / 525  Training Loss  0.0003796507662627846\n",
            "  19    |    -    |   0.000332   |   63.97  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 19\n",
            "Epoch  19 Batch  0 / 525  Training Loss  8.757179602980614e-05\n",
            "Epoch  19 Batch  1 / 525  Training Loss  0.00018701385124586523\n",
            "Epoch  19 Batch  2 / 525  Training Loss  0.00017549862968735397\n",
            "Epoch  19 Batch  3 / 525  Training Loss  0.00017900676175486296\n",
            "Epoch  19 Batch  4 / 525  Training Loss  0.00016127164417412132\n",
            "Epoch  19 Batch  5 / 525  Training Loss  0.00026203523157164454\n",
            "Epoch  19 Batch  6 / 525  Training Loss  0.00011665242345770821\n",
            "Epoch  19 Batch  7 / 525  Training Loss  0.00038512767059728503\n",
            "Epoch  19 Batch  8 / 525  Training Loss  0.0001285831385757774\n",
            "Epoch  19 Batch  9 / 525  Training Loss  0.00015544236521236598\n",
            "Epoch  19 Batch  10 / 525  Training Loss  0.00013708186452277005\n",
            "Epoch  19 Batch  11 / 525  Training Loss  9.101704199565575e-05\n",
            "Epoch  19 Batch  12 / 525  Training Loss  9.356479858979583e-05\n",
            "Epoch  19 Batch  13 / 525  Training Loss  8.848907600622624e-05\n",
            "Epoch  19 Batch  14 / 525  Training Loss  0.00012639783381018788\n",
            "Epoch  19 Batch  15 / 525  Training Loss  0.00011417982022976503\n",
            "Epoch  19 Batch  16 / 525  Training Loss  6.95266789989546e-05\n",
            "Epoch  19 Batch  17 / 525  Training Loss  0.00010208537423750386\n",
            "Epoch  19 Batch  18 / 525  Training Loss  0.0002116934338118881\n",
            "Epoch  19 Batch  19 / 525  Training Loss  7.905533129815012e-05\n",
            "Epoch  19 Batch  20 / 525  Training Loss  0.00013524029054678977\n",
            "Epoch  19 Batch  21 / 525  Training Loss  0.00013065082021057606\n",
            "Epoch  19 Batch  22 / 525  Training Loss  9.936369315255433e-05\n",
            "Epoch  19 Batch  23 / 525  Training Loss  0.0001017700633383356\n",
            "Epoch  19 Batch  24 / 525  Training Loss  0.00011257899313932285\n",
            "Epoch  19 Batch  25 / 525  Training Loss  0.0004842267371714115\n",
            "Epoch  19 Batch  26 / 525  Training Loss  0.00012151394184911624\n",
            "Epoch  19 Batch  27 / 525  Training Loss  0.00016171559400390834\n",
            "Epoch  19 Batch  28 / 525  Training Loss  0.00010766500781755894\n",
            "Epoch  19 Batch  29 / 525  Training Loss  0.00022852134134154767\n",
            "Epoch  19 Batch  30 / 525  Training Loss  0.00013639329699799418\n",
            "Epoch  19 Batch  31 / 525  Training Loss  9.456964471610263e-05\n",
            "Epoch  19 Batch  32 / 525  Training Loss  8.168314525391906e-05\n",
            "Epoch  19 Batch  33 / 525  Training Loss  8.348692063009366e-05\n",
            "Epoch  19 Batch  34 / 525  Training Loss  0.00010440147889312357\n",
            "Epoch  19 Batch  35 / 525  Training Loss  0.00013251374184619635\n",
            "Epoch  19 Batch  36 / 525  Training Loss  0.00027418616809882224\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  19 Batch  37 / 525  Training Loss  8.511553460266441e-05\n",
            "Epoch  19 Batch  38 / 525  Training Loss  0.00011611045192694291\n",
            "Epoch  19 Batch  39 / 525  Training Loss  0.00010780091542983428\n",
            "Epoch  19 Batch  40 / 525  Training Loss  0.00012826667807530612\n",
            "Epoch  19 Batch  41 / 525  Training Loss  0.00013192139158491045\n",
            "Epoch  19 Batch  42 / 525  Training Loss  7.048578117974102e-05\n",
            "Epoch  19 Batch  43 / 525  Training Loss  0.00010704727901611477\n",
            "Epoch  19 Batch  44 / 525  Training Loss  0.00019087694818153977\n",
            "Epoch  19 Batch  45 / 525  Training Loss  0.000157248301547952\n",
            "Epoch  19 Batch  46 / 525  Training Loss  0.00010222274431725964\n",
            "Epoch  19 Batch  47 / 525  Training Loss  0.00013828324154019356\n",
            "Epoch  19 Batch  48 / 525  Training Loss  0.0001648358447710052\n",
            "Epoch  19 Batch  49 / 525  Training Loss  0.00013586849672719836\n",
            "Epoch  19 Batch  50 / 525  Training Loss  0.00015113680274225771\n",
            "Epoch  19 Batch  51 / 525  Training Loss  0.00010256010864395648\n",
            "Epoch  19 Batch  52 / 525  Training Loss  8.302439528051764e-05\n",
            "Epoch  19 Batch  53 / 525  Training Loss  0.0007567703141830862\n",
            "Epoch  19 Batch  54 / 525  Training Loss  0.00020111720368731767\n",
            "Epoch  19 Batch  55 / 525  Training Loss  0.0001341894530924037\n",
            "Epoch  19 Batch  56 / 525  Training Loss  0.00010217844828730449\n",
            "Epoch  19 Batch  57 / 525  Training Loss  0.00014115130761638284\n",
            "Epoch  19 Batch  58 / 525  Training Loss  0.00014975029625929892\n",
            "Epoch  19 Batch  59 / 525  Training Loss  0.00014696356083732098\n",
            "Epoch  19 Batch  60 / 525  Training Loss  0.00017861457308754325\n",
            "Epoch  19 Batch  61 / 525  Training Loss  0.00024034656235016882\n",
            "Epoch  19 Batch  62 / 525  Training Loss  7.169228047132492e-05\n",
            "Epoch  19 Batch  63 / 525  Training Loss  0.00016643716662656516\n",
            "Epoch  19 Batch  64 / 525  Training Loss  0.00012293242616578937\n",
            "Epoch  19 Batch  65 / 525  Training Loss  0.0044826967641711235\n",
            "Epoch  19 Batch  66 / 525  Training Loss  0.00011140988499391824\n",
            "Epoch  19 Batch  67 / 525  Training Loss  8.160636207321659e-05\n",
            "Epoch  19 Batch  68 / 525  Training Loss  0.00011894614726770669\n",
            "Epoch  19 Batch  69 / 525  Training Loss  0.0021476401016116142\n",
            "Epoch  19 Batch  70 / 525  Training Loss  0.00013911919086240232\n",
            "Epoch  19 Batch  71 / 525  Training Loss  9.90131520666182e-05\n",
            "Epoch  19 Batch  72 / 525  Training Loss  0.00015743127733003348\n",
            "Epoch  19 Batch  73 / 525  Training Loss  0.0002680968027561903\n",
            "Epoch  19 Batch  74 / 525  Training Loss  0.00015529655502177775\n",
            "Epoch  19 Batch  75 / 525  Training Loss  0.00021652851137332618\n",
            "Epoch  19 Batch  76 / 525  Training Loss  0.0001379645400447771\n",
            "Epoch  19 Batch  77 / 525  Training Loss  0.0001472678268328309\n",
            "Epoch  19 Batch  78 / 525  Training Loss  0.00027327120187692344\n",
            "Epoch  19 Batch  79 / 525  Training Loss  0.00023554047220386565\n",
            "Epoch  19 Batch  80 / 525  Training Loss  0.00012113896809751168\n",
            "Epoch  19 Batch  81 / 525  Training Loss  9.823487926041707e-05\n",
            "Epoch  19 Batch  82 / 525  Training Loss  0.00022891536355018616\n",
            "Epoch  19 Batch  83 / 525  Training Loss  0.00011701645416906103\n",
            "Epoch  19 Batch  84 / 525  Training Loss  0.00017129405750893056\n",
            "Epoch  19 Batch  85 / 525  Training Loss  9.46112340898253e-05\n",
            "Epoch  19 Batch  86 / 525  Training Loss  0.0001568049192428589\n",
            "Epoch  19 Batch  87 / 525  Training Loss  0.00018762538093142211\n",
            "Epoch  19 Batch  88 / 525  Training Loss  0.00012614882143680006\n",
            "Epoch  19 Batch  89 / 525  Training Loss  0.00012523005716502666\n",
            "Epoch  19 Batch  90 / 525  Training Loss  0.00011167870979988948\n",
            "Epoch  19 Batch  91 / 525  Training Loss  0.00016916595632210374\n",
            "Epoch  19 Batch  92 / 525  Training Loss  0.0002011060423683375\n",
            "Epoch  19 Batch  93 / 525  Training Loss  9.81805642368272e-05\n",
            "Epoch  19 Batch  94 / 525  Training Loss  0.00015054672257974744\n",
            "Epoch  19 Batch  95 / 525  Training Loss  0.00013262238644529134\n",
            "Epoch  19 Batch  96 / 525  Training Loss  8.40707216411829e-05\n",
            "Epoch  19 Batch  97 / 525  Training Loss  0.00013095229223836213\n",
            "Epoch  19 Batch  98 / 525  Training Loss  0.00024491234216839075\n",
            "Epoch  19 Batch  99 / 525  Training Loss  0.0001443486544303596\n",
            "Epoch  19 Batch  100 / 525  Training Loss  0.00019385898485779762\n",
            "Epoch  19 Batch  101 / 525  Training Loss  0.00023656820121686906\n",
            "Epoch  19 Batch  102 / 525  Training Loss  0.00011184015602339059\n",
            "Epoch  19 Batch  103 / 525  Training Loss  0.00012269338185433298\n",
            "Epoch  19 Batch  104 / 525  Training Loss  8.933745993999764e-05\n",
            "Epoch  19 Batch  105 / 525  Training Loss  0.00015328409790527076\n",
            "Epoch  19 Batch  106 / 525  Training Loss  0.00011203125905012712\n",
            "Epoch  19 Batch  107 / 525  Training Loss  0.0001071021324605681\n",
            "Epoch  19 Batch  108 / 525  Training Loss  0.0001160914107458666\n",
            "Epoch  19 Batch  109 / 525  Training Loss  0.00013132308959029615\n",
            "Epoch  19 Batch  110 / 525  Training Loss  0.00013251646305434406\n",
            "Epoch  19 Batch  111 / 525  Training Loss  0.00022384831390809268\n",
            "Epoch  19 Batch  112 / 525  Training Loss  0.00013996619964018464\n",
            "Epoch  19 Batch  113 / 525  Training Loss  0.0001590145257068798\n",
            "Epoch  19 Batch  114 / 525  Training Loss  0.0001457732287235558\n",
            "Epoch  19 Batch  115 / 525  Training Loss  0.00011288914538454264\n",
            "Epoch  19 Batch  116 / 525  Training Loss  0.00014931577607057989\n",
            "Epoch  19 Batch  117 / 525  Training Loss  0.00013312151713762432\n",
            "Epoch  19 Batch  118 / 525  Training Loss  0.00016319405403919518\n",
            "Epoch  19 Batch  119 / 525  Training Loss  0.0001146205686382018\n",
            "Epoch  19 Batch  120 / 525  Training Loss  0.00010519538773223758\n",
            "Epoch  19 Batch  121 / 525  Training Loss  7.89256882853806e-05\n",
            "Epoch  19 Batch  122 / 525  Training Loss  7.943138189148158e-05\n",
            "Epoch  19 Batch  123 / 525  Training Loss  0.00023469116422347724\n",
            "Epoch  19 Batch  124 / 525  Training Loss  0.00011009593436028808\n",
            "Epoch  19 Batch  125 / 525  Training Loss  0.00011737697059288621\n",
            "Epoch  19 Batch  126 / 525  Training Loss  0.00015045976033434272\n",
            "Epoch  19 Batch  127 / 525  Training Loss  8.240975148510188e-05\n",
            "Epoch  19 Batch  128 / 525  Training Loss  0.00010706608736654744\n",
            "Epoch  19 Batch  129 / 525  Training Loss  0.00011936051305383444\n",
            "Epoch  19 Batch  130 / 525  Training Loss  9.409260383108631e-05\n",
            "Epoch  19 Batch  131 / 525  Training Loss  0.00011238492152187973\n",
            "Epoch  19 Batch  132 / 525  Training Loss  0.00011475590144982561\n",
            "Epoch  19 Batch  133 / 525  Training Loss  0.0003298308001831174\n",
            "Epoch  19 Batch  134 / 525  Training Loss  0.0001020661584334448\n",
            "Epoch  19 Batch  135 / 525  Training Loss  8.734522998565808e-05\n",
            "Epoch  19 Batch  136 / 525  Training Loss  9.172153659164906e-05\n",
            "Epoch  19 Batch  137 / 525  Training Loss  0.00010960958752548322\n",
            "Epoch  19 Batch  138 / 525  Training Loss  0.00013306699111126363\n",
            "Epoch  19 Batch  139 / 525  Training Loss  0.00011783299123635516\n",
            "Epoch  19 Batch  140 / 525  Training Loss  0.00013882188068237156\n",
            "Epoch  19 Batch  141 / 525  Training Loss  0.00010685717279557139\n",
            "Epoch  19 Batch  142 / 525  Training Loss  0.000136508111609146\n",
            "Epoch  19 Batch  143 / 525  Training Loss  0.0001140303211286664\n",
            "Epoch  19 Batch  144 / 525  Training Loss  0.00021021455177105963\n",
            "Epoch  19 Batch  145 / 525  Training Loss  0.00011266610817983747\n",
            "Epoch  19 Batch  146 / 525  Training Loss  0.00012821734708268195\n",
            "Epoch  19 Batch  147 / 525  Training Loss  0.00016715160745661706\n",
            "Epoch  19 Batch  148 / 525  Training Loss  0.00012629563570953906\n",
            "Epoch  19 Batch  149 / 525  Training Loss  0.00012117938604205847\n",
            "Epoch  19 Batch  150 / 525  Training Loss  0.00014744710642844439\n",
            "Epoch  19 Batch  151 / 525  Training Loss  8.697615703567863e-05\n",
            "Epoch  19 Batch  152 / 525  Training Loss  0.00011789482232416049\n",
            "Epoch  19 Batch  153 / 525  Training Loss  0.00013736948312725872\n",
            "Epoch  19 Batch  154 / 525  Training Loss  0.00020310278341639787\n",
            "Epoch  19 Batch  155 / 525  Training Loss  0.00022824663028586656\n",
            "Epoch  19 Batch  156 / 525  Training Loss  0.00011752264254027978\n",
            "Epoch  19 Batch  157 / 525  Training Loss  0.00018694160098675638\n",
            "Epoch  19 Batch  158 / 525  Training Loss  0.00010380301682744175\n",
            "Epoch  19 Batch  159 / 525  Training Loss  0.000152540291310288\n",
            "Epoch  19 Batch  160 / 525  Training Loss  0.0001359798916382715\n",
            "Epoch  19 Batch  161 / 525  Training Loss  0.00011799868661910295\n",
            "Epoch  19 Batch  162 / 525  Training Loss  0.0001383212220389396\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  19 Batch  163 / 525  Training Loss  0.00014505010040011257\n",
            "Epoch  19 Batch  164 / 525  Training Loss  0.00013724098971579224\n",
            "Epoch  19 Batch  165 / 525  Training Loss  8.60153668327257e-05\n",
            "Epoch  19 Batch  166 / 525  Training Loss  0.00010527707490837201\n",
            "Epoch  19 Batch  167 / 525  Training Loss  0.00014087659656070173\n",
            "Epoch  19 Batch  168 / 525  Training Loss  0.00010543405369389802\n",
            "Epoch  19 Batch  169 / 525  Training Loss  0.0004936348414048553\n",
            "Epoch  19 Batch  170 / 525  Training Loss  9.421163122169673e-05\n",
            "Epoch  19 Batch  171 / 525  Training Loss  0.00018308471771888435\n",
            "Epoch  19 Batch  172 / 525  Training Loss  0.0001396763836964965\n",
            "Epoch  19 Batch  173 / 525  Training Loss  8.644970512250438e-05\n",
            "Epoch  19 Batch  174 / 525  Training Loss  0.00010708853369578719\n",
            "Epoch  19 Batch  175 / 525  Training Loss  0.00010394594573881477\n",
            "Epoch  19 Batch  176 / 525  Training Loss  0.00012915433035232127\n",
            "Epoch  19 Batch  177 / 525  Training Loss  0.00012566157965920866\n",
            "Epoch  19 Batch  178 / 525  Training Loss  0.0001109993681893684\n",
            "Epoch  19 Batch  179 / 525  Training Loss  0.00023652864911127836\n",
            "Epoch  19 Batch  180 / 525  Training Loss  0.0001786489156074822\n",
            "Epoch  19 Batch  181 / 525  Training Loss  7.526289846282452e-05\n",
            "Epoch  19 Batch  182 / 525  Training Loss  0.00010721721628215164\n",
            "Epoch  19 Batch  183 / 525  Training Loss  0.00025813939282670617\n",
            "Epoch  19 Batch  184 / 525  Training Loss  0.00028888724045827985\n",
            "Epoch  19 Batch  185 / 525  Training Loss  0.0005763982771895826\n",
            "Epoch  19 Batch  186 / 525  Training Loss  0.0001746028137858957\n",
            "Epoch  19 Batch  187 / 525  Training Loss  0.00016225027502514422\n",
            "Epoch  19 Batch  188 / 525  Training Loss  0.0001994959602598101\n",
            "Epoch  19 Batch  189 / 525  Training Loss  0.00015478039858862758\n",
            "Epoch  19 Batch  190 / 525  Training Loss  0.00010622034460538998\n",
            "Epoch  19 Batch  191 / 525  Training Loss  0.0001514672621851787\n",
            "Epoch  19 Batch  192 / 525  Training Loss  0.0002346049586776644\n",
            "Epoch  19 Batch  193 / 525  Training Loss  0.00016445746587123722\n",
            "Epoch  19 Batch  194 / 525  Training Loss  8.735352457733825e-05\n",
            "Epoch  19 Batch  195 / 525  Training Loss  8.385938417632133e-05\n",
            "Epoch  19 Batch  196 / 525  Training Loss  0.00020602616132237017\n",
            "Epoch  19 Batch  197 / 525  Training Loss  7.682306022616103e-05\n",
            "Epoch  19 Batch  198 / 525  Training Loss  0.0001449464471079409\n",
            "Epoch  19 Batch  199 / 525  Training Loss  0.00013400395982898772\n",
            "Epoch  19 Batch  200 / 525  Training Loss  7.822913175914437e-05\n",
            "Epoch  19 Batch  201 / 525  Training Loss  0.00012257110211066902\n",
            "Epoch  19 Batch  202 / 525  Training Loss  0.00018860542331822217\n",
            "Epoch  19 Batch  203 / 525  Training Loss  0.00012104069901397452\n",
            "Epoch  19 Batch  204 / 525  Training Loss  0.00023002782836556435\n",
            "Epoch  19 Batch  205 / 525  Training Loss  0.00010439664038131014\n",
            "Epoch  19 Batch  206 / 525  Training Loss  0.0001411236880812794\n",
            "Epoch  19 Batch  207 / 525  Training Loss  0.00011298280151095241\n",
            "Epoch  19 Batch  208 / 525  Training Loss  0.00014191619993653148\n",
            "Epoch  19 Batch  209 / 525  Training Loss  0.0001747289497870952\n",
            "Epoch  19 Batch  210 / 525  Training Loss  8.098033140413463e-05\n",
            "Epoch  19 Batch  211 / 525  Training Loss  8.212500688387081e-05\n",
            "Epoch  19 Batch  212 / 525  Training Loss  0.00010564735566731542\n",
            "Epoch  19 Batch  213 / 525  Training Loss  0.00012941143359057605\n",
            "Epoch  19 Batch  214 / 525  Training Loss  0.0003423443413339555\n",
            "Epoch  19 Batch  215 / 525  Training Loss  6.444835162255913e-05\n",
            "Epoch  19 Batch  216 / 525  Training Loss  0.00014868061407469213\n",
            "Epoch  19 Batch  217 / 525  Training Loss  0.00013759321882389486\n",
            "Epoch  19 Batch  218 / 525  Training Loss  0.00012139986210968345\n",
            "Epoch  19 Batch  219 / 525  Training Loss  0.00012882192095275968\n",
            "Epoch  19 Batch  220 / 525  Training Loss  0.00012762095138896257\n",
            "Epoch  19 Batch  221 / 525  Training Loss  9.203815716318786e-05\n",
            "Epoch  19 Batch  222 / 525  Training Loss  0.00013018790923524648\n",
            "Epoch  19 Batch  223 / 525  Training Loss  0.00013414343993645161\n",
            "Epoch  19 Batch  224 / 525  Training Loss  4.055603130836971e-05\n",
            "Epoch  19 Batch  225 / 525  Training Loss  0.00011509156320244074\n",
            "Epoch  19 Batch  226 / 525  Training Loss  0.00016420005704276264\n",
            "Epoch  19 Batch  227 / 525  Training Loss  0.00019113649614155293\n",
            "Epoch  19 Batch  228 / 525  Training Loss  7.738516433164477e-05\n",
            "Epoch  19 Batch  229 / 525  Training Loss  0.00011148884368594736\n",
            "Epoch  19 Batch  230 / 525  Training Loss  0.00011273021664237604\n",
            "Epoch  19 Batch  231 / 525  Training Loss  0.00013649286120198667\n",
            "Epoch  19 Batch  232 / 525  Training Loss  0.00011056385847041383\n",
            "Epoch  19 Batch  233 / 525  Training Loss  0.00025848523364402354\n",
            "Epoch  19 Batch  234 / 525  Training Loss  9.737483924254775e-05\n",
            "Epoch  19 Batch  235 / 525  Training Loss  0.00011035508941859007\n",
            "Epoch  19 Batch  236 / 525  Training Loss  6.321619002847001e-05\n",
            "Epoch  19 Batch  237 / 525  Training Loss  6.64532053633593e-05\n",
            "Epoch  19 Batch  238 / 525  Training Loss  0.00012757939111907035\n",
            "Epoch  19 Batch  239 / 525  Training Loss  0.0001504426181782037\n",
            "Epoch  19 Batch  240 / 525  Training Loss  0.00010067880793940276\n",
            "Epoch  19 Batch  241 / 525  Training Loss  0.0001913238229462877\n",
            "Epoch  19 Batch  242 / 525  Training Loss  0.00010432896669954062\n",
            "Epoch  19 Batch  243 / 525  Training Loss  9.556432632962242e-05\n",
            "Epoch  19 Batch  244 / 525  Training Loss  0.00013830191164743155\n",
            "Epoch  19 Batch  245 / 525  Training Loss  0.00011515044752741233\n",
            "Epoch  19 Batch  246 / 525  Training Loss  0.00013237902021501213\n",
            "Epoch  19 Batch  247 / 525  Training Loss  0.00013258129183668643\n",
            "Epoch  19 Batch  248 / 525  Training Loss  0.00010716480028349906\n",
            "Epoch  19 Batch  249 / 525  Training Loss  0.00011274040298303589\n",
            "Epoch  19 Batch  250 / 525  Training Loss  0.00013217495870776474\n",
            "Epoch  19 Batch  251 / 525  Training Loss  0.00014066490984987468\n",
            "Epoch  19 Batch  252 / 525  Training Loss  6.33237068541348e-05\n",
            "Epoch  19 Batch  253 / 525  Training Loss  0.0001679804117884487\n",
            "Epoch  19 Batch  254 / 525  Training Loss  0.0001167390146292746\n",
            "Epoch  19 Batch  255 / 525  Training Loss  0.00011436677596066147\n",
            "Epoch  19 Batch  256 / 525  Training Loss  0.0001107215866795741\n",
            "Epoch  19 Batch  257 / 525  Training Loss  7.864775398047641e-05\n",
            "Epoch  19 Batch  258 / 525  Training Loss  0.00015925653860904276\n",
            "Epoch  19 Batch  259 / 525  Training Loss  8.173378591891378e-05\n",
            "Epoch  19 Batch  260 / 525  Training Loss  8.655402052681893e-05\n",
            "Epoch  19 Batch  261 / 525  Training Loss  0.00016253087960649282\n",
            "Epoch  19 Batch  262 / 525  Training Loss  7.94833613326773e-05\n",
            "Epoch  19 Batch  263 / 525  Training Loss  8.517839887645096e-05\n",
            "Epoch  19 Batch  264 / 525  Training Loss  0.00010127756104338914\n",
            "Epoch  19 Batch  265 / 525  Training Loss  0.00010772555833682418\n",
            "Epoch  19 Batch  266 / 525  Training Loss  0.0001354105188511312\n",
            "Epoch  19 Batch  267 / 525  Training Loss  0.00011962506687268615\n",
            "Epoch  19 Batch  268 / 525  Training Loss  9.385285375174135e-05\n",
            "Epoch  19 Batch  269 / 525  Training Loss  6.937533908057958e-05\n",
            "Epoch  19 Batch  270 / 525  Training Loss  0.00011567794717848301\n",
            "Epoch  19 Batch  271 / 525  Training Loss  0.00011116631503682584\n",
            "Epoch  19 Batch  272 / 525  Training Loss  0.00014061323599889874\n",
            "Epoch  19 Batch  273 / 525  Training Loss  7.910343265393749e-05\n",
            "Epoch  19 Batch  274 / 525  Training Loss  9.66171792242676e-05\n",
            "Epoch  19 Batch  275 / 525  Training Loss  0.00011586248729145154\n",
            "Epoch  19 Batch  276 / 525  Training Loss  9.838286496233195e-05\n",
            "Epoch  19 Batch  277 / 525  Training Loss  0.00011049349268432707\n",
            "Epoch  19 Batch  278 / 525  Training Loss  0.00013736743130721152\n",
            "Epoch  19 Batch  279 / 525  Training Loss  0.00020526717707980424\n",
            "Epoch  19 Batch  280 / 525  Training Loss  6.2330967921298e-05\n",
            "Epoch  19 Batch  281 / 525  Training Loss  8.176290430128574e-05\n",
            "Epoch  19 Batch  282 / 525  Training Loss  9.240531653631479e-05\n",
            "Epoch  19 Batch  283 / 525  Training Loss  0.004918969236314297\n",
            "Epoch  19 Batch  284 / 525  Training Loss  0.0001515045005362481\n",
            "Epoch  19 Batch  285 / 525  Training Loss  0.00019332754891365767\n",
            "Epoch  19 Batch  286 / 525  Training Loss  0.00014592698425985873\n",
            "Epoch  19 Batch  287 / 525  Training Loss  0.0001433721190551296\n",
            "Epoch  19 Batch  288 / 525  Training Loss  0.00011082084529334679\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  19 Batch  289 / 525  Training Loss  0.009204966947436333\n",
            "Epoch  19 Batch  290 / 525  Training Loss  0.00014758309407625347\n",
            "Epoch  19 Batch  291 / 525  Training Loss  0.0002269689430249855\n",
            "Epoch  19 Batch  292 / 525  Training Loss  0.00012034390965709463\n",
            "Epoch  19 Batch  293 / 525  Training Loss  0.00015561516920570284\n",
            "Epoch  19 Batch  294 / 525  Training Loss  0.00013815201236866415\n",
            "Epoch  19 Batch  295 / 525  Training Loss  0.00018120257300324738\n",
            "Epoch  19 Batch  296 / 525  Training Loss  9.829176997300237e-05\n",
            "Epoch  19 Batch  297 / 525  Training Loss  0.00010517396731302142\n",
            "Epoch  19 Batch  298 / 525  Training Loss  0.00014281505718827248\n",
            "Epoch  19 Batch  299 / 525  Training Loss  0.00015442166477441788\n",
            "Epoch  19 Batch  300 / 525  Training Loss  0.00015838249237276614\n",
            "Epoch  19 Batch  301 / 525  Training Loss  0.00013922294601798058\n",
            "Epoch  19 Batch  302 / 525  Training Loss  0.00015800580149516463\n",
            "Epoch  19 Batch  303 / 525  Training Loss  0.0001901568321045488\n",
            "Epoch  19 Batch  304 / 525  Training Loss  0.00013412832049652934\n",
            "Epoch  19 Batch  305 / 525  Training Loss  0.00034457063884474337\n",
            "Epoch  19 Batch  306 / 525  Training Loss  0.00019382838218007237\n",
            "Epoch  19 Batch  307 / 525  Training Loss  0.00011370408901711926\n",
            "Epoch  19 Batch  308 / 525  Training Loss  0.00017473820480518043\n",
            "Epoch  19 Batch  309 / 525  Training Loss  0.00015489032375626266\n",
            "Epoch  19 Batch  310 / 525  Training Loss  9.187495743390173e-05\n",
            "Epoch  19 Batch  311 / 525  Training Loss  0.00012996770965401083\n",
            "Epoch  19 Batch  312 / 525  Training Loss  8.903889829525724e-05\n",
            "Epoch  19 Batch  313 / 525  Training Loss  0.0001702803565422073\n",
            "Epoch  19 Batch  314 / 525  Training Loss  0.00017278289305977523\n",
            "Epoch  19 Batch  315 / 525  Training Loss  0.00014047320291865617\n",
            "Epoch  19 Batch  316 / 525  Training Loss  7.652144995518029e-05\n",
            "Epoch  19 Batch  317 / 525  Training Loss  0.00016406072245445102\n",
            "Epoch  19 Batch  318 / 525  Training Loss  0.00011662157339742407\n",
            "Epoch  19 Batch  319 / 525  Training Loss  0.00011289909889455885\n",
            "Epoch  19 Batch  320 / 525  Training Loss  0.00011057391384383664\n",
            "Epoch  19 Batch  321 / 525  Training Loss  0.00011646506754914299\n",
            "Epoch  19 Batch  322 / 525  Training Loss  0.00011467176955193281\n",
            "Epoch  19 Batch  323 / 525  Training Loss  8.94767654244788e-05\n",
            "Epoch  19 Batch  324 / 525  Training Loss  0.00019996387709397823\n",
            "Epoch  19 Batch  325 / 525  Training Loss  7.181885302998126e-05\n",
            "Epoch  19 Batch  326 / 525  Training Loss  0.00010546960402280092\n",
            "Epoch  19 Batch  327 / 525  Training Loss  0.0001046724064508453\n",
            "Epoch  19 Batch  328 / 525  Training Loss  0.00010225856385659426\n",
            "Epoch  19 Batch  329 / 525  Training Loss  0.0001065503092831932\n",
            "Epoch  19 Batch  330 / 525  Training Loss  0.00012362212873995304\n",
            "Epoch  19 Batch  331 / 525  Training Loss  9.706575656309724e-05\n",
            "Epoch  19 Batch  332 / 525  Training Loss  0.00014578514674212784\n",
            "Epoch  19 Batch  333 / 525  Training Loss  0.00015732334577478468\n",
            "Epoch  19 Batch  334 / 525  Training Loss  0.0002187125792261213\n",
            "Epoch  19 Batch  335 / 525  Training Loss  0.00012083215551683679\n",
            "Epoch  19 Batch  336 / 525  Training Loss  8.949746552389115e-05\n",
            "Epoch  19 Batch  337 / 525  Training Loss  0.00013000835315324366\n",
            "Epoch  19 Batch  338 / 525  Training Loss  0.00010683901200536638\n",
            "Epoch  19 Batch  339 / 525  Training Loss  0.0001543255930300802\n",
            "Epoch  19 Batch  340 / 525  Training Loss  0.0001976654602913186\n",
            "Epoch  19 Batch  341 / 525  Training Loss  0.00017216571723110974\n",
            "Epoch  19 Batch  342 / 525  Training Loss  9.535571007290855e-05\n",
            "Epoch  19 Batch  343 / 525  Training Loss  0.0001318728318437934\n",
            "Epoch  19 Batch  344 / 525  Training Loss  9.334207425126806e-05\n",
            "Epoch  19 Batch  345 / 525  Training Loss  0.0001576204813318327\n",
            "Epoch  19 Batch  346 / 525  Training Loss  0.0003003943129442632\n",
            "Epoch  19 Batch  347 / 525  Training Loss  0.00012314252671785653\n",
            "Epoch  19 Batch  348 / 525  Training Loss  0.00011310017725918442\n",
            "Epoch  19 Batch  349 / 525  Training Loss  0.00013091455912217498\n",
            "Epoch  19 Batch  350 / 525  Training Loss  0.00010602222755551338\n",
            "Epoch  19 Batch  351 / 525  Training Loss  0.00013610979658551514\n",
            "Epoch  19 Batch  352 / 525  Training Loss  8.975287346402183e-05\n",
            "Epoch  19 Batch  353 / 525  Training Loss  0.00014907008153386414\n",
            "Epoch  19 Batch  354 / 525  Training Loss  9.38179437071085e-05\n",
            "Epoch  19 Batch  355 / 525  Training Loss  0.0001324402546742931\n",
            "Epoch  19 Batch  356 / 525  Training Loss  0.00018316807108931243\n",
            "Epoch  19 Batch  357 / 525  Training Loss  0.00011124768207082525\n",
            "Epoch  19 Batch  358 / 525  Training Loss  7.669808837817982e-05\n",
            "Epoch  19 Batch  359 / 525  Training Loss  7.349324732786044e-05\n",
            "Epoch  19 Batch  360 / 525  Training Loss  0.00012094180419808254\n",
            "Epoch  19 Batch  361 / 525  Training Loss  0.00011167387856403366\n",
            "Epoch  19 Batch  362 / 525  Training Loss  0.00012076403072569519\n",
            "Epoch  19 Batch  363 / 525  Training Loss  0.00012973762932233512\n",
            "Epoch  19 Batch  364 / 525  Training Loss  9.854867676040158e-05\n",
            "Epoch  19 Batch  365 / 525  Training Loss  0.00012055328261340037\n",
            "Epoch  19 Batch  366 / 525  Training Loss  0.00011420434020692483\n",
            "Epoch  19 Batch  367 / 525  Training Loss  0.00010062052024295554\n",
            "Epoch  19 Batch  368 / 525  Training Loss  0.00021761588868685067\n",
            "Epoch  19 Batch  369 / 525  Training Loss  9.27243527257815e-05\n",
            "Epoch  19 Batch  370 / 525  Training Loss  0.00022210525639820844\n",
            "Epoch  19 Batch  371 / 525  Training Loss  0.0001724114699754864\n",
            "Epoch  19 Batch  372 / 525  Training Loss  9.586022497387603e-05\n",
            "Epoch  19 Batch  373 / 525  Training Loss  8.634370169602334e-05\n",
            "Epoch  19 Batch  374 / 525  Training Loss  0.00013431459956336766\n",
            "Epoch  19 Batch  375 / 525  Training Loss  8.276232983916998e-05\n",
            "Epoch  19 Batch  376 / 525  Training Loss  0.00013748426863458008\n",
            "Epoch  19 Batch  377 / 525  Training Loss  8.337611507158726e-05\n",
            "Epoch  19 Batch  378 / 525  Training Loss  8.795638859737664e-05\n",
            "Epoch  19 Batch  379 / 525  Training Loss  8.582782174926251e-05\n",
            "Epoch  19 Batch  380 / 525  Training Loss  0.0001243370061274618\n",
            "Epoch  19 Batch  381 / 525  Training Loss  0.0001591089239809662\n",
            "Epoch  19 Batch  382 / 525  Training Loss  0.00014286920486483723\n",
            "Epoch  19 Batch  383 / 525  Training Loss  0.00011068048479501158\n",
            "Epoch  19 Batch  384 / 525  Training Loss  0.0001112609970732592\n",
            "Epoch  19 Batch  385 / 525  Training Loss  0.00013090172433294356\n",
            "Epoch  19 Batch  386 / 525  Training Loss  0.00011629481741692871\n",
            "Epoch  19 Batch  387 / 525  Training Loss  6.862192822154611e-05\n",
            "Epoch  19 Batch  388 / 525  Training Loss  6.835741078248248e-05\n",
            "Epoch  19 Batch  389 / 525  Training Loss  0.0001461564825149253\n",
            "Epoch  19 Batch  390 / 525  Training Loss  8.601568697486073e-05\n",
            "Epoch  19 Batch  391 / 525  Training Loss  8.740415069041774e-05\n",
            "Epoch  19 Batch  392 / 525  Training Loss  0.0002878875529859215\n",
            "Epoch  19 Batch  393 / 525  Training Loss  0.0005836073542013764\n",
            "Epoch  19 Batch  394 / 525  Training Loss  0.00027760191005654633\n",
            "Epoch  19 Batch  395 / 525  Training Loss  0.00011827496200567111\n",
            "Epoch  19 Batch  396 / 525  Training Loss  0.000107793697679881\n",
            "Epoch  19 Batch  397 / 525  Training Loss  0.00011959394032601267\n",
            "Epoch  19 Batch  398 / 525  Training Loss  0.00012237155169714242\n",
            "Epoch  19 Batch  399 / 525  Training Loss  0.000122422628919594\n",
            "Epoch  19 Batch  400 / 525  Training Loss  7.861477934056893e-05\n",
            "Epoch  19 Batch  401 / 525  Training Loss  0.00010846629447769374\n",
            "Epoch  19 Batch  402 / 525  Training Loss  9.799686085898429e-05\n",
            "Epoch  19 Batch  403 / 525  Training Loss  0.00014177644334267825\n",
            "Epoch  19 Batch  404 / 525  Training Loss  0.00015604848158545792\n",
            "Epoch  19 Batch  405 / 525  Training Loss  8.03233269834891e-05\n",
            "Epoch  19 Batch  406 / 525  Training Loss  9.565743675921112e-05\n",
            "Epoch  19 Batch  407 / 525  Training Loss  0.0001399054890498519\n",
            "Epoch  19 Batch  408 / 525  Training Loss  0.00015250266005750746\n",
            "Epoch  19 Batch  409 / 525  Training Loss  0.00015938769502099603\n",
            "Epoch  19 Batch  410 / 525  Training Loss  9.654687164584175e-05\n",
            "Epoch  19 Batch  411 / 525  Training Loss  0.00010322032176190987\n",
            "Epoch  19 Batch  412 / 525  Training Loss  9.386659075971693e-05\n",
            "Epoch  19 Batch  413 / 525  Training Loss  0.00011659642041195184\n",
            "Epoch  19 Batch  414 / 525  Training Loss  9.73424466792494e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  19 Batch  415 / 525  Training Loss  0.0001721348671708256\n",
            "Epoch  19 Batch  416 / 525  Training Loss  0.00013725514872930944\n",
            "Epoch  19 Batch  417 / 525  Training Loss  0.000191632701898925\n",
            "Epoch  19 Batch  418 / 525  Training Loss  0.00011808423005277291\n",
            "Epoch  19 Batch  419 / 525  Training Loss  9.250516450265422e-05\n",
            "Epoch  19 Batch  420 / 525  Training Loss  0.00010011375707108527\n",
            "Epoch  19 Batch  421 / 525  Training Loss  0.00011319203622406349\n",
            "Epoch  19 Batch  422 / 525  Training Loss  0.0001104335897252895\n",
            "Epoch  19 Batch  423 / 525  Training Loss  8.693741983734071e-05\n",
            "Epoch  19 Batch  424 / 525  Training Loss  0.0001732337404973805\n",
            "Epoch  19 Batch  425 / 525  Training Loss  0.00010632119665388018\n",
            "Epoch  19 Batch  426 / 525  Training Loss  0.00018542076577432454\n",
            "Epoch  19 Batch  427 / 525  Training Loss  0.0001298203133046627\n",
            "Epoch  19 Batch  428 / 525  Training Loss  7.413358252961189e-05\n",
            "Epoch  19 Batch  429 / 525  Training Loss  0.0002234037674497813\n",
            "Epoch  19 Batch  430 / 525  Training Loss  0.0001407791714882478\n",
            "Epoch  19 Batch  431 / 525  Training Loss  0.00013322233280632645\n",
            "Epoch  19 Batch  432 / 525  Training Loss  0.00013922504149377346\n",
            "Epoch  19 Batch  433 / 525  Training Loss  0.00013604499690700322\n",
            "Epoch  19 Batch  434 / 525  Training Loss  0.0003942689800169319\n",
            "Epoch  19 Batch  435 / 525  Training Loss  0.00011030385212507099\n",
            "Epoch  19 Batch  436 / 525  Training Loss  8.166003681253642e-05\n",
            "Epoch  19 Batch  437 / 525  Training Loss  0.0001701212750049308\n",
            "Epoch  19 Batch  438 / 525  Training Loss  0.00011602358426898718\n",
            "Epoch  19 Batch  439 / 525  Training Loss  0.00011354230809956789\n",
            "Epoch  19 Batch  440 / 525  Training Loss  0.00012481505109462887\n",
            "Epoch  19 Batch  441 / 525  Training Loss  0.00012197475734865293\n",
            "Epoch  19 Batch  442 / 525  Training Loss  0.00011764565715566278\n",
            "Epoch  19 Batch  443 / 525  Training Loss  0.0001298597053391859\n",
            "Epoch  19 Batch  444 / 525  Training Loss  0.00011317272583255544\n",
            "Epoch  19 Batch  445 / 525  Training Loss  0.00012330085155554116\n",
            "Epoch  19 Batch  446 / 525  Training Loss  0.00010211666813120246\n",
            "Epoch  19 Batch  447 / 525  Training Loss  0.00010414351709187031\n",
            "Epoch  19 Batch  448 / 525  Training Loss  0.0001174338540295139\n",
            "Epoch  19 Batch  449 / 525  Training Loss  8.573297236580402e-05\n",
            "Epoch  19 Batch  450 / 525  Training Loss  7.07024009898305e-05\n",
            "Epoch  19 Batch  451 / 525  Training Loss  6.609955744352192e-05\n",
            "Epoch  19 Batch  452 / 525  Training Loss  0.0001530501467641443\n",
            "Epoch  19 Batch  453 / 525  Training Loss  8.039678505156189e-05\n",
            "Epoch  19 Batch  454 / 525  Training Loss  0.00015657514450140297\n",
            "Epoch  19 Batch  455 / 525  Training Loss  9.272155875805765e-05\n",
            "Epoch  19 Batch  456 / 525  Training Loss  0.00010571429447736591\n",
            "Epoch  19 Batch  457 / 525  Training Loss  0.00010064931120723486\n",
            "Epoch  19 Batch  458 / 525  Training Loss  0.0001505750697106123\n",
            "Epoch  19 Batch  459 / 525  Training Loss  0.00013521307846531272\n",
            "Epoch  19 Batch  460 / 525  Training Loss  0.00011588232882786542\n",
            "Epoch  19 Batch  461 / 525  Training Loss  0.00015533021360170096\n",
            "Epoch  19 Batch  462 / 525  Training Loss  0.0001805456995498389\n",
            "Epoch  19 Batch  463 / 525  Training Loss  0.00010374867997597903\n",
            "Epoch  19 Batch  464 / 525  Training Loss  0.00010657562233973294\n",
            "Epoch  19 Batch  465 / 525  Training Loss  6.705784471705556e-05\n",
            "Epoch  19 Batch  466 / 525  Training Loss  0.00012584278010763228\n",
            "Epoch  19 Batch  467 / 525  Training Loss  0.0001638089888729155\n",
            "Epoch  19 Batch  468 / 525  Training Loss  0.00012700716615654528\n",
            "Epoch  19 Batch  469 / 525  Training Loss  0.00011059296957682818\n",
            "Epoch  19 Batch  470 / 525  Training Loss  0.00010854438733076677\n",
            "Epoch  19 Batch  471 / 525  Training Loss  0.00024343025870621204\n",
            "Epoch  19 Batch  472 / 525  Training Loss  0.00015245404210872948\n",
            "Epoch  19 Batch  473 / 525  Training Loss  0.00012170040281489491\n",
            "Epoch  19 Batch  474 / 525  Training Loss  0.00011139769048895687\n",
            "Epoch  19 Batch  475 / 525  Training Loss  0.0001402331836288795\n",
            "Epoch  19 Batch  476 / 525  Training Loss  0.00013181005488149822\n",
            "Epoch  19 Batch  477 / 525  Training Loss  0.00019431195687502623\n",
            "Epoch  19 Batch  478 / 525  Training Loss  0.00010388832015451044\n",
            "Epoch  19 Batch  479 / 525  Training Loss  0.00010781700257211924\n",
            "Epoch  19 Batch  480 / 525  Training Loss  0.00014037582150194794\n",
            "Epoch  19 Batch  481 / 525  Training Loss  0.00013317861885298043\n",
            "Epoch  19 Batch  482 / 525  Training Loss  0.00012727083230856806\n",
            "Epoch  19 Batch  483 / 525  Training Loss  7.397339504677802e-05\n",
            "Epoch  19 Batch  484 / 525  Training Loss  0.0005779097555205226\n",
            "Epoch  19 Batch  485 / 525  Training Loss  0.00012681368389166892\n",
            "Epoch  19 Batch  486 / 525  Training Loss  0.0001113072139560245\n",
            "Epoch  19 Batch  487 / 525  Training Loss  0.00015162758063524961\n",
            "Epoch  19 Batch  488 / 525  Training Loss  0.00010147956345463172\n",
            "Epoch  19 Batch  489 / 525  Training Loss  0.00013280459097586572\n",
            "Epoch  19 Batch  490 / 525  Training Loss  0.00012780439283233136\n",
            "Epoch  19 Batch  491 / 525  Training Loss  0.0001446062233299017\n",
            "Epoch  19 Batch  492 / 525  Training Loss  0.00011753847502404824\n",
            "Epoch  19 Batch  493 / 525  Training Loss  0.00010784957703435794\n",
            "Epoch  19 Batch  494 / 525  Training Loss  0.00011374647874617949\n",
            "Epoch  19 Batch  495 / 525  Training Loss  0.00013137017958797514\n",
            "Epoch  19 Batch  496 / 525  Training Loss  0.0001188523237942718\n",
            "Epoch  19 Batch  497 / 525  Training Loss  8.949347102316096e-05\n",
            "Epoch  19 Batch  498 / 525  Training Loss  9.568706445861608e-05\n",
            "Epoch  19 Batch  499 / 525  Training Loss  0.0001611974003026262\n",
            "Epoch  19 Batch  500 / 525  Training Loss  0.00010741666483227164\n",
            "Epoch  19 Batch  501 / 525  Training Loss  0.0001310930383624509\n",
            "Epoch  19 Batch  502 / 525  Training Loss  6.139239121694118e-05\n",
            "Epoch  19 Batch  503 / 525  Training Loss  0.00015065382467582822\n",
            "Epoch  19 Batch  504 / 525  Training Loss  0.00015289821021724492\n",
            "Epoch  19 Batch  505 / 525  Training Loss  0.00019539357163012028\n",
            "Epoch  19 Batch  506 / 525  Training Loss  8.349238487426192e-05\n",
            "Epoch  19 Batch  507 / 525  Training Loss  8.842916577123106e-05\n",
            "Epoch  19 Batch  508 / 525  Training Loss  8.108251495286822e-05\n",
            "Epoch  19 Batch  509 / 525  Training Loss  8.411953604081646e-05\n",
            "Epoch  19 Batch  510 / 525  Training Loss  0.00015599234029650688\n",
            "Epoch  19 Batch  511 / 525  Training Loss  0.0001229993940796703\n",
            "Epoch  19 Batch  512 / 525  Training Loss  0.00014832004671916366\n",
            "Epoch  19 Batch  513 / 525  Training Loss  0.000151786400238052\n",
            "Epoch  19 Batch  514 / 525  Training Loss  0.00013469548139255494\n",
            "Epoch  19 Batch  515 / 525  Training Loss  8.515162335243076e-05\n",
            "Epoch  19 Batch  516 / 525  Training Loss  8.859347144607455e-05\n",
            "Epoch  19 Batch  517 / 525  Training Loss  0.00014370700228028\n",
            "Epoch  19 Batch  518 / 525  Training Loss  0.00010830645624082536\n",
            "Epoch  19 Batch  519 / 525  Training Loss  0.00015050631191115826\n",
            "Epoch  19 Batch  520 / 525  Training Loss  9.805843001231551e-05\n",
            "Epoch  19 Batch  521 / 525  Training Loss  0.00011418886424507946\n",
            "Epoch  19 Batch  522 / 525  Training Loss  0.0001100836307159625\n",
            "Epoch  19 Batch  523 / 525  Training Loss  0.00011333378643030301\n",
            "Epoch  19 Batch  524 / 525  Training Loss  0.00010620192915666848\n",
            "  20    |    -    |   0.000176   |   64.31  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 20\n",
            "Epoch  20 Batch  0 / 525  Training Loss  6.910667434567586e-05\n",
            "Epoch  20 Batch  1 / 525  Training Loss  8.678557787789032e-05\n",
            "Epoch  20 Batch  2 / 525  Training Loss  8.491425251122564e-05\n",
            "Epoch  20 Batch  3 / 525  Training Loss  0.00012031954247504473\n",
            "Epoch  20 Batch  4 / 525  Training Loss  8.537782559869811e-05\n",
            "Epoch  20 Batch  5 / 525  Training Loss  0.00011632694804575294\n",
            "Epoch  20 Batch  6 / 525  Training Loss  9.260461229132488e-05\n",
            "Epoch  20 Batch  7 / 525  Training Loss  5.94143130001612e-05\n",
            "Epoch  20 Batch  8 / 525  Training Loss  0.00010314860992366448\n",
            "Epoch  20 Batch  9 / 525  Training Loss  8.308776159537956e-05\n",
            "Epoch  20 Batch  10 / 525  Training Loss  6.432148802559823e-05\n",
            "Epoch  20 Batch  11 / 525  Training Loss  7.800957973813638e-05\n",
            "Epoch  20 Batch  12 / 525  Training Loss  6.342075357679278e-05\n",
            "Epoch  20 Batch  13 / 525  Training Loss  7.82761926529929e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  20 Batch  14 / 525  Training Loss  8.990483911475167e-05\n",
            "Epoch  20 Batch  15 / 525  Training Loss  8.539814007235691e-05\n",
            "Epoch  20 Batch  16 / 525  Training Loss  8.7683045421727e-05\n",
            "Epoch  20 Batch  17 / 525  Training Loss  7.709757483098656e-05\n",
            "Epoch  20 Batch  18 / 525  Training Loss  8.253895794041455e-05\n",
            "Epoch  20 Batch  19 / 525  Training Loss  5.5516538850497454e-05\n",
            "Epoch  20 Batch  20 / 525  Training Loss  0.00010980371007462963\n",
            "Epoch  20 Batch  21 / 525  Training Loss  8.342946239281446e-05\n",
            "Epoch  20 Batch  22 / 525  Training Loss  0.00010760578152257949\n",
            "Epoch  20 Batch  23 / 525  Training Loss  9.865461470326409e-05\n",
            "Epoch  20 Batch  24 / 525  Training Loss  5.46151350135915e-05\n",
            "Epoch  20 Batch  25 / 525  Training Loss  9.699772635940462e-05\n",
            "Epoch  20 Batch  26 / 525  Training Loss  0.00011004586849594489\n",
            "Epoch  20 Batch  27 / 525  Training Loss  7.684347656322643e-05\n",
            "Epoch  20 Batch  28 / 525  Training Loss  6.706864951411262e-05\n",
            "Epoch  20 Batch  29 / 525  Training Loss  0.00010454668517922983\n",
            "Epoch  20 Batch  30 / 525  Training Loss  0.00010287806071573868\n",
            "Epoch  20 Batch  31 / 525  Training Loss  7.802702748449519e-05\n",
            "Epoch  20 Batch  32 / 525  Training Loss  6.556425796588883e-05\n",
            "Epoch  20 Batch  33 / 525  Training Loss  8.99858059710823e-05\n",
            "Epoch  20 Batch  34 / 525  Training Loss  4.301062654121779e-05\n",
            "Epoch  20 Batch  35 / 525  Training Loss  0.00012461337610147893\n",
            "Epoch  20 Batch  36 / 525  Training Loss  0.00011305078078294173\n",
            "Epoch  20 Batch  37 / 525  Training Loss  0.0001486775727244094\n",
            "Epoch  20 Batch  38 / 525  Training Loss  8.90718147275038e-05\n",
            "Epoch  20 Batch  39 / 525  Training Loss  0.00010274887608829886\n",
            "Epoch  20 Batch  40 / 525  Training Loss  7.529821596108377e-05\n",
            "Epoch  20 Batch  41 / 525  Training Loss  9.814539225772023e-05\n",
            "Epoch  20 Batch  42 / 525  Training Loss  4.838042877963744e-05\n",
            "Epoch  20 Batch  43 / 525  Training Loss  6.445215694839135e-05\n",
            "Epoch  20 Batch  44 / 525  Training Loss  6.491698150057346e-05\n",
            "Epoch  20 Batch  45 / 525  Training Loss  9.584813960827887e-05\n",
            "Epoch  20 Batch  46 / 525  Training Loss  8.357623300980777e-05\n",
            "Epoch  20 Batch  47 / 525  Training Loss  8.450944005744532e-05\n",
            "Epoch  20 Batch  48 / 525  Training Loss  5.2098912419751287e-05\n",
            "Epoch  20 Batch  49 / 525  Training Loss  8.322762732859701e-05\n",
            "Epoch  20 Batch  50 / 525  Training Loss  9.473435784457251e-05\n",
            "Epoch  20 Batch  51 / 525  Training Loss  6.996279262239113e-05\n",
            "Epoch  20 Batch  52 / 525  Training Loss  0.00010213586210738868\n",
            "Epoch  20 Batch  53 / 525  Training Loss  7.196048682089895e-05\n",
            "Epoch  20 Batch  54 / 525  Training Loss  7.097698107827455e-05\n",
            "Epoch  20 Batch  55 / 525  Training Loss  8.547214383725077e-05\n",
            "Epoch  20 Batch  56 / 525  Training Loss  7.017455936875194e-05\n",
            "Epoch  20 Batch  57 / 525  Training Loss  6.731458415742964e-05\n",
            "Epoch  20 Batch  58 / 525  Training Loss  0.00010263535659760237\n",
            "Epoch  20 Batch  59 / 525  Training Loss  9.244243847206235e-05\n",
            "Epoch  20 Batch  60 / 525  Training Loss  9.338824020233005e-05\n",
            "Epoch  20 Batch  61 / 525  Training Loss  8.223773329518735e-05\n",
            "Epoch  20 Batch  62 / 525  Training Loss  6.858997221570462e-05\n",
            "Epoch  20 Batch  63 / 525  Training Loss  0.00010435300646349788\n",
            "Epoch  20 Batch  64 / 525  Training Loss  7.746931805741042e-05\n",
            "Epoch  20 Batch  65 / 525  Training Loss  7.928297418402508e-05\n",
            "Epoch  20 Batch  66 / 525  Training Loss  7.88865436334163e-05\n",
            "Epoch  20 Batch  67 / 525  Training Loss  6.548278906848282e-05\n",
            "Epoch  20 Batch  68 / 525  Training Loss  7.814221316948533e-05\n",
            "Epoch  20 Batch  69 / 525  Training Loss  8.968079782789573e-05\n",
            "Epoch  20 Batch  70 / 525  Training Loss  8.378855272894725e-05\n",
            "Epoch  20 Batch  71 / 525  Training Loss  7.847926463000476e-05\n",
            "Epoch  20 Batch  72 / 525  Training Loss  0.00010421047772979364\n",
            "Epoch  20 Batch  73 / 525  Training Loss  8.285481453640386e-05\n",
            "Epoch  20 Batch  74 / 525  Training Loss  0.00012361520202830434\n",
            "Epoch  20 Batch  75 / 525  Training Loss  9.168427641270682e-05\n",
            "Epoch  20 Batch  76 / 525  Training Loss  8.299777255160734e-05\n",
            "Epoch  20 Batch  77 / 525  Training Loss  8.070062904153019e-05\n",
            "Epoch  20 Batch  78 / 525  Training Loss  8.95739794941619e-05\n",
            "Epoch  20 Batch  79 / 525  Training Loss  8.868454460753128e-05\n",
            "Epoch  20 Batch  80 / 525  Training Loss  9.955019049812108e-05\n",
            "Epoch  20 Batch  81 / 525  Training Loss  7.095743058016524e-05\n",
            "Epoch  20 Batch  82 / 525  Training Loss  8.969010377768427e-05\n",
            "Epoch  20 Batch  83 / 525  Training Loss  8.987323235487565e-05\n",
            "Epoch  20 Batch  84 / 525  Training Loss  8.475272625219077e-05\n",
            "Epoch  20 Batch  85 / 525  Training Loss  0.00010541869414737448\n",
            "Epoch  20 Batch  86 / 525  Training Loss  6.333891360554844e-05\n",
            "Epoch  20 Batch  87 / 525  Training Loss  8.78150895005092e-05\n",
            "Epoch  20 Batch  88 / 525  Training Loss  7.95796950114891e-05\n",
            "Epoch  20 Batch  89 / 525  Training Loss  7.921581709524617e-05\n",
            "Epoch  20 Batch  90 / 525  Training Loss  9.864020103123039e-05\n",
            "Epoch  20 Batch  91 / 525  Training Loss  9.118034358834848e-05\n",
            "Epoch  20 Batch  92 / 525  Training Loss  6.782672426197678e-05\n",
            "Epoch  20 Batch  93 / 525  Training Loss  8.709602116141468e-05\n",
            "Epoch  20 Batch  94 / 525  Training Loss  0.00011790233838837594\n",
            "Epoch  20 Batch  95 / 525  Training Loss  8.354611054528505e-05\n",
            "Epoch  20 Batch  96 / 525  Training Loss  8.17965337773785e-05\n",
            "Epoch  20 Batch  97 / 525  Training Loss  0.00010338553693145514\n",
            "Epoch  20 Batch  98 / 525  Training Loss  7.489820563932881e-05\n",
            "Epoch  20 Batch  99 / 525  Training Loss  7.902313518570736e-05\n",
            "Epoch  20 Batch  100 / 525  Training Loss  8.714818977750838e-05\n",
            "Epoch  20 Batch  101 / 525  Training Loss  5.658706504618749e-05\n",
            "Epoch  20 Batch  102 / 525  Training Loss  7.148696022341028e-05\n",
            "Epoch  20 Batch  103 / 525  Training Loss  9.634351590648293e-05\n",
            "Epoch  20 Batch  104 / 525  Training Loss  6.568980461452156e-05\n",
            "Epoch  20 Batch  105 / 525  Training Loss  6.650436262134463e-05\n",
            "Epoch  20 Batch  106 / 525  Training Loss  6.472564564319327e-05\n",
            "Epoch  20 Batch  107 / 525  Training Loss  0.00011306983651593328\n",
            "Epoch  20 Batch  108 / 525  Training Loss  7.66468292567879e-05\n",
            "Epoch  20 Batch  109 / 525  Training Loss  7.253547664731741e-05\n",
            "Epoch  20 Batch  110 / 525  Training Loss  7.476485188817605e-05\n",
            "Epoch  20 Batch  111 / 525  Training Loss  8.901667024474591e-05\n",
            "Epoch  20 Batch  112 / 525  Training Loss  0.00011055544018745422\n",
            "Epoch  20 Batch  113 / 525  Training Loss  5.907900776946917e-05\n",
            "Epoch  20 Batch  114 / 525  Training Loss  7.563853432657197e-05\n",
            "Epoch  20 Batch  115 / 525  Training Loss  7.425125659210607e-05\n",
            "Epoch  20 Batch  116 / 525  Training Loss  0.00012969545787200332\n",
            "Epoch  20 Batch  117 / 525  Training Loss  0.00011993921361863613\n",
            "Epoch  20 Batch  118 / 525  Training Loss  8.7306609202642e-05\n",
            "Epoch  20 Batch  119 / 525  Training Loss  6.43044913886115e-05\n",
            "Epoch  20 Batch  120 / 525  Training Loss  8.703376079211012e-05\n",
            "Epoch  20 Batch  121 / 525  Training Loss  5.97998259763699e-05\n",
            "Epoch  20 Batch  122 / 525  Training Loss  6.593290891032666e-05\n",
            "Epoch  20 Batch  123 / 525  Training Loss  7.9217272286769e-05\n",
            "Epoch  20 Batch  124 / 525  Training Loss  9.035743278218433e-05\n",
            "Epoch  20 Batch  125 / 525  Training Loss  3.879316500388086e-05\n",
            "Epoch  20 Batch  126 / 525  Training Loss  8.099056140054017e-05\n",
            "Epoch  20 Batch  127 / 525  Training Loss  8.87159476405941e-05\n",
            "Epoch  20 Batch  128 / 525  Training Loss  6.768554158043116e-05\n",
            "Epoch  20 Batch  129 / 525  Training Loss  8.990483183879405e-05\n",
            "Epoch  20 Batch  130 / 525  Training Loss  7.958065543789417e-05\n",
            "Epoch  20 Batch  131 / 525  Training Loss  6.934399425517768e-05\n",
            "Epoch  20 Batch  132 / 525  Training Loss  8.954224904300645e-05\n",
            "Epoch  20 Batch  133 / 525  Training Loss  3.8192723877727985e-05\n",
            "Epoch  20 Batch  134 / 525  Training Loss  6.90976157784462e-05\n",
            "Epoch  20 Batch  135 / 525  Training Loss  8.421622624155134e-05\n",
            "Epoch  20 Batch  136 / 525  Training Loss  6.761171971447766e-05\n",
            "Epoch  20 Batch  137 / 525  Training Loss  7.767872011754662e-05\n",
            "Epoch  20 Batch  138 / 525  Training Loss  0.0002868281735572964\n",
            "Epoch  20 Batch  139 / 525  Training Loss  5.149565913598053e-05\n",
            "Epoch  20 Batch  140 / 525  Training Loss  0.00011545112647581846\n",
            "Epoch  20 Batch  141 / 525  Training Loss  9.520268940832466e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  20 Batch  142 / 525  Training Loss  5.5505341151729226e-05\n",
            "Epoch  20 Batch  143 / 525  Training Loss  6.38740457361564e-05\n",
            "Epoch  20 Batch  144 / 525  Training Loss  8.970346243586391e-05\n",
            "Epoch  20 Batch  145 / 525  Training Loss  7.646321319043636e-05\n",
            "Epoch  20 Batch  146 / 525  Training Loss  9.823592699831352e-05\n",
            "Epoch  20 Batch  147 / 525  Training Loss  8.225727651733905e-05\n",
            "Epoch  20 Batch  148 / 525  Training Loss  8.813856402412057e-05\n",
            "Epoch  20 Batch  149 / 525  Training Loss  6.787099118810147e-05\n",
            "Epoch  20 Batch  150 / 525  Training Loss  0.00010536918125580996\n",
            "Epoch  20 Batch  151 / 525  Training Loss  7.586359424749389e-05\n",
            "Epoch  20 Batch  152 / 525  Training Loss  8.394019096158445e-05\n",
            "Epoch  20 Batch  153 / 525  Training Loss  4.546541458694264e-05\n",
            "Epoch  20 Batch  154 / 525  Training Loss  8.525488374289125e-05\n",
            "Epoch  20 Batch  155 / 525  Training Loss  8.737472671782598e-05\n",
            "Epoch  20 Batch  156 / 525  Training Loss  0.0001008242616080679\n",
            "Epoch  20 Batch  157 / 525  Training Loss  0.0001129714073613286\n",
            "Epoch  20 Batch  158 / 525  Training Loss  7.594477938255295e-05\n",
            "Epoch  20 Batch  159 / 525  Training Loss  0.00011699584138114005\n",
            "Epoch  20 Batch  160 / 525  Training Loss  4.608613016898744e-05\n",
            "Epoch  20 Batch  161 / 525  Training Loss  7.087260746629909e-05\n",
            "Epoch  20 Batch  162 / 525  Training Loss  0.0001051696963259019\n",
            "Epoch  20 Batch  163 / 525  Training Loss  0.00011429970618337393\n",
            "Epoch  20 Batch  164 / 525  Training Loss  9.055554983206093e-05\n",
            "Epoch  20 Batch  165 / 525  Training Loss  7.001832273090258e-05\n",
            "Epoch  20 Batch  166 / 525  Training Loss  8.096532837953418e-05\n",
            "Epoch  20 Batch  167 / 525  Training Loss  7.591136818518862e-05\n",
            "Epoch  20 Batch  168 / 525  Training Loss  0.00011139283742522821\n",
            "Epoch  20 Batch  169 / 525  Training Loss  6.609745469177142e-05\n",
            "Epoch  20 Batch  170 / 525  Training Loss  7.242621359182522e-05\n",
            "Epoch  20 Batch  171 / 525  Training Loss  0.00010288246267009526\n",
            "Epoch  20 Batch  172 / 525  Training Loss  0.00010350602678954601\n",
            "Epoch  20 Batch  173 / 525  Training Loss  4.824185816687532e-05\n",
            "Epoch  20 Batch  174 / 525  Training Loss  9.09527443582192e-05\n",
            "Epoch  20 Batch  175 / 525  Training Loss  8.3638631622307e-05\n",
            "Epoch  20 Batch  176 / 525  Training Loss  0.00010426576773170382\n",
            "Epoch  20 Batch  177 / 525  Training Loss  8.766509563429281e-05\n",
            "Epoch  20 Batch  178 / 525  Training Loss  9.783986024558544e-05\n",
            "Epoch  20 Batch  179 / 525  Training Loss  7.379871385637671e-05\n",
            "Epoch  20 Batch  180 / 525  Training Loss  5.7550154451746494e-05\n",
            "Epoch  20 Batch  181 / 525  Training Loss  8.073760545812547e-05\n",
            "Epoch  20 Batch  182 / 525  Training Loss  7.220657425932586e-05\n",
            "Epoch  20 Batch  183 / 525  Training Loss  0.00010890742123592645\n",
            "Epoch  20 Batch  184 / 525  Training Loss  7.953964814078063e-05\n",
            "Epoch  20 Batch  185 / 525  Training Loss  6.321224645944312e-05\n",
            "Epoch  20 Batch  186 / 525  Training Loss  4.203453499940224e-05\n",
            "Epoch  20 Batch  187 / 525  Training Loss  7.406315125990659e-05\n",
            "Epoch  20 Batch  188 / 525  Training Loss  6.997965829214081e-05\n",
            "Epoch  20 Batch  189 / 525  Training Loss  9.89800100796856e-05\n",
            "Epoch  20 Batch  190 / 525  Training Loss  0.0001075679756468162\n",
            "Epoch  20 Batch  191 / 525  Training Loss  8.373438322450966e-05\n",
            "Epoch  20 Batch  192 / 525  Training Loss  8.683387568453327e-05\n",
            "Epoch  20 Batch  193 / 525  Training Loss  7.941488001961261e-05\n",
            "Epoch  20 Batch  194 / 525  Training Loss  0.00010193798516411334\n",
            "Epoch  20 Batch  195 / 525  Training Loss  5.567947664530948e-05\n",
            "Epoch  20 Batch  196 / 525  Training Loss  7.146240386646241e-05\n",
            "Epoch  20 Batch  197 / 525  Training Loss  9.347620652988553e-05\n",
            "Epoch  20 Batch  198 / 525  Training Loss  7.240129343699664e-05\n",
            "Epoch  20 Batch  199 / 525  Training Loss  7.518463826272637e-05\n",
            "Epoch  20 Batch  200 / 525  Training Loss  0.00011585924949031323\n",
            "Epoch  20 Batch  201 / 525  Training Loss  8.833401079755276e-05\n",
            "Epoch  20 Batch  202 / 525  Training Loss  5.367792982724495e-05\n",
            "Epoch  20 Batch  203 / 525  Training Loss  0.00010259762348141521\n",
            "Epoch  20 Batch  204 / 525  Training Loss  0.0001190841430798173\n",
            "Epoch  20 Batch  205 / 525  Training Loss  6.627290713367984e-05\n",
            "Epoch  20 Batch  206 / 525  Training Loss  8.390587026951835e-05\n",
            "Epoch  20 Batch  207 / 525  Training Loss  7.950612052809447e-05\n",
            "Epoch  20 Batch  208 / 525  Training Loss  8.686914952704683e-05\n",
            "Epoch  20 Batch  209 / 525  Training Loss  8.304328366648406e-05\n",
            "Epoch  20 Batch  210 / 525  Training Loss  4.816601358470507e-05\n",
            "Epoch  20 Batch  211 / 525  Training Loss  8.001044625416398e-05\n",
            "Epoch  20 Batch  212 / 525  Training Loss  7.469119009329006e-05\n",
            "Epoch  20 Batch  213 / 525  Training Loss  9.479688014835119e-05\n",
            "Epoch  20 Batch  214 / 525  Training Loss  7.515781180700287e-05\n",
            "Epoch  20 Batch  215 / 525  Training Loss  8.373335003852844e-05\n",
            "Epoch  20 Batch  216 / 525  Training Loss  6.666398257948458e-05\n",
            "Epoch  20 Batch  217 / 525  Training Loss  8.105778397293761e-05\n",
            "Epoch  20 Batch  218 / 525  Training Loss  6.57525597489439e-05\n",
            "Epoch  20 Batch  219 / 525  Training Loss  6.397765537258238e-05\n",
            "Epoch  20 Batch  220 / 525  Training Loss  9.092253458220512e-05\n",
            "Epoch  20 Batch  221 / 525  Training Loss  0.00010841320909094065\n",
            "Epoch  20 Batch  222 / 525  Training Loss  5.8291374443797395e-05\n",
            "Epoch  20 Batch  223 / 525  Training Loss  7.041842764010653e-05\n",
            "Epoch  20 Batch  224 / 525  Training Loss  5.549216439248994e-05\n",
            "Epoch  20 Batch  225 / 525  Training Loss  0.00010839493188541383\n",
            "Epoch  20 Batch  226 / 525  Training Loss  7.677549729123712e-05\n",
            "Epoch  20 Batch  227 / 525  Training Loss  7.442010246450081e-05\n",
            "Epoch  20 Batch  228 / 525  Training Loss  7.51586485421285e-05\n",
            "Epoch  20 Batch  229 / 525  Training Loss  8.479478856315836e-05\n",
            "Epoch  20 Batch  230 / 525  Training Loss  7.44514400139451e-05\n",
            "Epoch  20 Batch  231 / 525  Training Loss  5.446669092634693e-05\n",
            "Epoch  20 Batch  232 / 525  Training Loss  8.458049705950543e-05\n",
            "Epoch  20 Batch  233 / 525  Training Loss  5.400802911026403e-05\n",
            "Epoch  20 Batch  234 / 525  Training Loss  0.00011144727614009753\n",
            "Epoch  20 Batch  235 / 525  Training Loss  8.462808182230219e-05\n",
            "Epoch  20 Batch  236 / 525  Training Loss  6.117076554801315e-05\n",
            "Epoch  20 Batch  237 / 525  Training Loss  6.85524646542035e-05\n",
            "Epoch  20 Batch  238 / 525  Training Loss  5.878994124941528e-05\n",
            "Epoch  20 Batch  239 / 525  Training Loss  9.64113642112352e-05\n",
            "Epoch  20 Batch  240 / 525  Training Loss  5.323526420397684e-05\n",
            "Epoch  20 Batch  241 / 525  Training Loss  6.964729982428253e-05\n",
            "Epoch  20 Batch  242 / 525  Training Loss  5.746924944105558e-05\n",
            "Epoch  20 Batch  243 / 525  Training Loss  6.656813638983294e-05\n",
            "Epoch  20 Batch  244 / 525  Training Loss  0.00010685126471798867\n",
            "Epoch  20 Batch  245 / 525  Training Loss  0.00011385176185285673\n",
            "Epoch  20 Batch  246 / 525  Training Loss  9.208968549501151e-05\n",
            "Epoch  20 Batch  247 / 525  Training Loss  7.865014777053148e-05\n",
            "Epoch  20 Batch  248 / 525  Training Loss  9.908913489198312e-05\n",
            "Epoch  20 Batch  249 / 525  Training Loss  0.00010000958718592301\n",
            "Epoch  20 Batch  250 / 525  Training Loss  4.1101182432612404e-05\n",
            "Epoch  20 Batch  251 / 525  Training Loss  8.890961180441082e-05\n",
            "Epoch  20 Batch  252 / 525  Training Loss  6.070623567211442e-05\n",
            "Epoch  20 Batch  253 / 525  Training Loss  7.395403372356668e-05\n",
            "Epoch  20 Batch  254 / 525  Training Loss  6.658115307800472e-05\n",
            "Epoch  20 Batch  255 / 525  Training Loss  8.835474727675319e-05\n",
            "Epoch  20 Batch  256 / 525  Training Loss  9.0928835561499e-05\n",
            "Epoch  20 Batch  257 / 525  Training Loss  5.16204527230002e-05\n",
            "Epoch  20 Batch  258 / 525  Training Loss  7.530399307142943e-05\n",
            "Epoch  20 Batch  259 / 525  Training Loss  0.0001224914303747937\n",
            "Epoch  20 Batch  260 / 525  Training Loss  7.84165458753705e-05\n",
            "Epoch  20 Batch  261 / 525  Training Loss  8.075215737335384e-05\n",
            "Epoch  20 Batch  262 / 525  Training Loss  5.3327297791838646e-05\n",
            "Epoch  20 Batch  263 / 525  Training Loss  6.518708687508479e-05\n",
            "Epoch  20 Batch  264 / 525  Training Loss  6.934049451956525e-05\n",
            "Epoch  20 Batch  265 / 525  Training Loss  0.00011149766214657575\n",
            "Epoch  20 Batch  266 / 525  Training Loss  7.834635471226647e-05\n",
            "Epoch  20 Batch  267 / 525  Training Loss  0.00010819599265232682\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  20 Batch  268 / 525  Training Loss  9.575676813255996e-05\n",
            "Epoch  20 Batch  269 / 525  Training Loss  9.922308527166024e-05\n",
            "Epoch  20 Batch  270 / 525  Training Loss  9.410969505552202e-05\n",
            "Epoch  20 Batch  271 / 525  Training Loss  7.138255750760436e-05\n",
            "Epoch  20 Batch  272 / 525  Training Loss  7.487966649932787e-05\n",
            "Epoch  20 Batch  273 / 525  Training Loss  8.17499021650292e-05\n",
            "Epoch  20 Batch  274 / 525  Training Loss  9.208032861351967e-05\n",
            "Epoch  20 Batch  275 / 525  Training Loss  9.38125085667707e-05\n",
            "Epoch  20 Batch  276 / 525  Training Loss  8.39314452605322e-05\n",
            "Epoch  20 Batch  277 / 525  Training Loss  0.0001395369617966935\n",
            "Epoch  20 Batch  278 / 525  Training Loss  4.834956052945927e-05\n",
            "Epoch  20 Batch  279 / 525  Training Loss  7.303160236915573e-05\n",
            "Epoch  20 Batch  280 / 525  Training Loss  8.323070505866781e-05\n",
            "Epoch  20 Batch  281 / 525  Training Loss  6.526429206132889e-05\n",
            "Epoch  20 Batch  282 / 525  Training Loss  0.00010325301991542801\n",
            "Epoch  20 Batch  283 / 525  Training Loss  5.511508061317727e-05\n",
            "Epoch  20 Batch  284 / 525  Training Loss  6.479995499830693e-05\n",
            "Epoch  20 Batch  285 / 525  Training Loss  6.36208351352252e-05\n",
            "Epoch  20 Batch  286 / 525  Training Loss  3.6451470805332065e-05\n",
            "Epoch  20 Batch  287 / 525  Training Loss  8.083022839855403e-05\n",
            "Epoch  20 Batch  288 / 525  Training Loss  8.461455581709743e-05\n",
            "Epoch  20 Batch  289 / 525  Training Loss  7.788319635437801e-05\n",
            "Epoch  20 Batch  290 / 525  Training Loss  6.262760143727064e-05\n",
            "Epoch  20 Batch  291 / 525  Training Loss  4.487136175157502e-05\n",
            "Epoch  20 Batch  292 / 525  Training Loss  5.992736987536773e-05\n",
            "Epoch  20 Batch  293 / 525  Training Loss  8.991479262476787e-05\n",
            "Epoch  20 Batch  294 / 525  Training Loss  8.999954297905788e-05\n",
            "Epoch  20 Batch  295 / 525  Training Loss  9.910910011967644e-05\n",
            "Epoch  20 Batch  296 / 525  Training Loss  9.294828487327322e-05\n",
            "Epoch  20 Batch  297 / 525  Training Loss  8.205875928979367e-05\n",
            "Epoch  20 Batch  298 / 525  Training Loss  4.65520606667269e-05\n",
            "Epoch  20 Batch  299 / 525  Training Loss  8.321368659380823e-05\n",
            "Epoch  20 Batch  300 / 525  Training Loss  5.4690404795110226e-05\n",
            "Epoch  20 Batch  301 / 525  Training Loss  5.9845555369975045e-05\n",
            "Epoch  20 Batch  302 / 525  Training Loss  6.0740549088222906e-05\n",
            "Epoch  20 Batch  303 / 525  Training Loss  5.069822509540245e-05\n",
            "Epoch  20 Batch  304 / 525  Training Loss  7.807702058926225e-05\n",
            "Epoch  20 Batch  305 / 525  Training Loss  8.087686728686094e-05\n",
            "Epoch  20 Batch  306 / 525  Training Loss  6.317096995189786e-05\n",
            "Epoch  20 Batch  307 / 525  Training Loss  7.95506639406085e-05\n",
            "Epoch  20 Batch  308 / 525  Training Loss  9.712581959320232e-05\n",
            "Epoch  20 Batch  309 / 525  Training Loss  6.359798135235906e-05\n",
            "Epoch  20 Batch  310 / 525  Training Loss  0.00012763831182383\n",
            "Epoch  20 Batch  311 / 525  Training Loss  8.90015799086541e-05\n",
            "Epoch  20 Batch  312 / 525  Training Loss  9.006141044665128e-05\n",
            "Epoch  20 Batch  313 / 525  Training Loss  8.674655691720545e-05\n",
            "Epoch  20 Batch  314 / 525  Training Loss  7.99019107944332e-05\n",
            "Epoch  20 Batch  315 / 525  Training Loss  8.482169505441561e-05\n",
            "Epoch  20 Batch  316 / 525  Training Loss  5.01067734148819e-05\n",
            "Epoch  20 Batch  317 / 525  Training Loss  6.890638178447261e-05\n",
            "Epoch  20 Batch  318 / 525  Training Loss  6.32298833806999e-05\n",
            "Epoch  20 Batch  319 / 525  Training Loss  5.379594585974701e-05\n",
            "Epoch  20 Batch  320 / 525  Training Loss  8.618536230642349e-05\n",
            "Epoch  20 Batch  321 / 525  Training Loss  8.345945389010012e-05\n",
            "Epoch  20 Batch  322 / 525  Training Loss  6.884991307742894e-05\n",
            "Epoch  20 Batch  323 / 525  Training Loss  7.265769818332046e-05\n",
            "Epoch  20 Batch  324 / 525  Training Loss  6.381121056620032e-05\n",
            "Epoch  20 Batch  325 / 525  Training Loss  6.94548652973026e-05\n",
            "Epoch  20 Batch  326 / 525  Training Loss  5.3881049097981304e-05\n",
            "Epoch  20 Batch  327 / 525  Training Loss  6.708034925395623e-05\n",
            "Epoch  20 Batch  328 / 525  Training Loss  8.032592450035736e-05\n",
            "Epoch  20 Batch  329 / 525  Training Loss  6.847720214864239e-05\n",
            "Epoch  20 Batch  330 / 525  Training Loss  6.572261918336153e-05\n",
            "Epoch  20 Batch  331 / 525  Training Loss  0.00010769969958346337\n",
            "Epoch  20 Batch  332 / 525  Training Loss  0.00011208141222596169\n",
            "Epoch  20 Batch  333 / 525  Training Loss  8.159074786817655e-05\n",
            "Epoch  20 Batch  334 / 525  Training Loss  5.500437691807747e-05\n",
            "Epoch  20 Batch  335 / 525  Training Loss  9.510782547295094e-05\n",
            "Epoch  20 Batch  336 / 525  Training Loss  7.621952681802213e-05\n",
            "Epoch  20 Batch  337 / 525  Training Loss  0.0001003834986477159\n",
            "Epoch  20 Batch  338 / 525  Training Loss  5.5752509069861844e-05\n",
            "Epoch  20 Batch  339 / 525  Training Loss  9.685273107606918e-05\n",
            "Epoch  20 Batch  340 / 525  Training Loss  9.311713802162558e-05\n",
            "Epoch  20 Batch  341 / 525  Training Loss  8.124576561385766e-05\n",
            "Epoch  20 Batch  342 / 525  Training Loss  9.135711297858506e-05\n",
            "Epoch  20 Batch  343 / 525  Training Loss  6.123262573964894e-05\n",
            "Epoch  20 Batch  344 / 525  Training Loss  6.148560350993648e-05\n",
            "Epoch  20 Batch  345 / 525  Training Loss  9.7850919701159e-05\n",
            "Epoch  20 Batch  346 / 525  Training Loss  0.00010050847049569711\n",
            "Epoch  20 Batch  347 / 525  Training Loss  8.455383067484945e-05\n",
            "Epoch  20 Batch  348 / 525  Training Loss  6.155735172796994e-05\n",
            "Epoch  20 Batch  349 / 525  Training Loss  8.61162188812159e-05\n",
            "Epoch  20 Batch  350 / 525  Training Loss  7.1705762820784e-05\n",
            "Epoch  20 Batch  351 / 525  Training Loss  7.647619349882007e-05\n",
            "Epoch  20 Batch  352 / 525  Training Loss  6.924286572029814e-05\n",
            "Epoch  20 Batch  353 / 525  Training Loss  9.417253022547811e-05\n",
            "Epoch  20 Batch  354 / 525  Training Loss  8.131926733767614e-05\n",
            "Epoch  20 Batch  355 / 525  Training Loss  8.44855749164708e-05\n",
            "Epoch  20 Batch  356 / 525  Training Loss  9.523231710772961e-05\n",
            "Epoch  20 Batch  357 / 525  Training Loss  6.676773045910522e-05\n",
            "Epoch  20 Batch  358 / 525  Training Loss  7.821329199941829e-05\n",
            "Epoch  20 Batch  359 / 525  Training Loss  7.707799522904679e-05\n",
            "Epoch  20 Batch  360 / 525  Training Loss  6.080151069909334e-05\n",
            "Epoch  20 Batch  361 / 525  Training Loss  6.638515333179384e-05\n",
            "Epoch  20 Batch  362 / 525  Training Loss  0.00010613072663545609\n",
            "Epoch  20 Batch  363 / 525  Training Loss  5.4555985116166994e-05\n",
            "Epoch  20 Batch  364 / 525  Training Loss  7.876709423726425e-05\n",
            "Epoch  20 Batch  365 / 525  Training Loss  5.359523856895976e-05\n",
            "Epoch  20 Batch  366 / 525  Training Loss  7.915423338999972e-05\n",
            "Epoch  20 Batch  367 / 525  Training Loss  9.172767022391781e-05\n",
            "Epoch  20 Batch  368 / 525  Training Loss  4.55845074611716e-05\n",
            "Epoch  20 Batch  369 / 525  Training Loss  6.940612365724519e-05\n",
            "Epoch  20 Batch  370 / 525  Training Loss  0.00012013416562695056\n",
            "Epoch  20 Batch  371 / 525  Training Loss  6.116439180914313e-05\n",
            "Epoch  20 Batch  372 / 525  Training Loss  7.449655095115304e-05\n",
            "Epoch  20 Batch  373 / 525  Training Loss  7.379506132565439e-05\n",
            "Epoch  20 Batch  374 / 525  Training Loss  7.522783562308177e-05\n",
            "Epoch  20 Batch  375 / 525  Training Loss  6.818656402174383e-05\n",
            "Epoch  20 Batch  376 / 525  Training Loss  5.991625221213326e-05\n",
            "Epoch  20 Batch  377 / 525  Training Loss  7.655866647837684e-05\n",
            "Epoch  20 Batch  378 / 525  Training Loss  4.685847306973301e-05\n",
            "Epoch  20 Batch  379 / 525  Training Loss  6.510623643407598e-05\n",
            "Epoch  20 Batch  380 / 525  Training Loss  5.906547448830679e-05\n",
            "Epoch  20 Batch  381 / 525  Training Loss  0.007748320698738098\n",
            "Epoch  20 Batch  382 / 525  Training Loss  0.00013849334209226072\n",
            "Epoch  20 Batch  383 / 525  Training Loss  8.623396570328623e-05\n",
            "Epoch  20 Batch  384 / 525  Training Loss  0.00010702080180635676\n",
            "Epoch  20 Batch  385 / 525  Training Loss  0.00010125298285856843\n",
            "Epoch  20 Batch  386 / 525  Training Loss  4.4453008740674704e-05\n",
            "Epoch  20 Batch  387 / 525  Training Loss  7.18117444193922e-05\n",
            "Epoch  20 Batch  388 / 525  Training Loss  7.148590520955622e-05\n",
            "Epoch  20 Batch  389 / 525  Training Loss  7.455914601450786e-05\n",
            "Epoch  20 Batch  390 / 525  Training Loss  9.09709488041699e-05\n",
            "Epoch  20 Batch  391 / 525  Training Loss  5.094858352094889e-05\n",
            "Epoch  20 Batch  392 / 525  Training Loss  0.00012601411435753107\n",
            "Epoch  20 Batch  393 / 525  Training Loss  7.603359699714929e-05\n",
            "Epoch  20 Batch  394 / 525  Training Loss  7.366336649283767e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  20 Batch  395 / 525  Training Loss  0.000103899379610084\n",
            "Epoch  20 Batch  396 / 525  Training Loss  7.337381975958124e-05\n",
            "Epoch  20 Batch  397 / 525  Training Loss  7.61981718824245e-05\n",
            "Epoch  20 Batch  398 / 525  Training Loss  6.750023021595553e-05\n",
            "Epoch  20 Batch  399 / 525  Training Loss  9.652510925661772e-05\n",
            "Epoch  20 Batch  400 / 525  Training Loss  7.795036071911454e-05\n",
            "Epoch  20 Batch  401 / 525  Training Loss  0.0001000429256237112\n",
            "Epoch  20 Batch  402 / 525  Training Loss  7.082770753186196e-05\n",
            "Epoch  20 Batch  403 / 525  Training Loss  6.675654731225222e-05\n",
            "Epoch  20 Batch  404 / 525  Training Loss  8.881004032446072e-05\n",
            "Epoch  20 Batch  405 / 525  Training Loss  7.408477540593594e-05\n",
            "Epoch  20 Batch  406 / 525  Training Loss  7.658114918740466e-05\n",
            "Epoch  20 Batch  407 / 525  Training Loss  9.108475933317095e-05\n",
            "Epoch  20 Batch  408 / 525  Training Loss  9.308368316851556e-05\n",
            "Epoch  20 Batch  409 / 525  Training Loss  8.048255403991789e-05\n",
            "Epoch  20 Batch  410 / 525  Training Loss  0.0001817198353819549\n",
            "Epoch  20 Batch  411 / 525  Training Loss  0.0001105193659896031\n",
            "Epoch  20 Batch  412 / 525  Training Loss  7.304043538169935e-05\n",
            "Epoch  20 Batch  413 / 525  Training Loss  0.00014845727127976716\n",
            "Epoch  20 Batch  414 / 525  Training Loss  8.04259252618067e-05\n",
            "Epoch  20 Batch  415 / 525  Training Loss  8.116634853649884e-05\n",
            "Epoch  20 Batch  416 / 525  Training Loss  0.00010213449422735721\n",
            "Epoch  20 Batch  417 / 525  Training Loss  6.768611638108268e-05\n",
            "Epoch  20 Batch  418 / 525  Training Loss  7.628312596352771e-05\n",
            "Epoch  20 Batch  419 / 525  Training Loss  6.858039705548435e-05\n",
            "Epoch  20 Batch  420 / 525  Training Loss  7.8366938396357e-05\n",
            "Epoch  20 Batch  421 / 525  Training Loss  8.552928920835257e-05\n",
            "Epoch  20 Batch  422 / 525  Training Loss  5.5118387535912916e-05\n",
            "Epoch  20 Batch  423 / 525  Training Loss  0.00010400209430372342\n",
            "Epoch  20 Batch  424 / 525  Training Loss  5.179536674404517e-05\n",
            "Epoch  20 Batch  425 / 525  Training Loss  9.416028478881344e-05\n",
            "Epoch  20 Batch  426 / 525  Training Loss  7.961611117934808e-05\n",
            "Epoch  20 Batch  427 / 525  Training Loss  6.225256947800517e-05\n",
            "Epoch  20 Batch  428 / 525  Training Loss  0.00017173434025608003\n",
            "Epoch  20 Batch  429 / 525  Training Loss  7.266979810083285e-05\n",
            "Epoch  20 Batch  430 / 525  Training Loss  8.333720325026661e-05\n",
            "Epoch  20 Batch  431 / 525  Training Loss  0.00011291252303635702\n",
            "Epoch  20 Batch  432 / 525  Training Loss  7.373098924290389e-05\n",
            "Epoch  20 Batch  433 / 525  Training Loss  7.507122063543648e-05\n",
            "Epoch  20 Batch  434 / 525  Training Loss  6.64537146803923e-05\n",
            "Epoch  20 Batch  435 / 525  Training Loss  6.126680091256276e-05\n",
            "Epoch  20 Batch  436 / 525  Training Loss  8.829960279399529e-05\n",
            "Epoch  20 Batch  437 / 525  Training Loss  0.00010429233952891082\n",
            "Epoch  20 Batch  438 / 525  Training Loss  7.263202132890001e-05\n",
            "Epoch  20 Batch  439 / 525  Training Loss  9.747971489559859e-05\n",
            "Epoch  20 Batch  440 / 525  Training Loss  8.328018157044426e-05\n",
            "Epoch  20 Batch  441 / 525  Training Loss  5.916438021813519e-05\n",
            "Epoch  20 Batch  442 / 525  Training Loss  4.696536052506417e-05\n",
            "Epoch  20 Batch  443 / 525  Training Loss  6.250505975913256e-05\n",
            "Epoch  20 Batch  444 / 525  Training Loss  6.386549648595974e-05\n",
            "Epoch  20 Batch  445 / 525  Training Loss  9.344911813968793e-05\n",
            "Epoch  20 Batch  446 / 525  Training Loss  8.009922748897225e-05\n",
            "Epoch  20 Batch  447 / 525  Training Loss  7.613417255925015e-05\n",
            "Epoch  20 Batch  448 / 525  Training Loss  8.97574209375307e-05\n",
            "Epoch  20 Batch  449 / 525  Training Loss  4.7736997657921165e-05\n",
            "Epoch  20 Batch  450 / 525  Training Loss  8.12973448773846e-05\n",
            "Epoch  20 Batch  451 / 525  Training Loss  0.00010330104123568162\n",
            "Epoch  20 Batch  452 / 525  Training Loss  7.983456453075632e-05\n",
            "Epoch  20 Batch  453 / 525  Training Loss  6.985965592321008e-05\n",
            "Epoch  20 Batch  454 / 525  Training Loss  6.974278221605346e-05\n",
            "Epoch  20 Batch  455 / 525  Training Loss  4.069449641974643e-05\n",
            "Epoch  20 Batch  456 / 525  Training Loss  6.820145063102245e-05\n",
            "Epoch  20 Batch  457 / 525  Training Loss  3.90219283872284e-05\n",
            "Epoch  20 Batch  458 / 525  Training Loss  5.695232539437711e-05\n",
            "Epoch  20 Batch  459 / 525  Training Loss  7.992228347575292e-05\n",
            "Epoch  20 Batch  460 / 525  Training Loss  0.00010705980093916878\n",
            "Epoch  20 Batch  461 / 525  Training Loss  7.732400263193995e-05\n",
            "Epoch  20 Batch  462 / 525  Training Loss  6.626959657296538e-05\n",
            "Epoch  20 Batch  463 / 525  Training Loss  4.72360807179939e-05\n",
            "Epoch  20 Batch  464 / 525  Training Loss  7.516445475630462e-05\n",
            "Epoch  20 Batch  465 / 525  Training Loss  7.070910214679316e-05\n",
            "Epoch  20 Batch  466 / 525  Training Loss  7.886247476562858e-05\n",
            "Epoch  20 Batch  467 / 525  Training Loss  7.978161738719791e-05\n",
            "Epoch  20 Batch  468 / 525  Training Loss  8.061156404437497e-05\n",
            "Epoch  20 Batch  469 / 525  Training Loss  7.44563149055466e-05\n",
            "Epoch  20 Batch  470 / 525  Training Loss  5.155274993740022e-05\n",
            "Epoch  20 Batch  471 / 525  Training Loss  0.0001003037323243916\n",
            "Epoch  20 Batch  472 / 525  Training Loss  8.829204307403415e-05\n",
            "Epoch  20 Batch  473 / 525  Training Loss  9.326169674750417e-05\n",
            "Epoch  20 Batch  474 / 525  Training Loss  0.00010545422264840454\n",
            "Epoch  20 Batch  475 / 525  Training Loss  8.970800263341516e-05\n",
            "Epoch  20 Batch  476 / 525  Training Loss  0.00010535332694416866\n",
            "Epoch  20 Batch  477 / 525  Training Loss  8.642290049465373e-05\n",
            "Epoch  20 Batch  478 / 525  Training Loss  0.00011152684601256624\n",
            "Epoch  20 Batch  479 / 525  Training Loss  0.00011232085671508685\n",
            "Epoch  20 Batch  480 / 525  Training Loss  7.315429684240371e-05\n",
            "Epoch  20 Batch  481 / 525  Training Loss  9.17103752726689e-05\n",
            "Epoch  20 Batch  482 / 525  Training Loss  8.009896555449814e-05\n",
            "Epoch  20 Batch  483 / 525  Training Loss  8.080322004389018e-05\n",
            "Epoch  20 Batch  484 / 525  Training Loss  6.120369653217494e-05\n",
            "Epoch  20 Batch  485 / 525  Training Loss  7.108569116098806e-05\n",
            "Epoch  20 Batch  486 / 525  Training Loss  7.605164137203246e-05\n",
            "Epoch  20 Batch  487 / 525  Training Loss  7.824518252164125e-05\n",
            "Epoch  20 Batch  488 / 525  Training Loss  6.31610382697545e-05\n",
            "Epoch  20 Batch  489 / 525  Training Loss  8.114746015053242e-05\n",
            "Epoch  20 Batch  490 / 525  Training Loss  0.0002811041777022183\n",
            "Epoch  20 Batch  491 / 525  Training Loss  9.985597716877237e-05\n",
            "Epoch  20 Batch  492 / 525  Training Loss  8.107508620014414e-05\n",
            "Epoch  20 Batch  493 / 525  Training Loss  6.894046964589506e-05\n",
            "Epoch  20 Batch  494 / 525  Training Loss  5.410376616055146e-05\n",
            "Epoch  20 Batch  495 / 525  Training Loss  7.030634151306003e-05\n",
            "Epoch  20 Batch  496 / 525  Training Loss  0.0001082424059859477\n",
            "Epoch  20 Batch  497 / 525  Training Loss  0.0001554476039018482\n",
            "Epoch  20 Batch  498 / 525  Training Loss  0.00010321821173420176\n",
            "Epoch  20 Batch  499 / 525  Training Loss  6.778065289836377e-05\n",
            "Epoch  20 Batch  500 / 525  Training Loss  6.409713387256488e-05\n",
            "Epoch  20 Batch  501 / 525  Training Loss  5.8642377553042024e-05\n",
            "Epoch  20 Batch  502 / 525  Training Loss  0.0001035216118907556\n",
            "Epoch  20 Batch  503 / 525  Training Loss  5.8044544857693836e-05\n",
            "Epoch  20 Batch  504 / 525  Training Loss  6.626807589782402e-05\n",
            "Epoch  20 Batch  505 / 525  Training Loss  8.136207907227799e-05\n",
            "Epoch  20 Batch  506 / 525  Training Loss  0.00011846669804072008\n",
            "Epoch  20 Batch  507 / 525  Training Loss  7.251308124978095e-05\n",
            "Epoch  20 Batch  508 / 525  Training Loss  8.626522321719676e-05\n",
            "Epoch  20 Batch  509 / 525  Training Loss  6.685889820801094e-05\n",
            "Epoch  20 Batch  510 / 525  Training Loss  0.00010135857155546546\n",
            "Epoch  20 Batch  511 / 525  Training Loss  5.458124360302463e-05\n",
            "Epoch  20 Batch  512 / 525  Training Loss  7.967864075908437e-05\n",
            "Epoch  20 Batch  513 / 525  Training Loss  8.344420348294079e-05\n",
            "Epoch  20 Batch  514 / 525  Training Loss  6.30859358352609e-05\n",
            "Epoch  20 Batch  515 / 525  Training Loss  6.342203414533287e-05\n",
            "Epoch  20 Batch  516 / 525  Training Loss  8.392762538278475e-05\n",
            "Epoch  20 Batch  517 / 525  Training Loss  8.354989404324442e-05\n",
            "Epoch  20 Batch  518 / 525  Training Loss  5.8525656640995294e-05\n",
            "Epoch  20 Batch  519 / 525  Training Loss  4.4402735511539504e-05\n",
            "Epoch  20 Batch  520 / 525  Training Loss  6.292750185821205e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  20 Batch  521 / 525  Training Loss  5.258773308014497e-05\n",
            "Epoch  20 Batch  522 / 525  Training Loss  5.911156040383503e-05\n",
            "Epoch  20 Batch  523 / 525  Training Loss  7.065293902996927e-05\n",
            "Epoch  20 Batch  524 / 525  Training Loss  9.937877621268854e-05\n",
            "  21    |    -    |   0.000096   |   64.21  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 21\n",
            "Epoch  21 Batch  0 / 525  Training Loss  6.0033595218556e-05\n",
            "Epoch  21 Batch  1 / 525  Training Loss  8.313867874676362e-05\n",
            "Epoch  21 Batch  2 / 525  Training Loss  5.920936382608488e-05\n",
            "Epoch  21 Batch  3 / 525  Training Loss  4.8942340072244406e-05\n",
            "Epoch  21 Batch  4 / 525  Training Loss  4.79472910228651e-05\n",
            "Epoch  21 Batch  5 / 525  Training Loss  6.62174861645326e-05\n",
            "Epoch  21 Batch  6 / 525  Training Loss  6.905048940097913e-05\n",
            "Epoch  21 Batch  7 / 525  Training Loss  7.034448208287358e-05\n",
            "Epoch  21 Batch  8 / 525  Training Loss  8.906183211365715e-05\n",
            "Epoch  21 Batch  9 / 525  Training Loss  7.201312109827995e-05\n",
            "Epoch  21 Batch  10 / 525  Training Loss  9.594584116712213e-05\n",
            "Epoch  21 Batch  11 / 525  Training Loss  8.327873365487903e-05\n",
            "Epoch  21 Batch  12 / 525  Training Loss  6.040615699021146e-05\n",
            "Epoch  21 Batch  13 / 525  Training Loss  6.502309406641871e-05\n",
            "Epoch  21 Batch  14 / 525  Training Loss  7.20551615813747e-05\n",
            "Epoch  21 Batch  15 / 525  Training Loss  5.7648907386465e-05\n",
            "Epoch  21 Batch  16 / 525  Training Loss  6.239784852368757e-05\n",
            "Epoch  21 Batch  17 / 525  Training Loss  4.0336206438951194e-05\n",
            "Epoch  21 Batch  18 / 525  Training Loss  6.147770909592509e-05\n",
            "Epoch  21 Batch  19 / 525  Training Loss  5.6665703596081585e-05\n",
            "Epoch  21 Batch  20 / 525  Training Loss  7.19893942005001e-05\n",
            "Epoch  21 Batch  21 / 525  Training Loss  5.9069985582027584e-05\n",
            "Epoch  21 Batch  22 / 525  Training Loss  6.458854477386922e-05\n",
            "Epoch  21 Batch  23 / 525  Training Loss  5.828654320794158e-05\n",
            "Epoch  21 Batch  24 / 525  Training Loss  5.420924935606308e-05\n",
            "Epoch  21 Batch  25 / 525  Training Loss  6.476601993199438e-05\n",
            "Epoch  21 Batch  26 / 525  Training Loss  8.643676119390875e-05\n",
            "Epoch  21 Batch  27 / 525  Training Loss  3.9951217331690714e-05\n",
            "Epoch  21 Batch  28 / 525  Training Loss  7.803484186297283e-05\n",
            "Epoch  21 Batch  29 / 525  Training Loss  6.382718856912106e-05\n",
            "Epoch  21 Batch  30 / 525  Training Loss  6.635562021983787e-05\n",
            "Epoch  21 Batch  31 / 525  Training Loss  6.32657392998226e-05\n",
            "Epoch  21 Batch  32 / 525  Training Loss  6.0052156186429784e-05\n",
            "Epoch  21 Batch  33 / 525  Training Loss  6.440280412789434e-05\n",
            "Epoch  21 Batch  34 / 525  Training Loss  5.707321179215796e-05\n",
            "Epoch  21 Batch  35 / 525  Training Loss  7.032549183350056e-05\n",
            "Epoch  21 Batch  36 / 525  Training Loss  7.469160482287407e-05\n",
            "Epoch  21 Batch  37 / 525  Training Loss  4.934834942105226e-05\n",
            "Epoch  21 Batch  38 / 525  Training Loss  5.2120518375886604e-05\n",
            "Epoch  21 Batch  39 / 525  Training Loss  5.623859033221379e-05\n",
            "Epoch  21 Batch  40 / 525  Training Loss  7.624377030879259e-05\n",
            "Epoch  21 Batch  41 / 525  Training Loss  5.8603960496839136e-05\n",
            "Epoch  21 Batch  42 / 525  Training Loss  8.532193896826357e-05\n",
            "Epoch  21 Batch  43 / 525  Training Loss  5.562947626458481e-05\n",
            "Epoch  21 Batch  44 / 525  Training Loss  9.370168845634907e-05\n",
            "Epoch  21 Batch  45 / 525  Training Loss  6.0066879086662084e-05\n",
            "Epoch  21 Batch  46 / 525  Training Loss  5.7654327974887565e-05\n",
            "Epoch  21 Batch  47 / 525  Training Loss  7.342946628341451e-05\n",
            "Epoch  21 Batch  48 / 525  Training Loss  6.580725312232971e-05\n",
            "Epoch  21 Batch  49 / 525  Training Loss  5.7990320783574134e-05\n",
            "Epoch  21 Batch  50 / 525  Training Loss  3.886760532623157e-05\n",
            "Epoch  21 Batch  51 / 525  Training Loss  5.1416758651612327e-05\n",
            "Epoch  21 Batch  52 / 525  Training Loss  7.383986667264253e-05\n",
            "Epoch  21 Batch  53 / 525  Training Loss  7.458005711669102e-05\n",
            "Epoch  21 Batch  54 / 525  Training Loss  5.1897757657570764e-05\n",
            "Epoch  21 Batch  55 / 525  Training Loss  4.094282485311851e-05\n",
            "Epoch  21 Batch  56 / 525  Training Loss  6.877884879941121e-05\n",
            "Epoch  21 Batch  57 / 525  Training Loss  6.277948705246672e-05\n",
            "Epoch  21 Batch  58 / 525  Training Loss  7.010782428551465e-05\n",
            "Epoch  21 Batch  59 / 525  Training Loss  6.566337106050923e-05\n",
            "Epoch  21 Batch  60 / 525  Training Loss  4.335743869887665e-05\n",
            "Epoch  21 Batch  61 / 525  Training Loss  5.222476465860382e-05\n",
            "Epoch  21 Batch  62 / 525  Training Loss  4.9874935939442366e-05\n",
            "Epoch  21 Batch  63 / 525  Training Loss  7.879658369347453e-05\n",
            "Epoch  21 Batch  64 / 525  Training Loss  5.6227338063763455e-05\n",
            "Epoch  21 Batch  65 / 525  Training Loss  6.073949043639004e-05\n",
            "Epoch  21 Batch  66 / 525  Training Loss  6.057876089471392e-05\n",
            "Epoch  21 Batch  67 / 525  Training Loss  5.934620276093483e-05\n",
            "Epoch  21 Batch  68 / 525  Training Loss  7.493289740523323e-05\n",
            "Epoch  21 Batch  69 / 525  Training Loss  6.556225707754493e-05\n",
            "Epoch  21 Batch  70 / 525  Training Loss  6.59046127111651e-05\n",
            "Epoch  21 Batch  71 / 525  Training Loss  4.626604641089216e-05\n",
            "Epoch  21 Batch  72 / 525  Training Loss  6.485825724666938e-05\n",
            "Epoch  21 Batch  73 / 525  Training Loss  7.298366108443588e-05\n",
            "Epoch  21 Batch  74 / 525  Training Loss  6.210239371284842e-05\n",
            "Epoch  21 Batch  75 / 525  Training Loss  8.106329914880916e-05\n",
            "Epoch  21 Batch  76 / 525  Training Loss  5.087695171823725e-05\n",
            "Epoch  21 Batch  77 / 525  Training Loss  8.993497613118961e-05\n",
            "Epoch  21 Batch  78 / 525  Training Loss  6.416861288016662e-05\n",
            "Epoch  21 Batch  79 / 525  Training Loss  6.679109355900437e-05\n",
            "Epoch  21 Batch  80 / 525  Training Loss  3.2599906262476e-05\n",
            "Epoch  21 Batch  81 / 525  Training Loss  6.320569082163274e-05\n",
            "Epoch  21 Batch  82 / 525  Training Loss  6.548983947141096e-05\n",
            "Epoch  21 Batch  83 / 525  Training Loss  3.492422547424212e-05\n",
            "Epoch  21 Batch  84 / 525  Training Loss  6.844187737442553e-05\n",
            "Epoch  21 Batch  85 / 525  Training Loss  6.918272993061692e-05\n",
            "Epoch  21 Batch  86 / 525  Training Loss  5.5414926464436576e-05\n",
            "Epoch  21 Batch  87 / 525  Training Loss  8.383932436117902e-05\n",
            "Epoch  21 Batch  88 / 525  Training Loss  5.410596349975094e-05\n",
            "Epoch  21 Batch  89 / 525  Training Loss  6.0046149883419275e-05\n",
            "Epoch  21 Batch  90 / 525  Training Loss  7.047804683679715e-05\n",
            "Epoch  21 Batch  91 / 525  Training Loss  5.9508638514671475e-05\n",
            "Epoch  21 Batch  92 / 525  Training Loss  5.631256499327719e-05\n",
            "Epoch  21 Batch  93 / 525  Training Loss  8.041986438911408e-05\n",
            "Epoch  21 Batch  94 / 525  Training Loss  4.146807623328641e-05\n",
            "Epoch  21 Batch  95 / 525  Training Loss  5.7921733969124034e-05\n",
            "Epoch  21 Batch  96 / 525  Training Loss  6.99474912835285e-05\n",
            "Epoch  21 Batch  97 / 525  Training Loss  3.460660082055256e-05\n",
            "Epoch  21 Batch  98 / 525  Training Loss  5.583789243246429e-05\n",
            "Epoch  21 Batch  99 / 525  Training Loss  4.614067074726336e-05\n",
            "Epoch  21 Batch  100 / 525  Training Loss  9.317961666965857e-05\n",
            "Epoch  21 Batch  101 / 525  Training Loss  7.491598080378026e-05\n",
            "Epoch  21 Batch  102 / 525  Training Loss  6.580402259714901e-05\n",
            "Epoch  21 Batch  103 / 525  Training Loss  3.8887101254658774e-05\n",
            "Epoch  21 Batch  104 / 525  Training Loss  6.46500411676243e-05\n",
            "Epoch  21 Batch  105 / 525  Training Loss  5.555346797336824e-05\n",
            "Epoch  21 Batch  106 / 525  Training Loss  6.841214053565636e-05\n",
            "Epoch  21 Batch  107 / 525  Training Loss  9.59825556492433e-05\n",
            "Epoch  21 Batch  108 / 525  Training Loss  5.302611316437833e-05\n",
            "Epoch  21 Batch  109 / 525  Training Loss  6.087200017645955e-05\n",
            "Epoch  21 Batch  110 / 525  Training Loss  6.902137829456478e-05\n",
            "Epoch  21 Batch  111 / 525  Training Loss  5.437795334728435e-05\n",
            "Epoch  21 Batch  112 / 525  Training Loss  6.782067066524178e-05\n",
            "Epoch  21 Batch  113 / 525  Training Loss  6.733387999702245e-05\n",
            "Epoch  21 Batch  114 / 525  Training Loss  5.8094981795875356e-05\n",
            "Epoch  21 Batch  115 / 525  Training Loss  5.211880124988966e-05\n",
            "Epoch  21 Batch  116 / 525  Training Loss  6.334866338875145e-05\n",
            "Epoch  21 Batch  117 / 525  Training Loss  5.00848182127811e-05\n",
            "Epoch  21 Batch  118 / 525  Training Loss  7.27178921806626e-05\n",
            "Epoch  21 Batch  119 / 525  Training Loss  6.943932385183871e-05\n",
            "Epoch  21 Batch  120 / 525  Training Loss  6.237757770577446e-05\n",
            "Epoch  21 Batch  121 / 525  Training Loss  9.304603736381978e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  21 Batch  122 / 525  Training Loss  6.249282159842551e-05\n",
            "Epoch  21 Batch  123 / 525  Training Loss  6.045985355740413e-05\n",
            "Epoch  21 Batch  124 / 525  Training Loss  6.969251262489706e-05\n",
            "Epoch  21 Batch  125 / 525  Training Loss  6.302803376456723e-05\n",
            "Epoch  21 Batch  126 / 525  Training Loss  6.08666377956979e-05\n",
            "Epoch  21 Batch  127 / 525  Training Loss  7.802779146004468e-05\n",
            "Epoch  21 Batch  128 / 525  Training Loss  6.320558895822614e-05\n",
            "Epoch  21 Batch  129 / 525  Training Loss  7.721803558524698e-05\n",
            "Epoch  21 Batch  130 / 525  Training Loss  4.7154764615697786e-05\n",
            "Epoch  21 Batch  131 / 525  Training Loss  6.076248246245086e-05\n",
            "Epoch  21 Batch  132 / 525  Training Loss  6.739822856616229e-05\n",
            "Epoch  21 Batch  133 / 525  Training Loss  6.078525984776206e-05\n",
            "Epoch  21 Batch  134 / 525  Training Loss  7.311726221814752e-05\n",
            "Epoch  21 Batch  135 / 525  Training Loss  8.417076605837792e-05\n",
            "Epoch  21 Batch  136 / 525  Training Loss  6.630767893511802e-05\n",
            "Epoch  21 Batch  137 / 525  Training Loss  5.145426985109225e-05\n",
            "Epoch  21 Batch  138 / 525  Training Loss  7.61536430218257e-05\n",
            "Epoch  21 Batch  139 / 525  Training Loss  7.182034460129216e-05\n",
            "Epoch  21 Batch  140 / 525  Training Loss  8.027815056266263e-05\n",
            "Epoch  21 Batch  141 / 525  Training Loss  5.815557233290747e-05\n",
            "Epoch  21 Batch  142 / 525  Training Loss  5.572757072513923e-05\n",
            "Epoch  21 Batch  143 / 525  Training Loss  9.387651516590267e-05\n",
            "Epoch  21 Batch  144 / 525  Training Loss  5.600871372735128e-05\n",
            "Epoch  21 Batch  145 / 525  Training Loss  6.912337266840041e-05\n",
            "Epoch  21 Batch  146 / 525  Training Loss  7.548000576207414e-05\n",
            "Epoch  21 Batch  147 / 525  Training Loss  8.158684067893773e-05\n",
            "Epoch  21 Batch  148 / 525  Training Loss  8.150225039571524e-05\n",
            "Epoch  21 Batch  149 / 525  Training Loss  5.1747960242209956e-05\n",
            "Epoch  21 Batch  150 / 525  Training Loss  5.3382827900350094e-05\n",
            "Epoch  21 Batch  151 / 525  Training Loss  5.889163730898872e-05\n",
            "Epoch  21 Batch  152 / 525  Training Loss  7.461397035513073e-05\n",
            "Epoch  21 Batch  153 / 525  Training Loss  4.848924800171517e-05\n",
            "Epoch  21 Batch  154 / 525  Training Loss  7.65464938012883e-05\n",
            "Epoch  21 Batch  155 / 525  Training Loss  5.626774509437382e-05\n",
            "Epoch  21 Batch  156 / 525  Training Loss  4.382245970191434e-05\n",
            "Epoch  21 Batch  157 / 525  Training Loss  7.265056046890095e-05\n",
            "Epoch  21 Batch  158 / 525  Training Loss  6.774078065063804e-05\n",
            "Epoch  21 Batch  159 / 525  Training Loss  6.827355537097901e-05\n",
            "Epoch  21 Batch  160 / 525  Training Loss  5.560495628742501e-05\n",
            "Epoch  21 Batch  161 / 525  Training Loss  6.206494435900822e-05\n",
            "Epoch  21 Batch  162 / 525  Training Loss  5.21113324793987e-05\n",
            "Epoch  21 Batch  163 / 525  Training Loss  7.933685992611572e-05\n",
            "Epoch  21 Batch  164 / 525  Training Loss  6.785655568819493e-05\n",
            "Epoch  21 Batch  165 / 525  Training Loss  7.15005662641488e-05\n",
            "Epoch  21 Batch  166 / 525  Training Loss  8.097522368188947e-05\n",
            "Epoch  21 Batch  167 / 525  Training Loss  6.108952948125079e-05\n",
            "Epoch  21 Batch  168 / 525  Training Loss  4.7941481170710176e-05\n",
            "Epoch  21 Batch  169 / 525  Training Loss  4.5950961066409945e-05\n",
            "Epoch  21 Batch  170 / 525  Training Loss  7.00034070177935e-05\n",
            "Epoch  21 Batch  171 / 525  Training Loss  5.0069676944985986e-05\n",
            "Epoch  21 Batch  172 / 525  Training Loss  7.485443347832188e-05\n",
            "Epoch  21 Batch  173 / 525  Training Loss  5.201910244068131e-05\n",
            "Epoch  21 Batch  174 / 525  Training Loss  5.0237147661391646e-05\n",
            "Epoch  21 Batch  175 / 525  Training Loss  5.6447570386808366e-05\n",
            "Epoch  21 Batch  176 / 525  Training Loss  6.172064604470506e-05\n",
            "Epoch  21 Batch  177 / 525  Training Loss  6.618611223530024e-05\n",
            "Epoch  21 Batch  178 / 525  Training Loss  6.316778308246285e-05\n",
            "Epoch  21 Batch  179 / 525  Training Loss  4.1520332160871476e-05\n",
            "Epoch  21 Batch  180 / 525  Training Loss  4.8365425755036995e-05\n",
            "Epoch  21 Batch  181 / 525  Training Loss  7.482418732251972e-05\n",
            "Epoch  21 Batch  182 / 525  Training Loss  5.4146388720255345e-05\n",
            "Epoch  21 Batch  183 / 525  Training Loss  7.320500299101695e-05\n",
            "Epoch  21 Batch  184 / 525  Training Loss  6.337009108392522e-05\n",
            "Epoch  21 Batch  185 / 525  Training Loss  7.155349885579199e-05\n",
            "Epoch  21 Batch  186 / 525  Training Loss  4.8364301619585603e-05\n",
            "Epoch  21 Batch  187 / 525  Training Loss  3.938719237339683e-05\n",
            "Epoch  21 Batch  188 / 525  Training Loss  5.5642780353082344e-05\n",
            "Epoch  21 Batch  189 / 525  Training Loss  5.5588403483852744e-05\n",
            "Epoch  21 Batch  190 / 525  Training Loss  6.910748197697103e-05\n",
            "Epoch  21 Batch  191 / 525  Training Loss  5.422351023298688e-05\n",
            "Epoch  21 Batch  192 / 525  Training Loss  6.2744235037826e-05\n",
            "Epoch  21 Batch  193 / 525  Training Loss  5.176332706469111e-05\n",
            "Epoch  21 Batch  194 / 525  Training Loss  5.6589797168271616e-05\n",
            "Epoch  21 Batch  195 / 525  Training Loss  6.558185850735754e-05\n",
            "Epoch  21 Batch  196 / 525  Training Loss  3.459109575487673e-05\n",
            "Epoch  21 Batch  197 / 525  Training Loss  6.890540680615231e-05\n",
            "Epoch  21 Batch  198 / 525  Training Loss  6.78894721204415e-05\n",
            "Epoch  21 Batch  199 / 525  Training Loss  5.5541004257975146e-05\n",
            "Epoch  21 Batch  200 / 525  Training Loss  4.6654018660774454e-05\n",
            "Epoch  21 Batch  201 / 525  Training Loss  4.050724601256661e-05\n",
            "Epoch  21 Batch  202 / 525  Training Loss  6.023690730216913e-05\n",
            "Epoch  21 Batch  203 / 525  Training Loss  6.204872624948621e-05\n",
            "Epoch  21 Batch  204 / 525  Training Loss  5.470249379868619e-05\n",
            "Epoch  21 Batch  205 / 525  Training Loss  5.3289044444682077e-05\n",
            "Epoch  21 Batch  206 / 525  Training Loss  5.155100370757282e-05\n",
            "Epoch  21 Batch  207 / 525  Training Loss  4.9816306272987276e-05\n",
            "Epoch  21 Batch  208 / 525  Training Loss  6.093932461226359e-05\n",
            "Epoch  21 Batch  209 / 525  Training Loss  5.5026885092956945e-05\n",
            "Epoch  21 Batch  210 / 525  Training Loss  6.468188075814396e-05\n",
            "Epoch  21 Batch  211 / 525  Training Loss  5.094380321679637e-05\n",
            "Epoch  21 Batch  212 / 525  Training Loss  6.509629020001739e-05\n",
            "Epoch  21 Batch  213 / 525  Training Loss  5.274089198792353e-05\n",
            "Epoch  21 Batch  214 / 525  Training Loss  6.818976544309407e-05\n",
            "Epoch  21 Batch  215 / 525  Training Loss  6.377925456035882e-05\n",
            "Epoch  21 Batch  216 / 525  Training Loss  6.222141382750124e-05\n",
            "Epoch  21 Batch  217 / 525  Training Loss  6.557033339049667e-05\n",
            "Epoch  21 Batch  218 / 525  Training Loss  6.19965503574349e-05\n",
            "Epoch  21 Batch  219 / 525  Training Loss  9.572157432558015e-05\n",
            "Epoch  21 Batch  220 / 525  Training Loss  6.0416867199819535e-05\n",
            "Epoch  21 Batch  221 / 525  Training Loss  4.377234654384665e-05\n",
            "Epoch  21 Batch  222 / 525  Training Loss  6.863509770482779e-05\n",
            "Epoch  21 Batch  223 / 525  Training Loss  5.183321627555415e-05\n",
            "Epoch  21 Batch  224 / 525  Training Loss  4.948255809722468e-05\n",
            "Epoch  21 Batch  225 / 525  Training Loss  6.365226727211848e-05\n",
            "Epoch  21 Batch  226 / 525  Training Loss  8.158583659678698e-05\n",
            "Epoch  21 Batch  227 / 525  Training Loss  4.848599201068282e-05\n",
            "Epoch  21 Batch  228 / 525  Training Loss  5.5549298849655315e-05\n",
            "Epoch  21 Batch  229 / 525  Training Loss  7.263028237503022e-05\n",
            "Epoch  21 Batch  230 / 525  Training Loss  6.437270349124447e-05\n",
            "Epoch  21 Batch  231 / 525  Training Loss  7.584042759845033e-05\n",
            "Epoch  21 Batch  232 / 525  Training Loss  8.029491436900571e-05\n",
            "Epoch  21 Batch  233 / 525  Training Loss  6.926497007953003e-05\n",
            "Epoch  21 Batch  234 / 525  Training Loss  4.10764550906606e-05\n",
            "Epoch  21 Batch  235 / 525  Training Loss  6.017263513058424e-05\n",
            "Epoch  21 Batch  236 / 525  Training Loss  5.8130983234150335e-05\n",
            "Epoch  21 Batch  237 / 525  Training Loss  6.0041835240554065e-05\n",
            "Epoch  21 Batch  238 / 525  Training Loss  6.444881728384644e-05\n",
            "Epoch  21 Batch  239 / 525  Training Loss  6.997111631790176e-05\n",
            "Epoch  21 Batch  240 / 525  Training Loss  7.300225115614012e-05\n",
            "Epoch  21 Batch  241 / 525  Training Loss  5.768625851487741e-05\n",
            "Epoch  21 Batch  242 / 525  Training Loss  5.997528205625713e-05\n",
            "Epoch  21 Batch  243 / 525  Training Loss  7.445966912200674e-05\n",
            "Epoch  21 Batch  244 / 525  Training Loss  6.122163176769391e-05\n",
            "Epoch  21 Batch  245 / 525  Training Loss  7.197869126684964e-05\n",
            "Epoch  21 Batch  246 / 525  Training Loss  7.555419870186597e-05\n",
            "Epoch  21 Batch  247 / 525  Training Loss  7.285900937858969e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  21 Batch  248 / 525  Training Loss  7.312757952604443e-05\n",
            "Epoch  21 Batch  249 / 525  Training Loss  4.2566487536532804e-05\n",
            "Epoch  21 Batch  250 / 525  Training Loss  4.2780615331139416e-05\n",
            "Epoch  21 Batch  251 / 525  Training Loss  5.0169124733656645e-05\n",
            "Epoch  21 Batch  252 / 525  Training Loss  6.346742156893015e-05\n",
            "Epoch  21 Batch  253 / 525  Training Loss  4.9173519073519856e-05\n",
            "Epoch  21 Batch  254 / 525  Training Loss  4.8422854888485745e-05\n",
            "Epoch  21 Batch  255 / 525  Training Loss  4.3284067942295223e-05\n",
            "Epoch  21 Batch  256 / 525  Training Loss  4.8747708206065e-05\n",
            "Epoch  21 Batch  257 / 525  Training Loss  3.594995359890163e-05\n",
            "Epoch  21 Batch  258 / 525  Training Loss  7.25319332559593e-05\n",
            "Epoch  21 Batch  259 / 525  Training Loss  7.667070167372003e-05\n",
            "Epoch  21 Batch  260 / 525  Training Loss  4.9422022129874676e-05\n",
            "Epoch  21 Batch  261 / 525  Training Loss  5.8561992773320526e-05\n",
            "Epoch  21 Batch  262 / 525  Training Loss  5.836457057739608e-05\n",
            "Epoch  21 Batch  263 / 525  Training Loss  6.796185334678739e-05\n",
            "Epoch  21 Batch  264 / 525  Training Loss  6.131066038506106e-05\n",
            "Epoch  21 Batch  265 / 525  Training Loss  5.175535989110358e-05\n",
            "Epoch  21 Batch  266 / 525  Training Loss  4.906480171484873e-05\n",
            "Epoch  21 Batch  267 / 525  Training Loss  4.501475632423535e-05\n",
            "Epoch  21 Batch  268 / 525  Training Loss  6.332830525934696e-05\n",
            "Epoch  21 Batch  269 / 525  Training Loss  5.464844434754923e-05\n",
            "Epoch  21 Batch  270 / 525  Training Loss  6.884113827254623e-05\n",
            "Epoch  21 Batch  271 / 525  Training Loss  3.151825148961507e-05\n",
            "Epoch  21 Batch  272 / 525  Training Loss  6.458338611992076e-05\n",
            "Epoch  21 Batch  273 / 525  Training Loss  8.8987268100027e-05\n",
            "Epoch  21 Batch  274 / 525  Training Loss  6.562915223184973e-05\n",
            "Epoch  21 Batch  275 / 525  Training Loss  5.1446921133901924e-05\n",
            "Epoch  21 Batch  276 / 525  Training Loss  5.7632900279713795e-05\n",
            "Epoch  21 Batch  277 / 525  Training Loss  6.0536804085131735e-05\n",
            "Epoch  21 Batch  278 / 525  Training Loss  4.54845794592984e-05\n",
            "Epoch  21 Batch  279 / 525  Training Loss  5.5267533753067255e-05\n",
            "Epoch  21 Batch  280 / 525  Training Loss  3.988792741438374e-05\n",
            "Epoch  21 Batch  281 / 525  Training Loss  5.1276583690196276e-05\n",
            "Epoch  21 Batch  282 / 525  Training Loss  6.452050001826137e-05\n",
            "Epoch  21 Batch  283 / 525  Training Loss  8.849842561176047e-05\n",
            "Epoch  21 Batch  284 / 525  Training Loss  6.870773358969018e-05\n",
            "Epoch  21 Batch  285 / 525  Training Loss  6.867462070658803e-05\n",
            "Epoch  21 Batch  286 / 525  Training Loss  7.418282621074468e-05\n",
            "Epoch  21 Batch  287 / 525  Training Loss  8.746080129640177e-05\n",
            "Epoch  21 Batch  288 / 525  Training Loss  5.2222116210032254e-05\n",
            "Epoch  21 Batch  289 / 525  Training Loss  6.9804722443223e-05\n",
            "Epoch  21 Batch  290 / 525  Training Loss  6.306421710178256e-05\n",
            "Epoch  21 Batch  291 / 525  Training Loss  5.443999543786049e-05\n",
            "Epoch  21 Batch  292 / 525  Training Loss  6.861196015961468e-05\n",
            "Epoch  21 Batch  293 / 525  Training Loss  4.698863995145075e-05\n",
            "Epoch  21 Batch  294 / 525  Training Loss  6.524774653371423e-05\n",
            "Epoch  21 Batch  295 / 525  Training Loss  5.3959705837769434e-05\n",
            "Epoch  21 Batch  296 / 525  Training Loss  7.274516246980056e-05\n",
            "Epoch  21 Batch  297 / 525  Training Loss  6.801731797168031e-05\n",
            "Epoch  21 Batch  298 / 525  Training Loss  5.235142816673033e-05\n",
            "Epoch  21 Batch  299 / 525  Training Loss  4.691479989560321e-05\n",
            "Epoch  21 Batch  300 / 525  Training Loss  5.18514025316108e-05\n",
            "Epoch  21 Batch  301 / 525  Training Loss  5.653801417793147e-05\n",
            "Epoch  21 Batch  302 / 525  Training Loss  4.463710865820758e-05\n",
            "Epoch  21 Batch  303 / 525  Training Loss  7.218406244646758e-05\n",
            "Epoch  21 Batch  304 / 525  Training Loss  5.427482392406091e-05\n",
            "Epoch  21 Batch  305 / 525  Training Loss  8.966411405708641e-05\n",
            "Epoch  21 Batch  306 / 525  Training Loss  5.3961579396855086e-05\n",
            "Epoch  21 Batch  307 / 525  Training Loss  4.9338625103700906e-05\n",
            "Epoch  21 Batch  308 / 525  Training Loss  5.47608760825824e-05\n",
            "Epoch  21 Batch  309 / 525  Training Loss  4.362851177575067e-05\n",
            "Epoch  21 Batch  310 / 525  Training Loss  4.386646833154373e-05\n",
            "Epoch  21 Batch  311 / 525  Training Loss  5.6168781156884506e-05\n",
            "Epoch  21 Batch  312 / 525  Training Loss  6.638620106969029e-05\n",
            "Epoch  21 Batch  313 / 525  Training Loss  6.107239460106939e-05\n",
            "Epoch  21 Batch  314 / 525  Training Loss  4.7946185077307746e-05\n",
            "Epoch  21 Batch  315 / 525  Training Loss  4.278754931874573e-05\n",
            "Epoch  21 Batch  316 / 525  Training Loss  6.972915434744209e-05\n",
            "Epoch  21 Batch  317 / 525  Training Loss  6.156637391541153e-05\n",
            "Epoch  21 Batch  318 / 525  Training Loss  6.218867201823741e-05\n",
            "Epoch  21 Batch  319 / 525  Training Loss  4.953008101438172e-05\n",
            "Epoch  21 Batch  320 / 525  Training Loss  7.546396227553487e-05\n",
            "Epoch  21 Batch  321 / 525  Training Loss  4.577518120640889e-05\n",
            "Epoch  21 Batch  322 / 525  Training Loss  7.505839312216267e-05\n",
            "Epoch  21 Batch  323 / 525  Training Loss  4.042701766593382e-05\n",
            "Epoch  21 Batch  324 / 525  Training Loss  5.881641845917329e-05\n",
            "Epoch  21 Batch  325 / 525  Training Loss  5.003899059374817e-05\n",
            "Epoch  21 Batch  326 / 525  Training Loss  5.133564627612941e-05\n",
            "Epoch  21 Batch  327 / 525  Training Loss  4.58817339676898e-05\n",
            "Epoch  21 Batch  328 / 525  Training Loss  4.457736940821633e-05\n",
            "Epoch  21 Batch  329 / 525  Training Loss  8.626723138149828e-05\n",
            "Epoch  21 Batch  330 / 525  Training Loss  5.668508674716577e-05\n",
            "Epoch  21 Batch  331 / 525  Training Loss  7.65723452786915e-05\n",
            "Epoch  21 Batch  332 / 525  Training Loss  5.708267417503521e-05\n",
            "Epoch  21 Batch  333 / 525  Training Loss  7.240953709697351e-05\n",
            "Epoch  21 Batch  334 / 525  Training Loss  5.1994826208101586e-05\n",
            "Epoch  21 Batch  335 / 525  Training Loss  6.0158479755045846e-05\n",
            "Epoch  21 Batch  336 / 525  Training Loss  6.58221761113964e-05\n",
            "Epoch  21 Batch  337 / 525  Training Loss  5.273540591588244e-05\n",
            "Epoch  21 Batch  338 / 525  Training Loss  4.7616649681003764e-05\n",
            "Epoch  21 Batch  339 / 525  Training Loss  4.9721402319846675e-05\n",
            "Epoch  21 Batch  340 / 525  Training Loss  6.11845898674801e-05\n",
            "Epoch  21 Batch  341 / 525  Training Loss  4.8084672016557306e-05\n",
            "Epoch  21 Batch  342 / 525  Training Loss  7.523871317971498e-05\n",
            "Epoch  21 Batch  343 / 525  Training Loss  6.481945456471294e-05\n",
            "Epoch  21 Batch  344 / 525  Training Loss  7.93718354543671e-05\n",
            "Epoch  21 Batch  345 / 525  Training Loss  7.713576633250341e-05\n",
            "Epoch  21 Batch  346 / 525  Training Loss  6.582569039892405e-05\n",
            "Epoch  21 Batch  347 / 525  Training Loss  6.0142243455629796e-05\n",
            "Epoch  21 Batch  348 / 525  Training Loss  4.862251444137655e-05\n",
            "Epoch  21 Batch  349 / 525  Training Loss  6.50436122668907e-05\n",
            "Epoch  21 Batch  350 / 525  Training Loss  7.43299096939154e-05\n",
            "Epoch  21 Batch  351 / 525  Training Loss  6.043058237992227e-05\n",
            "Epoch  21 Batch  352 / 525  Training Loss  6.044161273166537e-05\n",
            "Epoch  21 Batch  353 / 525  Training Loss  7.072043081279844e-05\n",
            "Epoch  21 Batch  354 / 525  Training Loss  8.96483616088517e-05\n",
            "Epoch  21 Batch  355 / 525  Training Loss  7.037725299596786e-05\n",
            "Epoch  21 Batch  356 / 525  Training Loss  6.148136162664741e-05\n",
            "Epoch  21 Batch  357 / 525  Training Loss  6.509677041321993e-05\n",
            "Epoch  21 Batch  358 / 525  Training Loss  5.389866419136524e-05\n",
            "Epoch  21 Batch  359 / 525  Training Loss  6.459209544118494e-05\n",
            "Epoch  21 Batch  360 / 525  Training Loss  6.694679905194789e-05\n",
            "Epoch  21 Batch  361 / 525  Training Loss  5.00748647027649e-05\n",
            "Epoch  21 Batch  362 / 525  Training Loss  7.205683505162597e-05\n",
            "Epoch  21 Batch  363 / 525  Training Loss  4.1874936869135126e-05\n",
            "Epoch  21 Batch  364 / 525  Training Loss  5.5431213695555925e-05\n",
            "Epoch  21 Batch  365 / 525  Training Loss  7.020960038062185e-05\n",
            "Epoch  21 Batch  366 / 525  Training Loss  6.690721784252673e-05\n",
            "Epoch  21 Batch  367 / 525  Training Loss  8.103057916741818e-05\n",
            "Epoch  21 Batch  368 / 525  Training Loss  7.891231507528573e-05\n",
            "Epoch  21 Batch  369 / 525  Training Loss  6.802841380704194e-05\n",
            "Epoch  21 Batch  370 / 525  Training Loss  6.0961359849898145e-05\n",
            "Epoch  21 Batch  371 / 525  Training Loss  5.657150177285075e-05\n",
            "Epoch  21 Batch  372 / 525  Training Loss  8.101630373857915e-05\n",
            "Epoch  21 Batch  373 / 525  Training Loss  7.974638720043004e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  21 Batch  374 / 525  Training Loss  6.331202166620642e-05\n",
            "Epoch  21 Batch  375 / 525  Training Loss  7.197995000751689e-05\n",
            "Epoch  21 Batch  376 / 525  Training Loss  4.1354265704285353e-05\n",
            "Epoch  21 Batch  377 / 525  Training Loss  5.509334005182609e-05\n",
            "Epoch  21 Batch  378 / 525  Training Loss  5.801855513709597e-05\n",
            "Epoch  21 Batch  379 / 525  Training Loss  6.832482176832855e-05\n",
            "Epoch  21 Batch  380 / 525  Training Loss  5.346236866898835e-05\n",
            "Epoch  21 Batch  381 / 525  Training Loss  7.147262658691034e-05\n",
            "Epoch  21 Batch  382 / 525  Training Loss  4.641645500669256e-05\n",
            "Epoch  21 Batch  383 / 525  Training Loss  3.0628951208200306e-05\n",
            "Epoch  21 Batch  384 / 525  Training Loss  7.433362770825624e-05\n",
            "Epoch  21 Batch  385 / 525  Training Loss  7.520501094404608e-05\n",
            "Epoch  21 Batch  386 / 525  Training Loss  6.686126289423555e-05\n",
            "Epoch  21 Batch  387 / 525  Training Loss  6.210542778717354e-05\n",
            "Epoch  21 Batch  388 / 525  Training Loss  5.5629217968089506e-05\n",
            "Epoch  21 Batch  389 / 525  Training Loss  4.443725265446119e-05\n",
            "Epoch  21 Batch  390 / 525  Training Loss  5.23217095178552e-05\n",
            "Epoch  21 Batch  391 / 525  Training Loss  5.897091614315286e-05\n",
            "Epoch  21 Batch  392 / 525  Training Loss  5.052563574281521e-05\n",
            "Epoch  21 Batch  393 / 525  Training Loss  6.969577225390822e-05\n",
            "Epoch  21 Batch  394 / 525  Training Loss  4.842204361921176e-05\n",
            "Epoch  21 Batch  395 / 525  Training Loss  4.443108991836198e-05\n",
            "Epoch  21 Batch  396 / 525  Training Loss  5.810578659293242e-05\n",
            "Epoch  21 Batch  397 / 525  Training Loss  6.734440103173256e-05\n",
            "Epoch  21 Batch  398 / 525  Training Loss  4.7099940275074914e-05\n",
            "Epoch  21 Batch  399 / 525  Training Loss  4.425602310220711e-05\n",
            "Epoch  21 Batch  400 / 525  Training Loss  6.543028575833887e-05\n",
            "Epoch  21 Batch  401 / 525  Training Loss  5.194519690121524e-05\n",
            "Epoch  21 Batch  402 / 525  Training Loss  7.851442933315411e-05\n",
            "Epoch  21 Batch  403 / 525  Training Loss  6.0813890740973875e-05\n",
            "Epoch  21 Batch  404 / 525  Training Loss  4.3298037780914456e-05\n",
            "Epoch  21 Batch  405 / 525  Training Loss  5.0464448577258736e-05\n",
            "Epoch  21 Batch  406 / 525  Training Loss  6.406522879842669e-05\n",
            "Epoch  21 Batch  407 / 525  Training Loss  6.059851875761524e-05\n",
            "Epoch  21 Batch  408 / 525  Training Loss  8.032080950215459e-05\n",
            "Epoch  21 Batch  409 / 525  Training Loss  8.013250771909952e-05\n",
            "Epoch  21 Batch  410 / 525  Training Loss  5.2552139095496386e-05\n",
            "Epoch  21 Batch  411 / 525  Training Loss  6.433733506128192e-05\n",
            "Epoch  21 Batch  412 / 525  Training Loss  4.941683801007457e-05\n",
            "Epoch  21 Batch  413 / 525  Training Loss  6.697337084915489e-05\n",
            "Epoch  21 Batch  414 / 525  Training Loss  3.901805393979885e-05\n",
            "Epoch  21 Batch  415 / 525  Training Loss  6.435318209696561e-05\n",
            "Epoch  21 Batch  416 / 525  Training Loss  5.967319157207385e-05\n",
            "Epoch  21 Batch  417 / 525  Training Loss  5.873025656910613e-05\n",
            "Epoch  21 Batch  418 / 525  Training Loss  5.4813855967950076e-05\n",
            "Epoch  21 Batch  419 / 525  Training Loss  5.742747453041375e-05\n",
            "Epoch  21 Batch  420 / 525  Training Loss  6.826967000961304e-05\n",
            "Epoch  21 Batch  421 / 525  Training Loss  4.376769720693119e-05\n",
            "Epoch  21 Batch  422 / 525  Training Loss  7.435221050400287e-05\n",
            "Epoch  21 Batch  423 / 525  Training Loss  5.3894218581262976e-05\n",
            "Epoch  21 Batch  424 / 525  Training Loss  9.216820035362616e-05\n",
            "Epoch  21 Batch  425 / 525  Training Loss  2.9973871278343722e-05\n",
            "Epoch  21 Batch  426 / 525  Training Loss  4.9393827794119716e-05\n",
            "Epoch  21 Batch  427 / 525  Training Loss  5.436317223939113e-05\n",
            "Epoch  21 Batch  428 / 525  Training Loss  4.513826934271492e-05\n",
            "Epoch  21 Batch  429 / 525  Training Loss  3.019705218321178e-05\n",
            "Epoch  21 Batch  430 / 525  Training Loss  8.108805923257023e-05\n",
            "Epoch  21 Batch  431 / 525  Training Loss  5.418956425273791e-05\n",
            "Epoch  21 Batch  432 / 525  Training Loss  6.375555676640943e-05\n",
            "Epoch  21 Batch  433 / 525  Training Loss  4.053876182297245e-05\n",
            "Epoch  21 Batch  434 / 525  Training Loss  6.341797416098416e-05\n",
            "Epoch  21 Batch  435 / 525  Training Loss  4.138399526709691e-05\n",
            "Epoch  21 Batch  436 / 525  Training Loss  6.218373164301738e-05\n",
            "Epoch  21 Batch  437 / 525  Training Loss  6.46342959953472e-05\n",
            "Epoch  21 Batch  438 / 525  Training Loss  4.529828947852366e-05\n",
            "Epoch  21 Batch  439 / 525  Training Loss  6.302011024672538e-05\n",
            "Epoch  21 Batch  440 / 525  Training Loss  5.691966362064704e-05\n",
            "Epoch  21 Batch  441 / 525  Training Loss  3.975031359004788e-05\n",
            "Epoch  21 Batch  442 / 525  Training Loss  7.447222014889121e-05\n",
            "Epoch  21 Batch  443 / 525  Training Loss  7.910912245279178e-05\n",
            "Epoch  21 Batch  444 / 525  Training Loss  5.197989958105609e-05\n",
            "Epoch  21 Batch  445 / 525  Training Loss  5.20649591635447e-05\n",
            "Epoch  21 Batch  446 / 525  Training Loss  6.160623161122203e-05\n",
            "Epoch  21 Batch  447 / 525  Training Loss  5.92714459344279e-05\n",
            "Epoch  21 Batch  448 / 525  Training Loss  5.69161020393949e-05\n",
            "Epoch  21 Batch  449 / 525  Training Loss  5.0972641474800184e-05\n",
            "Epoch  21 Batch  450 / 525  Training Loss  5.5036718549672514e-05\n",
            "Epoch  21 Batch  451 / 525  Training Loss  4.29128049290739e-05\n",
            "Epoch  21 Batch  452 / 525  Training Loss  5.617773422272876e-05\n",
            "Epoch  21 Batch  453 / 525  Training Loss  8.226120553445071e-05\n",
            "Epoch  21 Batch  454 / 525  Training Loss  5.8034474932355806e-05\n",
            "Epoch  21 Batch  455 / 525  Training Loss  7.122138777049258e-05\n",
            "Epoch  21 Batch  456 / 525  Training Loss  4.464085941435769e-05\n",
            "Epoch  21 Batch  457 / 525  Training Loss  4.192740743746981e-05\n",
            "Epoch  21 Batch  458 / 525  Training Loss  5.851677997270599e-05\n",
            "Epoch  21 Batch  459 / 525  Training Loss  4.924117092741653e-05\n",
            "Epoch  21 Batch  460 / 525  Training Loss  4.7803769120946527e-05\n",
            "Epoch  21 Batch  461 / 525  Training Loss  5.0664191803662106e-05\n",
            "Epoch  21 Batch  462 / 525  Training Loss  5.8967321820091456e-05\n",
            "Epoch  21 Batch  463 / 525  Training Loss  6.694041076116264e-05\n",
            "Epoch  21 Batch  464 / 525  Training Loss  3.547895175870508e-05\n",
            "Epoch  21 Batch  465 / 525  Training Loss  5.6432862038491294e-05\n",
            "Epoch  21 Batch  466 / 525  Training Loss  3.4385269827907905e-05\n",
            "Epoch  21 Batch  467 / 525  Training Loss  6.109506648499519e-05\n",
            "Epoch  21 Batch  468 / 525  Training Loss  6.401621794793755e-05\n",
            "Epoch  21 Batch  469 / 525  Training Loss  5.713254358852282e-05\n",
            "Epoch  21 Batch  470 / 525  Training Loss  3.811056740232743e-05\n",
            "Epoch  21 Batch  471 / 525  Training Loss  6.573760038008913e-05\n",
            "Epoch  21 Batch  472 / 525  Training Loss  5.461873297463171e-05\n",
            "Epoch  21 Batch  473 / 525  Training Loss  5.743720612372272e-05\n",
            "Epoch  21 Batch  474 / 525  Training Loss  6.285824929364026e-05\n",
            "Epoch  21 Batch  475 / 525  Training Loss  5.311087443260476e-05\n",
            "Epoch  21 Batch  476 / 525  Training Loss  4.0003353205975145e-05\n",
            "Epoch  21 Batch  477 / 525  Training Loss  7.310787623282522e-05\n",
            "Epoch  21 Batch  478 / 525  Training Loss  7.466101669706404e-05\n",
            "Epoch  21 Batch  479 / 525  Training Loss  4.9457048589829355e-05\n",
            "Epoch  21 Batch  480 / 525  Training Loss  5.271497502690181e-05\n",
            "Epoch  21 Batch  481 / 525  Training Loss  5.3313015087042004e-05\n",
            "Epoch  21 Batch  482 / 525  Training Loss  5.832054739585146e-05\n",
            "Epoch  21 Batch  483 / 525  Training Loss  6.166657112771645e-05\n",
            "Epoch  21 Batch  484 / 525  Training Loss  6.704559200443327e-05\n",
            "Epoch  21 Batch  485 / 525  Training Loss  5.909673200221732e-05\n",
            "Epoch  21 Batch  486 / 525  Training Loss  7.491769065381959e-05\n",
            "Epoch  21 Batch  487 / 525  Training Loss  6.988218956394121e-05\n",
            "Epoch  21 Batch  488 / 525  Training Loss  7.41827316232957e-05\n",
            "Epoch  21 Batch  489 / 525  Training Loss  7.18256487743929e-05\n",
            "Epoch  21 Batch  490 / 525  Training Loss  6.124690116848797e-05\n",
            "Epoch  21 Batch  491 / 525  Training Loss  8.567726763430983e-05\n",
            "Epoch  21 Batch  492 / 525  Training Loss  7.790368545101956e-05\n",
            "Epoch  21 Batch  493 / 525  Training Loss  7.576476491522044e-05\n",
            "Epoch  21 Batch  494 / 525  Training Loss  4.644969158107415e-05\n",
            "Epoch  21 Batch  495 / 525  Training Loss  3.893945540767163e-05\n",
            "Epoch  21 Batch  496 / 525  Training Loss  5.660515671479516e-05\n",
            "Epoch  21 Batch  497 / 525  Training Loss  6.370690243784338e-05\n",
            "Epoch  21 Batch  498 / 525  Training Loss  6.078457954572514e-05\n",
            "Epoch  21 Batch  499 / 525  Training Loss  7.057077891658992e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  21 Batch  500 / 525  Training Loss  6.467852654168382e-05\n",
            "Epoch  21 Batch  501 / 525  Training Loss  3.975240542786196e-05\n",
            "Epoch  21 Batch  502 / 525  Training Loss  6.286964344326407e-05\n",
            "Epoch  21 Batch  503 / 525  Training Loss  0.00010237479727948084\n",
            "Epoch  21 Batch  504 / 525  Training Loss  5.932218482485041e-05\n",
            "Epoch  21 Batch  505 / 525  Training Loss  7.381649629678577e-05\n",
            "Epoch  21 Batch  506 / 525  Training Loss  6.2813691329211e-05\n",
            "Epoch  21 Batch  507 / 525  Training Loss  3.9435090002371e-05\n",
            "Epoch  21 Batch  508 / 525  Training Loss  3.8882171793375164e-05\n",
            "Epoch  21 Batch  509 / 525  Training Loss  4.9130314437206835e-05\n",
            "Epoch  21 Batch  510 / 525  Training Loss  4.7101839300012216e-05\n",
            "Epoch  21 Batch  511 / 525  Training Loss  5.257258453639224e-05\n",
            "Epoch  21 Batch  512 / 525  Training Loss  4.523991447058506e-05\n",
            "Epoch  21 Batch  513 / 525  Training Loss  6.677473720628768e-05\n",
            "Epoch  21 Batch  514 / 525  Training Loss  8.471183537039906e-05\n",
            "Epoch  21 Batch  515 / 525  Training Loss  5.5755495850462466e-05\n",
            "Epoch  21 Batch  516 / 525  Training Loss  8.043575508054346e-05\n",
            "Epoch  21 Batch  517 / 525  Training Loss  5.0949274736922234e-05\n",
            "Epoch  21 Batch  518 / 525  Training Loss  7.543298852397129e-05\n",
            "Epoch  21 Batch  519 / 525  Training Loss  6.126536027295515e-05\n",
            "Epoch  21 Batch  520 / 525  Training Loss  6.626920367125422e-05\n",
            "Epoch  21 Batch  521 / 525  Training Loss  5.3247349569574e-05\n",
            "Epoch  21 Batch  522 / 525  Training Loss  7.596729847136885e-05\n",
            "Epoch  21 Batch  523 / 525  Training Loss  5.565692117670551e-05\n",
            "Epoch  21 Batch  524 / 525  Training Loss  7.049136911518872e-05\n",
            "  22    |    -    |   0.000061   |   64.25  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 22\n",
            "Epoch  22 Batch  0 / 525  Training Loss  4.049052222399041e-05\n",
            "Epoch  22 Batch  1 / 525  Training Loss  4.22965495090466e-05\n",
            "Epoch  22 Batch  2 / 525  Training Loss  5.3759948059450835e-05\n",
            "Epoch  22 Batch  3 / 525  Training Loss  5.672366387443617e-05\n",
            "Epoch  22 Batch  4 / 525  Training Loss  5.226027133176103e-05\n",
            "Epoch  22 Batch  5 / 525  Training Loss  5.797393532702699e-05\n",
            "Epoch  22 Batch  6 / 525  Training Loss  5.6577475334051996e-05\n",
            "Epoch  22 Batch  7 / 525  Training Loss  4.7157904191408306e-05\n",
            "Epoch  22 Batch  8 / 525  Training Loss  5.3468789701582864e-05\n",
            "Epoch  22 Batch  9 / 525  Training Loss  6.044390465831384e-05\n",
            "Epoch  22 Batch  10 / 525  Training Loss  4.3755968363257125e-05\n",
            "Epoch  22 Batch  11 / 525  Training Loss  4.3774743971880525e-05\n",
            "Epoch  22 Batch  12 / 525  Training Loss  4.7537443606415763e-05\n",
            "Epoch  22 Batch  13 / 525  Training Loss  4.545057163340971e-05\n",
            "Epoch  22 Batch  14 / 525  Training Loss  2.7039379347115755e-05\n",
            "Epoch  22 Batch  15 / 525  Training Loss  3.579046460799873e-05\n",
            "Epoch  22 Batch  16 / 525  Training Loss  4.6575623855460435e-05\n",
            "Epoch  22 Batch  17 / 525  Training Loss  4.647165405913256e-05\n",
            "Epoch  22 Batch  18 / 525  Training Loss  4.5745389797957614e-05\n",
            "Epoch  22 Batch  19 / 525  Training Loss  6.452948582591489e-05\n",
            "Epoch  22 Batch  20 / 525  Training Loss  5.1645438361447304e-05\n",
            "Epoch  22 Batch  21 / 525  Training Loss  4.7572204493917525e-05\n",
            "Epoch  22 Batch  22 / 525  Training Loss  4.126115527469665e-05\n",
            "Epoch  22 Batch  23 / 525  Training Loss  4.759904550155625e-05\n",
            "Epoch  22 Batch  24 / 525  Training Loss  4.62428797618486e-05\n",
            "Epoch  22 Batch  25 / 525  Training Loss  5.167655035620555e-05\n",
            "Epoch  22 Batch  26 / 525  Training Loss  9.248882997781038e-05\n",
            "Epoch  22 Batch  27 / 525  Training Loss  4.1041890654014423e-05\n",
            "Epoch  22 Batch  28 / 525  Training Loss  6.394869706127793e-05\n",
            "Epoch  22 Batch  29 / 525  Training Loss  5.754440280725248e-05\n",
            "Epoch  22 Batch  30 / 525  Training Loss  4.9965339712798595e-05\n",
            "Epoch  22 Batch  31 / 525  Training Loss  5.126100586494431e-05\n",
            "Epoch  22 Batch  32 / 525  Training Loss  4.3912470573559403e-05\n",
            "Epoch  22 Batch  33 / 525  Training Loss  6.50401707389392e-05\n",
            "Epoch  22 Batch  34 / 525  Training Loss  5.8601774071576074e-05\n",
            "Epoch  22 Batch  35 / 525  Training Loss  4.820447429665364e-05\n",
            "Epoch  22 Batch  36 / 525  Training Loss  5.1806255214614794e-05\n",
            "Epoch  22 Batch  37 / 525  Training Loss  5.3485575335798785e-05\n",
            "Epoch  22 Batch  38 / 525  Training Loss  4.7219124098774046e-05\n",
            "Epoch  22 Batch  39 / 525  Training Loss  3.4411739761708304e-05\n",
            "Epoch  22 Batch  40 / 525  Training Loss  3.370233025634661e-05\n",
            "Epoch  22 Batch  41 / 525  Training Loss  6.805334851378575e-05\n",
            "Epoch  22 Batch  42 / 525  Training Loss  4.282020017853938e-05\n",
            "Epoch  22 Batch  43 / 525  Training Loss  3.745513458852656e-05\n",
            "Epoch  22 Batch  44 / 525  Training Loss  5.560252975556068e-05\n",
            "Epoch  22 Batch  45 / 525  Training Loss  5.634934132103808e-05\n",
            "Epoch  22 Batch  46 / 525  Training Loss  5.026475992053747e-05\n",
            "Epoch  22 Batch  47 / 525  Training Loss  5.029993917560205e-05\n",
            "Epoch  22 Batch  48 / 525  Training Loss  4.8203735786955804e-05\n",
            "Epoch  22 Batch  49 / 525  Training Loss  4.8000551032600924e-05\n",
            "Epoch  22 Batch  50 / 525  Training Loss  5.074132786830887e-05\n",
            "Epoch  22 Batch  51 / 525  Training Loss  4.3151660065632313e-05\n",
            "Epoch  22 Batch  52 / 525  Training Loss  3.326997102703899e-05\n",
            "Epoch  22 Batch  53 / 525  Training Loss  7.291439396794885e-05\n",
            "Epoch  22 Batch  54 / 525  Training Loss  4.496184556046501e-05\n",
            "Epoch  22 Batch  55 / 525  Training Loss  4.099252328160219e-05\n",
            "Epoch  22 Batch  56 / 525  Training Loss  4.205203731544316e-05\n",
            "Epoch  22 Batch  57 / 525  Training Loss  4.5860509999329224e-05\n",
            "Epoch  22 Batch  58 / 525  Training Loss  5.3519714128924534e-05\n",
            "Epoch  22 Batch  59 / 525  Training Loss  5.068508835393004e-05\n",
            "Epoch  22 Batch  60 / 525  Training Loss  4.310992517275736e-05\n",
            "Epoch  22 Batch  61 / 525  Training Loss  3.3609212550800294e-05\n",
            "Epoch  22 Batch  62 / 525  Training Loss  6.511991523439065e-05\n",
            "Epoch  22 Batch  63 / 525  Training Loss  3.9599399315193295e-05\n",
            "Epoch  22 Batch  64 / 525  Training Loss  3.452880628174171e-05\n",
            "Epoch  22 Batch  65 / 525  Training Loss  4.619222090695985e-05\n",
            "Epoch  22 Batch  66 / 525  Training Loss  3.087652294198051e-05\n",
            "Epoch  22 Batch  67 / 525  Training Loss  4.5360797230387107e-05\n",
            "Epoch  22 Batch  68 / 525  Training Loss  2.719105759751983e-05\n",
            "Epoch  22 Batch  69 / 525  Training Loss  4.5375021727522835e-05\n",
            "Epoch  22 Batch  70 / 525  Training Loss  4.095937038073316e-05\n",
            "Epoch  22 Batch  71 / 525  Training Loss  5.747769682784565e-05\n",
            "Epoch  22 Batch  72 / 525  Training Loss  5.354006862035021e-05\n",
            "Epoch  22 Batch  73 / 525  Training Loss  3.718604421010241e-05\n",
            "Epoch  22 Batch  74 / 525  Training Loss  5.204805711400695e-05\n",
            "Epoch  22 Batch  75 / 525  Training Loss  5.659174712491222e-05\n",
            "Epoch  22 Batch  76 / 525  Training Loss  3.584379010135308e-05\n",
            "Epoch  22 Batch  77 / 525  Training Loss  4.2032716009998694e-05\n",
            "Epoch  22 Batch  78 / 525  Training Loss  5.5630145652685314e-05\n",
            "Epoch  22 Batch  79 / 525  Training Loss  5.130303907208145e-05\n",
            "Epoch  22 Batch  80 / 525  Training Loss  3.1445590138901025e-05\n",
            "Epoch  22 Batch  81 / 525  Training Loss  4.221966810291633e-05\n",
            "Epoch  22 Batch  82 / 525  Training Loss  4.584764610626735e-05\n",
            "Epoch  22 Batch  83 / 525  Training Loss  4.860400076722726e-05\n",
            "Epoch  22 Batch  84 / 525  Training Loss  4.223618088872172e-05\n",
            "Epoch  22 Batch  85 / 525  Training Loss  3.797478711931035e-05\n",
            "Epoch  22 Batch  86 / 525  Training Loss  5.497475649463013e-05\n",
            "Epoch  22 Batch  87 / 525  Training Loss  4.321468804846518e-05\n",
            "Epoch  22 Batch  88 / 525  Training Loss  3.373763320269063e-05\n",
            "Epoch  22 Batch  89 / 525  Training Loss  3.4159507777076215e-05\n",
            "Epoch  22 Batch  90 / 525  Training Loss  4.775521665578708e-05\n",
            "Epoch  22 Batch  91 / 525  Training Loss  8.21771754999645e-05\n",
            "Epoch  22 Batch  92 / 525  Training Loss  4.905441892333329e-05\n",
            "Epoch  22 Batch  93 / 525  Training Loss  5.5031887313816696e-05\n",
            "Epoch  22 Batch  94 / 525  Training Loss  5.2475348638836294e-05\n",
            "Epoch  22 Batch  95 / 525  Training Loss  5.7303062931168824e-05\n",
            "Epoch  22 Batch  96 / 525  Training Loss  5.671641702065244e-05\n",
            "Epoch  22 Batch  97 / 525  Training Loss  6.0485665017040446e-05\n",
            "Epoch  22 Batch  98 / 525  Training Loss  5.626205893349834e-05\n",
            "Epoch  22 Batch  99 / 525  Training Loss  4.191005791653879e-05\n",
            "Epoch  22 Batch  100 / 525  Training Loss  7.828920206520706e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  22 Batch  101 / 525  Training Loss  4.411549889482558e-05\n",
            "Epoch  22 Batch  102 / 525  Training Loss  5.2063602197449654e-05\n",
            "Epoch  22 Batch  103 / 525  Training Loss  6.232098530745134e-05\n",
            "Epoch  22 Batch  104 / 525  Training Loss  4.286801413400099e-05\n",
            "Epoch  22 Batch  105 / 525  Training Loss  4.66363453597296e-05\n",
            "Epoch  22 Batch  106 / 525  Training Loss  5.0979433581233025e-05\n",
            "Epoch  22 Batch  107 / 525  Training Loss  5.0256097892997786e-05\n",
            "Epoch  22 Batch  108 / 525  Training Loss  5.296896051731892e-05\n",
            "Epoch  22 Batch  109 / 525  Training Loss  4.1126837459160015e-05\n",
            "Epoch  22 Batch  110 / 525  Training Loss  4.757497663376853e-05\n",
            "Epoch  22 Batch  111 / 525  Training Loss  5.073308784631081e-05\n",
            "Epoch  22 Batch  112 / 525  Training Loss  4.4719025027006865e-05\n",
            "Epoch  22 Batch  113 / 525  Training Loss  5.3295912948669866e-05\n",
            "Epoch  22 Batch  114 / 525  Training Loss  5.046492879046127e-05\n",
            "Epoch  22 Batch  115 / 525  Training Loss  4.313638055464253e-05\n",
            "Epoch  22 Batch  116 / 525  Training Loss  6.338847015285864e-05\n",
            "Epoch  22 Batch  117 / 525  Training Loss  5.0446862587705255e-05\n",
            "Epoch  22 Batch  118 / 525  Training Loss  3.969106182921678e-05\n",
            "Epoch  22 Batch  119 / 525  Training Loss  5.0494185416027904e-05\n",
            "Epoch  22 Batch  120 / 525  Training Loss  6.472603126894683e-05\n",
            "Epoch  22 Batch  121 / 525  Training Loss  6.42433442408219e-05\n",
            "Epoch  22 Batch  122 / 525  Training Loss  5.452560799312778e-05\n",
            "Epoch  22 Batch  123 / 525  Training Loss  3.698198270285502e-05\n",
            "Epoch  22 Batch  124 / 525  Training Loss  4.478675327845849e-05\n",
            "Epoch  22 Batch  125 / 525  Training Loss  5.3171988838585094e-05\n",
            "Epoch  22 Batch  126 / 525  Training Loss  4.6874763938831165e-05\n",
            "Epoch  22 Batch  127 / 525  Training Loss  5.49991600564681e-05\n",
            "Epoch  22 Batch  128 / 525  Training Loss  5.3464445954887196e-05\n",
            "Epoch  22 Batch  129 / 525  Training Loss  4.428637112141587e-05\n",
            "Epoch  22 Batch  130 / 525  Training Loss  4.784274642588571e-05\n",
            "Epoch  22 Batch  131 / 525  Training Loss  3.401947469683364e-05\n",
            "Epoch  22 Batch  132 / 525  Training Loss  5.7544937590137124e-05\n",
            "Epoch  22 Batch  133 / 525  Training Loss  5.632466127281077e-05\n",
            "Epoch  22 Batch  134 / 525  Training Loss  5.272226917441003e-05\n",
            "Epoch  22 Batch  135 / 525  Training Loss  6.306710565695539e-05\n",
            "Epoch  22 Batch  136 / 525  Training Loss  6.335502985166386e-05\n",
            "Epoch  22 Batch  137 / 525  Training Loss  5.3287018090486526e-05\n",
            "Epoch  22 Batch  138 / 525  Training Loss  4.298572821426205e-05\n",
            "Epoch  22 Batch  139 / 525  Training Loss  6.887321796966717e-05\n",
            "Epoch  22 Batch  140 / 525  Training Loss  2.9311224352568388e-05\n",
            "Epoch  22 Batch  141 / 525  Training Loss  7.61316332500428e-05\n",
            "Epoch  22 Batch  142 / 525  Training Loss  5.298243559082039e-05\n",
            "Epoch  22 Batch  143 / 525  Training Loss  6.140700861578807e-05\n",
            "Epoch  22 Batch  144 / 525  Training Loss  5.4076088417787105e-05\n",
            "Epoch  22 Batch  145 / 525  Training Loss  5.777212936664e-05\n",
            "Epoch  22 Batch  146 / 525  Training Loss  4.487220576265827e-05\n",
            "Epoch  22 Batch  147 / 525  Training Loss  5.819966827402823e-05\n",
            "Epoch  22 Batch  148 / 525  Training Loss  6.725543789798394e-05\n",
            "Epoch  22 Batch  149 / 525  Training Loss  5.6363198382314295e-05\n",
            "Epoch  22 Batch  150 / 525  Training Loss  5.5488257203251123e-05\n",
            "Epoch  22 Batch  151 / 525  Training Loss  4.1986968426499516e-05\n",
            "Epoch  22 Batch  152 / 525  Training Loss  3.462880340521224e-05\n",
            "Epoch  22 Batch  153 / 525  Training Loss  4.300350701669231e-05\n",
            "Epoch  22 Batch  154 / 525  Training Loss  6.152961577754468e-05\n",
            "Epoch  22 Batch  155 / 525  Training Loss  4.966167762177065e-05\n",
            "Epoch  22 Batch  156 / 525  Training Loss  3.6917670513503253e-05\n",
            "Epoch  22 Batch  157 / 525  Training Loss  3.712287434609607e-05\n",
            "Epoch  22 Batch  158 / 525  Training Loss  7.691530481679365e-05\n",
            "Epoch  22 Batch  159 / 525  Training Loss  5.4262334742816165e-05\n",
            "Epoch  22 Batch  160 / 525  Training Loss  4.942089435644448e-05\n",
            "Epoch  22 Batch  161 / 525  Training Loss  5.398108623921871e-05\n",
            "Epoch  22 Batch  162 / 525  Training Loss  5.359047281672247e-05\n",
            "Epoch  22 Batch  163 / 525  Training Loss  4.17406226915773e-05\n",
            "Epoch  22 Batch  164 / 525  Training Loss  4.111316957278177e-05\n",
            "Epoch  22 Batch  165 / 525  Training Loss  4.7518340579699725e-05\n",
            "Epoch  22 Batch  166 / 525  Training Loss  3.330351319164038e-05\n",
            "Epoch  22 Batch  167 / 525  Training Loss  2.849198608601e-05\n",
            "Epoch  22 Batch  168 / 525  Training Loss  5.5267846619244665e-05\n",
            "Epoch  22 Batch  169 / 525  Training Loss  5.660660826833919e-05\n",
            "Epoch  22 Batch  170 / 525  Training Loss  5.4968830227153376e-05\n",
            "Epoch  22 Batch  171 / 525  Training Loss  3.953487248509191e-05\n",
            "Epoch  22 Batch  172 / 525  Training Loss  4.920906576444395e-05\n",
            "Epoch  22 Batch  173 / 525  Training Loss  4.67547433800064e-05\n",
            "Epoch  22 Batch  174 / 525  Training Loss  4.2139843571931124e-05\n",
            "Epoch  22 Batch  175 / 525  Training Loss  4.3427931814221665e-05\n",
            "Epoch  22 Batch  176 / 525  Training Loss  4.097861528862268e-05\n",
            "Epoch  22 Batch  177 / 525  Training Loss  5.2083854825468734e-05\n",
            "Epoch  22 Batch  178 / 525  Training Loss  4.533030005404726e-05\n",
            "Epoch  22 Batch  179 / 525  Training Loss  4.9812944780569524e-05\n",
            "Epoch  22 Batch  180 / 525  Training Loss  4.487600381253287e-05\n",
            "Epoch  22 Batch  181 / 525  Training Loss  4.686809188569896e-05\n",
            "Epoch  22 Batch  182 / 525  Training Loss  6.300417589955032e-05\n",
            "Epoch  22 Batch  183 / 525  Training Loss  6.129044049885124e-05\n",
            "Epoch  22 Batch  184 / 525  Training Loss  6.376889359671623e-05\n",
            "Epoch  22 Batch  185 / 525  Training Loss  4.672647264669649e-05\n",
            "Epoch  22 Batch  186 / 525  Training Loss  5.3031410061521456e-05\n",
            "Epoch  22 Batch  187 / 525  Training Loss  7.578917575301602e-05\n",
            "Epoch  22 Batch  188 / 525  Training Loss  5.5400003475369886e-05\n",
            "Epoch  22 Batch  189 / 525  Training Loss  4.75098131573759e-05\n",
            "Epoch  22 Batch  190 / 525  Training Loss  4.582531255437061e-05\n",
            "Epoch  22 Batch  191 / 525  Training Loss  4.4802371121477336e-05\n",
            "Epoch  22 Batch  192 / 525  Training Loss  5.5737520597176626e-05\n",
            "Epoch  22 Batch  193 / 525  Training Loss  6.664903776254505e-05\n",
            "Epoch  22 Batch  194 / 525  Training Loss  6.68186039547436e-05\n",
            "Epoch  22 Batch  195 / 525  Training Loss  5.267770029604435e-05\n",
            "Epoch  22 Batch  196 / 525  Training Loss  6.008213313180022e-05\n",
            "Epoch  22 Batch  197 / 525  Training Loss  3.511397517286241e-05\n",
            "Epoch  22 Batch  198 / 525  Training Loss  7.250517955981195e-05\n",
            "Epoch  22 Batch  199 / 525  Training Loss  5.53125973965507e-05\n",
            "Epoch  22 Batch  200 / 525  Training Loss  5.124005110701546e-05\n",
            "Epoch  22 Batch  201 / 525  Training Loss  5.1760183851001784e-05\n",
            "Epoch  22 Batch  202 / 525  Training Loss  5.267518645268865e-05\n",
            "Epoch  22 Batch  203 / 525  Training Loss  5.616613634629175e-05\n",
            "Epoch  22 Batch  204 / 525  Training Loss  5.6181692343670875e-05\n",
            "Epoch  22 Batch  205 / 525  Training Loss  5.594746835413389e-05\n",
            "Epoch  22 Batch  206 / 525  Training Loss  6.065519846742973e-05\n",
            "Epoch  22 Batch  207 / 525  Training Loss  5.8191915741190314e-05\n",
            "Epoch  22 Batch  208 / 525  Training Loss  5.155198596185073e-05\n",
            "Epoch  22 Batch  209 / 525  Training Loss  3.855451359413564e-05\n",
            "Epoch  22 Batch  210 / 525  Training Loss  4.562913454719819e-05\n",
            "Epoch  22 Batch  211 / 525  Training Loss  5.4022435506340116e-05\n",
            "Epoch  22 Batch  212 / 525  Training Loss  4.584343696478754e-05\n",
            "Epoch  22 Batch  213 / 525  Training Loss  5.0826685765059665e-05\n",
            "Epoch  22 Batch  214 / 525  Training Loss  2.970395507873036e-05\n",
            "Epoch  22 Batch  215 / 525  Training Loss  4.040732528665103e-05\n",
            "Epoch  22 Batch  216 / 525  Training Loss  4.273137165000662e-05\n",
            "Epoch  22 Batch  217 / 525  Training Loss  4.3998115870635957e-05\n",
            "Epoch  22 Batch  218 / 525  Training Loss  4.4096326746512204e-05\n",
            "Epoch  22 Batch  219 / 525  Training Loss  5.429558950709179e-05\n",
            "Epoch  22 Batch  220 / 525  Training Loss  6.258428038563579e-05\n",
            "Epoch  22 Batch  221 / 525  Training Loss  5.875930582988076e-05\n",
            "Epoch  22 Batch  222 / 525  Training Loss  3.969318277086131e-05\n",
            "Epoch  22 Batch  223 / 525  Training Loss  5.1270431868033484e-05\n",
            "Epoch  22 Batch  224 / 525  Training Loss  6.377977842930704e-05\n",
            "Epoch  22 Batch  225 / 525  Training Loss  6.502961332444102e-05\n",
            "Epoch  22 Batch  226 / 525  Training Loss  4.634269862435758e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  22 Batch  227 / 525  Training Loss  4.606311995303258e-05\n",
            "Epoch  22 Batch  228 / 525  Training Loss  5.2763080020667985e-05\n",
            "Epoch  22 Batch  229 / 525  Training Loss  5.506907473318279e-05\n",
            "Epoch  22 Batch  230 / 525  Training Loss  4.88797013531439e-05\n",
            "Epoch  22 Batch  231 / 525  Training Loss  5.569161658058874e-05\n",
            "Epoch  22 Batch  232 / 525  Training Loss  3.726422437466681e-05\n",
            "Epoch  22 Batch  233 / 525  Training Loss  5.0302169256610796e-05\n",
            "Epoch  22 Batch  234 / 525  Training Loss  4.707287371275015e-05\n",
            "Epoch  22 Batch  235 / 525  Training Loss  4.561007517622784e-05\n",
            "Epoch  22 Batch  236 / 525  Training Loss  6.449494685512036e-05\n",
            "Epoch  22 Batch  237 / 525  Training Loss  2.092925387842115e-05\n",
            "Epoch  22 Batch  238 / 525  Training Loss  5.869457891094498e-05\n",
            "Epoch  22 Batch  239 / 525  Training Loss  4.711265864898451e-05\n",
            "Epoch  22 Batch  240 / 525  Training Loss  4.954098767484538e-05\n",
            "Epoch  22 Batch  241 / 525  Training Loss  6.360661791404709e-05\n",
            "Epoch  22 Batch  242 / 525  Training Loss  6.015209874021821e-05\n",
            "Epoch  22 Batch  243 / 525  Training Loss  5.1714701839955524e-05\n",
            "Epoch  22 Batch  244 / 525  Training Loss  5.5832577345427126e-05\n",
            "Epoch  22 Batch  245 / 525  Training Loss  5.653786865877919e-05\n",
            "Epoch  22 Batch  246 / 525  Training Loss  6.125839718151838e-05\n",
            "Epoch  22 Batch  247 / 525  Training Loss  6.27147892373614e-05\n",
            "Epoch  22 Batch  248 / 525  Training Loss  7.397434819722548e-05\n",
            "Epoch  22 Batch  249 / 525  Training Loss  5.15887513756752e-05\n",
            "Epoch  22 Batch  250 / 525  Training Loss  3.6525372706819326e-05\n",
            "Epoch  22 Batch  251 / 525  Training Loss  4.800229362444952e-05\n",
            "Epoch  22 Batch  252 / 525  Training Loss  4.9883783503901213e-05\n",
            "Epoch  22 Batch  253 / 525  Training Loss  4.122089012525976e-05\n",
            "Epoch  22 Batch  254 / 525  Training Loss  4.78675719932653e-05\n",
            "Epoch  22 Batch  255 / 525  Training Loss  4.5796150516252965e-05\n",
            "Epoch  22 Batch  256 / 525  Training Loss  4.706527397502214e-05\n",
            "Epoch  22 Batch  257 / 525  Training Loss  6.722239777445793e-05\n",
            "Epoch  22 Batch  258 / 525  Training Loss  6.351798947434872e-05\n",
            "Epoch  22 Batch  259 / 525  Training Loss  5.3040654165670276e-05\n",
            "Epoch  22 Batch  260 / 525  Training Loss  6.813013897044584e-05\n",
            "Epoch  22 Batch  261 / 525  Training Loss  5.4092513892101124e-05\n",
            "Epoch  22 Batch  262 / 525  Training Loss  5.430929741123691e-05\n",
            "Epoch  22 Batch  263 / 525  Training Loss  4.5786182454321533e-05\n",
            "Epoch  22 Batch  264 / 525  Training Loss  4.804372292710468e-05\n",
            "Epoch  22 Batch  265 / 525  Training Loss  4.8311936552636325e-05\n",
            "Epoch  22 Batch  266 / 525  Training Loss  5.872965630260296e-05\n",
            "Epoch  22 Batch  267 / 525  Training Loss  5.425462586572394e-05\n",
            "Epoch  22 Batch  268 / 525  Training Loss  5.6932669394882396e-05\n",
            "Epoch  22 Batch  269 / 525  Training Loss  3.992127312812954e-05\n",
            "Epoch  22 Batch  270 / 525  Training Loss  3.9900165575090796e-05\n",
            "Epoch  22 Batch  271 / 525  Training Loss  6.175260932650417e-05\n",
            "Epoch  22 Batch  272 / 525  Training Loss  5.064930155640468e-05\n",
            "Epoch  22 Batch  273 / 525  Training Loss  6.0555430536624044e-05\n",
            "Epoch  22 Batch  274 / 525  Training Loss  4.3141590140294284e-05\n",
            "Epoch  22 Batch  275 / 525  Training Loss  4.682985309045762e-05\n",
            "Epoch  22 Batch  276 / 525  Training Loss  7.015292794676498e-05\n",
            "Epoch  22 Batch  277 / 525  Training Loss  4.4172003981657326e-05\n",
            "Epoch  22 Batch  278 / 525  Training Loss  5.5690248700557277e-05\n",
            "Epoch  22 Batch  279 / 525  Training Loss  4.265342795406468e-05\n",
            "Epoch  22 Batch  280 / 525  Training Loss  3.7648969737347215e-05\n",
            "Epoch  22 Batch  281 / 525  Training Loss  5.354860331863165e-05\n",
            "Epoch  22 Batch  282 / 525  Training Loss  5.017608782509342e-05\n",
            "Epoch  22 Batch  283 / 525  Training Loss  4.9017857236322016e-05\n",
            "Epoch  22 Batch  284 / 525  Training Loss  4.7401827032445e-05\n",
            "Epoch  22 Batch  285 / 525  Training Loss  4.895166421192698e-05\n",
            "Epoch  22 Batch  286 / 525  Training Loss  5.007189975003712e-05\n",
            "Epoch  22 Batch  287 / 525  Training Loss  6.210313586052507e-05\n",
            "Epoch  22 Batch  288 / 525  Training Loss  4.885155794909224e-05\n",
            "Epoch  22 Batch  289 / 525  Training Loss  5.43422756891232e-05\n",
            "Epoch  22 Batch  290 / 525  Training Loss  4.2992807721020654e-05\n",
            "Epoch  22 Batch  291 / 525  Training Loss  5.253655763226561e-05\n",
            "Epoch  22 Batch  292 / 525  Training Loss  4.029145202366635e-05\n",
            "Epoch  22 Batch  293 / 525  Training Loss  5.0734455726342276e-05\n",
            "Epoch  22 Batch  294 / 525  Training Loss  5.816498742206022e-05\n",
            "Epoch  22 Batch  295 / 525  Training Loss  6.666796980425715e-05\n",
            "Epoch  22 Batch  296 / 525  Training Loss  4.020897176815197e-05\n",
            "Epoch  22 Batch  297 / 525  Training Loss  4.75109918625094e-05\n",
            "Epoch  22 Batch  298 / 525  Training Loss  4.97304426971823e-05\n",
            "Epoch  22 Batch  299 / 525  Training Loss  5.7215413107769564e-05\n",
            "Epoch  22 Batch  300 / 525  Training Loss  3.095939609920606e-05\n",
            "Epoch  22 Batch  301 / 525  Training Loss  3.5416749597061425e-05\n",
            "Epoch  22 Batch  302 / 525  Training Loss  3.238261706428602e-05\n",
            "Epoch  22 Batch  303 / 525  Training Loss  2.798538116621785e-05\n",
            "Epoch  22 Batch  304 / 525  Training Loss  4.212024578009732e-05\n",
            "Epoch  22 Batch  305 / 525  Training Loss  6.137789750937372e-05\n",
            "Epoch  22 Batch  306 / 525  Training Loss  6.997110176598653e-05\n",
            "Epoch  22 Batch  307 / 525  Training Loss  4.7828376409597695e-05\n",
            "Epoch  22 Batch  308 / 525  Training Loss  2.5105051463469863e-05\n",
            "Epoch  22 Batch  309 / 525  Training Loss  5.0610040489118546e-05\n",
            "Epoch  22 Batch  310 / 525  Training Loss  5.0337170250713825e-05\n",
            "Epoch  22 Batch  311 / 525  Training Loss  3.426254988880828e-05\n",
            "Epoch  22 Batch  312 / 525  Training Loss  4.121597157791257e-05\n",
            "Epoch  22 Batch  313 / 525  Training Loss  4.434961738297716e-05\n",
            "Epoch  22 Batch  314 / 525  Training Loss  6.132611451903358e-05\n",
            "Epoch  22 Batch  315 / 525  Training Loss  5.965980744804256e-05\n",
            "Epoch  22 Batch  316 / 525  Training Loss  6.0337399190757424e-05\n",
            "Epoch  22 Batch  317 / 525  Training Loss  5.6382785260211676e-05\n",
            "Epoch  22 Batch  318 / 525  Training Loss  5.53233185200952e-05\n",
            "Epoch  22 Batch  319 / 525  Training Loss  6.329116149572656e-05\n",
            "Epoch  22 Batch  320 / 525  Training Loss  6.112125265644863e-05\n",
            "Epoch  22 Batch  321 / 525  Training Loss  5.4253818234428763e-05\n",
            "Epoch  22 Batch  322 / 525  Training Loss  4.654776057577692e-05\n",
            "Epoch  22 Batch  323 / 525  Training Loss  4.138756048632786e-05\n",
            "Epoch  22 Batch  324 / 525  Training Loss  5.378971036407165e-05\n",
            "Epoch  22 Batch  325 / 525  Training Loss  4.242389331921004e-05\n",
            "Epoch  22 Batch  326 / 525  Training Loss  5.770385541836731e-05\n",
            "Epoch  22 Batch  327 / 525  Training Loss  3.397805994609371e-05\n",
            "Epoch  22 Batch  328 / 525  Training Loss  3.326829755678773e-05\n",
            "Epoch  22 Batch  329 / 525  Training Loss  6.141159974504262e-05\n",
            "Epoch  22 Batch  330 / 525  Training Loss  5.982979200780392e-05\n",
            "Epoch  22 Batch  331 / 525  Training Loss  5.2560772019205615e-05\n",
            "Epoch  22 Batch  332 / 525  Training Loss  6.926001515239477e-05\n",
            "Epoch  22 Batch  333 / 525  Training Loss  4.712459849542938e-05\n",
            "Epoch  22 Batch  334 / 525  Training Loss  7.051890133880079e-05\n",
            "Epoch  22 Batch  335 / 525  Training Loss  4.878624895354733e-05\n",
            "Epoch  22 Batch  336 / 525  Training Loss  5.77630999032408e-05\n",
            "Epoch  22 Batch  337 / 525  Training Loss  5.312504799803719e-05\n",
            "Epoch  22 Batch  338 / 525  Training Loss  5.776438774773851e-05\n",
            "Epoch  22 Batch  339 / 525  Training Loss  4.9610847781877965e-05\n",
            "Epoch  22 Batch  340 / 525  Training Loss  4.118374999961816e-05\n",
            "Epoch  22 Batch  341 / 525  Training Loss  6.0340698837535456e-05\n",
            "Epoch  22 Batch  342 / 525  Training Loss  3.0137362045934424e-05\n",
            "Epoch  22 Batch  343 / 525  Training Loss  3.7497349694604054e-05\n",
            "Epoch  22 Batch  344 / 525  Training Loss  5.022934783482924e-05\n",
            "Epoch  22 Batch  345 / 525  Training Loss  5.231983959674835e-05\n",
            "Epoch  22 Batch  346 / 525  Training Loss  5.5206339311553165e-05\n",
            "Epoch  22 Batch  347 / 525  Training Loss  4.50073421234265e-05\n",
            "Epoch  22 Batch  348 / 525  Training Loss  4.364228880149312e-05\n",
            "Epoch  22 Batch  349 / 525  Training Loss  4.6138909965520725e-05\n",
            "Epoch  22 Batch  350 / 525  Training Loss  3.9711168938083574e-05\n",
            "Epoch  22 Batch  351 / 525  Training Loss  4.4030002754880115e-05\n",
            "Epoch  22 Batch  352 / 525  Training Loss  4.134174741921015e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  22 Batch  353 / 525  Training Loss  5.315592352417298e-05\n",
            "Epoch  22 Batch  354 / 525  Training Loss  4.6088935050647706e-05\n",
            "Epoch  22 Batch  355 / 525  Training Loss  6.428026972571388e-05\n",
            "Epoch  22 Batch  356 / 525  Training Loss  6.525680510094389e-05\n",
            "Epoch  22 Batch  357 / 525  Training Loss  6.840618152637035e-05\n",
            "Epoch  22 Batch  358 / 525  Training Loss  4.654553049476817e-05\n",
            "Epoch  22 Batch  359 / 525  Training Loss  5.1503484428394586e-05\n",
            "Epoch  22 Batch  360 / 525  Training Loss  4.601437831297517e-05\n",
            "Epoch  22 Batch  361 / 525  Training Loss  4.085747059434652e-05\n",
            "Epoch  22 Batch  362 / 525  Training Loss  8.15928797237575e-05\n",
            "Epoch  22 Batch  363 / 525  Training Loss  5.8506364439381287e-05\n",
            "Epoch  22 Batch  364 / 525  Training Loss  4.5282929932000116e-05\n",
            "Epoch  22 Batch  365 / 525  Training Loss  3.331707921461202e-05\n",
            "Epoch  22 Batch  366 / 525  Training Loss  3.9921193092595786e-05\n",
            "Epoch  22 Batch  367 / 525  Training Loss  4.5265529479365796e-05\n",
            "Epoch  22 Batch  368 / 525  Training Loss  4.9700400268193334e-05\n",
            "Epoch  22 Batch  369 / 525  Training Loss  4.7434554289793596e-05\n",
            "Epoch  22 Batch  370 / 525  Training Loss  5.26104231539648e-05\n",
            "Epoch  22 Batch  371 / 525  Training Loss  6.193060107761994e-05\n",
            "Epoch  22 Batch  372 / 525  Training Loss  4.2468367610126734e-05\n",
            "Epoch  22 Batch  373 / 525  Training Loss  6.408451008610427e-05\n",
            "Epoch  22 Batch  374 / 525  Training Loss  4.7046149120433256e-05\n",
            "Epoch  22 Batch  375 / 525  Training Loss  8.153808448696509e-05\n",
            "Epoch  22 Batch  376 / 525  Training Loss  3.876370828947984e-05\n",
            "Epoch  22 Batch  377 / 525  Training Loss  4.860501940129325e-05\n",
            "Epoch  22 Batch  378 / 525  Training Loss  9.133885760093108e-05\n",
            "Epoch  22 Batch  379 / 525  Training Loss  5.91693096794188e-05\n",
            "Epoch  22 Batch  380 / 525  Training Loss  4.764210825669579e-05\n",
            "Epoch  22 Batch  381 / 525  Training Loss  5.709275865228847e-05\n",
            "Epoch  22 Batch  382 / 525  Training Loss  6.161702185636386e-05\n",
            "Epoch  22 Batch  383 / 525  Training Loss  5.519148544408381e-05\n",
            "Epoch  22 Batch  384 / 525  Training Loss  5.422093818197027e-05\n",
            "Epoch  22 Batch  385 / 525  Training Loss  5.232534749666229e-05\n",
            "Epoch  22 Batch  386 / 525  Training Loss  4.241334681864828e-05\n",
            "Epoch  22 Batch  387 / 525  Training Loss  4.785355122294277e-05\n",
            "Epoch  22 Batch  388 / 525  Training Loss  5.637747744913213e-05\n",
            "Epoch  22 Batch  389 / 525  Training Loss  4.6018474677111953e-05\n",
            "Epoch  22 Batch  390 / 525  Training Loss  4.585691203828901e-05\n",
            "Epoch  22 Batch  391 / 525  Training Loss  5.0849990657297894e-05\n",
            "Epoch  22 Batch  392 / 525  Training Loss  6.379867409123108e-05\n",
            "Epoch  22 Batch  393 / 525  Training Loss  5.459500971483067e-05\n",
            "Epoch  22 Batch  394 / 525  Training Loss  3.394114537513815e-05\n",
            "Epoch  22 Batch  395 / 525  Training Loss  4.311952943680808e-05\n",
            "Epoch  22 Batch  396 / 525  Training Loss  5.576236799242906e-05\n",
            "Epoch  22 Batch  397 / 525  Training Loss  3.734652273124084e-05\n",
            "Epoch  22 Batch  398 / 525  Training Loss  4.017115497845225e-05\n",
            "Epoch  22 Batch  399 / 525  Training Loss  4.307908966438845e-05\n",
            "Epoch  22 Batch  400 / 525  Training Loss  5.101311398902908e-05\n",
            "Epoch  22 Batch  401 / 525  Training Loss  4.7613710194127634e-05\n",
            "Epoch  22 Batch  402 / 525  Training Loss  5.153436723048799e-05\n",
            "Epoch  22 Batch  403 / 525  Training Loss  5.7291639677714556e-05\n",
            "Epoch  22 Batch  404 / 525  Training Loss  4.8000794777181e-05\n",
            "Epoch  22 Batch  405 / 525  Training Loss  4.700561839854345e-05\n",
            "Epoch  22 Batch  406 / 525  Training Loss  3.966793155996129e-05\n",
            "Epoch  22 Batch  407 / 525  Training Loss  5.3085113904671744e-05\n",
            "Epoch  22 Batch  408 / 525  Training Loss  5.925452569499612e-05\n",
            "Epoch  22 Batch  409 / 525  Training Loss  4.566417192108929e-05\n",
            "Epoch  22 Batch  410 / 525  Training Loss  4.9456575652584434e-05\n",
            "Epoch  22 Batch  411 / 525  Training Loss  4.856742816627957e-05\n",
            "Epoch  22 Batch  412 / 525  Training Loss  5.6105862313415855e-05\n",
            "Epoch  22 Batch  413 / 525  Training Loss  3.9214748539961874e-05\n",
            "Epoch  22 Batch  414 / 525  Training Loss  4.3528554670047015e-05\n",
            "Epoch  22 Batch  415 / 525  Training Loss  5.8726698625832796e-05\n",
            "Epoch  22 Batch  416 / 525  Training Loss  5.072724889032543e-05\n",
            "Epoch  22 Batch  417 / 525  Training Loss  5.888273517484777e-05\n",
            "Epoch  22 Batch  418 / 525  Training Loss  5.2503193728625774e-05\n",
            "Epoch  22 Batch  419 / 525  Training Loss  3.942249531974085e-05\n",
            "Epoch  22 Batch  420 / 525  Training Loss  6.100651080487296e-05\n",
            "Epoch  22 Batch  421 / 525  Training Loss  3.739749809028581e-05\n",
            "Epoch  22 Batch  422 / 525  Training Loss  7.196151273092255e-05\n",
            "Epoch  22 Batch  423 / 525  Training Loss  5.560122735914774e-05\n",
            "Epoch  22 Batch  424 / 525  Training Loss  5.70121337659657e-05\n",
            "Epoch  22 Batch  425 / 525  Training Loss  4.8555113608017564e-05\n",
            "Epoch  22 Batch  426 / 525  Training Loss  5.7421722885919735e-05\n",
            "Epoch  22 Batch  427 / 525  Training Loss  3.66408348781988e-05\n",
            "Epoch  22 Batch  428 / 525  Training Loss  3.2884414395084605e-05\n",
            "Epoch  22 Batch  429 / 525  Training Loss  6.347549788188189e-05\n",
            "Epoch  22 Batch  430 / 525  Training Loss  5.1904837164329365e-05\n",
            "Epoch  22 Batch  431 / 525  Training Loss  6.139842298580334e-05\n",
            "Epoch  22 Batch  432 / 525  Training Loss  6.335958460113034e-05\n",
            "Epoch  22 Batch  433 / 525  Training Loss  5.110395068186335e-05\n",
            "Epoch  22 Batch  434 / 525  Training Loss  4.8822868848219514e-05\n",
            "Epoch  22 Batch  435 / 525  Training Loss  4.470571002457291e-05\n",
            "Epoch  22 Batch  436 / 525  Training Loss  4.288770287530497e-05\n",
            "Epoch  22 Batch  437 / 525  Training Loss  4.180752512183972e-05\n",
            "Epoch  22 Batch  438 / 525  Training Loss  5.1889790483983234e-05\n",
            "Epoch  22 Batch  439 / 525  Training Loss  5.590229920926504e-05\n",
            "Epoch  22 Batch  440 / 525  Training Loss  6.68622597004287e-05\n",
            "Epoch  22 Batch  441 / 525  Training Loss  4.853778955293819e-05\n",
            "Epoch  22 Batch  442 / 525  Training Loss  4.37647795479279e-05\n",
            "Epoch  22 Batch  443 / 525  Training Loss  3.9547674532514066e-05\n",
            "Epoch  22 Batch  444 / 525  Training Loss  5.759824489359744e-05\n",
            "Epoch  22 Batch  445 / 525  Training Loss  4.501995135797188e-05\n",
            "Epoch  22 Batch  446 / 525  Training Loss  3.633531378000043e-05\n",
            "Epoch  22 Batch  447 / 525  Training Loss  4.0024191548582166e-05\n",
            "Epoch  22 Batch  448 / 525  Training Loss  4.421900666784495e-05\n",
            "Epoch  22 Batch  449 / 525  Training Loss  6.47867564111948e-05\n",
            "Epoch  22 Batch  450 / 525  Training Loss  7.085259858286008e-05\n",
            "Epoch  22 Batch  451 / 525  Training Loss  4.823472772841342e-05\n",
            "Epoch  22 Batch  452 / 525  Training Loss  4.2211642721667886e-05\n",
            "Epoch  22 Batch  453 / 525  Training Loss  4.6145712985889986e-05\n",
            "Epoch  22 Batch  454 / 525  Training Loss  4.117337812203914e-05\n",
            "Epoch  22 Batch  455 / 525  Training Loss  6.629872223129496e-05\n",
            "Epoch  22 Batch  456 / 525  Training Loss  4.177878508926369e-05\n",
            "Epoch  22 Batch  457 / 525  Training Loss  5.179381696507335e-05\n",
            "Epoch  22 Batch  458 / 525  Training Loss  4.6291017497424036e-05\n",
            "Epoch  22 Batch  459 / 525  Training Loss  5.48608077224344e-05\n",
            "Epoch  22 Batch  460 / 525  Training Loss  4.01872384827584e-05\n",
            "Epoch  22 Batch  461 / 525  Training Loss  3.9418318920070305e-05\n",
            "Epoch  22 Batch  462 / 525  Training Loss  5.433176556834951e-05\n",
            "Epoch  22 Batch  463 / 525  Training Loss  6.402294820873067e-05\n",
            "Epoch  22 Batch  464 / 525  Training Loss  4.888145485892892e-05\n",
            "Epoch  22 Batch  465 / 525  Training Loss  5.7978111726697534e-05\n",
            "Epoch  22 Batch  466 / 525  Training Loss  5.691447950084694e-05\n",
            "Epoch  22 Batch  467 / 525  Training Loss  4.253043152857572e-05\n",
            "Epoch  22 Batch  468 / 525  Training Loss  3.8363861676771194e-05\n",
            "Epoch  22 Batch  469 / 525  Training Loss  5.434996637632139e-05\n",
            "Epoch  22 Batch  470 / 525  Training Loss  5.5337535741273314e-05\n",
            "Epoch  22 Batch  471 / 525  Training Loss  3.586606908356771e-05\n",
            "Epoch  22 Batch  472 / 525  Training Loss  4.988112414139323e-05\n",
            "Epoch  22 Batch  473 / 525  Training Loss  4.493286905926652e-05\n",
            "Epoch  22 Batch  474 / 525  Training Loss  4.493762389756739e-05\n",
            "Epoch  22 Batch  475 / 525  Training Loss  4.754135443363339e-05\n",
            "Epoch  22 Batch  476 / 525  Training Loss  5.394695472205058e-05\n",
            "Epoch  22 Batch  477 / 525  Training Loss  4.215969966026023e-05\n",
            "Epoch  22 Batch  478 / 525  Training Loss  4.678997356677428e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  22 Batch  479 / 525  Training Loss  4.8486937885172665e-05\n",
            "Epoch  22 Batch  480 / 525  Training Loss  3.794334770645946e-05\n",
            "Epoch  22 Batch  481 / 525  Training Loss  5.016205977881327e-05\n",
            "Epoch  22 Batch  482 / 525  Training Loss  3.9593134715687484e-05\n",
            "Epoch  22 Batch  483 / 525  Training Loss  6.049801595509052e-05\n",
            "Epoch  22 Batch  484 / 525  Training Loss  3.879897849401459e-05\n",
            "Epoch  22 Batch  485 / 525  Training Loss  3.360873961355537e-05\n",
            "Epoch  22 Batch  486 / 525  Training Loss  4.414232535054907e-05\n",
            "Epoch  22 Batch  487 / 525  Training Loss  4.5114935346646234e-05\n",
            "Epoch  22 Batch  488 / 525  Training Loss  4.3728512537200004e-05\n",
            "Epoch  22 Batch  489 / 525  Training Loss  7.651084888493642e-05\n",
            "Epoch  22 Batch  490 / 525  Training Loss  4.434392394614406e-05\n",
            "Epoch  22 Batch  491 / 525  Training Loss  4.845378134632483e-05\n",
            "Epoch  22 Batch  492 / 525  Training Loss  6.738085357937962e-05\n",
            "Epoch  22 Batch  493 / 525  Training Loss  5.9212150517851114e-05\n",
            "Epoch  22 Batch  494 / 525  Training Loss  5.669709571520798e-05\n",
            "Epoch  22 Batch  495 / 525  Training Loss  4.544247349258512e-05\n",
            "Epoch  22 Batch  496 / 525  Training Loss  4.342568718129769e-05\n",
            "Epoch  22 Batch  497 / 525  Training Loss  5.396929191192612e-05\n",
            "Epoch  22 Batch  498 / 525  Training Loss  4.774639819515869e-05\n",
            "Epoch  22 Batch  499 / 525  Training Loss  4.630230978364125e-05\n",
            "Epoch  22 Batch  500 / 525  Training Loss  4.520141374086961e-05\n",
            "Epoch  22 Batch  501 / 525  Training Loss  4.484258533921093e-05\n",
            "Epoch  22 Batch  502 / 525  Training Loss  5.430589590105228e-05\n",
            "Epoch  22 Batch  503 / 525  Training Loss  3.76318785129115e-05\n",
            "Epoch  22 Batch  504 / 525  Training Loss  5.3544492402579635e-05\n",
            "Epoch  22 Batch  505 / 525  Training Loss  5.202300235396251e-05\n",
            "Epoch  22 Batch  506 / 525  Training Loss  7.349636143771932e-05\n",
            "Epoch  22 Batch  507 / 525  Training Loss  5.963553485344164e-05\n",
            "Epoch  22 Batch  508 / 525  Training Loss  4.536131746135652e-05\n",
            "Epoch  22 Batch  509 / 525  Training Loss  4.241111673763953e-05\n",
            "Epoch  22 Batch  510 / 525  Training Loss  6.805767043260857e-05\n",
            "Epoch  22 Batch  511 / 525  Training Loss  4.6816083340672776e-05\n",
            "Epoch  22 Batch  512 / 525  Training Loss  6.677747296635062e-05\n",
            "Epoch  22 Batch  513 / 525  Training Loss  4.8611065722070634e-05\n",
            "Epoch  22 Batch  514 / 525  Training Loss  8.184339094441384e-05\n",
            "Epoch  22 Batch  515 / 525  Training Loss  3.384247611393221e-05\n",
            "Epoch  22 Batch  516 / 525  Training Loss  4.646389061235823e-05\n",
            "Epoch  22 Batch  517 / 525  Training Loss  6.0489168390631676e-05\n",
            "Epoch  22 Batch  518 / 525  Training Loss  3.495671262498945e-05\n",
            "Epoch  22 Batch  519 / 525  Training Loss  5.269411485642195e-05\n",
            "Epoch  22 Batch  520 / 525  Training Loss  6.675205077044666e-05\n",
            "Epoch  22 Batch  521 / 525  Training Loss  4.0211114537669346e-05\n",
            "Epoch  22 Batch  522 / 525  Training Loss  5.393964238464832e-05\n",
            "Epoch  22 Batch  523 / 525  Training Loss  3.8295223930617794e-05\n",
            "Epoch  22 Batch  524 / 525  Training Loss  3.409167402423918e-05\n",
            "  23    |    -    |   0.000050   |   64.28  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 23\n",
            "Epoch  23 Batch  0 / 525  Training Loss  3.4501965274102986e-05\n",
            "Epoch  23 Batch  1 / 525  Training Loss  2.8236210710019805e-05\n",
            "Epoch  23 Batch  2 / 525  Training Loss  3.807868779404089e-05\n",
            "Epoch  23 Batch  3 / 525  Training Loss  3.3560863812454045e-05\n",
            "Epoch  23 Batch  4 / 525  Training Loss  3.9326892874669284e-05\n",
            "Epoch  23 Batch  5 / 525  Training Loss  3.509131784085184e-05\n",
            "Epoch  23 Batch  6 / 525  Training Loss  4.2292514990549535e-05\n",
            "Epoch  23 Batch  7 / 525  Training Loss  3.6255867598811164e-05\n",
            "Epoch  23 Batch  8 / 525  Training Loss  4.2210805986542255e-05\n",
            "Epoch  23 Batch  9 / 525  Training Loss  4.731718945549801e-05\n",
            "Epoch  23 Batch  10 / 525  Training Loss  4.316481499699876e-05\n",
            "Epoch  23 Batch  11 / 525  Training Loss  4.672205977840349e-05\n",
            "Epoch  23 Batch  12 / 525  Training Loss  4.840434485231526e-05\n",
            "Epoch  23 Batch  13 / 525  Training Loss  5.1581591833382845e-05\n",
            "Epoch  23 Batch  14 / 525  Training Loss  4.5589462388306856e-05\n",
            "Epoch  23 Batch  15 / 525  Training Loss  4.257554974174127e-05\n",
            "Epoch  23 Batch  16 / 525  Training Loss  5.758947736467235e-05\n",
            "Epoch  23 Batch  17 / 525  Training Loss  4.1238494304707274e-05\n",
            "Epoch  23 Batch  18 / 525  Training Loss  5.016491195419803e-05\n",
            "Epoch  23 Batch  19 / 525  Training Loss  3.958405432058498e-05\n",
            "Epoch  23 Batch  20 / 525  Training Loss  4.4445441744755954e-05\n",
            "Epoch  23 Batch  21 / 525  Training Loss  4.0168310079025105e-05\n",
            "Epoch  23 Batch  22 / 525  Training Loss  2.692226553335786e-05\n",
            "Epoch  23 Batch  23 / 525  Training Loss  2.7895483071915805e-05\n",
            "Epoch  23 Batch  24 / 525  Training Loss  4.1077226342167705e-05\n",
            "Epoch  23 Batch  25 / 525  Training Loss  4.45682235294953e-05\n",
            "Epoch  23 Batch  26 / 525  Training Loss  3.4269200114067644e-05\n",
            "Epoch  23 Batch  27 / 525  Training Loss  3.971980549977161e-05\n",
            "Epoch  23 Batch  28 / 525  Training Loss  4.423120844876394e-05\n",
            "Epoch  23 Batch  29 / 525  Training Loss  4.2760086216731e-05\n",
            "Epoch  23 Batch  30 / 525  Training Loss  5.0036316679324955e-05\n",
            "Epoch  23 Batch  31 / 525  Training Loss  3.5927005228586495e-05\n",
            "Epoch  23 Batch  32 / 525  Training Loss  4.650160553865135e-05\n",
            "Epoch  23 Batch  33 / 525  Training Loss  4.949305366608314e-05\n",
            "Epoch  23 Batch  34 / 525  Training Loss  4.863789217779413e-05\n",
            "Epoch  23 Batch  35 / 525  Training Loss  3.4322849387535825e-05\n",
            "Epoch  23 Batch  36 / 525  Training Loss  4.282042573322542e-05\n",
            "Epoch  23 Batch  37 / 525  Training Loss  3.97334988520015e-05\n",
            "Epoch  23 Batch  38 / 525  Training Loss  3.447505150688812e-05\n",
            "Epoch  23 Batch  39 / 525  Training Loss  4.5142252929508686e-05\n",
            "Epoch  23 Batch  40 / 525  Training Loss  5.054028588347137e-05\n",
            "Epoch  23 Batch  41 / 525  Training Loss  4.464592711883597e-05\n",
            "Epoch  23 Batch  42 / 525  Training Loss  5.880192475160584e-05\n",
            "Epoch  23 Batch  43 / 525  Training Loss  4.823398558073677e-05\n",
            "Epoch  23 Batch  44 / 525  Training Loss  3.721260145539418e-05\n",
            "Epoch  23 Batch  45 / 525  Training Loss  4.7619025281164795e-05\n",
            "Epoch  23 Batch  46 / 525  Training Loss  4.326704947743565e-05\n",
            "Epoch  23 Batch  47 / 525  Training Loss  3.4670250897761434e-05\n",
            "Epoch  23 Batch  48 / 525  Training Loss  6.429255881812423e-05\n",
            "Epoch  23 Batch  49 / 525  Training Loss  4.41224365204107e-05\n",
            "Epoch  23 Batch  50 / 525  Training Loss  5.1410392188699916e-05\n",
            "Epoch  23 Batch  51 / 525  Training Loss  4.219419133733027e-05\n",
            "Epoch  23 Batch  52 / 525  Training Loss  3.981576446676627e-05\n",
            "Epoch  23 Batch  53 / 525  Training Loss  5.5281569075305015e-05\n",
            "Epoch  23 Batch  54 / 525  Training Loss  4.0685896237846464e-05\n",
            "Epoch  23 Batch  55 / 525  Training Loss  5.050086110713892e-05\n",
            "Epoch  23 Batch  56 / 525  Training Loss  3.382632712600753e-05\n",
            "Epoch  23 Batch  57 / 525  Training Loss  4.339510269346647e-05\n",
            "Epoch  23 Batch  58 / 525  Training Loss  4.725494727608748e-05\n",
            "Epoch  23 Batch  59 / 525  Training Loss  3.554878639988601e-05\n",
            "Epoch  23 Batch  60 / 525  Training Loss  5.636667992803268e-05\n",
            "Epoch  23 Batch  61 / 525  Training Loss  4.8671434342395514e-05\n",
            "Epoch  23 Batch  62 / 525  Training Loss  4.0445484046358615e-05\n",
            "Epoch  23 Batch  63 / 525  Training Loss  3.668625868158415e-05\n",
            "Epoch  23 Batch  64 / 525  Training Loss  4.833423008676618e-05\n",
            "Epoch  23 Batch  65 / 525  Training Loss  4.1294293623650447e-05\n",
            "Epoch  23 Batch  66 / 525  Training Loss  2.737446629907936e-05\n",
            "Epoch  23 Batch  67 / 525  Training Loss  4.655147495213896e-05\n",
            "Epoch  23 Batch  68 / 525  Training Loss  4.8623212933307514e-05\n",
            "Epoch  23 Batch  69 / 525  Training Loss  4.6759694669162855e-05\n",
            "Epoch  23 Batch  70 / 525  Training Loss  3.626771649578586e-05\n",
            "Epoch  23 Batch  71 / 525  Training Loss  4.4923846871824935e-05\n",
            "Epoch  23 Batch  72 / 525  Training Loss  2.7193204005016014e-05\n",
            "Epoch  23 Batch  73 / 525  Training Loss  5.3099811339052394e-05\n",
            "Epoch  23 Batch  74 / 525  Training Loss  3.320500036352314e-05\n",
            "Epoch  23 Batch  75 / 525  Training Loss  4.28129787906073e-05\n",
            "Epoch  23 Batch  76 / 525  Training Loss  3.091495455009863e-05\n",
            "Epoch  23 Batch  77 / 525  Training Loss  3.599253614083864e-05\n",
            "Epoch  23 Batch  78 / 525  Training Loss  4.546951095107943e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  23 Batch  79 / 525  Training Loss  3.9908845792524517e-05\n",
            "Epoch  23 Batch  80 / 525  Training Loss  4.5395659981295466e-05\n",
            "Epoch  23 Batch  81 / 525  Training Loss  3.6839635868091136e-05\n",
            "Epoch  23 Batch  82 / 525  Training Loss  3.558649405022152e-05\n",
            "Epoch  23 Batch  83 / 525  Training Loss  4.001816705567762e-05\n",
            "Epoch  23 Batch  84 / 525  Training Loss  5.4039323003962636e-05\n",
            "Epoch  23 Batch  85 / 525  Training Loss  3.684506373247132e-05\n",
            "Epoch  23 Batch  86 / 525  Training Loss  4.2699281038949266e-05\n",
            "Epoch  23 Batch  87 / 525  Training Loss  3.7752844946226105e-05\n",
            "Epoch  23 Batch  88 / 525  Training Loss  4.7382942284457386e-05\n",
            "Epoch  23 Batch  89 / 525  Training Loss  5.25478899362497e-05\n",
            "Epoch  23 Batch  90 / 525  Training Loss  5.718399916077033e-05\n",
            "Epoch  23 Batch  91 / 525  Training Loss  3.82760918000713e-05\n",
            "Epoch  23 Batch  92 / 525  Training Loss  6.045678674126975e-05\n",
            "Epoch  23 Batch  93 / 525  Training Loss  3.406245377846062e-05\n",
            "Epoch  23 Batch  94 / 525  Training Loss  4.082161103724502e-05\n",
            "Epoch  23 Batch  95 / 525  Training Loss  3.890336665790528e-05\n",
            "Epoch  23 Batch  96 / 525  Training Loss  4.655797965824604e-05\n",
            "Epoch  23 Batch  97 / 525  Training Loss  3.5952572943642735e-05\n",
            "Epoch  23 Batch  98 / 525  Training Loss  3.116003790637478e-05\n",
            "Epoch  23 Batch  99 / 525  Training Loss  5.2233976020943373e-05\n",
            "Epoch  23 Batch  100 / 525  Training Loss  3.3971271477639675e-05\n",
            "Epoch  23 Batch  101 / 525  Training Loss  4.441606142790988e-05\n",
            "Epoch  23 Batch  102 / 525  Training Loss  3.779156395466998e-05\n",
            "Epoch  23 Batch  103 / 525  Training Loss  4.41101765318308e-05\n",
            "Epoch  23 Batch  104 / 525  Training Loss  3.735167410923168e-05\n",
            "Epoch  23 Batch  105 / 525  Training Loss  4.4120817619841546e-05\n",
            "Epoch  23 Batch  106 / 525  Training Loss  4.224517033435404e-05\n",
            "Epoch  23 Batch  107 / 525  Training Loss  5.68269970244728e-05\n",
            "Epoch  23 Batch  108 / 525  Training Loss  3.5846798709826544e-05\n",
            "Epoch  23 Batch  109 / 525  Training Loss  5.7111476053250954e-05\n",
            "Epoch  23 Batch  110 / 525  Training Loss  3.826957254204899e-05\n",
            "Epoch  23 Batch  111 / 525  Training Loss  4.988160799257457e-05\n",
            "Epoch  23 Batch  112 / 525  Training Loss  3.8847083487780765e-05\n",
            "Epoch  23 Batch  113 / 525  Training Loss  4.499443821259774e-05\n",
            "Epoch  23 Batch  114 / 525  Training Loss  4.9012167437467724e-05\n",
            "Epoch  23 Batch  115 / 525  Training Loss  4.1430957935517654e-05\n",
            "Epoch  23 Batch  116 / 525  Training Loss  4.462284050532617e-05\n",
            "Epoch  23 Batch  117 / 525  Training Loss  4.407703090691939e-05\n",
            "Epoch  23 Batch  118 / 525  Training Loss  4.2587002099025995e-05\n",
            "Epoch  23 Batch  119 / 525  Training Loss  4.968891153112054e-05\n",
            "Epoch  23 Batch  120 / 525  Training Loss  4.8231555410893634e-05\n",
            "Epoch  23 Batch  121 / 525  Training Loss  5.781489744549617e-05\n",
            "Epoch  23 Batch  122 / 525  Training Loss  6.491936801467091e-05\n",
            "Epoch  23 Batch  123 / 525  Training Loss  5.0129798182751983e-05\n",
            "Epoch  23 Batch  124 / 525  Training Loss  5.1970848289784044e-05\n",
            "Epoch  23 Batch  125 / 525  Training Loss  4.092759991181083e-05\n",
            "Epoch  23 Batch  126 / 525  Training Loss  5.3445110097527504e-05\n",
            "Epoch  23 Batch  127 / 525  Training Loss  4.382707993499935e-05\n",
            "Epoch  23 Batch  128 / 525  Training Loss  5.565533137996681e-05\n",
            "Epoch  23 Batch  129 / 525  Training Loss  4.9764457799028605e-05\n",
            "Epoch  23 Batch  130 / 525  Training Loss  4.3869949877262115e-05\n",
            "Epoch  23 Batch  131 / 525  Training Loss  3.430485958233476e-05\n",
            "Epoch  23 Batch  132 / 525  Training Loss  3.6308079870650545e-05\n",
            "Epoch  23 Batch  133 / 525  Training Loss  5.108596451464109e-05\n",
            "Epoch  23 Batch  134 / 525  Training Loss  5.6883211072999984e-05\n",
            "Epoch  23 Batch  135 / 525  Training Loss  4.365042696008459e-05\n",
            "Epoch  23 Batch  136 / 525  Training Loss  5.001164390705526e-05\n",
            "Epoch  23 Batch  137 / 525  Training Loss  4.578594962367788e-05\n",
            "Epoch  23 Batch  138 / 525  Training Loss  3.215996184735559e-05\n",
            "Epoch  23 Batch  139 / 525  Training Loss  4.724316386273131e-05\n",
            "Epoch  23 Batch  140 / 525  Training Loss  4.160844400757924e-05\n",
            "Epoch  23 Batch  141 / 525  Training Loss  4.310146687203087e-05\n",
            "Epoch  23 Batch  142 / 525  Training Loss  3.697595457197167e-05\n",
            "Epoch  23 Batch  143 / 525  Training Loss  3.96269497286994e-05\n",
            "Epoch  23 Batch  144 / 525  Training Loss  4.549217555904761e-05\n",
            "Epoch  23 Batch  145 / 525  Training Loss  6.811840285081416e-05\n",
            "Epoch  23 Batch  146 / 525  Training Loss  4.362659092294052e-05\n",
            "Epoch  23 Batch  147 / 525  Training Loss  3.726586146513e-05\n",
            "Epoch  23 Batch  148 / 525  Training Loss  3.0604947824031115e-05\n",
            "Epoch  23 Batch  149 / 525  Training Loss  3.362187271704897e-05\n",
            "Epoch  23 Batch  150 / 525  Training Loss  5.5797914683353156e-05\n",
            "Epoch  23 Batch  151 / 525  Training Loss  3.652323721325956e-05\n",
            "Epoch  23 Batch  152 / 525  Training Loss  4.3222269596299157e-05\n",
            "Epoch  23 Batch  153 / 525  Training Loss  5.6021806813078e-05\n",
            "Epoch  23 Batch  154 / 525  Training Loss  4.567168798530474e-05\n",
            "Epoch  23 Batch  155 / 525  Training Loss  5.1187445933464915e-05\n",
            "Epoch  23 Batch  156 / 525  Training Loss  4.4958022044738755e-05\n",
            "Epoch  23 Batch  157 / 525  Training Loss  4.53052889497485e-05\n",
            "Epoch  23 Batch  158 / 525  Training Loss  2.865853275579866e-05\n",
            "Epoch  23 Batch  159 / 525  Training Loss  4.998433723812923e-05\n",
            "Epoch  23 Batch  160 / 525  Training Loss  3.582139470381662e-05\n",
            "Epoch  23 Batch  161 / 525  Training Loss  4.328379509388469e-05\n",
            "Epoch  23 Batch  162 / 525  Training Loss  4.504418029682711e-05\n",
            "Epoch  23 Batch  163 / 525  Training Loss  6.119406316429377e-05\n",
            "Epoch  23 Batch  164 / 525  Training Loss  5.14775856572669e-05\n",
            "Epoch  23 Batch  165 / 525  Training Loss  2.900753497669939e-05\n",
            "Epoch  23 Batch  166 / 525  Training Loss  4.491184154176153e-05\n",
            "Epoch  23 Batch  167 / 525  Training Loss  4.5555363612947986e-05\n",
            "Epoch  23 Batch  168 / 525  Training Loss  5.300237535266206e-05\n",
            "Epoch  23 Batch  169 / 525  Training Loss  4.6122673666104674e-05\n",
            "Epoch  23 Batch  170 / 525  Training Loss  3.549824032234028e-05\n",
            "Epoch  23 Batch  171 / 525  Training Loss  3.723810368683189e-05\n",
            "Epoch  23 Batch  172 / 525  Training Loss  4.554837869363837e-05\n",
            "Epoch  23 Batch  173 / 525  Training Loss  4.418711978360079e-05\n",
            "Epoch  23 Batch  174 / 525  Training Loss  4.780080053023994e-05\n",
            "Epoch  23 Batch  175 / 525  Training Loss  3.9282345824176446e-05\n",
            "Epoch  23 Batch  176 / 525  Training Loss  5.1027869631070644e-05\n",
            "Epoch  23 Batch  177 / 525  Training Loss  3.9463138818973675e-05\n",
            "Epoch  23 Batch  178 / 525  Training Loss  4.67437130282633e-05\n",
            "Epoch  23 Batch  179 / 525  Training Loss  4.651393464882858e-05\n",
            "Epoch  23 Batch  180 / 525  Training Loss  4.103922037757002e-05\n",
            "Epoch  23 Batch  181 / 525  Training Loss  3.81632344215177e-05\n",
            "Epoch  23 Batch  182 / 525  Training Loss  6.0108934121672064e-05\n",
            "Epoch  23 Batch  183 / 525  Training Loss  3.8282287277979776e-05\n",
            "Epoch  23 Batch  184 / 525  Training Loss  4.9958471208810806e-05\n",
            "Epoch  23 Batch  185 / 525  Training Loss  3.601447679102421e-05\n",
            "Epoch  23 Batch  186 / 525  Training Loss  5.553529626922682e-05\n",
            "Epoch  23 Batch  187 / 525  Training Loss  3.227167690056376e-05\n",
            "Epoch  23 Batch  188 / 525  Training Loss  4.816437649424188e-05\n",
            "Epoch  23 Batch  189 / 525  Training Loss  4.05117025366053e-05\n",
            "Epoch  23 Batch  190 / 525  Training Loss  4.0974166040541604e-05\n",
            "Epoch  23 Batch  191 / 525  Training Loss  3.7297468225006014e-05\n",
            "Epoch  23 Batch  192 / 525  Training Loss  3.6708930565509945e-05\n",
            "Epoch  23 Batch  193 / 525  Training Loss  4.11703840654809e-05\n",
            "Epoch  23 Batch  194 / 525  Training Loss  3.5506003769114614e-05\n",
            "Epoch  23 Batch  195 / 525  Training Loss  4.912979784421623e-05\n",
            "Epoch  23 Batch  196 / 525  Training Loss  4.3915410060435534e-05\n",
            "Epoch  23 Batch  197 / 525  Training Loss  5.1207560318289325e-05\n",
            "Epoch  23 Batch  198 / 525  Training Loss  3.949330857722089e-05\n",
            "Epoch  23 Batch  199 / 525  Training Loss  4.736481423606165e-05\n",
            "Epoch  23 Batch  200 / 525  Training Loss  3.816159733105451e-05\n",
            "Epoch  23 Batch  201 / 525  Training Loss  4.5196829887572676e-05\n",
            "Epoch  23 Batch  202 / 525  Training Loss  3.1770174246048555e-05\n",
            "Epoch  23 Batch  203 / 525  Training Loss  4.245570016792044e-05\n",
            "Epoch  23 Batch  204 / 525  Training Loss  3.8351725379470736e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  23 Batch  205 / 525  Training Loss  4.0301954868482426e-05\n",
            "Epoch  23 Batch  206 / 525  Training Loss  6.36385811958462e-05\n",
            "Epoch  23 Batch  207 / 525  Training Loss  4.872433783020824e-05\n",
            "Epoch  23 Batch  208 / 525  Training Loss  5.025550490245223e-05\n",
            "Epoch  23 Batch  209 / 525  Training Loss  4.226074815960601e-05\n",
            "Epoch  23 Batch  210 / 525  Training Loss  4.3194962927373126e-05\n",
            "Epoch  23 Batch  211 / 525  Training Loss  3.9985116018215194e-05\n",
            "Epoch  23 Batch  212 / 525  Training Loss  4.106811684323475e-05\n",
            "Epoch  23 Batch  213 / 525  Training Loss  4.911767246085219e-05\n",
            "Epoch  23 Batch  214 / 525  Training Loss  4.348619404481724e-05\n",
            "Epoch  23 Batch  215 / 525  Training Loss  4.188619277556427e-05\n",
            "Epoch  23 Batch  216 / 525  Training Loss  5.465048525366001e-05\n",
            "Epoch  23 Batch  217 / 525  Training Loss  4.037879261886701e-05\n",
            "Epoch  23 Batch  218 / 525  Training Loss  3.2724437915021554e-05\n",
            "Epoch  23 Batch  219 / 525  Training Loss  5.881631659576669e-05\n",
            "Epoch  23 Batch  220 / 525  Training Loss  6.515829591080546e-05\n",
            "Epoch  23 Batch  221 / 525  Training Loss  5.131844591232948e-05\n",
            "Epoch  23 Batch  222 / 525  Training Loss  3.474159530014731e-05\n",
            "Epoch  23 Batch  223 / 525  Training Loss  5.1199353038100526e-05\n",
            "Epoch  23 Batch  224 / 525  Training Loss  3.949724850826897e-05\n",
            "Epoch  23 Batch  225 / 525  Training Loss  3.901001036865637e-05\n",
            "Epoch  23 Batch  226 / 525  Training Loss  3.356085653649643e-05\n",
            "Epoch  23 Batch  227 / 525  Training Loss  5.227584551903419e-05\n",
            "Epoch  23 Batch  228 / 525  Training Loss  5.0524518883321434e-05\n",
            "Epoch  23 Batch  229 / 525  Training Loss  3.683221075334586e-05\n",
            "Epoch  23 Batch  230 / 525  Training Loss  4.335370613262057e-05\n",
            "Epoch  23 Batch  231 / 525  Training Loss  3.8704303733538836e-05\n",
            "Epoch  23 Batch  232 / 525  Training Loss  3.9864964492153376e-05\n",
            "Epoch  23 Batch  233 / 525  Training Loss  4.999070006306283e-05\n",
            "Epoch  23 Batch  234 / 525  Training Loss  4.080948929185979e-05\n",
            "Epoch  23 Batch  235 / 525  Training Loss  6.071071402402595e-05\n",
            "Epoch  23 Batch  236 / 525  Training Loss  4.3715415813494474e-05\n",
            "Epoch  23 Batch  237 / 525  Training Loss  4.665980668505654e-05\n",
            "Epoch  23 Batch  238 / 525  Training Loss  4.761573291034438e-05\n",
            "Epoch  23 Batch  239 / 525  Training Loss  3.649177233455703e-05\n",
            "Epoch  23 Batch  240 / 525  Training Loss  4.8299054469680414e-05\n",
            "Epoch  23 Batch  241 / 525  Training Loss  4.085269756615162e-05\n",
            "Epoch  23 Batch  242 / 525  Training Loss  4.5758799387840554e-05\n",
            "Epoch  23 Batch  243 / 525  Training Loss  4.963371975463815e-05\n",
            "Epoch  23 Batch  244 / 525  Training Loss  4.1603016143199056e-05\n",
            "Epoch  23 Batch  245 / 525  Training Loss  4.3180403736187145e-05\n",
            "Epoch  23 Batch  246 / 525  Training Loss  4.4514577894005924e-05\n",
            "Epoch  23 Batch  247 / 525  Training Loss  5.216793579165824e-05\n",
            "Epoch  23 Batch  248 / 525  Training Loss  4.787980651599355e-05\n",
            "Epoch  23 Batch  249 / 525  Training Loss  4.3899384763790295e-05\n",
            "Epoch  23 Batch  250 / 525  Training Loss  5.7323515648022294e-05\n",
            "Epoch  23 Batch  251 / 525  Training Loss  5.160390355740674e-05\n",
            "Epoch  23 Batch  252 / 525  Training Loss  3.576662129489705e-05\n",
            "Epoch  23 Batch  253 / 525  Training Loss  4.3351927160983905e-05\n",
            "Epoch  23 Batch  254 / 525  Training Loss  3.1896503060124815e-05\n",
            "Epoch  23 Batch  255 / 525  Training Loss  4.804580385098234e-05\n",
            "Epoch  23 Batch  256 / 525  Training Loss  4.9091660912381485e-05\n",
            "Epoch  23 Batch  257 / 525  Training Loss  5.5503565818071365e-05\n",
            "Epoch  23 Batch  258 / 525  Training Loss  7.73676874814555e-05\n",
            "Epoch  23 Batch  259 / 525  Training Loss  4.680282290792093e-05\n",
            "Epoch  23 Batch  260 / 525  Training Loss  5.29260651092045e-05\n",
            "Epoch  23 Batch  261 / 525  Training Loss  4.594925121637061e-05\n",
            "Epoch  23 Batch  262 / 525  Training Loss  6.035807746229693e-05\n",
            "Epoch  23 Batch  263 / 525  Training Loss  4.798483132617548e-05\n",
            "Epoch  23 Batch  264 / 525  Training Loss  4.3925396312261e-05\n",
            "Epoch  23 Batch  265 / 525  Training Loss  5.520482227439061e-05\n",
            "Epoch  23 Batch  266 / 525  Training Loss  3.35307267960161e-05\n",
            "Epoch  23 Batch  267 / 525  Training Loss  3.4921016776934266e-05\n",
            "Epoch  23 Batch  268 / 525  Training Loss  4.470327257877216e-05\n",
            "Epoch  23 Batch  269 / 525  Training Loss  3.719004598679021e-05\n",
            "Epoch  23 Batch  270 / 525  Training Loss  2.467495869495906e-05\n",
            "Epoch  23 Batch  271 / 525  Training Loss  3.682782335090451e-05\n",
            "Epoch  23 Batch  272 / 525  Training Loss  5.4204418120207265e-05\n",
            "Epoch  23 Batch  273 / 525  Training Loss  5.752062861574814e-05\n",
            "Epoch  23 Batch  274 / 525  Training Loss  4.4640466512646526e-05\n",
            "Epoch  23 Batch  275 / 525  Training Loss  3.346020821481943e-05\n",
            "Epoch  23 Batch  276 / 525  Training Loss  3.4315082302782685e-05\n",
            "Epoch  23 Batch  277 / 525  Training Loss  4.419661854626611e-05\n",
            "Epoch  23 Batch  278 / 525  Training Loss  4.2320520151406527e-05\n",
            "Epoch  23 Batch  279 / 525  Training Loss  7.03561381669715e-05\n",
            "Epoch  23 Batch  280 / 525  Training Loss  2.889597453759052e-05\n",
            "Epoch  23 Batch  281 / 525  Training Loss  2.8980361093999818e-05\n",
            "Epoch  23 Batch  282 / 525  Training Loss  2.9630948120029643e-05\n",
            "Epoch  23 Batch  283 / 525  Training Loss  4.243553848937154e-05\n",
            "Epoch  23 Batch  284 / 525  Training Loss  3.518980156513862e-05\n",
            "Epoch  23 Batch  285 / 525  Training Loss  4.4339474698062986e-05\n",
            "Epoch  23 Batch  286 / 525  Training Loss  4.555569466901943e-05\n",
            "Epoch  23 Batch  287 / 525  Training Loss  4.266322866897099e-05\n",
            "Epoch  23 Batch  288 / 525  Training Loss  4.155083297519013e-05\n",
            "Epoch  23 Batch  289 / 525  Training Loss  3.962689152103849e-05\n",
            "Epoch  23 Batch  290 / 525  Training Loss  5.0218222895637155e-05\n",
            "Epoch  23 Batch  291 / 525  Training Loss  3.5699289583135396e-05\n",
            "Epoch  23 Batch  292 / 525  Training Loss  3.418589403736405e-05\n",
            "Epoch  23 Batch  293 / 525  Training Loss  3.298045339761302e-05\n",
            "Epoch  23 Batch  294 / 525  Training Loss  5.222343679633923e-05\n",
            "Epoch  23 Batch  295 / 525  Training Loss  3.363235737197101e-05\n",
            "Epoch  23 Batch  296 / 525  Training Loss  3.700561137520708e-05\n",
            "Epoch  23 Batch  297 / 525  Training Loss  5.4355950851459056e-05\n",
            "Epoch  23 Batch  298 / 525  Training Loss  3.4722852433333173e-05\n",
            "Epoch  23 Batch  299 / 525  Training Loss  4.801231625606306e-05\n",
            "Epoch  23 Batch  300 / 525  Training Loss  4.3357205868232995e-05\n",
            "Epoch  23 Batch  301 / 525  Training Loss  3.707034557010047e-05\n",
            "Epoch  23 Batch  302 / 525  Training Loss  4.270624413038604e-05\n",
            "Epoch  23 Batch  303 / 525  Training Loss  4.778674338012934e-05\n",
            "Epoch  23 Batch  304 / 525  Training Loss  4.595299833454192e-05\n",
            "Epoch  23 Batch  305 / 525  Training Loss  4.855196311837062e-05\n",
            "Epoch  23 Batch  306 / 525  Training Loss  4.806648939847946e-05\n",
            "Epoch  23 Batch  307 / 525  Training Loss  4.541593443718739e-05\n",
            "Epoch  23 Batch  308 / 525  Training Loss  4.0791219362290576e-05\n",
            "Epoch  23 Batch  309 / 525  Training Loss  5.432659963844344e-05\n",
            "Epoch  23 Batch  310 / 525  Training Loss  4.891741627943702e-05\n",
            "Epoch  23 Batch  311 / 525  Training Loss  3.765240762731992e-05\n",
            "Epoch  23 Batch  312 / 525  Training Loss  6.339915125863627e-05\n",
            "Epoch  23 Batch  313 / 525  Training Loss  4.847558375331573e-05\n",
            "Epoch  23 Batch  314 / 525  Training Loss  5.449023956316523e-05\n",
            "Epoch  23 Batch  315 / 525  Training Loss  4.326042108004913e-05\n",
            "Epoch  23 Batch  316 / 525  Training Loss  5.54920407012105e-05\n",
            "Epoch  23 Batch  317 / 525  Training Loss  3.390602796571329e-05\n",
            "Epoch  23 Batch  318 / 525  Training Loss  3.9260445191757753e-05\n",
            "Epoch  23 Batch  319 / 525  Training Loss  4.7851466661086306e-05\n",
            "Epoch  23 Batch  320 / 525  Training Loss  4.386012369650416e-05\n",
            "Epoch  23 Batch  321 / 525  Training Loss  4.760600859299302e-05\n",
            "Epoch  23 Batch  322 / 525  Training Loss  4.601090040523559e-05\n",
            "Epoch  23 Batch  323 / 525  Training Loss  3.643222225946374e-05\n",
            "Epoch  23 Batch  324 / 525  Training Loss  3.0172543119988404e-05\n",
            "Epoch  23 Batch  325 / 525  Training Loss  5.827683344250545e-05\n",
            "Epoch  23 Batch  326 / 525  Training Loss  4.3199055653531104e-05\n",
            "Epoch  23 Batch  327 / 525  Training Loss  4.7425925004063174e-05\n",
            "Epoch  23 Batch  328 / 525  Training Loss  5.742824941989966e-05\n",
            "Epoch  23 Batch  329 / 525  Training Loss  5.2484254410956055e-05\n",
            "Epoch  23 Batch  330 / 525  Training Loss  3.759578612516634e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  23 Batch  331 / 525  Training Loss  4.214110231259838e-05\n",
            "Epoch  23 Batch  332 / 525  Training Loss  4.9645470426185057e-05\n",
            "Epoch  23 Batch  333 / 525  Training Loss  4.6620458306279033e-05\n",
            "Epoch  23 Batch  334 / 525  Training Loss  3.468245267868042e-05\n",
            "Epoch  23 Batch  335 / 525  Training Loss  3.922131872968748e-05\n",
            "Epoch  23 Batch  336 / 525  Training Loss  4.696224277722649e-05\n",
            "Epoch  23 Batch  337 / 525  Training Loss  3.539030876709148e-05\n",
            "Epoch  23 Batch  338 / 525  Training Loss  3.9792699681129307e-05\n",
            "Epoch  23 Batch  339 / 525  Training Loss  3.819544144789688e-05\n",
            "Epoch  23 Batch  340 / 525  Training Loss  4.564269329421222e-05\n",
            "Epoch  23 Batch  341 / 525  Training Loss  3.7601450458168983e-05\n",
            "Epoch  23 Batch  342 / 525  Training Loss  6.846695760032162e-05\n",
            "Epoch  23 Batch  343 / 525  Training Loss  4.408340100781061e-05\n",
            "Epoch  23 Batch  344 / 525  Training Loss  2.965203566418495e-05\n",
            "Epoch  23 Batch  345 / 525  Training Loss  5.676932050846517e-05\n",
            "Epoch  23 Batch  346 / 525  Training Loss  5.1016522775171325e-05\n",
            "Epoch  23 Batch  347 / 525  Training Loss  5.4722528147976846e-05\n",
            "Epoch  23 Batch  348 / 525  Training Loss  3.826025567832403e-05\n",
            "Epoch  23 Batch  349 / 525  Training Loss  3.655099862953648e-05\n",
            "Epoch  23 Batch  350 / 525  Training Loss  3.852404552162625e-05\n",
            "Epoch  23 Batch  351 / 525  Training Loss  6.042833410901949e-05\n",
            "Epoch  23 Batch  352 / 525  Training Loss  3.980574547313154e-05\n",
            "Epoch  23 Batch  353 / 525  Training Loss  5.296015297062695e-05\n",
            "Epoch  23 Batch  354 / 525  Training Loss  4.116422132938169e-05\n",
            "Epoch  23 Batch  355 / 525  Training Loss  4.3613305024337023e-05\n",
            "Epoch  23 Batch  356 / 525  Training Loss  3.405692405067384e-05\n",
            "Epoch  23 Batch  357 / 525  Training Loss  3.3907665056176484e-05\n",
            "Epoch  23 Batch  358 / 525  Training Loss  5.333942317520268e-05\n",
            "Epoch  23 Batch  359 / 525  Training Loss  4.262981019564904e-05\n",
            "Epoch  23 Batch  360 / 525  Training Loss  3.712041143444367e-05\n",
            "Epoch  23 Batch  361 / 525  Training Loss  3.922503674402833e-05\n",
            "Epoch  23 Batch  362 / 525  Training Loss  4.434370202943683e-05\n",
            "Epoch  23 Batch  363 / 525  Training Loss  3.141540219075978e-05\n",
            "Epoch  23 Batch  364 / 525  Training Loss  4.2359661165392026e-05\n",
            "Epoch  23 Batch  365 / 525  Training Loss  4.378126686788164e-05\n",
            "Epoch  23 Batch  366 / 525  Training Loss  4.300574801163748e-05\n",
            "Epoch  23 Batch  367 / 525  Training Loss  4.234031439409591e-05\n",
            "Epoch  23 Batch  368 / 525  Training Loss  4.817579247173853e-05\n",
            "Epoch  23 Batch  369 / 525  Training Loss  4.293764504836872e-05\n",
            "Epoch  23 Batch  370 / 525  Training Loss  3.4217773645650595e-05\n",
            "Epoch  23 Batch  371 / 525  Training Loss  4.381879261927679e-05\n",
            "Epoch  23 Batch  372 / 525  Training Loss  4.211239502183162e-05\n",
            "Epoch  23 Batch  373 / 525  Training Loss  5.811133814859204e-05\n",
            "Epoch  23 Batch  374 / 525  Training Loss  5.370588041841984e-05\n",
            "Epoch  23 Batch  375 / 525  Training Loss  3.763613131013699e-05\n",
            "Epoch  23 Batch  376 / 525  Training Loss  3.828912667813711e-05\n",
            "Epoch  23 Batch  377 / 525  Training Loss  4.5469503675121814e-05\n",
            "Epoch  23 Batch  378 / 525  Training Loss  3.8052385207265615e-05\n",
            "Epoch  23 Batch  379 / 525  Training Loss  2.679133649508003e-05\n",
            "Epoch  23 Batch  380 / 525  Training Loss  4.280711436877027e-05\n",
            "Epoch  23 Batch  381 / 525  Training Loss  3.319320239825174e-05\n",
            "Epoch  23 Batch  382 / 525  Training Loss  4.05575628974475e-05\n",
            "Epoch  23 Batch  383 / 525  Training Loss  4.5022756239632145e-05\n",
            "Epoch  23 Batch  384 / 525  Training Loss  5.35410872544162e-05\n",
            "Epoch  23 Batch  385 / 525  Training Loss  5.460280226543546e-05\n",
            "Epoch  23 Batch  386 / 525  Training Loss  4.965746120433323e-05\n",
            "Epoch  23 Batch  387 / 525  Training Loss  4.560845991363749e-05\n",
            "Epoch  23 Batch  388 / 525  Training Loss  3.7291814805939794e-05\n",
            "Epoch  23 Batch  389 / 525  Training Loss  4.0028760849963874e-05\n",
            "Epoch  23 Batch  390 / 525  Training Loss  4.438345058588311e-05\n",
            "Epoch  23 Batch  391 / 525  Training Loss  4.3321819248376414e-05\n",
            "Epoch  23 Batch  392 / 525  Training Loss  4.3388339690864086e-05\n",
            "Epoch  23 Batch  393 / 525  Training Loss  3.9049089537002146e-05\n",
            "Epoch  23 Batch  394 / 525  Training Loss  4.712775262305513e-05\n",
            "Epoch  23 Batch  395 / 525  Training Loss  3.50759582943283e-05\n",
            "Epoch  23 Batch  396 / 525  Training Loss  3.302574623376131e-05\n",
            "Epoch  23 Batch  397 / 525  Training Loss  3.679522706079297e-05\n",
            "Epoch  23 Batch  398 / 525  Training Loss  4.349443770479411e-05\n",
            "Epoch  23 Batch  399 / 525  Training Loss  3.6960562283638865e-05\n",
            "Epoch  23 Batch  400 / 525  Training Loss  3.360673144925386e-05\n",
            "Epoch  23 Batch  401 / 525  Training Loss  4.3373704102123156e-05\n",
            "Epoch  23 Batch  402 / 525  Training Loss  4.073081800015643e-05\n",
            "Epoch  23 Batch  403 / 525  Training Loss  2.0569146727211773e-05\n",
            "Epoch  23 Batch  404 / 525  Training Loss  3.0915856768842787e-05\n",
            "Epoch  23 Batch  405 / 525  Training Loss  4.4172691559651867e-05\n",
            "Epoch  23 Batch  406 / 525  Training Loss  4.282070949557237e-05\n",
            "Epoch  23 Batch  407 / 525  Training Loss  4.5452878111973405e-05\n",
            "Epoch  23 Batch  408 / 525  Training Loss  3.830239438684657e-05\n",
            "Epoch  23 Batch  409 / 525  Training Loss  4.467751568881795e-05\n",
            "Epoch  23 Batch  410 / 525  Training Loss  3.849729910143651e-05\n",
            "Epoch  23 Batch  411 / 525  Training Loss  4.450716733117588e-05\n",
            "Epoch  23 Batch  412 / 525  Training Loss  4.869244367000647e-05\n",
            "Epoch  23 Batch  413 / 525  Training Loss  4.0932070987764746e-05\n",
            "Epoch  23 Batch  414 / 525  Training Loss  3.491207098704763e-05\n",
            "Epoch  23 Batch  415 / 525  Training Loss  4.686251486418769e-05\n",
            "Epoch  23 Batch  416 / 525  Training Loss  4.5299329940462485e-05\n",
            "Epoch  23 Batch  417 / 525  Training Loss  3.777754682232626e-05\n",
            "Epoch  23 Batch  418 / 525  Training Loss  5.087898898636922e-05\n",
            "Epoch  23 Batch  419 / 525  Training Loss  4.219834227114916e-05\n",
            "Epoch  23 Batch  420 / 525  Training Loss  4.368558802525513e-05\n",
            "Epoch  23 Batch  421 / 525  Training Loss  3.5744847991736606e-05\n",
            "Epoch  23 Batch  422 / 525  Training Loss  4.2419669625815004e-05\n",
            "Epoch  23 Batch  423 / 525  Training Loss  3.554202339728363e-05\n",
            "Epoch  23 Batch  424 / 525  Training Loss  4.3756179366027936e-05\n",
            "Epoch  23 Batch  425 / 525  Training Loss  4.2489420593483374e-05\n",
            "Epoch  23 Batch  426 / 525  Training Loss  4.1595536458771676e-05\n",
            "Epoch  23 Batch  427 / 525  Training Loss  4.673036892199889e-05\n",
            "Epoch  23 Batch  428 / 525  Training Loss  4.3633604946080595e-05\n",
            "Epoch  23 Batch  429 / 525  Training Loss  3.331894185976125e-05\n",
            "Epoch  23 Batch  430 / 525  Training Loss  2.8972639483981766e-05\n",
            "Epoch  23 Batch  431 / 525  Training Loss  4.0854203689377755e-05\n",
            "Epoch  23 Batch  432 / 525  Training Loss  3.7823985621798784e-05\n",
            "Epoch  23 Batch  433 / 525  Training Loss  5.603175304713659e-05\n",
            "Epoch  23 Batch  434 / 525  Training Loss  3.480999657767825e-05\n",
            "Epoch  23 Batch  435 / 525  Training Loss  5.142371082911268e-05\n",
            "Epoch  23 Batch  436 / 525  Training Loss  3.809112968156114e-05\n",
            "Epoch  23 Batch  437 / 525  Training Loss  4.451479617273435e-05\n",
            "Epoch  23 Batch  438 / 525  Training Loss  4.595237987814471e-05\n",
            "Epoch  23 Batch  439 / 525  Training Loss  5.926023004576564e-05\n",
            "Epoch  23 Batch  440 / 525  Training Loss  3.900906449416652e-05\n",
            "Epoch  23 Batch  441 / 525  Training Loss  4.302351953811012e-05\n",
            "Epoch  23 Batch  442 / 525  Training Loss  4.6847846533637494e-05\n",
            "Epoch  23 Batch  443 / 525  Training Loss  5.8942892792401835e-05\n",
            "Epoch  23 Batch  444 / 525  Training Loss  3.973471393692307e-05\n",
            "Epoch  23 Batch  445 / 525  Training Loss  3.5264842153992504e-05\n",
            "Epoch  23 Batch  446 / 525  Training Loss  4.963258470525034e-05\n",
            "Epoch  23 Batch  447 / 525  Training Loss  4.481854921323247e-05\n",
            "Epoch  23 Batch  448 / 525  Training Loss  3.7673289625672624e-05\n",
            "Epoch  23 Batch  449 / 525  Training Loss  4.936938057653606e-05\n",
            "Epoch  23 Batch  450 / 525  Training Loss  4.695703319157474e-05\n",
            "Epoch  23 Batch  451 / 525  Training Loss  3.633839151007123e-05\n",
            "Epoch  23 Batch  452 / 525  Training Loss  4.9598274927120656e-05\n",
            "Epoch  23 Batch  453 / 525  Training Loss  3.947360528400168e-05\n",
            "Epoch  23 Batch  454 / 525  Training Loss  4.2388106521684676e-05\n",
            "Epoch  23 Batch  455 / 525  Training Loss  2.6235840778099373e-05\n",
            "Epoch  23 Batch  456 / 525  Training Loss  4.954777978127822e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  23 Batch  457 / 525  Training Loss  4.079904829268344e-05\n",
            "Epoch  23 Batch  458 / 525  Training Loss  3.30659095197916e-05\n",
            "Epoch  23 Batch  459 / 525  Training Loss  5.3996969654690474e-05\n",
            "Epoch  23 Batch  460 / 525  Training Loss  4.519025605986826e-05\n",
            "Epoch  23 Batch  461 / 525  Training Loss  3.702673711813986e-05\n",
            "Epoch  23 Batch  462 / 525  Training Loss  4.390470712678507e-05\n",
            "Epoch  23 Batch  463 / 525  Training Loss  3.6111687222728506e-05\n",
            "Epoch  23 Batch  464 / 525  Training Loss  4.757438728120178e-05\n",
            "Epoch  23 Batch  465 / 525  Training Loss  4.7881439968477935e-05\n",
            "Epoch  23 Batch  466 / 525  Training Loss  5.660306851495989e-05\n",
            "Epoch  23 Batch  467 / 525  Training Loss  3.958148226956837e-05\n",
            "Epoch  23 Batch  468 / 525  Training Loss  4.1696221160236746e-05\n",
            "Epoch  23 Batch  469 / 525  Training Loss  4.120317316846922e-05\n",
            "Epoch  23 Batch  470 / 525  Training Loss  3.727682997123338e-05\n",
            "Epoch  23 Batch  471 / 525  Training Loss  3.938689405913465e-05\n",
            "Epoch  23 Batch  472 / 525  Training Loss  3.787913010455668e-05\n",
            "Epoch  23 Batch  473 / 525  Training Loss  4.319266008678824e-05\n",
            "Epoch  23 Batch  474 / 525  Training Loss  5.51114026166033e-05\n",
            "Epoch  23 Batch  475 / 525  Training Loss  2.661870894371532e-05\n",
            "Epoch  23 Batch  476 / 525  Training Loss  4.5311739086173475e-05\n",
            "Epoch  23 Batch  477 / 525  Training Loss  5.70894917473197e-05\n",
            "Epoch  23 Batch  478 / 525  Training Loss  4.305060065235011e-05\n",
            "Epoch  23 Batch  479 / 525  Training Loss  5.0492631999077275e-05\n",
            "Epoch  23 Batch  480 / 525  Training Loss  4.277153129805811e-05\n",
            "Epoch  23 Batch  481 / 525  Training Loss  4.178425297141075e-05\n",
            "Epoch  23 Batch  482 / 525  Training Loss  4.655466182157397e-05\n",
            "Epoch  23 Batch  483 / 525  Training Loss  5.96145328017883e-05\n",
            "Epoch  23 Batch  484 / 525  Training Loss  2.878899613278918e-05\n",
            "Epoch  23 Batch  485 / 525  Training Loss  3.980191468144767e-05\n",
            "Epoch  23 Batch  486 / 525  Training Loss  5.800467260996811e-05\n",
            "Epoch  23 Batch  487 / 525  Training Loss  3.5859680792782456e-05\n",
            "Epoch  23 Batch  488 / 525  Training Loss  5.4769661801401526e-05\n",
            "Epoch  23 Batch  489 / 525  Training Loss  3.51986636815127e-05\n",
            "Epoch  23 Batch  490 / 525  Training Loss  2.9776816518278793e-05\n",
            "Epoch  23 Batch  491 / 525  Training Loss  4.7642191930208355e-05\n",
            "Epoch  23 Batch  492 / 525  Training Loss  3.0458231776719913e-05\n",
            "Epoch  23 Batch  493 / 525  Training Loss  4.545898627839051e-05\n",
            "Epoch  23 Batch  494 / 525  Training Loss  5.788577982457355e-05\n",
            "Epoch  23 Batch  495 / 525  Training Loss  4.7464713134104386e-05\n",
            "Epoch  23 Batch  496 / 525  Training Loss  4.605456706485711e-05\n",
            "Epoch  23 Batch  497 / 525  Training Loss  3.63363724318333e-05\n",
            "Epoch  23 Batch  498 / 525  Training Loss  4.525027179624885e-05\n",
            "Epoch  23 Batch  499 / 525  Training Loss  6.0785831010434777e-05\n",
            "Epoch  23 Batch  500 / 525  Training Loss  3.618906339397654e-05\n",
            "Epoch  23 Batch  501 / 525  Training Loss  3.8838239561300725e-05\n",
            "Epoch  23 Batch  502 / 525  Training Loss  5.409419463831e-05\n",
            "Epoch  23 Batch  503 / 525  Training Loss  3.5930468584410846e-05\n",
            "Epoch  23 Batch  504 / 525  Training Loss  4.557079591904767e-05\n",
            "Epoch  23 Batch  505 / 525  Training Loss  3.525008651195094e-05\n",
            "Epoch  23 Batch  506 / 525  Training Loss  4.831240585190244e-05\n",
            "Epoch  23 Batch  507 / 525  Training Loss  3.596934402594343e-05\n",
            "Epoch  23 Batch  508 / 525  Training Loss  5.7616813137428835e-05\n",
            "Epoch  23 Batch  509 / 525  Training Loss  3.9518043195130304e-05\n",
            "Epoch  23 Batch  510 / 525  Training Loss  5.0742673920467496e-05\n",
            "Epoch  23 Batch  511 / 525  Training Loss  3.294591442681849e-05\n",
            "Epoch  23 Batch  512 / 525  Training Loss  4.469866325962357e-05\n",
            "Epoch  23 Batch  513 / 525  Training Loss  4.458742478163913e-05\n",
            "Epoch  23 Batch  514 / 525  Training Loss  5.274071008898318e-05\n",
            "Epoch  23 Batch  515 / 525  Training Loss  4.693105074693449e-05\n",
            "Epoch  23 Batch  516 / 525  Training Loss  3.539819590514526e-05\n",
            "Epoch  23 Batch  517 / 525  Training Loss  3.669651778182015e-05\n",
            "Epoch  23 Batch  518 / 525  Training Loss  4.9562786443857476e-05\n",
            "Epoch  23 Batch  519 / 525  Training Loss  3.1116640457184985e-05\n",
            "Epoch  23 Batch  520 / 525  Training Loss  3.862111043417826e-05\n",
            "Epoch  23 Batch  521 / 525  Training Loss  3.0544964829459786e-05\n",
            "Epoch  23 Batch  522 / 525  Training Loss  4.997526411898434e-05\n",
            "Epoch  23 Batch  523 / 525  Training Loss  3.476767960819416e-05\n",
            "Epoch  23 Batch  524 / 525  Training Loss  4.019342304673046e-05\n",
            "  24    |    -    |   0.000044   |   64.32  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 24\n",
            "Epoch  24 Batch  0 / 525  Training Loss  3.7179481296334416e-05\n",
            "Epoch  24 Batch  1 / 525  Training Loss  3.507720248308033e-05\n",
            "Epoch  24 Batch  2 / 525  Training Loss  2.524415685911663e-05\n",
            "Epoch  24 Batch  3 / 525  Training Loss  3.102986374869943e-05\n",
            "Epoch  24 Batch  4 / 525  Training Loss  6.2646358856e-05\n",
            "Epoch  24 Batch  5 / 525  Training Loss  3.961769834859297e-05\n",
            "Epoch  24 Batch  6 / 525  Training Loss  3.210400609532371e-05\n",
            "Epoch  24 Batch  7 / 525  Training Loss  4.1141396650345996e-05\n",
            "Epoch  24 Batch  8 / 525  Training Loss  4.321410597185604e-05\n",
            "Epoch  24 Batch  9 / 525  Training Loss  3.4183813113486394e-05\n",
            "Epoch  24 Batch  10 / 525  Training Loss  3.7361227441579103e-05\n",
            "Epoch  24 Batch  11 / 525  Training Loss  3.5691624361788854e-05\n",
            "Epoch  24 Batch  12 / 525  Training Loss  4.40734947915189e-05\n",
            "Epoch  24 Batch  13 / 525  Training Loss  3.4285883884876966e-05\n",
            "Epoch  24 Batch  14 / 525  Training Loss  4.136187635594979e-05\n",
            "Epoch  24 Batch  15 / 525  Training Loss  3.241746162530035e-05\n",
            "Epoch  24 Batch  16 / 525  Training Loss  5.11569160153158e-05\n",
            "Epoch  24 Batch  17 / 525  Training Loss  3.56272175849881e-05\n",
            "Epoch  24 Batch  18 / 525  Training Loss  3.839148484985344e-05\n",
            "Epoch  24 Batch  19 / 525  Training Loss  4.313943281886168e-05\n",
            "Epoch  24 Batch  20 / 525  Training Loss  2.8848182410001755e-05\n",
            "Epoch  24 Batch  21 / 525  Training Loss  3.947049845010042e-05\n",
            "Epoch  24 Batch  22 / 525  Training Loss  4.7814828576520085e-05\n",
            "Epoch  24 Batch  23 / 525  Training Loss  3.6818546504946426e-05\n",
            "Epoch  24 Batch  24 / 525  Training Loss  3.711739555001259e-05\n",
            "Epoch  24 Batch  25 / 525  Training Loss  4.2888019379461184e-05\n",
            "Epoch  24 Batch  26 / 525  Training Loss  3.6335222830530256e-05\n",
            "Epoch  24 Batch  27 / 525  Training Loss  3.722578549059108e-05\n",
            "Epoch  24 Batch  28 / 525  Training Loss  3.262949758209288e-05\n",
            "Epoch  24 Batch  29 / 525  Training Loss  4.877911851508543e-05\n",
            "Epoch  24 Batch  30 / 525  Training Loss  3.416790423216298e-05\n",
            "Epoch  24 Batch  31 / 525  Training Loss  3.174327503074892e-05\n",
            "Epoch  24 Batch  32 / 525  Training Loss  3.0245089874370024e-05\n",
            "Epoch  24 Batch  33 / 525  Training Loss  4.9150326958624646e-05\n",
            "Epoch  24 Batch  34 / 525  Training Loss  4.007690949947573e-05\n",
            "Epoch  24 Batch  35 / 525  Training Loss  2.6711670216172934e-05\n",
            "Epoch  24 Batch  36 / 525  Training Loss  4.9942249461309984e-05\n",
            "Epoch  24 Batch  37 / 525  Training Loss  3.832439688267186e-05\n",
            "Epoch  24 Batch  38 / 525  Training Loss  4.401908518048003e-05\n",
            "Epoch  24 Batch  39 / 525  Training Loss  3.601156276999973e-05\n",
            "Epoch  24 Batch  40 / 525  Training Loss  4.406748848850839e-05\n",
            "Epoch  24 Batch  41 / 525  Training Loss  3.180267594871111e-05\n",
            "Epoch  24 Batch  42 / 525  Training Loss  2.8162467060610652e-05\n",
            "Epoch  24 Batch  43 / 525  Training Loss  5.248220986686647e-05\n",
            "Epoch  24 Batch  44 / 525  Training Loss  4.0021066524786875e-05\n",
            "Epoch  24 Batch  45 / 525  Training Loss  4.8165544285438955e-05\n",
            "Epoch  24 Batch  46 / 525  Training Loss  4.9053294787881896e-05\n",
            "Epoch  24 Batch  47 / 525  Training Loss  3.057433423236944e-05\n",
            "Epoch  24 Batch  48 / 525  Training Loss  3.7282967241480947e-05\n",
            "Epoch  24 Batch  49 / 525  Training Loss  3.310599640826695e-05\n",
            "Epoch  24 Batch  50 / 525  Training Loss  4.121874371776357e-05\n",
            "Epoch  24 Batch  51 / 525  Training Loss  3.5232264053775e-05\n",
            "Epoch  24 Batch  52 / 525  Training Loss  4.061498475493863e-05\n",
            "Epoch  24 Batch  53 / 525  Training Loss  3.841215948341414e-05\n",
            "Epoch  24 Batch  54 / 525  Training Loss  4.736908522318117e-05\n",
            "Epoch  24 Batch  55 / 525  Training Loss  2.503264659026172e-05\n",
            "Epoch  24 Batch  56 / 525  Training Loss  2.625833440106362e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  24 Batch  57 / 525  Training Loss  4.2125295294681564e-05\n",
            "Epoch  24 Batch  58 / 525  Training Loss  4.992135291104205e-05\n",
            "Epoch  24 Batch  59 / 525  Training Loss  3.091132384724915e-05\n",
            "Epoch  24 Batch  60 / 525  Training Loss  4.929366332362406e-05\n",
            "Epoch  24 Batch  61 / 525  Training Loss  4.861250636167824e-05\n",
            "Epoch  24 Batch  62 / 525  Training Loss  3.790188202401623e-05\n",
            "Epoch  24 Batch  63 / 525  Training Loss  4.7136567445704713e-05\n",
            "Epoch  24 Batch  64 / 525  Training Loss  4.9685815611155704e-05\n",
            "Epoch  24 Batch  65 / 525  Training Loss  3.096253567491658e-05\n",
            "Epoch  24 Batch  66 / 525  Training Loss  4.496120527619496e-05\n",
            "Epoch  24 Batch  67 / 525  Training Loss  3.385956370038912e-05\n",
            "Epoch  24 Batch  68 / 525  Training Loss  4.032406650367193e-05\n",
            "Epoch  24 Batch  69 / 525  Training Loss  4.803726187674329e-05\n",
            "Epoch  24 Batch  70 / 525  Training Loss  4.656793316826224e-05\n",
            "Epoch  24 Batch  71 / 525  Training Loss  4.126010026084259e-05\n",
            "Epoch  24 Batch  72 / 525  Training Loss  4.192120468360372e-05\n",
            "Epoch  24 Batch  73 / 525  Training Loss  4.956374323228374e-05\n",
            "Epoch  24 Batch  74 / 525  Training Loss  3.876809569192119e-05\n",
            "Epoch  24 Batch  75 / 525  Training Loss  3.797854878939688e-05\n",
            "Epoch  24 Batch  76 / 525  Training Loss  3.818317418335937e-05\n",
            "Epoch  24 Batch  77 / 525  Training Loss  3.9650098187848926e-05\n",
            "Epoch  24 Batch  78 / 525  Training Loss  4.5679516915697604e-05\n",
            "Epoch  24 Batch  79 / 525  Training Loss  3.151035343762487e-05\n",
            "Epoch  24 Batch  80 / 525  Training Loss  4.188488674117252e-05\n",
            "Epoch  24 Batch  81 / 525  Training Loss  4.7946148697519675e-05\n",
            "Epoch  24 Batch  82 / 525  Training Loss  3.478676808299497e-05\n",
            "Epoch  24 Batch  83 / 525  Training Loss  3.858219861285761e-05\n",
            "Epoch  24 Batch  84 / 525  Training Loss  4.1760165913729e-05\n",
            "Epoch  24 Batch  85 / 525  Training Loss  5.103186049382202e-05\n",
            "Epoch  24 Batch  86 / 525  Training Loss  3.762072810786776e-05\n",
            "Epoch  24 Batch  87 / 525  Training Loss  3.967155498685315e-05\n",
            "Epoch  24 Batch  88 / 525  Training Loss  3.471431045909412e-05\n",
            "Epoch  24 Batch  89 / 525  Training Loss  2.852638681360986e-05\n",
            "Epoch  24 Batch  90 / 525  Training Loss  3.760728577617556e-05\n",
            "Epoch  24 Batch  91 / 525  Training Loss  2.96565758617362e-05\n",
            "Epoch  24 Batch  92 / 525  Training Loss  2.6867923224926926e-05\n",
            "Epoch  24 Batch  93 / 525  Training Loss  3.037516762560699e-05\n",
            "Epoch  24 Batch  94 / 525  Training Loss  2.8613852919079363e-05\n",
            "Epoch  24 Batch  95 / 525  Training Loss  4.604952846420929e-05\n",
            "Epoch  24 Batch  96 / 525  Training Loss  2.78596744465176e-05\n",
            "Epoch  24 Batch  97 / 525  Training Loss  4.3710158934118226e-05\n",
            "Epoch  24 Batch  98 / 525  Training Loss  4.53565880889073e-05\n",
            "Epoch  24 Batch  99 / 525  Training Loss  4.564478877000511e-05\n",
            "Epoch  24 Batch  100 / 525  Training Loss  3.4344840969424695e-05\n",
            "Epoch  24 Batch  101 / 525  Training Loss  3.2688698411220685e-05\n",
            "Epoch  24 Batch  102 / 525  Training Loss  3.5755343560595065e-05\n",
            "Epoch  24 Batch  103 / 525  Training Loss  3.063897747779265e-05\n",
            "Epoch  24 Batch  104 / 525  Training Loss  4.208119207760319e-05\n",
            "Epoch  24 Batch  105 / 525  Training Loss  3.544065111782402e-05\n",
            "Epoch  24 Batch  106 / 525  Training Loss  7.281913713086396e-05\n",
            "Epoch  24 Batch  107 / 525  Training Loss  4.74844200653024e-05\n",
            "Epoch  24 Batch  108 / 525  Training Loss  3.3194948628079146e-05\n",
            "Epoch  24 Batch  109 / 525  Training Loss  3.653102976386435e-05\n",
            "Epoch  24 Batch  110 / 525  Training Loss  3.911610474460758e-05\n",
            "Epoch  24 Batch  111 / 525  Training Loss  3.6380155506776646e-05\n",
            "Epoch  24 Batch  112 / 525  Training Loss  3.711038516485132e-05\n",
            "Epoch  24 Batch  113 / 525  Training Loss  3.9891536289360374e-05\n",
            "Epoch  24 Batch  114 / 525  Training Loss  5.184759356779978e-05\n",
            "Epoch  24 Batch  115 / 525  Training Loss  4.548881042865105e-05\n",
            "Epoch  24 Batch  116 / 525  Training Loss  3.174387165927328e-05\n",
            "Epoch  24 Batch  117 / 525  Training Loss  3.6668850952992216e-05\n",
            "Epoch  24 Batch  118 / 525  Training Loss  3.575045047909953e-05\n",
            "Epoch  24 Batch  119 / 525  Training Loss  3.6194796848576516e-05\n",
            "Epoch  24 Batch  120 / 525  Training Loss  4.727780105895363e-05\n",
            "Epoch  24 Batch  121 / 525  Training Loss  3.327723970869556e-05\n",
            "Epoch  24 Batch  122 / 525  Training Loss  4.6842593292240053e-05\n",
            "Epoch  24 Batch  123 / 525  Training Loss  3.7945406802464277e-05\n",
            "Epoch  24 Batch  124 / 525  Training Loss  4.8226876970147714e-05\n",
            "Epoch  24 Batch  125 / 525  Training Loss  3.417995321797207e-05\n",
            "Epoch  24 Batch  126 / 525  Training Loss  4.2167510400759056e-05\n",
            "Epoch  24 Batch  127 / 525  Training Loss  4.5460808905772865e-05\n",
            "Epoch  24 Batch  128 / 525  Training Loss  2.0290046450099908e-05\n",
            "Epoch  24 Batch  129 / 525  Training Loss  3.697095235111192e-05\n",
            "Epoch  24 Batch  130 / 525  Training Loss  3.546178777469322e-05\n",
            "Epoch  24 Batch  131 / 525  Training Loss  3.673944956972264e-05\n",
            "Epoch  24 Batch  132 / 525  Training Loss  4.2691815906437114e-05\n",
            "Epoch  24 Batch  133 / 525  Training Loss  5.266153675620444e-05\n",
            "Epoch  24 Batch  134 / 525  Training Loss  5.44395406905096e-05\n",
            "Epoch  24 Batch  135 / 525  Training Loss  4.240756970830262e-05\n",
            "Epoch  24 Batch  136 / 525  Training Loss  4.128745786147192e-05\n",
            "Epoch  24 Batch  137 / 525  Training Loss  3.067866055062041e-05\n",
            "Epoch  24 Batch  138 / 525  Training Loss  5.5437507398892194e-05\n",
            "Epoch  24 Batch  139 / 525  Training Loss  3.627079786383547e-05\n",
            "Epoch  24 Batch  140 / 525  Training Loss  3.593098517740145e-05\n",
            "Epoch  24 Batch  141 / 525  Training Loss  3.5192490031477064e-05\n",
            "Epoch  24 Batch  142 / 525  Training Loss  4.700474892160855e-05\n",
            "Epoch  24 Batch  143 / 525  Training Loss  4.662187348003499e-05\n",
            "Epoch  24 Batch  144 / 525  Training Loss  4.106707638129592e-05\n",
            "Epoch  24 Batch  145 / 525  Training Loss  3.933170228265226e-05\n",
            "Epoch  24 Batch  146 / 525  Training Loss  3.077038491028361e-05\n",
            "Epoch  24 Batch  147 / 525  Training Loss  4.1678060370031744e-05\n",
            "Epoch  24 Batch  148 / 525  Training Loss  3.5085049603367224e-05\n",
            "Epoch  24 Batch  149 / 525  Training Loss  3.251498128520325e-05\n",
            "Epoch  24 Batch  150 / 525  Training Loss  5.021888136980124e-05\n",
            "Epoch  24 Batch  151 / 525  Training Loss  4.009008262073621e-05\n",
            "Epoch  24 Batch  152 / 525  Training Loss  4.0592229197500274e-05\n",
            "Epoch  24 Batch  153 / 525  Training Loss  3.2583622669335455e-05\n",
            "Epoch  24 Batch  154 / 525  Training Loss  2.948757537524216e-05\n",
            "Epoch  24 Batch  155 / 525  Training Loss  4.432275454746559e-05\n",
            "Epoch  24 Batch  156 / 525  Training Loss  3.456052945693955e-05\n",
            "Epoch  24 Batch  157 / 525  Training Loss  3.213247691746801e-05\n",
            "Epoch  24 Batch  158 / 525  Training Loss  2.13674757105764e-05\n",
            "Epoch  24 Batch  159 / 525  Training Loss  4.461963544599712e-05\n",
            "Epoch  24 Batch  160 / 525  Training Loss  2.5184806872857735e-05\n",
            "Epoch  24 Batch  161 / 525  Training Loss  3.391836798982695e-05\n",
            "Epoch  24 Batch  162 / 525  Training Loss  5.042618795414455e-05\n",
            "Epoch  24 Batch  163 / 525  Training Loss  2.8625694540096447e-05\n",
            "Epoch  24 Batch  164 / 525  Training Loss  3.817536708083935e-05\n",
            "Epoch  24 Batch  165 / 525  Training Loss  4.129105218453333e-05\n",
            "Epoch  24 Batch  166 / 525  Training Loss  3.868406201945618e-05\n",
            "Epoch  24 Batch  167 / 525  Training Loss  3.4144257369916886e-05\n",
            "Epoch  24 Batch  168 / 525  Training Loss  3.654604370240122e-05\n",
            "Epoch  24 Batch  169 / 525  Training Loss  3.3573745895409957e-05\n",
            "Epoch  24 Batch  170 / 525  Training Loss  4.1782397602219135e-05\n",
            "Epoch  24 Batch  171 / 525  Training Loss  3.53852374246344e-05\n",
            "Epoch  24 Batch  172 / 525  Training Loss  5.206210698815994e-05\n",
            "Epoch  24 Batch  173 / 525  Training Loss  2.9891298254369758e-05\n",
            "Epoch  24 Batch  174 / 525  Training Loss  3.392002690816298e-05\n",
            "Epoch  24 Batch  175 / 525  Training Loss  3.320227187941782e-05\n",
            "Epoch  24 Batch  176 / 525  Training Loss  3.31383416778408e-05\n",
            "Epoch  24 Batch  177 / 525  Training Loss  4.523515235632658e-05\n",
            "Epoch  24 Batch  178 / 525  Training Loss  3.795931479544379e-05\n",
            "Epoch  24 Batch  179 / 525  Training Loss  3.311687396490015e-05\n",
            "Epoch  24 Batch  180 / 525  Training Loss  4.01509678340517e-05\n",
            "Epoch  24 Batch  181 / 525  Training Loss  3.573628418962471e-05\n",
            "Epoch  24 Batch  182 / 525  Training Loss  4.3219733925070614e-05\n",
            "Epoch  24 Batch  183 / 525  Training Loss  4.4604465074371547e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  24 Batch  184 / 525  Training Loss  3.0337014322867617e-05\n",
            "Epoch  24 Batch  185 / 525  Training Loss  3.08415837935172e-05\n",
            "Epoch  24 Batch  186 / 525  Training Loss  3.819892663159408e-05\n",
            "Epoch  24 Batch  187 / 525  Training Loss  4.143607657169923e-05\n",
            "Epoch  24 Batch  188 / 525  Training Loss  3.9599086449015886e-05\n",
            "Epoch  24 Batch  189 / 525  Training Loss  3.0459525078185834e-05\n",
            "Epoch  24 Batch  190 / 525  Training Loss  3.234803807572462e-05\n",
            "Epoch  24 Batch  191 / 525  Training Loss  4.8682468332117423e-05\n",
            "Epoch  24 Batch  192 / 525  Training Loss  2.6288669687346555e-05\n",
            "Epoch  24 Batch  193 / 525  Training Loss  4.503110903897323e-05\n",
            "Epoch  24 Batch  194 / 525  Training Loss  4.374172567622736e-05\n",
            "Epoch  24 Batch  195 / 525  Training Loss  4.654080112231895e-05\n",
            "Epoch  24 Batch  196 / 525  Training Loss  3.281623139628209e-05\n",
            "Epoch  24 Batch  197 / 525  Training Loss  4.8293975851265714e-05\n",
            "Epoch  24 Batch  198 / 525  Training Loss  2.8826112611568533e-05\n",
            "Epoch  24 Batch  199 / 525  Training Loss  5.625258927466348e-05\n",
            "Epoch  24 Batch  200 / 525  Training Loss  3.794756412389688e-05\n",
            "Epoch  24 Batch  201 / 525  Training Loss  3.053942782571539e-05\n",
            "Epoch  24 Batch  202 / 525  Training Loss  4.673774674301967e-05\n",
            "Epoch  24 Batch  203 / 525  Training Loss  3.025911792065017e-05\n",
            "Epoch  24 Batch  204 / 525  Training Loss  5.740937922382727e-05\n",
            "Epoch  24 Batch  205 / 525  Training Loss  4.696492032962851e-05\n",
            "Epoch  24 Batch  206 / 525  Training Loss  3.8537145883310586e-05\n",
            "Epoch  24 Batch  207 / 525  Training Loss  3.175802339683287e-05\n",
            "Epoch  24 Batch  208 / 525  Training Loss  4.482514850678854e-05\n",
            "Epoch  24 Batch  209 / 525  Training Loss  2.9820652343914844e-05\n",
            "Epoch  24 Batch  210 / 525  Training Loss  3.150142583763227e-05\n",
            "Epoch  24 Batch  211 / 525  Training Loss  3.612996806623414e-05\n",
            "Epoch  24 Batch  212 / 525  Training Loss  4.138189251534641e-05\n",
            "Epoch  24 Batch  213 / 525  Training Loss  3.474578625173308e-05\n",
            "Epoch  24 Batch  214 / 525  Training Loss  2.5973437004722655e-05\n",
            "Epoch  24 Batch  215 / 525  Training Loss  4.206451922073029e-05\n",
            "Epoch  24 Batch  216 / 525  Training Loss  3.759497849387117e-05\n",
            "Epoch  24 Batch  217 / 525  Training Loss  3.0133309337543324e-05\n",
            "Epoch  24 Batch  218 / 525  Training Loss  4.787263969774358e-05\n",
            "Epoch  24 Batch  219 / 525  Training Loss  3.0741270165890455e-05\n",
            "Epoch  24 Batch  220 / 525  Training Loss  3.0280885766842403e-05\n",
            "Epoch  24 Batch  221 / 525  Training Loss  3.454593024798669e-05\n",
            "Epoch  24 Batch  222 / 525  Training Loss  3.5379121982259676e-05\n",
            "Epoch  24 Batch  223 / 525  Training Loss  4.343076216173358e-05\n",
            "Epoch  24 Batch  224 / 525  Training Loss  3.017194103449583e-05\n",
            "Epoch  24 Batch  225 / 525  Training Loss  3.9161110180430114e-05\n",
            "Epoch  24 Batch  226 / 525  Training Loss  3.0341039746417664e-05\n",
            "Epoch  24 Batch  227 / 525  Training Loss  3.237041528336704e-05\n",
            "Epoch  24 Batch  228 / 525  Training Loss  3.453805766184814e-05\n",
            "Epoch  24 Batch  229 / 525  Training Loss  3.287265280960128e-05\n",
            "Epoch  24 Batch  230 / 525  Training Loss  3.824281520792283e-05\n",
            "Epoch  24 Batch  231 / 525  Training Loss  3.9622806070838124e-05\n",
            "Epoch  24 Batch  232 / 525  Training Loss  3.188256232533604e-05\n",
            "Epoch  24 Batch  233 / 525  Training Loss  4.039893246954307e-05\n",
            "Epoch  24 Batch  234 / 525  Training Loss  2.8133956220699474e-05\n",
            "Epoch  24 Batch  235 / 525  Training Loss  4.066981273354031e-05\n",
            "Epoch  24 Batch  236 / 525  Training Loss  5.184450856177136e-05\n",
            "Epoch  24 Batch  237 / 525  Training Loss  2.772677362372633e-05\n",
            "Epoch  24 Batch  238 / 525  Training Loss  3.091676626354456e-05\n",
            "Epoch  24 Batch  239 / 525  Training Loss  5.004595732316375e-05\n",
            "Epoch  24 Batch  240 / 525  Training Loss  4.264426388544962e-05\n",
            "Epoch  24 Batch  241 / 525  Training Loss  4.591246761265211e-05\n",
            "Epoch  24 Batch  242 / 525  Training Loss  3.2323510822607204e-05\n",
            "Epoch  24 Batch  243 / 525  Training Loss  5.320682976162061e-05\n",
            "Epoch  24 Batch  244 / 525  Training Loss  2.2689049728796817e-05\n",
            "Epoch  24 Batch  245 / 525  Training Loss  4.4265321776038036e-05\n",
            "Epoch  24 Batch  246 / 525  Training Loss  5.5793887440813705e-05\n",
            "Epoch  24 Batch  247 / 525  Training Loss  3.6404122511157766e-05\n",
            "Epoch  24 Batch  248 / 525  Training Loss  3.273245965829119e-05\n",
            "Epoch  24 Batch  249 / 525  Training Loss  5.653035987052135e-05\n",
            "Epoch  24 Batch  250 / 525  Training Loss  3.332002233946696e-05\n",
            "Epoch  24 Batch  251 / 525  Training Loss  3.0263885491876863e-05\n",
            "Epoch  24 Batch  252 / 525  Training Loss  5.1589984650490806e-05\n",
            "Epoch  24 Batch  253 / 525  Training Loss  5.176962440600619e-05\n",
            "Epoch  24 Batch  254 / 525  Training Loss  5.625874473480508e-05\n",
            "Epoch  24 Batch  255 / 525  Training Loss  4.889028059551492e-05\n",
            "Epoch  24 Batch  256 / 525  Training Loss  3.622141957748681e-05\n",
            "Epoch  24 Batch  257 / 525  Training Loss  3.2087002182379365e-05\n",
            "Epoch  24 Batch  258 / 525  Training Loss  4.4413893192540854e-05\n",
            "Epoch  24 Batch  259 / 525  Training Loss  2.8056927476427518e-05\n",
            "Epoch  24 Batch  260 / 525  Training Loss  3.629640195867978e-05\n",
            "Epoch  24 Batch  261 / 525  Training Loss  2.9805325539200567e-05\n",
            "Epoch  24 Batch  262 / 525  Training Loss  2.255708386655897e-05\n",
            "Epoch  24 Batch  263 / 525  Training Loss  4.1608145693317056e-05\n",
            "Epoch  24 Batch  264 / 525  Training Loss  2.6154913939535618e-05\n",
            "Epoch  24 Batch  265 / 525  Training Loss  5.0583599659148604e-05\n",
            "Epoch  24 Batch  266 / 525  Training Loss  4.038479528389871e-05\n",
            "Epoch  24 Batch  267 / 525  Training Loss  3.155906961183064e-05\n",
            "Epoch  24 Batch  268 / 525  Training Loss  3.3977688872255385e-05\n",
            "Epoch  24 Batch  269 / 525  Training Loss  3.3688869734760374e-05\n",
            "Epoch  24 Batch  270 / 525  Training Loss  3.690930316224694e-05\n",
            "Epoch  24 Batch  271 / 525  Training Loss  4.401652768137865e-05\n",
            "Epoch  24 Batch  272 / 525  Training Loss  4.797857400262728e-05\n",
            "Epoch  24 Batch  273 / 525  Training Loss  2.2256783267948776e-05\n",
            "Epoch  24 Batch  274 / 525  Training Loss  2.8690596082014963e-05\n",
            "Epoch  24 Batch  275 / 525  Training Loss  4.2692940041888505e-05\n",
            "Epoch  24 Batch  276 / 525  Training Loss  4.8341269575757906e-05\n",
            "Epoch  24 Batch  277 / 525  Training Loss  4.967660061083734e-05\n",
            "Epoch  24 Batch  278 / 525  Training Loss  3.657545312307775e-05\n",
            "Epoch  24 Batch  279 / 525  Training Loss  3.7272839108482e-05\n",
            "Epoch  24 Batch  280 / 525  Training Loss  3.019709220097866e-05\n",
            "Epoch  24 Batch  281 / 525  Training Loss  3.74079063476529e-05\n",
            "Epoch  24 Batch  282 / 525  Training Loss  3.318446397315711e-05\n",
            "Epoch  24 Batch  283 / 525  Training Loss  4.97101791552268e-05\n",
            "Epoch  24 Batch  284 / 525  Training Loss  4.655106022255495e-05\n",
            "Epoch  24 Batch  285 / 525  Training Loss  3.3657270250841975e-05\n",
            "Epoch  24 Batch  286 / 525  Training Loss  2.8365680918795988e-05\n",
            "Epoch  24 Batch  287 / 525  Training Loss  4.4757791329175234e-05\n",
            "Epoch  24 Batch  288 / 525  Training Loss  5.3023442887933925e-05\n",
            "Epoch  24 Batch  289 / 525  Training Loss  4.5436154323397204e-05\n",
            "Epoch  24 Batch  290 / 525  Training Loss  3.213019954273477e-05\n",
            "Epoch  24 Batch  291 / 525  Training Loss  2.465059151290916e-05\n",
            "Epoch  24 Batch  292 / 525  Training Loss  4.1604329453548416e-05\n",
            "Epoch  24 Batch  293 / 525  Training Loss  4.922658990835771e-05\n",
            "Epoch  24 Batch  294 / 525  Training Loss  3.697788997669704e-05\n",
            "Epoch  24 Batch  295 / 525  Training Loss  3.4380678698653355e-05\n",
            "Epoch  24 Batch  296 / 525  Training Loss  4.9721576942829415e-05\n",
            "Epoch  24 Batch  297 / 525  Training Loss  4.04520433221478e-05\n",
            "Epoch  24 Batch  298 / 525  Training Loss  3.773633579839952e-05\n",
            "Epoch  24 Batch  299 / 525  Training Loss  3.988916432717815e-05\n",
            "Epoch  24 Batch  300 / 525  Training Loss  4.043769877171144e-05\n",
            "Epoch  24 Batch  301 / 525  Training Loss  5.720359695260413e-05\n",
            "Epoch  24 Batch  302 / 525  Training Loss  3.1382784072775394e-05\n",
            "Epoch  24 Batch  303 / 525  Training Loss  3.855261093121953e-05\n",
            "Epoch  24 Batch  304 / 525  Training Loss  3.4239750675624236e-05\n",
            "Epoch  24 Batch  305 / 525  Training Loss  3.570563785615377e-05\n",
            "Epoch  24 Batch  306 / 525  Training Loss  2.8448941520764492e-05\n",
            "Epoch  24 Batch  307 / 525  Training Loss  4.247982724336907e-05\n",
            "Epoch  24 Batch  308 / 525  Training Loss  5.5404969316441566e-05\n",
            "Epoch  24 Batch  309 / 525  Training Loss  3.423640737310052e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  24 Batch  310 / 525  Training Loss  2.8166130505269393e-05\n",
            "Epoch  24 Batch  311 / 525  Training Loss  4.7679870476713404e-05\n",
            "Epoch  24 Batch  312 / 525  Training Loss  5.550572677748278e-05\n",
            "Epoch  24 Batch  313 / 525  Training Loss  3.894021938322112e-05\n",
            "Epoch  24 Batch  314 / 525  Training Loss  4.340688610682264e-05\n",
            "Epoch  24 Batch  315 / 525  Training Loss  4.532084130914882e-05\n",
            "Epoch  24 Batch  316 / 525  Training Loss  3.6626519431592897e-05\n",
            "Epoch  24 Batch  317 / 525  Training Loss  5.788928683614358e-05\n",
            "Epoch  24 Batch  318 / 525  Training Loss  3.87088002753444e-05\n",
            "Epoch  24 Batch  319 / 525  Training Loss  4.7395551519002765e-05\n",
            "Epoch  24 Batch  320 / 525  Training Loss  3.130088953184895e-05\n",
            "Epoch  24 Batch  321 / 525  Training Loss  4.732529487228021e-05\n",
            "Epoch  24 Batch  322 / 525  Training Loss  3.602421566029079e-05\n",
            "Epoch  24 Batch  323 / 525  Training Loss  3.618283153627999e-05\n",
            "Epoch  24 Batch  324 / 525  Training Loss  4.6229437430156395e-05\n",
            "Epoch  24 Batch  325 / 525  Training Loss  4.10851243941579e-05\n",
            "Epoch  24 Batch  326 / 525  Training Loss  4.564328264677897e-05\n",
            "Epoch  24 Batch  327 / 525  Training Loss  5.0568487495183945e-05\n",
            "Epoch  24 Batch  328 / 525  Training Loss  3.4961522032972425e-05\n",
            "Epoch  24 Batch  329 / 525  Training Loss  2.1911211661063135e-05\n",
            "Epoch  24 Batch  330 / 525  Training Loss  3.350363112986088e-05\n",
            "Epoch  24 Batch  331 / 525  Training Loss  3.194005330442451e-05\n",
            "Epoch  24 Batch  332 / 525  Training Loss  3.2644154998706654e-05\n",
            "Epoch  24 Batch  333 / 525  Training Loss  4.004326547146775e-05\n",
            "Epoch  24 Batch  334 / 525  Training Loss  4.4691190851153806e-05\n",
            "Epoch  24 Batch  335 / 525  Training Loss  4.548285869532265e-05\n",
            "Epoch  24 Batch  336 / 525  Training Loss  4.1372339182998985e-05\n",
            "Epoch  24 Batch  337 / 525  Training Loss  4.255237217876129e-05\n",
            "Epoch  24 Batch  338 / 525  Training Loss  3.295196802355349e-05\n",
            "Epoch  24 Batch  339 / 525  Training Loss  3.4254437196068466e-05\n",
            "Epoch  24 Batch  340 / 525  Training Loss  4.56688940175809e-05\n",
            "Epoch  24 Batch  341 / 525  Training Loss  3.888795254169963e-05\n",
            "Epoch  24 Batch  342 / 525  Training Loss  2.913316711783409e-05\n",
            "Epoch  24 Batch  343 / 525  Training Loss  3.591026325011626e-05\n",
            "Epoch  24 Batch  344 / 525  Training Loss  3.988635944551788e-05\n",
            "Epoch  24 Batch  345 / 525  Training Loss  3.2202347938437015e-05\n",
            "Epoch  24 Batch  346 / 525  Training Loss  5.992811202304438e-05\n",
            "Epoch  24 Batch  347 / 525  Training Loss  2.78312545560766e-05\n",
            "Epoch  24 Batch  348 / 525  Training Loss  3.974408173235133e-05\n",
            "Epoch  24 Batch  349 / 525  Training Loss  3.8509300793521106e-05\n",
            "Epoch  24 Batch  350 / 525  Training Loss  4.365486529422924e-05\n",
            "Epoch  24 Batch  351 / 525  Training Loss  2.959964876936283e-05\n",
            "Epoch  24 Batch  352 / 525  Training Loss  4.6796871174592525e-05\n",
            "Epoch  24 Batch  353 / 525  Training Loss  4.084741885890253e-05\n",
            "Epoch  24 Batch  354 / 525  Training Loss  3.4427761420374736e-05\n",
            "Epoch  24 Batch  355 / 525  Training Loss  4.7794026613701135e-05\n",
            "Epoch  24 Batch  356 / 525  Training Loss  3.703243419295177e-05\n",
            "Epoch  24 Batch  357 / 525  Training Loss  2.4458300686092116e-05\n",
            "Epoch  24 Batch  358 / 525  Training Loss  3.725559508893639e-05\n",
            "Epoch  24 Batch  359 / 525  Training Loss  4.75294582429342e-05\n",
            "Epoch  24 Batch  360 / 525  Training Loss  3.628051490522921e-05\n",
            "Epoch  24 Batch  361 / 525  Training Loss  3.55794581992086e-05\n",
            "Epoch  24 Batch  362 / 525  Training Loss  3.611982538131997e-05\n",
            "Epoch  24 Batch  363 / 525  Training Loss  3.6942372389603406e-05\n",
            "Epoch  24 Batch  364 / 525  Training Loss  4.148181324126199e-05\n",
            "Epoch  24 Batch  365 / 525  Training Loss  4.557440843200311e-05\n",
            "Epoch  24 Batch  366 / 525  Training Loss  4.0198570786742494e-05\n",
            "Epoch  24 Batch  367 / 525  Training Loss  2.8542446671053767e-05\n",
            "Epoch  24 Batch  368 / 525  Training Loss  4.217913738102652e-05\n",
            "Epoch  24 Batch  369 / 525  Training Loss  6.412738730432466e-05\n",
            "Epoch  24 Batch  370 / 525  Training Loss  3.458086575847119e-05\n",
            "Epoch  24 Batch  371 / 525  Training Loss  4.2929008486680686e-05\n",
            "Epoch  24 Batch  372 / 525  Training Loss  3.3378568332409486e-05\n",
            "Epoch  24 Batch  373 / 525  Training Loss  3.4014276025118306e-05\n",
            "Epoch  24 Batch  374 / 525  Training Loss  3.576259041437879e-05\n",
            "Epoch  24 Batch  375 / 525  Training Loss  3.30195834976621e-05\n",
            "Epoch  24 Batch  376 / 525  Training Loss  2.5228011509170756e-05\n",
            "Epoch  24 Batch  377 / 525  Training Loss  3.472099342616275e-05\n",
            "Epoch  24 Batch  378 / 525  Training Loss  4.513234307523817e-05\n",
            "Epoch  24 Batch  379 / 525  Training Loss  3.9844202547101304e-05\n",
            "Epoch  24 Batch  380 / 525  Training Loss  3.6904282751493156e-05\n",
            "Epoch  24 Batch  381 / 525  Training Loss  3.087485674768686e-05\n",
            "Epoch  24 Batch  382 / 525  Training Loss  4.884119698544964e-05\n",
            "Epoch  24 Batch  383 / 525  Training Loss  3.301183460280299e-05\n",
            "Epoch  24 Batch  384 / 525  Training Loss  3.453559475019574e-05\n",
            "Epoch  24 Batch  385 / 525  Training Loss  2.5684155843919143e-05\n",
            "Epoch  24 Batch  386 / 525  Training Loss  4.1909290303010494e-05\n",
            "Epoch  24 Batch  387 / 525  Training Loss  3.5101555113215e-05\n",
            "Epoch  24 Batch  388 / 525  Training Loss  4.7526184062007815e-05\n",
            "Epoch  24 Batch  389 / 525  Training Loss  5.119632260175422e-05\n",
            "Epoch  24 Batch  390 / 525  Training Loss  3.6502144212136045e-05\n",
            "Epoch  24 Batch  391 / 525  Training Loss  3.030311199836433e-05\n",
            "Epoch  24 Batch  392 / 525  Training Loss  3.753460259758867e-05\n",
            "Epoch  24 Batch  393 / 525  Training Loss  4.6568256948376074e-05\n",
            "Epoch  24 Batch  394 / 525  Training Loss  4.073276068083942e-05\n",
            "Epoch  24 Batch  395 / 525  Training Loss  2.9101962354616262e-05\n",
            "Epoch  24 Batch  396 / 525  Training Loss  4.577486834023148e-05\n",
            "Epoch  24 Batch  397 / 525  Training Loss  4.7486428229603916e-05\n",
            "Epoch  24 Batch  398 / 525  Training Loss  3.5002878576051444e-05\n",
            "Epoch  24 Batch  399 / 525  Training Loss  3.4870132367359474e-05\n",
            "Epoch  24 Batch  400 / 525  Training Loss  4.4343603804009035e-05\n",
            "Epoch  24 Batch  401 / 525  Training Loss  4.6211975131882355e-05\n",
            "Epoch  24 Batch  402 / 525  Training Loss  3.9339087379630655e-05\n",
            "Epoch  24 Batch  403 / 525  Training Loss  3.2347452361136675e-05\n",
            "Epoch  24 Batch  404 / 525  Training Loss  2.9655817343154922e-05\n",
            "Epoch  24 Batch  405 / 525  Training Loss  4.951667506247759e-05\n",
            "Epoch  24 Batch  406 / 525  Training Loss  4.093451934750192e-05\n",
            "Epoch  24 Batch  407 / 525  Training Loss  2.1574971469817683e-05\n",
            "Epoch  24 Batch  408 / 525  Training Loss  3.704426490003243e-05\n",
            "Epoch  24 Batch  409 / 525  Training Loss  3.5293611290398985e-05\n",
            "Epoch  24 Batch  410 / 525  Training Loss  3.501144965412095e-05\n",
            "Epoch  24 Batch  411 / 525  Training Loss  4.363076368463226e-05\n",
            "Epoch  24 Batch  412 / 525  Training Loss  4.8617519496474415e-05\n",
            "Epoch  24 Batch  413 / 525  Training Loss  4.271380748832598e-05\n",
            "Epoch  24 Batch  414 / 525  Training Loss  3.36258381139487e-05\n",
            "Epoch  24 Batch  415 / 525  Training Loss  3.7792247894685715e-05\n",
            "Epoch  24 Batch  416 / 525  Training Loss  5.596302798949182e-05\n",
            "Epoch  24 Batch  417 / 525  Training Loss  4.830530087929219e-05\n",
            "Epoch  24 Batch  418 / 525  Training Loss  3.716178616741672e-05\n",
            "Epoch  24 Batch  419 / 525  Training Loss  2.5610677766962908e-05\n",
            "Epoch  24 Batch  420 / 525  Training Loss  3.410218050703406e-05\n",
            "Epoch  24 Batch  421 / 525  Training Loss  4.0926744986791164e-05\n",
            "Epoch  24 Batch  422 / 525  Training Loss  4.183917917544022e-05\n",
            "Epoch  24 Batch  423 / 525  Training Loss  4.146535866311751e-05\n",
            "Epoch  24 Batch  424 / 525  Training Loss  2.799325193336699e-05\n",
            "Epoch  24 Batch  425 / 525  Training Loss  3.053542604902759e-05\n",
            "Epoch  24 Batch  426 / 525  Training Loss  3.4627933928277344e-05\n",
            "Epoch  24 Batch  427 / 525  Training Loss  3.836523319478147e-05\n",
            "Epoch  24 Batch  428 / 525  Training Loss  4.42549935542047e-05\n",
            "Epoch  24 Batch  429 / 525  Training Loss  4.4101943785790354e-05\n",
            "Epoch  24 Batch  430 / 525  Training Loss  3.67452266800683e-05\n",
            "Epoch  24 Batch  431 / 525  Training Loss  3.601253411034122e-05\n",
            "Epoch  24 Batch  432 / 525  Training Loss  3.357217428856529e-05\n",
            "Epoch  24 Batch  433 / 525  Training Loss  3.334944631205872e-05\n",
            "Epoch  24 Batch  434 / 525  Training Loss  4.242557770339772e-05\n",
            "Epoch  24 Batch  435 / 525  Training Loss  2.9072989491396584e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  24 Batch  436 / 525  Training Loss  4.4544885895447806e-05\n",
            "Epoch  24 Batch  437 / 525  Training Loss  3.851849396596663e-05\n",
            "Epoch  24 Batch  438 / 525  Training Loss  3.6874436773359776e-05\n",
            "Epoch  24 Batch  439 / 525  Training Loss  3.1588504498358816e-05\n",
            "Epoch  24 Batch  440 / 525  Training Loss  3.5488490539137274e-05\n",
            "Epoch  24 Batch  441 / 525  Training Loss  3.348913378431462e-05\n",
            "Epoch  24 Batch  442 / 525  Training Loss  3.763441782211885e-05\n",
            "Epoch  24 Batch  443 / 525  Training Loss  3.8254842365859076e-05\n",
            "Epoch  24 Batch  444 / 525  Training Loss  4.430274566402659e-05\n",
            "Epoch  24 Batch  445 / 525  Training Loss  3.0431732739089057e-05\n",
            "Epoch  24 Batch  446 / 525  Training Loss  2.6962585252476856e-05\n",
            "Epoch  24 Batch  447 / 525  Training Loss  3.725100032170303e-05\n",
            "Epoch  24 Batch  448 / 525  Training Loss  5.24672977917362e-05\n",
            "Epoch  24 Batch  449 / 525  Training Loss  4.485536555876024e-05\n",
            "Epoch  24 Batch  450 / 525  Training Loss  3.8788191886851564e-05\n",
            "Epoch  24 Batch  451 / 525  Training Loss  2.4050134015851654e-05\n",
            "Epoch  24 Batch  452 / 525  Training Loss  4.166434518992901e-05\n",
            "Epoch  24 Batch  453 / 525  Training Loss  3.9202765037771314e-05\n",
            "Epoch  24 Batch  454 / 525  Training Loss  4.3283602281007916e-05\n",
            "Epoch  24 Batch  455 / 525  Training Loss  3.216960249119438e-05\n",
            "Epoch  24 Batch  456 / 525  Training Loss  3.0582705221604556e-05\n",
            "Epoch  24 Batch  457 / 525  Training Loss  3.419776476221159e-05\n",
            "Epoch  24 Batch  458 / 525  Training Loss  3.648941492429003e-05\n",
            "Epoch  24 Batch  459 / 525  Training Loss  3.546296647982672e-05\n",
            "Epoch  24 Batch  460 / 525  Training Loss  2.6537940357229672e-05\n",
            "Epoch  24 Batch  461 / 525  Training Loss  3.472226671874523e-05\n",
            "Epoch  24 Batch  462 / 525  Training Loss  3.4178759960923344e-05\n",
            "Epoch  24 Batch  463 / 525  Training Loss  5.227079236647114e-05\n",
            "Epoch  24 Batch  464 / 525  Training Loss  3.6753837775904685e-05\n",
            "Epoch  24 Batch  465 / 525  Training Loss  2.946765744127333e-05\n",
            "Epoch  24 Batch  466 / 525  Training Loss  3.3851796615635976e-05\n",
            "Epoch  24 Batch  467 / 525  Training Loss  2.7540751034393907e-05\n",
            "Epoch  24 Batch  468 / 525  Training Loss  2.8432721592253074e-05\n",
            "Epoch  24 Batch  469 / 525  Training Loss  4.800259557669051e-05\n",
            "Epoch  24 Batch  470 / 525  Training Loss  3.418569758650847e-05\n",
            "Epoch  24 Batch  471 / 525  Training Loss  2.9525243007810786e-05\n",
            "Epoch  24 Batch  472 / 525  Training Loss  4.6084358473308384e-05\n",
            "Epoch  24 Batch  473 / 525  Training Loss  5.4447969887405634e-05\n",
            "Epoch  24 Batch  474 / 525  Training Loss  2.909994509536773e-05\n",
            "Epoch  24 Batch  475 / 525  Training Loss  2.975870120280888e-05\n",
            "Epoch  24 Batch  476 / 525  Training Loss  2.8194888727739453e-05\n",
            "Epoch  24 Batch  477 / 525  Training Loss  3.6988745705457404e-05\n",
            "Epoch  24 Batch  478 / 525  Training Loss  3.0859016987960786e-05\n",
            "Epoch  24 Batch  479 / 525  Training Loss  3.732544428203255e-05\n",
            "Epoch  24 Batch  480 / 525  Training Loss  4.927019472233951e-05\n",
            "Epoch  24 Batch  481 / 525  Training Loss  4.2170653614448383e-05\n",
            "Epoch  24 Batch  482 / 525  Training Loss  3.417137486394495e-05\n",
            "Epoch  24 Batch  483 / 525  Training Loss  3.275138806202449e-05\n",
            "Epoch  24 Batch  484 / 525  Training Loss  4.158433512202464e-05\n",
            "Epoch  24 Batch  485 / 525  Training Loss  3.383737202966586e-05\n",
            "Epoch  24 Batch  486 / 525  Training Loss  5.01174945384264e-05\n",
            "Epoch  24 Batch  487 / 525  Training Loss  3.051730163861066e-05\n",
            "Epoch  24 Batch  488 / 525  Training Loss  3.340315743116662e-05\n",
            "Epoch  24 Batch  489 / 525  Training Loss  3.544936771504581e-05\n",
            "Epoch  24 Batch  490 / 525  Training Loss  4.2039282561745495e-05\n",
            "Epoch  24 Batch  491 / 525  Training Loss  3.898996510542929e-05\n",
            "Epoch  24 Batch  492 / 525  Training Loss  4.1620318370405585e-05\n",
            "Epoch  24 Batch  493 / 525  Training Loss  4.1399234760319814e-05\n",
            "Epoch  24 Batch  494 / 525  Training Loss  2.929924266936723e-05\n",
            "Epoch  24 Batch  495 / 525  Training Loss  3.0541865271516144e-05\n",
            "Epoch  24 Batch  496 / 525  Training Loss  3.548185122781433e-05\n",
            "Epoch  24 Batch  497 / 525  Training Loss  3.247246058890596e-05\n",
            "Epoch  24 Batch  498 / 525  Training Loss  2.8696435038000345e-05\n",
            "Epoch  24 Batch  499 / 525  Training Loss  2.2886528313392773e-05\n",
            "Epoch  24 Batch  500 / 525  Training Loss  4.545942647382617e-05\n",
            "Epoch  24 Batch  501 / 525  Training Loss  5.5122218327596784e-05\n",
            "Epoch  24 Batch  502 / 525  Training Loss  4.8511192289879546e-05\n",
            "Epoch  24 Batch  503 / 525  Training Loss  5.434218473965302e-05\n",
            "Epoch  24 Batch  504 / 525  Training Loss  5.456948565552011e-05\n",
            "Epoch  24 Batch  505 / 525  Training Loss  3.543739876477048e-05\n",
            "Epoch  24 Batch  506 / 525  Training Loss  3.3602129406062886e-05\n",
            "Epoch  24 Batch  507 / 525  Training Loss  4.467224789550528e-05\n",
            "Epoch  24 Batch  508 / 525  Training Loss  2.970199602714274e-05\n",
            "Epoch  24 Batch  509 / 525  Training Loss  3.527084845700301e-05\n",
            "Epoch  24 Batch  510 / 525  Training Loss  6.123638013377786e-05\n",
            "Epoch  24 Batch  511 / 525  Training Loss  2.5474564608884975e-05\n",
            "Epoch  24 Batch  512 / 525  Training Loss  4.705994069809094e-05\n",
            "Epoch  24 Batch  513 / 525  Training Loss  3.2479623769177124e-05\n",
            "Epoch  24 Batch  514 / 525  Training Loss  3.871063745464198e-05\n",
            "Epoch  24 Batch  515 / 525  Training Loss  2.6759040338220075e-05\n",
            "Epoch  24 Batch  516 / 525  Training Loss  5.22963164257817e-05\n",
            "Epoch  24 Batch  517 / 525  Training Loss  4.204003926133737e-05\n",
            "Epoch  24 Batch  518 / 525  Training Loss  4.158437513979152e-05\n",
            "Epoch  24 Batch  519 / 525  Training Loss  4.754645487992093e-05\n",
            "Epoch  24 Batch  520 / 525  Training Loss  4.31597072747536e-05\n",
            "Epoch  24 Batch  521 / 525  Training Loss  3.4671851608436555e-05\n",
            "Epoch  24 Batch  522 / 525  Training Loss  3.458988430793397e-05\n",
            "Epoch  24 Batch  523 / 525  Training Loss  3.4552278521005064e-05\n",
            "Epoch  24 Batch  524 / 525  Training Loss  3.406128234928474e-05\n",
            "  25    |    -    |   0.000039   |   64.19  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 25\n",
            "Epoch  25 Batch  0 / 525  Training Loss  4.0552073187427595e-05\n",
            "Epoch  25 Batch  1 / 525  Training Loss  4.917817568639293e-05\n",
            "Epoch  25 Batch  2 / 525  Training Loss  3.9604827179573476e-05\n",
            "Epoch  25 Batch  3 / 525  Training Loss  3.816404932877049e-05\n",
            "Epoch  25 Batch  4 / 525  Training Loss  3.064738120883703e-05\n",
            "Epoch  25 Batch  5 / 525  Training Loss  3.8952366594458e-05\n",
            "Epoch  25 Batch  6 / 525  Training Loss  4.01782090193592e-05\n",
            "Epoch  25 Batch  7 / 525  Training Loss  2.879076055251062e-05\n",
            "Epoch  25 Batch  8 / 525  Training Loss  4.38434726675041e-05\n",
            "Epoch  25 Batch  9 / 525  Training Loss  4.8099558625835925e-05\n",
            "Epoch  25 Batch  10 / 525  Training Loss  3.234645555494353e-05\n",
            "Epoch  25 Batch  11 / 525  Training Loss  3.4191780287073925e-05\n",
            "Epoch  25 Batch  12 / 525  Training Loss  3.0265600798884407e-05\n",
            "Epoch  25 Batch  13 / 525  Training Loss  4.1782772314036265e-05\n",
            "Epoch  25 Batch  14 / 525  Training Loss  2.584377034509089e-05\n",
            "Epoch  25 Batch  15 / 525  Training Loss  4.098930367035791e-05\n",
            "Epoch  25 Batch  16 / 525  Training Loss  3.686695345095359e-05\n",
            "Epoch  25 Batch  17 / 525  Training Loss  3.389318590052426e-05\n",
            "Epoch  25 Batch  18 / 525  Training Loss  2.9519123927457258e-05\n",
            "Epoch  25 Batch  19 / 525  Training Loss  3.3311509469058365e-05\n",
            "Epoch  25 Batch  20 / 525  Training Loss  3.697697684401646e-05\n",
            "Epoch  25 Batch  21 / 525  Training Loss  2.355649121454917e-05\n",
            "Epoch  25 Batch  22 / 525  Training Loss  2.7596503059612587e-05\n",
            "Epoch  25 Batch  23 / 525  Training Loss  2.98941922665108e-05\n",
            "Epoch  25 Batch  24 / 525  Training Loss  3.923924668924883e-05\n",
            "Epoch  25 Batch  25 / 525  Training Loss  4.2027622839668766e-05\n",
            "Epoch  25 Batch  26 / 525  Training Loss  3.004696554853581e-05\n",
            "Epoch  25 Batch  27 / 525  Training Loss  2.589551695564296e-05\n",
            "Epoch  25 Batch  28 / 525  Training Loss  2.783744093903806e-05\n",
            "Epoch  25 Batch  29 / 525  Training Loss  3.693710459629074e-05\n",
            "Epoch  25 Batch  30 / 525  Training Loss  2.6731155230663717e-05\n",
            "Epoch  25 Batch  31 / 525  Training Loss  2.3821703507564962e-05\n",
            "Epoch  25 Batch  32 / 525  Training Loss  2.86665563180577e-05\n",
            "Epoch  25 Batch  33 / 525  Training Loss  4.52205931651406e-05\n",
            "Epoch  25 Batch  34 / 525  Training Loss  3.624977034633048e-05\n",
            "Epoch  25 Batch  35 / 525  Training Loss  3.926022327505052e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  25 Batch  36 / 525  Training Loss  3.542060949257575e-05\n",
            "Epoch  25 Batch  37 / 525  Training Loss  3.6188117519486696e-05\n",
            "Epoch  25 Batch  38 / 525  Training Loss  3.0335044357343577e-05\n",
            "Epoch  25 Batch  39 / 525  Training Loss  2.9856968467356637e-05\n",
            "Epoch  25 Batch  40 / 525  Training Loss  3.2851789001142606e-05\n",
            "Epoch  25 Batch  41 / 525  Training Loss  3.6623881896957755e-05\n",
            "Epoch  25 Batch  42 / 525  Training Loss  4.900954809272662e-05\n",
            "Epoch  25 Batch  43 / 525  Training Loss  3.3763320971047506e-05\n",
            "Epoch  25 Batch  44 / 525  Training Loss  3.646373443189077e-05\n",
            "Epoch  25 Batch  45 / 525  Training Loss  2.6652200176613405e-05\n",
            "Epoch  25 Batch  46 / 525  Training Loss  3.161099448334426e-05\n",
            "Epoch  25 Batch  47 / 525  Training Loss  3.6069708585273474e-05\n",
            "Epoch  25 Batch  48 / 525  Training Loss  2.7739655706682242e-05\n",
            "Epoch  25 Batch  49 / 525  Training Loss  3.678834400488995e-05\n",
            "Epoch  25 Batch  50 / 525  Training Loss  3.461947926552966e-05\n",
            "Epoch  25 Batch  51 / 525  Training Loss  3.71145797544159e-05\n",
            "Epoch  25 Batch  52 / 525  Training Loss  2.6902465833700262e-05\n",
            "Epoch  25 Batch  53 / 525  Training Loss  2.966662577819079e-05\n",
            "Epoch  25 Batch  54 / 525  Training Loss  3.713212936418131e-05\n",
            "Epoch  25 Batch  55 / 525  Training Loss  2.6747558877104893e-05\n",
            "Epoch  25 Batch  56 / 525  Training Loss  3.382031718501821e-05\n",
            "Epoch  25 Batch  57 / 525  Training Loss  3.922616451745853e-05\n",
            "Epoch  25 Batch  58 / 525  Training Loss  4.068138514412567e-05\n",
            "Epoch  25 Batch  59 / 525  Training Loss  3.0126398996799253e-05\n",
            "Epoch  25 Batch  60 / 525  Training Loss  3.378404653631151e-05\n",
            "Epoch  25 Batch  61 / 525  Training Loss  3.0286397304735146e-05\n",
            "Epoch  25 Batch  62 / 525  Training Loss  2.4294820832437836e-05\n",
            "Epoch  25 Batch  63 / 525  Training Loss  2.588762072264217e-05\n",
            "Epoch  25 Batch  64 / 525  Training Loss  2.1194508008193225e-05\n",
            "Epoch  25 Batch  65 / 525  Training Loss  3.880113217746839e-05\n",
            "Epoch  25 Batch  66 / 525  Training Loss  2.931217932200525e-05\n",
            "Epoch  25 Batch  67 / 525  Training Loss  4.073029413120821e-05\n",
            "Epoch  25 Batch  68 / 525  Training Loss  3.0374765628948808e-05\n",
            "Epoch  25 Batch  69 / 525  Training Loss  3.0469989724224433e-05\n",
            "Epoch  25 Batch  70 / 525  Training Loss  3.2384239602833986e-05\n",
            "Epoch  25 Batch  71 / 525  Training Loss  3.098968954873271e-05\n",
            "Epoch  25 Batch  72 / 525  Training Loss  2.9906823328929022e-05\n",
            "Epoch  25 Batch  73 / 525  Training Loss  3.015385300386697e-05\n",
            "Epoch  25 Batch  74 / 525  Training Loss  3.315735375508666e-05\n",
            "Epoch  25 Batch  75 / 525  Training Loss  3.267997090006247e-05\n",
            "Epoch  25 Batch  76 / 525  Training Loss  2.7543017495190725e-05\n",
            "Epoch  25 Batch  77 / 525  Training Loss  4.1198174585588276e-05\n",
            "Epoch  25 Batch  78 / 525  Training Loss  3.4322387364227325e-05\n",
            "Epoch  25 Batch  79 / 525  Training Loss  4.6741781261516735e-05\n",
            "Epoch  25 Batch  80 / 525  Training Loss  4.049166454933584e-05\n",
            "Epoch  25 Batch  81 / 525  Training Loss  3.228114655939862e-05\n",
            "Epoch  25 Batch  82 / 525  Training Loss  3.40508995577693e-05\n",
            "Epoch  25 Batch  83 / 525  Training Loss  4.135102790314704e-05\n",
            "Epoch  25 Batch  84 / 525  Training Loss  3.364994336152449e-05\n",
            "Epoch  25 Batch  85 / 525  Training Loss  3.547020605765283e-05\n",
            "Epoch  25 Batch  86 / 525  Training Loss  4.658383841160685e-05\n",
            "Epoch  25 Batch  87 / 525  Training Loss  3.203183950972743e-05\n",
            "Epoch  25 Batch  88 / 525  Training Loss  4.080110738868825e-05\n",
            "Epoch  25 Batch  89 / 525  Training Loss  3.6344041291158646e-05\n",
            "Epoch  25 Batch  90 / 525  Training Loss  2.8942249628016725e-05\n",
            "Epoch  25 Batch  91 / 525  Training Loss  3.147675670334138e-05\n",
            "Epoch  25 Batch  92 / 525  Training Loss  3.7934962165309116e-05\n",
            "Epoch  25 Batch  93 / 525  Training Loss  3.42736420861911e-05\n",
            "Epoch  25 Batch  94 / 525  Training Loss  3.0341067031258717e-05\n",
            "Epoch  25 Batch  95 / 525  Training Loss  3.280752571299672e-05\n",
            "Epoch  25 Batch  96 / 525  Training Loss  4.0935588913271204e-05\n",
            "Epoch  25 Batch  97 / 525  Training Loss  3.3012431231327355e-05\n",
            "Epoch  25 Batch  98 / 525  Training Loss  3.669953002827242e-05\n",
            "Epoch  25 Batch  99 / 525  Training Loss  4.049912604386918e-05\n",
            "Epoch  25 Batch  100 / 525  Training Loss  3.364290387253277e-05\n",
            "Epoch  25 Batch  101 / 525  Training Loss  3.956145519623533e-05\n",
            "Epoch  25 Batch  102 / 525  Training Loss  3.5192613722756505e-05\n",
            "Epoch  25 Batch  103 / 525  Training Loss  2.4745211703702807e-05\n",
            "Epoch  25 Batch  104 / 525  Training Loss  3.659790309029631e-05\n",
            "Epoch  25 Batch  105 / 525  Training Loss  3.359778202138841e-05\n",
            "Epoch  25 Batch  106 / 525  Training Loss  3.6210985854268074e-05\n",
            "Epoch  25 Batch  107 / 525  Training Loss  3.261644815211184e-05\n",
            "Epoch  25 Batch  108 / 525  Training Loss  4.034385710838251e-05\n",
            "Epoch  25 Batch  109 / 525  Training Loss  4.117356729693711e-05\n",
            "Epoch  25 Batch  110 / 525  Training Loss  3.199553975719027e-05\n",
            "Epoch  25 Batch  111 / 525  Training Loss  3.41778650181368e-05\n",
            "Epoch  25 Batch  112 / 525  Training Loss  4.360498860478401e-05\n",
            "Epoch  25 Batch  113 / 525  Training Loss  3.066073986701667e-05\n",
            "Epoch  25 Batch  114 / 525  Training Loss  2.9542536140070297e-05\n",
            "Epoch  25 Batch  115 / 525  Training Loss  2.622014108055737e-05\n",
            "Epoch  25 Batch  116 / 525  Training Loss  3.061578536289744e-05\n",
            "Epoch  25 Batch  117 / 525  Training Loss  2.8841688617831096e-05\n",
            "Epoch  25 Batch  118 / 525  Training Loss  4.335434641689062e-05\n",
            "Epoch  25 Batch  119 / 525  Training Loss  2.19405374082271e-05\n",
            "Epoch  25 Batch  120 / 525  Training Loss  3.494363772915676e-05\n",
            "Epoch  25 Batch  121 / 525  Training Loss  2.4846798623912036e-05\n",
            "Epoch  25 Batch  122 / 525  Training Loss  2.636516364873387e-05\n",
            "Epoch  25 Batch  123 / 525  Training Loss  3.852619556710124e-05\n",
            "Epoch  25 Batch  124 / 525  Training Loss  3.1657156796427444e-05\n",
            "Epoch  25 Batch  125 / 525  Training Loss  3.2706284400774166e-05\n",
            "Epoch  25 Batch  126 / 525  Training Loss  4.3721080146497115e-05\n",
            "Epoch  25 Batch  127 / 525  Training Loss  3.749031020561233e-05\n",
            "Epoch  25 Batch  128 / 525  Training Loss  3.6153131077298895e-05\n",
            "Epoch  25 Batch  129 / 525  Training Loss  3.312428088975139e-05\n",
            "Epoch  25 Batch  130 / 525  Training Loss  4.2918341932818294e-05\n",
            "Epoch  25 Batch  131 / 525  Training Loss  3.5706019843928516e-05\n",
            "Epoch  25 Batch  132 / 525  Training Loss  3.6065106542082503e-05\n",
            "Epoch  25 Batch  133 / 525  Training Loss  3.859698335872963e-05\n",
            "Epoch  25 Batch  134 / 525  Training Loss  4.612818156601861e-05\n",
            "Epoch  25 Batch  135 / 525  Training Loss  4.136488860240206e-05\n",
            "Epoch  25 Batch  136 / 525  Training Loss  2.5606557755963877e-05\n",
            "Epoch  25 Batch  137 / 525  Training Loss  3.586809179978445e-05\n",
            "Epoch  25 Batch  138 / 525  Training Loss  2.598869104986079e-05\n",
            "Epoch  25 Batch  139 / 525  Training Loss  3.692616883199662e-05\n",
            "Epoch  25 Batch  140 / 525  Training Loss  2.7577538276091218e-05\n",
            "Epoch  25 Batch  141 / 525  Training Loss  3.540969191817567e-05\n",
            "Epoch  25 Batch  142 / 525  Training Loss  2.973168739117682e-05\n",
            "Epoch  25 Batch  143 / 525  Training Loss  3.456888589425944e-05\n",
            "Epoch  25 Batch  144 / 525  Training Loss  3.211654620827176e-05\n",
            "Epoch  25 Batch  145 / 525  Training Loss  3.134172220597975e-05\n",
            "Epoch  25 Batch  146 / 525  Training Loss  3.564189319149591e-05\n",
            "Epoch  25 Batch  147 / 525  Training Loss  3.532135087880306e-05\n",
            "Epoch  25 Batch  148 / 525  Training Loss  3.7943405914120376e-05\n",
            "Epoch  25 Batch  149 / 525  Training Loss  2.5388342692167498e-05\n",
            "Epoch  25 Batch  150 / 525  Training Loss  4.2366256820969284e-05\n",
            "Epoch  25 Batch  151 / 525  Training Loss  2.4292192392749712e-05\n",
            "Epoch  25 Batch  152 / 525  Training Loss  3.155637750751339e-05\n",
            "Epoch  25 Batch  153 / 525  Training Loss  3.1790637876838446e-05\n",
            "Epoch  25 Batch  154 / 525  Training Loss  5.240321115707047e-05\n",
            "Epoch  25 Batch  155 / 525  Training Loss  4.286799958208576e-05\n",
            "Epoch  25 Batch  156 / 525  Training Loss  3.364936128491536e-05\n",
            "Epoch  25 Batch  157 / 525  Training Loss  3.498224396025762e-05\n",
            "Epoch  25 Batch  158 / 525  Training Loss  4.0911207179306075e-05\n",
            "Epoch  25 Batch  159 / 525  Training Loss  2.242543632746674e-05\n",
            "Epoch  25 Batch  160 / 525  Training Loss  4.05693135689944e-05\n",
            "Epoch  25 Batch  161 / 525  Training Loss  4.197171801934019e-05\n",
            "Epoch  25 Batch  162 / 525  Training Loss  4.432161949807778e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  25 Batch  163 / 525  Training Loss  3.839562486973591e-05\n",
            "Epoch  25 Batch  164 / 525  Training Loss  3.44649342878256e-05\n",
            "Epoch  25 Batch  165 / 525  Training Loss  3.112948979833163e-05\n",
            "Epoch  25 Batch  166 / 525  Training Loss  4.0605700633022934e-05\n",
            "Epoch  25 Batch  167 / 525  Training Loss  3.5502474929671735e-05\n",
            "Epoch  25 Batch  168 / 525  Training Loss  3.607801772886887e-05\n",
            "Epoch  25 Batch  169 / 525  Training Loss  2.8527636459330097e-05\n",
            "Epoch  25 Batch  170 / 525  Training Loss  2.3804674128768966e-05\n",
            "Epoch  25 Batch  171 / 525  Training Loss  4.573121987050399e-05\n",
            "Epoch  25 Batch  172 / 525  Training Loss  4.1592997149564326e-05\n",
            "Epoch  25 Batch  173 / 525  Training Loss  3.505013955873437e-05\n",
            "Epoch  25 Batch  174 / 525  Training Loss  2.9036711566732265e-05\n",
            "Epoch  25 Batch  175 / 525  Training Loss  4.570405144477263e-05\n",
            "Epoch  25 Batch  176 / 525  Training Loss  3.7263216654537246e-05\n",
            "Epoch  25 Batch  177 / 525  Training Loss  3.0376471841009334e-05\n",
            "Epoch  25 Batch  178 / 525  Training Loss  2.363456951570697e-05\n",
            "Epoch  25 Batch  179 / 525  Training Loss  3.6847155570285395e-05\n",
            "Epoch  25 Batch  180 / 525  Training Loss  4.209824692225084e-05\n",
            "Epoch  25 Batch  181 / 525  Training Loss  4.926233668811619e-05\n",
            "Epoch  25 Batch  182 / 525  Training Loss  2.4809512979118153e-05\n",
            "Epoch  25 Batch  183 / 525  Training Loss  3.8330937968567014e-05\n",
            "Epoch  25 Batch  184 / 525  Training Loss  3.238417411921546e-05\n",
            "Epoch  25 Batch  185 / 525  Training Loss  3.519107849569991e-05\n",
            "Epoch  25 Batch  186 / 525  Training Loss  3.020410986209754e-05\n",
            "Epoch  25 Batch  187 / 525  Training Loss  2.8752576326951385e-05\n",
            "Epoch  25 Batch  188 / 525  Training Loss  4.4349795643938705e-05\n",
            "Epoch  25 Batch  189 / 525  Training Loss  2.3484173652832396e-05\n",
            "Epoch  25 Batch  190 / 525  Training Loss  4.89357698825188e-05\n",
            "Epoch  25 Batch  191 / 525  Training Loss  2.7561463866732083e-05\n",
            "Epoch  25 Batch  192 / 525  Training Loss  3.0480867280857638e-05\n",
            "Epoch  25 Batch  193 / 525  Training Loss  3.515868593240157e-05\n",
            "Epoch  25 Batch  194 / 525  Training Loss  2.9535829526139423e-05\n",
            "Epoch  25 Batch  195 / 525  Training Loss  5.302678982843645e-05\n",
            "Epoch  25 Batch  196 / 525  Training Loss  3.339254544698633e-05\n",
            "Epoch  25 Batch  197 / 525  Training Loss  3.3318287023575976e-05\n",
            "Epoch  25 Batch  198 / 525  Training Loss  3.906040728907101e-05\n",
            "Epoch  25 Batch  199 / 525  Training Loss  4.07817424274981e-05\n",
            "Epoch  25 Batch  200 / 525  Training Loss  3.911980093107559e-05\n",
            "Epoch  25 Batch  201 / 525  Training Loss  3.3706557587720454e-05\n",
            "Epoch  25 Batch  202 / 525  Training Loss  3.407241092645563e-05\n",
            "Epoch  25 Batch  203 / 525  Training Loss  3.3582378819119185e-05\n",
            "Epoch  25 Batch  204 / 525  Training Loss  3.278191070421599e-05\n",
            "Epoch  25 Batch  205 / 525  Training Loss  3.4019005397567526e-05\n",
            "Epoch  25 Batch  206 / 525  Training Loss  3.417769767111167e-05\n",
            "Epoch  25 Batch  207 / 525  Training Loss  2.7503154342412017e-05\n",
            "Epoch  25 Batch  208 / 525  Training Loss  3.086257856921293e-05\n",
            "Epoch  25 Batch  209 / 525  Training Loss  2.723181387409568e-05\n",
            "Epoch  25 Batch  210 / 525  Training Loss  2.68840176431695e-05\n",
            "Epoch  25 Batch  211 / 525  Training Loss  3.5103184927720577e-05\n",
            "Epoch  25 Batch  212 / 525  Training Loss  3.601573553169146e-05\n",
            "Epoch  25 Batch  213 / 525  Training Loss  2.8643484256463125e-05\n",
            "Epoch  25 Batch  214 / 525  Training Loss  3.295034548500553e-05\n",
            "Epoch  25 Batch  215 / 525  Training Loss  3.279071097495034e-05\n",
            "Epoch  25 Batch  216 / 525  Training Loss  4.162154073128477e-05\n",
            "Epoch  25 Batch  217 / 525  Training Loss  3.196543548256159e-05\n",
            "Epoch  25 Batch  218 / 525  Training Loss  3.8987484003882855e-05\n",
            "Epoch  25 Batch  219 / 525  Training Loss  4.688900298788212e-05\n",
            "Epoch  25 Batch  220 / 525  Training Loss  4.261733920429833e-05\n",
            "Epoch  25 Batch  221 / 525  Training Loss  3.750700125237927e-05\n",
            "Epoch  25 Batch  222 / 525  Training Loss  3.6703153455164284e-05\n",
            "Epoch  25 Batch  223 / 525  Training Loss  3.2789946999400854e-05\n",
            "Epoch  25 Batch  224 / 525  Training Loss  2.9890035875723697e-05\n",
            "Epoch  25 Batch  225 / 525  Training Loss  3.791479684878141e-05\n",
            "Epoch  25 Batch  226 / 525  Training Loss  3.591196218621917e-05\n",
            "Epoch  25 Batch  227 / 525  Training Loss  2.2260501282289624e-05\n",
            "Epoch  25 Batch  228 / 525  Training Loss  3.9522408769698814e-05\n",
            "Epoch  25 Batch  229 / 525  Training Loss  2.5967785404645838e-05\n",
            "Epoch  25 Batch  230 / 525  Training Loss  3.421397559577599e-05\n",
            "Epoch  25 Batch  231 / 525  Training Loss  2.7466108804219402e-05\n",
            "Epoch  25 Batch  232 / 525  Training Loss  2.9441998776746914e-05\n",
            "Epoch  25 Batch  233 / 525  Training Loss  3.219820428057574e-05\n",
            "Epoch  25 Batch  234 / 525  Training Loss  3.592035864130594e-05\n",
            "Epoch  25 Batch  235 / 525  Training Loss  3.233691677451134e-05\n",
            "Epoch  25 Batch  236 / 525  Training Loss  3.8879083149367943e-05\n",
            "Epoch  25 Batch  237 / 525  Training Loss  5.880338721908629e-05\n",
            "Epoch  25 Batch  238 / 525  Training Loss  3.552299313014373e-05\n",
            "Epoch  25 Batch  239 / 525  Training Loss  3.596474198275246e-05\n",
            "Epoch  25 Batch  240 / 525  Training Loss  4.46438389190007e-05\n",
            "Epoch  25 Batch  241 / 525  Training Loss  2.6235220502712764e-05\n",
            "Epoch  25 Batch  242 / 525  Training Loss  3.477518112049438e-05\n",
            "Epoch  25 Batch  243 / 525  Training Loss  3.552763519110158e-05\n",
            "Epoch  25 Batch  244 / 525  Training Loss  3.590224150684662e-05\n",
            "Epoch  25 Batch  245 / 525  Training Loss  2.616176425362937e-05\n",
            "Epoch  25 Batch  246 / 525  Training Loss  2.0643090465455316e-05\n",
            "Epoch  25 Batch  247 / 525  Training Loss  3.0235842132242396e-05\n",
            "Epoch  25 Batch  248 / 525  Training Loss  4.246903699822724e-05\n",
            "Epoch  25 Batch  249 / 525  Training Loss  2.382769889663905e-05\n",
            "Epoch  25 Batch  250 / 525  Training Loss  2.5864103008643724e-05\n",
            "Epoch  25 Batch  251 / 525  Training Loss  3.2589152397122234e-05\n",
            "Epoch  25 Batch  252 / 525  Training Loss  2.999962816829793e-05\n",
            "Epoch  25 Batch  253 / 525  Training Loss  3.379239933565259e-05\n",
            "Epoch  25 Batch  254 / 525  Training Loss  3.782584462896921e-05\n",
            "Epoch  25 Batch  255 / 525  Training Loss  3.1549188861390576e-05\n",
            "Epoch  25 Batch  256 / 525  Training Loss  3.1501269404543564e-05\n",
            "Epoch  25 Batch  257 / 525  Training Loss  3.532097252900712e-05\n",
            "Epoch  25 Batch  258 / 525  Training Loss  3.660139918792993e-05\n",
            "Epoch  25 Batch  259 / 525  Training Loss  3.092488623224199e-05\n",
            "Epoch  25 Batch  260 / 525  Training Loss  2.3822858565836214e-05\n",
            "Epoch  25 Batch  261 / 525  Training Loss  3.7157089536776766e-05\n",
            "Epoch  25 Batch  262 / 525  Training Loss  2.5859311790554784e-05\n",
            "Epoch  25 Batch  263 / 525  Training Loss  3.973270577262156e-05\n",
            "Epoch  25 Batch  264 / 525  Training Loss  3.331245534354821e-05\n",
            "Epoch  25 Batch  265 / 525  Training Loss  2.6277009965269826e-05\n",
            "Epoch  25 Batch  266 / 525  Training Loss  4.113551403861493e-05\n",
            "Epoch  25 Batch  267 / 525  Training Loss  3.8122121623018757e-05\n",
            "Epoch  25 Batch  268 / 525  Training Loss  2.7810630854219198e-05\n",
            "Epoch  25 Batch  269 / 525  Training Loss  3.5478569770930335e-05\n",
            "Epoch  25 Batch  270 / 525  Training Loss  2.6250501832691953e-05\n",
            "Epoch  25 Batch  271 / 525  Training Loss  3.271380046498962e-05\n",
            "Epoch  25 Batch  272 / 525  Training Loss  3.25031069223769e-05\n",
            "Epoch  25 Batch  273 / 525  Training Loss  2.5716803065733984e-05\n",
            "Epoch  25 Batch  274 / 525  Training Loss  3.509457383188419e-05\n",
            "Epoch  25 Batch  275 / 525  Training Loss  2.3715803763479926e-05\n",
            "Epoch  25 Batch  276 / 525  Training Loss  3.3506024919915944e-05\n",
            "Epoch  25 Batch  277 / 525  Training Loss  3.63736180588603e-05\n",
            "Epoch  25 Batch  278 / 525  Training Loss  5.3443032811628655e-05\n",
            "Epoch  25 Batch  279 / 525  Training Loss  3.6053410440217704e-05\n",
            "Epoch  25 Batch  280 / 525  Training Loss  3.0200759283616208e-05\n",
            "Epoch  25 Batch  281 / 525  Training Loss  4.0958278987091035e-05\n",
            "Epoch  25 Batch  282 / 525  Training Loss  3.684938928927295e-05\n",
            "Epoch  25 Batch  283 / 525  Training Loss  4.537948552751914e-05\n",
            "Epoch  25 Batch  284 / 525  Training Loss  2.618294456624426e-05\n",
            "Epoch  25 Batch  285 / 525  Training Loss  3.7485366192413494e-05\n",
            "Epoch  25 Batch  286 / 525  Training Loss  5.025367863709107e-05\n",
            "Epoch  25 Batch  287 / 525  Training Loss  2.9321348847588524e-05\n",
            "Epoch  25 Batch  288 / 525  Training Loss  2.84421730611939e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  25 Batch  289 / 525  Training Loss  3.896377165801823e-05\n",
            "Epoch  25 Batch  290 / 525  Training Loss  4.027066825074144e-05\n",
            "Epoch  25 Batch  291 / 525  Training Loss  2.7055148166255094e-05\n",
            "Epoch  25 Batch  292 / 525  Training Loss  4.4765118218492717e-05\n",
            "Epoch  25 Batch  293 / 525  Training Loss  3.862931771436706e-05\n",
            "Epoch  25 Batch  294 / 525  Training Loss  4.6085559006314725e-05\n",
            "Epoch  25 Batch  295 / 525  Training Loss  2.7773485271609388e-05\n",
            "Epoch  25 Batch  296 / 525  Training Loss  4.7898451157379895e-05\n",
            "Epoch  25 Batch  297 / 525  Training Loss  4.060252831550315e-05\n",
            "Epoch  25 Batch  298 / 525  Training Loss  4.0177517803385854e-05\n",
            "Epoch  25 Batch  299 / 525  Training Loss  4.305733818910085e-05\n",
            "Epoch  25 Batch  300 / 525  Training Loss  4.035557867609896e-05\n",
            "Epoch  25 Batch  301 / 525  Training Loss  3.924777411157265e-05\n",
            "Epoch  25 Batch  302 / 525  Training Loss  2.893901546485722e-05\n",
            "Epoch  25 Batch  303 / 525  Training Loss  3.55696574843023e-05\n",
            "Epoch  25 Batch  304 / 525  Training Loss  3.672353341244161e-05\n",
            "Epoch  25 Batch  305 / 525  Training Loss  4.12456392950844e-05\n",
            "Epoch  25 Batch  306 / 525  Training Loss  3.8419362681452185e-05\n",
            "Epoch  25 Batch  307 / 525  Training Loss  2.745704841800034e-05\n",
            "Epoch  25 Batch  308 / 525  Training Loss  3.310830652480945e-05\n",
            "Epoch  25 Batch  309 / 525  Training Loss  3.182046566507779e-05\n",
            "Epoch  25 Batch  310 / 525  Training Loss  3.502456092974171e-05\n",
            "Epoch  25 Batch  311 / 525  Training Loss  3.2890395232243463e-05\n",
            "Epoch  25 Batch  312 / 525  Training Loss  3.318217568448745e-05\n",
            "Epoch  25 Batch  313 / 525  Training Loss  3.642853334895335e-05\n",
            "Epoch  25 Batch  314 / 525  Training Loss  3.406633186386898e-05\n",
            "Epoch  25 Batch  315 / 525  Training Loss  4.3075291614513844e-05\n",
            "Epoch  25 Batch  316 / 525  Training Loss  3.422733789193444e-05\n",
            "Epoch  25 Batch  317 / 525  Training Loss  3.2424570235889405e-05\n",
            "Epoch  25 Batch  318 / 525  Training Loss  3.3079115382861346e-05\n",
            "Epoch  25 Batch  319 / 525  Training Loss  4.220268237986602e-05\n",
            "Epoch  25 Batch  320 / 525  Training Loss  3.976469088229351e-05\n",
            "Epoch  25 Batch  321 / 525  Training Loss  3.8135094655444846e-05\n",
            "Epoch  25 Batch  322 / 525  Training Loss  3.371153798070736e-05\n",
            "Epoch  25 Batch  323 / 525  Training Loss  4.44159159087576e-05\n",
            "Epoch  25 Batch  324 / 525  Training Loss  3.6682275094790384e-05\n",
            "Epoch  25 Batch  325 / 525  Training Loss  4.9983016651822254e-05\n",
            "Epoch  25 Batch  326 / 525  Training Loss  2.8952166758244857e-05\n",
            "Epoch  25 Batch  327 / 525  Training Loss  3.73925176972989e-05\n",
            "Epoch  25 Batch  328 / 525  Training Loss  2.1692421796615236e-05\n",
            "Epoch  25 Batch  329 / 525  Training Loss  3.7484100175788626e-05\n",
            "Epoch  25 Batch  330 / 525  Training Loss  3.4428689104970545e-05\n",
            "Epoch  25 Batch  331 / 525  Training Loss  3.429747448535636e-05\n",
            "Epoch  25 Batch  332 / 525  Training Loss  4.693754090112634e-05\n",
            "Epoch  25 Batch  333 / 525  Training Loss  3.038097383978311e-05\n",
            "Epoch  25 Batch  334 / 525  Training Loss  2.8541286155814305e-05\n",
            "Epoch  25 Batch  335 / 525  Training Loss  4.450607957551256e-05\n",
            "Epoch  25 Batch  336 / 525  Training Loss  2.4538279831176624e-05\n",
            "Epoch  25 Batch  337 / 525  Training Loss  2.1813353669131175e-05\n",
            "Epoch  25 Batch  338 / 525  Training Loss  2.6851766961044632e-05\n",
            "Epoch  25 Batch  339 / 525  Training Loss  3.537485463311896e-05\n",
            "Epoch  25 Batch  340 / 525  Training Loss  3.0426201192312874e-05\n",
            "Epoch  25 Batch  341 / 525  Training Loss  3.3712924050632864e-05\n",
            "Epoch  25 Batch  342 / 525  Training Loss  3.26985027641058e-05\n",
            "Epoch  25 Batch  343 / 525  Training Loss  3.3148615329992026e-05\n",
            "Epoch  25 Batch  344 / 525  Training Loss  3.41336453857366e-05\n",
            "Epoch  25 Batch  345 / 525  Training Loss  4.212036947137676e-05\n",
            "Epoch  25 Batch  346 / 525  Training Loss  2.2678630557493307e-05\n",
            "Epoch  25 Batch  347 / 525  Training Loss  3.3500582503620535e-05\n",
            "Epoch  25 Batch  348 / 525  Training Loss  4.217924288241193e-05\n",
            "Epoch  25 Batch  349 / 525  Training Loss  3.440584987401962e-05\n",
            "Epoch  25 Batch  350 / 525  Training Loss  3.624855889938772e-05\n",
            "Epoch  25 Batch  351 / 525  Training Loss  4.4141059333924204e-05\n",
            "Epoch  25 Batch  352 / 525  Training Loss  4.614703357219696e-05\n",
            "Epoch  25 Batch  353 / 525  Training Loss  3.229904541512951e-05\n",
            "Epoch  25 Batch  354 / 525  Training Loss  3.50071131833829e-05\n",
            "Epoch  25 Batch  355 / 525  Training Loss  4.418715980136767e-05\n",
            "Epoch  25 Batch  356 / 525  Training Loss  3.4002907341346145e-05\n",
            "Epoch  25 Batch  357 / 525  Training Loss  3.8148005842231214e-05\n",
            "Epoch  25 Batch  358 / 525  Training Loss  4.560734305414371e-05\n",
            "Epoch  25 Batch  359 / 525  Training Loss  4.556848216452636e-05\n",
            "Epoch  25 Batch  360 / 525  Training Loss  3.120815381407738e-05\n",
            "Epoch  25 Batch  361 / 525  Training Loss  3.9263872167794034e-05\n",
            "Epoch  25 Batch  362 / 525  Training Loss  3.0148899895721115e-05\n",
            "Epoch  25 Batch  363 / 525  Training Loss  3.9492784708272666e-05\n",
            "Epoch  25 Batch  364 / 525  Training Loss  3.580441261874512e-05\n",
            "Epoch  25 Batch  365 / 525  Training Loss  4.3351650674594566e-05\n",
            "Epoch  25 Batch  366 / 525  Training Loss  3.727138027898036e-05\n",
            "Epoch  25 Batch  367 / 525  Training Loss  3.1341864087153226e-05\n",
            "Epoch  25 Batch  368 / 525  Training Loss  5.051351763540879e-05\n",
            "Epoch  25 Batch  369 / 525  Training Loss  3.991623088950291e-05\n",
            "Epoch  25 Batch  370 / 525  Training Loss  3.307179213152267e-05\n",
            "Epoch  25 Batch  371 / 525  Training Loss  3.209546412108466e-05\n",
            "Epoch  25 Batch  372 / 525  Training Loss  2.8503467547125183e-05\n",
            "Epoch  25 Batch  373 / 525  Training Loss  2.740901618381031e-05\n",
            "Epoch  25 Batch  374 / 525  Training Loss  3.024481156899128e-05\n",
            "Epoch  25 Batch  375 / 525  Training Loss  2.5209988962160423e-05\n",
            "Epoch  25 Batch  376 / 525  Training Loss  2.9991959308972582e-05\n",
            "Epoch  25 Batch  377 / 525  Training Loss  2.9723090847255662e-05\n",
            "Epoch  25 Batch  378 / 525  Training Loss  3.290084350737743e-05\n",
            "Epoch  25 Batch  379 / 525  Training Loss  4.54791916126851e-05\n",
            "Epoch  25 Batch  380 / 525  Training Loss  3.392166763660498e-05\n",
            "Epoch  25 Batch  381 / 525  Training Loss  4.964479012414813e-05\n",
            "Epoch  25 Batch  382 / 525  Training Loss  3.3978387364186347e-05\n",
            "Epoch  25 Batch  383 / 525  Training Loss  2.890641917474568e-05\n",
            "Epoch  25 Batch  384 / 525  Training Loss  3.1867621146375313e-05\n",
            "Epoch  25 Batch  385 / 525  Training Loss  3.435525286477059e-05\n",
            "Epoch  25 Batch  386 / 525  Training Loss  3.2130395993590355e-05\n",
            "Epoch  25 Batch  387 / 525  Training Loss  2.596723243186716e-05\n",
            "Epoch  25 Batch  388 / 525  Training Loss  3.4539792977739125e-05\n",
            "Epoch  25 Batch  389 / 525  Training Loss  3.057568028452806e-05\n",
            "Epoch  25 Batch  390 / 525  Training Loss  3.6188375815982e-05\n",
            "Epoch  25 Batch  391 / 525  Training Loss  3.5166394809493795e-05\n",
            "Epoch  25 Batch  392 / 525  Training Loss  3.7198220525169745e-05\n",
            "Epoch  25 Batch  393 / 525  Training Loss  3.055817069252953e-05\n",
            "Epoch  25 Batch  394 / 525  Training Loss  3.3211435948032886e-05\n",
            "Epoch  25 Batch  395 / 525  Training Loss  2.454433160892222e-05\n",
            "Epoch  25 Batch  396 / 525  Training Loss  3.0433846404775977e-05\n",
            "Epoch  25 Batch  397 / 525  Training Loss  2.7690559363691136e-05\n",
            "Epoch  25 Batch  398 / 525  Training Loss  3.5645003663375974e-05\n",
            "Epoch  25 Batch  399 / 525  Training Loss  4.486374746193178e-05\n",
            "Epoch  25 Batch  400 / 525  Training Loss  2.6257819627062418e-05\n",
            "Epoch  25 Batch  401 / 525  Training Loss  4.925394750898704e-05\n",
            "Epoch  25 Batch  402 / 525  Training Loss  3.3126438211183995e-05\n",
            "Epoch  25 Batch  403 / 525  Training Loss  2.894020508392714e-05\n",
            "Epoch  25 Batch  404 / 525  Training Loss  4.520885704550892e-05\n",
            "Epoch  25 Batch  405 / 525  Training Loss  4.649989932659082e-05\n",
            "Epoch  25 Batch  406 / 525  Training Loss  2.6600066121318378e-05\n",
            "Epoch  25 Batch  407 / 525  Training Loss  3.40632104780525e-05\n",
            "Epoch  25 Batch  408 / 525  Training Loss  2.3824813979445025e-05\n",
            "Epoch  25 Batch  409 / 525  Training Loss  3.383539660717361e-05\n",
            "Epoch  25 Batch  410 / 525  Training Loss  4.606716538546607e-05\n",
            "Epoch  25 Batch  411 / 525  Training Loss  4.2903342546196654e-05\n",
            "Epoch  25 Batch  412 / 525  Training Loss  3.0065581086091697e-05\n",
            "Epoch  25 Batch  413 / 525  Training Loss  3.328784805489704e-05\n",
            "Epoch  25 Batch  414 / 525  Training Loss  3.319550523883663e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  25 Batch  415 / 525  Training Loss  3.109057433903217e-05\n",
            "Epoch  25 Batch  416 / 525  Training Loss  3.258082870161161e-05\n",
            "Epoch  25 Batch  417 / 525  Training Loss  3.8973386836005375e-05\n",
            "Epoch  25 Batch  418 / 525  Training Loss  2.7676253012032248e-05\n",
            "Epoch  25 Batch  419 / 525  Training Loss  2.770846549537964e-05\n",
            "Epoch  25 Batch  420 / 525  Training Loss  2.6346941012889147e-05\n",
            "Epoch  25 Batch  421 / 525  Training Loss  4.830709076486528e-05\n",
            "Epoch  25 Batch  422 / 525  Training Loss  3.100092726526782e-05\n",
            "Epoch  25 Batch  423 / 525  Training Loss  3.8726044294890016e-05\n",
            "Epoch  25 Batch  424 / 525  Training Loss  4.33447930845432e-05\n",
            "Epoch  25 Batch  425 / 525  Training Loss  2.895124998758547e-05\n",
            "Epoch  25 Batch  426 / 525  Training Loss  2.9374621590250172e-05\n",
            "Epoch  25 Batch  427 / 525  Training Loss  3.006304905284196e-05\n",
            "Epoch  25 Batch  428 / 525  Training Loss  2.4317869247170165e-05\n",
            "Epoch  25 Batch  429 / 525  Training Loss  3.053039108635858e-05\n",
            "Epoch  25 Batch  430 / 525  Training Loss  2.914954166044481e-05\n",
            "Epoch  25 Batch  431 / 525  Training Loss  3.550741166691296e-05\n",
            "Epoch  25 Batch  432 / 525  Training Loss  3.312736953375861e-05\n",
            "Epoch  25 Batch  433 / 525  Training Loss  3.494687189231627e-05\n",
            "Epoch  25 Batch  434 / 525  Training Loss  3.683779141283594e-05\n",
            "Epoch  25 Batch  435 / 525  Training Loss  2.5165791157633066e-05\n",
            "Epoch  25 Batch  436 / 525  Training Loss  2.496948218322359e-05\n",
            "Epoch  25 Batch  437 / 525  Training Loss  3.204361200914718e-05\n",
            "Epoch  25 Batch  438 / 525  Training Loss  2.629515074659139e-05\n",
            "Epoch  25 Batch  439 / 525  Training Loss  3.873964669764973e-05\n",
            "Epoch  25 Batch  440 / 525  Training Loss  3.035284680663608e-05\n",
            "Epoch  25 Batch  441 / 525  Training Loss  3.0287765184766613e-05\n",
            "Epoch  25 Batch  442 / 525  Training Loss  3.47977111232467e-05\n",
            "Epoch  25 Batch  443 / 525  Training Loss  4.098281351616606e-05\n",
            "Epoch  25 Batch  444 / 525  Training Loss  2.812736784107983e-05\n",
            "Epoch  25 Batch  445 / 525  Training Loss  2.8165633921162225e-05\n",
            "Epoch  25 Batch  446 / 525  Training Loss  3.419129643589258e-05\n",
            "Epoch  25 Batch  447 / 525  Training Loss  4.9923186452360824e-05\n",
            "Epoch  25 Batch  448 / 525  Training Loss  3.084632771788165e-05\n",
            "Epoch  25 Batch  449 / 525  Training Loss  4.275428000255488e-05\n",
            "Epoch  25 Batch  450 / 525  Training Loss  4.434788570506498e-05\n",
            "Epoch  25 Batch  451 / 525  Training Loss  2.9765229555778205e-05\n",
            "Epoch  25 Batch  452 / 525  Training Loss  4.049036942888051e-05\n",
            "Epoch  25 Batch  453 / 525  Training Loss  2.9037502827122808e-05\n",
            "Epoch  25 Batch  454 / 525  Training Loss  3.078299050685018e-05\n",
            "Epoch  25 Batch  455 / 525  Training Loss  3.5051187296630815e-05\n",
            "Epoch  25 Batch  456 / 525  Training Loss  3.885948899551295e-05\n",
            "Epoch  25 Batch  457 / 525  Training Loss  3.132320853183046e-05\n",
            "Epoch  25 Batch  458 / 525  Training Loss  3.327673039166257e-05\n",
            "Epoch  25 Batch  459 / 525  Training Loss  3.606130485422909e-05\n",
            "Epoch  25 Batch  460 / 525  Training Loss  4.198699753032997e-05\n",
            "Epoch  25 Batch  461 / 525  Training Loss  2.2313632143777795e-05\n",
            "Epoch  25 Batch  462 / 525  Training Loss  2.8519536499516107e-05\n",
            "Epoch  25 Batch  463 / 525  Training Loss  3.891764936270192e-05\n",
            "Epoch  25 Batch  464 / 525  Training Loss  3.890090010827407e-05\n",
            "Epoch  25 Batch  465 / 525  Training Loss  4.4930413423571736e-05\n",
            "Epoch  25 Batch  466 / 525  Training Loss  4.3145391828147694e-05\n",
            "Epoch  25 Batch  467 / 525  Training Loss  2.609526927699335e-05\n",
            "Epoch  25 Batch  468 / 525  Training Loss  3.628629201557487e-05\n",
            "Epoch  25 Batch  469 / 525  Training Loss  5.284998405841179e-05\n",
            "Epoch  25 Batch  470 / 525  Training Loss  3.0737905035493895e-05\n",
            "Epoch  25 Batch  471 / 525  Training Loss  2.6117730158148333e-05\n",
            "Epoch  25 Batch  472 / 525  Training Loss  3.449153882684186e-05\n",
            "Epoch  25 Batch  473 / 525  Training Loss  3.7911748222541064e-05\n",
            "Epoch  25 Batch  474 / 525  Training Loss  3.045901576115284e-05\n",
            "Epoch  25 Batch  475 / 525  Training Loss  4.121519668842666e-05\n",
            "Epoch  25 Batch  476 / 525  Training Loss  4.2869556637015194e-05\n",
            "Epoch  25 Batch  477 / 525  Training Loss  3.878025017911568e-05\n",
            "Epoch  25 Batch  478 / 525  Training Loss  4.149229425820522e-05\n",
            "Epoch  25 Batch  479 / 525  Training Loss  4.077317862538621e-05\n",
            "Epoch  25 Batch  480 / 525  Training Loss  2.828185461112298e-05\n",
            "Epoch  25 Batch  481 / 525  Training Loss  3.511981412884779e-05\n",
            "Epoch  25 Batch  482 / 525  Training Loss  3.226743501727469e-05\n",
            "Epoch  25 Batch  483 / 525  Training Loss  4.2787745769601315e-05\n",
            "Epoch  25 Batch  484 / 525  Training Loss  3.344529977766797e-05\n",
            "Epoch  25 Batch  485 / 525  Training Loss  3.915468187187798e-05\n",
            "Epoch  25 Batch  486 / 525  Training Loss  5.0173890485893935e-05\n",
            "Epoch  25 Batch  487 / 525  Training Loss  3.6659144825534895e-05\n",
            "Epoch  25 Batch  488 / 525  Training Loss  3.7708814488723874e-05\n",
            "Epoch  25 Batch  489 / 525  Training Loss  4.324445762904361e-05\n",
            "Epoch  25 Batch  490 / 525  Training Loss  2.648031477292534e-05\n",
            "Epoch  25 Batch  491 / 525  Training Loss  3.469944203970954e-05\n",
            "Epoch  25 Batch  492 / 525  Training Loss  3.22777996188961e-05\n",
            "Epoch  25 Batch  493 / 525  Training Loss  3.42507628374733e-05\n",
            "Epoch  25 Batch  494 / 525  Training Loss  2.9512873879866675e-05\n",
            "Epoch  25 Batch  495 / 525  Training Loss  2.5470691980444826e-05\n",
            "Epoch  25 Batch  496 / 525  Training Loss  3.558767639333382e-05\n",
            "Epoch  25 Batch  497 / 525  Training Loss  4.527732016867958e-05\n",
            "Epoch  25 Batch  498 / 525  Training Loss  3.6371377063915133e-05\n",
            "Epoch  25 Batch  499 / 525  Training Loss  2.729014704527799e-05\n",
            "Epoch  25 Batch  500 / 525  Training Loss  4.025959788123146e-05\n",
            "Epoch  25 Batch  501 / 525  Training Loss  3.0435665394179523e-05\n",
            "Epoch  25 Batch  502 / 525  Training Loss  3.763927088584751e-05\n",
            "Epoch  25 Batch  503 / 525  Training Loss  3.9163387555163354e-05\n",
            "Epoch  25 Batch  504 / 525  Training Loss  4.0157225157599896e-05\n",
            "Epoch  25 Batch  505 / 525  Training Loss  3.710907549248077e-05\n",
            "Epoch  25 Batch  506 / 525  Training Loss  3.584809019230306e-05\n",
            "Epoch  25 Batch  507 / 525  Training Loss  2.955297168227844e-05\n",
            "Epoch  25 Batch  508 / 525  Training Loss  3.0120587325654924e-05\n",
            "Epoch  25 Batch  509 / 525  Training Loss  3.54789117409382e-05\n",
            "Epoch  25 Batch  510 / 525  Training Loss  2.7733753086067736e-05\n",
            "Epoch  25 Batch  511 / 525  Training Loss  2.2417349100578576e-05\n",
            "Epoch  25 Batch  512 / 525  Training Loss  3.4112537832697853e-05\n",
            "Epoch  25 Batch  513 / 525  Training Loss  4.230364356772043e-05\n",
            "Epoch  25 Batch  514 / 525  Training Loss  3.542201011441648e-05\n",
            "Epoch  25 Batch  515 / 525  Training Loss  5.404388139140792e-05\n",
            "Epoch  25 Batch  516 / 525  Training Loss  3.500475941109471e-05\n",
            "Epoch  25 Batch  517 / 525  Training Loss  4.203388380119577e-05\n",
            "Epoch  25 Batch  518 / 525  Training Loss  3.3292184525635093e-05\n",
            "Epoch  25 Batch  519 / 525  Training Loss  4.4568652810994536e-05\n",
            "Epoch  25 Batch  520 / 525  Training Loss  2.7255257009528577e-05\n",
            "Epoch  25 Batch  521 / 525  Training Loss  2.037531157839112e-05\n",
            "Epoch  25 Batch  522 / 525  Training Loss  3.448547431617044e-05\n",
            "Epoch  25 Batch  523 / 525  Training Loss  3.8307207432808354e-05\n",
            "Epoch  25 Batch  524 / 525  Training Loss  4.141602403251454e-05\n",
            "  26    |    -    |   0.000035   |   64.20  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 26\n",
            "Epoch  26 Batch  0 / 525  Training Loss  2.707460407691542e-05\n",
            "Epoch  26 Batch  1 / 525  Training Loss  3.6886522138956934e-05\n",
            "Epoch  26 Batch  2 / 525  Training Loss  3.120910332654603e-05\n",
            "Epoch  26 Batch  3 / 525  Training Loss  3.600928539526649e-05\n",
            "Epoch  26 Batch  4 / 525  Training Loss  3.988987373304553e-05\n",
            "Epoch  26 Batch  5 / 525  Training Loss  3.0186532967491075e-05\n",
            "Epoch  26 Batch  6 / 525  Training Loss  3.0453560611931607e-05\n",
            "Epoch  26 Batch  7 / 525  Training Loss  4.704085586126894e-05\n",
            "Epoch  26 Batch  8 / 525  Training Loss  2.7925689209951088e-05\n",
            "Epoch  26 Batch  9 / 525  Training Loss  3.0174651328707114e-05\n",
            "Epoch  26 Batch  10 / 525  Training Loss  2.7047150069847703e-05\n",
            "Epoch  26 Batch  11 / 525  Training Loss  2.373604911554139e-05\n",
            "Epoch  26 Batch  12 / 525  Training Loss  4.5783053792547435e-05\n",
            "Epoch  26 Batch  13 / 525  Training Loss  4.010898555861786e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  26 Batch  14 / 525  Training Loss  3.3906588214449584e-05\n",
            "Epoch  26 Batch  15 / 525  Training Loss  3.0488659831462428e-05\n",
            "Epoch  26 Batch  16 / 525  Training Loss  2.5868022930808365e-05\n",
            "Epoch  26 Batch  17 / 525  Training Loss  3.308720624772832e-05\n",
            "Epoch  26 Batch  18 / 525  Training Loss  2.0655832486227155e-05\n",
            "Epoch  26 Batch  19 / 525  Training Loss  3.918889706255868e-05\n",
            "Epoch  26 Batch  20 / 525  Training Loss  4.30264844908379e-05\n",
            "Epoch  26 Batch  21 / 525  Training Loss  2.9467541025951505e-05\n",
            "Epoch  26 Batch  22 / 525  Training Loss  3.6082092265132815e-05\n",
            "Epoch  26 Batch  23 / 525  Training Loss  2.6897367206402123e-05\n",
            "Epoch  26 Batch  24 / 525  Training Loss  4.7414716391358525e-05\n",
            "Epoch  26 Batch  25 / 525  Training Loss  3.632074367487803e-05\n",
            "Epoch  26 Batch  26 / 525  Training Loss  2.8464803108363412e-05\n",
            "Epoch  26 Batch  27 / 525  Training Loss  2.9422779334709048e-05\n",
            "Epoch  26 Batch  28 / 525  Training Loss  2.394483090029098e-05\n",
            "Epoch  26 Batch  29 / 525  Training Loss  2.9896682463004254e-05\n",
            "Epoch  26 Batch  30 / 525  Training Loss  2.989404856634792e-05\n",
            "Epoch  26 Batch  31 / 525  Training Loss  3.070323873544112e-05\n",
            "Epoch  26 Batch  32 / 525  Training Loss  2.4346602003788576e-05\n",
            "Epoch  26 Batch  33 / 525  Training Loss  3.52243150700815e-05\n",
            "Epoch  26 Batch  34 / 525  Training Loss  3.102334085269831e-05\n",
            "Epoch  26 Batch  35 / 525  Training Loss  2.6674611945054494e-05\n",
            "Epoch  26 Batch  36 / 525  Training Loss  2.599876097519882e-05\n",
            "Epoch  26 Batch  37 / 525  Training Loss  2.9534445275203325e-05\n",
            "Epoch  26 Batch  38 / 525  Training Loss  2.6228281058138236e-05\n",
            "Epoch  26 Batch  39 / 525  Training Loss  2.6467483621672727e-05\n",
            "Epoch  26 Batch  40 / 525  Training Loss  2.5933371944120154e-05\n",
            "Epoch  26 Batch  41 / 525  Training Loss  2.414746631984599e-05\n",
            "Epoch  26 Batch  42 / 525  Training Loss  3.948313678847626e-05\n",
            "Epoch  26 Batch  43 / 525  Training Loss  3.70743109670002e-05\n",
            "Epoch  26 Batch  44 / 525  Training Loss  3.0313709430629387e-05\n",
            "Epoch  26 Batch  45 / 525  Training Loss  3.872417801176198e-05\n",
            "Epoch  26 Batch  46 / 525  Training Loss  3.153090437990613e-05\n",
            "Epoch  26 Batch  47 / 525  Training Loss  4.227679528412409e-05\n",
            "Epoch  26 Batch  48 / 525  Training Loss  2.986772778967861e-05\n",
            "Epoch  26 Batch  49 / 525  Training Loss  2.9820372219546698e-05\n",
            "Epoch  26 Batch  50 / 525  Training Loss  3.3746448025340214e-05\n",
            "Epoch  26 Batch  51 / 525  Training Loss  4.113978138775565e-05\n",
            "Epoch  26 Batch  52 / 525  Training Loss  3.307588485768065e-05\n",
            "Epoch  26 Batch  53 / 525  Training Loss  2.0638071873690933e-05\n",
            "Epoch  26 Batch  54 / 525  Training Loss  2.7272631996311247e-05\n",
            "Epoch  26 Batch  55 / 525  Training Loss  3.638811904238537e-05\n",
            "Epoch  26 Batch  56 / 525  Training Loss  2.9898623324697837e-05\n",
            "Epoch  26 Batch  57 / 525  Training Loss  3.3980217267526314e-05\n",
            "Epoch  26 Batch  58 / 525  Training Loss  2.1997178919264115e-05\n",
            "Epoch  26 Batch  59 / 525  Training Loss  3.23939420923125e-05\n",
            "Epoch  26 Batch  60 / 525  Training Loss  3.1533392757410184e-05\n",
            "Epoch  26 Batch  61 / 525  Training Loss  2.674834649951663e-05\n",
            "Epoch  26 Batch  62 / 525  Training Loss  4.035303936689161e-05\n",
            "Epoch  26 Batch  63 / 525  Training Loss  2.5459570679231547e-05\n",
            "Epoch  26 Batch  64 / 525  Training Loss  2.7347185096004978e-05\n",
            "Epoch  26 Batch  65 / 525  Training Loss  4.72268475277815e-05\n",
            "Epoch  26 Batch  66 / 525  Training Loss  3.086447395617142e-05\n",
            "Epoch  26 Batch  67 / 525  Training Loss  2.864870839403011e-05\n",
            "Epoch  26 Batch  68 / 525  Training Loss  3.841717625618912e-05\n",
            "Epoch  26 Batch  69 / 525  Training Loss  4.495503526413813e-05\n",
            "Epoch  26 Batch  70 / 525  Training Loss  3.077700603171252e-05\n",
            "Epoch  26 Batch  71 / 525  Training Loss  3.865991675411351e-05\n",
            "Epoch  26 Batch  72 / 525  Training Loss  4.253810038790107e-05\n",
            "Epoch  26 Batch  73 / 525  Training Loss  2.679671160876751e-05\n",
            "Epoch  26 Batch  74 / 525  Training Loss  3.896973066730425e-05\n",
            "Epoch  26 Batch  75 / 525  Training Loss  3.510464011924341e-05\n",
            "Epoch  26 Batch  76 / 525  Training Loss  4.0767241443973035e-05\n",
            "Epoch  26 Batch  77 / 525  Training Loss  3.3989348594332114e-05\n",
            "Epoch  26 Batch  78 / 525  Training Loss  3.562725396477617e-05\n",
            "Epoch  26 Batch  79 / 525  Training Loss  2.7738686185330153e-05\n",
            "Epoch  26 Batch  80 / 525  Training Loss  3.205001848982647e-05\n",
            "Epoch  26 Batch  81 / 525  Training Loss  3.1003884942037985e-05\n",
            "Epoch  26 Batch  82 / 525  Training Loss  3.229728463338688e-05\n",
            "Epoch  26 Batch  83 / 525  Training Loss  3.583074067137204e-05\n",
            "Epoch  26 Batch  84 / 525  Training Loss  2.9040482331765816e-05\n",
            "Epoch  26 Batch  85 / 525  Training Loss  3.190330971847288e-05\n",
            "Epoch  26 Batch  86 / 525  Training Loss  3.424610986257903e-05\n",
            "Epoch  26 Batch  87 / 525  Training Loss  3.300212847534567e-05\n",
            "Epoch  26 Batch  88 / 525  Training Loss  5.197622886043973e-05\n",
            "Epoch  26 Batch  89 / 525  Training Loss  3.3796583011280745e-05\n",
            "Epoch  26 Batch  90 / 525  Training Loss  3.346888843225315e-05\n",
            "Epoch  26 Batch  91 / 525  Training Loss  5.060520561528392e-05\n",
            "Epoch  26 Batch  92 / 525  Training Loss  4.4193810026627034e-05\n",
            "Epoch  26 Batch  93 / 525  Training Loss  2.8501133783720434e-05\n",
            "Epoch  26 Batch  94 / 525  Training Loss  3.778221071115695e-05\n",
            "Epoch  26 Batch  95 / 525  Training Loss  3.825563544523902e-05\n",
            "Epoch  26 Batch  96 / 525  Training Loss  2.8631280656554736e-05\n",
            "Epoch  26 Batch  97 / 525  Training Loss  2.4957527784863487e-05\n",
            "Epoch  26 Batch  98 / 525  Training Loss  3.160409687552601e-05\n",
            "Epoch  26 Batch  99 / 525  Training Loss  3.3631025871727616e-05\n",
            "Epoch  26 Batch  100 / 525  Training Loss  3.279350130469538e-05\n",
            "Epoch  26 Batch  101 / 525  Training Loss  2.7787498765974306e-05\n",
            "Epoch  26 Batch  102 / 525  Training Loss  2.587002563814167e-05\n",
            "Epoch  26 Batch  103 / 525  Training Loss  3.989085234934464e-05\n",
            "Epoch  26 Batch  104 / 525  Training Loss  3.3613378036534414e-05\n",
            "Epoch  26 Batch  105 / 525  Training Loss  2.73797195404768e-05\n",
            "Epoch  26 Batch  106 / 525  Training Loss  2.5586215997464024e-05\n",
            "Epoch  26 Batch  107 / 525  Training Loss  3.65514206350781e-05\n",
            "Epoch  26 Batch  108 / 525  Training Loss  3.795001975959167e-05\n",
            "Epoch  26 Batch  109 / 525  Training Loss  2.2725533199263737e-05\n",
            "Epoch  26 Batch  110 / 525  Training Loss  3.0027378670638427e-05\n",
            "Epoch  26 Batch  111 / 525  Training Loss  3.320353789604269e-05\n",
            "Epoch  26 Batch  112 / 525  Training Loss  3.601029675337486e-05\n",
            "Epoch  26 Batch  113 / 525  Training Loss  3.22582054650411e-05\n",
            "Epoch  26 Batch  114 / 525  Training Loss  3.6363759136293083e-05\n",
            "Epoch  26 Batch  115 / 525  Training Loss  3.149120311718434e-05\n",
            "Epoch  26 Batch  116 / 525  Training Loss  2.78851694019977e-05\n",
            "Epoch  26 Batch  117 / 525  Training Loss  3.533560084179044e-05\n",
            "Epoch  26 Batch  118 / 525  Training Loss  3.12452029902488e-05\n",
            "Epoch  26 Batch  119 / 525  Training Loss  3.646558252512477e-05\n",
            "Epoch  26 Batch  120 / 525  Training Loss  2.9891214580857195e-05\n",
            "Epoch  26 Batch  121 / 525  Training Loss  4.048556002089754e-05\n",
            "Epoch  26 Batch  122 / 525  Training Loss  2.9426368200802244e-05\n",
            "Epoch  26 Batch  123 / 525  Training Loss  2.7699290512828156e-05\n",
            "Epoch  26 Batch  124 / 525  Training Loss  3.4002106986008584e-05\n",
            "Epoch  26 Batch  125 / 525  Training Loss  4.1796120058279485e-05\n",
            "Epoch  26 Batch  126 / 525  Training Loss  3.43326864822302e-05\n",
            "Epoch  26 Batch  127 / 525  Training Loss  2.615969606267754e-05\n",
            "Epoch  26 Batch  128 / 525  Training Loss  5.2007137128384784e-05\n",
            "Epoch  26 Batch  129 / 525  Training Loss  4.052871372550726e-05\n",
            "Epoch  26 Batch  130 / 525  Training Loss  3.2186148018809035e-05\n",
            "Epoch  26 Batch  131 / 525  Training Loss  3.4336433600401506e-05\n",
            "Epoch  26 Batch  132 / 525  Training Loss  2.7436140953795984e-05\n",
            "Epoch  26 Batch  133 / 525  Training Loss  2.4803777705528773e-05\n",
            "Epoch  26 Batch  134 / 525  Training Loss  2.972939910250716e-05\n",
            "Epoch  26 Batch  135 / 525  Training Loss  3.414166712900624e-05\n",
            "Epoch  26 Batch  136 / 525  Training Loss  2.466106707288418e-05\n",
            "Epoch  26 Batch  137 / 525  Training Loss  4.1476927435724065e-05\n",
            "Epoch  26 Batch  138 / 525  Training Loss  2.4941240553744137e-05\n",
            "Epoch  26 Batch  139 / 525  Training Loss  3.8544640119653195e-05\n",
            "Epoch  26 Batch  140 / 525  Training Loss  2.9271246603457257e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  26 Batch  141 / 525  Training Loss  4.0253507904708385e-05\n",
            "Epoch  26 Batch  142 / 525  Training Loss  2.8673592169070616e-05\n",
            "Epoch  26 Batch  143 / 525  Training Loss  4.022071880172007e-05\n",
            "Epoch  26 Batch  144 / 525  Training Loss  3.397500404389575e-05\n",
            "Epoch  26 Batch  145 / 525  Training Loss  3.0526367481797934e-05\n",
            "Epoch  26 Batch  146 / 525  Training Loss  2.697485251701437e-05\n",
            "Epoch  26 Batch  147 / 525  Training Loss  3.601994467317127e-05\n",
            "Epoch  26 Batch  148 / 525  Training Loss  3.509935413603671e-05\n",
            "Epoch  26 Batch  149 / 525  Training Loss  2.793055318761617e-05\n",
            "Epoch  26 Batch  150 / 525  Training Loss  3.402878064662218e-05\n",
            "Epoch  26 Batch  151 / 525  Training Loss  2.231767211924307e-05\n",
            "Epoch  26 Batch  152 / 525  Training Loss  3.9703074435237795e-05\n",
            "Epoch  26 Batch  153 / 525  Training Loss  2.3805103410268202e-05\n",
            "Epoch  26 Batch  154 / 525  Training Loss  3.088410448981449e-05\n",
            "Epoch  26 Batch  155 / 525  Training Loss  3.0415170840569772e-05\n",
            "Epoch  26 Batch  156 / 525  Training Loss  2.8693317290162668e-05\n",
            "Epoch  26 Batch  157 / 525  Training Loss  3.901184390997514e-05\n",
            "Epoch  26 Batch  158 / 525  Training Loss  2.5974488380597904e-05\n",
            "Epoch  26 Batch  159 / 525  Training Loss  2.8528907932923175e-05\n",
            "Epoch  26 Batch  160 / 525  Training Loss  3.279834345448762e-05\n",
            "Epoch  26 Batch  161 / 525  Training Loss  2.667756416485645e-05\n",
            "Epoch  26 Batch  162 / 525  Training Loss  3.1337138352682814e-05\n",
            "Epoch  26 Batch  163 / 525  Training Loss  3.011376793438103e-05\n",
            "Epoch  26 Batch  164 / 525  Training Loss  2.9330578399822116e-05\n",
            "Epoch  26 Batch  165 / 525  Training Loss  4.3323470890754834e-05\n",
            "Epoch  26 Batch  166 / 525  Training Loss  2.8917396775796078e-05\n",
            "Epoch  26 Batch  167 / 525  Training Loss  2.9242492018966004e-05\n",
            "Epoch  26 Batch  168 / 525  Training Loss  2.312245123903267e-05\n",
            "Epoch  26 Batch  169 / 525  Training Loss  2.9760511097265407e-05\n",
            "Epoch  26 Batch  170 / 525  Training Loss  2.9656885089934804e-05\n",
            "Epoch  26 Batch  171 / 525  Training Loss  2.7033584046876058e-05\n",
            "Epoch  26 Batch  172 / 525  Training Loss  3.567826934158802e-05\n",
            "Epoch  26 Batch  173 / 525  Training Loss  4.290133801987395e-05\n",
            "Epoch  26 Batch  174 / 525  Training Loss  3.9809252484701574e-05\n",
            "Epoch  26 Batch  175 / 525  Training Loss  2.3618475097464398e-05\n",
            "Epoch  26 Batch  176 / 525  Training Loss  2.0815112293348648e-05\n",
            "Epoch  26 Batch  177 / 525  Training Loss  3.035968984477222e-05\n",
            "Epoch  26 Batch  178 / 525  Training Loss  2.6489720767131075e-05\n",
            "Epoch  26 Batch  179 / 525  Training Loss  3.445479160291143e-05\n",
            "Epoch  26 Batch  180 / 525  Training Loss  3.4035292628686875e-05\n",
            "Epoch  26 Batch  181 / 525  Training Loss  3.101479524048045e-05\n",
            "Epoch  26 Batch  182 / 525  Training Loss  3.02824228128884e-05\n",
            "Epoch  26 Batch  183 / 525  Training Loss  2.2332782464218326e-05\n",
            "Epoch  26 Batch  184 / 525  Training Loss  4.06801700592041e-05\n",
            "Epoch  26 Batch  185 / 525  Training Loss  2.955919990199618e-05\n",
            "Epoch  26 Batch  186 / 525  Training Loss  3.465440386207774e-05\n",
            "Epoch  26 Batch  187 / 525  Training Loss  3.212120645912364e-05\n",
            "Epoch  26 Batch  188 / 525  Training Loss  2.5721348720253445e-05\n",
            "Epoch  26 Batch  189 / 525  Training Loss  2.848017538781278e-05\n",
            "Epoch  26 Batch  190 / 525  Training Loss  3.575553637347184e-05\n",
            "Epoch  26 Batch  191 / 525  Training Loss  4.5148372009862214e-05\n",
            "Epoch  26 Batch  192 / 525  Training Loss  1.8900862414739095e-05\n",
            "Epoch  26 Batch  193 / 525  Training Loss  3.411844227230176e-05\n",
            "Epoch  26 Batch  194 / 525  Training Loss  4.376716970000416e-05\n",
            "Epoch  26 Batch  195 / 525  Training Loss  3.0301216611405835e-05\n",
            "Epoch  26 Batch  196 / 525  Training Loss  3.0836024961899966e-05\n",
            "Epoch  26 Batch  197 / 525  Training Loss  3.060281596845016e-05\n",
            "Epoch  26 Batch  198 / 525  Training Loss  2.515782580303494e-05\n",
            "Epoch  26 Batch  199 / 525  Training Loss  2.1024452507845126e-05\n",
            "Epoch  26 Batch  200 / 525  Training Loss  2.582735032774508e-05\n",
            "Epoch  26 Batch  201 / 525  Training Loss  2.58569089055527e-05\n",
            "Epoch  26 Batch  202 / 525  Training Loss  2.082006903947331e-05\n",
            "Epoch  26 Batch  203 / 525  Training Loss  3.420826396904886e-05\n",
            "Epoch  26 Batch  204 / 525  Training Loss  2.455208232277073e-05\n",
            "Epoch  26 Batch  205 / 525  Training Loss  3.850101711577736e-05\n",
            "Epoch  26 Batch  206 / 525  Training Loss  4.2452567868167534e-05\n",
            "Epoch  26 Batch  207 / 525  Training Loss  3.883461977238767e-05\n",
            "Epoch  26 Batch  208 / 525  Training Loss  3.2646028557792306e-05\n",
            "Epoch  26 Batch  209 / 525  Training Loss  2.7846075681736693e-05\n",
            "Epoch  26 Batch  210 / 525  Training Loss  2.5842551622190513e-05\n",
            "Epoch  26 Batch  211 / 525  Training Loss  3.116391962976195e-05\n",
            "Epoch  26 Batch  212 / 525  Training Loss  3.712440229719505e-05\n",
            "Epoch  26 Batch  213 / 525  Training Loss  2.7395502911531366e-05\n",
            "Epoch  26 Batch  214 / 525  Training Loss  2.7370382667868398e-05\n",
            "Epoch  26 Batch  215 / 525  Training Loss  2.6147268727072515e-05\n",
            "Epoch  26 Batch  216 / 525  Training Loss  2.854023114196025e-05\n",
            "Epoch  26 Batch  217 / 525  Training Loss  3.403105802135542e-05\n",
            "Epoch  26 Batch  218 / 525  Training Loss  3.076338907703757e-05\n",
            "Epoch  26 Batch  219 / 525  Training Loss  3.986168667324819e-05\n",
            "Epoch  26 Batch  220 / 525  Training Loss  2.800763104460202e-05\n",
            "Epoch  26 Batch  221 / 525  Training Loss  3.3745585824362934e-05\n",
            "Epoch  26 Batch  222 / 525  Training Loss  1.999826417886652e-05\n",
            "Epoch  26 Batch  223 / 525  Training Loss  3.7920013710390776e-05\n",
            "Epoch  26 Batch  224 / 525  Training Loss  2.487512210791465e-05\n",
            "Epoch  26 Batch  225 / 525  Training Loss  2.6865978725254536e-05\n",
            "Epoch  26 Batch  226 / 525  Training Loss  3.9827551518101245e-05\n",
            "Epoch  26 Batch  227 / 525  Training Loss  3.4305350709473714e-05\n",
            "Epoch  26 Batch  228 / 525  Training Loss  2.876349572034087e-05\n",
            "Epoch  26 Batch  229 / 525  Training Loss  2.8802494853152893e-05\n",
            "Epoch  26 Batch  230 / 525  Training Loss  3.1073872378328815e-05\n",
            "Epoch  26 Batch  231 / 525  Training Loss  3.4788547054631636e-05\n",
            "Epoch  26 Batch  232 / 525  Training Loss  3.437429404584691e-05\n",
            "Epoch  26 Batch  233 / 525  Training Loss  3.19125538226217e-05\n",
            "Epoch  26 Batch  234 / 525  Training Loss  3.3629727113293484e-05\n",
            "Epoch  26 Batch  235 / 525  Training Loss  3.908114740625024e-05\n",
            "Epoch  26 Batch  236 / 525  Training Loss  3.129266042378731e-05\n",
            "Epoch  26 Batch  237 / 525  Training Loss  2.270363984280266e-05\n",
            "Epoch  26 Batch  238 / 525  Training Loss  2.7962621970800683e-05\n",
            "Epoch  26 Batch  239 / 525  Training Loss  3.0424092983594164e-05\n",
            "Epoch  26 Batch  240 / 525  Training Loss  4.374442505650222e-05\n",
            "Epoch  26 Batch  241 / 525  Training Loss  3.878094503306784e-05\n",
            "Epoch  26 Batch  242 / 525  Training Loss  2.9996886951266788e-05\n",
            "Epoch  26 Batch  243 / 525  Training Loss  3.233174720662646e-05\n",
            "Epoch  26 Batch  244 / 525  Training Loss  3.025176738447044e-05\n",
            "Epoch  26 Batch  245 / 525  Training Loss  3.0368348234333098e-05\n",
            "Epoch  26 Batch  246 / 525  Training Loss  3.0396233341889456e-05\n",
            "Epoch  26 Batch  247 / 525  Training Loss  3.331492916913703e-05\n",
            "Epoch  26 Batch  248 / 525  Training Loss  2.1182015188969672e-05\n",
            "Epoch  26 Batch  249 / 525  Training Loss  2.4056775146164e-05\n",
            "Epoch  26 Batch  250 / 525  Training Loss  3.825961539405398e-05\n",
            "Epoch  26 Batch  251 / 525  Training Loss  4.272401565685868e-05\n",
            "Epoch  26 Batch  252 / 525  Training Loss  2.798813147819601e-05\n",
            "Epoch  26 Batch  253 / 525  Training Loss  3.141975685139187e-05\n",
            "Epoch  26 Batch  254 / 525  Training Loss  2.1594803911284544e-05\n",
            "Epoch  26 Batch  255 / 525  Training Loss  2.6981180781149305e-05\n",
            "Epoch  26 Batch  256 / 525  Training Loss  2.7507174308993854e-05\n",
            "Epoch  26 Batch  257 / 525  Training Loss  2.465981015120633e-05\n",
            "Epoch  26 Batch  258 / 525  Training Loss  2.547323856560979e-05\n",
            "Epoch  26 Batch  259 / 525  Training Loss  2.921427039836999e-05\n",
            "Epoch  26 Batch  260 / 525  Training Loss  2.3588427211507224e-05\n",
            "Epoch  26 Batch  261 / 525  Training Loss  2.9007758712396026e-05\n",
            "Epoch  26 Batch  262 / 525  Training Loss  3.0302126106107607e-05\n",
            "Epoch  26 Batch  263 / 525  Training Loss  3.6249362892704085e-05\n",
            "Epoch  26 Batch  264 / 525  Training Loss  3.6640041798818856e-05\n",
            "Epoch  26 Batch  265 / 525  Training Loss  3.052093234146014e-05\n",
            "Epoch  26 Batch  266 / 525  Training Loss  3.333126733195968e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  26 Batch  267 / 525  Training Loss  3.588753679650836e-05\n",
            "Epoch  26 Batch  268 / 525  Training Loss  3.365041993674822e-05\n",
            "Epoch  26 Batch  269 / 525  Training Loss  2.150523323507514e-05\n",
            "Epoch  26 Batch  270 / 525  Training Loss  3.969321187469177e-05\n",
            "Epoch  26 Batch  271 / 525  Training Loss  3.965767245972529e-05\n",
            "Epoch  26 Batch  272 / 525  Training Loss  3.940259193768725e-05\n",
            "Epoch  26 Batch  273 / 525  Training Loss  3.9438360545318574e-05\n",
            "Epoch  26 Batch  274 / 525  Training Loss  2.5266999728046358e-05\n",
            "Epoch  26 Batch  275 / 525  Training Loss  3.0128436264931224e-05\n",
            "Epoch  26 Batch  276 / 525  Training Loss  2.185778066632338e-05\n",
            "Epoch  26 Batch  277 / 525  Training Loss  2.6216404876322486e-05\n",
            "Epoch  26 Batch  278 / 525  Training Loss  4.429216278367676e-05\n",
            "Epoch  26 Batch  279 / 525  Training Loss  4.3597879994194955e-05\n",
            "Epoch  26 Batch  280 / 525  Training Loss  3.216706318198703e-05\n",
            "Epoch  26 Batch  281 / 525  Training Loss  3.321756230434403e-05\n",
            "Epoch  26 Batch  282 / 525  Training Loss  3.990304685430601e-05\n",
            "Epoch  26 Batch  283 / 525  Training Loss  3.392653525224887e-05\n",
            "Epoch  26 Batch  284 / 525  Training Loss  3.433616438996978e-05\n",
            "Epoch  26 Batch  285 / 525  Training Loss  2.1525542251765728e-05\n",
            "Epoch  26 Batch  286 / 525  Training Loss  2.29913457587827e-05\n",
            "Epoch  26 Batch  287 / 525  Training Loss  3.601053685997613e-05\n",
            "Epoch  26 Batch  288 / 525  Training Loss  2.5061308406293392e-05\n",
            "Epoch  26 Batch  289 / 525  Training Loss  3.726069917320274e-05\n",
            "Epoch  26 Batch  290 / 525  Training Loss  3.4361313737463206e-05\n",
            "Epoch  26 Batch  291 / 525  Training Loss  4.388492743601091e-05\n",
            "Epoch  26 Batch  292 / 525  Training Loss  3.114975697826594e-05\n",
            "Epoch  26 Batch  293 / 525  Training Loss  2.8518970793811604e-05\n",
            "Epoch  26 Batch  294 / 525  Training Loss  2.494791078788694e-05\n",
            "Epoch  26 Batch  295 / 525  Training Loss  3.448762436164543e-05\n",
            "Epoch  26 Batch  296 / 525  Training Loss  2.551886427681893e-05\n",
            "Epoch  26 Batch  297 / 525  Training Loss  2.6441601221449673e-05\n",
            "Epoch  26 Batch  298 / 525  Training Loss  2.7106030756840482e-05\n",
            "Epoch  26 Batch  299 / 525  Training Loss  2.7289739591651596e-05\n",
            "Epoch  26 Batch  300 / 525  Training Loss  3.56538912456017e-05\n",
            "Epoch  26 Batch  301 / 525  Training Loss  2.979859709739685e-05\n",
            "Epoch  26 Batch  302 / 525  Training Loss  2.4591257897554897e-05\n",
            "Epoch  26 Batch  303 / 525  Training Loss  3.153229408781044e-05\n",
            "Epoch  26 Batch  304 / 525  Training Loss  3.487619324005209e-05\n",
            "Epoch  26 Batch  305 / 525  Training Loss  2.171258893213235e-05\n",
            "Epoch  26 Batch  306 / 525  Training Loss  3.07290320051834e-05\n",
            "Epoch  26 Batch  307 / 525  Training Loss  2.785813921946101e-05\n",
            "Epoch  26 Batch  308 / 525  Training Loss  2.4882199795683846e-05\n",
            "Epoch  26 Batch  309 / 525  Training Loss  3.0739469366380945e-05\n",
            "Epoch  26 Batch  310 / 525  Training Loss  3.115096114925109e-05\n",
            "Epoch  26 Batch  311 / 525  Training Loss  3.296704016975127e-05\n",
            "Epoch  26 Batch  312 / 525  Training Loss  3.896885027643293e-05\n",
            "Epoch  26 Batch  313 / 525  Training Loss  3.9259517507161945e-05\n",
            "Epoch  26 Batch  314 / 525  Training Loss  3.488172296783887e-05\n",
            "Epoch  26 Batch  315 / 525  Training Loss  2.9404269298538566e-05\n",
            "Epoch  26 Batch  316 / 525  Training Loss  2.821388261509128e-05\n",
            "Epoch  26 Batch  317 / 525  Training Loss  2.454675814078655e-05\n",
            "Epoch  26 Batch  318 / 525  Training Loss  3.726573777385056e-05\n",
            "Epoch  26 Batch  319 / 525  Training Loss  2.9996552257216536e-05\n",
            "Epoch  26 Batch  320 / 525  Training Loss  3.5075267078354955e-05\n",
            "Epoch  26 Batch  321 / 525  Training Loss  2.9873039238736965e-05\n",
            "Epoch  26 Batch  322 / 525  Training Loss  2.790306825772859e-05\n",
            "Epoch  26 Batch  323 / 525  Training Loss  2.1551943063968793e-05\n",
            "Epoch  26 Batch  324 / 525  Training Loss  3.1851333915255964e-05\n",
            "Epoch  26 Batch  325 / 525  Training Loss  2.9976217774674296e-05\n",
            "Epoch  26 Batch  326 / 525  Training Loss  3.109662065980956e-05\n",
            "Epoch  26 Batch  327 / 525  Training Loss  2.7349542506271973e-05\n",
            "Epoch  26 Batch  328 / 525  Training Loss  2.8617261705221608e-05\n",
            "Epoch  26 Batch  329 / 525  Training Loss  3.47429231624119e-05\n",
            "Epoch  26 Batch  330 / 525  Training Loss  2.6704288757173344e-05\n",
            "Epoch  26 Batch  331 / 525  Training Loss  2.2502186766359955e-05\n",
            "Epoch  26 Batch  332 / 525  Training Loss  3.133299105684273e-05\n",
            "Epoch  26 Batch  333 / 525  Training Loss  3.071642277063802e-05\n",
            "Epoch  26 Batch  334 / 525  Training Loss  2.5001674657687545e-05\n",
            "Epoch  26 Batch  335 / 525  Training Loss  2.779842725431081e-05\n",
            "Epoch  26 Batch  336 / 525  Training Loss  2.8741784262820147e-05\n",
            "Epoch  26 Batch  337 / 525  Training Loss  2.5748671760084108e-05\n",
            "Epoch  26 Batch  338 / 525  Training Loss  2.7377833248465322e-05\n",
            "Epoch  26 Batch  339 / 525  Training Loss  3.76278258045204e-05\n",
            "Epoch  26 Batch  340 / 525  Training Loss  3.085227945121005e-05\n",
            "Epoch  26 Batch  341 / 525  Training Loss  3.709135853569023e-05\n",
            "Epoch  26 Batch  342 / 525  Training Loss  3.368060424691066e-05\n",
            "Epoch  26 Batch  343 / 525  Training Loss  2.4530712835257873e-05\n",
            "Epoch  26 Batch  344 / 525  Training Loss  3.5146080335834995e-05\n",
            "Epoch  26 Batch  345 / 525  Training Loss  4.1574872739147395e-05\n",
            "Epoch  26 Batch  346 / 525  Training Loss  3.31058690790087e-05\n",
            "Epoch  26 Batch  347 / 525  Training Loss  2.79275936918566e-05\n",
            "Epoch  26 Batch  348 / 525  Training Loss  3.047016252821777e-05\n",
            "Epoch  26 Batch  349 / 525  Training Loss  4.184056888334453e-05\n",
            "Epoch  26 Batch  350 / 525  Training Loss  4.161048127571121e-05\n",
            "Epoch  26 Batch  351 / 525  Training Loss  3.4606891858857125e-05\n",
            "Epoch  26 Batch  352 / 525  Training Loss  2.6897279894910753e-05\n",
            "Epoch  26 Batch  353 / 525  Training Loss  2.8112717700423673e-05\n",
            "Epoch  26 Batch  354 / 525  Training Loss  2.833488724718336e-05\n",
            "Epoch  26 Batch  355 / 525  Training Loss  2.4597460651420988e-05\n",
            "Epoch  26 Batch  356 / 525  Training Loss  2.8485626899055205e-05\n",
            "Epoch  26 Batch  357 / 525  Training Loss  3.1444877095054835e-05\n",
            "Epoch  26 Batch  358 / 525  Training Loss  3.3544176403665915e-05\n",
            "Epoch  26 Batch  359 / 525  Training Loss  3.1201627280097455e-05\n",
            "Epoch  26 Batch  360 / 525  Training Loss  2.9748514862149023e-05\n",
            "Epoch  26 Batch  361 / 525  Training Loss  2.351888906559907e-05\n",
            "Epoch  26 Batch  362 / 525  Training Loss  2.9047403586446308e-05\n",
            "Epoch  26 Batch  363 / 525  Training Loss  3.627701516961679e-05\n",
            "Epoch  26 Batch  364 / 525  Training Loss  2.4997725631692447e-05\n",
            "Epoch  26 Batch  365 / 525  Training Loss  4.156491195317358e-05\n",
            "Epoch  26 Batch  366 / 525  Training Loss  3.6301098589319736e-05\n",
            "Epoch  26 Batch  367 / 525  Training Loss  4.779543451149948e-05\n",
            "Epoch  26 Batch  368 / 525  Training Loss  2.4329818188562058e-05\n",
            "Epoch  26 Batch  369 / 525  Training Loss  2.7714175303117372e-05\n",
            "Epoch  26 Batch  370 / 525  Training Loss  3.8167032471392304e-05\n",
            "Epoch  26 Batch  371 / 525  Training Loss  3.5437715268926695e-05\n",
            "Epoch  26 Batch  372 / 525  Training Loss  3.101432594121434e-05\n",
            "Epoch  26 Batch  373 / 525  Training Loss  3.4083888749592006e-05\n",
            "Epoch  26 Batch  374 / 525  Training Loss  3.4513312130002305e-05\n",
            "Epoch  26 Batch  375 / 525  Training Loss  2.428658990538679e-05\n",
            "Epoch  26 Batch  376 / 525  Training Loss  3.70495326933451e-05\n",
            "Epoch  26 Batch  377 / 525  Training Loss  3.269843728048727e-05\n",
            "Epoch  26 Batch  378 / 525  Training Loss  3.3970609365496784e-05\n",
            "Epoch  26 Batch  379 / 525  Training Loss  3.676619235193357e-05\n",
            "Epoch  26 Batch  380 / 525  Training Loss  3.687415301101282e-05\n",
            "Epoch  26 Batch  381 / 525  Training Loss  2.9845108656445518e-05\n",
            "Epoch  26 Batch  382 / 525  Training Loss  2.9621354769915342e-05\n",
            "Epoch  26 Batch  383 / 525  Training Loss  3.223052408429794e-05\n",
            "Epoch  26 Batch  384 / 525  Training Loss  2.6969873943016864e-05\n",
            "Epoch  26 Batch  385 / 525  Training Loss  3.0150855309329927e-05\n",
            "Epoch  26 Batch  386 / 525  Training Loss  2.945528831332922e-05\n",
            "Epoch  26 Batch  387 / 525  Training Loss  1.8822040146915242e-05\n",
            "Epoch  26 Batch  388 / 525  Training Loss  3.500720777083188e-05\n",
            "Epoch  26 Batch  389 / 525  Training Loss  3.0171384423738346e-05\n",
            "Epoch  26 Batch  390 / 525  Training Loss  3.2396761525887996e-05\n",
            "Epoch  26 Batch  391 / 525  Training Loss  3.1851093808654696e-05\n",
            "Epoch  26 Batch  392 / 525  Training Loss  2.9369766707532108e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  26 Batch  393 / 525  Training Loss  2.9889382858527824e-05\n",
            "Epoch  26 Batch  394 / 525  Training Loss  3.314104105811566e-05\n",
            "Epoch  26 Batch  395 / 525  Training Loss  2.763955490081571e-05\n",
            "Epoch  26 Batch  396 / 525  Training Loss  3.1743729778099805e-05\n",
            "Epoch  26 Batch  397 / 525  Training Loss  2.9990245820954442e-05\n",
            "Epoch  26 Batch  398 / 525  Training Loss  3.675139669212513e-05\n",
            "Epoch  26 Batch  399 / 525  Training Loss  2.9456354241119698e-05\n",
            "Epoch  26 Batch  400 / 525  Training Loss  4.181765689281747e-05\n",
            "Epoch  26 Batch  401 / 525  Training Loss  3.385300442459993e-05\n",
            "Epoch  26 Batch  402 / 525  Training Loss  2.088996916427277e-05\n",
            "Epoch  26 Batch  403 / 525  Training Loss  4.3747473682742566e-05\n",
            "Epoch  26 Batch  404 / 525  Training Loss  2.4090471924864687e-05\n",
            "Epoch  26 Batch  405 / 525  Training Loss  2.9238373826956376e-05\n",
            "Epoch  26 Batch  406 / 525  Training Loss  3.8680787838529795e-05\n",
            "Epoch  26 Batch  407 / 525  Training Loss  3.629038474173285e-05\n",
            "Epoch  26 Batch  408 / 525  Training Loss  3.1636427593184635e-05\n",
            "Epoch  26 Batch  409 / 525  Training Loss  3.5839919291902333e-05\n",
            "Epoch  26 Batch  410 / 525  Training Loss  3.638076668721624e-05\n",
            "Epoch  26 Batch  411 / 525  Training Loss  3.7973641155986115e-05\n",
            "Epoch  26 Batch  412 / 525  Training Loss  2.094683986797463e-05\n",
            "Epoch  26 Batch  413 / 525  Training Loss  2.979381497425493e-05\n",
            "Epoch  26 Batch  414 / 525  Training Loss  3.3179065212607384e-05\n",
            "Epoch  26 Batch  415 / 525  Training Loss  2.328308801224921e-05\n",
            "Epoch  26 Batch  416 / 525  Training Loss  2.609709554235451e-05\n",
            "Epoch  26 Batch  417 / 525  Training Loss  3.0928757041692734e-05\n",
            "Epoch  26 Batch  418 / 525  Training Loss  3.3088697819039226e-05\n",
            "Epoch  26 Batch  419 / 525  Training Loss  2.1411118723335676e-05\n",
            "Epoch  26 Batch  420 / 525  Training Loss  3.2889365684241056e-05\n",
            "Epoch  26 Batch  421 / 525  Training Loss  2.9608952900161967e-05\n",
            "Epoch  26 Batch  422 / 525  Training Loss  3.697900683619082e-05\n",
            "Epoch  26 Batch  423 / 525  Training Loss  3.635035682236776e-05\n",
            "Epoch  26 Batch  424 / 525  Training Loss  3.214292155462317e-05\n",
            "Epoch  26 Batch  425 / 525  Training Loss  3.0578965379390866e-05\n",
            "Epoch  26 Batch  426 / 525  Training Loss  2.4798919184831902e-05\n",
            "Epoch  26 Batch  427 / 525  Training Loss  4.46478406956885e-05\n",
            "Epoch  26 Batch  428 / 525  Training Loss  3.9793863834347576e-05\n",
            "Epoch  26 Batch  429 / 525  Training Loss  2.0972505808458664e-05\n",
            "Epoch  26 Batch  430 / 525  Training Loss  3.372683568159118e-05\n",
            "Epoch  26 Batch  431 / 525  Training Loss  3.380871203262359e-05\n",
            "Epoch  26 Batch  432 / 525  Training Loss  3.454918987699784e-05\n",
            "Epoch  26 Batch  433 / 525  Training Loss  3.1412353564519435e-05\n",
            "Epoch  26 Batch  434 / 525  Training Loss  3.047934478672687e-05\n",
            "Epoch  26 Batch  435 / 525  Training Loss  3.305680729681626e-05\n",
            "Epoch  26 Batch  436 / 525  Training Loss  3.2571275369264185e-05\n",
            "Epoch  26 Batch  437 / 525  Training Loss  3.674143954413012e-05\n",
            "Epoch  26 Batch  438 / 525  Training Loss  4.1274870454799384e-05\n",
            "Epoch  26 Batch  439 / 525  Training Loss  3.6735298635903746e-05\n",
            "Epoch  26 Batch  440 / 525  Training Loss  3.4991342545254156e-05\n",
            "Epoch  26 Batch  441 / 525  Training Loss  3.8848822441650555e-05\n",
            "Epoch  26 Batch  442 / 525  Training Loss  3.318574817967601e-05\n",
            "Epoch  26 Batch  443 / 525  Training Loss  2.8005440981360152e-05\n",
            "Epoch  26 Batch  444 / 525  Training Loss  2.7688143745763227e-05\n",
            "Epoch  26 Batch  445 / 525  Training Loss  3.513790579745546e-05\n",
            "Epoch  26 Batch  446 / 525  Training Loss  2.7253281587036327e-05\n",
            "Epoch  26 Batch  447 / 525  Training Loss  3.0491044526570477e-05\n",
            "Epoch  26 Batch  448 / 525  Training Loss  3.871711669489741e-05\n",
            "Epoch  26 Batch  449 / 525  Training Loss  2.1393492716015317e-05\n",
            "Epoch  26 Batch  450 / 525  Training Loss  2.3776419766363688e-05\n",
            "Epoch  26 Batch  451 / 525  Training Loss  3.64560037269257e-05\n",
            "Epoch  26 Batch  452 / 525  Training Loss  2.6075318601215258e-05\n",
            "Epoch  26 Batch  453 / 525  Training Loss  3.25560467899777e-05\n",
            "Epoch  26 Batch  454 / 525  Training Loss  2.4042470613494515e-05\n",
            "Epoch  26 Batch  455 / 525  Training Loss  3.665783879114315e-05\n",
            "Epoch  26 Batch  456 / 525  Training Loss  2.990407847391907e-05\n",
            "Epoch  26 Batch  457 / 525  Training Loss  3.120338806184009e-05\n",
            "Epoch  26 Batch  458 / 525  Training Loss  3.637017653090879e-05\n",
            "Epoch  26 Batch  459 / 525  Training Loss  2.638532532728277e-05\n",
            "Epoch  26 Batch  460 / 525  Training Loss  4.213138890918344e-05\n",
            "Epoch  26 Batch  461 / 525  Training Loss  2.4557553842896596e-05\n",
            "Epoch  26 Batch  462 / 525  Training Loss  2.2614196495851502e-05\n",
            "Epoch  26 Batch  463 / 525  Training Loss  2.3592305296915583e-05\n",
            "Epoch  26 Batch  464 / 525  Training Loss  4.0205261029768735e-05\n",
            "Epoch  26 Batch  465 / 525  Training Loss  3.0857339879730716e-05\n",
            "Epoch  26 Batch  466 / 525  Training Loss  3.2572752388659865e-05\n",
            "Epoch  26 Batch  467 / 525  Training Loss  2.9501574317691848e-05\n",
            "Epoch  26 Batch  468 / 525  Training Loss  3.642767478595488e-05\n",
            "Epoch  26 Batch  469 / 525  Training Loss  3.6404548154678196e-05\n",
            "Epoch  26 Batch  470 / 525  Training Loss  3.3131065720226616e-05\n",
            "Epoch  26 Batch  471 / 525  Training Loss  3.223113890271634e-05\n",
            "Epoch  26 Batch  472 / 525  Training Loss  2.5386601919308305e-05\n",
            "Epoch  26 Batch  473 / 525  Training Loss  2.506105010979809e-05\n",
            "Epoch  26 Batch  474 / 525  Training Loss  3.9562699384987354e-05\n",
            "Epoch  26 Batch  475 / 525  Training Loss  3.519186793710105e-05\n",
            "Epoch  26 Batch  476 / 525  Training Loss  2.923616921179928e-05\n",
            "Epoch  26 Batch  477 / 525  Training Loss  2.901110019593034e-05\n",
            "Epoch  26 Batch  478 / 525  Training Loss  3.1653427868150175e-05\n",
            "Epoch  26 Batch  479 / 525  Training Loss  3.0166334909154102e-05\n",
            "Epoch  26 Batch  480 / 525  Training Loss  3.9915503293741494e-05\n",
            "Epoch  26 Batch  481 / 525  Training Loss  1.8798922610585578e-05\n",
            "Epoch  26 Batch  482 / 525  Training Loss  2.2514610463986173e-05\n",
            "Epoch  26 Batch  483 / 525  Training Loss  2.4792980184429325e-05\n",
            "Epoch  26 Batch  484 / 525  Training Loss  2.8195319828228094e-05\n",
            "Epoch  26 Batch  485 / 525  Training Loss  3.0876053642714396e-05\n",
            "Epoch  26 Batch  486 / 525  Training Loss  2.9488033760571852e-05\n",
            "Epoch  26 Batch  487 / 525  Training Loss  2.8692686100839637e-05\n",
            "Epoch  26 Batch  488 / 525  Training Loss  2.5434803319512866e-05\n",
            "Epoch  26 Batch  489 / 525  Training Loss  3.2626689062453806e-05\n",
            "Epoch  26 Batch  490 / 525  Training Loss  3.8622281863354146e-05\n",
            "Epoch  26 Batch  491 / 525  Training Loss  2.7706706532626413e-05\n",
            "Epoch  26 Batch  492 / 525  Training Loss  3.514158743200824e-05\n",
            "Epoch  26 Batch  493 / 525  Training Loss  3.4030064853141084e-05\n",
            "Epoch  26 Batch  494 / 525  Training Loss  2.251121804874856e-05\n",
            "Epoch  26 Batch  495 / 525  Training Loss  3.476492565823719e-05\n",
            "Epoch  26 Batch  496 / 525  Training Loss  2.107418367813807e-05\n",
            "Epoch  26 Batch  497 / 525  Training Loss  3.2054729672381654e-05\n",
            "Epoch  26 Batch  498 / 525  Training Loss  2.5027558876900002e-05\n",
            "Epoch  26 Batch  499 / 525  Training Loss  2.447002407279797e-05\n",
            "Epoch  26 Batch  500 / 525  Training Loss  2.576248334662523e-05\n",
            "Epoch  26 Batch  501 / 525  Training Loss  3.359685797477141e-05\n",
            "Epoch  26 Batch  502 / 525  Training Loss  3.442508386797272e-05\n",
            "Epoch  26 Batch  503 / 525  Training Loss  3.368654506630264e-05\n",
            "Epoch  26 Batch  504 / 525  Training Loss  3.274700793554075e-05\n",
            "Epoch  26 Batch  505 / 525  Training Loss  2.979324199259281e-05\n",
            "Epoch  26 Batch  506 / 525  Training Loss  2.7025385861634277e-05\n",
            "Epoch  26 Batch  507 / 525  Training Loss  3.259059303672984e-05\n",
            "Epoch  26 Batch  508 / 525  Training Loss  3.4247495932504535e-05\n",
            "Epoch  26 Batch  509 / 525  Training Loss  4.2638901504687965e-05\n",
            "Epoch  26 Batch  510 / 525  Training Loss  2.564315582276322e-05\n",
            "Epoch  26 Batch  511 / 525  Training Loss  2.99819657811895e-05\n",
            "Epoch  26 Batch  512 / 525  Training Loss  2.369869798712898e-05\n",
            "Epoch  26 Batch  513 / 525  Training Loss  3.3088061172747985e-05\n",
            "Epoch  26 Batch  514 / 525  Training Loss  2.8214548365212977e-05\n",
            "Epoch  26 Batch  515 / 525  Training Loss  2.47148909693351e-05\n",
            "Epoch  26 Batch  516 / 525  Training Loss  3.5355442378204316e-05\n",
            "Epoch  26 Batch  517 / 525  Training Loss  3.3944415918085724e-05\n",
            "Epoch  26 Batch  518 / 525  Training Loss  2.6659725335775875e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  26 Batch  519 / 525  Training Loss  4.0542978240409866e-05\n",
            "Epoch  26 Batch  520 / 525  Training Loss  2.7551152015803382e-05\n",
            "Epoch  26 Batch  521 / 525  Training Loss  2.873669109249022e-05\n",
            "Epoch  26 Batch  522 / 525  Training Loss  2.9064551199553534e-05\n",
            "Epoch  26 Batch  523 / 525  Training Loss  3.058846778003499e-05\n",
            "Epoch  26 Batch  524 / 525  Training Loss  3.9462502172682434e-05\n",
            "  27    |    -    |   0.000032   |   64.03  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 27\n",
            "Epoch  27 Batch  0 / 525  Training Loss  2.7796244467026554e-05\n",
            "Epoch  27 Batch  1 / 525  Training Loss  3.0147606594255194e-05\n",
            "Epoch  27 Batch  2 / 525  Training Loss  3.2984662539092824e-05\n",
            "Epoch  27 Batch  3 / 525  Training Loss  2.792536179185845e-05\n",
            "Epoch  27 Batch  4 / 525  Training Loss  2.8028505766997114e-05\n",
            "Epoch  27 Batch  5 / 525  Training Loss  2.3119448087527417e-05\n",
            "Epoch  27 Batch  6 / 525  Training Loss  3.400995774427429e-05\n",
            "Epoch  27 Batch  7 / 525  Training Loss  3.324465433252044e-05\n",
            "Epoch  27 Batch  8 / 525  Training Loss  2.8898382879560813e-05\n",
            "Epoch  27 Batch  9 / 525  Training Loss  2.9699474907829426e-05\n",
            "Epoch  27 Batch  10 / 525  Training Loss  2.2645373974228278e-05\n",
            "Epoch  27 Batch  11 / 525  Training Loss  3.396970350877382e-05\n",
            "Epoch  27 Batch  12 / 525  Training Loss  3.069975718972273e-05\n",
            "Epoch  27 Batch  13 / 525  Training Loss  4.2026258597616106e-05\n",
            "Epoch  27 Batch  14 / 525  Training Loss  3.250946247135289e-05\n",
            "Epoch  27 Batch  15 / 525  Training Loss  2.7539363145479e-05\n",
            "Epoch  27 Batch  16 / 525  Training Loss  2.029892675636802e-05\n",
            "Epoch  27 Batch  17 / 525  Training Loss  1.627669553272426e-05\n",
            "Epoch  27 Batch  18 / 525  Training Loss  3.7106779927853495e-05\n",
            "Epoch  27 Batch  19 / 525  Training Loss  2.27112541324459e-05\n",
            "Epoch  27 Batch  20 / 525  Training Loss  2.868063711503055e-05\n",
            "Epoch  27 Batch  21 / 525  Training Loss  4.11533001170028e-05\n",
            "Epoch  27 Batch  22 / 525  Training Loss  2.4746677809162065e-05\n",
            "Epoch  27 Batch  23 / 525  Training Loss  3.5873705201083794e-05\n",
            "Epoch  27 Batch  24 / 525  Training Loss  2.8293856303207576e-05\n",
            "Epoch  27 Batch  25 / 525  Training Loss  3.1427727662958205e-05\n",
            "Epoch  27 Batch  26 / 525  Training Loss  2.6336285372963175e-05\n",
            "Epoch  27 Batch  27 / 525  Training Loss  3.010447289852891e-05\n",
            "Epoch  27 Batch  28 / 525  Training Loss  3.763591303140856e-05\n",
            "Epoch  27 Batch  29 / 525  Training Loss  2.637078614498023e-05\n",
            "Epoch  27 Batch  30 / 525  Training Loss  3.551954432623461e-05\n",
            "Epoch  27 Batch  31 / 525  Training Loss  3.355364242452197e-05\n",
            "Epoch  27 Batch  32 / 525  Training Loss  1.993179648707155e-05\n",
            "Epoch  27 Batch  33 / 525  Training Loss  2.4568853405071422e-05\n",
            "Epoch  27 Batch  34 / 525  Training Loss  2.63933434325736e-05\n",
            "Epoch  27 Batch  35 / 525  Training Loss  3.2724761695135385e-05\n",
            "Epoch  27 Batch  36 / 525  Training Loss  3.672761522466317e-05\n",
            "Epoch  27 Batch  37 / 525  Training Loss  2.986193612741772e-05\n",
            "Epoch  27 Batch  38 / 525  Training Loss  3.146718518109992e-05\n",
            "Epoch  27 Batch  39 / 525  Training Loss  3.4180371585534886e-05\n",
            "Epoch  27 Batch  40 / 525  Training Loss  2.8879116143798456e-05\n",
            "Epoch  27 Batch  41 / 525  Training Loss  1.963805334526114e-05\n",
            "Epoch  27 Batch  42 / 525  Training Loss  2.059312282653991e-05\n",
            "Epoch  27 Batch  43 / 525  Training Loss  3.638394991867244e-05\n",
            "Epoch  27 Batch  44 / 525  Training Loss  2.37712447415106e-05\n",
            "Epoch  27 Batch  45 / 525  Training Loss  2.715110531426035e-05\n",
            "Epoch  27 Batch  46 / 525  Training Loss  2.483194475644268e-05\n",
            "Epoch  27 Batch  47 / 525  Training Loss  2.4619203031761572e-05\n",
            "Epoch  27 Batch  48 / 525  Training Loss  2.1800540707772598e-05\n",
            "Epoch  27 Batch  49 / 525  Training Loss  2.6771967895911075e-05\n",
            "Epoch  27 Batch  50 / 525  Training Loss  4.402747072163038e-05\n",
            "Epoch  27 Batch  51 / 525  Training Loss  2.3923794287838973e-05\n",
            "Epoch  27 Batch  52 / 525  Training Loss  2.3785189114278182e-05\n",
            "Epoch  27 Batch  53 / 525  Training Loss  3.759774335776456e-05\n",
            "Epoch  27 Batch  54 / 525  Training Loss  3.168129114783369e-05\n",
            "Epoch  27 Batch  55 / 525  Training Loss  2.7537291316548362e-05\n",
            "Epoch  27 Batch  56 / 525  Training Loss  2.7439431505627e-05\n",
            "Epoch  27 Batch  57 / 525  Training Loss  3.0617018637713045e-05\n",
            "Epoch  27 Batch  58 / 525  Training Loss  2.7620098990155384e-05\n",
            "Epoch  27 Batch  59 / 525  Training Loss  3.2712690881453454e-05\n",
            "Epoch  27 Batch  60 / 525  Training Loss  3.0329170840559527e-05\n",
            "Epoch  27 Batch  61 / 525  Training Loss  2.1145775463082828e-05\n",
            "Epoch  27 Batch  62 / 525  Training Loss  3.614894376369193e-05\n",
            "Epoch  27 Batch  63 / 525  Training Loss  2.9300523237907328e-05\n",
            "Epoch  27 Batch  64 / 525  Training Loss  2.7665746529237367e-05\n",
            "Epoch  27 Batch  65 / 525  Training Loss  2.7847312594531104e-05\n",
            "Epoch  27 Batch  66 / 525  Training Loss  3.026649574167095e-05\n",
            "Epoch  27 Batch  67 / 525  Training Loss  3.8265388866420835e-05\n",
            "Epoch  27 Batch  68 / 525  Training Loss  3.644627213361673e-05\n",
            "Epoch  27 Batch  69 / 525  Training Loss  3.031208507309202e-05\n",
            "Epoch  27 Batch  70 / 525  Training Loss  2.6386964236735366e-05\n",
            "Epoch  27 Batch  71 / 525  Training Loss  3.182156797265634e-05\n",
            "Epoch  27 Batch  72 / 525  Training Loss  2.5905881557264365e-05\n",
            "Epoch  27 Batch  73 / 525  Training Loss  2.8466451112763025e-05\n",
            "Epoch  27 Batch  74 / 525  Training Loss  3.6797246139030904e-05\n",
            "Epoch  27 Batch  75 / 525  Training Loss  2.4612931156298146e-05\n",
            "Epoch  27 Batch  76 / 525  Training Loss  2.2936608729651198e-05\n",
            "Epoch  27 Batch  77 / 525  Training Loss  3.27747875417117e-05\n",
            "Epoch  27 Batch  78 / 525  Training Loss  3.639715941972099e-05\n",
            "Epoch  27 Batch  79 / 525  Training Loss  2.7993350158794783e-05\n",
            "Epoch  27 Batch  80 / 525  Training Loss  2.9374778023338877e-05\n",
            "Epoch  27 Batch  81 / 525  Training Loss  2.9151031412766315e-05\n",
            "Epoch  27 Batch  82 / 525  Training Loss  2.508771649445407e-05\n",
            "Epoch  27 Batch  83 / 525  Training Loss  2.6111698389286175e-05\n",
            "Epoch  27 Batch  84 / 525  Training Loss  3.812366776401177e-05\n",
            "Epoch  27 Batch  85 / 525  Training Loss  3.567053499864414e-05\n",
            "Epoch  27 Batch  86 / 525  Training Loss  2.7997706638416275e-05\n",
            "Epoch  27 Batch  87 / 525  Training Loss  2.507538920326624e-05\n",
            "Epoch  27 Batch  88 / 525  Training Loss  3.518021185300313e-05\n",
            "Epoch  27 Batch  89 / 525  Training Loss  2.621094972710125e-05\n",
            "Epoch  27 Batch  90 / 525  Training Loss  4.0086411900119856e-05\n",
            "Epoch  27 Batch  91 / 525  Training Loss  2.815889638441149e-05\n",
            "Epoch  27 Batch  92 / 525  Training Loss  3.6885896406602114e-05\n",
            "Epoch  27 Batch  93 / 525  Training Loss  3.966605800087564e-05\n",
            "Epoch  27 Batch  94 / 525  Training Loss  2.9697950594709255e-05\n",
            "Epoch  27 Batch  95 / 525  Training Loss  2.6067858925671317e-05\n",
            "Epoch  27 Batch  96 / 525  Training Loss  2.629266782605555e-05\n",
            "Epoch  27 Batch  97 / 525  Training Loss  1.878567854873836e-05\n",
            "Epoch  27 Batch  98 / 525  Training Loss  1.4963700778025668e-05\n",
            "Epoch  27 Batch  99 / 525  Training Loss  3.604882658692077e-05\n",
            "Epoch  27 Batch  100 / 525  Training Loss  2.3998185497475788e-05\n",
            "Epoch  27 Batch  101 / 525  Training Loss  2.4947701604105532e-05\n",
            "Epoch  27 Batch  102 / 525  Training Loss  3.8991962355794385e-05\n",
            "Epoch  27 Batch  103 / 525  Training Loss  2.4204939109040424e-05\n",
            "Epoch  27 Batch  104 / 525  Training Loss  3.418952474021353e-05\n",
            "Epoch  27 Batch  105 / 525  Training Loss  2.025390313065145e-05\n",
            "Epoch  27 Batch  106 / 525  Training Loss  3.6252138670533895e-05\n",
            "Epoch  27 Batch  107 / 525  Training Loss  3.274046321166679e-05\n",
            "Epoch  27 Batch  108 / 525  Training Loss  2.9006690965616144e-05\n",
            "Epoch  27 Batch  109 / 525  Training Loss  2.4366254365304485e-05\n",
            "Epoch  27 Batch  110 / 525  Training Loss  3.288045627414249e-05\n",
            "Epoch  27 Batch  111 / 525  Training Loss  2.9992490453878418e-05\n",
            "Epoch  27 Batch  112 / 525  Training Loss  2.8711661798297428e-05\n",
            "Epoch  27 Batch  113 / 525  Training Loss  2.536306783440523e-05\n",
            "Epoch  27 Batch  114 / 525  Training Loss  3.226962871849537e-05\n",
            "Epoch  27 Batch  115 / 525  Training Loss  2.2074331354815513e-05\n",
            "Epoch  27 Batch  116 / 525  Training Loss  2.094169940392021e-05\n",
            "Epoch  27 Batch  117 / 525  Training Loss  3.629273851402104e-05\n",
            "Epoch  27 Batch  118 / 525  Training Loss  3.1762094295118004e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  27 Batch  119 / 525  Training Loss  2.318606675544288e-05\n",
            "Epoch  27 Batch  120 / 525  Training Loss  3.077924338867888e-05\n",
            "Epoch  27 Batch  121 / 525  Training Loss  3.1150226277532056e-05\n",
            "Epoch  27 Batch  122 / 525  Training Loss  2.3227323254104704e-05\n",
            "Epoch  27 Batch  123 / 525  Training Loss  3.9876351365819573e-05\n",
            "Epoch  27 Batch  124 / 525  Training Loss  3.263665348640643e-05\n",
            "Epoch  27 Batch  125 / 525  Training Loss  2.9045000701444224e-05\n",
            "Epoch  27 Batch  126 / 525  Training Loss  2.8239566745469347e-05\n",
            "Epoch  27 Batch  127 / 525  Training Loss  2.2915159206604585e-05\n",
            "Epoch  27 Batch  128 / 525  Training Loss  2.6076435460709035e-05\n",
            "Epoch  27 Batch  129 / 525  Training Loss  3.116313018836081e-05\n",
            "Epoch  27 Batch  130 / 525  Training Loss  3.1535480957245454e-05\n",
            "Epoch  27 Batch  131 / 525  Training Loss  2.3106986191123724e-05\n",
            "Epoch  27 Batch  132 / 525  Training Loss  2.1283392925397493e-05\n",
            "Epoch  27 Batch  133 / 525  Training Loss  3.4974444133695215e-05\n",
            "Epoch  27 Batch  134 / 525  Training Loss  2.2223228370421566e-05\n",
            "Epoch  27 Batch  135 / 525  Training Loss  3.330252366140485e-05\n",
            "Epoch  27 Batch  136 / 525  Training Loss  2.886622678488493e-05\n",
            "Epoch  27 Batch  137 / 525  Training Loss  2.6027968488051556e-05\n",
            "Epoch  27 Batch  138 / 525  Training Loss  2.580552791187074e-05\n",
            "Epoch  27 Batch  139 / 525  Training Loss  2.805344047374092e-05\n",
            "Epoch  27 Batch  140 / 525  Training Loss  2.3117512682802044e-05\n",
            "Epoch  27 Batch  141 / 525  Training Loss  2.8814898541895673e-05\n",
            "Epoch  27 Batch  142 / 525  Training Loss  3.121196641586721e-05\n",
            "Epoch  27 Batch  143 / 525  Training Loss  2.7452630092739128e-05\n",
            "Epoch  27 Batch  144 / 525  Training Loss  3.313163688289933e-05\n",
            "Epoch  27 Batch  145 / 525  Training Loss  2.758070513664279e-05\n",
            "Epoch  27 Batch  146 / 525  Training Loss  3.3165204513352364e-05\n",
            "Epoch  27 Batch  147 / 525  Training Loss  3.060833478230052e-05\n",
            "Epoch  27 Batch  148 / 525  Training Loss  3.454818943282589e-05\n",
            "Epoch  27 Batch  149 / 525  Training Loss  3.0601055186707526e-05\n",
            "Epoch  27 Batch  150 / 525  Training Loss  3.0752129532629624e-05\n",
            "Epoch  27 Batch  151 / 525  Training Loss  2.371878144913353e-05\n",
            "Epoch  27 Batch  152 / 525  Training Loss  3.1183437386061996e-05\n",
            "Epoch  27 Batch  153 / 525  Training Loss  2.549773853388615e-05\n",
            "Epoch  27 Batch  154 / 525  Training Loss  4.029761839774437e-05\n",
            "Epoch  27 Batch  155 / 525  Training Loss  3.017559902218636e-05\n",
            "Epoch  27 Batch  156 / 525  Training Loss  3.3247626561205834e-05\n",
            "Epoch  27 Batch  157 / 525  Training Loss  3.447526978561655e-05\n",
            "Epoch  27 Batch  158 / 525  Training Loss  3.443935202085413e-05\n",
            "Epoch  27 Batch  159 / 525  Training Loss  2.6607338440953754e-05\n",
            "Epoch  27 Batch  160 / 525  Training Loss  1.9565221009543166e-05\n",
            "Epoch  27 Batch  161 / 525  Training Loss  2.8798862331314012e-05\n",
            "Epoch  27 Batch  162 / 525  Training Loss  1.6382156900363043e-05\n",
            "Epoch  27 Batch  163 / 525  Training Loss  2.1282348825479858e-05\n",
            "Epoch  27 Batch  164 / 525  Training Loss  1.8437431208440103e-05\n",
            "Epoch  27 Batch  165 / 525  Training Loss  3.864561585942283e-05\n",
            "Epoch  27 Batch  166 / 525  Training Loss  2.36844462051522e-05\n",
            "Epoch  27 Batch  167 / 525  Training Loss  3.0391014661290683e-05\n",
            "Epoch  27 Batch  168 / 525  Training Loss  1.617573798284866e-05\n",
            "Epoch  27 Batch  169 / 525  Training Loss  3.902568278135732e-05\n",
            "Epoch  27 Batch  170 / 525  Training Loss  3.566858504200354e-05\n",
            "Epoch  27 Batch  171 / 525  Training Loss  3.7490768590942025e-05\n",
            "Epoch  27 Batch  172 / 525  Training Loss  2.455034336890094e-05\n",
            "Epoch  27 Batch  173 / 525  Training Loss  2.7350452000973746e-05\n",
            "Epoch  27 Batch  174 / 525  Training Loss  2.4980694433907047e-05\n",
            "Epoch  27 Batch  175 / 525  Training Loss  2.7712148948921822e-05\n",
            "Epoch  27 Batch  176 / 525  Training Loss  2.7151845642947592e-05\n",
            "Epoch  27 Batch  177 / 525  Training Loss  3.347628080518916e-05\n",
            "Epoch  27 Batch  178 / 525  Training Loss  2.7761296223616228e-05\n",
            "Epoch  27 Batch  179 / 525  Training Loss  2.9680037187063135e-05\n",
            "Epoch  27 Batch  180 / 525  Training Loss  2.9587192329927348e-05\n",
            "Epoch  27 Batch  181 / 525  Training Loss  2.7589925593929365e-05\n",
            "Epoch  27 Batch  182 / 525  Training Loss  3.140975604765117e-05\n",
            "Epoch  27 Batch  183 / 525  Training Loss  3.558538082870655e-05\n",
            "Epoch  27 Batch  184 / 525  Training Loss  1.9398754375288263e-05\n",
            "Epoch  27 Batch  185 / 525  Training Loss  2.5651916075730696e-05\n",
            "Epoch  27 Batch  186 / 525  Training Loss  2.7111153031000867e-05\n",
            "Epoch  27 Batch  187 / 525  Training Loss  4.052028089063242e-05\n",
            "Epoch  27 Batch  188 / 525  Training Loss  2.3610748030478135e-05\n",
            "Epoch  27 Batch  189 / 525  Training Loss  3.184310844517313e-05\n",
            "Epoch  27 Batch  190 / 525  Training Loss  2.76828977803234e-05\n",
            "Epoch  27 Batch  191 / 525  Training Loss  4.523914685705677e-05\n",
            "Epoch  27 Batch  192 / 525  Training Loss  2.437745570205152e-05\n",
            "Epoch  27 Batch  193 / 525  Training Loss  2.4386963559663855e-05\n",
            "Epoch  27 Batch  194 / 525  Training Loss  3.9283248042920604e-05\n",
            "Epoch  27 Batch  195 / 525  Training Loss  3.878010102198459e-05\n",
            "Epoch  27 Batch  196 / 525  Training Loss  2.6563817300484516e-05\n",
            "Epoch  27 Batch  197 / 525  Training Loss  2.918736390711274e-05\n",
            "Epoch  27 Batch  198 / 525  Training Loss  3.219195787096396e-05\n",
            "Epoch  27 Batch  199 / 525  Training Loss  3.0180362955434248e-05\n",
            "Epoch  27 Batch  200 / 525  Training Loss  3.6562974855769426e-05\n",
            "Epoch  27 Batch  201 / 525  Training Loss  3.4031952964141965e-05\n",
            "Epoch  27 Batch  202 / 525  Training Loss  2.3920330932014622e-05\n",
            "Epoch  27 Batch  203 / 525  Training Loss  3.215918331989087e-05\n",
            "Epoch  27 Batch  204 / 525  Training Loss  2.931832023023162e-05\n",
            "Epoch  27 Batch  205 / 525  Training Loss  1.997621075133793e-05\n",
            "Epoch  27 Batch  206 / 525  Training Loss  3.045754237973597e-05\n",
            "Epoch  27 Batch  207 / 525  Training Loss  3.160613414365798e-05\n",
            "Epoch  27 Batch  208 / 525  Training Loss  2.6680510927690193e-05\n",
            "Epoch  27 Batch  209 / 525  Training Loss  3.477572317933664e-05\n",
            "Epoch  27 Batch  210 / 525  Training Loss  2.6449153665453196e-05\n",
            "Epoch  27 Batch  211 / 525  Training Loss  3.263987309765071e-05\n",
            "Epoch  27 Batch  212 / 525  Training Loss  2.858528205251787e-05\n",
            "Epoch  27 Batch  213 / 525  Training Loss  3.027642742381431e-05\n",
            "Epoch  27 Batch  214 / 525  Training Loss  3.161218046443537e-05\n",
            "Epoch  27 Batch  215 / 525  Training Loss  3.0210398108465597e-05\n",
            "Epoch  27 Batch  216 / 525  Training Loss  3.8752528780605644e-05\n",
            "Epoch  27 Batch  217 / 525  Training Loss  2.7010240955860354e-05\n",
            "Epoch  27 Batch  218 / 525  Training Loss  3.49287802237086e-05\n",
            "Epoch  27 Batch  219 / 525  Training Loss  2.6022165911854245e-05\n",
            "Epoch  27 Batch  220 / 525  Training Loss  3.168608236592263e-05\n",
            "Epoch  27 Batch  221 / 525  Training Loss  3.422958980081603e-05\n",
            "Epoch  27 Batch  222 / 525  Training Loss  3.0716077162651345e-05\n",
            "Epoch  27 Batch  223 / 525  Training Loss  2.5654095225036144e-05\n",
            "Epoch  27 Batch  224 / 525  Training Loss  3.371309139765799e-05\n",
            "Epoch  27 Batch  225 / 525  Training Loss  2.490661063347943e-05\n",
            "Epoch  27 Batch  226 / 525  Training Loss  2.2926102246856317e-05\n",
            "Epoch  27 Batch  227 / 525  Training Loss  3.355949229444377e-05\n",
            "Epoch  27 Batch  228 / 525  Training Loss  3.111374462605454e-05\n",
            "Epoch  27 Batch  229 / 525  Training Loss  2.8658072551479563e-05\n",
            "Epoch  27 Batch  230 / 525  Training Loss  2.0423796740942635e-05\n",
            "Epoch  27 Batch  231 / 525  Training Loss  2.8847967769252136e-05\n",
            "Epoch  27 Batch  232 / 525  Training Loss  3.4690212487475947e-05\n",
            "Epoch  27 Batch  233 / 525  Training Loss  2.7425752705312334e-05\n",
            "Epoch  27 Batch  234 / 525  Training Loss  2.8074777219444513e-05\n",
            "Epoch  27 Batch  235 / 525  Training Loss  2.60420983977383e-05\n",
            "Epoch  27 Batch  236 / 525  Training Loss  3.0009279726073146e-05\n",
            "Epoch  27 Batch  237 / 525  Training Loss  2.5867924705380574e-05\n",
            "Epoch  27 Batch  238 / 525  Training Loss  2.9956558137200773e-05\n",
            "Epoch  27 Batch  239 / 525  Training Loss  3.1603180104866624e-05\n",
            "Epoch  27 Batch  240 / 525  Training Loss  3.842208388959989e-05\n",
            "Epoch  27 Batch  241 / 525  Training Loss  3.2479460060130805e-05\n",
            "Epoch  27 Batch  242 / 525  Training Loss  3.268372529419139e-05\n",
            "Epoch  27 Batch  243 / 525  Training Loss  2.1809410100104287e-05\n",
            "Epoch  27 Batch  244 / 525  Training Loss  3.0871440685587004e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  27 Batch  245 / 525  Training Loss  2.999692878802307e-05\n",
            "Epoch  27 Batch  246 / 525  Training Loss  2.790433427435346e-05\n",
            "Epoch  27 Batch  247 / 525  Training Loss  2.2790769435232505e-05\n",
            "Epoch  27 Batch  248 / 525  Training Loss  3.4606629924383014e-05\n",
            "Epoch  27 Batch  249 / 525  Training Loss  2.6863510356633924e-05\n",
            "Epoch  27 Batch  250 / 525  Training Loss  2.760985262284521e-05\n",
            "Epoch  27 Batch  251 / 525  Training Loss  2.5607578209019266e-05\n",
            "Epoch  27 Batch  252 / 525  Training Loss  2.4979308363981545e-05\n",
            "Epoch  27 Batch  253 / 525  Training Loss  3.748559538507834e-05\n",
            "Epoch  27 Batch  254 / 525  Training Loss  2.7211901397095062e-05\n",
            "Epoch  27 Batch  255 / 525  Training Loss  3.249406290706247e-05\n",
            "Epoch  27 Batch  256 / 525  Training Loss  1.9358147255843505e-05\n",
            "Epoch  27 Batch  257 / 525  Training Loss  2.2761807485949248e-05\n",
            "Epoch  27 Batch  258 / 525  Training Loss  3.339148679515347e-05\n",
            "Epoch  27 Batch  259 / 525  Training Loss  2.894457065849565e-05\n",
            "Epoch  27 Batch  260 / 525  Training Loss  2.4173050405806862e-05\n",
            "Epoch  27 Batch  261 / 525  Training Loss  3.5157987440470606e-05\n",
            "Epoch  27 Batch  262 / 525  Training Loss  2.562204645073507e-05\n",
            "Epoch  27 Batch  263 / 525  Training Loss  3.276079223724082e-05\n",
            "Epoch  27 Batch  264 / 525  Training Loss  2.6968526071868837e-05\n",
            "Epoch  27 Batch  265 / 525  Training Loss  3.028287574124988e-05\n",
            "Epoch  27 Batch  266 / 525  Training Loss  4.008297764812596e-05\n",
            "Epoch  27 Batch  267 / 525  Training Loss  2.132820554834325e-05\n",
            "Epoch  27 Batch  268 / 525  Training Loss  2.6100988179678097e-05\n",
            "Epoch  27 Batch  269 / 525  Training Loss  2.2841846657684073e-05\n",
            "Epoch  27 Batch  270 / 525  Training Loss  2.794292595353909e-05\n",
            "Epoch  27 Batch  271 / 525  Training Loss  3.016861228388734e-05\n",
            "Epoch  27 Batch  272 / 525  Training Loss  3.4937089367304e-05\n",
            "Epoch  27 Batch  273 / 525  Training Loss  2.115171628247481e-05\n",
            "Epoch  27 Batch  274 / 525  Training Loss  2.5408869987586513e-05\n",
            "Epoch  27 Batch  275 / 525  Training Loss  2.5782646844163537e-05\n",
            "Epoch  27 Batch  276 / 525  Training Loss  2.8615080736926757e-05\n",
            "Epoch  27 Batch  277 / 525  Training Loss  2.9937127692392096e-05\n",
            "Epoch  27 Batch  278 / 525  Training Loss  4.0697366785025224e-05\n",
            "Epoch  27 Batch  279 / 525  Training Loss  2.6944249839289114e-05\n",
            "Epoch  27 Batch  280 / 525  Training Loss  3.096596265095286e-05\n",
            "Epoch  27 Batch  281 / 525  Training Loss  3.202403604518622e-05\n",
            "Epoch  27 Batch  282 / 525  Training Loss  2.3421536752721295e-05\n",
            "Epoch  27 Batch  283 / 525  Training Loss  2.971156209241599e-05\n",
            "Epoch  27 Batch  284 / 525  Training Loss  2.58482905337587e-05\n",
            "Epoch  27 Batch  285 / 525  Training Loss  2.6396053726784885e-05\n",
            "Epoch  27 Batch  286 / 525  Training Loss  2.9819584597134963e-05\n",
            "Epoch  27 Batch  287 / 525  Training Loss  3.4665772545849904e-05\n",
            "Epoch  27 Batch  288 / 525  Training Loss  1.4434796867135447e-05\n",
            "Epoch  27 Batch  289 / 525  Training Loss  2.1011359422118403e-05\n",
            "Epoch  27 Batch  290 / 525  Training Loss  3.474944969639182e-05\n",
            "Epoch  27 Batch  291 / 525  Training Loss  2.7016683816327713e-05\n",
            "Epoch  27 Batch  292 / 525  Training Loss  2.686787411221303e-05\n",
            "Epoch  27 Batch  293 / 525  Training Loss  2.6494588382774964e-05\n",
            "Epoch  27 Batch  294 / 525  Training Loss  2.5692632334539667e-05\n",
            "Epoch  27 Batch  295 / 525  Training Loss  2.8296311938902363e-05\n",
            "Epoch  27 Batch  296 / 525  Training Loss  3.5994715290144086e-05\n",
            "Epoch  27 Batch  297 / 525  Training Loss  4.078749043401331e-05\n",
            "Epoch  27 Batch  298 / 525  Training Loss  2.8304653824307024e-05\n",
            "Epoch  27 Batch  299 / 525  Training Loss  2.9405549867078662e-05\n",
            "Epoch  27 Batch  300 / 525  Training Loss  2.975202187371906e-05\n",
            "Epoch  27 Batch  301 / 525  Training Loss  2.8779240892617963e-05\n",
            "Epoch  27 Batch  302 / 525  Training Loss  3.135132647003047e-05\n",
            "Epoch  27 Batch  303 / 525  Training Loss  2.897449303418398e-05\n",
            "Epoch  27 Batch  304 / 525  Training Loss  3.6275981983635575e-05\n",
            "Epoch  27 Batch  305 / 525  Training Loss  2.4726254196139053e-05\n",
            "Epoch  27 Batch  306 / 525  Training Loss  2.0428044081199914e-05\n",
            "Epoch  27 Batch  307 / 525  Training Loss  3.407547774259001e-05\n",
            "Epoch  27 Batch  308 / 525  Training Loss  3.0207776944735087e-05\n",
            "Epoch  27 Batch  309 / 525  Training Loss  2.9632738005602732e-05\n",
            "Epoch  27 Batch  310 / 525  Training Loss  3.501021274132654e-05\n",
            "Epoch  27 Batch  311 / 525  Training Loss  3.995029692305252e-05\n",
            "Epoch  27 Batch  312 / 525  Training Loss  2.1859801563550718e-05\n",
            "Epoch  27 Batch  313 / 525  Training Loss  2.555440914875362e-05\n",
            "Epoch  27 Batch  314 / 525  Training Loss  2.487855454091914e-05\n",
            "Epoch  27 Batch  315 / 525  Training Loss  3.3552856621099636e-05\n",
            "Epoch  27 Batch  316 / 525  Training Loss  2.070217306027189e-05\n",
            "Epoch  27 Batch  317 / 525  Training Loss  2.7725915060727857e-05\n",
            "Epoch  27 Batch  318 / 525  Training Loss  3.124212889815681e-05\n",
            "Epoch  27 Batch  319 / 525  Training Loss  2.430814492981881e-05\n",
            "Epoch  27 Batch  320 / 525  Training Loss  3.572231071302667e-05\n",
            "Epoch  27 Batch  321 / 525  Training Loss  2.0581612261594273e-05\n",
            "Epoch  27 Batch  322 / 525  Training Loss  2.0243116523488425e-05\n",
            "Epoch  27 Batch  323 / 525  Training Loss  2.271466473757755e-05\n",
            "Epoch  27 Batch  324 / 525  Training Loss  3.215825563529506e-05\n",
            "Epoch  27 Batch  325 / 525  Training Loss  1.7434333130950108e-05\n",
            "Epoch  27 Batch  326 / 525  Training Loss  2.806279735523276e-05\n",
            "Epoch  27 Batch  327 / 525  Training Loss  2.3152868379838765e-05\n",
            "Epoch  27 Batch  328 / 525  Training Loss  2.6002666345448233e-05\n",
            "Epoch  27 Batch  329 / 525  Training Loss  3.121280315099284e-05\n",
            "Epoch  27 Batch  330 / 525  Training Loss  2.8455668143578805e-05\n",
            "Epoch  27 Batch  331 / 525  Training Loss  2.225994467153214e-05\n",
            "Epoch  27 Batch  332 / 525  Training Loss  3.620203278842382e-05\n",
            "Epoch  27 Batch  333 / 525  Training Loss  2.85450805677101e-05\n",
            "Epoch  27 Batch  334 / 525  Training Loss  2.707248495426029e-05\n",
            "Epoch  27 Batch  335 / 525  Training Loss  3.259746154071763e-05\n",
            "Epoch  27 Batch  336 / 525  Training Loss  2.7124653570353985e-05\n",
            "Epoch  27 Batch  337 / 525  Training Loss  2.846077040885575e-05\n",
            "Epoch  27 Batch  338 / 525  Training Loss  2.872999721148517e-05\n",
            "Epoch  27 Batch  339 / 525  Training Loss  3.3947340853046626e-05\n",
            "Epoch  27 Batch  340 / 525  Training Loss  3.631290383054875e-05\n",
            "Epoch  27 Batch  341 / 525  Training Loss  2.762456460914109e-05\n",
            "Epoch  27 Batch  342 / 525  Training Loss  2.7288260753266513e-05\n",
            "Epoch  27 Batch  343 / 525  Training Loss  3.314445712021552e-05\n",
            "Epoch  27 Batch  344 / 525  Training Loss  3.0249164410633966e-05\n",
            "Epoch  27 Batch  345 / 525  Training Loss  2.5827306671999395e-05\n",
            "Epoch  27 Batch  346 / 525  Training Loss  2.1093663235660642e-05\n",
            "Epoch  27 Batch  347 / 525  Training Loss  2.529273842810653e-05\n",
            "Epoch  27 Batch  348 / 525  Training Loss  3.110524266958237e-05\n",
            "Epoch  27 Batch  349 / 525  Training Loss  2.9502823963412084e-05\n",
            "Epoch  27 Batch  350 / 525  Training Loss  3.652121813502163e-05\n",
            "Epoch  27 Batch  351 / 525  Training Loss  2.9076607461320236e-05\n",
            "Epoch  27 Batch  352 / 525  Training Loss  2.8697224479401484e-05\n",
            "Epoch  27 Batch  353 / 525  Training Loss  1.849894761107862e-05\n",
            "Epoch  27 Batch  354 / 525  Training Loss  3.252535316278227e-05\n",
            "Epoch  27 Batch  355 / 525  Training Loss  3.060035305679776e-05\n",
            "Epoch  27 Batch  356 / 525  Training Loss  3.435586768318899e-05\n",
            "Epoch  27 Batch  357 / 525  Training Loss  2.9245846235426143e-05\n",
            "Epoch  27 Batch  358 / 525  Training Loss  2.8993195883231238e-05\n",
            "Epoch  27 Batch  359 / 525  Training Loss  4.9792703066486865e-05\n",
            "Epoch  27 Batch  360 / 525  Training Loss  2.898670754802879e-05\n",
            "Epoch  27 Batch  361 / 525  Training Loss  3.3876054658321664e-05\n",
            "Epoch  27 Batch  362 / 525  Training Loss  3.062410905840807e-05\n",
            "Epoch  27 Batch  363 / 525  Training Loss  3.0426564990193583e-05\n",
            "Epoch  27 Batch  364 / 525  Training Loss  2.3550495825475082e-05\n",
            "Epoch  27 Batch  365 / 525  Training Loss  2.621888052090071e-05\n",
            "Epoch  27 Batch  366 / 525  Training Loss  2.8700334951281548e-05\n",
            "Epoch  27 Batch  367 / 525  Training Loss  2.0810202840948477e-05\n",
            "Epoch  27 Batch  368 / 525  Training Loss  2.6308916858397424e-05\n",
            "Epoch  27 Batch  369 / 525  Training Loss  2.3087119188858196e-05\n",
            "Epoch  27 Batch  370 / 525  Training Loss  3.184470551786944e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  27 Batch  371 / 525  Training Loss  1.4600786016671918e-05\n",
            "Epoch  27 Batch  372 / 525  Training Loss  3.458192804828286e-05\n",
            "Epoch  27 Batch  373 / 525  Training Loss  4.142264515394345e-05\n",
            "Epoch  27 Batch  374 / 525  Training Loss  3.388831828488037e-05\n",
            "Epoch  27 Batch  375 / 525  Training Loss  2.4330027372343466e-05\n",
            "Epoch  27 Batch  376 / 525  Training Loss  2.8295200536376797e-05\n",
            "Epoch  27 Batch  377 / 525  Training Loss  2.5513934815535322e-05\n",
            "Epoch  27 Batch  378 / 525  Training Loss  2.7265796234132722e-05\n",
            "Epoch  27 Batch  379 / 525  Training Loss  3.391674181330018e-05\n",
            "Epoch  27 Batch  380 / 525  Training Loss  3.269128501415253e-05\n",
            "Epoch  27 Batch  381 / 525  Training Loss  2.739692536124494e-05\n",
            "Epoch  27 Batch  382 / 525  Training Loss  3.014159665326588e-05\n",
            "Epoch  27 Batch  383 / 525  Training Loss  3.37325400323607e-05\n",
            "Epoch  27 Batch  384 / 525  Training Loss  3.7182508094701916e-05\n",
            "Epoch  27 Batch  385 / 525  Training Loss  3.152850695187226e-05\n",
            "Epoch  27 Batch  386 / 525  Training Loss  3.3064068702515215e-05\n",
            "Epoch  27 Batch  387 / 525  Training Loss  2.9117527446942404e-05\n",
            "Epoch  27 Batch  388 / 525  Training Loss  3.0820814572507516e-05\n",
            "Epoch  27 Batch  389 / 525  Training Loss  3.403503433219157e-05\n",
            "Epoch  27 Batch  390 / 525  Training Loss  3.501226819935255e-05\n",
            "Epoch  27 Batch  391 / 525  Training Loss  2.685204890440218e-05\n",
            "Epoch  27 Batch  392 / 525  Training Loss  2.2619457013206556e-05\n",
            "Epoch  27 Batch  393 / 525  Training Loss  3.204803942935541e-05\n",
            "Epoch  27 Batch  394 / 525  Training Loss  3.0433327992795967e-05\n",
            "Epoch  27 Batch  395 / 525  Training Loss  2.8762733563780785e-05\n",
            "Epoch  27 Batch  396 / 525  Training Loss  3.2075833587441593e-05\n",
            "Epoch  27 Batch  397 / 525  Training Loss  3.6021821870235726e-05\n",
            "Epoch  27 Batch  398 / 525  Training Loss  2.2381778762792237e-05\n",
            "Epoch  27 Batch  399 / 525  Training Loss  3.3795658964663744e-05\n",
            "Epoch  27 Batch  400 / 525  Training Loss  3.0483814043691382e-05\n",
            "Epoch  27 Batch  401 / 525  Training Loss  1.720643376756925e-05\n",
            "Epoch  27 Batch  402 / 525  Training Loss  2.649278030730784e-05\n",
            "Epoch  27 Batch  403 / 525  Training Loss  2.8350308639346622e-05\n",
            "Epoch  27 Batch  404 / 525  Training Loss  2.734843837970402e-05\n",
            "Epoch  27 Batch  405 / 525  Training Loss  2.7039055567001924e-05\n",
            "Epoch  27 Batch  406 / 525  Training Loss  5.085622615297325e-05\n",
            "Epoch  27 Batch  407 / 525  Training Loss  2.7780848540714942e-05\n",
            "Epoch  27 Batch  408 / 525  Training Loss  3.449724317761138e-05\n",
            "Epoch  27 Batch  409 / 525  Training Loss  2.0731504264404066e-05\n",
            "Epoch  27 Batch  410 / 525  Training Loss  3.211866714991629e-05\n",
            "Epoch  27 Batch  411 / 525  Training Loss  3.002446101163514e-05\n",
            "Epoch  27 Batch  412 / 525  Training Loss  2.902278720284812e-05\n",
            "Epoch  27 Batch  413 / 525  Training Loss  3.0193294151104055e-05\n",
            "Epoch  27 Batch  414 / 525  Training Loss  2.4294635295518674e-05\n",
            "Epoch  27 Batch  415 / 525  Training Loss  2.723381112446077e-05\n",
            "Epoch  27 Batch  416 / 525  Training Loss  2.3206450350699015e-05\n",
            "Epoch  27 Batch  417 / 525  Training Loss  2.985533865285106e-05\n",
            "Epoch  27 Batch  418 / 525  Training Loss  4.065663233632222e-05\n",
            "Epoch  27 Batch  419 / 525  Training Loss  3.156347156618722e-05\n",
            "Epoch  27 Batch  420 / 525  Training Loss  3.1400420994032174e-05\n",
            "Epoch  27 Batch  421 / 525  Training Loss  2.58995387412142e-05\n",
            "Epoch  27 Batch  422 / 525  Training Loss  2.358707570238039e-05\n",
            "Epoch  27 Batch  423 / 525  Training Loss  2.7849722755490802e-05\n",
            "Epoch  27 Batch  424 / 525  Training Loss  3.97764342778828e-05\n",
            "Epoch  27 Batch  425 / 525  Training Loss  2.917603706009686e-05\n",
            "Epoch  27 Batch  426 / 525  Training Loss  2.639591184561141e-05\n",
            "Epoch  27 Batch  427 / 525  Training Loss  2.4087330530164763e-05\n",
            "Epoch  27 Batch  428 / 525  Training Loss  2.467382546456065e-05\n",
            "Epoch  27 Batch  429 / 525  Training Loss  2.5601304514566436e-05\n",
            "Epoch  27 Batch  430 / 525  Training Loss  2.4137403670465574e-05\n",
            "Epoch  27 Batch  431 / 525  Training Loss  2.2280129996943288e-05\n",
            "Epoch  27 Batch  432 / 525  Training Loss  2.7837526431540027e-05\n",
            "Epoch  27 Batch  433 / 525  Training Loss  3.487968206172809e-05\n",
            "Epoch  27 Batch  434 / 525  Training Loss  1.9627343135653064e-05\n",
            "Epoch  27 Batch  435 / 525  Training Loss  4.2459170799702406e-05\n",
            "Epoch  27 Batch  436 / 525  Training Loss  2.7504152967594564e-05\n",
            "Epoch  27 Batch  437 / 525  Training Loss  2.180623050662689e-05\n",
            "Epoch  27 Batch  438 / 525  Training Loss  2.6958587113767862e-05\n",
            "Epoch  27 Batch  439 / 525  Training Loss  2.5400620870641433e-05\n",
            "Epoch  27 Batch  440 / 525  Training Loss  4.4023425289196894e-05\n",
            "Epoch  27 Batch  441 / 525  Training Loss  2.5077437385334633e-05\n",
            "Epoch  27 Batch  442 / 525  Training Loss  2.9045104383840226e-05\n",
            "Epoch  27 Batch  443 / 525  Training Loss  2.6409217753098346e-05\n",
            "Epoch  27 Batch  444 / 525  Training Loss  2.629867594805546e-05\n",
            "Epoch  27 Batch  445 / 525  Training Loss  3.3113115932792425e-05\n",
            "Epoch  27 Batch  446 / 525  Training Loss  3.076720895478502e-05\n",
            "Epoch  27 Batch  447 / 525  Training Loss  3.4791213693097234e-05\n",
            "Epoch  27 Batch  448 / 525  Training Loss  3.206668407074176e-05\n",
            "Epoch  27 Batch  449 / 525  Training Loss  3.902455500792712e-05\n",
            "Epoch  27 Batch  450 / 525  Training Loss  2.8288539397181012e-05\n",
            "Epoch  27 Batch  451 / 525  Training Loss  2.620619852677919e-05\n",
            "Epoch  27 Batch  452 / 525  Training Loss  3.5832788853440434e-05\n",
            "Epoch  27 Batch  453 / 525  Training Loss  2.7824140488519333e-05\n",
            "Epoch  27 Batch  454 / 525  Training Loss  2.816879168676678e-05\n",
            "Epoch  27 Batch  455 / 525  Training Loss  2.7905121896765195e-05\n",
            "Epoch  27 Batch  456 / 525  Training Loss  2.9995233489898965e-05\n",
            "Epoch  27 Batch  457 / 525  Training Loss  2.3281771063921042e-05\n",
            "Epoch  27 Batch  458 / 525  Training Loss  2.5562741939211264e-05\n",
            "Epoch  27 Batch  459 / 525  Training Loss  2.788775964290835e-05\n",
            "Epoch  27 Batch  460 / 525  Training Loss  2.3945316570461728e-05\n",
            "Epoch  27 Batch  461 / 525  Training Loss  2.814804611261934e-05\n",
            "Epoch  27 Batch  462 / 525  Training Loss  2.7641552151180804e-05\n",
            "Epoch  27 Batch  463 / 525  Training Loss  2.4939761715359055e-05\n",
            "Epoch  27 Batch  464 / 525  Training Loss  2.5420053134439513e-05\n",
            "Epoch  27 Batch  465 / 525  Training Loss  2.6704259653342888e-05\n",
            "Epoch  27 Batch  466 / 525  Training Loss  2.5130964786512777e-05\n",
            "Epoch  27 Batch  467 / 525  Training Loss  3.0774830520385876e-05\n",
            "Epoch  27 Batch  468 / 525  Training Loss  2.510464400984347e-05\n",
            "Epoch  27 Batch  469 / 525  Training Loss  2.968641638290137e-05\n",
            "Epoch  27 Batch  470 / 525  Training Loss  3.8229314668569714e-05\n",
            "Epoch  27 Batch  471 / 525  Training Loss  2.4510634830221534e-05\n",
            "Epoch  27 Batch  472 / 525  Training Loss  2.3541226255474612e-05\n",
            "Epoch  27 Batch  473 / 525  Training Loss  3.023210410901811e-05\n",
            "Epoch  27 Batch  474 / 525  Training Loss  2.5229333914467134e-05\n",
            "Epoch  27 Batch  475 / 525  Training Loss  2.9926964998594485e-05\n",
            "Epoch  27 Batch  476 / 525  Training Loss  2.7618103558779694e-05\n",
            "Epoch  27 Batch  477 / 525  Training Loss  3.147946699755266e-05\n",
            "Epoch  27 Batch  478 / 525  Training Loss  3.329913306515664e-05\n",
            "Epoch  27 Batch  479 / 525  Training Loss  2.700433469726704e-05\n",
            "Epoch  27 Batch  480 / 525  Training Loss  3.275236304034479e-05\n",
            "Epoch  27 Batch  481 / 525  Training Loss  2.927763489424251e-05\n",
            "Epoch  27 Batch  482 / 525  Training Loss  3.171169737470336e-05\n",
            "Epoch  27 Batch  483 / 525  Training Loss  2.6550920665613376e-05\n",
            "Epoch  27 Batch  484 / 525  Training Loss  2.2131611331133172e-05\n",
            "Epoch  27 Batch  485 / 525  Training Loss  2.646000029926654e-05\n",
            "Epoch  27 Batch  486 / 525  Training Loss  2.2464020730694756e-05\n",
            "Epoch  27 Batch  487 / 525  Training Loss  2.3910761228762567e-05\n",
            "Epoch  27 Batch  488 / 525  Training Loss  2.7251397114014253e-05\n",
            "Epoch  27 Batch  489 / 525  Training Loss  3.83038459403906e-05\n",
            "Epoch  27 Batch  490 / 525  Training Loss  2.587219023553189e-05\n",
            "Epoch  27 Batch  491 / 525  Training Loss  4.368527515907772e-05\n",
            "Epoch  27 Batch  492 / 525  Training Loss  2.60064462054288e-05\n",
            "Epoch  27 Batch  493 / 525  Training Loss  3.010706859640777e-05\n",
            "Epoch  27 Batch  494 / 525  Training Loss  2.3909375158837065e-05\n",
            "Epoch  27 Batch  495 / 525  Training Loss  2.9644434107467532e-05\n",
            "Epoch  27 Batch  496 / 525  Training Loss  3.496706631267443e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  27 Batch  497 / 525  Training Loss  3.0009421607246622e-05\n",
            "Epoch  27 Batch  498 / 525  Training Loss  3.130161712761037e-05\n",
            "Epoch  27 Batch  499 / 525  Training Loss  3.832540096482262e-05\n",
            "Epoch  27 Batch  500 / 525  Training Loss  3.331560583319515e-05\n",
            "Epoch  27 Batch  501 / 525  Training Loss  2.723153738770634e-05\n",
            "Epoch  27 Batch  502 / 525  Training Loss  2.8494880098151043e-05\n",
            "Epoch  27 Batch  503 / 525  Training Loss  3.015392940142192e-05\n",
            "Epoch  27 Batch  504 / 525  Training Loss  2.8880345780635253e-05\n",
            "Epoch  27 Batch  505 / 525  Training Loss  2.471641346346587e-05\n",
            "Epoch  27 Batch  506 / 525  Training Loss  2.3293778212973848e-05\n",
            "Epoch  27 Batch  507 / 525  Training Loss  3.2440781069453806e-05\n",
            "Epoch  27 Batch  508 / 525  Training Loss  2.5756045943126082e-05\n",
            "Epoch  27 Batch  509 / 525  Training Loss  2.4792369003989734e-05\n",
            "Epoch  27 Batch  510 / 525  Training Loss  1.8376667867414653e-05\n",
            "Epoch  27 Batch  511 / 525  Training Loss  2.952014074253384e-05\n",
            "Epoch  27 Batch  512 / 525  Training Loss  3.19343525916338e-05\n",
            "Epoch  27 Batch  513 / 525  Training Loss  2.906483132392168e-05\n",
            "Epoch  27 Batch  514 / 525  Training Loss  3.6194258427713066e-05\n",
            "Epoch  27 Batch  515 / 525  Training Loss  2.9193475711508654e-05\n",
            "Epoch  27 Batch  516 / 525  Training Loss  2.7807833248516545e-05\n",
            "Epoch  27 Batch  517 / 525  Training Loss  3.175818346790038e-05\n",
            "Epoch  27 Batch  518 / 525  Training Loss  2.264146678498946e-05\n",
            "Epoch  27 Batch  519 / 525  Training Loss  3.6014178476762027e-05\n",
            "Epoch  27 Batch  520 / 525  Training Loss  3.770205148612149e-05\n",
            "Epoch  27 Batch  521 / 525  Training Loss  2.8907426894875243e-05\n",
            "Epoch  27 Batch  522 / 525  Training Loss  2.6951160180033185e-05\n",
            "Epoch  27 Batch  523 / 525  Training Loss  2.194342050643172e-05\n",
            "Epoch  27 Batch  524 / 525  Training Loss  3.1140119972405955e-05\n",
            "  28    |    -    |   0.000029   |   64.09  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 28\n",
            "Epoch  28 Batch  0 / 525  Training Loss  1.831765257520601e-05\n",
            "Epoch  28 Batch  1 / 525  Training Loss  3.620777715696022e-05\n",
            "Epoch  28 Batch  2 / 525  Training Loss  2.9818411348969676e-05\n",
            "Epoch  28 Batch  3 / 525  Training Loss  2.6276660719304346e-05\n",
            "Epoch  28 Batch  4 / 525  Training Loss  2.3865422917879187e-05\n",
            "Epoch  28 Batch  5 / 525  Training Loss  2.4952960302471183e-05\n",
            "Epoch  28 Batch  6 / 525  Training Loss  2.7136673452332616e-05\n",
            "Epoch  28 Batch  7 / 525  Training Loss  2.4928691345849074e-05\n",
            "Epoch  28 Batch  8 / 525  Training Loss  2.5850673409877345e-05\n",
            "Epoch  28 Batch  9 / 525  Training Loss  2.706055784074124e-05\n",
            "Epoch  28 Batch  10 / 525  Training Loss  1.8030712453764863e-05\n",
            "Epoch  28 Batch  11 / 525  Training Loss  2.53072903433349e-05\n",
            "Epoch  28 Batch  12 / 525  Training Loss  1.4746581655344926e-05\n",
            "Epoch  28 Batch  13 / 525  Training Loss  2.2632621039520018e-05\n",
            "Epoch  28 Batch  14 / 525  Training Loss  3.319123788969591e-05\n",
            "Epoch  28 Batch  15 / 525  Training Loss  3.2946009014267474e-05\n",
            "Epoch  28 Batch  16 / 525  Training Loss  2.495718581485562e-05\n",
            "Epoch  28 Batch  17 / 525  Training Loss  2.0923314878018573e-05\n",
            "Epoch  28 Batch  18 / 525  Training Loss  2.2489393813884817e-05\n",
            "Epoch  28 Batch  19 / 525  Training Loss  2.3246586351888254e-05\n",
            "Epoch  28 Batch  20 / 525  Training Loss  1.9197550500393845e-05\n",
            "Epoch  28 Batch  21 / 525  Training Loss  2.0861420125584118e-05\n",
            "Epoch  28 Batch  22 / 525  Training Loss  2.644304186105728e-05\n",
            "Epoch  28 Batch  23 / 525  Training Loss  3.3624033676460385e-05\n",
            "Epoch  28 Batch  24 / 525  Training Loss  2.157168091798667e-05\n",
            "Epoch  28 Batch  25 / 525  Training Loss  3.399969864403829e-05\n",
            "Epoch  28 Batch  26 / 525  Training Loss  2.3848540877224877e-05\n",
            "Epoch  28 Batch  27 / 525  Training Loss  2.263124224555213e-05\n",
            "Epoch  28 Batch  28 / 525  Training Loss  2.1576779545284808e-05\n",
            "Epoch  28 Batch  29 / 525  Training Loss  2.6891852030530572e-05\n",
            "Epoch  28 Batch  30 / 525  Training Loss  1.853760659287218e-05\n",
            "Epoch  28 Batch  31 / 525  Training Loss  1.9669956600409932e-05\n",
            "Epoch  28 Batch  32 / 525  Training Loss  2.2508462279802188e-05\n",
            "Epoch  28 Batch  33 / 525  Training Loss  2.9666396585525945e-05\n",
            "Epoch  28 Batch  34 / 525  Training Loss  3.509527232381515e-05\n",
            "Epoch  28 Batch  35 / 525  Training Loss  2.970137575175613e-05\n",
            "Epoch  28 Batch  36 / 525  Training Loss  2.5858167646219954e-05\n",
            "Epoch  28 Batch  37 / 525  Training Loss  2.997886622324586e-05\n",
            "Epoch  28 Batch  38 / 525  Training Loss  2.426219725748524e-05\n",
            "Epoch  28 Batch  39 / 525  Training Loss  3.1255880458047614e-05\n",
            "Epoch  28 Batch  40 / 525  Training Loss  2.5275599909946322e-05\n",
            "Epoch  28 Batch  41 / 525  Training Loss  2.4359682356589474e-05\n",
            "Epoch  28 Batch  42 / 525  Training Loss  3.4128435800084844e-05\n",
            "Epoch  28 Batch  43 / 525  Training Loss  2.947721441159956e-05\n",
            "Epoch  28 Batch  44 / 525  Training Loss  2.8373984605423175e-05\n",
            "Epoch  28 Batch  45 / 525  Training Loss  2.6960993636748753e-05\n",
            "Epoch  28 Batch  46 / 525  Training Loss  3.0925017199479043e-05\n",
            "Epoch  28 Batch  47 / 525  Training Loss  2.1345753339119256e-05\n",
            "Epoch  28 Batch  48 / 525  Training Loss  2.551587749621831e-05\n",
            "Epoch  28 Batch  49 / 525  Training Loss  3.1186391424853355e-05\n",
            "Epoch  28 Batch  50 / 525  Training Loss  2.4925184334279038e-05\n",
            "Epoch  28 Batch  51 / 525  Training Loss  2.7273543310002424e-05\n",
            "Epoch  28 Batch  52 / 525  Training Loss  3.201836807420477e-05\n",
            "Epoch  28 Batch  53 / 525  Training Loss  2.9979715691297315e-05\n",
            "Epoch  28 Batch  54 / 525  Training Loss  2.4314616894116625e-05\n",
            "Epoch  28 Batch  55 / 525  Training Loss  2.405633858870715e-05\n",
            "Epoch  28 Batch  56 / 525  Training Loss  2.2470925614470616e-05\n",
            "Epoch  28 Batch  57 / 525  Training Loss  2.3104668798623607e-05\n",
            "Epoch  28 Batch  58 / 525  Training Loss  3.156820093863644e-05\n",
            "Epoch  28 Batch  59 / 525  Training Loss  2.5491175620118156e-05\n",
            "Epoch  28 Batch  60 / 525  Training Loss  2.2355492546921596e-05\n",
            "Epoch  28 Batch  61 / 525  Training Loss  2.635036435094662e-05\n",
            "Epoch  28 Batch  62 / 525  Training Loss  2.121566649293527e-05\n",
            "Epoch  28 Batch  63 / 525  Training Loss  3.0449979021796025e-05\n",
            "Epoch  28 Batch  64 / 525  Training Loss  2.18706682062475e-05\n",
            "Epoch  28 Batch  65 / 525  Training Loss  2.2732525394530967e-05\n",
            "Epoch  28 Batch  66 / 525  Training Loss  2.8539798222482204e-05\n",
            "Epoch  28 Batch  67 / 525  Training Loss  2.7883248549187556e-05\n",
            "Epoch  28 Batch  68 / 525  Training Loss  2.6629986678017303e-05\n",
            "Epoch  28 Batch  69 / 525  Training Loss  1.9950250134570524e-05\n",
            "Epoch  28 Batch  70 / 525  Training Loss  2.584703361208085e-05\n",
            "Epoch  28 Batch  71 / 525  Training Loss  2.6134881409234367e-05\n",
            "Epoch  28 Batch  72 / 525  Training Loss  2.8979051421629265e-05\n",
            "Epoch  28 Batch  73 / 525  Training Loss  3.365602606208995e-05\n",
            "Epoch  28 Batch  74 / 525  Training Loss  3.394483428564854e-05\n",
            "Epoch  28 Batch  75 / 525  Training Loss  2.791422957670875e-05\n",
            "Epoch  28 Batch  76 / 525  Training Loss  2.9769158572889864e-05\n",
            "Epoch  28 Batch  77 / 525  Training Loss  2.219547241111286e-05\n",
            "Epoch  28 Batch  78 / 525  Training Loss  2.665304418769665e-05\n",
            "Epoch  28 Batch  79 / 525  Training Loss  1.9820310626528226e-05\n",
            "Epoch  28 Batch  80 / 525  Training Loss  3.095171632594429e-05\n",
            "Epoch  28 Batch  81 / 525  Training Loss  4.138550139032304e-05\n",
            "Epoch  28 Batch  82 / 525  Training Loss  3.821722202701494e-05\n",
            "Epoch  28 Batch  83 / 525  Training Loss  3.2219199056271464e-05\n",
            "Epoch  28 Batch  84 / 525  Training Loss  2.2062586140236817e-05\n",
            "Epoch  28 Batch  85 / 525  Training Loss  3.0352875910466537e-05\n",
            "Epoch  28 Batch  86 / 525  Training Loss  3.0448229153989814e-05\n",
            "Epoch  28 Batch  87 / 525  Training Loss  2.1423671569209546e-05\n",
            "Epoch  28 Batch  88 / 525  Training Loss  2.644883352331817e-05\n",
            "Epoch  28 Batch  89 / 525  Training Loss  2.5715373340062797e-05\n",
            "Epoch  28 Batch  90 / 525  Training Loss  2.7384794520912692e-05\n",
            "Epoch  28 Batch  91 / 525  Training Loss  3.0475192033918574e-05\n",
            "Epoch  28 Batch  92 / 525  Training Loss  3.4736149245873094e-05\n",
            "Epoch  28 Batch  93 / 525  Training Loss  2.0513301933533512e-05\n",
            "Epoch  28 Batch  94 / 525  Training Loss  3.214077150914818e-05\n",
            "Epoch  28 Batch  95 / 525  Training Loss  3.434289828874171e-05\n",
            "Epoch  28 Batch  96 / 525  Training Loss  2.67959367192816e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  28 Batch  97 / 525  Training Loss  2.632286850712262e-05\n",
            "Epoch  28 Batch  98 / 525  Training Loss  3.2664294849382713e-05\n",
            "Epoch  28 Batch  99 / 525  Training Loss  2.005725764320232e-05\n",
            "Epoch  28 Batch  100 / 525  Training Loss  2.4437822503386997e-05\n",
            "Epoch  28 Batch  101 / 525  Training Loss  2.8197222491144203e-05\n",
            "Epoch  28 Batch  102 / 525  Training Loss  2.086123640765436e-05\n",
            "Epoch  28 Batch  103 / 525  Training Loss  2.9140301194274798e-05\n",
            "Epoch  28 Batch  104 / 525  Training Loss  2.5770475986064412e-05\n",
            "Epoch  28 Batch  105 / 525  Training Loss  3.123075657640584e-05\n",
            "Epoch  28 Batch  106 / 525  Training Loss  2.4626739104860462e-05\n",
            "Epoch  28 Batch  107 / 525  Training Loss  2.3477525246562436e-05\n",
            "Epoch  28 Batch  108 / 525  Training Loss  3.280119926785119e-05\n",
            "Epoch  28 Batch  109 / 525  Training Loss  3.829092383966781e-05\n",
            "Epoch  28 Batch  110 / 525  Training Loss  3.070453385589644e-05\n",
            "Epoch  28 Batch  111 / 525  Training Loss  3.874523827107623e-05\n",
            "Epoch  28 Batch  112 / 525  Training Loss  3.4978984331246465e-05\n",
            "Epoch  28 Batch  113 / 525  Training Loss  3.2973839552141726e-05\n",
            "Epoch  28 Batch  114 / 525  Training Loss  2.6702817194745876e-05\n",
            "Epoch  28 Batch  115 / 525  Training Loss  2.1217798348516226e-05\n",
            "Epoch  28 Batch  116 / 525  Training Loss  2.5417646611458622e-05\n",
            "Epoch  28 Batch  117 / 525  Training Loss  2.9215530958026648e-05\n",
            "Epoch  28 Batch  118 / 525  Training Loss  3.19907849188894e-05\n",
            "Epoch  28 Batch  119 / 525  Training Loss  2.5901777917169966e-05\n",
            "Epoch  28 Batch  120 / 525  Training Loss  3.1846491765463725e-05\n",
            "Epoch  28 Batch  121 / 525  Training Loss  2.0566491002682596e-05\n",
            "Epoch  28 Batch  122 / 525  Training Loss  2.399392906227149e-05\n",
            "Epoch  28 Batch  123 / 525  Training Loss  2.33185310207773e-05\n",
            "Epoch  28 Batch  124 / 525  Training Loss  2.4637571186758578e-05\n",
            "Epoch  28 Batch  125 / 525  Training Loss  1.819888348109089e-05\n",
            "Epoch  28 Batch  126 / 525  Training Loss  2.4617776944069192e-05\n",
            "Epoch  28 Batch  127 / 525  Training Loss  2.632340692798607e-05\n",
            "Epoch  28 Batch  128 / 525  Training Loss  2.6580852136248723e-05\n",
            "Epoch  28 Batch  129 / 525  Training Loss  2.490521910658572e-05\n",
            "Epoch  28 Batch  130 / 525  Training Loss  3.5844972444465384e-05\n",
            "Epoch  28 Batch  131 / 525  Training Loss  2.3310847609536722e-05\n",
            "Epoch  28 Batch  132 / 525  Training Loss  3.176125028403476e-05\n",
            "Epoch  28 Batch  133 / 525  Training Loss  2.809974284900818e-05\n",
            "Epoch  28 Batch  134 / 525  Training Loss  2.7893791411770508e-05\n",
            "Epoch  28 Batch  135 / 525  Training Loss  3.1762545404490083e-05\n",
            "Epoch  28 Batch  136 / 525  Training Loss  2.5180313969030976e-05\n",
            "Epoch  28 Batch  137 / 525  Training Loss  2.6758611056720838e-05\n",
            "Epoch  28 Batch  138 / 525  Training Loss  2.6644740501069464e-05\n",
            "Epoch  28 Batch  139 / 525  Training Loss  2.3545951989945024e-05\n",
            "Epoch  28 Batch  140 / 525  Training Loss  1.818449527490884e-05\n",
            "Epoch  28 Batch  141 / 525  Training Loss  2.2228732632356696e-05\n",
            "Epoch  28 Batch  142 / 525  Training Loss  2.4568358639953658e-05\n",
            "Epoch  28 Batch  143 / 525  Training Loss  2.108453554683365e-05\n",
            "Epoch  28 Batch  144 / 525  Training Loss  2.7377132937544957e-05\n",
            "Epoch  28 Batch  145 / 525  Training Loss  3.293744885013439e-05\n",
            "Epoch  28 Batch  146 / 525  Training Loss  2.34326198551571e-05\n",
            "Epoch  28 Batch  147 / 525  Training Loss  3.123604619759135e-05\n",
            "Epoch  28 Batch  148 / 525  Training Loss  2.7928312192671e-05\n",
            "Epoch  28 Batch  149 / 525  Training Loss  2.9985672881593928e-05\n",
            "Epoch  28 Batch  150 / 525  Training Loss  2.6219222490908578e-05\n",
            "Epoch  28 Batch  151 / 525  Training Loss  2.742934157140553e-05\n",
            "Epoch  28 Batch  152 / 525  Training Loss  2.0926758224959485e-05\n",
            "Epoch  28 Batch  153 / 525  Training Loss  3.256258423789404e-05\n",
            "Epoch  28 Batch  154 / 525  Training Loss  1.8065689801005647e-05\n",
            "Epoch  28 Batch  155 / 525  Training Loss  4.156865907134488e-05\n",
            "Epoch  28 Batch  156 / 525  Training Loss  2.8221746106282808e-05\n",
            "Epoch  28 Batch  157 / 525  Training Loss  2.411460991424974e-05\n",
            "Epoch  28 Batch  158 / 525  Training Loss  2.771123035927303e-05\n",
            "Epoch  28 Batch  159 / 525  Training Loss  2.590693111415021e-05\n",
            "Epoch  28 Batch  160 / 525  Training Loss  2.6666213670978323e-05\n",
            "Epoch  28 Batch  161 / 525  Training Loss  2.3150794731918722e-05\n",
            "Epoch  28 Batch  162 / 525  Training Loss  2.8422893592505716e-05\n",
            "Epoch  28 Batch  163 / 525  Training Loss  2.6042742319987155e-05\n",
            "Epoch  28 Batch  164 / 525  Training Loss  2.368690365983639e-05\n",
            "Epoch  28 Batch  165 / 525  Training Loss  2.423747537250165e-05\n",
            "Epoch  28 Batch  166 / 525  Training Loss  2.392779424553737e-05\n",
            "Epoch  28 Batch  167 / 525  Training Loss  2.2041494958102703e-05\n",
            "Epoch  28 Batch  168 / 525  Training Loss  3.3503773011034355e-05\n",
            "Epoch  28 Batch  169 / 525  Training Loss  2.1799112801090814e-05\n",
            "Epoch  28 Batch  170 / 525  Training Loss  3.2055915653472766e-05\n",
            "Epoch  28 Batch  171 / 525  Training Loss  3.509605448925868e-05\n",
            "Epoch  28 Batch  172 / 525  Training Loss  3.132051642751321e-05\n",
            "Epoch  28 Batch  173 / 525  Training Loss  3.441622538957745e-05\n",
            "Epoch  28 Batch  174 / 525  Training Loss  2.381908052484505e-05\n",
            "Epoch  28 Batch  175 / 525  Training Loss  3.288572042947635e-05\n",
            "Epoch  28 Batch  176 / 525  Training Loss  1.8472352167009376e-05\n",
            "Epoch  28 Batch  177 / 525  Training Loss  2.3064085326041095e-05\n",
            "Epoch  28 Batch  178 / 525  Training Loss  2.5514314984320663e-05\n",
            "Epoch  28 Batch  179 / 525  Training Loss  2.965704697999172e-05\n",
            "Epoch  28 Batch  180 / 525  Training Loss  1.5984775018296205e-05\n",
            "Epoch  28 Batch  181 / 525  Training Loss  1.368871562590357e-05\n",
            "Epoch  28 Batch  182 / 525  Training Loss  2.0610934370779432e-05\n",
            "Epoch  28 Batch  183 / 525  Training Loss  3.228535206289962e-05\n",
            "Epoch  28 Batch  184 / 525  Training Loss  2.8502534405561164e-05\n",
            "Epoch  28 Batch  185 / 525  Training Loss  2.1649788322974928e-05\n",
            "Epoch  28 Batch  186 / 525  Training Loss  2.1592946723103523e-05\n",
            "Epoch  28 Batch  187 / 525  Training Loss  2.670612229849212e-05\n",
            "Epoch  28 Batch  188 / 525  Training Loss  2.1138865122338757e-05\n",
            "Epoch  28 Batch  189 / 525  Training Loss  2.403182770649437e-05\n",
            "Epoch  28 Batch  190 / 525  Training Loss  2.356752156629227e-05\n",
            "Epoch  28 Batch  191 / 525  Training Loss  2.233904888271354e-05\n",
            "Epoch  28 Batch  192 / 525  Training Loss  2.9880611691623926e-05\n",
            "Epoch  28 Batch  193 / 525  Training Loss  2.424448393867351e-05\n",
            "Epoch  28 Batch  194 / 525  Training Loss  1.9414153939578682e-05\n",
            "Epoch  28 Batch  195 / 525  Training Loss  2.8407384888851084e-05\n",
            "Epoch  28 Batch  196 / 525  Training Loss  2.5845822165138088e-05\n",
            "Epoch  28 Batch  197 / 525  Training Loss  2.43194263020996e-05\n",
            "Epoch  28 Batch  198 / 525  Training Loss  1.9102826627204195e-05\n",
            "Epoch  28 Batch  199 / 525  Training Loss  1.9864812202285975e-05\n",
            "Epoch  28 Batch  200 / 525  Training Loss  2.8749433113262057e-05\n",
            "Epoch  28 Batch  201 / 525  Training Loss  2.28466833505081e-05\n",
            "Epoch  28 Batch  202 / 525  Training Loss  2.4417804525000975e-05\n",
            "Epoch  28 Batch  203 / 525  Training Loss  2.1950900190859102e-05\n",
            "Epoch  28 Batch  204 / 525  Training Loss  3.474717595963739e-05\n",
            "Epoch  28 Batch  205 / 525  Training Loss  2.9169674235163257e-05\n",
            "Epoch  28 Batch  206 / 525  Training Loss  2.7535317713045515e-05\n",
            "Epoch  28 Batch  207 / 525  Training Loss  2.2577134586754255e-05\n",
            "Epoch  28 Batch  208 / 525  Training Loss  2.3177726689027622e-05\n",
            "Epoch  28 Batch  209 / 525  Training Loss  2.7627149393083528e-05\n",
            "Epoch  28 Batch  210 / 525  Training Loss  2.3752909328322858e-05\n",
            "Epoch  28 Batch  211 / 525  Training Loss  3.182868385920301e-05\n",
            "Epoch  28 Batch  212 / 525  Training Loss  1.8631233615451492e-05\n",
            "Epoch  28 Batch  213 / 525  Training Loss  2.4184260837500915e-05\n",
            "Epoch  28 Batch  214 / 525  Training Loss  3.985330477007665e-05\n",
            "Epoch  28 Batch  215 / 525  Training Loss  2.2868316591484472e-05\n",
            "Epoch  28 Batch  216 / 525  Training Loss  2.58309009950608e-05\n",
            "Epoch  28 Batch  217 / 525  Training Loss  3.0063154554227367e-05\n",
            "Epoch  28 Batch  218 / 525  Training Loss  2.4706529075047e-05\n",
            "Epoch  28 Batch  219 / 525  Training Loss  2.3252969185705297e-05\n",
            "Epoch  28 Batch  220 / 525  Training Loss  3.0086597689660266e-05\n",
            "Epoch  28 Batch  221 / 525  Training Loss  1.907109617604874e-05\n",
            "Epoch  28 Batch  222 / 525  Training Loss  3.221946462872438e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  28 Batch  223 / 525  Training Loss  3.123076385236345e-05\n",
            "Epoch  28 Batch  224 / 525  Training Loss  2.6703073672251776e-05\n",
            "Epoch  28 Batch  225 / 525  Training Loss  3.075239510508254e-05\n",
            "Epoch  28 Batch  226 / 525  Training Loss  3.2927757274592295e-05\n",
            "Epoch  28 Batch  227 / 525  Training Loss  2.1333386030164547e-05\n",
            "Epoch  28 Batch  228 / 525  Training Loss  2.7817091904580593e-05\n",
            "Epoch  28 Batch  229 / 525  Training Loss  4.45956175099127e-05\n",
            "Epoch  28 Batch  230 / 525  Training Loss  2.5023449779837392e-05\n",
            "Epoch  28 Batch  231 / 525  Training Loss  3.45820517395623e-05\n",
            "Epoch  28 Batch  232 / 525  Training Loss  3.266466228524223e-05\n",
            "Epoch  28 Batch  233 / 525  Training Loss  2.7674168450175785e-05\n",
            "Epoch  28 Batch  234 / 525  Training Loss  3.048879443667829e-05\n",
            "Epoch  28 Batch  235 / 525  Training Loss  2.6434678147779778e-05\n",
            "Epoch  28 Batch  236 / 525  Training Loss  2.8473408747231588e-05\n",
            "Epoch  28 Batch  237 / 525  Training Loss  2.429549567750655e-05\n",
            "Epoch  28 Batch  238 / 525  Training Loss  2.8935051886946894e-05\n",
            "Epoch  28 Batch  239 / 525  Training Loss  3.106582153122872e-05\n",
            "Epoch  28 Batch  240 / 525  Training Loss  2.297386345162522e-05\n",
            "Epoch  28 Batch  241 / 525  Training Loss  2.620093619043473e-05\n",
            "Epoch  28 Batch  242 / 525  Training Loss  3.862958328681998e-05\n",
            "Epoch  28 Batch  243 / 525  Training Loss  3.1380692234961316e-05\n",
            "Epoch  28 Batch  244 / 525  Training Loss  3.131472840323113e-05\n",
            "Epoch  28 Batch  245 / 525  Training Loss  2.5411845854250714e-05\n",
            "Epoch  28 Batch  246 / 525  Training Loss  2.851043791451957e-05\n",
            "Epoch  28 Batch  247 / 525  Training Loss  2.38595821429044e-05\n",
            "Epoch  28 Batch  248 / 525  Training Loss  3.437200575717725e-05\n",
            "Epoch  28 Batch  249 / 525  Training Loss  2.3349737602984533e-05\n",
            "Epoch  28 Batch  250 / 525  Training Loss  3.108949385932647e-05\n",
            "Epoch  28 Batch  251 / 525  Training Loss  2.144480458809994e-05\n",
            "Epoch  28 Batch  252 / 525  Training Loss  2.8550017304951325e-05\n",
            "Epoch  28 Batch  253 / 525  Training Loss  2.7868478355230764e-05\n",
            "Epoch  28 Batch  254 / 525  Training Loss  2.3168420739239082e-05\n",
            "Epoch  28 Batch  255 / 525  Training Loss  3.3768974390113726e-05\n",
            "Epoch  28 Batch  256 / 525  Training Loss  2.5831750463112257e-05\n",
            "Epoch  28 Batch  257 / 525  Training Loss  1.8429218471283093e-05\n",
            "Epoch  28 Batch  258 / 525  Training Loss  3.433972960920073e-05\n",
            "Epoch  28 Batch  259 / 525  Training Loss  3.3595475542824715e-05\n",
            "Epoch  28 Batch  260 / 525  Training Loss  2.3758417228236794e-05\n",
            "Epoch  28 Batch  261 / 525  Training Loss  3.91096509702038e-05\n",
            "Epoch  28 Batch  262 / 525  Training Loss  3.2878833735594526e-05\n",
            "Epoch  28 Batch  263 / 525  Training Loss  3.358729009050876e-05\n",
            "Epoch  28 Batch  264 / 525  Training Loss  1.6244286598521285e-05\n",
            "Epoch  28 Batch  265 / 525  Training Loss  2.768072226899676e-05\n",
            "Epoch  28 Batch  266 / 525  Training Loss  2.8268037567613646e-05\n",
            "Epoch  28 Batch  267 / 525  Training Loss  2.692474481591489e-05\n",
            "Epoch  28 Batch  268 / 525  Training Loss  2.9293349143699743e-05\n",
            "Epoch  28 Batch  269 / 525  Training Loss  2.8091928470530547e-05\n",
            "Epoch  28 Batch  270 / 525  Training Loss  3.696881321957335e-05\n",
            "Epoch  28 Batch  271 / 525  Training Loss  1.8480412109056488e-05\n",
            "Epoch  28 Batch  272 / 525  Training Loss  2.421446333755739e-05\n",
            "Epoch  28 Batch  273 / 525  Training Loss  2.7321953893988393e-05\n",
            "Epoch  28 Batch  274 / 525  Training Loss  2.792914710880723e-05\n",
            "Epoch  28 Batch  275 / 525  Training Loss  3.078961162827909e-05\n",
            "Epoch  28 Batch  276 / 525  Training Loss  3.4456585126463324e-05\n",
            "Epoch  28 Batch  277 / 525  Training Loss  2.1515834305319004e-05\n",
            "Epoch  28 Batch  278 / 525  Training Loss  1.997892468352802e-05\n",
            "Epoch  28 Batch  279 / 525  Training Loss  2.25151889026165e-05\n",
            "Epoch  28 Batch  280 / 525  Training Loss  3.415574974496849e-05\n",
            "Epoch  28 Batch  281 / 525  Training Loss  2.3057134967530146e-05\n",
            "Epoch  28 Batch  282 / 525  Training Loss  2.332898657186888e-05\n",
            "Epoch  28 Batch  283 / 525  Training Loss  3.2722833566367626e-05\n",
            "Epoch  28 Batch  284 / 525  Training Loss  2.5265433578169905e-05\n",
            "Epoch  28 Batch  285 / 525  Training Loss  2.4378960006288253e-05\n",
            "Epoch  28 Batch  286 / 525  Training Loss  2.649042653501965e-05\n",
            "Epoch  28 Batch  287 / 525  Training Loss  2.19671092054341e-05\n",
            "Epoch  28 Batch  288 / 525  Training Loss  2.4072553060250357e-05\n",
            "Epoch  28 Batch  289 / 525  Training Loss  3.291803295724094e-05\n",
            "Epoch  28 Batch  290 / 525  Training Loss  1.9937881006626412e-05\n",
            "Epoch  28 Batch  291 / 525  Training Loss  3.798381658270955e-05\n",
            "Epoch  28 Batch  292 / 525  Training Loss  3.517789446050301e-05\n",
            "Epoch  28 Batch  293 / 525  Training Loss  3.4080410841852427e-05\n",
            "Epoch  28 Batch  294 / 525  Training Loss  2.4707962438696995e-05\n",
            "Epoch  28 Batch  295 / 525  Training Loss  1.8123697373084724e-05\n",
            "Epoch  28 Batch  296 / 525  Training Loss  2.6620080461725593e-05\n",
            "Epoch  28 Batch  297 / 525  Training Loss  2.5904839276336133e-05\n",
            "Epoch  28 Batch  298 / 525  Training Loss  2.5666658984846435e-05\n",
            "Epoch  28 Batch  299 / 525  Training Loss  2.07423963729525e-05\n",
            "Epoch  28 Batch  300 / 525  Training Loss  2.7589947421802208e-05\n",
            "Epoch  28 Batch  301 / 525  Training Loss  2.8655704227276146e-05\n",
            "Epoch  28 Batch  302 / 525  Training Loss  2.3356511519523337e-05\n",
            "Epoch  28 Batch  303 / 525  Training Loss  2.587393464636989e-05\n",
            "Epoch  28 Batch  304 / 525  Training Loss  3.965314681408927e-05\n",
            "Epoch  28 Batch  305 / 525  Training Loss  3.2503128750249743e-05\n",
            "Epoch  28 Batch  306 / 525  Training Loss  3.447143899393268e-05\n",
            "Epoch  28 Batch  307 / 525  Training Loss  2.339981983823236e-05\n",
            "Epoch  28 Batch  308 / 525  Training Loss  2.898322964028921e-05\n",
            "Epoch  28 Batch  309 / 525  Training Loss  3.5210126952733845e-05\n",
            "Epoch  28 Batch  310 / 525  Training Loss  1.7570551790413447e-05\n",
            "Epoch  28 Batch  311 / 525  Training Loss  3.082936382270418e-05\n",
            "Epoch  28 Batch  312 / 525  Training Loss  1.723535569908563e-05\n",
            "Epoch  28 Batch  313 / 525  Training Loss  2.5633420591475442e-05\n",
            "Epoch  28 Batch  314 / 525  Training Loss  1.7590196875971742e-05\n",
            "Epoch  28 Batch  315 / 525  Training Loss  1.9974935639766045e-05\n",
            "Epoch  28 Batch  316 / 525  Training Loss  2.9269624064909294e-05\n",
            "Epoch  28 Batch  317 / 525  Training Loss  2.7709098503692076e-05\n",
            "Epoch  28 Batch  318 / 525  Training Loss  3.0116108973743394e-05\n",
            "Epoch  28 Batch  319 / 525  Training Loss  2.7288615456200205e-05\n",
            "Epoch  28 Batch  320 / 525  Training Loss  2.34486760746222e-05\n",
            "Epoch  28 Batch  321 / 525  Training Loss  2.2726024326402694e-05\n",
            "Epoch  28 Batch  322 / 525  Training Loss  3.5085849958704785e-05\n",
            "Epoch  28 Batch  323 / 525  Training Loss  3.0378369046957232e-05\n",
            "Epoch  28 Batch  324 / 525  Training Loss  3.1129147828323767e-05\n",
            "Epoch  28 Batch  325 / 525  Training Loss  2.806648626574315e-05\n",
            "Epoch  28 Batch  326 / 525  Training Loss  3.0095465263002552e-05\n",
            "Epoch  28 Batch  327 / 525  Training Loss  2.9823422664776444e-05\n",
            "Epoch  28 Batch  328 / 525  Training Loss  2.4772502001724206e-05\n",
            "Epoch  28 Batch  329 / 525  Training Loss  1.711602453724481e-05\n",
            "Epoch  28 Batch  330 / 525  Training Loss  2.5872792321024463e-05\n",
            "Epoch  28 Batch  331 / 525  Training Loss  2.958139157271944e-05\n",
            "Epoch  28 Batch  332 / 525  Training Loss  2.3854288883740082e-05\n",
            "Epoch  28 Batch  333 / 525  Training Loss  1.921950934047345e-05\n",
            "Epoch  28 Batch  334 / 525  Training Loss  2.5934645236702636e-05\n",
            "Epoch  28 Batch  335 / 525  Training Loss  2.013402081502136e-05\n",
            "Epoch  28 Batch  336 / 525  Training Loss  3.1762356229592115e-05\n",
            "Epoch  28 Batch  337 / 525  Training Loss  3.9075810491340235e-05\n",
            "Epoch  28 Batch  338 / 525  Training Loss  3.679283690871671e-05\n",
            "Epoch  28 Batch  339 / 525  Training Loss  2.6674451873986982e-05\n",
            "Epoch  28 Batch  340 / 525  Training Loss  2.2742106011719443e-05\n",
            "Epoch  28 Batch  341 / 525  Training Loss  2.4130469682859257e-05\n",
            "Epoch  28 Batch  342 / 525  Training Loss  4.3173611629754305e-05\n",
            "Epoch  28 Batch  343 / 525  Training Loss  2.524763476685621e-05\n",
            "Epoch  28 Batch  344 / 525  Training Loss  3.319363895570859e-05\n",
            "Epoch  28 Batch  345 / 525  Training Loss  3.167467002640478e-05\n",
            "Epoch  28 Batch  346 / 525  Training Loss  2.3421307560056448e-05\n",
            "Epoch  28 Batch  347 / 525  Training Loss  3.5796874726656824e-05\n",
            "Epoch  28 Batch  348 / 525  Training Loss  2.7163583581568673e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  28 Batch  349 / 525  Training Loss  2.478009628248401e-05\n",
            "Epoch  28 Batch  350 / 525  Training Loss  2.3697248252574354e-05\n",
            "Epoch  28 Batch  351 / 525  Training Loss  2.8951733838766813e-05\n",
            "Epoch  28 Batch  352 / 525  Training Loss  2.8048360036336817e-05\n",
            "Epoch  28 Batch  353 / 525  Training Loss  3.900121373590082e-05\n",
            "Epoch  28 Batch  354 / 525  Training Loss  2.0891329768346623e-05\n",
            "Epoch  28 Batch  355 / 525  Training Loss  2.2129417629912496e-05\n",
            "Epoch  28 Batch  356 / 525  Training Loss  2.688846507226117e-05\n",
            "Epoch  28 Batch  357 / 525  Training Loss  2.5546658434905112e-05\n",
            "Epoch  28 Batch  358 / 525  Training Loss  3.2952888432191685e-05\n",
            "Epoch  28 Batch  359 / 525  Training Loss  3.3515381801407784e-05\n",
            "Epoch  28 Batch  360 / 525  Training Loss  2.5299465050920844e-05\n",
            "Epoch  28 Batch  361 / 525  Training Loss  2.0281124307075515e-05\n",
            "Epoch  28 Batch  362 / 525  Training Loss  2.3302645786316134e-05\n",
            "Epoch  28 Batch  363 / 525  Training Loss  2.732356733758934e-05\n",
            "Epoch  28 Batch  364 / 525  Training Loss  2.4586963263573125e-05\n",
            "Epoch  28 Batch  365 / 525  Training Loss  1.964141301868949e-05\n",
            "Epoch  28 Batch  366 / 525  Training Loss  3.1306564778788015e-05\n",
            "Epoch  28 Batch  367 / 525  Training Loss  2.7395439246902242e-05\n",
            "Epoch  28 Batch  368 / 525  Training Loss  3.208341513527557e-05\n",
            "Epoch  28 Batch  369 / 525  Training Loss  2.1963522158330306e-05\n",
            "Epoch  28 Batch  370 / 525  Training Loss  1.8961927708005533e-05\n",
            "Epoch  28 Batch  371 / 525  Training Loss  3.3991698728641495e-05\n",
            "Epoch  28 Batch  372 / 525  Training Loss  2.56163675658172e-05\n",
            "Epoch  28 Batch  373 / 525  Training Loss  2.923706779256463e-05\n",
            "Epoch  28 Batch  374 / 525  Training Loss  2.364173997193575e-05\n",
            "Epoch  28 Batch  375 / 525  Training Loss  2.4015071176108904e-05\n",
            "Epoch  28 Batch  376 / 525  Training Loss  2.64061673078686e-05\n",
            "Epoch  28 Batch  377 / 525  Training Loss  2.9863722375012003e-05\n",
            "Epoch  28 Batch  378 / 525  Training Loss  2.2861455363454297e-05\n",
            "Epoch  28 Batch  379 / 525  Training Loss  2.61196109931916e-05\n",
            "Epoch  28 Batch  380 / 525  Training Loss  3.6395467759575695e-05\n",
            "Epoch  28 Batch  381 / 525  Training Loss  2.8877751901745796e-05\n",
            "Epoch  28 Batch  382 / 525  Training Loss  3.063042458961718e-05\n",
            "Epoch  28 Batch  383 / 525  Training Loss  2.567096998973284e-05\n",
            "Epoch  28 Batch  384 / 525  Training Loss  2.5828121579252183e-05\n",
            "Epoch  28 Batch  385 / 525  Training Loss  2.8174425096949562e-05\n",
            "Epoch  28 Batch  386 / 525  Training Loss  1.8884358723880723e-05\n",
            "Epoch  28 Batch  387 / 525  Training Loss  2.7005520678358153e-05\n",
            "Epoch  28 Batch  388 / 525  Training Loss  2.6969788450514898e-05\n",
            "Epoch  28 Batch  389 / 525  Training Loss  1.582238110131584e-05\n",
            "Epoch  28 Batch  390 / 525  Training Loss  2.086789936583955e-05\n",
            "Epoch  28 Batch  391 / 525  Training Loss  2.5183258912875317e-05\n",
            "Epoch  28 Batch  392 / 525  Training Loss  2.9757007723674178e-05\n",
            "Epoch  28 Batch  393 / 525  Training Loss  3.427358387853019e-05\n",
            "Epoch  28 Batch  394 / 525  Training Loss  2.5330435164505616e-05\n",
            "Epoch  28 Batch  395 / 525  Training Loss  3.292024484835565e-05\n",
            "Epoch  28 Batch  396 / 525  Training Loss  2.3051969037624076e-05\n",
            "Epoch  28 Batch  397 / 525  Training Loss  2.2485302906716242e-05\n",
            "Epoch  28 Batch  398 / 525  Training Loss  2.7247899197391234e-05\n",
            "Epoch  28 Batch  399 / 525  Training Loss  2.5574496248736978e-05\n",
            "Epoch  28 Batch  400 / 525  Training Loss  2.6462304958840832e-05\n",
            "Epoch  28 Batch  401 / 525  Training Loss  2.361012047913391e-05\n",
            "Epoch  28 Batch  402 / 525  Training Loss  3.607802136684768e-05\n",
            "Epoch  28 Batch  403 / 525  Training Loss  2.3829903511796147e-05\n",
            "Epoch  28 Batch  404 / 525  Training Loss  1.5135390640352853e-05\n",
            "Epoch  28 Batch  405 / 525  Training Loss  1.9652206901810132e-05\n",
            "Epoch  28 Batch  406 / 525  Training Loss  4.156427530688234e-05\n",
            "Epoch  28 Batch  407 / 525  Training Loss  1.91644394362811e-05\n",
            "Epoch  28 Batch  408 / 525  Training Loss  2.3126089217839763e-05\n",
            "Epoch  28 Batch  409 / 525  Training Loss  2.512876017135568e-05\n",
            "Epoch  28 Batch  410 / 525  Training Loss  2.1903582819504663e-05\n",
            "Epoch  28 Batch  411 / 525  Training Loss  1.892129694169853e-05\n",
            "Epoch  28 Batch  412 / 525  Training Loss  2.8779209969798103e-05\n",
            "Epoch  28 Batch  413 / 525  Training Loss  2.2364596588886343e-05\n",
            "Epoch  28 Batch  414 / 525  Training Loss  2.9249340514070354e-05\n",
            "Epoch  28 Batch  415 / 525  Training Loss  2.231461985502392e-05\n",
            "Epoch  28 Batch  416 / 525  Training Loss  2.2806434571975842e-05\n",
            "Epoch  28 Batch  417 / 525  Training Loss  2.1356383513193578e-05\n",
            "Epoch  28 Batch  418 / 525  Training Loss  2.580232103355229e-05\n",
            "Epoch  28 Batch  419 / 525  Training Loss  2.5497929527773522e-05\n",
            "Epoch  28 Batch  420 / 525  Training Loss  1.6496396710863337e-05\n",
            "Epoch  28 Batch  421 / 525  Training Loss  2.786689219647087e-05\n",
            "Epoch  28 Batch  422 / 525  Training Loss  3.479967563180253e-05\n",
            "Epoch  28 Batch  423 / 525  Training Loss  3.457006096141413e-05\n",
            "Epoch  28 Batch  424 / 525  Training Loss  2.992476220242679e-05\n",
            "Epoch  28 Batch  425 / 525  Training Loss  3.28693822666537e-05\n",
            "Epoch  28 Batch  426 / 525  Training Loss  4.2541581933619455e-05\n",
            "Epoch  28 Batch  427 / 525  Training Loss  4.2386105633340776e-05\n",
            "Epoch  28 Batch  428 / 525  Training Loss  1.9130657165078446e-05\n",
            "Epoch  28 Batch  429 / 525  Training Loss  3.14286990032997e-05\n",
            "Epoch  28 Batch  430 / 525  Training Loss  3.872744491673075e-05\n",
            "Epoch  28 Batch  431 / 525  Training Loss  2.3664555556024425e-05\n",
            "Epoch  28 Batch  432 / 525  Training Loss  2.5566627300577238e-05\n",
            "Epoch  28 Batch  433 / 525  Training Loss  3.427547198953107e-05\n",
            "Epoch  28 Batch  434 / 525  Training Loss  2.638924888742622e-05\n",
            "Epoch  28 Batch  435 / 525  Training Loss  2.7409067115513608e-05\n",
            "Epoch  28 Batch  436 / 525  Training Loss  2.6142273782170378e-05\n",
            "Epoch  28 Batch  437 / 525  Training Loss  2.9818304028594866e-05\n",
            "Epoch  28 Batch  438 / 525  Training Loss  2.1708036001655273e-05\n",
            "Epoch  28 Batch  439 / 525  Training Loss  2.4548900910303928e-05\n",
            "Epoch  28 Batch  440 / 525  Training Loss  1.2675547623075545e-05\n",
            "Epoch  28 Batch  441 / 525  Training Loss  3.194710734533146e-05\n",
            "Epoch  28 Batch  442 / 525  Training Loss  1.823238198994659e-05\n",
            "Epoch  28 Batch  443 / 525  Training Loss  3.809734698734246e-05\n",
            "Epoch  28 Batch  444 / 525  Training Loss  2.5371464289491996e-05\n",
            "Epoch  28 Batch  445 / 525  Training Loss  2.5283608920290135e-05\n",
            "Epoch  28 Batch  446 / 525  Training Loss  2.255220533697866e-05\n",
            "Epoch  28 Batch  447 / 525  Training Loss  3.0634539143648e-05\n",
            "Epoch  28 Batch  448 / 525  Training Loss  2.028936069109477e-05\n",
            "Epoch  28 Batch  449 / 525  Training Loss  2.3629005227121525e-05\n",
            "Epoch  28 Batch  450 / 525  Training Loss  2.3824377421988174e-05\n",
            "Epoch  28 Batch  451 / 525  Training Loss  2.5156239644275047e-05\n",
            "Epoch  28 Batch  452 / 525  Training Loss  2.489543658157345e-05\n",
            "Epoch  28 Batch  453 / 525  Training Loss  2.5180974262184463e-05\n",
            "Epoch  28 Batch  454 / 525  Training Loss  3.1815732654649764e-05\n",
            "Epoch  28 Batch  455 / 525  Training Loss  2.8011296308250166e-05\n",
            "Epoch  28 Batch  456 / 525  Training Loss  2.4470940843457356e-05\n",
            "Epoch  28 Batch  457 / 525  Training Loss  2.2954931409913115e-05\n",
            "Epoch  28 Batch  458 / 525  Training Loss  2.6411868020659313e-05\n",
            "Epoch  28 Batch  459 / 525  Training Loss  2.8466969524743035e-05\n",
            "Epoch  28 Batch  460 / 525  Training Loss  2.415259586996399e-05\n",
            "Epoch  28 Batch  461 / 525  Training Loss  2.3033873731037602e-05\n",
            "Epoch  28 Batch  462 / 525  Training Loss  2.4223492800956592e-05\n",
            "Epoch  28 Batch  463 / 525  Training Loss  2.4499098799424246e-05\n",
            "Epoch  28 Batch  464 / 525  Training Loss  3.325471334392205e-05\n",
            "Epoch  28 Batch  465 / 525  Training Loss  3.163124347338453e-05\n",
            "Epoch  28 Batch  466 / 525  Training Loss  2.1716137780458666e-05\n",
            "Epoch  28 Batch  467 / 525  Training Loss  3.493663825793192e-05\n",
            "Epoch  28 Batch  468 / 525  Training Loss  2.1708498024963774e-05\n",
            "Epoch  28 Batch  469 / 525  Training Loss  2.540907371439971e-05\n",
            "Epoch  28 Batch  470 / 525  Training Loss  2.9005203032284044e-05\n",
            "Epoch  28 Batch  471 / 525  Training Loss  3.487067442620173e-05\n",
            "Epoch  28 Batch  472 / 525  Training Loss  2.0367227989481762e-05\n",
            "Epoch  28 Batch  473 / 525  Training Loss  3.2388568797614425e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  28 Batch  474 / 525  Training Loss  2.6319201424485072e-05\n",
            "Epoch  28 Batch  475 / 525  Training Loss  2.4300377845065668e-05\n",
            "Epoch  28 Batch  476 / 525  Training Loss  2.9401015126495622e-05\n",
            "Epoch  28 Batch  477 / 525  Training Loss  2.091630631184671e-05\n",
            "Epoch  28 Batch  478 / 525  Training Loss  2.8447941076592542e-05\n",
            "Epoch  28 Batch  479 / 525  Training Loss  4.176227594143711e-05\n",
            "Epoch  28 Batch  480 / 525  Training Loss  1.9833489204756916e-05\n",
            "Epoch  28 Batch  481 / 525  Training Loss  3.385882155271247e-05\n",
            "Epoch  28 Batch  482 / 525  Training Loss  2.3672790121054277e-05\n",
            "Epoch  28 Batch  483 / 525  Training Loss  2.756355024757795e-05\n",
            "Epoch  28 Batch  484 / 525  Training Loss  2.2771253497921862e-05\n",
            "Epoch  28 Batch  485 / 525  Training Loss  2.9389684641500935e-05\n",
            "Epoch  28 Batch  486 / 525  Training Loss  2.50603370659519e-05\n",
            "Epoch  28 Batch  487 / 525  Training Loss  2.541746653150767e-05\n",
            "Epoch  28 Batch  488 / 525  Training Loss  2.9285245545906946e-05\n",
            "Epoch  28 Batch  489 / 525  Training Loss  2.7011195925297216e-05\n",
            "Epoch  28 Batch  490 / 525  Training Loss  3.621750511229038e-05\n",
            "Epoch  28 Batch  491 / 525  Training Loss  3.340258990647271e-05\n",
            "Epoch  28 Batch  492 / 525  Training Loss  2.6182480723946355e-05\n",
            "Epoch  28 Batch  493 / 525  Training Loss  3.833909431705251e-05\n",
            "Epoch  28 Batch  494 / 525  Training Loss  2.2660184185951948e-05\n",
            "Epoch  28 Batch  495 / 525  Training Loss  2.754253728198819e-05\n",
            "Epoch  28 Batch  496 / 525  Training Loss  3.312470289529301e-05\n",
            "Epoch  28 Batch  497 / 525  Training Loss  3.289504093118012e-05\n",
            "Epoch  28 Batch  498 / 525  Training Loss  2.972446964122355e-05\n",
            "Epoch  28 Batch  499 / 525  Training Loss  2.1769810700789094e-05\n",
            "Epoch  28 Batch  500 / 525  Training Loss  2.9476723284460604e-05\n",
            "Epoch  28 Batch  501 / 525  Training Loss  2.1006770111853257e-05\n",
            "Epoch  28 Batch  502 / 525  Training Loss  3.1989042327040806e-05\n",
            "Epoch  28 Batch  503 / 525  Training Loss  3.661069058580324e-05\n",
            "Epoch  28 Batch  504 / 525  Training Loss  3.8286052586045116e-05\n",
            "Epoch  28 Batch  505 / 525  Training Loss  2.7066809707321227e-05\n",
            "Epoch  28 Batch  506 / 525  Training Loss  3.7248781154630706e-05\n",
            "Epoch  28 Batch  507 / 525  Training Loss  2.642558320076205e-05\n",
            "Epoch  28 Batch  508 / 525  Training Loss  2.5372079107910395e-05\n",
            "Epoch  28 Batch  509 / 525  Training Loss  2.7731217414839193e-05\n",
            "Epoch  28 Batch  510 / 525  Training Loss  3.083984847762622e-05\n",
            "Epoch  28 Batch  511 / 525  Training Loss  2.997306910401676e-05\n",
            "Epoch  28 Batch  512 / 525  Training Loss  2.8818443752243184e-05\n",
            "Epoch  28 Batch  513 / 525  Training Loss  1.4639075743616559e-05\n",
            "Epoch  28 Batch  514 / 525  Training Loss  3.1397354177897796e-05\n",
            "Epoch  28 Batch  515 / 525  Training Loss  2.0272278561606072e-05\n",
            "Epoch  28 Batch  516 / 525  Training Loss  2.6830204660655e-05\n",
            "Epoch  28 Batch  517 / 525  Training Loss  2.5697663659229875e-05\n",
            "Epoch  28 Batch  518 / 525  Training Loss  3.0238810722948983e-05\n",
            "Epoch  28 Batch  519 / 525  Training Loss  1.7850044969236478e-05\n",
            "Epoch  28 Batch  520 / 525  Training Loss  2.1356421711971052e-05\n",
            "Epoch  28 Batch  521 / 525  Training Loss  2.6035631890408695e-05\n",
            "Epoch  28 Batch  522 / 525  Training Loss  2.572749144746922e-05\n",
            "Epoch  28 Batch  523 / 525  Training Loss  2.6172583602601662e-05\n",
            "Epoch  28 Batch  524 / 525  Training Loss  2.6126581360585988e-05\n",
            "  29    |    -    |   0.000027   |   64.08  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 29\n",
            "Epoch  29 Batch  0 / 525  Training Loss  2.8224263587617315e-05\n",
            "Epoch  29 Batch  1 / 525  Training Loss  1.6854837667779066e-05\n",
            "Epoch  29 Batch  2 / 525  Training Loss  2.1799402020405978e-05\n",
            "Epoch  29 Batch  3 / 525  Training Loss  1.9542327208910137e-05\n",
            "Epoch  29 Batch  4 / 525  Training Loss  2.220326496171765e-05\n",
            "Epoch  29 Batch  5 / 525  Training Loss  2.1974377887090668e-05\n",
            "Epoch  29 Batch  6 / 525  Training Loss  1.9291279386379756e-05\n",
            "Epoch  29 Batch  7 / 525  Training Loss  2.0931447579641826e-05\n",
            "Epoch  29 Batch  8 / 525  Training Loss  2.2505648303194903e-05\n",
            "Epoch  29 Batch  9 / 525  Training Loss  2.1962890969007276e-05\n",
            "Epoch  29 Batch  10 / 525  Training Loss  2.3800133931217715e-05\n",
            "Epoch  29 Batch  11 / 525  Training Loss  2.841126661223825e-05\n",
            "Epoch  29 Batch  12 / 525  Training Loss  2.099672201438807e-05\n",
            "Epoch  29 Batch  13 / 525  Training Loss  1.724978028505575e-05\n",
            "Epoch  29 Batch  14 / 525  Training Loss  2.0958264940418303e-05\n",
            "Epoch  29 Batch  15 / 525  Training Loss  3.175387973897159e-05\n",
            "Epoch  29 Batch  16 / 525  Training Loss  2.1530897356569767e-05\n",
            "Epoch  29 Batch  17 / 525  Training Loss  3.2569376344326884e-05\n",
            "Epoch  29 Batch  18 / 525  Training Loss  2.8474460123106837e-05\n",
            "Epoch  29 Batch  19 / 525  Training Loss  2.7068910640082322e-05\n",
            "Epoch  29 Batch  20 / 525  Training Loss  3.141898923786357e-05\n",
            "Epoch  29 Batch  21 / 525  Training Loss  2.0689583834609948e-05\n",
            "Epoch  29 Batch  22 / 525  Training Loss  3.317218943266198e-05\n",
            "Epoch  29 Batch  23 / 525  Training Loss  2.6963918571709655e-05\n",
            "Epoch  29 Batch  24 / 525  Training Loss  2.824220064212568e-05\n",
            "Epoch  29 Batch  25 / 525  Training Loss  2.2806203560321592e-05\n",
            "Epoch  29 Batch  26 / 525  Training Loss  3.2480435038451105e-05\n",
            "Epoch  29 Batch  27 / 525  Training Loss  2.9808865292579867e-05\n",
            "Epoch  29 Batch  28 / 525  Training Loss  3.1256866350304335e-05\n",
            "Epoch  29 Batch  29 / 525  Training Loss  2.7769265216193162e-05\n",
            "Epoch  29 Batch  30 / 525  Training Loss  2.3477274226024747e-05\n",
            "Epoch  29 Batch  31 / 525  Training Loss  3.624760574894026e-05\n",
            "Epoch  29 Batch  32 / 525  Training Loss  3.015921356563922e-05\n",
            "Epoch  29 Batch  33 / 525  Training Loss  2.270544064231217e-05\n",
            "Epoch  29 Batch  34 / 525  Training Loss  2.26184019993525e-05\n",
            "Epoch  29 Batch  35 / 525  Training Loss  2.974305607494898e-05\n",
            "Epoch  29 Batch  36 / 525  Training Loss  1.951884405571036e-05\n",
            "Epoch  29 Batch  37 / 525  Training Loss  2.1339561499189585e-05\n",
            "Epoch  29 Batch  38 / 525  Training Loss  2.2560789147973992e-05\n",
            "Epoch  29 Batch  39 / 525  Training Loss  2.7162916012457572e-05\n",
            "Epoch  29 Batch  40 / 525  Training Loss  1.6272957509499975e-05\n",
            "Epoch  29 Batch  41 / 525  Training Loss  2.2494115910376422e-05\n",
            "Epoch  29 Batch  42 / 525  Training Loss  2.1648036636179313e-05\n",
            "Epoch  29 Batch  43 / 525  Training Loss  3.098569141002372e-05\n",
            "Epoch  29 Batch  44 / 525  Training Loss  3.347784877405502e-05\n",
            "Epoch  29 Batch  45 / 525  Training Loss  2.2453867131844163e-05\n",
            "Epoch  29 Batch  46 / 525  Training Loss  3.138013562420383e-05\n",
            "Epoch  29 Batch  47 / 525  Training Loss  2.2691023332299665e-05\n",
            "Epoch  29 Batch  48 / 525  Training Loss  3.1282947020372376e-05\n",
            "Epoch  29 Batch  49 / 525  Training Loss  2.51373894570861e-05\n",
            "Epoch  29 Batch  50 / 525  Training Loss  3.666832344606519e-05\n",
            "Epoch  29 Batch  51 / 525  Training Loss  2.177228088839911e-05\n",
            "Epoch  29 Batch  52 / 525  Training Loss  2.504369695088826e-05\n",
            "Epoch  29 Batch  53 / 525  Training Loss  2.1734336769441143e-05\n",
            "Epoch  29 Batch  54 / 525  Training Loss  2.375443909841124e-05\n",
            "Epoch  29 Batch  55 / 525  Training Loss  3.226751505280845e-05\n",
            "Epoch  29 Batch  56 / 525  Training Loss  2.3308062736759894e-05\n",
            "Epoch  29 Batch  57 / 525  Training Loss  3.425850081839599e-05\n",
            "Epoch  29 Batch  58 / 525  Training Loss  3.041618583665695e-05\n",
            "Epoch  29 Batch  59 / 525  Training Loss  3.1648225558456033e-05\n",
            "Epoch  29 Batch  60 / 525  Training Loss  2.0242419850546867e-05\n",
            "Epoch  29 Batch  61 / 525  Training Loss  2.5658227968961e-05\n",
            "Epoch  29 Batch  62 / 525  Training Loss  2.906491863541305e-05\n",
            "Epoch  29 Batch  63 / 525  Training Loss  3.052296960959211e-05\n",
            "Epoch  29 Batch  64 / 525  Training Loss  2.3347194655798376e-05\n",
            "Epoch  29 Batch  65 / 525  Training Loss  2.040044819295872e-05\n",
            "Epoch  29 Batch  66 / 525  Training Loss  2.636956560309045e-05\n",
            "Epoch  29 Batch  67 / 525  Training Loss  2.1044854292995296e-05\n",
            "Epoch  29 Batch  68 / 525  Training Loss  2.495877561159432e-05\n",
            "Epoch  29 Batch  69 / 525  Training Loss  3.974289575126022e-05\n",
            "Epoch  29 Batch  70 / 525  Training Loss  2.8724771254928783e-05\n",
            "Epoch  29 Batch  71 / 525  Training Loss  2.1617073798552155e-05\n",
            "Epoch  29 Batch  72 / 525  Training Loss  2.9235070542199537e-05\n",
            "Epoch  29 Batch  73 / 525  Training Loss  1.8931128579424694e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  29 Batch  74 / 525  Training Loss  1.6443142158095725e-05\n",
            "Epoch  29 Batch  75 / 525  Training Loss  2.9397764592431486e-05\n",
            "Epoch  29 Batch  76 / 525  Training Loss  2.467978629283607e-05\n",
            "Epoch  29 Batch  77 / 525  Training Loss  2.9561817427747883e-05\n",
            "Epoch  29 Batch  78 / 525  Training Loss  3.6378794902702793e-05\n",
            "Epoch  29 Batch  79 / 525  Training Loss  2.7106249035568908e-05\n",
            "Epoch  29 Batch  80 / 525  Training Loss  2.811893318721559e-05\n",
            "Epoch  29 Batch  81 / 525  Training Loss  2.0078066881978884e-05\n",
            "Epoch  29 Batch  82 / 525  Training Loss  2.7096593839814886e-05\n",
            "Epoch  29 Batch  83 / 525  Training Loss  1.9289433112135157e-05\n",
            "Epoch  29 Batch  84 / 525  Training Loss  1.6896334273042157e-05\n",
            "Epoch  29 Batch  85 / 525  Training Loss  2.7999592930427752e-05\n",
            "Epoch  29 Batch  86 / 525  Training Loss  3.6301898944657296e-05\n",
            "Epoch  29 Batch  87 / 525  Training Loss  2.130776920239441e-05\n",
            "Epoch  29 Batch  88 / 525  Training Loss  3.892999666277319e-05\n",
            "Epoch  29 Batch  89 / 525  Training Loss  2.6902707759290934e-05\n",
            "Epoch  29 Batch  90 / 525  Training Loss  2.5067387468880042e-05\n",
            "Epoch  29 Batch  91 / 525  Training Loss  2.2288253603619523e-05\n",
            "Epoch  29 Batch  92 / 525  Training Loss  1.855840309872292e-05\n",
            "Epoch  29 Batch  93 / 525  Training Loss  2.2703252398059703e-05\n",
            "Epoch  29 Batch  94 / 525  Training Loss  2.093742114084307e-05\n",
            "Epoch  29 Batch  95 / 525  Training Loss  1.7533355276100338e-05\n",
            "Epoch  29 Batch  96 / 525  Training Loss  2.9354287107707933e-05\n",
            "Epoch  29 Batch  97 / 525  Training Loss  2.6171159333898686e-05\n",
            "Epoch  29 Batch  98 / 525  Training Loss  2.318809856660664e-05\n",
            "Epoch  29 Batch  99 / 525  Training Loss  3.518462835927494e-05\n",
            "Epoch  29 Batch  100 / 525  Training Loss  2.908579153881874e-05\n",
            "Epoch  29 Batch  101 / 525  Training Loss  3.035514964722097e-05\n",
            "Epoch  29 Batch  102 / 525  Training Loss  1.809449167922139e-05\n",
            "Epoch  29 Batch  103 / 525  Training Loss  2.777540976239834e-05\n",
            "Epoch  29 Batch  104 / 525  Training Loss  2.0947471057297662e-05\n",
            "Epoch  29 Batch  105 / 525  Training Loss  2.944305379060097e-05\n",
            "Epoch  29 Batch  106 / 525  Training Loss  3.1712370400782675e-05\n",
            "Epoch  29 Batch  107 / 525  Training Loss  1.917699228215497e-05\n",
            "Epoch  29 Batch  108 / 525  Training Loss  2.953679722850211e-05\n",
            "Epoch  29 Batch  109 / 525  Training Loss  2.7984287953586318e-05\n",
            "Epoch  29 Batch  110 / 525  Training Loss  2.271481935167685e-05\n",
            "Epoch  29 Batch  111 / 525  Training Loss  2.7253210646449588e-05\n",
            "Epoch  29 Batch  112 / 525  Training Loss  2.645525091793388e-05\n",
            "Epoch  29 Batch  113 / 525  Training Loss  2.1700234356103465e-05\n",
            "Epoch  29 Batch  114 / 525  Training Loss  2.9611710488097742e-05\n",
            "Epoch  29 Batch  115 / 525  Training Loss  2.3522714400314726e-05\n",
            "Epoch  29 Batch  116 / 525  Training Loss  2.816515370795969e-05\n",
            "Epoch  29 Batch  117 / 525  Training Loss  1.8672413716558367e-05\n",
            "Epoch  29 Batch  118 / 525  Training Loss  1.9262386558693834e-05\n",
            "Epoch  29 Batch  119 / 525  Training Loss  2.657062759681139e-05\n",
            "Epoch  29 Batch  120 / 525  Training Loss  2.0738658349728212e-05\n",
            "Epoch  29 Batch  121 / 525  Training Loss  2.0763374777743593e-05\n",
            "Epoch  29 Batch  122 / 525  Training Loss  1.6782094462541863e-05\n",
            "Epoch  29 Batch  123 / 525  Training Loss  3.166679380228743e-05\n",
            "Epoch  29 Batch  124 / 525  Training Loss  2.1640491468133405e-05\n",
            "Epoch  29 Batch  125 / 525  Training Loss  1.9335395336383954e-05\n",
            "Epoch  29 Batch  126 / 525  Training Loss  3.1950265110936016e-05\n",
            "Epoch  29 Batch  127 / 525  Training Loss  3.158282197546214e-05\n",
            "Epoch  29 Batch  128 / 525  Training Loss  3.1582250812789425e-05\n",
            "Epoch  29 Batch  129 / 525  Training Loss  2.9247315978864208e-05\n",
            "Epoch  29 Batch  130 / 525  Training Loss  3.0443488867604174e-05\n",
            "Epoch  29 Batch  131 / 525  Training Loss  3.5642675356939435e-05\n",
            "Epoch  29 Batch  132 / 525  Training Loss  2.8207467039464973e-05\n",
            "Epoch  29 Batch  133 / 525  Training Loss  2.3334594516200013e-05\n",
            "Epoch  29 Batch  134 / 525  Training Loss  3.0427714591496624e-05\n",
            "Epoch  29 Batch  135 / 525  Training Loss  2.612654134281911e-05\n",
            "Epoch  29 Batch  136 / 525  Training Loss  3.358619142090902e-05\n",
            "Epoch  29 Batch  137 / 525  Training Loss  1.785525819286704e-05\n",
            "Epoch  29 Batch  138 / 525  Training Loss  2.119087366736494e-05\n",
            "Epoch  29 Batch  139 / 525  Training Loss  2.7479607524583116e-05\n",
            "Epoch  29 Batch  140 / 525  Training Loss  3.278766234871e-05\n",
            "Epoch  29 Batch  141 / 525  Training Loss  2.9023984097875655e-05\n",
            "Epoch  29 Batch  142 / 525  Training Loss  2.292963836225681e-05\n",
            "Epoch  29 Batch  143 / 525  Training Loss  2.4422159185633063e-05\n",
            "Epoch  29 Batch  144 / 525  Training Loss  2.3613712983205914e-05\n",
            "Epoch  29 Batch  145 / 525  Training Loss  2.477851194271352e-05\n",
            "Epoch  29 Batch  146 / 525  Training Loss  2.1157520677661523e-05\n",
            "Epoch  29 Batch  147 / 525  Training Loss  2.2123011149233207e-05\n",
            "Epoch  29 Batch  148 / 525  Training Loss  2.8869180823676288e-05\n",
            "Epoch  29 Batch  149 / 525  Training Loss  3.456185004324652e-05\n",
            "Epoch  29 Batch  150 / 525  Training Loss  2.4577853764640167e-05\n",
            "Epoch  29 Batch  151 / 525  Training Loss  2.2875488866702653e-05\n",
            "Epoch  29 Batch  152 / 525  Training Loss  3.148921314277686e-05\n",
            "Epoch  29 Batch  153 / 525  Training Loss  2.4206185116781853e-05\n",
            "Epoch  29 Batch  154 / 525  Training Loss  3.2648229534970596e-05\n",
            "Epoch  29 Batch  155 / 525  Training Loss  2.3235261323861778e-05\n",
            "Epoch  29 Batch  156 / 525  Training Loss  1.9251285266363993e-05\n",
            "Epoch  29 Batch  157 / 525  Training Loss  2.431719440210145e-05\n",
            "Epoch  29 Batch  158 / 525  Training Loss  2.2642128897132352e-05\n",
            "Epoch  29 Batch  159 / 525  Training Loss  1.9323582819197327e-05\n",
            "Epoch  29 Batch  160 / 525  Training Loss  3.066632052650675e-05\n",
            "Epoch  29 Batch  161 / 525  Training Loss  3.0404265999095514e-05\n",
            "Epoch  29 Batch  162 / 525  Training Loss  2.381199738010764e-05\n",
            "Epoch  29 Batch  163 / 525  Training Loss  2.605982263048645e-05\n",
            "Epoch  29 Batch  164 / 525  Training Loss  2.7868649340234697e-05\n",
            "Epoch  29 Batch  165 / 525  Training Loss  2.231433245469816e-05\n",
            "Epoch  29 Batch  166 / 525  Training Loss  2.191250678151846e-05\n",
            "Epoch  29 Batch  167 / 525  Training Loss  2.7601661713561043e-05\n",
            "Epoch  29 Batch  168 / 525  Training Loss  2.7402269552112557e-05\n",
            "Epoch  29 Batch  169 / 525  Training Loss  2.1275049221003428e-05\n",
            "Epoch  29 Batch  170 / 525  Training Loss  2.4885845050448552e-05\n",
            "Epoch  29 Batch  171 / 525  Training Loss  2.101395875797607e-05\n",
            "Epoch  29 Batch  172 / 525  Training Loss  2.51664750976488e-05\n",
            "Epoch  29 Batch  173 / 525  Training Loss  2.5839350200840272e-05\n",
            "Epoch  29 Batch  174 / 525  Training Loss  2.2677977540297434e-05\n",
            "Epoch  29 Batch  175 / 525  Training Loss  2.5038938474608585e-05\n",
            "Epoch  29 Batch  176 / 525  Training Loss  2.596009653643705e-05\n",
            "Epoch  29 Batch  177 / 525  Training Loss  2.5039780666702427e-05\n",
            "Epoch  29 Batch  178 / 525  Training Loss  2.4765025955275632e-05\n",
            "Epoch  29 Batch  179 / 525  Training Loss  1.7611831935937516e-05\n",
            "Epoch  29 Batch  180 / 525  Training Loss  2.1843696231371723e-05\n",
            "Epoch  29 Batch  181 / 525  Training Loss  2.6461833840585314e-05\n",
            "Epoch  29 Batch  182 / 525  Training Loss  2.572906123532448e-05\n",
            "Epoch  29 Batch  183 / 525  Training Loss  3.081487011513673e-05\n",
            "Epoch  29 Batch  184 / 525  Training Loss  2.4602742996648885e-05\n",
            "Epoch  29 Batch  185 / 525  Training Loss  2.5874725906760432e-05\n",
            "Epoch  29 Batch  186 / 525  Training Loss  2.4176775696105324e-05\n",
            "Epoch  29 Batch  187 / 525  Training Loss  2.2488547983812168e-05\n",
            "Epoch  29 Batch  188 / 525  Training Loss  2.7139391022501513e-05\n",
            "Epoch  29 Batch  189 / 525  Training Loss  2.739653064054437e-05\n",
            "Epoch  29 Batch  190 / 525  Training Loss  2.974789276777301e-05\n",
            "Epoch  29 Batch  191 / 525  Training Loss  2.2596172129851766e-05\n",
            "Epoch  29 Batch  192 / 525  Training Loss  2.367258639424108e-05\n",
            "Epoch  29 Batch  193 / 525  Training Loss  2.427303115837276e-05\n",
            "Epoch  29 Batch  194 / 525  Training Loss  2.3531121769337915e-05\n",
            "Epoch  29 Batch  195 / 525  Training Loss  2.347852205275558e-05\n",
            "Epoch  29 Batch  196 / 525  Training Loss  2.9893364626332186e-05\n",
            "Epoch  29 Batch  197 / 525  Training Loss  2.0274766939110123e-05\n",
            "Epoch  29 Batch  198 / 525  Training Loss  1.6526722902199253e-05\n",
            "Epoch  29 Batch  199 / 525  Training Loss  3.2707575883250684e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  29 Batch  200 / 525  Training Loss  2.6916037313640118e-05\n",
            "Epoch  29 Batch  201 / 525  Training Loss  2.6804880690178834e-05\n",
            "Epoch  29 Batch  202 / 525  Training Loss  1.980669367185328e-05\n",
            "Epoch  29 Batch  203 / 525  Training Loss  2.034950375673361e-05\n",
            "Epoch  29 Batch  204 / 525  Training Loss  2.972028414660599e-05\n",
            "Epoch  29 Batch  205 / 525  Training Loss  2.2870251996209845e-05\n",
            "Epoch  29 Batch  206 / 525  Training Loss  1.743890970828943e-05\n",
            "Epoch  29 Batch  207 / 525  Training Loss  2.806653174047824e-05\n",
            "Epoch  29 Batch  208 / 525  Training Loss  3.616096364567056e-05\n",
            "Epoch  29 Batch  209 / 525  Training Loss  2.976377072627656e-05\n",
            "Epoch  29 Batch  210 / 525  Training Loss  2.4521894374629483e-05\n",
            "Epoch  29 Batch  211 / 525  Training Loss  1.915932443807833e-05\n",
            "Epoch  29 Batch  212 / 525  Training Loss  3.597175600589253e-05\n",
            "Epoch  29 Batch  213 / 525  Training Loss  2.2348283891915344e-05\n",
            "Epoch  29 Batch  214 / 525  Training Loss  2.422227044007741e-05\n",
            "Epoch  29 Batch  215 / 525  Training Loss  1.7977832612814382e-05\n",
            "Epoch  29 Batch  216 / 525  Training Loss  1.9650387912406586e-05\n",
            "Epoch  29 Batch  217 / 525  Training Loss  2.4290957298944704e-05\n",
            "Epoch  29 Batch  218 / 525  Training Loss  1.926448203448672e-05\n",
            "Epoch  29 Batch  219 / 525  Training Loss  2.4232600480900146e-05\n",
            "Epoch  29 Batch  220 / 525  Training Loss  1.9834424165310338e-05\n",
            "Epoch  29 Batch  221 / 525  Training Loss  3.152522549498826e-05\n",
            "Epoch  29 Batch  222 / 525  Training Loss  2.317785401828587e-05\n",
            "Epoch  29 Batch  223 / 525  Training Loss  2.816452797560487e-05\n",
            "Epoch  29 Batch  224 / 525  Training Loss  2.887409573304467e-05\n",
            "Epoch  29 Batch  225 / 525  Training Loss  2.379324178036768e-05\n",
            "Epoch  29 Batch  226 / 525  Training Loss  3.335134169901721e-05\n",
            "Epoch  29 Batch  227 / 525  Training Loss  2.6969612008542754e-05\n",
            "Epoch  29 Batch  228 / 525  Training Loss  2.207242141594179e-05\n",
            "Epoch  29 Batch  229 / 525  Training Loss  3.2931711757555604e-05\n",
            "Epoch  29 Batch  230 / 525  Training Loss  2.195329579990357e-05\n",
            "Epoch  29 Batch  231 / 525  Training Loss  2.1613555873045698e-05\n",
            "Epoch  29 Batch  232 / 525  Training Loss  2.3964836145751178e-05\n",
            "Epoch  29 Batch  233 / 525  Training Loss  2.8280128390179016e-05\n",
            "Epoch  29 Batch  234 / 525  Training Loss  2.371624759689439e-05\n",
            "Epoch  29 Batch  235 / 525  Training Loss  3.2105152058647946e-05\n",
            "Epoch  29 Batch  236 / 525  Training Loss  2.9225815524114296e-05\n",
            "Epoch  29 Batch  237 / 525  Training Loss  1.968683864106424e-05\n",
            "Epoch  29 Batch  238 / 525  Training Loss  2.4678438421688043e-05\n",
            "Epoch  29 Batch  239 / 525  Training Loss  2.4203753127949312e-05\n",
            "Epoch  29 Batch  240 / 525  Training Loss  2.2425874703912996e-05\n",
            "Epoch  29 Batch  241 / 525  Training Loss  2.7784353733295575e-05\n",
            "Epoch  29 Batch  242 / 525  Training Loss  2.7996193239232525e-05\n",
            "Epoch  29 Batch  243 / 525  Training Loss  2.3925749701447785e-05\n",
            "Epoch  29 Batch  244 / 525  Training Loss  1.601828080310952e-05\n",
            "Epoch  29 Batch  245 / 525  Training Loss  2.2029656975064427e-05\n",
            "Epoch  29 Batch  246 / 525  Training Loss  2.326052708667703e-05\n",
            "Epoch  29 Batch  247 / 525  Training Loss  2.1621668565785512e-05\n",
            "Epoch  29 Batch  248 / 525  Training Loss  3.0492115911329165e-05\n",
            "Epoch  29 Batch  249 / 525  Training Loss  3.0940853321226314e-05\n",
            "Epoch  29 Batch  250 / 525  Training Loss  2.408173168078065e-05\n",
            "Epoch  29 Batch  251 / 525  Training Loss  1.759422229952179e-05\n",
            "Epoch  29 Batch  252 / 525  Training Loss  2.7851720005855896e-05\n",
            "Epoch  29 Batch  253 / 525  Training Loss  2.8764872695319355e-05\n",
            "Epoch  29 Batch  254 / 525  Training Loss  2.6418047127663158e-05\n",
            "Epoch  29 Batch  255 / 525  Training Loss  1.9216253349441104e-05\n",
            "Epoch  29 Batch  256 / 525  Training Loss  3.251407906645909e-05\n",
            "Epoch  29 Batch  257 / 525  Training Loss  2.462774318701122e-05\n",
            "Epoch  29 Batch  258 / 525  Training Loss  2.5095239834627137e-05\n",
            "Epoch  29 Batch  259 / 525  Training Loss  2.4155016944860108e-05\n",
            "Epoch  29 Batch  260 / 525  Training Loss  1.8081807866110466e-05\n",
            "Epoch  29 Batch  261 / 525  Training Loss  2.3692011382081546e-05\n",
            "Epoch  29 Batch  262 / 525  Training Loss  2.0011506421724334e-05\n",
            "Epoch  29 Batch  263 / 525  Training Loss  2.6053810870507732e-05\n",
            "Epoch  29 Batch  264 / 525  Training Loss  2.5696976081235334e-05\n",
            "Epoch  29 Batch  265 / 525  Training Loss  2.229735764558427e-05\n",
            "Epoch  29 Batch  266 / 525  Training Loss  3.314686910016462e-05\n",
            "Epoch  29 Batch  267 / 525  Training Loss  2.944370498880744e-05\n",
            "Epoch  29 Batch  268 / 525  Training Loss  2.7406489607528783e-05\n",
            "Epoch  29 Batch  269 / 525  Training Loss  2.0203138774377294e-05\n",
            "Epoch  29 Batch  270 / 525  Training Loss  2.1682824808522128e-05\n",
            "Epoch  29 Batch  271 / 525  Training Loss  3.0261235224315897e-05\n",
            "Epoch  29 Batch  272 / 525  Training Loss  2.6490355594432913e-05\n",
            "Epoch  29 Batch  273 / 525  Training Loss  2.5820418159128167e-05\n",
            "Epoch  29 Batch  274 / 525  Training Loss  2.7461308491183445e-05\n",
            "Epoch  29 Batch  275 / 525  Training Loss  2.35843617701903e-05\n",
            "Epoch  29 Batch  276 / 525  Training Loss  3.66339590982534e-05\n",
            "Epoch  29 Batch  277 / 525  Training Loss  2.374707219132688e-05\n",
            "Epoch  29 Batch  278 / 525  Training Loss  1.9735907699214295e-05\n",
            "Epoch  29 Batch  279 / 525  Training Loss  3.399808701942675e-05\n",
            "Epoch  29 Batch  280 / 525  Training Loss  2.6760488253785297e-05\n",
            "Epoch  29 Batch  281 / 525  Training Loss  1.7669850421953015e-05\n",
            "Epoch  29 Batch  282 / 525  Training Loss  3.0770752346143126e-05\n",
            "Epoch  29 Batch  283 / 525  Training Loss  2.1022995497332886e-05\n",
            "Epoch  29 Batch  284 / 525  Training Loss  2.4360397219425067e-05\n",
            "Epoch  29 Batch  285 / 525  Training Loss  2.176753696403466e-05\n",
            "Epoch  29 Batch  286 / 525  Training Loss  2.89099698420614e-05\n",
            "Epoch  29 Batch  287 / 525  Training Loss  2.2314832676784135e-05\n",
            "Epoch  29 Batch  288 / 525  Training Loss  2.2926502424525097e-05\n",
            "Epoch  29 Batch  289 / 525  Training Loss  2.3917740691103972e-05\n",
            "Epoch  29 Batch  290 / 525  Training Loss  2.2415506464312784e-05\n",
            "Epoch  29 Batch  291 / 525  Training Loss  2.6826897737919353e-05\n",
            "Epoch  29 Batch  292 / 525  Training Loss  2.6722578695625998e-05\n",
            "Epoch  29 Batch  293 / 525  Training Loss  2.5168967113131657e-05\n",
            "Epoch  29 Batch  294 / 525  Training Loss  1.7545087757753208e-05\n",
            "Epoch  29 Batch  295 / 525  Training Loss  2.2965617972658947e-05\n",
            "Epoch  29 Batch  296 / 525  Training Loss  2.789516656775959e-05\n",
            "Epoch  29 Batch  297 / 525  Training Loss  2.8404087061062455e-05\n",
            "Epoch  29 Batch  298 / 525  Training Loss  2.557757579779718e-05\n",
            "Epoch  29 Batch  299 / 525  Training Loss  2.8467937227105722e-05\n",
            "Epoch  29 Batch  300 / 525  Training Loss  3.2302119507221505e-05\n",
            "Epoch  29 Batch  301 / 525  Training Loss  2.002254404942505e-05\n",
            "Epoch  29 Batch  302 / 525  Training Loss  1.80976257979637e-05\n",
            "Epoch  29 Batch  303 / 525  Training Loss  2.4028751795412973e-05\n",
            "Epoch  29 Batch  304 / 525  Training Loss  3.416259278310463e-05\n",
            "Epoch  29 Batch  305 / 525  Training Loss  2.6344825528212823e-05\n",
            "Epoch  29 Batch  306 / 525  Training Loss  2.152885281248018e-05\n",
            "Epoch  29 Batch  307 / 525  Training Loss  2.4513114112778567e-05\n",
            "Epoch  29 Batch  308 / 525  Training Loss  2.590342228359077e-05\n",
            "Epoch  29 Batch  309 / 525  Training Loss  2.498541107343044e-05\n",
            "Epoch  29 Batch  310 / 525  Training Loss  1.9746312318602577e-05\n",
            "Epoch  29 Batch  311 / 525  Training Loss  2.5820441805990413e-05\n",
            "Epoch  29 Batch  312 / 525  Training Loss  1.9462657292024232e-05\n",
            "Epoch  29 Batch  313 / 525  Training Loss  2.6558580429991707e-05\n",
            "Epoch  29 Batch  314 / 525  Training Loss  3.382396243978292e-05\n",
            "Epoch  29 Batch  315 / 525  Training Loss  2.4115915948641486e-05\n",
            "Epoch  29 Batch  316 / 525  Training Loss  2.7696052711689845e-05\n",
            "Epoch  29 Batch  317 / 525  Training Loss  1.831699955801014e-05\n",
            "Epoch  29 Batch  318 / 525  Training Loss  3.826410829788074e-05\n",
            "Epoch  29 Batch  319 / 525  Training Loss  2.6586994863464497e-05\n",
            "Epoch  29 Batch  320 / 525  Training Loss  2.2303114747046493e-05\n",
            "Epoch  29 Batch  321 / 525  Training Loss  1.829144821385853e-05\n",
            "Epoch  29 Batch  322 / 525  Training Loss  2.527785545680672e-05\n",
            "Epoch  29 Batch  323 / 525  Training Loss  2.9199401978985406e-05\n",
            "Epoch  29 Batch  324 / 525  Training Loss  1.8259703210787848e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  29 Batch  325 / 525  Training Loss  2.1345975255826488e-05\n",
            "Epoch  29 Batch  326 / 525  Training Loss  2.7497977498569526e-05\n",
            "Epoch  29 Batch  327 / 525  Training Loss  2.3609496565768495e-05\n",
            "Epoch  29 Batch  328 / 525  Training Loss  2.4506716727046296e-05\n",
            "Epoch  29 Batch  329 / 525  Training Loss  2.449366365908645e-05\n",
            "Epoch  29 Batch  330 / 525  Training Loss  1.8993283447343856e-05\n",
            "Epoch  29 Batch  331 / 525  Training Loss  2.554995080572553e-05\n",
            "Epoch  29 Batch  332 / 525  Training Loss  2.5989782443502918e-05\n",
            "Epoch  29 Batch  333 / 525  Training Loss  2.2042968339519575e-05\n",
            "Epoch  29 Batch  334 / 525  Training Loss  2.690040855668485e-05\n",
            "Epoch  29 Batch  335 / 525  Training Loss  2.4259161364170723e-05\n",
            "Epoch  29 Batch  336 / 525  Training Loss  2.0918187146889977e-05\n",
            "Epoch  29 Batch  337 / 525  Training Loss  1.736074045766145e-05\n",
            "Epoch  29 Batch  338 / 525  Training Loss  2.345616121601779e-05\n",
            "Epoch  29 Batch  339 / 525  Training Loss  3.1965202651917934e-05\n",
            "Epoch  29 Batch  340 / 525  Training Loss  3.5824617953039706e-05\n",
            "Epoch  29 Batch  341 / 525  Training Loss  3.2603573345113546e-05\n",
            "Epoch  29 Batch  342 / 525  Training Loss  2.086226231767796e-05\n",
            "Epoch  29 Batch  343 / 525  Training Loss  3.217596167814918e-05\n",
            "Epoch  29 Batch  344 / 525  Training Loss  2.3699793018749915e-05\n",
            "Epoch  29 Batch  345 / 525  Training Loss  2.7333526304573752e-05\n",
            "Epoch  29 Batch  346 / 525  Training Loss  2.339803904760629e-05\n",
            "Epoch  29 Batch  347 / 525  Training Loss  2.6434738174430095e-05\n",
            "Epoch  29 Batch  348 / 525  Training Loss  2.7177773517905734e-05\n",
            "Epoch  29 Batch  349 / 525  Training Loss  2.4570723326178268e-05\n",
            "Epoch  29 Batch  350 / 525  Training Loss  2.8686021323665045e-05\n",
            "Epoch  29 Batch  351 / 525  Training Loss  2.9954102501505986e-05\n",
            "Epoch  29 Batch  352 / 525  Training Loss  2.1101770471432246e-05\n",
            "Epoch  29 Batch  353 / 525  Training Loss  2.1318455765140243e-05\n",
            "Epoch  29 Batch  354 / 525  Training Loss  2.9303937481017783e-05\n",
            "Epoch  29 Batch  355 / 525  Training Loss  2.5671932235127315e-05\n",
            "Epoch  29 Batch  356 / 525  Training Loss  2.1994659618940204e-05\n",
            "Epoch  29 Batch  357 / 525  Training Loss  2.291802411491517e-05\n",
            "Epoch  29 Batch  358 / 525  Training Loss  2.6764226277009584e-05\n",
            "Epoch  29 Batch  359 / 525  Training Loss  2.9374370569712482e-05\n",
            "Epoch  29 Batch  360 / 525  Training Loss  3.0457047614618205e-05\n",
            "Epoch  29 Batch  361 / 525  Training Loss  2.502916322555393e-05\n",
            "Epoch  29 Batch  362 / 525  Training Loss  2.7712309019989334e-05\n",
            "Epoch  29 Batch  363 / 525  Training Loss  2.8273741918383166e-05\n",
            "Epoch  29 Batch  364 / 525  Training Loss  2.6673918910091743e-05\n",
            "Epoch  29 Batch  365 / 525  Training Loss  2.6270459784427658e-05\n",
            "Epoch  29 Batch  366 / 525  Training Loss  2.3716387659078464e-05\n",
            "Epoch  29 Batch  367 / 525  Training Loss  1.9582317690947093e-05\n",
            "Epoch  29 Batch  368 / 525  Training Loss  3.325079887872562e-05\n",
            "Epoch  29 Batch  369 / 525  Training Loss  2.8497452149167657e-05\n",
            "Epoch  29 Batch  370 / 525  Training Loss  2.638341175043024e-05\n",
            "Epoch  29 Batch  371 / 525  Training Loss  1.981673267437145e-05\n",
            "Epoch  29 Batch  372 / 525  Training Loss  2.7965381377725862e-05\n",
            "Epoch  29 Batch  373 / 525  Training Loss  2.8484035283327103e-05\n",
            "Epoch  29 Batch  374 / 525  Training Loss  3.074943379033357e-05\n",
            "Epoch  29 Batch  375 / 525  Training Loss  3.159142943331972e-05\n",
            "Epoch  29 Batch  376 / 525  Training Loss  2.0207362467772327e-05\n",
            "Epoch  29 Batch  377 / 525  Training Loss  3.1742467399453744e-05\n",
            "Epoch  29 Batch  378 / 525  Training Loss  2.107672298734542e-05\n",
            "Epoch  29 Batch  379 / 525  Training Loss  2.6648276616469957e-05\n",
            "Epoch  29 Batch  380 / 525  Training Loss  3.304435085738078e-05\n",
            "Epoch  29 Batch  381 / 525  Training Loss  2.0745375877595507e-05\n",
            "Epoch  29 Batch  382 / 525  Training Loss  3.1999188649933785e-05\n",
            "Epoch  29 Batch  383 / 525  Training Loss  1.6332245650119148e-05\n",
            "Epoch  29 Batch  384 / 525  Training Loss  2.522760769352317e-05\n",
            "Epoch  29 Batch  385 / 525  Training Loss  2.3427604901371524e-05\n",
            "Epoch  29 Batch  386 / 525  Training Loss  2.795282671286259e-05\n",
            "Epoch  29 Batch  387 / 525  Training Loss  2.824818693625275e-05\n",
            "Epoch  29 Batch  388 / 525  Training Loss  2.6875510229729116e-05\n",
            "Epoch  29 Batch  389 / 525  Training Loss  2.1671583454008214e-05\n",
            "Epoch  29 Batch  390 / 525  Training Loss  2.2349593564285897e-05\n",
            "Epoch  29 Batch  391 / 525  Training Loss  1.9264105503680184e-05\n",
            "Epoch  29 Batch  392 / 525  Training Loss  2.7085803594673052e-05\n",
            "Epoch  29 Batch  393 / 525  Training Loss  2.2658408852294087e-05\n",
            "Epoch  29 Batch  394 / 525  Training Loss  2.2624843040830456e-05\n",
            "Epoch  29 Batch  395 / 525  Training Loss  2.88900137093151e-05\n",
            "Epoch  29 Batch  396 / 525  Training Loss  2.1915435354458168e-05\n",
            "Epoch  29 Batch  397 / 525  Training Loss  2.5489260224276222e-05\n",
            "Epoch  29 Batch  398 / 525  Training Loss  3.1558760383632034e-05\n",
            "Epoch  29 Batch  399 / 525  Training Loss  2.533442238927819e-05\n",
            "Epoch  29 Batch  400 / 525  Training Loss  2.335146928089671e-05\n",
            "Epoch  29 Batch  401 / 525  Training Loss  2.0387586118886247e-05\n",
            "Epoch  29 Batch  402 / 525  Training Loss  2.1903233573539183e-05\n",
            "Epoch  29 Batch  403 / 525  Training Loss  2.3904349291115068e-05\n",
            "Epoch  29 Batch  404 / 525  Training Loss  2.0110164768993855e-05\n",
            "Epoch  29 Batch  405 / 525  Training Loss  2.073490213660989e-05\n",
            "Epoch  29 Batch  406 / 525  Training Loss  2.1126292267581448e-05\n",
            "Epoch  29 Batch  407 / 525  Training Loss  2.4955224944278598e-05\n",
            "Epoch  29 Batch  408 / 525  Training Loss  2.0535828298307024e-05\n",
            "Epoch  29 Batch  409 / 525  Training Loss  3.2503790862392634e-05\n",
            "Epoch  29 Batch  410 / 525  Training Loss  3.057886351598427e-05\n",
            "Epoch  29 Batch  411 / 525  Training Loss  2.873237826861441e-05\n",
            "Epoch  29 Batch  412 / 525  Training Loss  2.6532627089181915e-05\n",
            "Epoch  29 Batch  413 / 525  Training Loss  4.118327342439443e-05\n",
            "Epoch  29 Batch  414 / 525  Training Loss  2.5641671527409926e-05\n",
            "Epoch  29 Batch  415 / 525  Training Loss  2.0682400645455346e-05\n",
            "Epoch  29 Batch  416 / 525  Training Loss  2.464663702994585e-05\n",
            "Epoch  29 Batch  417 / 525  Training Loss  2.220853275503032e-05\n",
            "Epoch  29 Batch  418 / 525  Training Loss  2.869900163204875e-05\n",
            "Epoch  29 Batch  419 / 525  Training Loss  2.225323987659067e-05\n",
            "Epoch  29 Batch  420 / 525  Training Loss  3.2847536203917116e-05\n",
            "Epoch  29 Batch  421 / 525  Training Loss  2.0446492271730676e-05\n",
            "Epoch  29 Batch  422 / 525  Training Loss  2.3055466954247095e-05\n",
            "Epoch  29 Batch  423 / 525  Training Loss  1.6813710317364894e-05\n",
            "Epoch  29 Batch  424 / 525  Training Loss  2.636535464262124e-05\n",
            "Epoch  29 Batch  425 / 525  Training Loss  2.8858627047156915e-05\n",
            "Epoch  29 Batch  426 / 525  Training Loss  2.6539559257798828e-05\n",
            "Epoch  29 Batch  427 / 525  Training Loss  1.9871844415320083e-05\n",
            "Epoch  29 Batch  428 / 525  Training Loss  1.7239563021576032e-05\n",
            "Epoch  29 Batch  429 / 525  Training Loss  2.55016111623263e-05\n",
            "Epoch  29 Batch  430 / 525  Training Loss  2.30004261538852e-05\n",
            "Epoch  29 Batch  431 / 525  Training Loss  2.328890150238294e-05\n",
            "Epoch  29 Batch  432 / 525  Training Loss  2.3634325771126896e-05\n",
            "Epoch  29 Batch  433 / 525  Training Loss  2.9823937438777648e-05\n",
            "Epoch  29 Batch  434 / 525  Training Loss  2.1598867533612065e-05\n",
            "Epoch  29 Batch  435 / 525  Training Loss  2.9922997782705352e-05\n",
            "Epoch  29 Batch  436 / 525  Training Loss  1.7779722838895395e-05\n",
            "Epoch  29 Batch  437 / 525  Training Loss  1.9306267859064974e-05\n",
            "Epoch  29 Batch  438 / 525  Training Loss  2.6978144887834787e-05\n",
            "Epoch  29 Batch  439 / 525  Training Loss  2.346690234844573e-05\n",
            "Epoch  29 Batch  440 / 525  Training Loss  2.5327937692054547e-05\n",
            "Epoch  29 Batch  441 / 525  Training Loss  2.692923226277344e-05\n",
            "Epoch  29 Batch  442 / 525  Training Loss  2.045983274001628e-05\n",
            "Epoch  29 Batch  443 / 525  Training Loss  1.9539318600436673e-05\n",
            "Epoch  29 Batch  444 / 525  Training Loss  1.9776325643761083e-05\n",
            "Epoch  29 Batch  445 / 525  Training Loss  2.5282661226810887e-05\n",
            "Epoch  29 Batch  446 / 525  Training Loss  1.9983883248642087e-05\n",
            "Epoch  29 Batch  447 / 525  Training Loss  3.132110941805877e-05\n",
            "Epoch  29 Batch  448 / 525  Training Loss  2.4544318875996396e-05\n",
            "Epoch  29 Batch  449 / 525  Training Loss  2.1895124518778175e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  29 Batch  450 / 525  Training Loss  2.268886055389885e-05\n",
            "Epoch  29 Batch  451 / 525  Training Loss  2.674849019967951e-05\n",
            "Epoch  29 Batch  452 / 525  Training Loss  2.7273694286122918e-05\n",
            "Epoch  29 Batch  453 / 525  Training Loss  1.6130597941810265e-05\n",
            "Epoch  29 Batch  454 / 525  Training Loss  1.8601718693389557e-05\n",
            "Epoch  29 Batch  455 / 525  Training Loss  2.6923193217953667e-05\n",
            "Epoch  29 Batch  456 / 525  Training Loss  2.6525794964982197e-05\n",
            "Epoch  29 Batch  457 / 525  Training Loss  2.5873141566989943e-05\n",
            "Epoch  29 Batch  458 / 525  Training Loss  2.571492768765893e-05\n",
            "Epoch  29 Batch  459 / 525  Training Loss  2.0707670046249405e-05\n",
            "Epoch  29 Batch  460 / 525  Training Loss  2.4387172743445262e-05\n",
            "Epoch  29 Batch  461 / 525  Training Loss  3.0073340894887224e-05\n",
            "Epoch  29 Batch  462 / 525  Training Loss  2.0389923520269804e-05\n",
            "Epoch  29 Batch  463 / 525  Training Loss  1.9578845240175724e-05\n",
            "Epoch  29 Batch  464 / 525  Training Loss  2.4929395294748247e-05\n",
            "Epoch  29 Batch  465 / 525  Training Loss  2.633602161949966e-05\n",
            "Epoch  29 Batch  466 / 525  Training Loss  2.415409471723251e-05\n",
            "Epoch  29 Batch  467 / 525  Training Loss  2.8956588721484877e-05\n",
            "Epoch  29 Batch  468 / 525  Training Loss  2.4150340323103592e-05\n",
            "Epoch  29 Batch  469 / 525  Training Loss  3.1314033549278975e-05\n",
            "Epoch  29 Batch  470 / 525  Training Loss  1.5609190086252056e-05\n",
            "Epoch  29 Batch  471 / 525  Training Loss  2.2398218789021485e-05\n",
            "Epoch  29 Batch  472 / 525  Training Loss  1.5811507182661444e-05\n",
            "Epoch  29 Batch  473 / 525  Training Loss  1.8196096789324656e-05\n",
            "Epoch  29 Batch  474 / 525  Training Loss  2.0014629626530223e-05\n",
            "Epoch  29 Batch  475 / 525  Training Loss  1.820965189835988e-05\n",
            "Epoch  29 Batch  476 / 525  Training Loss  2.036616024270188e-05\n",
            "Epoch  29 Batch  477 / 525  Training Loss  2.2479358449345455e-05\n",
            "Epoch  29 Batch  478 / 525  Training Loss  2.941578350146301e-05\n",
            "Epoch  29 Batch  479 / 525  Training Loss  2.5771145374164917e-05\n",
            "Epoch  29 Batch  480 / 525  Training Loss  2.875152313208673e-05\n",
            "Epoch  29 Batch  481 / 525  Training Loss  2.8967207981622778e-05\n",
            "Epoch  29 Batch  482 / 525  Training Loss  2.43376380240079e-05\n",
            "Epoch  29 Batch  483 / 525  Training Loss  2.607932583487127e-05\n",
            "Epoch  29 Batch  484 / 525  Training Loss  2.4159380700439215e-05\n",
            "Epoch  29 Batch  485 / 525  Training Loss  2.549782584537752e-05\n",
            "Epoch  29 Batch  486 / 525  Training Loss  2.7363732442609034e-05\n",
            "Epoch  29 Batch  487 / 525  Training Loss  1.6274234440061264e-05\n",
            "Epoch  29 Batch  488 / 525  Training Loss  2.6491479729884304e-05\n",
            "Epoch  29 Batch  489 / 525  Training Loss  2.4740522349020466e-05\n",
            "Epoch  29 Batch  490 / 525  Training Loss  2.7608853997662663e-05\n",
            "Epoch  29 Batch  491 / 525  Training Loss  2.949628105852753e-05\n",
            "Epoch  29 Batch  492 / 525  Training Loss  2.1547813958022743e-05\n",
            "Epoch  29 Batch  493 / 525  Training Loss  2.6550123948254623e-05\n",
            "Epoch  29 Batch  494 / 525  Training Loss  2.296843013027683e-05\n",
            "Epoch  29 Batch  495 / 525  Training Loss  2.236678483313881e-05\n",
            "Epoch  29 Batch  496 / 525  Training Loss  2.4298791686305776e-05\n",
            "Epoch  29 Batch  497 / 525  Training Loss  3.416977051529102e-05\n",
            "Epoch  29 Batch  498 / 525  Training Loss  3.1390201911563054e-05\n",
            "Epoch  29 Batch  499 / 525  Training Loss  2.572535231593065e-05\n",
            "Epoch  29 Batch  500 / 525  Training Loss  2.3765163859934546e-05\n",
            "Epoch  29 Batch  501 / 525  Training Loss  2.8582400773302652e-05\n",
            "Epoch  29 Batch  502 / 525  Training Loss  2.387400127190631e-05\n",
            "Epoch  29 Batch  503 / 525  Training Loss  2.1950143491267227e-05\n",
            "Epoch  29 Batch  504 / 525  Training Loss  2.4313831090694293e-05\n",
            "Epoch  29 Batch  505 / 525  Training Loss  2.0722316548926756e-05\n",
            "Epoch  29 Batch  506 / 525  Training Loss  1.787711698852945e-05\n",
            "Epoch  29 Batch  507 / 525  Training Loss  1.952792445081286e-05\n",
            "Epoch  29 Batch  508 / 525  Training Loss  2.9767532396363094e-05\n",
            "Epoch  29 Batch  509 / 525  Training Loss  2.494468390068505e-05\n",
            "Epoch  29 Batch  510 / 525  Training Loss  2.2481175619759597e-05\n",
            "Epoch  29 Batch  511 / 525  Training Loss  2.5169432774418965e-05\n",
            "Epoch  29 Batch  512 / 525  Training Loss  2.714762740652077e-05\n",
            "Epoch  29 Batch  513 / 525  Training Loss  2.585792935860809e-05\n",
            "Epoch  29 Batch  514 / 525  Training Loss  2.4254186428152025e-05\n",
            "Epoch  29 Batch  515 / 525  Training Loss  2.090563612000551e-05\n",
            "Epoch  29 Batch  516 / 525  Training Loss  2.0144714653724805e-05\n",
            "Epoch  29 Batch  517 / 525  Training Loss  2.4715898689464666e-05\n",
            "Epoch  29 Batch  518 / 525  Training Loss  2.3059099476085976e-05\n",
            "Epoch  29 Batch  519 / 525  Training Loss  2.7235766538069583e-05\n",
            "Epoch  29 Batch  520 / 525  Training Loss  2.2467447706731036e-05\n",
            "Epoch  29 Batch  521 / 525  Training Loss  2.2349946448230185e-05\n",
            "Epoch  29 Batch  522 / 525  Training Loss  2.246816256956663e-05\n",
            "Epoch  29 Batch  523 / 525  Training Loss  2.1264317183522508e-05\n",
            "Epoch  29 Batch  524 / 525  Training Loss  2.534138366172556e-05\n",
            "  30    |    -    |   0.000025   |   64.14  \n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 30\n",
            "Epoch  30 Batch  0 / 525  Training Loss  2.7962854801444337e-05\n",
            "Epoch  30 Batch  1 / 525  Training Loss  2.939042133220937e-05\n",
            "Epoch  30 Batch  2 / 525  Training Loss  2.527742435631808e-05\n",
            "Epoch  30 Batch  3 / 525  Training Loss  3.47244567819871e-05\n",
            "Epoch  30 Batch  4 / 525  Training Loss  2.266558658448048e-05\n",
            "Epoch  30 Batch  5 / 525  Training Loss  2.2591597371501848e-05\n",
            "Epoch  30 Batch  6 / 525  Training Loss  2.4854065486579202e-05\n",
            "Epoch  30 Batch  7 / 525  Training Loss  1.8993077901541255e-05\n",
            "Epoch  30 Batch  8 / 525  Training Loss  2.1757066861027852e-05\n",
            "Epoch  30 Batch  9 / 525  Training Loss  2.2106079995865002e-05\n",
            "Epoch  30 Batch  10 / 525  Training Loss  3.07261616399046e-05\n",
            "Epoch  30 Batch  11 / 525  Training Loss  2.811528065649327e-05\n",
            "Epoch  30 Batch  12 / 525  Training Loss  2.1565034330706112e-05\n",
            "Epoch  30 Batch  13 / 525  Training Loss  2.7955135010415688e-05\n",
            "Epoch  30 Batch  14 / 525  Training Loss  2.8858641599072143e-05\n",
            "Epoch  30 Batch  15 / 525  Training Loss  2.751410283963196e-05\n",
            "Epoch  30 Batch  16 / 525  Training Loss  1.6480578779010102e-05\n",
            "Epoch  30 Batch  17 / 525  Training Loss  2.3445363694918342e-05\n",
            "Epoch  30 Batch  18 / 525  Training Loss  3.611098509281874e-05\n",
            "Epoch  30 Batch  19 / 525  Training Loss  3.094195562880486e-05\n",
            "Epoch  30 Batch  20 / 525  Training Loss  2.2129770513856784e-05\n",
            "Epoch  30 Batch  21 / 525  Training Loss  2.918993959610816e-05\n",
            "Epoch  30 Batch  22 / 525  Training Loss  2.7267078621662222e-05\n",
            "Epoch  30 Batch  23 / 525  Training Loss  2.2584332327824086e-05\n",
            "Epoch  30 Batch  24 / 525  Training Loss  2.064578620775137e-05\n",
            "Epoch  30 Batch  25 / 525  Training Loss  2.531483733037021e-05\n",
            "Epoch  30 Batch  26 / 525  Training Loss  2.6009391149273142e-05\n",
            "Epoch  30 Batch  27 / 525  Training Loss  2.890820542233996e-05\n",
            "Epoch  30 Batch  28 / 525  Training Loss  2.1231278878985904e-05\n",
            "Epoch  30 Batch  29 / 525  Training Loss  1.5563749911962077e-05\n",
            "Epoch  30 Batch  30 / 525  Training Loss  3.3087460906244814e-05\n",
            "Epoch  30 Batch  31 / 525  Training Loss  2.4152959667844698e-05\n",
            "Epoch  30 Batch  32 / 525  Training Loss  2.578092971816659e-05\n",
            "Epoch  30 Batch  33 / 525  Training Loss  2.0787250832654536e-05\n",
            "Epoch  30 Batch  34 / 525  Training Loss  2.1771487809019163e-05\n",
            "Epoch  30 Batch  35 / 525  Training Loss  1.80070310307201e-05\n",
            "Epoch  30 Batch  36 / 525  Training Loss  1.592989792698063e-05\n",
            "Epoch  30 Batch  37 / 525  Training Loss  2.9831537176505663e-05\n",
            "Epoch  30 Batch  38 / 525  Training Loss  1.5272828022716567e-05\n",
            "Epoch  30 Batch  39 / 525  Training Loss  2.322944601473864e-05\n",
            "Epoch  30 Batch  40 / 525  Training Loss  2.5980561986216344e-05\n",
            "Epoch  30 Batch  41 / 525  Training Loss  2.2513635485665873e-05\n",
            "Epoch  30 Batch  42 / 525  Training Loss  2.1379288227763027e-05\n",
            "Epoch  30 Batch  43 / 525  Training Loss  2.1363466657930985e-05\n",
            "Epoch  30 Batch  44 / 525  Training Loss  2.4283184757223353e-05\n",
            "Epoch  30 Batch  45 / 525  Training Loss  2.191016210417729e-05\n",
            "Epoch  30 Batch  46 / 525  Training Loss  1.8259321223013103e-05\n",
            "Epoch  30 Batch  47 / 525  Training Loss  2.7187566956854425e-05\n",
            "Epoch  30 Batch  48 / 525  Training Loss  1.986833376577124e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  30 Batch  49 / 525  Training Loss  2.5214647394022904e-05\n",
            "Epoch  30 Batch  50 / 525  Training Loss  2.696797309909016e-05\n",
            "Epoch  30 Batch  51 / 525  Training Loss  2.0516232325462624e-05\n",
            "Epoch  30 Batch  52 / 525  Training Loss  2.618053440528456e-05\n",
            "Epoch  30 Batch  53 / 525  Training Loss  2.260960900457576e-05\n",
            "Epoch  30 Batch  54 / 525  Training Loss  2.79834584944183e-05\n",
            "Epoch  30 Batch  55 / 525  Training Loss  2.6141973648918793e-05\n",
            "Epoch  30 Batch  56 / 525  Training Loss  2.390726876910776e-05\n",
            "Epoch  30 Batch  57 / 525  Training Loss  2.8179387300042436e-05\n",
            "Epoch  30 Batch  58 / 525  Training Loss  2.9563403586507775e-05\n",
            "Epoch  30 Batch  59 / 525  Training Loss  2.3875070837675594e-05\n",
            "Epoch  30 Batch  60 / 525  Training Loss  2.1453490262501873e-05\n",
            "Epoch  30 Batch  61 / 525  Training Loss  2.109160050167702e-05\n",
            "Epoch  30 Batch  62 / 525  Training Loss  2.25673084059963e-05\n",
            "Epoch  30 Batch  63 / 525  Training Loss  2.3131124180508777e-05\n",
            "Epoch  30 Batch  64 / 525  Training Loss  2.3984841391211376e-05\n",
            "Epoch  30 Batch  65 / 525  Training Loss  1.742128370096907e-05\n",
            "Epoch  30 Batch  66 / 525  Training Loss  2.0463421606109478e-05\n",
            "Epoch  30 Batch  67 / 525  Training Loss  2.022704211412929e-05\n",
            "Epoch  30 Batch  68 / 525  Training Loss  2.3197857444756664e-05\n",
            "Epoch  30 Batch  69 / 525  Training Loss  2.3298376618186012e-05\n",
            "Epoch  30 Batch  70 / 525  Training Loss  1.88543890544679e-05\n",
            "Epoch  30 Batch  71 / 525  Training Loss  2.488945028744638e-05\n",
            "Epoch  30 Batch  72 / 525  Training Loss  2.9046041163383052e-05\n",
            "Epoch  30 Batch  73 / 525  Training Loss  2.540427158237435e-05\n",
            "Epoch  30 Batch  74 / 525  Training Loss  1.922262345033232e-05\n",
            "Epoch  30 Batch  75 / 525  Training Loss  1.6071193385869265e-05\n",
            "Epoch  30 Batch  76 / 525  Training Loss  2.0357470930321142e-05\n",
            "Epoch  30 Batch  77 / 525  Training Loss  2.7119964215671644e-05\n",
            "Epoch  30 Batch  78 / 525  Training Loss  1.96186410903465e-05\n",
            "Epoch  30 Batch  79 / 525  Training Loss  2.108203807438258e-05\n",
            "Epoch  30 Batch  80 / 525  Training Loss  2.6794843506650068e-05\n",
            "Epoch  30 Batch  81 / 525  Training Loss  3.0376453651115298e-05\n",
            "Epoch  30 Batch  82 / 525  Training Loss  2.2426587747759186e-05\n",
            "Epoch  30 Batch  83 / 525  Training Loss  2.966005922644399e-05\n",
            "Epoch  30 Batch  84 / 525  Training Loss  2.8258320526219904e-05\n",
            "Epoch  30 Batch  85 / 525  Training Loss  2.224269337602891e-05\n",
            "Epoch  30 Batch  86 / 525  Training Loss  2.3138145479606465e-05\n",
            "Epoch  30 Batch  87 / 525  Training Loss  3.414188540773466e-05\n",
            "Epoch  30 Batch  88 / 525  Training Loss  2.693525493668858e-05\n",
            "Epoch  30 Batch  89 / 525  Training Loss  2.9190885470598005e-05\n",
            "Epoch  30 Batch  90 / 525  Training Loss  2.7829297323478386e-05\n",
            "Epoch  30 Batch  91 / 525  Training Loss  2.157885865017306e-05\n",
            "Epoch  30 Batch  92 / 525  Training Loss  3.0853778298478574e-05\n",
            "Epoch  30 Batch  93 / 525  Training Loss  3.6798985092900693e-05\n",
            "Epoch  30 Batch  94 / 525  Training Loss  2.473960012139287e-05\n",
            "Epoch  30 Batch  95 / 525  Training Loss  2.2006637664162554e-05\n",
            "Epoch  30 Batch  96 / 525  Training Loss  2.3415832401951775e-05\n",
            "Epoch  30 Batch  97 / 525  Training Loss  2.5374840333824977e-05\n",
            "Epoch  30 Batch  98 / 525  Training Loss  2.2139507564133964e-05\n",
            "Epoch  30 Batch  99 / 525  Training Loss  1.9526705727912486e-05\n",
            "Epoch  30 Batch  100 / 525  Training Loss  2.066009255941026e-05\n",
            "Epoch  30 Batch  101 / 525  Training Loss  2.905903966166079e-05\n",
            "Epoch  30 Batch  102 / 525  Training Loss  1.9134120520902798e-05\n",
            "Epoch  30 Batch  103 / 525  Training Loss  2.22750059037935e-05\n",
            "Epoch  30 Batch  104 / 525  Training Loss  2.620498344185762e-05\n",
            "Epoch  30 Batch  105 / 525  Training Loss  2.30363857554039e-05\n",
            "Epoch  30 Batch  106 / 525  Training Loss  2.53678244916955e-05\n",
            "Epoch  30 Batch  107 / 525  Training Loss  2.333969678147696e-05\n",
            "Epoch  30 Batch  108 / 525  Training Loss  2.538651824579574e-05\n",
            "Epoch  30 Batch  109 / 525  Training Loss  2.3372314899461344e-05\n",
            "Epoch  30 Batch  110 / 525  Training Loss  2.3541575501440093e-05\n",
            "Epoch  30 Batch  111 / 525  Training Loss  2.604375549708493e-05\n",
            "Epoch  30 Batch  112 / 525  Training Loss  2.616962228785269e-05\n",
            "Epoch  30 Batch  113 / 525  Training Loss  2.4430704797850922e-05\n",
            "Epoch  30 Batch  114 / 525  Training Loss  2.7245463570579886e-05\n",
            "Epoch  30 Batch  115 / 525  Training Loss  2.36536161537515e-05\n",
            "Epoch  30 Batch  116 / 525  Training Loss  2.6605091989040375e-05\n",
            "Epoch  30 Batch  117 / 525  Training Loss  2.344655877095647e-05\n",
            "Epoch  30 Batch  118 / 525  Training Loss  2.004336056415923e-05\n",
            "Epoch  30 Batch  119 / 525  Training Loss  2.39790060732048e-05\n",
            "Epoch  30 Batch  120 / 525  Training Loss  2.5386951165273786e-05\n",
            "Epoch  30 Batch  121 / 525  Training Loss  2.115537427016534e-05\n",
            "Epoch  30 Batch  122 / 525  Training Loss  2.534690429456532e-05\n",
            "Epoch  30 Batch  123 / 525  Training Loss  2.8921644116053358e-05\n",
            "Epoch  30 Batch  124 / 525  Training Loss  2.1865025701117702e-05\n",
            "Epoch  30 Batch  125 / 525  Training Loss  2.041816696873866e-05\n",
            "Epoch  30 Batch  126 / 525  Training Loss  2.631561983434949e-05\n",
            "Epoch  30 Batch  127 / 525  Training Loss  2.1692763766623102e-05\n",
            "Epoch  30 Batch  128 / 525  Training Loss  1.9653225535876118e-05\n",
            "Epoch  30 Batch  129 / 525  Training Loss  2.2372707462636754e-05\n",
            "Epoch  30 Batch  130 / 525  Training Loss  2.3033368051983416e-05\n",
            "Epoch  30 Batch  131 / 525  Training Loss  2.0872457753284834e-05\n",
            "Epoch  30 Batch  132 / 525  Training Loss  2.488246900611557e-05\n",
            "Epoch  30 Batch  133 / 525  Training Loss  1.9393632101127878e-05\n",
            "Epoch  30 Batch  134 / 525  Training Loss  1.7520038454676978e-05\n",
            "Epoch  30 Batch  135 / 525  Training Loss  2.368492823734414e-05\n",
            "Epoch  30 Batch  136 / 525  Training Loss  2.022679655055981e-05\n",
            "Epoch  30 Batch  137 / 525  Training Loss  3.5856402973877266e-05\n",
            "Epoch  30 Batch  138 / 525  Training Loss  2.4800445316941477e-05\n",
            "Epoch  30 Batch  139 / 525  Training Loss  2.679401404748205e-05\n",
            "Epoch  30 Batch  140 / 525  Training Loss  2.4872821086319163e-05\n",
            "Epoch  30 Batch  141 / 525  Training Loss  2.606733141874429e-05\n",
            "Epoch  30 Batch  142 / 525  Training Loss  1.8196720702690072e-05\n",
            "Epoch  30 Batch  143 / 525  Training Loss  2.6304740458726883e-05\n",
            "Epoch  30 Batch  144 / 525  Training Loss  2.6055955459014513e-05\n",
            "Epoch  30 Batch  145 / 525  Training Loss  2.1612768250633962e-05\n",
            "Epoch  30 Batch  146 / 525  Training Loss  2.1348534573917277e-05\n",
            "Epoch  30 Batch  147 / 525  Training Loss  1.925593096530065e-05\n",
            "Epoch  30 Batch  148 / 525  Training Loss  1.7701351680443622e-05\n",
            "Epoch  30 Batch  149 / 525  Training Loss  2.682994272618089e-05\n",
            "Epoch  30 Batch  150 / 525  Training Loss  2.464923272782471e-05\n",
            "Epoch  30 Batch  151 / 525  Training Loss  1.7230689991265535e-05\n",
            "Epoch  30 Batch  152 / 525  Training Loss  2.346813380427193e-05\n",
            "Epoch  30 Batch  153 / 525  Training Loss  1.4922625268809497e-05\n",
            "Epoch  30 Batch  154 / 525  Training Loss  2.6524468921707012e-05\n",
            "Epoch  30 Batch  155 / 525  Training Loss  2.3824824893381447e-05\n",
            "Epoch  30 Batch  156 / 525  Training Loss  2.232996848761104e-05\n",
            "Epoch  30 Batch  157 / 525  Training Loss  2.0433164536370896e-05\n",
            "Epoch  30 Batch  158 / 525  Training Loss  1.9231621990911663e-05\n",
            "Epoch  30 Batch  159 / 525  Training Loss  2.5678786187199876e-05\n",
            "Epoch  30 Batch  160 / 525  Training Loss  2.0377277905936353e-05\n",
            "Epoch  30 Batch  161 / 525  Training Loss  2.2460815671365708e-05\n",
            "Epoch  30 Batch  162 / 525  Training Loss  1.7745023797033355e-05\n",
            "Epoch  30 Batch  163 / 525  Training Loss  2.8043326892657205e-05\n",
            "Epoch  30 Batch  164 / 525  Training Loss  1.9019660612684675e-05\n",
            "Epoch  30 Batch  165 / 525  Training Loss  2.0752439013449475e-05\n",
            "Epoch  30 Batch  166 / 525  Training Loss  2.2695236111758277e-05\n",
            "Epoch  30 Batch  167 / 525  Training Loss  2.5044417270692065e-05\n",
            "Epoch  30 Batch  168 / 525  Training Loss  2.2328380509861745e-05\n",
            "Epoch  30 Batch  169 / 525  Training Loss  2.602406675578095e-05\n",
            "Epoch  30 Batch  170 / 525  Training Loss  2.2204556444194168e-05\n",
            "Epoch  30 Batch  171 / 525  Training Loss  2.8002896215184592e-05\n",
            "Epoch  30 Batch  172 / 525  Training Loss  2.471454718033783e-05\n",
            "Epoch  30 Batch  173 / 525  Training Loss  2.7500855139805935e-05\n",
            "Epoch  30 Batch  174 / 525  Training Loss  2.4392345949308947e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  30 Batch  175 / 525  Training Loss  2.4753633624641225e-05\n",
            "Epoch  30 Batch  176 / 525  Training Loss  2.151489934476558e-05\n",
            "Epoch  30 Batch  177 / 525  Training Loss  1.945259100466501e-05\n",
            "Epoch  30 Batch  178 / 525  Training Loss  3.330807521706447e-05\n",
            "Epoch  30 Batch  179 / 525  Training Loss  2.7157822842127644e-05\n",
            "Epoch  30 Batch  180 / 525  Training Loss  2.125474566128105e-05\n",
            "Epoch  30 Batch  181 / 525  Training Loss  2.0766172383446246e-05\n",
            "Epoch  30 Batch  182 / 525  Training Loss  1.9666229491122067e-05\n",
            "Epoch  30 Batch  183 / 525  Training Loss  2.0253750335541554e-05\n",
            "Epoch  30 Batch  184 / 525  Training Loss  2.0464964109123684e-05\n",
            "Epoch  30 Batch  185 / 525  Training Loss  2.0664334442699328e-05\n",
            "Epoch  30 Batch  186 / 525  Training Loss  2.789017526083626e-05\n",
            "Epoch  30 Batch  187 / 525  Training Loss  2.1164580175536685e-05\n",
            "Epoch  30 Batch  188 / 525  Training Loss  2.4657716494402848e-05\n",
            "Epoch  30 Batch  189 / 525  Training Loss  2.748804763541557e-05\n",
            "Epoch  30 Batch  190 / 525  Training Loss  2.2083577277953736e-05\n",
            "Epoch  30 Batch  191 / 525  Training Loss  2.0880290321656503e-05\n",
            "Epoch  30 Batch  192 / 525  Training Loss  1.6697211322025396e-05\n",
            "Epoch  30 Batch  193 / 525  Training Loss  3.19801147270482e-05\n",
            "Epoch  30 Batch  194 / 525  Training Loss  2.400110497546848e-05\n",
            "Epoch  30 Batch  195 / 525  Training Loss  2.8783664674847387e-05\n",
            "Epoch  30 Batch  196 / 525  Training Loss  2.612916796351783e-05\n",
            "Epoch  30 Batch  197 / 525  Training Loss  2.299200241395738e-05\n",
            "Epoch  30 Batch  198 / 525  Training Loss  2.6115663786185905e-05\n",
            "Epoch  30 Batch  199 / 525  Training Loss  1.8971320969285443e-05\n",
            "Epoch  30 Batch  200 / 525  Training Loss  2.5641926185926422e-05\n",
            "Epoch  30 Batch  201 / 525  Training Loss  2.13381317735184e-05\n",
            "Epoch  30 Batch  202 / 525  Training Loss  2.6232981326757e-05\n",
            "Epoch  30 Batch  203 / 525  Training Loss  1.6478752513648942e-05\n",
            "Epoch  30 Batch  204 / 525  Training Loss  2.062875500996597e-05\n",
            "Epoch  30 Batch  205 / 525  Training Loss  2.3773551220074296e-05\n",
            "Epoch  30 Batch  206 / 525  Training Loss  2.058487370959483e-05\n",
            "Epoch  30 Batch  207 / 525  Training Loss  1.8970495148096234e-05\n",
            "Epoch  30 Batch  208 / 525  Training Loss  2.6466683266335167e-05\n",
            "Epoch  30 Batch  209 / 525  Training Loss  2.024953209911473e-05\n",
            "Epoch  30 Batch  210 / 525  Training Loss  2.6569530746201053e-05\n",
            "Epoch  30 Batch  211 / 525  Training Loss  2.6688267098506913e-05\n",
            "Epoch  30 Batch  212 / 525  Training Loss  2.135959039151203e-05\n",
            "Epoch  30 Batch  213 / 525  Training Loss  2.11674614547519e-05\n",
            "Epoch  30 Batch  214 / 525  Training Loss  2.588162897154689e-05\n",
            "Epoch  30 Batch  215 / 525  Training Loss  2.978197517222725e-05\n",
            "Epoch  30 Batch  216 / 525  Training Loss  2.6828880436369218e-05\n",
            "Epoch  30 Batch  217 / 525  Training Loss  2.3341130145126954e-05\n",
            "Epoch  30 Batch  218 / 525  Training Loss  2.5990791982621886e-05\n",
            "Epoch  30 Batch  219 / 525  Training Loss  2.0244358893251047e-05\n",
            "Epoch  30 Batch  220 / 525  Training Loss  2.4669836420798674e-05\n",
            "Epoch  30 Batch  221 / 525  Training Loss  2.541932371968869e-05\n",
            "Epoch  30 Batch  222 / 525  Training Loss  1.8384067516308278e-05\n",
            "Epoch  30 Batch  223 / 525  Training Loss  2.9295499189174734e-05\n",
            "Epoch  30 Batch  224 / 525  Training Loss  2.338175727345515e-05\n",
            "Epoch  30 Batch  225 / 525  Training Loss  2.2510957933263853e-05\n",
            "Epoch  30 Batch  226 / 525  Training Loss  2.5400338927283883e-05\n",
            "Epoch  30 Batch  227 / 525  Training Loss  2.938891338999383e-05\n",
            "Epoch  30 Batch  228 / 525  Training Loss  1.4609349818783812e-05\n",
            "Epoch  30 Batch  229 / 525  Training Loss  1.5636307580280118e-05\n",
            "Epoch  30 Batch  230 / 525  Training Loss  2.1384585124906152e-05\n",
            "Epoch  30 Batch  231 / 525  Training Loss  2.704280268517323e-05\n",
            "Epoch  30 Batch  232 / 525  Training Loss  2.3277216314454563e-05\n",
            "Epoch  30 Batch  233 / 525  Training Loss  2.2173397155711427e-05\n",
            "Epoch  30 Batch  234 / 525  Training Loss  2.08588462555781e-05\n",
            "Epoch  30 Batch  235 / 525  Training Loss  2.250839679618366e-05\n",
            "Epoch  30 Batch  236 / 525  Training Loss  2.0344556105555966e-05\n",
            "Epoch  30 Batch  237 / 525  Training Loss  2.5695126169011928e-05\n",
            "Epoch  30 Batch  238 / 525  Training Loss  2.5859077140921727e-05\n",
            "Epoch  30 Batch  239 / 525  Training Loss  2.7913551093661226e-05\n",
            "Epoch  30 Batch  240 / 525  Training Loss  2.037578815361485e-05\n",
            "Epoch  30 Batch  241 / 525  Training Loss  1.3728313206229359e-05\n",
            "Epoch  30 Batch  242 / 525  Training Loss  2.0947607481502928e-05\n",
            "Epoch  30 Batch  243 / 525  Training Loss  2.1774674678454176e-05\n",
            "Epoch  30 Batch  244 / 525  Training Loss  2.338161357329227e-05\n",
            "Epoch  30 Batch  245 / 525  Training Loss  2.148169733118266e-05\n",
            "Epoch  30 Batch  246 / 525  Training Loss  2.0991388737456873e-05\n",
            "Epoch  30 Batch  247 / 525  Training Loss  2.8223987101227976e-05\n",
            "Epoch  30 Batch  248 / 525  Training Loss  1.9690596673171967e-05\n",
            "Epoch  30 Batch  249 / 525  Training Loss  2.042418418568559e-05\n",
            "Epoch  30 Batch  250 / 525  Training Loss  1.7418500647181645e-05\n",
            "Epoch  30 Batch  251 / 525  Training Loss  2.459283496136777e-05\n",
            "Epoch  30 Batch  252 / 525  Training Loss  1.8673690647119656e-05\n",
            "Epoch  30 Batch  253 / 525  Training Loss  2.0974872313672677e-05\n",
            "Epoch  30 Batch  254 / 525  Training Loss  2.240051435364876e-05\n",
            "Epoch  30 Batch  255 / 525  Training Loss  1.4936444131308235e-05\n",
            "Epoch  30 Batch  256 / 525  Training Loss  2.6916450224234723e-05\n",
            "Epoch  30 Batch  257 / 525  Training Loss  2.163363024010323e-05\n",
            "Epoch  30 Batch  258 / 525  Training Loss  1.3340514669835102e-05\n",
            "Epoch  30 Batch  259 / 525  Training Loss  1.99726036953507e-05\n",
            "Epoch  30 Batch  260 / 525  Training Loss  2.466420119162649e-05\n",
            "Epoch  30 Batch  261 / 525  Training Loss  2.8506372473202646e-05\n",
            "Epoch  30 Batch  262 / 525  Training Loss  2.0062845578650013e-05\n",
            "Epoch  30 Batch  263 / 525  Training Loss  1.693733429419808e-05\n",
            "Epoch  30 Batch  264 / 525  Training Loss  2.5547420591465198e-05\n",
            "Epoch  30 Batch  265 / 525  Training Loss  2.970665809698403e-05\n",
            "Epoch  30 Batch  266 / 525  Training Loss  2.257117193948943e-05\n",
            "Epoch  30 Batch  267 / 525  Training Loss  2.228384983027354e-05\n",
            "Epoch  30 Batch  268 / 525  Training Loss  4.059340062667616e-05\n",
            "Epoch  30 Batch  269 / 525  Training Loss  1.895779132610187e-05\n",
            "Epoch  30 Batch  270 / 525  Training Loss  2.064837099169381e-05\n",
            "Epoch  30 Batch  271 / 525  Training Loss  2.945265077869408e-05\n",
            "Epoch  30 Batch  272 / 525  Training Loss  1.9222614355385303e-05\n",
            "Epoch  30 Batch  273 / 525  Training Loss  1.838045136537403e-05\n",
            "Epoch  30 Batch  274 / 525  Training Loss  2.0014364054077305e-05\n",
            "Epoch  30 Batch  275 / 525  Training Loss  2.0416107872733846e-05\n",
            "Epoch  30 Batch  276 / 525  Training Loss  2.6716437787399627e-05\n",
            "Epoch  30 Batch  277 / 525  Training Loss  2.9183463993831538e-05\n",
            "Epoch  30 Batch  278 / 525  Training Loss  2.001833126996644e-05\n",
            "Epoch  30 Batch  279 / 525  Training Loss  2.199187656515278e-05\n",
            "Epoch  30 Batch  280 / 525  Training Loss  2.236186082882341e-05\n",
            "Epoch  30 Batch  281 / 525  Training Loss  2.2693999198963866e-05\n",
            "Epoch  30 Batch  282 / 525  Training Loss  1.740909465297591e-05\n",
            "Epoch  30 Batch  283 / 525  Training Loss  1.9074015654041432e-05\n",
            "Epoch  30 Batch  284 / 525  Training Loss  2.356126242375467e-05\n",
            "Epoch  30 Batch  285 / 525  Training Loss  2.6553336283541285e-05\n",
            "Epoch  30 Batch  286 / 525  Training Loss  2.4469345589750446e-05\n",
            "Epoch  30 Batch  287 / 525  Training Loss  2.3954175048856996e-05\n",
            "Epoch  30 Batch  288 / 525  Training Loss  2.0787974790437147e-05\n",
            "Epoch  30 Batch  289 / 525  Training Loss  2.0394863895489834e-05\n",
            "Epoch  30 Batch  290 / 525  Training Loss  2.708212377910968e-05\n",
            "Epoch  30 Batch  291 / 525  Training Loss  2.6946061552735046e-05\n",
            "Epoch  30 Batch  292 / 525  Training Loss  2.3230195438372903e-05\n",
            "Epoch  30 Batch  293 / 525  Training Loss  2.4269738787552342e-05\n",
            "Epoch  30 Batch  294 / 525  Training Loss  3.058024594793096e-05\n",
            "Epoch  30 Batch  295 / 525  Training Loss  2.4236574972746894e-05\n",
            "Epoch  30 Batch  296 / 525  Training Loss  2.3968877940205857e-05\n",
            "Epoch  30 Batch  297 / 525  Training Loss  2.548454540374223e-05\n",
            "Epoch  30 Batch  298 / 525  Training Loss  3.3082742447732016e-05\n",
            "Epoch  30 Batch  299 / 525  Training Loss  2.2389314835891128e-05\n",
            "Epoch  30 Batch  300 / 525  Training Loss  2.4033739464357495e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  30 Batch  301 / 525  Training Loss  2.4093718820950016e-05\n",
            "Epoch  30 Batch  302 / 525  Training Loss  2.581080116215162e-05\n",
            "Epoch  30 Batch  303 / 525  Training Loss  2.1539119188673794e-05\n",
            "Epoch  30 Batch  304 / 525  Training Loss  2.6790312404045835e-05\n",
            "Epoch  30 Batch  305 / 525  Training Loss  1.9855184291373007e-05\n",
            "Epoch  30 Batch  306 / 525  Training Loss  3.1494251743424684e-05\n",
            "Epoch  30 Batch  307 / 525  Training Loss  1.6282341675832868e-05\n",
            "Epoch  30 Batch  308 / 525  Training Loss  2.4325678168679588e-05\n",
            "Epoch  30 Batch  309 / 525  Training Loss  2.937028148153331e-05\n",
            "Epoch  30 Batch  310 / 525  Training Loss  2.2211532268556766e-05\n",
            "Epoch  30 Batch  311 / 525  Training Loss  2.856633727787994e-05\n",
            "Epoch  30 Batch  312 / 525  Training Loss  1.5063766113598831e-05\n",
            "Epoch  30 Batch  313 / 525  Training Loss  2.0715966456918977e-05\n",
            "Epoch  30 Batch  314 / 525  Training Loss  2.5699493562569842e-05\n",
            "Epoch  30 Batch  315 / 525  Training Loss  1.8853106666938402e-05\n",
            "Epoch  30 Batch  316 / 525  Training Loss  2.4078626665868796e-05\n",
            "Epoch  30 Batch  317 / 525  Training Loss  2.310221316292882e-05\n",
            "Epoch  30 Batch  318 / 525  Training Loss  2.478324131516274e-05\n",
            "Epoch  30 Batch  319 / 525  Training Loss  1.8170734620071016e-05\n",
            "Epoch  30 Batch  320 / 525  Training Loss  2.0637837224057876e-05\n",
            "Epoch  30 Batch  321 / 525  Training Loss  2.9057158826617524e-05\n",
            "Epoch  30 Batch  322 / 525  Training Loss  1.9184950360795483e-05\n",
            "Epoch  30 Batch  323 / 525  Training Loss  2.1873300283914432e-05\n",
            "Epoch  30 Batch  324 / 525  Training Loss  2.4282560843857937e-05\n",
            "Epoch  30 Batch  325 / 525  Training Loss  1.882919059426058e-05\n",
            "Epoch  30 Batch  326 / 525  Training Loss  2.030391806329135e-05\n",
            "Epoch  30 Batch  327 / 525  Training Loss  2.2724143491359428e-05\n",
            "Epoch  30 Batch  328 / 525  Training Loss  2.47157622652594e-05\n",
            "Epoch  30 Batch  329 / 525  Training Loss  1.9644163330667652e-05\n",
            "Epoch  30 Batch  330 / 525  Training Loss  2.2780335712013766e-05\n",
            "Epoch  30 Batch  331 / 525  Training Loss  3.130830737063661e-05\n",
            "Epoch  30 Batch  332 / 525  Training Loss  1.9120303477393463e-05\n",
            "Epoch  30 Batch  333 / 525  Training Loss  1.817962584027555e-05\n",
            "Epoch  30 Batch  334 / 525  Training Loss  2.513100298529025e-05\n",
            "Epoch  30 Batch  335 / 525  Training Loss  1.9207736841053702e-05\n",
            "Epoch  30 Batch  336 / 525  Training Loss  2.7926231268793344e-05\n",
            "Epoch  30 Batch  337 / 525  Training Loss  3.1768046028446406e-05\n",
            "Epoch  30 Batch  338 / 525  Training Loss  1.916977998916991e-05\n",
            "Epoch  30 Batch  339 / 525  Training Loss  2.3484306439058855e-05\n",
            "Epoch  30 Batch  340 / 525  Training Loss  2.610419142001774e-05\n",
            "Epoch  30 Batch  341 / 525  Training Loss  2.3900049200165085e-05\n",
            "Epoch  30 Batch  342 / 525  Training Loss  2.0288387531763874e-05\n",
            "Epoch  30 Batch  343 / 525  Training Loss  2.3596281607751735e-05\n",
            "Epoch  30 Batch  344 / 525  Training Loss  2.6996614906238392e-05\n",
            "Epoch  30 Batch  345 / 525  Training Loss  2.6032619643956423e-05\n",
            "Epoch  30 Batch  346 / 525  Training Loss  2.1951436792733148e-05\n",
            "Epoch  30 Batch  347 / 525  Training Loss  2.3440214135916904e-05\n",
            "Epoch  30 Batch  348 / 525  Training Loss  3.0104234610917047e-05\n",
            "Epoch  30 Batch  349 / 525  Training Loss  2.7297617634758353e-05\n",
            "Epoch  30 Batch  350 / 525  Training Loss  2.714712536544539e-05\n",
            "Epoch  30 Batch  351 / 525  Training Loss  1.5899622667348012e-05\n",
            "Epoch  30 Batch  352 / 525  Training Loss  2.654552190506365e-05\n",
            "Epoch  30 Batch  353 / 525  Training Loss  2.40674453380052e-05\n",
            "Epoch  30 Batch  354 / 525  Training Loss  2.0635883629438467e-05\n",
            "Epoch  30 Batch  355 / 525  Training Loss  1.640725531615317e-05\n",
            "Epoch  30 Batch  356 / 525  Training Loss  3.072340041399002e-05\n",
            "Epoch  30 Batch  357 / 525  Training Loss  2.12055765587138e-05\n",
            "Epoch  30 Batch  358 / 525  Training Loss  2.3680227968725376e-05\n",
            "Epoch  30 Batch  359 / 525  Training Loss  3.2421507057733834e-05\n",
            "Epoch  30 Batch  360 / 525  Training Loss  2.5470964828855358e-05\n",
            "Epoch  30 Batch  361 / 525  Training Loss  2.3965701984707266e-05\n",
            "Epoch  30 Batch  362 / 525  Training Loss  2.1494408429134637e-05\n",
            "Epoch  30 Batch  363 / 525  Training Loss  2.646730536071118e-05\n",
            "Epoch  30 Batch  364 / 525  Training Loss  1.7952381313079968e-05\n",
            "Epoch  30 Batch  365 / 525  Training Loss  2.5108669433393516e-05\n",
            "Epoch  30 Batch  366 / 525  Training Loss  1.948163662746083e-05\n",
            "Epoch  30 Batch  367 / 525  Training Loss  1.9512730432325043e-05\n",
            "Epoch  30 Batch  368 / 525  Training Loss  3.1332696380559355e-05\n",
            "Epoch  30 Batch  369 / 525  Training Loss  2.1963502149446867e-05\n",
            "Epoch  30 Batch  370 / 525  Training Loss  2.2838303266325966e-05\n",
            "Epoch  30 Batch  371 / 525  Training Loss  2.553661397541873e-05\n",
            "Epoch  30 Batch  372 / 525  Training Loss  3.1643696274841204e-05\n",
            "Epoch  30 Batch  373 / 525  Training Loss  1.8456801626598462e-05\n",
            "Epoch  30 Batch  374 / 525  Training Loss  2.8946009479113854e-05\n",
            "Epoch  30 Batch  375 / 525  Training Loss  2.8069858672097325e-05\n",
            "Epoch  30 Batch  376 / 525  Training Loss  1.7881289750221185e-05\n",
            "Epoch  30 Batch  377 / 525  Training Loss  2.3005712137091905e-05\n",
            "Epoch  30 Batch  378 / 525  Training Loss  2.120502722391393e-05\n",
            "Epoch  30 Batch  379 / 525  Training Loss  1.758096186676994e-05\n",
            "Epoch  30 Batch  380 / 525  Training Loss  2.577866325736977e-05\n",
            "Epoch  30 Batch  381 / 525  Training Loss  2.2737140170647763e-05\n",
            "Epoch  30 Batch  382 / 525  Training Loss  2.3099126337911002e-05\n",
            "Epoch  30 Batch  383 / 525  Training Loss  2.9409711714833975e-05\n",
            "Epoch  30 Batch  384 / 525  Training Loss  2.139973184966948e-05\n",
            "Epoch  30 Batch  385 / 525  Training Loss  2.0635492546716705e-05\n",
            "Epoch  30 Batch  386 / 525  Training Loss  2.9007851480855606e-05\n",
            "Epoch  30 Batch  387 / 525  Training Loss  2.262708221678622e-05\n",
            "Epoch  30 Batch  388 / 525  Training Loss  2.7481150027597323e-05\n",
            "Epoch  30 Batch  389 / 525  Training Loss  1.9415314454818144e-05\n",
            "Epoch  30 Batch  390 / 525  Training Loss  1.9131093722535297e-05\n",
            "Epoch  30 Batch  391 / 525  Training Loss  2.3402264559990726e-05\n",
            "Epoch  30 Batch  392 / 525  Training Loss  2.5662953703431413e-05\n",
            "Epoch  30 Batch  393 / 525  Training Loss  2.1483207092387602e-05\n",
            "Epoch  30 Batch  394 / 525  Training Loss  2.166871854569763e-05\n",
            "Epoch  30 Batch  395 / 525  Training Loss  2.033697637671139e-05\n",
            "Epoch  30 Batch  396 / 525  Training Loss  2.237327498733066e-05\n",
            "Epoch  30 Batch  397 / 525  Training Loss  2.770858736766968e-05\n",
            "Epoch  30 Batch  398 / 525  Training Loss  1.734422767185606e-05\n",
            "Epoch  30 Batch  399 / 525  Training Loss  2.3402501028613187e-05\n",
            "Epoch  30 Batch  400 / 525  Training Loss  2.1449544874485582e-05\n",
            "Epoch  30 Batch  401 / 525  Training Loss  3.0289162168628536e-05\n",
            "Epoch  30 Batch  402 / 525  Training Loss  2.9110413379385136e-05\n",
            "Epoch  30 Batch  403 / 525  Training Loss  2.1500189177459106e-05\n",
            "Epoch  30 Batch  404 / 525  Training Loss  1.800377867766656e-05\n",
            "Epoch  30 Batch  405 / 525  Training Loss  2.211470200563781e-05\n",
            "Epoch  30 Batch  406 / 525  Training Loss  2.1033720258856192e-05\n",
            "Epoch  30 Batch  407 / 525  Training Loss  2.5732379071996547e-05\n",
            "Epoch  30 Batch  408 / 525  Training Loss  2.3659908038098365e-05\n",
            "Epoch  30 Batch  409 / 525  Training Loss  1.8766879293252714e-05\n",
            "Epoch  30 Batch  410 / 525  Training Loss  2.2190815798239782e-05\n",
            "Epoch  30 Batch  411 / 525  Training Loss  1.3417132322501857e-05\n",
            "Epoch  30 Batch  412 / 525  Training Loss  1.996312494156882e-05\n",
            "Epoch  30 Batch  413 / 525  Training Loss  1.7746435332810506e-05\n",
            "Epoch  30 Batch  414 / 525  Training Loss  2.084678635583259e-05\n",
            "Epoch  30 Batch  415 / 525  Training Loss  2.705761835386511e-05\n",
            "Epoch  30 Batch  416 / 525  Training Loss  2.9474216717062518e-05\n",
            "Epoch  30 Batch  417 / 525  Training Loss  2.0441337255761027e-05\n",
            "Epoch  30 Batch  418 / 525  Training Loss  2.9654478566953912e-05\n",
            "Epoch  30 Batch  419 / 525  Training Loss  1.9835073544527404e-05\n",
            "Epoch  30 Batch  420 / 525  Training Loss  2.960712845379021e-05\n",
            "Epoch  30 Batch  421 / 525  Training Loss  2.7628175303107128e-05\n",
            "Epoch  30 Batch  422 / 525  Training Loss  3.2162606657948345e-05\n",
            "Epoch  30 Batch  423 / 525  Training Loss  2.0803641746169887e-05\n",
            "Epoch  30 Batch  424 / 525  Training Loss  2.5642857508501038e-05\n",
            "Epoch  30 Batch  425 / 525  Training Loss  2.6691264793043956e-05\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch  30 Batch  426 / 525  Training Loss  1.9221432012273e-05\n",
            "Epoch  30 Batch  427 / 525  Training Loss  1.6495134332217276e-05\n",
            "Epoch  30 Batch  428 / 525  Training Loss  1.9125136532238685e-05\n",
            "Epoch  30 Batch  429 / 525  Training Loss  2.9980801627971232e-05\n",
            "Epoch  30 Batch  430 / 525  Training Loss  2.0714329366455786e-05\n",
            "Epoch  30 Batch  431 / 525  Training Loss  2.8467067750170827e-05\n",
            "Epoch  30 Batch  432 / 525  Training Loss  2.5043182176887058e-05\n",
            "Epoch  30 Batch  433 / 525  Training Loss  2.5268218450946733e-05\n",
            "Epoch  30 Batch  434 / 525  Training Loss  2.860740278265439e-05\n",
            "Epoch  30 Batch  435 / 525  Training Loss  1.7958136595552787e-05\n",
            "Epoch  30 Batch  436 / 525  Training Loss  2.136425973731093e-05\n",
            "Epoch  30 Batch  437 / 525  Training Loss  1.5975550923030823e-05\n",
            "Epoch  30 Batch  438 / 525  Training Loss  2.2462747438112274e-05\n",
            "Epoch  30 Batch  439 / 525  Training Loss  2.6637324481271207e-05\n",
            "Epoch  30 Batch  440 / 525  Training Loss  2.0196865079924464e-05\n",
            "Epoch  30 Batch  441 / 525  Training Loss  2.9953564080642536e-05\n",
            "Epoch  30 Batch  442 / 525  Training Loss  1.9609487935667858e-05\n",
            "Epoch  30 Batch  443 / 525  Training Loss  2.7653350116452202e-05\n",
            "Epoch  30 Batch  444 / 525  Training Loss  3.2787931559141725e-05\n",
            "Epoch  30 Batch  445 / 525  Training Loss  2.844307709892746e-05\n",
            "Epoch  30 Batch  446 / 525  Training Loss  2.83770750684198e-05\n",
            "Epoch  30 Batch  447 / 525  Training Loss  2.3955104552442208e-05\n",
            "Epoch  30 Batch  448 / 525  Training Loss  2.2714933948009275e-05\n",
            "Epoch  30 Batch  449 / 525  Training Loss  2.3153033907874487e-05\n",
            "Epoch  30 Batch  450 / 525  Training Loss  2.4707993361516856e-05\n",
            "Epoch  30 Batch  451 / 525  Training Loss  2.230153768323362e-05\n",
            "Epoch  30 Batch  452 / 525  Training Loss  1.869626976258587e-05\n",
            "Epoch  30 Batch  453 / 525  Training Loss  2.7596712243393995e-05\n",
            "Epoch  30 Batch  454 / 525  Training Loss  2.805386066029314e-05\n",
            "Epoch  30 Batch  455 / 525  Training Loss  2.23110691877082e-05\n",
            "Epoch  30 Batch  456 / 525  Training Loss  2.455939284118358e-05\n",
            "Epoch  30 Batch  457 / 525  Training Loss  2.8045713406754658e-05\n",
            "Epoch  30 Batch  458 / 525  Training Loss  1.6020285329432227e-05\n",
            "Epoch  30 Batch  459 / 525  Training Loss  1.6373329344787635e-05\n",
            "Epoch  30 Batch  460 / 525  Training Loss  2.2822096070740372e-05\n",
            "Epoch  30 Batch  461 / 525  Training Loss  2.2799402358941734e-05\n",
            "Epoch  30 Batch  462 / 525  Training Loss  2.628352740430273e-05\n",
            "Epoch  30 Batch  463 / 525  Training Loss  2.7494930691318586e-05\n",
            "Epoch  30 Batch  464 / 525  Training Loss  2.4923992896219715e-05\n",
            "Epoch  30 Batch  465 / 525  Training Loss  1.9111448636977002e-05\n",
            "Epoch  30 Batch  466 / 525  Training Loss  3.587772516766563e-05\n",
            "Epoch  30 Batch  467 / 525  Training Loss  2.2574755348614417e-05\n",
            "Epoch  30 Batch  468 / 525  Training Loss  2.1802114133606665e-05\n",
            "Epoch  30 Batch  469 / 525  Training Loss  3.2544317946303636e-05\n",
            "Epoch  30 Batch  470 / 525  Training Loss  2.1531190213863738e-05\n",
            "Epoch  30 Batch  471 / 525  Training Loss  1.9110078937956132e-05\n",
            "Epoch  30 Batch  472 / 525  Training Loss  1.591955151525326e-05\n",
            "Epoch  30 Batch  473 / 525  Training Loss  2.395419323875103e-05\n",
            "Epoch  30 Batch  474 / 525  Training Loss  2.2100761270849034e-05\n",
            "Epoch  30 Batch  475 / 525  Training Loss  2.3455224436474964e-05\n",
            "Epoch  30 Batch  476 / 525  Training Loss  2.9839919079677202e-05\n",
            "Epoch  30 Batch  477 / 525  Training Loss  2.25015937758144e-05\n",
            "Epoch  30 Batch  478 / 525  Training Loss  2.2660413378616795e-05\n",
            "Epoch  30 Batch  479 / 525  Training Loss  2.4027107428992167e-05\n",
            "Epoch  30 Batch  480 / 525  Training Loss  2.157396011170931e-05\n",
            "Epoch  30 Batch  481 / 525  Training Loss  2.2820231606601737e-05\n",
            "Epoch  30 Batch  482 / 525  Training Loss  1.7174485037685372e-05\n",
            "Epoch  30 Batch  483 / 525  Training Loss  2.165407749998849e-05\n",
            "Epoch  30 Batch  484 / 525  Training Loss  2.107811269524973e-05\n",
            "Epoch  30 Batch  485 / 525  Training Loss  2.5992310838773847e-05\n",
            "Epoch  30 Batch  486 / 525  Training Loss  1.7448848666390404e-05\n",
            "Epoch  30 Batch  487 / 525  Training Loss  2.608486101962626e-05\n",
            "Epoch  30 Batch  488 / 525  Training Loss  2.0023306205985136e-05\n",
            "Epoch  30 Batch  489 / 525  Training Loss  2.4590950488345698e-05\n",
            "Epoch  30 Batch  490 / 525  Training Loss  2.3703101760474965e-05\n",
            "Epoch  30 Batch  491 / 525  Training Loss  2.6759293177747168e-05\n",
            "Epoch  30 Batch  492 / 525  Training Loss  2.5791156076593325e-05\n",
            "Epoch  30 Batch  493 / 525  Training Loss  2.38340853684349e-05\n",
            "Epoch  30 Batch  494 / 525  Training Loss  2.4715467588976026e-05\n",
            "Epoch  30 Batch  495 / 525  Training Loss  2.37056337937247e-05\n",
            "Epoch  30 Batch  496 / 525  Training Loss  2.3439599317498505e-05\n",
            "Epoch  30 Batch  497 / 525  Training Loss  2.226389551651664e-05\n",
            "Epoch  30 Batch  498 / 525  Training Loss  3.0210172553779557e-05\n",
            "Epoch  30 Batch  499 / 525  Training Loss  2.7799955205409788e-05\n",
            "Epoch  30 Batch  500 / 525  Training Loss  2.752077125478536e-05\n",
            "Epoch  30 Batch  501 / 525  Training Loss  2.2883370547788218e-05\n",
            "Epoch  30 Batch  502 / 525  Training Loss  2.2461430489784107e-05\n",
            "Epoch  30 Batch  503 / 525  Training Loss  1.645296833885368e-05\n",
            "Epoch  30 Batch  504 / 525  Training Loss  2.774416316242423e-05\n",
            "Epoch  30 Batch  505 / 525  Training Loss  2.6774087018566206e-05\n",
            "Epoch  30 Batch  506 / 525  Training Loss  2.112405672960449e-05\n",
            "Epoch  30 Batch  507 / 525  Training Loss  2.4975193809950724e-05\n",
            "Epoch  30 Batch  508 / 525  Training Loss  2.0466861315071583e-05\n",
            "Epoch  30 Batch  509 / 525  Training Loss  2.7166199288330972e-05\n",
            "Epoch  30 Batch  510 / 525  Training Loss  2.323347143828869e-05\n",
            "Epoch  30 Batch  511 / 525  Training Loss  2.7764137485064566e-05\n",
            "Epoch  30 Batch  512 / 525  Training Loss  1.9443299606791697e-05\n",
            "Epoch  30 Batch  513 / 525  Training Loss  2.4860408302629367e-05\n",
            "Epoch  30 Batch  514 / 525  Training Loss  2.068542926281225e-05\n",
            "Epoch  30 Batch  515 / 525  Training Loss  2.741608113865368e-05\n",
            "Epoch  30 Batch  516 / 525  Training Loss  1.459286613680888e-05\n",
            "Epoch  30 Batch  517 / 525  Training Loss  2.3334476281888783e-05\n",
            "Epoch  30 Batch  518 / 525  Training Loss  2.298043546034023e-05\n",
            "Epoch  30 Batch  519 / 525  Training Loss  2.6228834030916914e-05\n",
            "Epoch  30 Batch  520 / 525  Training Loss  2.2794067263021134e-05\n",
            "Epoch  30 Batch  521 / 525  Training Loss  1.8913769963546656e-05\n",
            "Epoch  30 Batch  522 / 525  Training Loss  1.8270717191626318e-05\n",
            "Epoch  30 Batch  523 / 525  Training Loss  1.6241861885646358e-05\n",
            "Epoch  30 Batch  524 / 525  Training Loss  2.1270603610901162e-05\n",
            "  31    |    -    |   0.000024   |   64.08  \n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkRUKKJKpM52",
        "outputId": "b3b66653-d111-4c1e-c98e-5f5cc0f29dbf"
      },
      "source": [
        "evaluate(model, val_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "64.38333333333334"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amCbnsu2pM52",
        "outputId": "87783609-6d57-4543-b145-26d9c3f14be9"
      },
      "source": [
        "evaluate(model, test_dataloader)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "64.89285714285714"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YtLkSgAp4Ku"
      },
      "source": [
        "final_list = []\n",
        "for index, row in data_test_inter.iterrows():\n",
        "    tmp = predictSum(row['inputs'], row['target'])\n",
        "    final_list.append(tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwm8sZTZp_20"
      },
      "source": [
        "df_final = pd.DataFrame(final_list, columns=[\"text\", \"min_len\", \"max_len\", \"predict_len\", \"target\", \"predictions\"])\n",
        "df_final.to_csv('results_3num.csv', index = False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1kk-S-NoNeP"
      },
      "source": [
        "## Evaluating - Extrapolation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDkHtkROpQxB"
      },
      "source": [
        "data = pd.read_csv('finetune_extra_dataset.csv', header=None, names=['inputs', 'target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8r7hy5roNeP"
      },
      "source": [
        "data_extra = data.sample(n = 40000, random_state = 42).reset_index(drop=True)\n",
        "\n",
        "train_extra, validation_extra = train_test_split(data_extra, test_size=0.3, random_state=42)\n",
        "train_extra, test_extra = train_test_split(train_extra, test_size=0.4, random_state=42)\n",
        "\n",
        "data_train_extra = train_extra.reset_index(drop=True)\n",
        "data_valid_extra = validation_extra.reset_index(drop=True)\n",
        "data_test_extra = test_extra.reset_index(drop=True)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "train_inputs_extra, train_masks_extra = get_word_embeddings(data_train_extra['inputs'])\n",
        "val_inputs_extra, val_masks_extra = get_word_embeddings(data_valid_extra['inputs'])\n",
        "test_inputs_extra, test_masks_extra = get_word_embeddings(data_test_extra['inputs'])\n",
        "\n",
        "data_train_extra['target_str'] = data_train_extra['target'].astype(str)\n",
        "data_valid_extra['target_str'] = data_valid_extra['target'].astype(str)\n",
        "data_test_extra['target_str'] = data_test_extra['target'].astype(str)\n",
        "\n",
        "#convert lists to tensors\n",
        "train_labels_extra = get_word_embeddings(data_train_extra['target_str'])[0]\n",
        "val_labels_extra = get_word_embeddings(data_valid_extra['target_str'])[0]\n",
        "test_labels_extra = get_word_embeddings(data_test_extra['target_str'])[0]\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data_extra = TensorDataset(train_inputs_extra, train_masks_extra, train_labels_extra)\n",
        "train_dataloader_extra = DataLoader(train_data_extra, shuffle = True, batch_size = batch_size)\n",
        "\n",
        "val_data_extra = TensorDataset(val_inputs_extra, val_masks_extra, val_labels_extra)\n",
        "val_dataloader_extra = DataLoader(val_data_extra, shuffle = True, batch_size = batch_size)\n",
        "\n",
        "test_data_extra = TensorDataset(test_inputs_extra, test_masks_extra, test_labels_extra)\n",
        "test_dataloader_extra = DataLoader(test_data_extra, shuffle = True, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YNTfGsHoNeP"
      },
      "source": [
        "data_inputs_extra, data_masks_extra = get_word_embeddings(data_extra['inputs'])\n",
        "data_extra['target_str'] = data_extra['target'].astype(str)\n",
        "data_labels_extra = get_word_embeddings(data_extra['target_str'])[0]\n",
        "data_extra_loader = TensorDataset(data_inputs_extra, data_masks_extra, data_labels_extra)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRK4j0WWoNeP",
        "outputId": "02068fab-9bbd-41da-8ece-c26fbdb9d8bd"
      },
      "source": [
        "evaluate(model, train_dataloader_extra)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "551WeUjBoNeQ",
        "outputId": "6e7b4dbb-b6b9-4dea-aba4-4ed350c347e5"
      },
      "source": [
        "evaluate(model, val_dataloader_extra)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsOsu9weoNeQ",
        "outputId": "18cb3e6d-776f-4e12-8f95-f8d5f9887dd8"
      },
      "source": [
        "evaluate(model, test_dataloader_extra)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3NkZRq4qV5I"
      },
      "source": [
        "## Fine Tuning - With Bias"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BP7ijTcSrmoD"
      },
      "source": [
        "num_of_epochs = 30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kEYVrhQrpuL"
      },
      "source": [
        "# Loading the configuration file for 't5-base' model\n",
        "!wget https://s3.amazonaws.com/models.huggingface.co/bert/t5-base-config.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f49hnh6eryvv"
      },
      "source": [
        "model = T5ForConditionalGeneration.from_pretrained('Pretrain_NPM_3op.bin', return_dict=True, config='t5-base-config.json')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZtSAwmWsSEO"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmtUYJrzsZ16"
      },
      "source": [
        "data = pd.read_csv('finetune_bias_dataset.csv', header=None, names=['inputs', 'target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CRvyQ7nPsd1s"
      },
      "source": [
        "MAX_LEN = 20 \n",
        "\n",
        "data_inter = data.sample(n = 30000, random_state = 42).reset_index(drop=True)\n",
        "data_bias_train = bias[:3000]\n",
        "data_bias_test = bias[3000:]\n",
        "\n",
        "train_inter, validation_inter = train_test_split(data_inter, test_size=0.3, random_state=42)\n",
        "train_inter, test_inter = train_test_split(train_inter, test_size=0.4, random_state=42)\n",
        "\n",
        "frames_train = [train_inter, data_bias_train]\n",
        "train_inter = pd.concat(frames_train)\n",
        "frames_test = [test_inter, data_bias_test]\n",
        "test_inter = pd.concat(frames_test)\n",
        "\n",
        "data_train_inter = train_inter.reset_index(drop=True)\n",
        "data_valid_inter = validation_inter.reset_index(drop=True)\n",
        "data_test_inter = test_inter.reset_index(drop=True)\n",
        "\n",
        "data_train_inter['target_str'] = data_train_inter['target'].astype(str)\n",
        "data_valid_inter['target_str'] = data_valid_inter['target'].astype(str)\n",
        "data_test_inter['target_str'] = data_test_inter['target'].astype(str)\n",
        "data_train_inter['input_str'] = data_train_inter['inputs'].astype(str)\n",
        "data_valid_inter['input_str'] = data_valid_inter['inputs'].astype(str)\n",
        "data_test_inter['input_str'] = data_test_inter['inputs'].astype(str)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "train_inputs_inter, train_masks_inter = get_word_embeddings(data_train_inter['inputs'])\n",
        "val_inputs_inter, val_masks_inter = get_word_embeddings(data_valid_inter['inputs'])\n",
        "test_inputs_inter, test_masks_inter = get_word_embeddings(data_test_inter['inputs'])\n",
        "\n",
        "\n",
        "#convert lists to tensors\n",
        "train_labels_inter = get_word_embeddings(data_train_inter['target_str'])[0]\n",
        "val_labels_inter = get_word_embeddings(data_valid_inter['target_str'])[0]\n",
        "test_labels_inter = get_word_embeddings(data_test_inter['target_str'])[0]\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_data_inter = TensorDataset(train_inputs_inter, train_masks_inter, train_labels_inter)\n",
        "train_dataloader_inter = DataLoader(train_data_inter, shuffle = True, batch_size = batch_size)\n",
        "\n",
        "val_data_inter = TensorDataset(val_inputs_inter, val_masks_inter, val_labels_inter)\n",
        "val_dataloader_inter = DataLoader(val_data_inter, shuffle = True, batch_size = batch_size)\n",
        "\n",
        "test_data_inter = TensorDataset(test_inputs_inter, test_masks_inter, test_labels_inter)\n",
        "test_dataloader_inter = DataLoader(test_data_inter, shuffle = True, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6V1LMjzt4Rl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71d1f57-ac28-4ca7-bff4-8eafd7eb6cf2"
      },
      "source": [
        "import gc\n",
        "\n",
        "val_acc = 0\n",
        "train_accuracy = 0\n",
        "\n",
        "# Sets the module in training mode\n",
        "model.train()\n",
        "\n",
        "for epoch in range(1,num_of_epochs+1):\n",
        "    print('Running epoch: {}'.format(epoch))\n",
        "    running_loss=0\n",
        "    # out = display(progress(1, num_of_batches+1), display_id=True)\n",
        "    i =0 \n",
        "    for batch in train_dataloader_inter:\n",
        "        \n",
        "        input_ids, attn_mask, labels = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        # clear out the gradients of all Variables \n",
        "        optimizer.zero_grad()\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Forward propogation\n",
        "        # print(model(input_ids=input_ids, attention_mask=attn_mask, labels=labels))\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attn_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        loss_num=loss.item()\n",
        "        logits = outputs.logits\n",
        "        running_loss+=loss_num\n",
        "        # out.update(progress(loss_num,i, num_of_batches+1))\n",
        "\n",
        "        # calculating the gradients\n",
        "        loss.backward()\n",
        "\n",
        "        # updating the params\n",
        "        optimizer.step()\n",
        "\n",
        "        print(\"Epoch \", epoch, \"Batch \", i, \"/\", len(train_dataloader_inter), \" Training Loss \", loss_num)\n",
        "        i += 1\n",
        "\n",
        "    running_loss = running_loss/len(train_dataloader_inter)\n",
        "    # v_input_ids, v_attn_mask, v_labels = tuple(t.to(device) for t in data_valid)\n",
        "  \n",
        "    curr_accuracy = evaluate(model, val_dataloader_inter)\n",
        "\n",
        "    # print('Epoch: {} , Running loss: {}'.format(epoch,running_loss))\n",
        "    print(f\"{epoch + 1:^7} | {'-':^7} | {running_loss:^12.6f} | {curr_accuracy:^9.6f}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "    if curr_accuracy > val_acc:\n",
        "        val_acc = curr_accuracy\n",
        "        # Saving the best model\n",
        "        torch.save(model.state_dict(),'Best_Bias_FT_Pretrain_3op.bin')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch  15 Batch  400 / 488  Training Loss  0.006016754545271397\n",
            "Epoch  15 Batch  401 / 488  Training Loss  0.011933837085962296\n",
            "Epoch  15 Batch  402 / 488  Training Loss  0.00817395094782114\n",
            "Epoch  15 Batch  403 / 488  Training Loss  0.013663271442055702\n",
            "Epoch  15 Batch  404 / 488  Training Loss  0.007184218615293503\n",
            "Epoch  15 Batch  405 / 488  Training Loss  0.009564608335494995\n",
            "Epoch  15 Batch  406 / 488  Training Loss  0.019703447818756104\n",
            "Epoch  15 Batch  407 / 488  Training Loss  0.014817175455391407\n",
            "Epoch  15 Batch  408 / 488  Training Loss  0.00771026685833931\n",
            "Epoch  15 Batch  409 / 488  Training Loss  0.00441845552995801\n",
            "Epoch  15 Batch  410 / 488  Training Loss  0.006308409385383129\n",
            "Epoch  15 Batch  411 / 488  Training Loss  0.007758758030831814\n",
            "Epoch  15 Batch  412 / 488  Training Loss  0.004104703664779663\n",
            "Epoch  15 Batch  413 / 488  Training Loss  0.00611467519775033\n",
            "Epoch  15 Batch  414 / 488  Training Loss  0.003519261721521616\n",
            "Epoch  15 Batch  415 / 488  Training Loss  0.005985450465232134\n",
            "Epoch  15 Batch  416 / 488  Training Loss  0.00512623181566596\n",
            "Epoch  15 Batch  417 / 488  Training Loss  0.007503475993871689\n",
            "Epoch  15 Batch  418 / 488  Training Loss  0.006594481412321329\n",
            "Epoch  15 Batch  419 / 488  Training Loss  0.008390940725803375\n",
            "Epoch  15 Batch  420 / 488  Training Loss  0.005579798016697168\n",
            "Epoch  15 Batch  421 / 488  Training Loss  0.005976405926048756\n",
            "Epoch  15 Batch  422 / 488  Training Loss  0.005794967990368605\n",
            "Epoch  15 Batch  423 / 488  Training Loss  0.006330876611173153\n",
            "Epoch  15 Batch  424 / 488  Training Loss  0.008817624300718307\n",
            "Epoch  15 Batch  425 / 488  Training Loss  0.004793455824255943\n",
            "Epoch  15 Batch  426 / 488  Training Loss  0.010290367528796196\n",
            "Epoch  15 Batch  427 / 488  Training Loss  0.018024789169430733\n",
            "Epoch  15 Batch  428 / 488  Training Loss  0.0067870416678488255\n",
            "Epoch  15 Batch  429 / 488  Training Loss  0.0038114837370812893\n",
            "Epoch  15 Batch  430 / 488  Training Loss  0.006076873280107975\n",
            "Epoch  15 Batch  431 / 488  Training Loss  0.005569134838879108\n",
            "Epoch  15 Batch  432 / 488  Training Loss  0.002606096211820841\n",
            "Epoch  15 Batch  433 / 488  Training Loss  0.007747962139546871\n",
            "Epoch  15 Batch  434 / 488  Training Loss  0.004383319057524204\n",
            "Epoch  15 Batch  435 / 488  Training Loss  0.0032781909685581923\n",
            "Epoch  15 Batch  436 / 488  Training Loss  0.009403896518051624\n",
            "Epoch  15 Batch  437 / 488  Training Loss  0.011696388013660908\n",
            "Epoch  15 Batch  438 / 488  Training Loss  0.017131861299276352\n",
            "Epoch  15 Batch  439 / 488  Training Loss  0.0031350646167993546\n",
            "Epoch  15 Batch  440 / 488  Training Loss  0.0056824940256774426\n",
            "Epoch  15 Batch  441 / 488  Training Loss  0.0062881046906113625\n",
            "Epoch  15 Batch  442 / 488  Training Loss  0.013095242902636528\n",
            "Epoch  15 Batch  443 / 488  Training Loss  0.007626915816217661\n",
            "Epoch  15 Batch  444 / 488  Training Loss  0.013195747509598732\n",
            "Epoch  15 Batch  445 / 488  Training Loss  0.004700087942183018\n",
            "Epoch  15 Batch  446 / 488  Training Loss  0.0026735137216746807\n",
            "Epoch  15 Batch  447 / 488  Training Loss  0.009619202464818954\n",
            "Epoch  15 Batch  448 / 488  Training Loss  0.002821889705955982\n",
            "Epoch  15 Batch  449 / 488  Training Loss  0.0076331184245646\n",
            "Epoch  15 Batch  450 / 488  Training Loss  0.0034833885729312897\n",
            "Epoch  15 Batch  451 / 488  Training Loss  0.01024096179753542\n",
            "Epoch  15 Batch  452 / 488  Training Loss  0.019019562751054764\n",
            "Epoch  15 Batch  453 / 488  Training Loss  0.006506286561489105\n",
            "Epoch  15 Batch  454 / 488  Training Loss  0.008520661853253841\n",
            "Epoch  15 Batch  455 / 488  Training Loss  0.007223688066005707\n",
            "Epoch  15 Batch  456 / 488  Training Loss  0.007263640873134136\n",
            "Epoch  15 Batch  457 / 488  Training Loss  0.007043406367301941\n",
            "Epoch  15 Batch  458 / 488  Training Loss  0.004706952720880508\n",
            "Epoch  15 Batch  459 / 488  Training Loss  0.006038573570549488\n",
            "Epoch  15 Batch  460 / 488  Training Loss  0.010551449842751026\n",
            "Epoch  15 Batch  461 / 488  Training Loss  0.005671374034136534\n",
            "Epoch  15 Batch  462 / 488  Training Loss  0.008337053470313549\n",
            "Epoch  15 Batch  463 / 488  Training Loss  0.008027441799640656\n",
            "Epoch  15 Batch  464 / 488  Training Loss  0.00477761821821332\n",
            "Epoch  15 Batch  465 / 488  Training Loss  0.021518340334296227\n",
            "Epoch  15 Batch  466 / 488  Training Loss  0.0069345757365226746\n",
            "Epoch  15 Batch  467 / 488  Training Loss  0.008870463818311691\n",
            "Epoch  15 Batch  468 / 488  Training Loss  0.008280880749225616\n",
            "Epoch  15 Batch  469 / 488  Training Loss  0.008630190044641495\n",
            "Epoch  15 Batch  470 / 488  Training Loss  0.01818537339568138\n",
            "Epoch  15 Batch  471 / 488  Training Loss  0.017767513170838356\n",
            "Epoch  15 Batch  472 / 488  Training Loss  0.007960820570588112\n",
            "Epoch  15 Batch  473 / 488  Training Loss  0.004792246501892805\n",
            "Epoch  15 Batch  474 / 488  Training Loss  0.005300804506987333\n",
            "Epoch  15 Batch  475 / 488  Training Loss  0.012782968580722809\n",
            "Epoch  15 Batch  476 / 488  Training Loss  0.004791778977960348\n",
            "Epoch  15 Batch  477 / 488  Training Loss  0.010332144796848297\n",
            "Epoch  15 Batch  478 / 488  Training Loss  0.010963204316794872\n",
            "Epoch  15 Batch  479 / 488  Training Loss  0.011349691078066826\n",
            "Epoch  15 Batch  480 / 488  Training Loss  0.0056673758663237095\n",
            "Epoch  15 Batch  481 / 488  Training Loss  0.01754956692457199\n",
            "Epoch  15 Batch  482 / 488  Training Loss  0.0031202370300889015\n",
            "Epoch  15 Batch  483 / 488  Training Loss  0.010953438468277454\n",
            "Epoch  15 Batch  484 / 488  Training Loss  0.007459443062543869\n",
            "Epoch  15 Batch  485 / 488  Training Loss  0.005786229856312275\n",
            "Epoch  15 Batch  486 / 488  Training Loss  0.003349892096593976\n",
            "Epoch  15 Batch  487 / 488  Training Loss  0.0010815814603120089\n",
            "  16    |    -    |   0.007079   | 43.218085\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 16\n",
            "Epoch  16 Batch  0 / 488  Training Loss  0.004249297082424164\n",
            "Epoch  16 Batch  1 / 488  Training Loss  0.006303863134235144\n",
            "Epoch  16 Batch  2 / 488  Training Loss  0.003697741776704788\n",
            "Epoch  16 Batch  3 / 488  Training Loss  0.0021336975041776896\n",
            "Epoch  16 Batch  4 / 488  Training Loss  0.0015960328746587038\n",
            "Epoch  16 Batch  5 / 488  Training Loss  0.006432873662561178\n",
            "Epoch  16 Batch  6 / 488  Training Loss  0.006233785301446915\n",
            "Epoch  16 Batch  7 / 488  Training Loss  0.006331237964332104\n",
            "Epoch  16 Batch  8 / 488  Training Loss  0.004374832846224308\n",
            "Epoch  16 Batch  9 / 488  Training Loss  0.002950554946437478\n",
            "Epoch  16 Batch  10 / 488  Training Loss  0.0046294149942696095\n",
            "Epoch  16 Batch  11 / 488  Training Loss  0.004841447342187166\n",
            "Epoch  16 Batch  12 / 488  Training Loss  0.0032566972076892853\n",
            "Epoch  16 Batch  13 / 488  Training Loss  0.0037269226741045713\n",
            "Epoch  16 Batch  14 / 488  Training Loss  0.00258921692147851\n",
            "Epoch  16 Batch  15 / 488  Training Loss  0.004795946646481752\n",
            "Epoch  16 Batch  16 / 488  Training Loss  0.0034887369256466627\n",
            "Epoch  16 Batch  17 / 488  Training Loss  0.001617839909158647\n",
            "Epoch  16 Batch  18 / 488  Training Loss  0.0019549813587218523\n",
            "Epoch  16 Batch  19 / 488  Training Loss  0.005077393259853125\n",
            "Epoch  16 Batch  20 / 488  Training Loss  0.0034270361065864563\n",
            "Epoch  16 Batch  21 / 488  Training Loss  0.004224749282002449\n",
            "Epoch  16 Batch  22 / 488  Training Loss  0.005007392726838589\n",
            "Epoch  16 Batch  23 / 488  Training Loss  0.006961426697671413\n",
            "Epoch  16 Batch  24 / 488  Training Loss  0.0018684456590563059\n",
            "Epoch  16 Batch  25 / 488  Training Loss  0.0024235674645751715\n",
            "Epoch  16 Batch  26 / 488  Training Loss  0.0016189974267035723\n",
            "Epoch  16 Batch  27 / 488  Training Loss  0.00292815244756639\n",
            "Epoch  16 Batch  28 / 488  Training Loss  0.0025771798100322485\n",
            "Epoch  16 Batch  29 / 488  Training Loss  0.0041835554875433445\n",
            "Epoch  16 Batch  30 / 488  Training Loss  0.004635474644601345\n",
            "Epoch  16 Batch  31 / 488  Training Loss  0.005313520785421133\n",
            "Epoch  16 Batch  32 / 488  Training Loss  0.002671807771548629\n",
            "Epoch  16 Batch  33 / 488  Training Loss  0.0059736124239861965\n",
            "Epoch  16 Batch  34 / 488  Training Loss  0.01079967338591814\n",
            "Epoch  16 Batch  35 / 488  Training Loss  0.0030907404143363237\n",
            "Epoch  16 Batch  36 / 488  Training Loss  0.001183307496830821\n",
            "Epoch  16 Batch  37 / 488  Training Loss  0.002254443010315299\n",
            "Epoch  16 Batch  38 / 488  Training Loss  0.003507787361741066\n",
            "Epoch  16 Batch  39 / 488  Training Loss  0.005414139945060015\n",
            "Epoch  16 Batch  40 / 488  Training Loss  0.00157045922242105\n",
            "Epoch  16 Batch  41 / 488  Training Loss  0.012332146987318993\n",
            "Epoch  16 Batch  42 / 488  Training Loss  0.0030761733651161194\n",
            "Epoch  16 Batch  43 / 488  Training Loss  0.004184999968856573\n",
            "Epoch  16 Batch  44 / 488  Training Loss  0.0023362261708825827\n",
            "Epoch  16 Batch  45 / 488  Training Loss  0.0027948091737926006\n",
            "Epoch  16 Batch  46 / 488  Training Loss  0.006861784495413303\n",
            "Epoch  16 Batch  47 / 488  Training Loss  0.004008778370916843\n",
            "Epoch  16 Batch  48 / 488  Training Loss  0.0030018482357263565\n",
            "Epoch  16 Batch  49 / 488  Training Loss  0.0022445465438067913\n",
            "Epoch  16 Batch  50 / 488  Training Loss  0.0024688090197741985\n",
            "Epoch  16 Batch  51 / 488  Training Loss  0.0020506992004811764\n",
            "Epoch  16 Batch  52 / 488  Training Loss  0.004001702181994915\n",
            "Epoch  16 Batch  53 / 488  Training Loss  0.003650133963674307\n",
            "Epoch  16 Batch  54 / 488  Training Loss  0.001369713107123971\n",
            "Epoch  16 Batch  55 / 488  Training Loss  0.0022180695086717606\n",
            "Epoch  16 Batch  56 / 488  Training Loss  0.00531960092484951\n",
            "Epoch  16 Batch  57 / 488  Training Loss  0.00351732037961483\n",
            "Epoch  16 Batch  58 / 488  Training Loss  0.006564012262970209\n",
            "Epoch  16 Batch  59 / 488  Training Loss  0.005530362017452717\n",
            "Epoch  16 Batch  60 / 488  Training Loss  0.010514109395444393\n",
            "Epoch  16 Batch  61 / 488  Training Loss  0.0016938615590333939\n",
            "Epoch  16 Batch  62 / 488  Training Loss  0.006037954241037369\n",
            "Epoch  16 Batch  63 / 488  Training Loss  0.003781535429880023\n",
            "Epoch  16 Batch  64 / 488  Training Loss  0.005360976792871952\n",
            "Epoch  16 Batch  65 / 488  Training Loss  0.004680160898715258\n",
            "Epoch  16 Batch  66 / 488  Training Loss  0.0031766709871590137\n",
            "Epoch  16 Batch  67 / 488  Training Loss  0.010974561795592308\n",
            "Epoch  16 Batch  68 / 488  Training Loss  0.0018008919432759285\n",
            "Epoch  16 Batch  69 / 488  Training Loss  0.002019998151808977\n",
            "Epoch  16 Batch  70 / 488  Training Loss  0.005821650847792625\n",
            "Epoch  16 Batch  71 / 488  Training Loss  0.008789559826254845\n",
            "Epoch  16 Batch  72 / 488  Training Loss  0.0027343062683939934\n",
            "Epoch  16 Batch  73 / 488  Training Loss  0.0031602089293301105\n",
            "Epoch  16 Batch  74 / 488  Training Loss  0.0029698333237320185\n",
            "Epoch  16 Batch  75 / 488  Training Loss  0.003242009086534381\n",
            "Epoch  16 Batch  76 / 488  Training Loss  0.001514569972641766\n",
            "Epoch  16 Batch  77 / 488  Training Loss  0.002558186650276184\n",
            "Epoch  16 Batch  78 / 488  Training Loss  0.005154471844434738\n",
            "Epoch  16 Batch  79 / 488  Training Loss  0.002042075851932168\n",
            "Epoch  16 Batch  80 / 488  Training Loss  0.007046551443636417\n",
            "Epoch  16 Batch  81 / 488  Training Loss  0.0052108499221503735\n",
            "Epoch  16 Batch  82 / 488  Training Loss  0.004565905313938856\n",
            "Epoch  16 Batch  83 / 488  Training Loss  0.0045716119930148125\n",
            "Epoch  16 Batch  84 / 488  Training Loss  0.006809532642364502\n",
            "Epoch  16 Batch  85 / 488  Training Loss  0.005500977858901024\n",
            "Epoch  16 Batch  86 / 488  Training Loss  0.003733571618795395\n",
            "Epoch  16 Batch  87 / 488  Training Loss  0.003696382511407137\n",
            "Epoch  16 Batch  88 / 488  Training Loss  0.0031603449024260044\n",
            "Epoch  16 Batch  89 / 488  Training Loss  0.0045058270916342735\n",
            "Epoch  16 Batch  90 / 488  Training Loss  0.004775916691869497\n",
            "Epoch  16 Batch  91 / 488  Training Loss  0.007136906031519175\n",
            "Epoch  16 Batch  92 / 488  Training Loss  0.002027178183197975\n",
            "Epoch  16 Batch  93 / 488  Training Loss  0.002227633260190487\n",
            "Epoch  16 Batch  94 / 488  Training Loss  0.00416500074788928\n",
            "Epoch  16 Batch  95 / 488  Training Loss  0.0040535107254981995\n",
            "Epoch  16 Batch  96 / 488  Training Loss  0.001807185122743249\n",
            "Epoch  16 Batch  97 / 488  Training Loss  0.005223958287388086\n",
            "Epoch  16 Batch  98 / 488  Training Loss  0.0017587642651051283\n",
            "Epoch  16 Batch  99 / 488  Training Loss  0.005074985790997744\n",
            "Epoch  16 Batch  100 / 488  Training Loss  0.001790739013813436\n",
            "Epoch  16 Batch  101 / 488  Training Loss  0.0020542414858937263\n",
            "Epoch  16 Batch  102 / 488  Training Loss  0.0037745614536106586\n",
            "Epoch  16 Batch  103 / 488  Training Loss  0.00659590307623148\n",
            "Epoch  16 Batch  104 / 488  Training Loss  0.009161201305687428\n",
            "Epoch  16 Batch  105 / 488  Training Loss  0.0025565139949321747\n",
            "Epoch  16 Batch  106 / 488  Training Loss  0.004533692263066769\n",
            "Epoch  16 Batch  107 / 488  Training Loss  0.004639076068997383\n",
            "Epoch  16 Batch  108 / 488  Training Loss  0.006054524332284927\n",
            "Epoch  16 Batch  109 / 488  Training Loss  0.005185150541365147\n",
            "Epoch  16 Batch  110 / 488  Training Loss  0.0037548919208347797\n",
            "Epoch  16 Batch  111 / 488  Training Loss  0.0021334306802600622\n",
            "Epoch  16 Batch  112 / 488  Training Loss  0.0038808516692370176\n",
            "Epoch  16 Batch  113 / 488  Training Loss  0.005285072606056929\n",
            "Epoch  16 Batch  114 / 488  Training Loss  0.0029432387091219425\n",
            "Epoch  16 Batch  115 / 488  Training Loss  0.004773270338773727\n",
            "Epoch  16 Batch  116 / 488  Training Loss  0.003428397234529257\n",
            "Epoch  16 Batch  117 / 488  Training Loss  0.0019192624604329467\n",
            "Epoch  16 Batch  118 / 488  Training Loss  0.007727281656116247\n",
            "Epoch  16 Batch  119 / 488  Training Loss  0.005517094861716032\n",
            "Epoch  16 Batch  120 / 488  Training Loss  0.0029452580492943525\n",
            "Epoch  16 Batch  121 / 488  Training Loss  0.003671246347948909\n",
            "Epoch  16 Batch  122 / 488  Training Loss  0.0020852689631283283\n",
            "Epoch  16 Batch  123 / 488  Training Loss  0.002314990386366844\n",
            "Epoch  16 Batch  124 / 488  Training Loss  0.0027025870513170958\n",
            "Epoch  16 Batch  125 / 488  Training Loss  0.003425308270379901\n",
            "Epoch  16 Batch  126 / 488  Training Loss  0.009652171283960342\n",
            "Epoch  16 Batch  127 / 488  Training Loss  0.005415482446551323\n",
            "Epoch  16 Batch  128 / 488  Training Loss  0.005705561023205519\n",
            "Epoch  16 Batch  129 / 488  Training Loss  0.001560141914524138\n",
            "Epoch  16 Batch  130 / 488  Training Loss  0.004233105108141899\n",
            "Epoch  16 Batch  131 / 488  Training Loss  0.004698287695646286\n",
            "Epoch  16 Batch  132 / 488  Training Loss  0.005343092139810324\n",
            "Epoch  16 Batch  133 / 488  Training Loss  0.0017660601297393441\n",
            "Epoch  16 Batch  134 / 488  Training Loss  0.0036892578937113285\n",
            "Epoch  16 Batch  135 / 488  Training Loss  0.0031532521825283766\n",
            "Epoch  16 Batch  136 / 488  Training Loss  0.005605142563581467\n",
            "Epoch  16 Batch  137 / 488  Training Loss  0.005449631251394749\n",
            "Epoch  16 Batch  138 / 488  Training Loss  0.0036886781454086304\n",
            "Epoch  16 Batch  139 / 488  Training Loss  0.002036810852587223\n",
            "Epoch  16 Batch  140 / 488  Training Loss  0.0011021358659490943\n",
            "Epoch  16 Batch  141 / 488  Training Loss  0.00123266177251935\n",
            "Epoch  16 Batch  142 / 488  Training Loss  0.002680047880858183\n",
            "Epoch  16 Batch  143 / 488  Training Loss  0.0023442807141691446\n",
            "Epoch  16 Batch  144 / 488  Training Loss  0.003515135496854782\n",
            "Epoch  16 Batch  145 / 488  Training Loss  0.012248181737959385\n",
            "Epoch  16 Batch  146 / 488  Training Loss  0.004574209917336702\n",
            "Epoch  16 Batch  147 / 488  Training Loss  0.019559863954782486\n",
            "Epoch  16 Batch  148 / 488  Training Loss  0.0044089495204389095\n",
            "Epoch  16 Batch  149 / 488  Training Loss  0.005101780407130718\n",
            "Epoch  16 Batch  150 / 488  Training Loss  0.0031371708028018475\n",
            "Epoch  16 Batch  151 / 488  Training Loss  0.0035069137811660767\n",
            "Epoch  16 Batch  152 / 488  Training Loss  0.0023586011957377195\n",
            "Epoch  16 Batch  153 / 488  Training Loss  0.002875017002224922\n",
            "Epoch  16 Batch  154 / 488  Training Loss  0.003851689398288727\n",
            "Epoch  16 Batch  155 / 488  Training Loss  0.004186556674540043\n",
            "Epoch  16 Batch  156 / 488  Training Loss  0.004284200724214315\n",
            "Epoch  16 Batch  157 / 488  Training Loss  0.005387180019170046\n",
            "Epoch  16 Batch  158 / 488  Training Loss  0.004752583336085081\n",
            "Epoch  16 Batch  159 / 488  Training Loss  0.003270162967965007\n",
            "Epoch  16 Batch  160 / 488  Training Loss  0.030729111284017563\n",
            "Epoch  16 Batch  161 / 488  Training Loss  0.0030572530813515186\n",
            "Epoch  16 Batch  162 / 488  Training Loss  0.0030287059489637613\n",
            "Epoch  16 Batch  163 / 488  Training Loss  0.003968446981161833\n",
            "Epoch  16 Batch  164 / 488  Training Loss  0.004078921861946583\n",
            "Epoch  16 Batch  165 / 488  Training Loss  0.004637398757040501\n",
            "Epoch  16 Batch  166 / 488  Training Loss  0.002781346905976534\n",
            "Epoch  16 Batch  167 / 488  Training Loss  0.002244343049824238\n",
            "Epoch  16 Batch  168 / 488  Training Loss  0.003466926049441099\n",
            "Epoch  16 Batch  169 / 488  Training Loss  0.007631595246493816\n",
            "Epoch  16 Batch  170 / 488  Training Loss  0.0031052245758473873\n",
            "Epoch  16 Batch  171 / 488  Training Loss  0.006217913702130318\n",
            "Epoch  16 Batch  172 / 488  Training Loss  0.004296851810067892\n",
            "Epoch  16 Batch  173 / 488  Training Loss  0.002217294182628393\n",
            "Epoch  16 Batch  174 / 488  Training Loss  0.005978846922516823\n",
            "Epoch  16 Batch  175 / 488  Training Loss  0.003165746107697487\n",
            "Epoch  16 Batch  176 / 488  Training Loss  0.002401735633611679\n",
            "Epoch  16 Batch  177 / 488  Training Loss  0.002859812695533037\n",
            "Epoch  16 Batch  178 / 488  Training Loss  0.004952640272676945\n",
            "Epoch  16 Batch  179 / 488  Training Loss  0.0038346953224390745\n",
            "Epoch  16 Batch  180 / 488  Training Loss  0.004372529685497284\n",
            "Epoch  16 Batch  181 / 488  Training Loss  0.0021228143014013767\n",
            "Epoch  16 Batch  182 / 488  Training Loss  0.0017361281206831336\n",
            "Epoch  16 Batch  183 / 488  Training Loss  0.0016176558565348387\n",
            "Epoch  16 Batch  184 / 488  Training Loss  0.003894499037414789\n",
            "Epoch  16 Batch  185 / 488  Training Loss  0.016947828233242035\n",
            "Epoch  16 Batch  186 / 488  Training Loss  0.0021103848703205585\n",
            "Epoch  16 Batch  187 / 488  Training Loss  0.0040847319178283215\n",
            "Epoch  16 Batch  188 / 488  Training Loss  0.002767022931948304\n",
            "Epoch  16 Batch  189 / 488  Training Loss  0.0034513119608163834\n",
            "Epoch  16 Batch  190 / 488  Training Loss  0.0019011187832802534\n",
            "Epoch  16 Batch  191 / 488  Training Loss  0.004395519848912954\n",
            "Epoch  16 Batch  192 / 488  Training Loss  0.0022841941099613905\n",
            "Epoch  16 Batch  193 / 488  Training Loss  0.006074816919863224\n",
            "Epoch  16 Batch  194 / 488  Training Loss  0.002240068046376109\n",
            "Epoch  16 Batch  195 / 488  Training Loss  0.007947013713419437\n",
            "Epoch  16 Batch  196 / 488  Training Loss  0.0022032989654690027\n",
            "Epoch  16 Batch  197 / 488  Training Loss  0.0020333118736743927\n",
            "Epoch  16 Batch  198 / 488  Training Loss  0.005853450391441584\n",
            "Epoch  16 Batch  199 / 488  Training Loss  0.0033448473550379276\n",
            "Epoch  16 Batch  200 / 488  Training Loss  0.00346309132874012\n",
            "Epoch  16 Batch  201 / 488  Training Loss  0.006516552064567804\n",
            "Epoch  16 Batch  202 / 488  Training Loss  0.005091450177133083\n",
            "Epoch  16 Batch  203 / 488  Training Loss  0.012584565207362175\n",
            "Epoch  16 Batch  204 / 488  Training Loss  0.002620462793856859\n",
            "Epoch  16 Batch  205 / 488  Training Loss  0.0028859444428235292\n",
            "Epoch  16 Batch  206 / 488  Training Loss  0.004184720106422901\n",
            "Epoch  16 Batch  207 / 488  Training Loss  0.003673586994409561\n",
            "Epoch  16 Batch  208 / 488  Training Loss  0.007591482251882553\n",
            "Epoch  16 Batch  209 / 488  Training Loss  0.004347863607108593\n",
            "Epoch  16 Batch  210 / 488  Training Loss  0.002269399119541049\n",
            "Epoch  16 Batch  211 / 488  Training Loss  0.004051961936056614\n",
            "Epoch  16 Batch  212 / 488  Training Loss  0.0034178276546299458\n",
            "Epoch  16 Batch  213 / 488  Training Loss  0.005424069706350565\n",
            "Epoch  16 Batch  214 / 488  Training Loss  0.013968038372695446\n",
            "Epoch  16 Batch  215 / 488  Training Loss  0.0020489844027906656\n",
            "Epoch  16 Batch  216 / 488  Training Loss  0.0023946594446897507\n",
            "Epoch  16 Batch  217 / 488  Training Loss  0.002373739145696163\n",
            "Epoch  16 Batch  218 / 488  Training Loss  0.003962422721087933\n",
            "Epoch  16 Batch  219 / 488  Training Loss  0.0033488471526652575\n",
            "Epoch  16 Batch  220 / 488  Training Loss  0.003870937507599592\n",
            "Epoch  16 Batch  221 / 488  Training Loss  0.020246189087629318\n",
            "Epoch  16 Batch  222 / 488  Training Loss  0.0022590479347854853\n",
            "Epoch  16 Batch  223 / 488  Training Loss  0.0043403678573668\n",
            "Epoch  16 Batch  224 / 488  Training Loss  0.0041610123589634895\n",
            "Epoch  16 Batch  225 / 488  Training Loss  0.003140829037874937\n",
            "Epoch  16 Batch  226 / 488  Training Loss  0.00686092022806406\n",
            "Epoch  16 Batch  227 / 488  Training Loss  0.0035195753443986177\n",
            "Epoch  16 Batch  228 / 488  Training Loss  0.00402822345495224\n",
            "Epoch  16 Batch  229 / 488  Training Loss  0.008398232981562614\n",
            "Epoch  16 Batch  230 / 488  Training Loss  0.005445762071758509\n",
            "Epoch  16 Batch  231 / 488  Training Loss  0.0013565209228545427\n",
            "Epoch  16 Batch  232 / 488  Training Loss  0.006258341018110514\n",
            "Epoch  16 Batch  233 / 488  Training Loss  0.002302020788192749\n",
            "Epoch  16 Batch  234 / 488  Training Loss  0.0018686356488615274\n",
            "Epoch  16 Batch  235 / 488  Training Loss  0.0018760946113616228\n",
            "Epoch  16 Batch  236 / 488  Training Loss  0.003735907841473818\n",
            "Epoch  16 Batch  237 / 488  Training Loss  0.010465309955179691\n",
            "Epoch  16 Batch  238 / 488  Training Loss  0.006513558328151703\n",
            "Epoch  16 Batch  239 / 488  Training Loss  0.006330019794404507\n",
            "Epoch  16 Batch  240 / 488  Training Loss  0.002310848794877529\n",
            "Epoch  16 Batch  241 / 488  Training Loss  0.00495386216789484\n",
            "Epoch  16 Batch  242 / 488  Training Loss  0.002513502724468708\n",
            "Epoch  16 Batch  243 / 488  Training Loss  0.001807682914659381\n",
            "Epoch  16 Batch  244 / 488  Training Loss  0.010403810068964958\n",
            "Epoch  16 Batch  245 / 488  Training Loss  0.007016791962087154\n",
            "Epoch  16 Batch  246 / 488  Training Loss  0.003164127469062805\n",
            "Epoch  16 Batch  247 / 488  Training Loss  0.005336422938853502\n",
            "Epoch  16 Batch  248 / 488  Training Loss  0.004170976113528013\n",
            "Epoch  16 Batch  249 / 488  Training Loss  0.004066620022058487\n",
            "Epoch  16 Batch  250 / 488  Training Loss  0.0026370494160801172\n",
            "Epoch  16 Batch  251 / 488  Training Loss  0.0026938144583255053\n",
            "Epoch  16 Batch  252 / 488  Training Loss  0.004324178211390972\n",
            "Epoch  16 Batch  253 / 488  Training Loss  0.004829388111829758\n",
            "Epoch  16 Batch  254 / 488  Training Loss  0.0028571649454534054\n",
            "Epoch  16 Batch  255 / 488  Training Loss  0.0019517617765814066\n",
            "Epoch  16 Batch  256 / 488  Training Loss  0.002672735368832946\n",
            "Epoch  16 Batch  257 / 488  Training Loss  0.00406083045527339\n",
            "Epoch  16 Batch  258 / 488  Training Loss  0.005513910669833422\n",
            "Epoch  16 Batch  259 / 488  Training Loss  0.002524805720895529\n",
            "Epoch  16 Batch  260 / 488  Training Loss  0.005804789252579212\n",
            "Epoch  16 Batch  261 / 488  Training Loss  0.004639531020075083\n",
            "Epoch  16 Batch  262 / 488  Training Loss  0.002644970314577222\n",
            "Epoch  16 Batch  263 / 488  Training Loss  0.0032367990352213383\n",
            "Epoch  16 Batch  264 / 488  Training Loss  0.0019084431696683168\n",
            "Epoch  16 Batch  265 / 488  Training Loss  0.007085463497787714\n",
            "Epoch  16 Batch  266 / 488  Training Loss  0.002635579090565443\n",
            "Epoch  16 Batch  267 / 488  Training Loss  0.0038031216245144606\n",
            "Epoch  16 Batch  268 / 488  Training Loss  0.003759614424780011\n",
            "Epoch  16 Batch  269 / 488  Training Loss  0.007535337004810572\n",
            "Epoch  16 Batch  270 / 488  Training Loss  0.004849365912377834\n",
            "Epoch  16 Batch  271 / 488  Training Loss  0.007213751785457134\n",
            "Epoch  16 Batch  272 / 488  Training Loss  0.0033628803212195635\n",
            "Epoch  16 Batch  273 / 488  Training Loss  0.003753106575459242\n",
            "Epoch  16 Batch  274 / 488  Training Loss  0.005886119790375233\n",
            "Epoch  16 Batch  275 / 488  Training Loss  0.0039416952058672905\n",
            "Epoch  16 Batch  276 / 488  Training Loss  0.007707095239311457\n",
            "Epoch  16 Batch  277 / 488  Training Loss  0.0047300453297793865\n",
            "Epoch  16 Batch  278 / 488  Training Loss  0.0029427711851894855\n",
            "Epoch  16 Batch  279 / 488  Training Loss  0.002481758128851652\n",
            "Epoch  16 Batch  280 / 488  Training Loss  0.0029143900610506535\n",
            "Epoch  16 Batch  281 / 488  Training Loss  0.017043350264430046\n",
            "Epoch  16 Batch  282 / 488  Training Loss  0.003328463062644005\n",
            "Epoch  16 Batch  283 / 488  Training Loss  0.003541292157024145\n",
            "Epoch  16 Batch  284 / 488  Training Loss  0.005849599372595549\n",
            "Epoch  16 Batch  285 / 488  Training Loss  0.0031187855638563633\n",
            "Epoch  16 Batch  286 / 488  Training Loss  0.004706770181655884\n",
            "Epoch  16 Batch  287 / 488  Training Loss  0.0033738005440682173\n",
            "Epoch  16 Batch  288 / 488  Training Loss  0.006329970899969339\n",
            "Epoch  16 Batch  289 / 488  Training Loss  0.002309261355549097\n",
            "Epoch  16 Batch  290 / 488  Training Loss  0.005793806165456772\n",
            "Epoch  16 Batch  291 / 488  Training Loss  0.001996323000639677\n",
            "Epoch  16 Batch  292 / 488  Training Loss  0.0024689349811524153\n",
            "Epoch  16 Batch  293 / 488  Training Loss  0.003101357724517584\n",
            "Epoch  16 Batch  294 / 488  Training Loss  0.00392018910497427\n",
            "Epoch  16 Batch  295 / 488  Training Loss  0.00451422855257988\n",
            "Epoch  16 Batch  296 / 488  Training Loss  0.009866079315543175\n",
            "Epoch  16 Batch  297 / 488  Training Loss  0.0034272843040525913\n",
            "Epoch  16 Batch  298 / 488  Training Loss  0.0022118694614619017\n",
            "Epoch  16 Batch  299 / 488  Training Loss  0.003416525200009346\n",
            "Epoch  16 Batch  300 / 488  Training Loss  0.006826269440352917\n",
            "Epoch  16 Batch  301 / 488  Training Loss  0.0027429223991930485\n",
            "Epoch  16 Batch  302 / 488  Training Loss  0.0051845586858689785\n",
            "Epoch  16 Batch  303 / 488  Training Loss  0.0029492774046957493\n",
            "Epoch  16 Batch  304 / 488  Training Loss  0.0019497048342600465\n",
            "Epoch  16 Batch  305 / 488  Training Loss  0.004626242443919182\n",
            "Epoch  16 Batch  306 / 488  Training Loss  0.007577252574265003\n",
            "Epoch  16 Batch  307 / 488  Training Loss  0.004791561048477888\n",
            "Epoch  16 Batch  308 / 488  Training Loss  0.006283483002334833\n",
            "Epoch  16 Batch  309 / 488  Training Loss  0.0027997344732284546\n",
            "Epoch  16 Batch  310 / 488  Training Loss  0.007439030800014734\n",
            "Epoch  16 Batch  311 / 488  Training Loss  0.003320569172501564\n",
            "Epoch  16 Batch  312 / 488  Training Loss  0.008085587993264198\n",
            "Epoch  16 Batch  313 / 488  Training Loss  0.010069478303194046\n",
            "Epoch  16 Batch  314 / 488  Training Loss  0.0024064250756055117\n",
            "Epoch  16 Batch  315 / 488  Training Loss  0.004166138358414173\n",
            "Epoch  16 Batch  316 / 488  Training Loss  0.002670800546184182\n",
            "Epoch  16 Batch  317 / 488  Training Loss  0.005413571838289499\n",
            "Epoch  16 Batch  318 / 488  Training Loss  0.0031006752979010344\n",
            "Epoch  16 Batch  319 / 488  Training Loss  0.005286019295454025\n",
            "Epoch  16 Batch  320 / 488  Training Loss  0.005758692976087332\n",
            "Epoch  16 Batch  321 / 488  Training Loss  0.004847529344260693\n",
            "Epoch  16 Batch  322 / 488  Training Loss  0.0033984072506427765\n",
            "Epoch  16 Batch  323 / 488  Training Loss  0.004993249196559191\n",
            "Epoch  16 Batch  324 / 488  Training Loss  0.005906424485146999\n",
            "Epoch  16 Batch  325 / 488  Training Loss  0.003648509504273534\n",
            "Epoch  16 Batch  326 / 488  Training Loss  0.0031366695184260607\n",
            "Epoch  16 Batch  327 / 488  Training Loss  0.0033362177200615406\n",
            "Epoch  16 Batch  328 / 488  Training Loss  0.004478534683585167\n",
            "Epoch  16 Batch  329 / 488  Training Loss  0.002151381690055132\n",
            "Epoch  16 Batch  330 / 488  Training Loss  0.002259577391669154\n",
            "Epoch  16 Batch  331 / 488  Training Loss  0.002701177028939128\n",
            "Epoch  16 Batch  332 / 488  Training Loss  0.0042321886867284775\n",
            "Epoch  16 Batch  333 / 488  Training Loss  0.0031162737868726254\n",
            "Epoch  16 Batch  334 / 488  Training Loss  0.00155447272118181\n",
            "Epoch  16 Batch  335 / 488  Training Loss  0.002818662440404296\n",
            "Epoch  16 Batch  336 / 488  Training Loss  0.002006902126595378\n",
            "Epoch  16 Batch  337 / 488  Training Loss  0.0026442394591867924\n",
            "Epoch  16 Batch  338 / 488  Training Loss  0.0013320669531822205\n",
            "Epoch  16 Batch  339 / 488  Training Loss  0.0020877141505479813\n",
            "Epoch  16 Batch  340 / 488  Training Loss  0.00427636643871665\n",
            "Epoch  16 Batch  341 / 488  Training Loss  0.0047147260047495365\n",
            "Epoch  16 Batch  342 / 488  Training Loss  0.003999463748186827\n",
            "Epoch  16 Batch  343 / 488  Training Loss  0.002253553830087185\n",
            "Epoch  16 Batch  344 / 488  Training Loss  0.005691961385309696\n",
            "Epoch  16 Batch  345 / 488  Training Loss  0.003604371566325426\n",
            "Epoch  16 Batch  346 / 488  Training Loss  0.002947083441540599\n",
            "Epoch  16 Batch  347 / 488  Training Loss  0.004983570892363787\n",
            "Epoch  16 Batch  348 / 488  Training Loss  0.007004054728895426\n",
            "Epoch  16 Batch  349 / 488  Training Loss  0.0020779515616595745\n",
            "Epoch  16 Batch  350 / 488  Training Loss  0.005752469878643751\n",
            "Epoch  16 Batch  351 / 488  Training Loss  0.004876452032476664\n",
            "Epoch  16 Batch  352 / 488  Training Loss  0.010121002793312073\n",
            "Epoch  16 Batch  353 / 488  Training Loss  0.002762557240203023\n",
            "Epoch  16 Batch  354 / 488  Training Loss  0.004681145306676626\n",
            "Epoch  16 Batch  355 / 488  Training Loss  0.009541226550936699\n",
            "Epoch  16 Batch  356 / 488  Training Loss  0.0032510017044842243\n",
            "Epoch  16 Batch  357 / 488  Training Loss  0.00962613895535469\n",
            "Epoch  16 Batch  358 / 488  Training Loss  0.003489479422569275\n",
            "Epoch  16 Batch  359 / 488  Training Loss  0.0036393410991877317\n",
            "Epoch  16 Batch  360 / 488  Training Loss  0.004802735988050699\n",
            "Epoch  16 Batch  361 / 488  Training Loss  0.002355859847739339\n",
            "Epoch  16 Batch  362 / 488  Training Loss  0.0022676633670926094\n",
            "Epoch  16 Batch  363 / 488  Training Loss  0.0038441370707005262\n",
            "Epoch  16 Batch  364 / 488  Training Loss  0.007241685874760151\n",
            "Epoch  16 Batch  365 / 488  Training Loss  0.0021463194862008095\n",
            "Epoch  16 Batch  366 / 488  Training Loss  0.0071193752810359\n",
            "Epoch  16 Batch  367 / 488  Training Loss  0.0018891177605837584\n",
            "Epoch  16 Batch  368 / 488  Training Loss  0.004375896882265806\n",
            "Epoch  16 Batch  369 / 488  Training Loss  0.009190755896270275\n",
            "Epoch  16 Batch  370 / 488  Training Loss  0.002458584727719426\n",
            "Epoch  16 Batch  371 / 488  Training Loss  0.008081709034740925\n",
            "Epoch  16 Batch  372 / 488  Training Loss  0.0055500357411801815\n",
            "Epoch  16 Batch  373 / 488  Training Loss  0.004146899096667767\n",
            "Epoch  16 Batch  374 / 488  Training Loss  0.002173677086830139\n",
            "Epoch  16 Batch  375 / 488  Training Loss  0.002226398792117834\n",
            "Epoch  16 Batch  376 / 488  Training Loss  0.001785007887519896\n",
            "Epoch  16 Batch  377 / 488  Training Loss  0.02141542360186577\n",
            "Epoch  16 Batch  378 / 488  Training Loss  0.004076630342751741\n",
            "Epoch  16 Batch  379 / 488  Training Loss  0.003321239026263356\n",
            "Epoch  16 Batch  380 / 488  Training Loss  0.0065460288897156715\n",
            "Epoch  16 Batch  381 / 488  Training Loss  0.012504476122558117\n",
            "Epoch  16 Batch  382 / 488  Training Loss  0.004747025202959776\n",
            "Epoch  16 Batch  383 / 488  Training Loss  0.010434631258249283\n",
            "Epoch  16 Batch  384 / 488  Training Loss  0.001737926504574716\n",
            "Epoch  16 Batch  385 / 488  Training Loss  0.002941527869552374\n",
            "Epoch  16 Batch  386 / 488  Training Loss  0.0028383745811879635\n",
            "Epoch  16 Batch  387 / 488  Training Loss  0.003250804264098406\n",
            "Epoch  16 Batch  388 / 488  Training Loss  0.0031109447591006756\n",
            "Epoch  16 Batch  389 / 488  Training Loss  0.003027051454409957\n",
            "Epoch  16 Batch  390 / 488  Training Loss  0.004408392123878002\n",
            "Epoch  16 Batch  391 / 488  Training Loss  0.0020949288737028837\n",
            "Epoch  16 Batch  392 / 488  Training Loss  0.02804131805896759\n",
            "Epoch  16 Batch  393 / 488  Training Loss  0.004008707590401173\n",
            "Epoch  16 Batch  394 / 488  Training Loss  0.013392256572842598\n",
            "Epoch  16 Batch  395 / 488  Training Loss  0.0011028500739485025\n",
            "Epoch  16 Batch  396 / 488  Training Loss  0.016154905781149864\n",
            "Epoch  16 Batch  397 / 488  Training Loss  0.007336615119129419\n",
            "Epoch  16 Batch  398 / 488  Training Loss  0.004344775341451168\n",
            "Epoch  16 Batch  399 / 488  Training Loss  0.004247502889484167\n",
            "Epoch  16 Batch  400 / 488  Training Loss  0.006634284742176533\n",
            "Epoch  16 Batch  401 / 488  Training Loss  0.004620459862053394\n",
            "Epoch  16 Batch  402 / 488  Training Loss  0.0035225101746618748\n",
            "Epoch  16 Batch  403 / 488  Training Loss  0.014956939034163952\n",
            "Epoch  16 Batch  404 / 488  Training Loss  0.002550232456997037\n",
            "Epoch  16 Batch  405 / 488  Training Loss  0.004283305257558823\n",
            "Epoch  16 Batch  406 / 488  Training Loss  0.008552325889468193\n",
            "Epoch  16 Batch  407 / 488  Training Loss  0.0028682430274784565\n",
            "Epoch  16 Batch  408 / 488  Training Loss  0.002372217597439885\n",
            "Epoch  16 Batch  409 / 488  Training Loss  0.003911019768565893\n",
            "Epoch  16 Batch  410 / 488  Training Loss  0.0033096119295805693\n",
            "Epoch  16 Batch  411 / 488  Training Loss  0.003879418596625328\n",
            "Epoch  16 Batch  412 / 488  Training Loss  0.006279299966990948\n",
            "Epoch  16 Batch  413 / 488  Training Loss  0.0017558962572366\n",
            "Epoch  16 Batch  414 / 488  Training Loss  0.0019591818563640118\n",
            "Epoch  16 Batch  415 / 488  Training Loss  0.0045516835525631905\n",
            "Epoch  16 Batch  416 / 488  Training Loss  0.00303087430074811\n",
            "Epoch  16 Batch  417 / 488  Training Loss  0.008485878817737103\n",
            "Epoch  16 Batch  418 / 488  Training Loss  0.0037993830628693104\n",
            "Epoch  16 Batch  419 / 488  Training Loss  0.005606230814009905\n",
            "Epoch  16 Batch  420 / 488  Training Loss  0.004301963374018669\n",
            "Epoch  16 Batch  421 / 488  Training Loss  0.004597523249685764\n",
            "Epoch  16 Batch  422 / 488  Training Loss  0.003087912453338504\n",
            "Epoch  16 Batch  423 / 488  Training Loss  0.0033038246911019087\n",
            "Epoch  16 Batch  424 / 488  Training Loss  0.00330612575635314\n",
            "Epoch  16 Batch  425 / 488  Training Loss  0.0026194280944764614\n",
            "Epoch  16 Batch  426 / 488  Training Loss  0.0025509216357022524\n",
            "Epoch  16 Batch  427 / 488  Training Loss  0.005893895402550697\n",
            "Epoch  16 Batch  428 / 488  Training Loss  0.004421909339725971\n",
            "Epoch  16 Batch  429 / 488  Training Loss  0.002859241794794798\n",
            "Epoch  16 Batch  430 / 488  Training Loss  0.005099484696984291\n",
            "Epoch  16 Batch  431 / 488  Training Loss  0.012574372813105583\n",
            "Epoch  16 Batch  432 / 488  Training Loss  0.007678802125155926\n",
            "Epoch  16 Batch  433 / 488  Training Loss  0.00933559238910675\n",
            "Epoch  16 Batch  434 / 488  Training Loss  0.003966503776609898\n",
            "Epoch  16 Batch  435 / 488  Training Loss  0.003253759117797017\n",
            "Epoch  16 Batch  436 / 488  Training Loss  0.004678166005760431\n",
            "Epoch  16 Batch  437 / 488  Training Loss  0.008048564195632935\n",
            "Epoch  16 Batch  438 / 488  Training Loss  0.0038196160458028316\n",
            "Epoch  16 Batch  439 / 488  Training Loss  0.003143560839816928\n",
            "Epoch  16 Batch  440 / 488  Training Loss  0.0033183605410158634\n",
            "Epoch  16 Batch  441 / 488  Training Loss  0.01463289838284254\n",
            "Epoch  16 Batch  442 / 488  Training Loss  0.002689070301130414\n",
            "Epoch  16 Batch  443 / 488  Training Loss  0.00527657987549901\n",
            "Epoch  16 Batch  444 / 488  Training Loss  0.004223440773785114\n",
            "Epoch  16 Batch  445 / 488  Training Loss  0.0038636159151792526\n",
            "Epoch  16 Batch  446 / 488  Training Loss  0.010442426428198814\n",
            "Epoch  16 Batch  447 / 488  Training Loss  0.0034511766862124205\n",
            "Epoch  16 Batch  448 / 488  Training Loss  0.006242706906050444\n",
            "Epoch  16 Batch  449 / 488  Training Loss  0.0029570898041129112\n",
            "Epoch  16 Batch  450 / 488  Training Loss  0.005993613041937351\n",
            "Epoch  16 Batch  451 / 488  Training Loss  0.003479230450466275\n",
            "Epoch  16 Batch  452 / 488  Training Loss  0.003231419250369072\n",
            "Epoch  16 Batch  453 / 488  Training Loss  0.008519221097230911\n",
            "Epoch  16 Batch  454 / 488  Training Loss  0.002505502663552761\n",
            "Epoch  16 Batch  455 / 488  Training Loss  0.00987144373357296\n",
            "Epoch  16 Batch  456 / 488  Training Loss  0.0034644838888198137\n",
            "Epoch  16 Batch  457 / 488  Training Loss  0.00314900279045105\n",
            "Epoch  16 Batch  458 / 488  Training Loss  0.009391809813678265\n",
            "Epoch  16 Batch  459 / 488  Training Loss  0.0029670719522982836\n",
            "Epoch  16 Batch  460 / 488  Training Loss  0.008674589917063713\n",
            "Epoch  16 Batch  461 / 488  Training Loss  0.007364144083112478\n",
            "Epoch  16 Batch  462 / 488  Training Loss  0.0033982209861278534\n",
            "Epoch  16 Batch  463 / 488  Training Loss  0.002685925457626581\n",
            "Epoch  16 Batch  464 / 488  Training Loss  0.004644764121621847\n",
            "Epoch  16 Batch  465 / 488  Training Loss  0.002404593862593174\n",
            "Epoch  16 Batch  466 / 488  Training Loss  0.002176616806536913\n",
            "Epoch  16 Batch  467 / 488  Training Loss  0.004138207994401455\n",
            "Epoch  16 Batch  468 / 488  Training Loss  0.005202149506658316\n",
            "Epoch  16 Batch  469 / 488  Training Loss  0.004909397102892399\n",
            "Epoch  16 Batch  470 / 488  Training Loss  0.003474542871117592\n",
            "Epoch  16 Batch  471 / 488  Training Loss  0.002412499161437154\n",
            "Epoch  16 Batch  472 / 488  Training Loss  0.0022834078408777714\n",
            "Epoch  16 Batch  473 / 488  Training Loss  0.0066316635347902775\n",
            "Epoch  16 Batch  474 / 488  Training Loss  0.0025264981668442488\n",
            "Epoch  16 Batch  475 / 488  Training Loss  0.006987770088016987\n",
            "Epoch  16 Batch  476 / 488  Training Loss  0.005433409009128809\n",
            "Epoch  16 Batch  477 / 488  Training Loss  0.004351251758635044\n",
            "Epoch  16 Batch  478 / 488  Training Loss  0.006810017861425877\n",
            "Epoch  16 Batch  479 / 488  Training Loss  0.013957200571894646\n",
            "Epoch  16 Batch  480 / 488  Training Loss  0.004321751184761524\n",
            "Epoch  16 Batch  481 / 488  Training Loss  0.004980788100510836\n",
            "Epoch  16 Batch  482 / 488  Training Loss  0.0021215148735791445\n",
            "Epoch  16 Batch  483 / 488  Training Loss  0.011266213841736317\n",
            "Epoch  16 Batch  484 / 488  Training Loss  0.00488984864205122\n",
            "Epoch  16 Batch  485 / 488  Training Loss  0.0033356465864926577\n",
            "Epoch  16 Batch  486 / 488  Training Loss  0.0024912459775805473\n",
            "Epoch  16 Batch  487 / 488  Training Loss  0.002606597961857915\n",
            "  17    |    -    |   0.004651   | 44.824911\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 17\n",
            "Epoch  17 Batch  0 / 488  Training Loss  0.003139986889436841\n",
            "Epoch  17 Batch  1 / 488  Training Loss  0.0015968462685123086\n",
            "Epoch  17 Batch  2 / 488  Training Loss  0.002784355077892542\n",
            "Epoch  17 Batch  3 / 488  Training Loss  0.002170163206756115\n",
            "Epoch  17 Batch  4 / 488  Training Loss  0.001967737916857004\n",
            "Epoch  17 Batch  5 / 488  Training Loss  0.0007680159760639071\n",
            "Epoch  17 Batch  6 / 488  Training Loss  0.0036389504093676805\n",
            "Epoch  17 Batch  7 / 488  Training Loss  0.0008989592897705734\n",
            "Epoch  17 Batch  8 / 488  Training Loss  0.0010398856829851866\n",
            "Epoch  17 Batch  9 / 488  Training Loss  0.0009204702218994498\n",
            "Epoch  17 Batch  10 / 488  Training Loss  0.00226394971832633\n",
            "Epoch  17 Batch  11 / 488  Training Loss  0.0016161014791578054\n",
            "Epoch  17 Batch  12 / 488  Training Loss  0.0018294432666152716\n",
            "Epoch  17 Batch  13 / 488  Training Loss  0.008610058575868607\n",
            "Epoch  17 Batch  14 / 488  Training Loss  0.001091641839593649\n",
            "Epoch  17 Batch  15 / 488  Training Loss  0.0015933854738250375\n",
            "Epoch  17 Batch  16 / 488  Training Loss  0.005132350604981184\n",
            "Epoch  17 Batch  17 / 488  Training Loss  0.0019163035321980715\n",
            "Epoch  17 Batch  18 / 488  Training Loss  0.0013834289275109768\n",
            "Epoch  17 Batch  19 / 488  Training Loss  0.0038997374940663576\n",
            "Epoch  17 Batch  20 / 488  Training Loss  0.0013533441815525293\n",
            "Epoch  17 Batch  21 / 488  Training Loss  0.0019268244504928589\n",
            "Epoch  17 Batch  22 / 488  Training Loss  0.0010254981461912394\n",
            "Epoch  17 Batch  23 / 488  Training Loss  0.002595515688881278\n",
            "Epoch  17 Batch  24 / 488  Training Loss  0.0019751638174057007\n",
            "Epoch  17 Batch  25 / 488  Training Loss  0.0013729073107242584\n",
            "Epoch  17 Batch  26 / 488  Training Loss  0.0013621641555801034\n",
            "Epoch  17 Batch  27 / 488  Training Loss  0.0016999595100060105\n",
            "Epoch  17 Batch  28 / 488  Training Loss  0.0010879447218030691\n",
            "Epoch  17 Batch  29 / 488  Training Loss  0.0016583927208557725\n",
            "Epoch  17 Batch  30 / 488  Training Loss  0.0005361403455026448\n",
            "Epoch  17 Batch  31 / 488  Training Loss  0.0017643250757828355\n",
            "Epoch  17 Batch  32 / 488  Training Loss  0.0027574144769459963\n",
            "Epoch  17 Batch  33 / 488  Training Loss  0.002059368183836341\n",
            "Epoch  17 Batch  34 / 488  Training Loss  0.0013214285718277097\n",
            "Epoch  17 Batch  35 / 488  Training Loss  0.0025099324993789196\n",
            "Epoch  17 Batch  36 / 488  Training Loss  0.0017265023197978735\n",
            "Epoch  17 Batch  37 / 488  Training Loss  0.013471335172653198\n",
            "Epoch  17 Batch  38 / 488  Training Loss  0.0010747226187959313\n",
            "Epoch  17 Batch  39 / 488  Training Loss  0.0015588684473186731\n",
            "Epoch  17 Batch  40 / 488  Training Loss  0.0018485852051526308\n",
            "Epoch  17 Batch  41 / 488  Training Loss  0.0022300146520137787\n",
            "Epoch  17 Batch  42 / 488  Training Loss  0.0013310628710314631\n",
            "Epoch  17 Batch  43 / 488  Training Loss  0.005259154364466667\n",
            "Epoch  17 Batch  44 / 488  Training Loss  0.001887778053060174\n",
            "Epoch  17 Batch  45 / 488  Training Loss  0.0018895758548751473\n",
            "Epoch  17 Batch  46 / 488  Training Loss  0.0071870931424200535\n",
            "Epoch  17 Batch  47 / 488  Training Loss  0.001953932223841548\n",
            "Epoch  17 Batch  48 / 488  Training Loss  0.002807757817208767\n",
            "Epoch  17 Batch  49 / 488  Training Loss  0.002446230035275221\n",
            "Epoch  17 Batch  50 / 488  Training Loss  0.0012668977724388242\n",
            "Epoch  17 Batch  51 / 488  Training Loss  0.0011889279121533036\n",
            "Epoch  17 Batch  52 / 488  Training Loss  0.002937770914286375\n",
            "Epoch  17 Batch  53 / 488  Training Loss  0.0006808485486544669\n",
            "Epoch  17 Batch  54 / 488  Training Loss  0.0008472432382404804\n",
            "Epoch  17 Batch  55 / 488  Training Loss  0.00137006223667413\n",
            "Epoch  17 Batch  56 / 488  Training Loss  0.004077061079442501\n",
            "Epoch  17 Batch  57 / 488  Training Loss  0.0011004406260326505\n",
            "Epoch  17 Batch  58 / 488  Training Loss  0.001319854985922575\n",
            "Epoch  17 Batch  59 / 488  Training Loss  0.0019109789282083511\n",
            "Epoch  17 Batch  60 / 488  Training Loss  0.0017988437321037054\n",
            "Epoch  17 Batch  61 / 488  Training Loss  0.00193010363727808\n",
            "Epoch  17 Batch  62 / 488  Training Loss  0.0023870104923844337\n",
            "Epoch  17 Batch  63 / 488  Training Loss  0.0013764607720077038\n",
            "Epoch  17 Batch  64 / 488  Training Loss  0.005421947222203016\n",
            "Epoch  17 Batch  65 / 488  Training Loss  0.057596605271101\n",
            "Epoch  17 Batch  66 / 488  Training Loss  0.003194061340764165\n",
            "Epoch  17 Batch  67 / 488  Training Loss  0.007854660972952843\n",
            "Epoch  17 Batch  68 / 488  Training Loss  0.010251270607113838\n",
            "Epoch  17 Batch  69 / 488  Training Loss  0.0021035566460341215\n",
            "Epoch  17 Batch  70 / 488  Training Loss  0.019563283771276474\n",
            "Epoch  17 Batch  71 / 488  Training Loss  0.023771140724420547\n",
            "Epoch  17 Batch  72 / 488  Training Loss  0.003810520051047206\n",
            "Epoch  17 Batch  73 / 488  Training Loss  0.003427628893405199\n",
            "Epoch  17 Batch  74 / 488  Training Loss  0.0025838573928922415\n",
            "Epoch  17 Batch  75 / 488  Training Loss  0.0008417865028604865\n",
            "Epoch  17 Batch  76 / 488  Training Loss  0.007363148033618927\n",
            "Epoch  17 Batch  77 / 488  Training Loss  0.005139787215739489\n",
            "Epoch  17 Batch  78 / 488  Training Loss  0.009041501209139824\n",
            "Epoch  17 Batch  79 / 488  Training Loss  0.0036189816892147064\n",
            "Epoch  17 Batch  80 / 488  Training Loss  0.0035651724319905043\n",
            "Epoch  17 Batch  81 / 488  Training Loss  0.003938686568289995\n",
            "Epoch  17 Batch  82 / 488  Training Loss  0.003130634780973196\n",
            "Epoch  17 Batch  83 / 488  Training Loss  0.002314482582733035\n",
            "Epoch  17 Batch  84 / 488  Training Loss  0.0029209183994680643\n",
            "Epoch  17 Batch  85 / 488  Training Loss  0.0036255200393497944\n",
            "Epoch  17 Batch  86 / 488  Training Loss  0.0025959298945963383\n",
            "Epoch  17 Batch  87 / 488  Training Loss  0.00944318063557148\n",
            "Epoch  17 Batch  88 / 488  Training Loss  0.0022301243152469397\n",
            "Epoch  17 Batch  89 / 488  Training Loss  0.0029942060355097055\n",
            "Epoch  17 Batch  90 / 488  Training Loss  0.007969284430146217\n",
            "Epoch  17 Batch  91 / 488  Training Loss  0.0033979429863393307\n",
            "Epoch  17 Batch  92 / 488  Training Loss  0.0018407779280096292\n",
            "Epoch  17 Batch  93 / 488  Training Loss  0.002995198592543602\n",
            "Epoch  17 Batch  94 / 488  Training Loss  0.00305479159578681\n",
            "Epoch  17 Batch  95 / 488  Training Loss  0.0016184048727154732\n",
            "Epoch  17 Batch  96 / 488  Training Loss  0.003623509081080556\n",
            "Epoch  17 Batch  97 / 488  Training Loss  0.003774254349991679\n",
            "Epoch  17 Batch  98 / 488  Training Loss  0.005459302105009556\n",
            "Epoch  17 Batch  99 / 488  Training Loss  0.0019890114199370146\n",
            "Epoch  17 Batch  100 / 488  Training Loss  0.0033561333548277617\n",
            "Epoch  17 Batch  101 / 488  Training Loss  0.004079886712133884\n",
            "Epoch  17 Batch  102 / 488  Training Loss  0.0014846703270450234\n",
            "Epoch  17 Batch  103 / 488  Training Loss  0.002663017250597477\n",
            "Epoch  17 Batch  104 / 488  Training Loss  0.00237799109891057\n",
            "Epoch  17 Batch  105 / 488  Training Loss  0.0037470695096999407\n",
            "Epoch  17 Batch  106 / 488  Training Loss  0.0016864649951457977\n",
            "Epoch  17 Batch  107 / 488  Training Loss  0.004728711675852537\n",
            "Epoch  17 Batch  108 / 488  Training Loss  0.00329168071039021\n",
            "Epoch  17 Batch  109 / 488  Training Loss  0.002035753335803747\n",
            "Epoch  17 Batch  110 / 488  Training Loss  0.002268956508487463\n",
            "Epoch  17 Batch  111 / 488  Training Loss  0.002005388494580984\n",
            "Epoch  17 Batch  112 / 488  Training Loss  0.00503704184666276\n",
            "Epoch  17 Batch  113 / 488  Training Loss  0.006332571152597666\n",
            "Epoch  17 Batch  114 / 488  Training Loss  0.0011483512353152037\n",
            "Epoch  17 Batch  115 / 488  Training Loss  0.0006302200490608811\n",
            "Epoch  17 Batch  116 / 488  Training Loss  0.002673544455319643\n",
            "Epoch  17 Batch  117 / 488  Training Loss  0.0027634194120764732\n",
            "Epoch  17 Batch  118 / 488  Training Loss  0.006084130145609379\n",
            "Epoch  17 Batch  119 / 488  Training Loss  0.005399087909609079\n",
            "Epoch  17 Batch  120 / 488  Training Loss  0.018132198601961136\n",
            "Epoch  17 Batch  121 / 488  Training Loss  0.0025957198813557625\n",
            "Epoch  17 Batch  122 / 488  Training Loss  0.0031208922155201435\n",
            "Epoch  17 Batch  123 / 488  Training Loss  0.005531048867851496\n",
            "Epoch  17 Batch  124 / 488  Training Loss  0.005631252191960812\n",
            "Epoch  17 Batch  125 / 488  Training Loss  0.0029074852354824543\n",
            "Epoch  17 Batch  126 / 488  Training Loss  0.001340896007604897\n",
            "Epoch  17 Batch  127 / 488  Training Loss  0.003018620191141963\n",
            "Epoch  17 Batch  128 / 488  Training Loss  0.007586940191686153\n",
            "Epoch  17 Batch  129 / 488  Training Loss  0.0013754983665421605\n",
            "Epoch  17 Batch  130 / 488  Training Loss  0.003428966971114278\n",
            "Epoch  17 Batch  131 / 488  Training Loss  0.008971458300948143\n",
            "Epoch  17 Batch  132 / 488  Training Loss  0.004326495807617903\n",
            "Epoch  17 Batch  133 / 488  Training Loss  0.002956025768071413\n",
            "Epoch  17 Batch  134 / 488  Training Loss  0.0062339953146874905\n",
            "Epoch  17 Batch  135 / 488  Training Loss  0.0016091354191303253\n",
            "Epoch  17 Batch  136 / 488  Training Loss  0.001917957910336554\n",
            "Epoch  17 Batch  137 / 488  Training Loss  0.004307187162339687\n",
            "Epoch  17 Batch  138 / 488  Training Loss  0.001184291671961546\n",
            "Epoch  17 Batch  139 / 488  Training Loss  0.0011486064177006483\n",
            "Epoch  17 Batch  140 / 488  Training Loss  0.002017207210883498\n",
            "Epoch  17 Batch  141 / 488  Training Loss  0.001559537835419178\n",
            "Epoch  17 Batch  142 / 488  Training Loss  0.018300332129001617\n",
            "Epoch  17 Batch  143 / 488  Training Loss  0.012444863095879555\n",
            "Epoch  17 Batch  144 / 488  Training Loss  0.006011777091771364\n",
            "Epoch  17 Batch  145 / 488  Training Loss  0.003897275310009718\n",
            "Epoch  17 Batch  146 / 488  Training Loss  0.004021559841930866\n",
            "Epoch  17 Batch  147 / 488  Training Loss  0.0033923685550689697\n",
            "Epoch  17 Batch  148 / 488  Training Loss  0.005604343023151159\n",
            "Epoch  17 Batch  149 / 488  Training Loss  0.0028724197763949633\n",
            "Epoch  17 Batch  150 / 488  Training Loss  0.0029499479569494724\n",
            "Epoch  17 Batch  151 / 488  Training Loss  0.0035994381178170443\n",
            "Epoch  17 Batch  152 / 488  Training Loss  0.015019210986793041\n",
            "Epoch  17 Batch  153 / 488  Training Loss  0.004350076429545879\n",
            "Epoch  17 Batch  154 / 488  Training Loss  0.0028125920798629522\n",
            "Epoch  17 Batch  155 / 488  Training Loss  0.001078062690794468\n",
            "Epoch  17 Batch  156 / 488  Training Loss  0.002970140427350998\n",
            "Epoch  17 Batch  157 / 488  Training Loss  0.004379525780677795\n",
            "Epoch  17 Batch  158 / 488  Training Loss  0.0038290831726044416\n",
            "Epoch  17 Batch  159 / 488  Training Loss  0.005144009832292795\n",
            "Epoch  17 Batch  160 / 488  Training Loss  0.005507389083504677\n",
            "Epoch  17 Batch  161 / 488  Training Loss  0.0014576248358935118\n",
            "Epoch  17 Batch  162 / 488  Training Loss  0.0056187487207353115\n",
            "Epoch  17 Batch  163 / 488  Training Loss  0.0025120454374700785\n",
            "Epoch  17 Batch  164 / 488  Training Loss  0.0018050807993859053\n",
            "Epoch  17 Batch  165 / 488  Training Loss  0.0066797854378819466\n",
            "Epoch  17 Batch  166 / 488  Training Loss  0.0015601339982822537\n",
            "Epoch  17 Batch  167 / 488  Training Loss  0.003508778987452388\n",
            "Epoch  17 Batch  168 / 488  Training Loss  0.0012973740231245756\n",
            "Epoch  17 Batch  169 / 488  Training Loss  0.005356432404369116\n",
            "Epoch  17 Batch  170 / 488  Training Loss  0.0010073917219415307\n",
            "Epoch  17 Batch  171 / 488  Training Loss  0.002628362737596035\n",
            "Epoch  17 Batch  172 / 488  Training Loss  0.0010156226344406605\n",
            "Epoch  17 Batch  173 / 488  Training Loss  0.0011823594104498625\n",
            "Epoch  17 Batch  174 / 488  Training Loss  0.0044648293405771255\n",
            "Epoch  17 Batch  175 / 488  Training Loss  0.0026251182425767183\n",
            "Epoch  17 Batch  176 / 488  Training Loss  0.0013189328601583838\n",
            "Epoch  17 Batch  177 / 488  Training Loss  0.0034576994366943836\n",
            "Epoch  17 Batch  178 / 488  Training Loss  0.002130978973582387\n",
            "Epoch  17 Batch  179 / 488  Training Loss  0.0029598341789096594\n",
            "Epoch  17 Batch  180 / 488  Training Loss  0.003906081197783351\n",
            "Epoch  17 Batch  181 / 488  Training Loss  0.0019871401600539684\n",
            "Epoch  17 Batch  182 / 488  Training Loss  0.0020516165532171726\n",
            "Epoch  17 Batch  183 / 488  Training Loss  0.00478023337200284\n",
            "Epoch  17 Batch  184 / 488  Training Loss  0.007893390022218227\n",
            "Epoch  17 Batch  185 / 488  Training Loss  0.005550697445869446\n",
            "Epoch  17 Batch  186 / 488  Training Loss  0.004082388710230589\n",
            "Epoch  17 Batch  187 / 488  Training Loss  0.0014161088038235903\n",
            "Epoch  17 Batch  188 / 488  Training Loss  0.006505151279270649\n",
            "Epoch  17 Batch  189 / 488  Training Loss  0.007898015901446342\n",
            "Epoch  17 Batch  190 / 488  Training Loss  0.0015040759462863207\n",
            "Epoch  17 Batch  191 / 488  Training Loss  0.0056952000595629215\n",
            "Epoch  17 Batch  192 / 488  Training Loss  0.0022137616761028767\n",
            "Epoch  17 Batch  193 / 488  Training Loss  0.0014971273485571146\n",
            "Epoch  17 Batch  194 / 488  Training Loss  0.0036836799699813128\n",
            "Epoch  17 Batch  195 / 488  Training Loss  0.006948111113160849\n",
            "Epoch  17 Batch  196 / 488  Training Loss  0.002261837711557746\n",
            "Epoch  17 Batch  197 / 488  Training Loss  0.002031647600233555\n",
            "Epoch  17 Batch  198 / 488  Training Loss  0.003414153354242444\n",
            "Epoch  17 Batch  199 / 488  Training Loss  0.000935802236199379\n",
            "Epoch  17 Batch  200 / 488  Training Loss  0.004530246369540691\n",
            "Epoch  17 Batch  201 / 488  Training Loss  0.000848360185045749\n",
            "Epoch  17 Batch  202 / 488  Training Loss  0.0043372199870646\n",
            "Epoch  17 Batch  203 / 488  Training Loss  0.002843032358214259\n",
            "Epoch  17 Batch  204 / 488  Training Loss  0.00825110636651516\n",
            "Epoch  17 Batch  205 / 488  Training Loss  0.005941371433436871\n",
            "Epoch  17 Batch  206 / 488  Training Loss  0.0020923519041389227\n",
            "Epoch  17 Batch  207 / 488  Training Loss  0.0026484450791031122\n",
            "Epoch  17 Batch  208 / 488  Training Loss  0.0037137058097869158\n",
            "Epoch  17 Batch  209 / 488  Training Loss  0.004417306277900934\n",
            "Epoch  17 Batch  210 / 488  Training Loss  0.0020252191461622715\n",
            "Epoch  17 Batch  211 / 488  Training Loss  0.004265950061380863\n",
            "Epoch  17 Batch  212 / 488  Training Loss  0.001692619756795466\n",
            "Epoch  17 Batch  213 / 488  Training Loss  0.004338585771620274\n",
            "Epoch  17 Batch  214 / 488  Training Loss  0.0008271993137896061\n",
            "Epoch  17 Batch  215 / 488  Training Loss  0.005012970417737961\n",
            "Epoch  17 Batch  216 / 488  Training Loss  0.011538557708263397\n",
            "Epoch  17 Batch  217 / 488  Training Loss  0.005402241833508015\n",
            "Epoch  17 Batch  218 / 488  Training Loss  0.0024498028215020895\n",
            "Epoch  17 Batch  219 / 488  Training Loss  0.0027818232774734497\n",
            "Epoch  17 Batch  220 / 488  Training Loss  0.001990597229450941\n",
            "Epoch  17 Batch  221 / 488  Training Loss  0.001985925016924739\n",
            "Epoch  17 Batch  222 / 488  Training Loss  0.002012978307902813\n",
            "Epoch  17 Batch  223 / 488  Training Loss  0.0046198079362511635\n",
            "Epoch  17 Batch  224 / 488  Training Loss  0.0033011112827807665\n",
            "Epoch  17 Batch  225 / 488  Training Loss  0.002906632609665394\n",
            "Epoch  17 Batch  226 / 488  Training Loss  0.005641389172524214\n",
            "Epoch  17 Batch  227 / 488  Training Loss  0.0013323230668902397\n",
            "Epoch  17 Batch  228 / 488  Training Loss  0.0025497819297015667\n",
            "Epoch  17 Batch  229 / 488  Training Loss  0.00406718859449029\n",
            "Epoch  17 Batch  230 / 488  Training Loss  0.006396631710231304\n",
            "Epoch  17 Batch  231 / 488  Training Loss  0.0033300076611340046\n",
            "Epoch  17 Batch  232 / 488  Training Loss  0.0048058899119496346\n",
            "Epoch  17 Batch  233 / 488  Training Loss  0.005845028907060623\n",
            "Epoch  17 Batch  234 / 488  Training Loss  0.006408907473087311\n",
            "Epoch  17 Batch  235 / 488  Training Loss  0.0029257633723318577\n",
            "Epoch  17 Batch  236 / 488  Training Loss  0.004895712248980999\n",
            "Epoch  17 Batch  237 / 488  Training Loss  0.004081172868609428\n",
            "Epoch  17 Batch  238 / 488  Training Loss  0.0027843057177960873\n",
            "Epoch  17 Batch  239 / 488  Training Loss  0.006408140063285828\n",
            "Epoch  17 Batch  240 / 488  Training Loss  0.002369357505813241\n",
            "Epoch  17 Batch  241 / 488  Training Loss  0.002172616310417652\n",
            "Epoch  17 Batch  242 / 488  Training Loss  0.0022576404735445976\n",
            "Epoch  17 Batch  243 / 488  Training Loss  0.004622614476829767\n",
            "Epoch  17 Batch  244 / 488  Training Loss  0.0025707376189529896\n",
            "Epoch  17 Batch  245 / 488  Training Loss  0.003887107130140066\n",
            "Epoch  17 Batch  246 / 488  Training Loss  0.0012237986084073782\n",
            "Epoch  17 Batch  247 / 488  Training Loss  0.0023370985873043537\n",
            "Epoch  17 Batch  248 / 488  Training Loss  0.006986662745475769\n",
            "Epoch  17 Batch  249 / 488  Training Loss  0.004643513821065426\n",
            "Epoch  17 Batch  250 / 488  Training Loss  0.004523460753262043\n",
            "Epoch  17 Batch  251 / 488  Training Loss  0.0008178786374628544\n",
            "Epoch  17 Batch  252 / 488  Training Loss  0.001600585295818746\n",
            "Epoch  17 Batch  253 / 488  Training Loss  0.001031364081427455\n",
            "Epoch  17 Batch  254 / 488  Training Loss  0.0051576741971075535\n",
            "Epoch  17 Batch  255 / 488  Training Loss  0.002538159256801009\n",
            "Epoch  17 Batch  256 / 488  Training Loss  0.0012851203791797161\n",
            "Epoch  17 Batch  257 / 488  Training Loss  0.0034966743551194668\n",
            "Epoch  17 Batch  258 / 488  Training Loss  0.0027558289002627134\n",
            "Epoch  17 Batch  259 / 488  Training Loss  0.002099350793287158\n",
            "Epoch  17 Batch  260 / 488  Training Loss  0.0023617311380803585\n",
            "Epoch  17 Batch  261 / 488  Training Loss  0.0034802041482180357\n",
            "Epoch  17 Batch  262 / 488  Training Loss  0.002539403736591339\n",
            "Epoch  17 Batch  263 / 488  Training Loss  0.0026483472902327776\n",
            "Epoch  17 Batch  264 / 488  Training Loss  0.004118810873478651\n",
            "Epoch  17 Batch  265 / 488  Training Loss  0.0038350378163158894\n",
            "Epoch  17 Batch  266 / 488  Training Loss  0.0019263072172179818\n",
            "Epoch  17 Batch  267 / 488  Training Loss  0.00474578095600009\n",
            "Epoch  17 Batch  268 / 488  Training Loss  0.003399371402338147\n",
            "Epoch  17 Batch  269 / 488  Training Loss  0.0029735900461673737\n",
            "Epoch  17 Batch  270 / 488  Training Loss  0.001990714343264699\n",
            "Epoch  17 Batch  271 / 488  Training Loss  0.00366200995631516\n",
            "Epoch  17 Batch  272 / 488  Training Loss  0.0020798405166715384\n",
            "Epoch  17 Batch  273 / 488  Training Loss  0.005099985282868147\n",
            "Epoch  17 Batch  274 / 488  Training Loss  0.0030053514055907726\n",
            "Epoch  17 Batch  275 / 488  Training Loss  0.002767897443845868\n",
            "Epoch  17 Batch  276 / 488  Training Loss  0.0015083077596500516\n",
            "Epoch  17 Batch  277 / 488  Training Loss  0.002537816297262907\n",
            "Epoch  17 Batch  278 / 488  Training Loss  0.0020642506424337626\n",
            "Epoch  17 Batch  279 / 488  Training Loss  0.0026242234744131565\n",
            "Epoch  17 Batch  280 / 488  Training Loss  0.0009895191760733724\n",
            "Epoch  17 Batch  281 / 488  Training Loss  0.0026771805714815855\n",
            "Epoch  17 Batch  282 / 488  Training Loss  0.0025547384284436703\n",
            "Epoch  17 Batch  283 / 488  Training Loss  0.0030421805568039417\n",
            "Epoch  17 Batch  284 / 488  Training Loss  0.002713471883907914\n",
            "Epoch  17 Batch  285 / 488  Training Loss  0.0009597871685400605\n",
            "Epoch  17 Batch  286 / 488  Training Loss  0.0018729318398982286\n",
            "Epoch  17 Batch  287 / 488  Training Loss  0.0020469119772315025\n",
            "Epoch  17 Batch  288 / 488  Training Loss  0.001725295907817781\n",
            "Epoch  17 Batch  289 / 488  Training Loss  0.002177785150706768\n",
            "Epoch  17 Batch  290 / 488  Training Loss  0.0032784610521048307\n",
            "Epoch  17 Batch  291 / 488  Training Loss  0.0025635811034590006\n",
            "Epoch  17 Batch  292 / 488  Training Loss  0.008560368791222572\n",
            "Epoch  17 Batch  293 / 488  Training Loss  0.0014015897177159786\n",
            "Epoch  17 Batch  294 / 488  Training Loss  0.0014834447065368295\n",
            "Epoch  17 Batch  295 / 488  Training Loss  0.0020903812255710363\n",
            "Epoch  17 Batch  296 / 488  Training Loss  0.002315689343959093\n",
            "Epoch  17 Batch  297 / 488  Training Loss  0.002484007040038705\n",
            "Epoch  17 Batch  298 / 488  Training Loss  0.002099386416375637\n",
            "Epoch  17 Batch  299 / 488  Training Loss  0.0040942467749118805\n",
            "Epoch  17 Batch  300 / 488  Training Loss  0.003994110971689224\n",
            "Epoch  17 Batch  301 / 488  Training Loss  0.004403997212648392\n",
            "Epoch  17 Batch  302 / 488  Training Loss  0.015316003933548927\n",
            "Epoch  17 Batch  303 / 488  Training Loss  0.00311683164909482\n",
            "Epoch  17 Batch  304 / 488  Training Loss  0.002781038638204336\n",
            "Epoch  17 Batch  305 / 488  Training Loss  0.006742909550666809\n",
            "Epoch  17 Batch  306 / 488  Training Loss  0.0018816819647327065\n",
            "Epoch  17 Batch  307 / 488  Training Loss  0.001982730347663164\n",
            "Epoch  17 Batch  308 / 488  Training Loss  0.0038474486209452152\n",
            "Epoch  17 Batch  309 / 488  Training Loss  0.003506277920678258\n",
            "Epoch  17 Batch  310 / 488  Training Loss  0.00223171291872859\n",
            "Epoch  17 Batch  311 / 488  Training Loss  0.006387896835803986\n",
            "Epoch  17 Batch  312 / 488  Training Loss  0.00430794432759285\n",
            "Epoch  17 Batch  313 / 488  Training Loss  0.005926704499870539\n",
            "Epoch  17 Batch  314 / 488  Training Loss  0.0017870042938739061\n",
            "Epoch  17 Batch  315 / 488  Training Loss  0.004536750726401806\n",
            "Epoch  17 Batch  316 / 488  Training Loss  0.0015669645508751273\n",
            "Epoch  17 Batch  317 / 488  Training Loss  0.002520471578463912\n",
            "Epoch  17 Batch  318 / 488  Training Loss  0.0014513748465105891\n",
            "Epoch  17 Batch  319 / 488  Training Loss  0.0014241748722270131\n",
            "Epoch  17 Batch  320 / 488  Training Loss  0.0063890391029417515\n",
            "Epoch  17 Batch  321 / 488  Training Loss  0.0016538295894861221\n",
            "Epoch  17 Batch  322 / 488  Training Loss  0.006154204718768597\n",
            "Epoch  17 Batch  323 / 488  Training Loss  0.004197297617793083\n",
            "Epoch  17 Batch  324 / 488  Training Loss  0.003887349274009466\n",
            "Epoch  17 Batch  325 / 488  Training Loss  0.0013744633179157972\n",
            "Epoch  17 Batch  326 / 488  Training Loss  0.002498398767784238\n",
            "Epoch  17 Batch  327 / 488  Training Loss  0.0028847460635006428\n",
            "Epoch  17 Batch  328 / 488  Training Loss  0.001731142750941217\n",
            "Epoch  17 Batch  329 / 488  Training Loss  0.005082175135612488\n",
            "Epoch  17 Batch  330 / 488  Training Loss  0.0018804306164383888\n",
            "Epoch  17 Batch  331 / 488  Training Loss  0.003387837205082178\n",
            "Epoch  17 Batch  332 / 488  Training Loss  0.003669094992801547\n",
            "Epoch  17 Batch  333 / 488  Training Loss  0.00198598625138402\n",
            "Epoch  17 Batch  334 / 488  Training Loss  0.0016948938136920333\n",
            "Epoch  17 Batch  335 / 488  Training Loss  0.0055905906483531\n",
            "Epoch  17 Batch  336 / 488  Training Loss  0.0016917751636356115\n",
            "Epoch  17 Batch  337 / 488  Training Loss  0.002227672142907977\n",
            "Epoch  17 Batch  338 / 488  Training Loss  0.0034079551696777344\n",
            "Epoch  17 Batch  339 / 488  Training Loss  0.008340811356902122\n",
            "Epoch  17 Batch  340 / 488  Training Loss  0.0021763150580227375\n",
            "Epoch  17 Batch  341 / 488  Training Loss  0.0020740574691444635\n",
            "Epoch  17 Batch  342 / 488  Training Loss  0.00280363648198545\n",
            "Epoch  17 Batch  343 / 488  Training Loss  0.0007057682960294187\n",
            "Epoch  17 Batch  344 / 488  Training Loss  0.0023977188393473625\n",
            "Epoch  17 Batch  345 / 488  Training Loss  0.00501449266448617\n",
            "Epoch  17 Batch  346 / 488  Training Loss  0.002326031681150198\n",
            "Epoch  17 Batch  347 / 488  Training Loss  0.0018944566836580634\n",
            "Epoch  17 Batch  348 / 488  Training Loss  0.0017751868581399322\n",
            "Epoch  17 Batch  349 / 488  Training Loss  0.002087330212816596\n",
            "Epoch  17 Batch  350 / 488  Training Loss  0.0068228901363909245\n",
            "Epoch  17 Batch  351 / 488  Training Loss  0.003107043681666255\n",
            "Epoch  17 Batch  352 / 488  Training Loss  0.0027278366032987833\n",
            "Epoch  17 Batch  353 / 488  Training Loss  0.00326223555020988\n",
            "Epoch  17 Batch  354 / 488  Training Loss  0.004264979623258114\n",
            "Epoch  17 Batch  355 / 488  Training Loss  0.0031615905463695526\n",
            "Epoch  17 Batch  356 / 488  Training Loss  0.004135075956583023\n",
            "Epoch  17 Batch  357 / 488  Training Loss  0.002004641108214855\n",
            "Epoch  17 Batch  358 / 488  Training Loss  0.0021530373487621546\n",
            "Epoch  17 Batch  359 / 488  Training Loss  0.0013712748186662793\n",
            "Epoch  17 Batch  360 / 488  Training Loss  0.003882934106513858\n",
            "Epoch  17 Batch  361 / 488  Training Loss  0.0021364022977650166\n",
            "Epoch  17 Batch  362 / 488  Training Loss  0.0053562261164188385\n",
            "Epoch  17 Batch  363 / 488  Training Loss  0.005286772735416889\n",
            "Epoch  17 Batch  364 / 488  Training Loss  0.004576358012855053\n",
            "Epoch  17 Batch  365 / 488  Training Loss  0.006126349791884422\n",
            "Epoch  17 Batch  366 / 488  Training Loss  0.0014804757665842772\n",
            "Epoch  17 Batch  367 / 488  Training Loss  0.011444349773228168\n",
            "Epoch  17 Batch  368 / 488  Training Loss  0.0058365389704704285\n",
            "Epoch  17 Batch  369 / 488  Training Loss  0.003313062246888876\n",
            "Epoch  17 Batch  370 / 488  Training Loss  0.0021516787819564342\n",
            "Epoch  17 Batch  371 / 488  Training Loss  0.002350473077967763\n",
            "Epoch  17 Batch  372 / 488  Training Loss  0.004525851923972368\n",
            "Epoch  17 Batch  373 / 488  Training Loss  0.0011190093355253339\n",
            "Epoch  17 Batch  374 / 488  Training Loss  0.0011683969059959054\n",
            "Epoch  17 Batch  375 / 488  Training Loss  0.0015859489794820547\n",
            "Epoch  17 Batch  376 / 488  Training Loss  0.0038694809190928936\n",
            "Epoch  17 Batch  377 / 488  Training Loss  0.0016014300053939223\n",
            "Epoch  17 Batch  378 / 488  Training Loss  0.007629492785781622\n",
            "Epoch  17 Batch  379 / 488  Training Loss  0.007737905718386173\n",
            "Epoch  17 Batch  380 / 488  Training Loss  0.0031978716142475605\n",
            "Epoch  17 Batch  381 / 488  Training Loss  0.005588348023593426\n",
            "Epoch  17 Batch  382 / 488  Training Loss  0.004763749428093433\n",
            "Epoch  17 Batch  383 / 488  Training Loss  0.0007681752904318273\n",
            "Epoch  17 Batch  384 / 488  Training Loss  0.0021130708046257496\n",
            "Epoch  17 Batch  385 / 488  Training Loss  0.005319991614669561\n",
            "Epoch  17 Batch  386 / 488  Training Loss  0.007780263666063547\n",
            "Epoch  17 Batch  387 / 488  Training Loss  0.0025267829187214375\n",
            "Epoch  17 Batch  388 / 488  Training Loss  0.0020961330737918615\n",
            "Epoch  17 Batch  389 / 488  Training Loss  0.0021027240436524153\n",
            "Epoch  17 Batch  390 / 488  Training Loss  0.0058602942153811455\n",
            "Epoch  17 Batch  391 / 488  Training Loss  0.0021187984384596348\n",
            "Epoch  17 Batch  392 / 488  Training Loss  0.011359697207808495\n",
            "Epoch  17 Batch  393 / 488  Training Loss  0.002411657478660345\n",
            "Epoch  17 Batch  394 / 488  Training Loss  0.00625974964350462\n",
            "Epoch  17 Batch  395 / 488  Training Loss  0.0039565302431583405\n",
            "Epoch  17 Batch  396 / 488  Training Loss  0.0021545258350670338\n",
            "Epoch  17 Batch  397 / 488  Training Loss  0.0027994071133434772\n",
            "Epoch  17 Batch  398 / 488  Training Loss  0.0038680131547152996\n",
            "Epoch  17 Batch  399 / 488  Training Loss  0.0027286417316645384\n",
            "Epoch  17 Batch  400 / 488  Training Loss  0.0029263272881507874\n",
            "Epoch  17 Batch  401 / 488  Training Loss  0.0019426631042733788\n",
            "Epoch  17 Batch  402 / 488  Training Loss  0.002144792815670371\n",
            "Epoch  17 Batch  403 / 488  Training Loss  0.0018881618743762374\n",
            "Epoch  17 Batch  404 / 488  Training Loss  0.0027836314402520657\n",
            "Epoch  17 Batch  405 / 488  Training Loss  0.0029521468095481396\n",
            "Epoch  17 Batch  406 / 488  Training Loss  0.0012545717181637883\n",
            "Epoch  17 Batch  407 / 488  Training Loss  0.0013424578355625272\n",
            "Epoch  17 Batch  408 / 488  Training Loss  0.003543758299201727\n",
            "Epoch  17 Batch  409 / 488  Training Loss  0.0041006868705153465\n",
            "Epoch  17 Batch  410 / 488  Training Loss  0.0022903764620423317\n",
            "Epoch  17 Batch  411 / 488  Training Loss  0.001163778593763709\n",
            "Epoch  17 Batch  412 / 488  Training Loss  0.0028290972113609314\n",
            "Epoch  17 Batch  413 / 488  Training Loss  0.0020522424019873142\n",
            "Epoch  17 Batch  414 / 488  Training Loss  0.003404209390282631\n",
            "Epoch  17 Batch  415 / 488  Training Loss  0.002202026080340147\n",
            "Epoch  17 Batch  416 / 488  Training Loss  0.006919294595718384\n",
            "Epoch  17 Batch  417 / 488  Training Loss  0.0015197840984910727\n",
            "Epoch  17 Batch  418 / 488  Training Loss  0.001714460551738739\n",
            "Epoch  17 Batch  419 / 488  Training Loss  0.005227027926594019\n",
            "Epoch  17 Batch  420 / 488  Training Loss  0.026094499975442886\n",
            "Epoch  17 Batch  421 / 488  Training Loss  0.002294339006766677\n",
            "Epoch  17 Batch  422 / 488  Training Loss  0.0022005082573741674\n",
            "Epoch  17 Batch  423 / 488  Training Loss  0.003131094854325056\n",
            "Epoch  17 Batch  424 / 488  Training Loss  0.0024966083001345396\n",
            "Epoch  17 Batch  425 / 488  Training Loss  0.009577346965670586\n",
            "Epoch  17 Batch  426 / 488  Training Loss  0.002319066086784005\n",
            "Epoch  17 Batch  427 / 488  Training Loss  0.0033290020655840635\n",
            "Epoch  17 Batch  428 / 488  Training Loss  0.0017531957710161805\n",
            "Epoch  17 Batch  429 / 488  Training Loss  0.001968498807400465\n",
            "Epoch  17 Batch  430 / 488  Training Loss  0.0012265309924259782\n",
            "Epoch  17 Batch  431 / 488  Training Loss  0.0013198688393458724\n",
            "Epoch  17 Batch  432 / 488  Training Loss  0.001664983807131648\n",
            "Epoch  17 Batch  433 / 488  Training Loss  0.01334399450570345\n",
            "Epoch  17 Batch  434 / 488  Training Loss  0.002519383793696761\n",
            "Epoch  17 Batch  435 / 488  Training Loss  0.002571206307038665\n",
            "Epoch  17 Batch  436 / 488  Training Loss  0.004335039760917425\n",
            "Epoch  17 Batch  437 / 488  Training Loss  0.0029010851867496967\n",
            "Epoch  17 Batch  438 / 488  Training Loss  0.002803162205964327\n",
            "Epoch  17 Batch  439 / 488  Training Loss  0.0016799286240711808\n",
            "Epoch  17 Batch  440 / 488  Training Loss  0.0024350364692509174\n",
            "Epoch  17 Batch  441 / 488  Training Loss  0.004567273892462254\n",
            "Epoch  17 Batch  442 / 488  Training Loss  0.007553786970674992\n",
            "Epoch  17 Batch  443 / 488  Training Loss  0.0031699002720415592\n",
            "Epoch  17 Batch  444 / 488  Training Loss  0.004026603884994984\n",
            "Epoch  17 Batch  445 / 488  Training Loss  0.0026199042331427336\n",
            "Epoch  17 Batch  446 / 488  Training Loss  0.0011431907769292593\n",
            "Epoch  17 Batch  447 / 488  Training Loss  0.005549703724682331\n",
            "Epoch  17 Batch  448 / 488  Training Loss  0.0027886859606951475\n",
            "Epoch  17 Batch  449 / 488  Training Loss  0.0013508361298590899\n",
            "Epoch  17 Batch  450 / 488  Training Loss  0.002671848051249981\n",
            "Epoch  17 Batch  451 / 488  Training Loss  0.003668536664918065\n",
            "Epoch  17 Batch  452 / 488  Training Loss  0.0013187106233090162\n",
            "Epoch  17 Batch  453 / 488  Training Loss  0.0009301624377258122\n",
            "Epoch  17 Batch  454 / 488  Training Loss  0.0010114737087860703\n",
            "Epoch  17 Batch  455 / 488  Training Loss  0.00206163851544261\n",
            "Epoch  17 Batch  456 / 488  Training Loss  0.0022417372092604637\n",
            "Epoch  17 Batch  457 / 488  Training Loss  0.0019064430380240083\n",
            "Epoch  17 Batch  458 / 488  Training Loss  0.002253086306154728\n",
            "Epoch  17 Batch  459 / 488  Training Loss  0.010764135047793388\n",
            "Epoch  17 Batch  460 / 488  Training Loss  0.0024207155220210552\n",
            "Epoch  17 Batch  461 / 488  Training Loss  0.00400493573397398\n",
            "Epoch  17 Batch  462 / 488  Training Loss  0.0027954978868365288\n",
            "Epoch  17 Batch  463 / 488  Training Loss  0.0038324929773807526\n",
            "Epoch  17 Batch  464 / 488  Training Loss  0.0040044598281383514\n",
            "Epoch  17 Batch  465 / 488  Training Loss  0.0033136378042399883\n",
            "Epoch  17 Batch  466 / 488  Training Loss  0.0032987999729812145\n",
            "Epoch  17 Batch  467 / 488  Training Loss  0.003784178290516138\n",
            "Epoch  17 Batch  468 / 488  Training Loss  0.003436468541622162\n",
            "Epoch  17 Batch  469 / 488  Training Loss  0.00138836819678545\n",
            "Epoch  17 Batch  470 / 488  Training Loss  0.0030222474597394466\n",
            "Epoch  17 Batch  471 / 488  Training Loss  0.003941765986382961\n",
            "Epoch  17 Batch  472 / 488  Training Loss  0.005993857979774475\n",
            "Epoch  17 Batch  473 / 488  Training Loss  0.0028768726624548435\n",
            "Epoch  17 Batch  474 / 488  Training Loss  0.006428723689168692\n",
            "Epoch  17 Batch  475 / 488  Training Loss  0.0023413258604705334\n",
            "Epoch  17 Batch  476 / 488  Training Loss  0.003965589217841625\n",
            "Epoch  17 Batch  477 / 488  Training Loss  0.0015131545951589942\n",
            "Epoch  17 Batch  478 / 488  Training Loss  0.002020755782723427\n",
            "Epoch  17 Batch  479 / 488  Training Loss  0.0036876301746815443\n",
            "Epoch  17 Batch  480 / 488  Training Loss  0.0034051034599542618\n",
            "Epoch  17 Batch  481 / 488  Training Loss  0.0013155099004507065\n",
            "Epoch  17 Batch  482 / 488  Training Loss  0.0016084176022559404\n",
            "Epoch  17 Batch  483 / 488  Training Loss  0.0025191172026097775\n",
            "Epoch  17 Batch  484 / 488  Training Loss  0.0012018404668197036\n",
            "Epoch  17 Batch  485 / 488  Training Loss  0.00879576988518238\n",
            "Epoch  17 Batch  486 / 488  Training Loss  0.0026702582836151123\n",
            "Epoch  17 Batch  487 / 488  Training Loss  0.0013097056653350592\n",
            "  18    |    -    |   0.003628   | 42.985372\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 18\n",
            "Epoch  18 Batch  0 / 488  Training Loss  0.0029277787543833256\n",
            "Epoch  18 Batch  1 / 488  Training Loss  0.0008325449889525771\n",
            "Epoch  18 Batch  2 / 488  Training Loss  0.0012994259595870972\n",
            "Epoch  18 Batch  3 / 488  Training Loss  0.001213231822475791\n",
            "Epoch  18 Batch  4 / 488  Training Loss  0.0010519369971007109\n",
            "Epoch  18 Batch  5 / 488  Training Loss  0.0013520563952624798\n",
            "Epoch  18 Batch  6 / 488  Training Loss  0.0011145650641992688\n",
            "Epoch  18 Batch  7 / 488  Training Loss  0.0008718984900042415\n",
            "Epoch  18 Batch  8 / 488  Training Loss  0.001180528779514134\n",
            "Epoch  18 Batch  9 / 488  Training Loss  0.0009001491707749665\n",
            "Epoch  18 Batch  10 / 488  Training Loss  0.003849706845358014\n",
            "Epoch  18 Batch  11 / 488  Training Loss  0.0008885584538802505\n",
            "Epoch  18 Batch  12 / 488  Training Loss  0.003585252445191145\n",
            "Epoch  18 Batch  13 / 488  Training Loss  0.004096865653991699\n",
            "Epoch  18 Batch  14 / 488  Training Loss  0.0011869187001138926\n",
            "Epoch  18 Batch  15 / 488  Training Loss  0.0018960356246680021\n",
            "Epoch  18 Batch  16 / 488  Training Loss  0.0008548656478524208\n",
            "Epoch  18 Batch  17 / 488  Training Loss  0.0033957436680793762\n",
            "Epoch  18 Batch  18 / 488  Training Loss  0.0006407752516679466\n",
            "Epoch  18 Batch  19 / 488  Training Loss  0.0012178777251392603\n",
            "Epoch  18 Batch  20 / 488  Training Loss  0.001120234141126275\n",
            "Epoch  18 Batch  21 / 488  Training Loss  0.008220969699323177\n",
            "Epoch  18 Batch  22 / 488  Training Loss  0.0006684659165330231\n",
            "Epoch  18 Batch  23 / 488  Training Loss  0.0013397845905274153\n",
            "Epoch  18 Batch  24 / 488  Training Loss  0.0019483536016196012\n",
            "Epoch  18 Batch  25 / 488  Training Loss  0.001231157686561346\n",
            "Epoch  18 Batch  26 / 488  Training Loss  0.001627997844479978\n",
            "Epoch  18 Batch  27 / 488  Training Loss  0.001967835705727339\n",
            "Epoch  18 Batch  28 / 488  Training Loss  0.0013496989849954844\n",
            "Epoch  18 Batch  29 / 488  Training Loss  0.0009945929050445557\n",
            "Epoch  18 Batch  30 / 488  Training Loss  0.001106214476749301\n",
            "Epoch  18 Batch  31 / 488  Training Loss  0.000834781676530838\n",
            "Epoch  18 Batch  32 / 488  Training Loss  0.0027853096835315228\n",
            "Epoch  18 Batch  33 / 488  Training Loss  0.0005488925380632281\n",
            "Epoch  18 Batch  34 / 488  Training Loss  0.0020368718542158604\n",
            "Epoch  18 Batch  35 / 488  Training Loss  0.0023070613387972116\n",
            "Epoch  18 Batch  36 / 488  Training Loss  0.0015699833165854216\n",
            "Epoch  18 Batch  37 / 488  Training Loss  0.0010104486718773842\n",
            "Epoch  18 Batch  38 / 488  Training Loss  0.001926751108840108\n",
            "Epoch  18 Batch  39 / 488  Training Loss  0.0010353857651352882\n",
            "Epoch  18 Batch  40 / 488  Training Loss  0.0013487408868968487\n",
            "Epoch  18 Batch  41 / 488  Training Loss  0.0014713354175910354\n",
            "Epoch  18 Batch  42 / 488  Training Loss  0.0006545502110384405\n",
            "Epoch  18 Batch  43 / 488  Training Loss  0.0008403660613112152\n",
            "Epoch  18 Batch  44 / 488  Training Loss  0.0010516643524169922\n",
            "Epoch  18 Batch  45 / 488  Training Loss  0.000836207065731287\n",
            "Epoch  18 Batch  46 / 488  Training Loss  0.0011314158327877522\n",
            "Epoch  18 Batch  47 / 488  Training Loss  0.0010586293647065759\n",
            "Epoch  18 Batch  48 / 488  Training Loss  0.0008387052221223712\n",
            "Epoch  18 Batch  49 / 488  Training Loss  0.003007947001606226\n",
            "Epoch  18 Batch  50 / 488  Training Loss  0.0007072030566632748\n",
            "Epoch  18 Batch  51 / 488  Training Loss  0.003253515111282468\n",
            "Epoch  18 Batch  52 / 488  Training Loss  0.001152589451521635\n",
            "Epoch  18 Batch  53 / 488  Training Loss  0.0010364193003624678\n",
            "Epoch  18 Batch  54 / 488  Training Loss  0.0011627417989075184\n",
            "Epoch  18 Batch  55 / 488  Training Loss  0.003863086923956871\n",
            "Epoch  18 Batch  56 / 488  Training Loss  0.0012745166895911098\n",
            "Epoch  18 Batch  57 / 488  Training Loss  0.0011060823453590274\n",
            "Epoch  18 Batch  58 / 488  Training Loss  0.0008626445196568966\n",
            "Epoch  18 Batch  59 / 488  Training Loss  0.0009699794463813305\n",
            "Epoch  18 Batch  60 / 488  Training Loss  0.0021635766606777906\n",
            "Epoch  18 Batch  61 / 488  Training Loss  0.0011942306300625205\n",
            "Epoch  18 Batch  62 / 488  Training Loss  0.0013607211876660585\n",
            "Epoch  18 Batch  63 / 488  Training Loss  0.0012817435199394822\n",
            "Epoch  18 Batch  64 / 488  Training Loss  0.0022214441560208797\n",
            "Epoch  18 Batch  65 / 488  Training Loss  0.0026089902967214584\n",
            "Epoch  18 Batch  66 / 488  Training Loss  0.003358646063134074\n",
            "Epoch  18 Batch  67 / 488  Training Loss  0.001986223738640547\n",
            "Epoch  18 Batch  68 / 488  Training Loss  0.001390727236866951\n",
            "Epoch  18 Batch  69 / 488  Training Loss  0.003935394808650017\n",
            "Epoch  18 Batch  70 / 488  Training Loss  0.0038417638279497623\n",
            "Epoch  18 Batch  71 / 488  Training Loss  0.0013033706927672029\n",
            "Epoch  18 Batch  72 / 488  Training Loss  0.0011649623047560453\n",
            "Epoch  18 Batch  73 / 488  Training Loss  0.0029764336068183184\n",
            "Epoch  18 Batch  74 / 488  Training Loss  0.002206082921475172\n",
            "Epoch  18 Batch  75 / 488  Training Loss  0.0012039181310683489\n",
            "Epoch  18 Batch  76 / 488  Training Loss  0.0012274845503270626\n",
            "Epoch  18 Batch  77 / 488  Training Loss  0.0037005215417593718\n",
            "Epoch  18 Batch  78 / 488  Training Loss  0.0026259892620146275\n",
            "Epoch  18 Batch  79 / 488  Training Loss  0.002394113689661026\n",
            "Epoch  18 Batch  80 / 488  Training Loss  0.0013214342761784792\n",
            "Epoch  18 Batch  81 / 488  Training Loss  0.0005759339546784759\n",
            "Epoch  18 Batch  82 / 488  Training Loss  0.0008320583146996796\n",
            "Epoch  18 Batch  83 / 488  Training Loss  0.0010457527823746204\n",
            "Epoch  18 Batch  84 / 488  Training Loss  0.001734224846586585\n",
            "Epoch  18 Batch  85 / 488  Training Loss  0.004697497468441725\n",
            "Epoch  18 Batch  86 / 488  Training Loss  0.0011453721672296524\n",
            "Epoch  18 Batch  87 / 488  Training Loss  0.0006117265438660979\n",
            "Epoch  18 Batch  88 / 488  Training Loss  0.0018534604460000992\n",
            "Epoch  18 Batch  89 / 488  Training Loss  0.0014259342569857836\n",
            "Epoch  18 Batch  90 / 488  Training Loss  0.0009565297514200211\n",
            "Epoch  18 Batch  91 / 488  Training Loss  0.0006237831548787653\n",
            "Epoch  18 Batch  92 / 488  Training Loss  0.0013304518070071936\n",
            "Epoch  18 Batch  93 / 488  Training Loss  0.0005857849610038102\n",
            "Epoch  18 Batch  94 / 488  Training Loss  0.0019213309278711677\n",
            "Epoch  18 Batch  95 / 488  Training Loss  0.0015689905267208815\n",
            "Epoch  18 Batch  96 / 488  Training Loss  0.0011242263717576861\n",
            "Epoch  18 Batch  97 / 488  Training Loss  0.0010513814631849527\n",
            "Epoch  18 Batch  98 / 488  Training Loss  0.011963611468672752\n",
            "Epoch  18 Batch  99 / 488  Training Loss  0.0010986655252054334\n",
            "Epoch  18 Batch  100 / 488  Training Loss  0.0008198143914341927\n",
            "Epoch  18 Batch  101 / 488  Training Loss  0.0025698584504425526\n",
            "Epoch  18 Batch  102 / 488  Training Loss  0.001284759258851409\n",
            "Epoch  18 Batch  103 / 488  Training Loss  0.0013970917789265513\n",
            "Epoch  18 Batch  104 / 488  Training Loss  0.0015579060418531299\n",
            "Epoch  18 Batch  105 / 488  Training Loss  0.0018306162673979998\n",
            "Epoch  18 Batch  106 / 488  Training Loss  0.0010432759299874306\n",
            "Epoch  18 Batch  107 / 488  Training Loss  0.001160493353381753\n",
            "Epoch  18 Batch  108 / 488  Training Loss  0.001514732139185071\n",
            "Epoch  18 Batch  109 / 488  Training Loss  0.0016474192962050438\n",
            "Epoch  18 Batch  110 / 488  Training Loss  0.0015358581440523267\n",
            "Epoch  18 Batch  111 / 488  Training Loss  0.0011157809058204293\n",
            "Epoch  18 Batch  112 / 488  Training Loss  0.0017399148782715201\n",
            "Epoch  18 Batch  113 / 488  Training Loss  0.0009400994749739766\n",
            "Epoch  18 Batch  114 / 488  Training Loss  0.0010624811984598637\n",
            "Epoch  18 Batch  115 / 488  Training Loss  0.0008920058608055115\n",
            "Epoch  18 Batch  116 / 488  Training Loss  0.0009864524472504854\n",
            "Epoch  18 Batch  117 / 488  Training Loss  0.0011963050346821547\n",
            "Epoch  18 Batch  118 / 488  Training Loss  0.0016133685130625963\n",
            "Epoch  18 Batch  119 / 488  Training Loss  0.0015674801543354988\n",
            "Epoch  18 Batch  120 / 488  Training Loss  0.0009806512389332056\n",
            "Epoch  18 Batch  121 / 488  Training Loss  0.0020076094660907984\n",
            "Epoch  18 Batch  122 / 488  Training Loss  0.0006662275409325957\n",
            "Epoch  18 Batch  123 / 488  Training Loss  0.0007653856882825494\n",
            "Epoch  18 Batch  124 / 488  Training Loss  0.004422090947628021\n",
            "Epoch  18 Batch  125 / 488  Training Loss  0.001918958267197013\n",
            "Epoch  18 Batch  126 / 488  Training Loss  0.00413078349083662\n",
            "Epoch  18 Batch  127 / 488  Training Loss  0.0009801832493394613\n",
            "Epoch  18 Batch  128 / 488  Training Loss  0.004755817353725433\n",
            "Epoch  18 Batch  129 / 488  Training Loss  0.0021476387046277523\n",
            "Epoch  18 Batch  130 / 488  Training Loss  0.002367099281400442\n",
            "Epoch  18 Batch  131 / 488  Training Loss  0.0017138163093477488\n",
            "Epoch  18 Batch  132 / 488  Training Loss  0.0008753533475100994\n",
            "Epoch  18 Batch  133 / 488  Training Loss  0.0012667961418628693\n",
            "Epoch  18 Batch  134 / 488  Training Loss  0.0008896711515262723\n",
            "Epoch  18 Batch  135 / 488  Training Loss  0.0007111890008673072\n",
            "Epoch  18 Batch  136 / 488  Training Loss  0.0007419848116114736\n",
            "Epoch  18 Batch  137 / 488  Training Loss  0.0017156979301944375\n",
            "Epoch  18 Batch  138 / 488  Training Loss  0.0006268892320804298\n",
            "Epoch  18 Batch  139 / 488  Training Loss  0.002835889346897602\n",
            "Epoch  18 Batch  140 / 488  Training Loss  0.0010659731924533844\n",
            "Epoch  18 Batch  141 / 488  Training Loss  0.0013228619936853647\n",
            "Epoch  18 Batch  142 / 488  Training Loss  0.0006621317006647587\n",
            "Epoch  18 Batch  143 / 488  Training Loss  0.0010986006818711758\n",
            "Epoch  18 Batch  144 / 488  Training Loss  0.001154323574155569\n",
            "Epoch  18 Batch  145 / 488  Training Loss  0.001361536094918847\n",
            "Epoch  18 Batch  146 / 488  Training Loss  0.00194366869982332\n",
            "Epoch  18 Batch  147 / 488  Training Loss  0.003849693341180682\n",
            "Epoch  18 Batch  148 / 488  Training Loss  0.0021209598053246737\n",
            "Epoch  18 Batch  149 / 488  Training Loss  0.0010558966314420104\n",
            "Epoch  18 Batch  150 / 488  Training Loss  0.0010015994776040316\n",
            "Epoch  18 Batch  151 / 488  Training Loss  0.0008720337646082044\n",
            "Epoch  18 Batch  152 / 488  Training Loss  0.0007039442425593734\n",
            "Epoch  18 Batch  153 / 488  Training Loss  0.0023086974397301674\n",
            "Epoch  18 Batch  154 / 488  Training Loss  0.003913273103535175\n",
            "Epoch  18 Batch  155 / 488  Training Loss  0.000839531421661377\n",
            "Epoch  18 Batch  156 / 488  Training Loss  0.0010512399021536112\n",
            "Epoch  18 Batch  157 / 488  Training Loss  0.0007474494050256908\n",
            "Epoch  18 Batch  158 / 488  Training Loss  0.0009301191312260926\n",
            "Epoch  18 Batch  159 / 488  Training Loss  0.0009396717068739235\n",
            "Epoch  18 Batch  160 / 488  Training Loss  0.0009612356079742312\n",
            "Epoch  18 Batch  161 / 488  Training Loss  0.0024989615194499493\n",
            "Epoch  18 Batch  162 / 488  Training Loss  0.002762423362582922\n",
            "Epoch  18 Batch  163 / 488  Training Loss  0.0015739025548100471\n",
            "Epoch  18 Batch  164 / 488  Training Loss  0.0010366637725383043\n",
            "Epoch  18 Batch  165 / 488  Training Loss  0.0018749444279819727\n",
            "Epoch  18 Batch  166 / 488  Training Loss  0.0006195084424689412\n",
            "Epoch  18 Batch  167 / 488  Training Loss  0.0037816506810486317\n",
            "Epoch  18 Batch  168 / 488  Training Loss  0.0032307044602930546\n",
            "Epoch  18 Batch  169 / 488  Training Loss  0.0014699500752612948\n",
            "Epoch  18 Batch  170 / 488  Training Loss  0.0011147480690851808\n",
            "Epoch  18 Batch  171 / 488  Training Loss  0.0009317826479673386\n",
            "Epoch  18 Batch  172 / 488  Training Loss  0.004431219771504402\n",
            "Epoch  18 Batch  173 / 488  Training Loss  0.0008804415119811893\n",
            "Epoch  18 Batch  174 / 488  Training Loss  0.0007536042830906808\n",
            "Epoch  18 Batch  175 / 488  Training Loss  0.001167903421446681\n",
            "Epoch  18 Batch  176 / 488  Training Loss  0.0013503958471119404\n",
            "Epoch  18 Batch  177 / 488  Training Loss  0.0012983933556824923\n",
            "Epoch  18 Batch  178 / 488  Training Loss  0.001278796000406146\n",
            "Epoch  18 Batch  179 / 488  Training Loss  0.0006795112858526409\n",
            "Epoch  18 Batch  180 / 488  Training Loss  0.0009691432351246476\n",
            "Epoch  18 Batch  181 / 488  Training Loss  0.004785559140145779\n",
            "Epoch  18 Batch  182 / 488  Training Loss  0.0003548258391674608\n",
            "Epoch  18 Batch  183 / 488  Training Loss  0.002624497516080737\n",
            "Epoch  18 Batch  184 / 488  Training Loss  0.005230031441897154\n",
            "Epoch  18 Batch  185 / 488  Training Loss  0.002933592302724719\n",
            "Epoch  18 Batch  186 / 488  Training Loss  0.0010769970249384642\n",
            "Epoch  18 Batch  187 / 488  Training Loss  0.0016535052563995123\n",
            "Epoch  18 Batch  188 / 488  Training Loss  0.001126838382333517\n",
            "Epoch  18 Batch  189 / 488  Training Loss  0.0009580762125551701\n",
            "Epoch  18 Batch  190 / 488  Training Loss  0.0014902239199727774\n",
            "Epoch  18 Batch  191 / 488  Training Loss  0.0021834473591297865\n",
            "Epoch  18 Batch  192 / 488  Training Loss  0.0007366474601440132\n",
            "Epoch  18 Batch  193 / 488  Training Loss  0.0014532313216477633\n",
            "Epoch  18 Batch  194 / 488  Training Loss  0.0015253175515681505\n",
            "Epoch  18 Batch  195 / 488  Training Loss  0.0014813545858487487\n",
            "Epoch  18 Batch  196 / 488  Training Loss  0.001028232160024345\n",
            "Epoch  18 Batch  197 / 488  Training Loss  0.0013381303288042545\n",
            "Epoch  18 Batch  198 / 488  Training Loss  0.0007733612437732518\n",
            "Epoch  18 Batch  199 / 488  Training Loss  0.001587137347087264\n",
            "Epoch  18 Batch  200 / 488  Training Loss  0.002223877003416419\n",
            "Epoch  18 Batch  201 / 488  Training Loss  0.002440548501908779\n",
            "Epoch  18 Batch  202 / 488  Training Loss  0.001229258836247027\n",
            "Epoch  18 Batch  203 / 488  Training Loss  0.0023314938880503178\n",
            "Epoch  18 Batch  204 / 488  Training Loss  0.0018973269034177065\n",
            "Epoch  18 Batch  205 / 488  Training Loss  0.000816169660538435\n",
            "Epoch  18 Batch  206 / 488  Training Loss  0.0006104533094912767\n",
            "Epoch  18 Batch  207 / 488  Training Loss  0.0019343930762261152\n",
            "Epoch  18 Batch  208 / 488  Training Loss  0.003676579799503088\n",
            "Epoch  18 Batch  209 / 488  Training Loss  0.0008683904889039695\n",
            "Epoch  18 Batch  210 / 488  Training Loss  0.0011996610555797815\n",
            "Epoch  18 Batch  211 / 488  Training Loss  0.0013136009220033884\n",
            "Epoch  18 Batch  212 / 488  Training Loss  0.001783503801561892\n",
            "Epoch  18 Batch  213 / 488  Training Loss  0.0014416839694604278\n",
            "Epoch  18 Batch  214 / 488  Training Loss  0.001227030297741294\n",
            "Epoch  18 Batch  215 / 488  Training Loss  0.0016067357501015067\n",
            "Epoch  18 Batch  216 / 488  Training Loss  0.0022148608695715666\n",
            "Epoch  18 Batch  217 / 488  Training Loss  0.0016771688824519515\n",
            "Epoch  18 Batch  218 / 488  Training Loss  0.004810130223631859\n",
            "Epoch  18 Batch  219 / 488  Training Loss  0.00095722870901227\n",
            "Epoch  18 Batch  220 / 488  Training Loss  0.0007932656444609165\n",
            "Epoch  18 Batch  221 / 488  Training Loss  0.0005798229831270874\n",
            "Epoch  18 Batch  222 / 488  Training Loss  0.002224249066784978\n",
            "Epoch  18 Batch  223 / 488  Training Loss  0.0011914551723748446\n",
            "Epoch  18 Batch  224 / 488  Training Loss  0.001797548495233059\n",
            "Epoch  18 Batch  225 / 488  Training Loss  0.001183174317702651\n",
            "Epoch  18 Batch  226 / 488  Training Loss  0.0017924833809956908\n",
            "Epoch  18 Batch  227 / 488  Training Loss  0.00099369534291327\n",
            "Epoch  18 Batch  228 / 488  Training Loss  0.005959223955869675\n",
            "Epoch  18 Batch  229 / 488  Training Loss  0.0013441239716485143\n",
            "Epoch  18 Batch  230 / 488  Training Loss  0.002802494913339615\n",
            "Epoch  18 Batch  231 / 488  Training Loss  0.0007773136021569371\n",
            "Epoch  18 Batch  232 / 488  Training Loss  0.0013391476823017001\n",
            "Epoch  18 Batch  233 / 488  Training Loss  0.0009101228788495064\n",
            "Epoch  18 Batch  234 / 488  Training Loss  0.001271426910534501\n",
            "Epoch  18 Batch  235 / 488  Training Loss  0.001645191223360598\n",
            "Epoch  18 Batch  236 / 488  Training Loss  0.0011657222639769316\n",
            "Epoch  18 Batch  237 / 488  Training Loss  0.0008349480340257287\n",
            "Epoch  18 Batch  238 / 488  Training Loss  0.0014276442816480994\n",
            "Epoch  18 Batch  239 / 488  Training Loss  0.0006174358422867954\n",
            "Epoch  18 Batch  240 / 488  Training Loss  0.0012007071636617184\n",
            "Epoch  18 Batch  241 / 488  Training Loss  0.0005949645419605076\n",
            "Epoch  18 Batch  242 / 488  Training Loss  0.0010518139461055398\n",
            "Epoch  18 Batch  243 / 488  Training Loss  0.0010085218818858266\n",
            "Epoch  18 Batch  244 / 488  Training Loss  0.0005438344669528306\n",
            "Epoch  18 Batch  245 / 488  Training Loss  0.0009110138635151088\n",
            "Epoch  18 Batch  246 / 488  Training Loss  0.0031926073133945465\n",
            "Epoch  18 Batch  247 / 488  Training Loss  0.0007554954499937594\n",
            "Epoch  18 Batch  248 / 488  Training Loss  0.005646281875669956\n",
            "Epoch  18 Batch  249 / 488  Training Loss  0.0033808399457484484\n",
            "Epoch  18 Batch  250 / 488  Training Loss  0.001078569097444415\n",
            "Epoch  18 Batch  251 / 488  Training Loss  0.0012354820501059294\n",
            "Epoch  18 Batch  252 / 488  Training Loss  0.0017932566115632653\n",
            "Epoch  18 Batch  253 / 488  Training Loss  0.0017854131292551756\n",
            "Epoch  18 Batch  254 / 488  Training Loss  0.0013183949049562216\n",
            "Epoch  18 Batch  255 / 488  Training Loss  0.0015526434872299433\n",
            "Epoch  18 Batch  256 / 488  Training Loss  0.0010281514842063189\n",
            "Epoch  18 Batch  257 / 488  Training Loss  0.0005042469128966331\n",
            "Epoch  18 Batch  258 / 488  Training Loss  0.005678199697285891\n",
            "Epoch  18 Batch  259 / 488  Training Loss  0.002379381563514471\n",
            "Epoch  18 Batch  260 / 488  Training Loss  0.0012073059333488345\n",
            "Epoch  18 Batch  261 / 488  Training Loss  0.0028677864465862513\n",
            "Epoch  18 Batch  262 / 488  Training Loss  0.01724112033843994\n",
            "Epoch  18 Batch  263 / 488  Training Loss  0.006230086553841829\n",
            "Epoch  18 Batch  264 / 488  Training Loss  0.002129125874489546\n",
            "Epoch  18 Batch  265 / 488  Training Loss  0.0018738983199000359\n",
            "Epoch  18 Batch  266 / 488  Training Loss  0.0032847956754267216\n",
            "Epoch  18 Batch  267 / 488  Training Loss  0.004975104238837957\n",
            "Epoch  18 Batch  268 / 488  Training Loss  0.000917519791983068\n",
            "Epoch  18 Batch  269 / 488  Training Loss  0.001786645851098001\n",
            "Epoch  18 Batch  270 / 488  Training Loss  0.002138273324817419\n",
            "Epoch  18 Batch  271 / 488  Training Loss  0.0025553544983267784\n",
            "Epoch  18 Batch  272 / 488  Training Loss  0.0030652147252112627\n",
            "Epoch  18 Batch  273 / 488  Training Loss  0.001041439245454967\n",
            "Epoch  18 Batch  274 / 488  Training Loss  0.002324913628399372\n",
            "Epoch  18 Batch  275 / 488  Training Loss  0.0018051875522360206\n",
            "Epoch  18 Batch  276 / 488  Training Loss  0.0009029271895997226\n",
            "Epoch  18 Batch  277 / 488  Training Loss  0.0016051849815994501\n",
            "Epoch  18 Batch  278 / 488  Training Loss  0.0018746696878224611\n",
            "Epoch  18 Batch  279 / 488  Training Loss  0.0014049164019525051\n",
            "Epoch  18 Batch  280 / 488  Training Loss  0.001247213571332395\n",
            "Epoch  18 Batch  281 / 488  Training Loss  0.0004627176676876843\n",
            "Epoch  18 Batch  282 / 488  Training Loss  0.0030204281210899353\n",
            "Epoch  18 Batch  283 / 488  Training Loss  0.0035915845073759556\n",
            "Epoch  18 Batch  284 / 488  Training Loss  0.005482705309987068\n",
            "Epoch  18 Batch  285 / 488  Training Loss  0.002189677208662033\n",
            "Epoch  18 Batch  286 / 488  Training Loss  0.0015072786482051015\n",
            "Epoch  18 Batch  287 / 488  Training Loss  0.0033235095907002687\n",
            "Epoch  18 Batch  288 / 488  Training Loss  0.001499992678873241\n",
            "Epoch  18 Batch  289 / 488  Training Loss  0.0027807834558188915\n",
            "Epoch  18 Batch  290 / 488  Training Loss  0.0010620026150718331\n",
            "Epoch  18 Batch  291 / 488  Training Loss  0.0008118621190078557\n",
            "Epoch  18 Batch  292 / 488  Training Loss  0.003084925701841712\n",
            "Epoch  18 Batch  293 / 488  Training Loss  0.0020682583563029766\n",
            "Epoch  18 Batch  294 / 488  Training Loss  0.002455148147419095\n",
            "Epoch  18 Batch  295 / 488  Training Loss  0.002493710722774267\n",
            "Epoch  18 Batch  296 / 488  Training Loss  0.004009002819657326\n",
            "Epoch  18 Batch  297 / 488  Training Loss  0.0020953475032001734\n",
            "Epoch  18 Batch  298 / 488  Training Loss  0.002609089482575655\n",
            "Epoch  18 Batch  299 / 488  Training Loss  0.0013436565641313791\n",
            "Epoch  18 Batch  300 / 488  Training Loss  0.006687249056994915\n",
            "Epoch  18 Batch  301 / 488  Training Loss  0.0017210955265909433\n",
            "Epoch  18 Batch  302 / 488  Training Loss  0.003537772921845317\n",
            "Epoch  18 Batch  303 / 488  Training Loss  0.003033468034118414\n",
            "Epoch  18 Batch  304 / 488  Training Loss  0.00146362220402807\n",
            "Epoch  18 Batch  305 / 488  Training Loss  0.0021196415182203054\n",
            "Epoch  18 Batch  306 / 488  Training Loss  0.0012391293421387672\n",
            "Epoch  18 Batch  307 / 488  Training Loss  0.005126738455146551\n",
            "Epoch  18 Batch  308 / 488  Training Loss  0.0010236755479127169\n",
            "Epoch  18 Batch  309 / 488  Training Loss  0.0019515708554536104\n",
            "Epoch  18 Batch  310 / 488  Training Loss  0.0037957809399813414\n",
            "Epoch  18 Batch  311 / 488  Training Loss  0.0012773062335327268\n",
            "Epoch  18 Batch  312 / 488  Training Loss  0.003808563109487295\n",
            "Epoch  18 Batch  313 / 488  Training Loss  0.0025543561205267906\n",
            "Epoch  18 Batch  314 / 488  Training Loss  0.001285668695345521\n",
            "Epoch  18 Batch  315 / 488  Training Loss  0.0011557808611541986\n",
            "Epoch  18 Batch  316 / 488  Training Loss  0.00141282775439322\n",
            "Epoch  18 Batch  317 / 488  Training Loss  0.002000383334234357\n",
            "Epoch  18 Batch  318 / 488  Training Loss  0.004521348979324102\n",
            "Epoch  18 Batch  319 / 488  Training Loss  0.004824768751859665\n",
            "Epoch  18 Batch  320 / 488  Training Loss  0.0019675525836646557\n",
            "Epoch  18 Batch  321 / 488  Training Loss  0.002095082774758339\n",
            "Epoch  18 Batch  322 / 488  Training Loss  0.004404404200613499\n",
            "Epoch  18 Batch  323 / 488  Training Loss  0.0036134403198957443\n",
            "Epoch  18 Batch  324 / 488  Training Loss  0.0024100576993077993\n",
            "Epoch  18 Batch  325 / 488  Training Loss  0.0010295299580320716\n",
            "Epoch  18 Batch  326 / 488  Training Loss  0.0034344117157161236\n",
            "Epoch  18 Batch  327 / 488  Training Loss  0.0011133388616144657\n",
            "Epoch  18 Batch  328 / 488  Training Loss  0.0013558437349274755\n",
            "Epoch  18 Batch  329 / 488  Training Loss  0.0014560709241777658\n",
            "Epoch  18 Batch  330 / 488  Training Loss  0.0013198945671319962\n",
            "Epoch  18 Batch  331 / 488  Training Loss  0.0012054655235260725\n",
            "Epoch  18 Batch  332 / 488  Training Loss  0.0013660816475749016\n",
            "Epoch  18 Batch  333 / 488  Training Loss  0.001061769900843501\n",
            "Epoch  18 Batch  334 / 488  Training Loss  0.014671149663627148\n",
            "Epoch  18 Batch  335 / 488  Training Loss  0.004856869578361511\n",
            "Epoch  18 Batch  336 / 488  Training Loss  0.0031735911034047604\n",
            "Epoch  18 Batch  337 / 488  Training Loss  0.006074636243283749\n",
            "Epoch  18 Batch  338 / 488  Training Loss  0.0016076064202934504\n",
            "Epoch  18 Batch  339 / 488  Training Loss  0.0011993709485977888\n",
            "Epoch  18 Batch  340 / 488  Training Loss  0.0014225754421204329\n",
            "Epoch  18 Batch  341 / 488  Training Loss  0.004639506805688143\n",
            "Epoch  18 Batch  342 / 488  Training Loss  0.0019983930978924036\n",
            "Epoch  18 Batch  343 / 488  Training Loss  0.002130580134689808\n",
            "Epoch  18 Batch  344 / 488  Training Loss  0.001648774603381753\n",
            "Epoch  18 Batch  345 / 488  Training Loss  0.0009155388106592\n",
            "Epoch  18 Batch  346 / 488  Training Loss  0.0011754819424822927\n",
            "Epoch  18 Batch  347 / 488  Training Loss  0.0013392657274380326\n",
            "Epoch  18 Batch  348 / 488  Training Loss  0.0010423196945339441\n",
            "Epoch  18 Batch  349 / 488  Training Loss  0.0020765417721122503\n",
            "Epoch  18 Batch  350 / 488  Training Loss  0.0017344948137179017\n",
            "Epoch  18 Batch  351 / 488  Training Loss  0.0010523160453885794\n",
            "Epoch  18 Batch  352 / 488  Training Loss  0.0011101249838247895\n",
            "Epoch  18 Batch  353 / 488  Training Loss  0.0021693729795515537\n",
            "Epoch  18 Batch  354 / 488  Training Loss  0.0011145492317155004\n",
            "Epoch  18 Batch  355 / 488  Training Loss  0.002670960035175085\n",
            "Epoch  18 Batch  356 / 488  Training Loss  0.004049554001539946\n",
            "Epoch  18 Batch  357 / 488  Training Loss  0.0012708143331110477\n",
            "Epoch  18 Batch  358 / 488  Training Loss  0.0036930684000253677\n",
            "Epoch  18 Batch  359 / 488  Training Loss  0.0015724331606179476\n",
            "Epoch  18 Batch  360 / 488  Training Loss  0.003413387108594179\n",
            "Epoch  18 Batch  361 / 488  Training Loss  0.01594536192715168\n",
            "Epoch  18 Batch  362 / 488  Training Loss  0.0034477910958230495\n",
            "Epoch  18 Batch  363 / 488  Training Loss  0.0009416084503754973\n",
            "Epoch  18 Batch  364 / 488  Training Loss  0.0031821217853575945\n",
            "Epoch  18 Batch  365 / 488  Training Loss  0.008983084931969643\n",
            "Epoch  18 Batch  366 / 488  Training Loss  0.002046742243692279\n",
            "Epoch  18 Batch  367 / 488  Training Loss  0.008543233387172222\n",
            "Epoch  18 Batch  368 / 488  Training Loss  0.001034951419569552\n",
            "Epoch  18 Batch  369 / 488  Training Loss  0.0034914680290967226\n",
            "Epoch  18 Batch  370 / 488  Training Loss  0.0016849447274580598\n",
            "Epoch  18 Batch  371 / 488  Training Loss  0.0016297396505251527\n",
            "Epoch  18 Batch  372 / 488  Training Loss  0.002651654416695237\n",
            "Epoch  18 Batch  373 / 488  Training Loss  0.0009394142543897033\n",
            "Epoch  18 Batch  374 / 488  Training Loss  0.002137120347470045\n",
            "Epoch  18 Batch  375 / 488  Training Loss  0.003835352137684822\n",
            "Epoch  18 Batch  376 / 488  Training Loss  0.001129179378040135\n",
            "Epoch  18 Batch  377 / 488  Training Loss  0.0025200473610311747\n",
            "Epoch  18 Batch  378 / 488  Training Loss  0.0014765060041099787\n",
            "Epoch  18 Batch  379 / 488  Training Loss  0.002500910311937332\n",
            "Epoch  18 Batch  380 / 488  Training Loss  0.0015277017373591661\n",
            "Epoch  18 Batch  381 / 488  Training Loss  0.0020795422606170177\n",
            "Epoch  18 Batch  382 / 488  Training Loss  0.0025712635833770037\n",
            "Epoch  18 Batch  383 / 488  Training Loss  0.0026128976605832577\n",
            "Epoch  18 Batch  384 / 488  Training Loss  0.0024350120220333338\n",
            "Epoch  18 Batch  385 / 488  Training Loss  0.0013688477920368314\n",
            "Epoch  18 Batch  386 / 488  Training Loss  0.001563869183883071\n",
            "Epoch  18 Batch  387 / 488  Training Loss  0.0032210983335971832\n",
            "Epoch  18 Batch  388 / 488  Training Loss  0.0019036342855542898\n",
            "Epoch  18 Batch  389 / 488  Training Loss  0.0008388882270082831\n",
            "Epoch  18 Batch  390 / 488  Training Loss  0.0007948122802190483\n",
            "Epoch  18 Batch  391 / 488  Training Loss  0.0012808912433683872\n",
            "Epoch  18 Batch  392 / 488  Training Loss  0.0012810648186132312\n",
            "Epoch  18 Batch  393 / 488  Training Loss  0.0020549511536955833\n",
            "Epoch  18 Batch  394 / 488  Training Loss  0.0009633271838538349\n",
            "Epoch  18 Batch  395 / 488  Training Loss  0.0016388647491112351\n",
            "Epoch  18 Batch  396 / 488  Training Loss  0.0012241846416145563\n",
            "Epoch  18 Batch  397 / 488  Training Loss  0.002389755565673113\n",
            "Epoch  18 Batch  398 / 488  Training Loss  0.0021216138266026974\n",
            "Epoch  18 Batch  399 / 488  Training Loss  0.003281719982624054\n",
            "Epoch  18 Batch  400 / 488  Training Loss  0.0021333456970751286\n",
            "Epoch  18 Batch  401 / 488  Training Loss  0.0016054384177550673\n",
            "Epoch  18 Batch  402 / 488  Training Loss  0.001796112977899611\n",
            "Epoch  18 Batch  403 / 488  Training Loss  0.0012362144188955426\n",
            "Epoch  18 Batch  404 / 488  Training Loss  0.0013310067588463426\n",
            "Epoch  18 Batch  405 / 488  Training Loss  0.002732574474066496\n",
            "Epoch  18 Batch  406 / 488  Training Loss  0.002075477968901396\n",
            "Epoch  18 Batch  407 / 488  Training Loss  0.003138795029371977\n",
            "Epoch  18 Batch  408 / 488  Training Loss  0.0013841742184013128\n",
            "Epoch  18 Batch  409 / 488  Training Loss  0.0012191750574856997\n",
            "Epoch  18 Batch  410 / 488  Training Loss  0.002087484346702695\n",
            "Epoch  18 Batch  411 / 488  Training Loss  0.0009849716443568468\n",
            "Epoch  18 Batch  412 / 488  Training Loss  0.004158397670835257\n",
            "Epoch  18 Batch  413 / 488  Training Loss  0.0008880198001861572\n",
            "Epoch  18 Batch  414 / 488  Training Loss  0.005340932868421078\n",
            "Epoch  18 Batch  415 / 488  Training Loss  0.001961001195013523\n",
            "Epoch  18 Batch  416 / 488  Training Loss  0.0009746577707119286\n",
            "Epoch  18 Batch  417 / 488  Training Loss  0.001218541874550283\n",
            "Epoch  18 Batch  418 / 488  Training Loss  0.0016950828721746802\n",
            "Epoch  18 Batch  419 / 488  Training Loss  0.0023615523241460323\n",
            "Epoch  18 Batch  420 / 488  Training Loss  0.0010047886753454804\n",
            "Epoch  18 Batch  421 / 488  Training Loss  0.0018443036824464798\n",
            "Epoch  18 Batch  422 / 488  Training Loss  0.00171917793340981\n",
            "Epoch  18 Batch  423 / 488  Training Loss  0.0017315264558419585\n",
            "Epoch  18 Batch  424 / 488  Training Loss  0.001202726736664772\n",
            "Epoch  18 Batch  425 / 488  Training Loss  0.0010382896289229393\n",
            "Epoch  18 Batch  426 / 488  Training Loss  0.0008306474192067981\n",
            "Epoch  18 Batch  427 / 488  Training Loss  0.0015266190748661757\n",
            "Epoch  18 Batch  428 / 488  Training Loss  0.0021626546513289213\n",
            "Epoch  18 Batch  429 / 488  Training Loss  0.005043626297265291\n",
            "Epoch  18 Batch  430 / 488  Training Loss  0.0013301221188157797\n",
            "Epoch  18 Batch  431 / 488  Training Loss  0.006032400764524937\n",
            "Epoch  18 Batch  432 / 488  Training Loss  0.0025901522021740675\n",
            "Epoch  18 Batch  433 / 488  Training Loss  0.0022339201532304287\n",
            "Epoch  18 Batch  434 / 488  Training Loss  0.0014182047452777624\n",
            "Epoch  18 Batch  435 / 488  Training Loss  0.002339939121156931\n",
            "Epoch  18 Batch  436 / 488  Training Loss  0.0019803866744041443\n",
            "Epoch  18 Batch  437 / 488  Training Loss  0.0036811467725783587\n",
            "Epoch  18 Batch  438 / 488  Training Loss  0.000815176113974303\n",
            "Epoch  18 Batch  439 / 488  Training Loss  0.000525984272826463\n",
            "Epoch  18 Batch  440 / 488  Training Loss  0.0018940223380923271\n",
            "Epoch  18 Batch  441 / 488  Training Loss  0.0032956325449049473\n",
            "Epoch  18 Batch  442 / 488  Training Loss  0.0011460918467491865\n",
            "Epoch  18 Batch  443 / 488  Training Loss  0.0028546368703246117\n",
            "Epoch  18 Batch  444 / 488  Training Loss  0.001364853815175593\n",
            "Epoch  18 Batch  445 / 488  Training Loss  0.0038816421292722225\n",
            "Epoch  18 Batch  446 / 488  Training Loss  0.0031273316126316786\n",
            "Epoch  18 Batch  447 / 488  Training Loss  0.001955983228981495\n",
            "Epoch  18 Batch  448 / 488  Training Loss  0.0011670079547911882\n",
            "Epoch  18 Batch  449 / 488  Training Loss  0.004711124114692211\n",
            "Epoch  18 Batch  450 / 488  Training Loss  0.0007211398333311081\n",
            "Epoch  18 Batch  451 / 488  Training Loss  0.0019278194522485137\n",
            "Epoch  18 Batch  452 / 488  Training Loss  0.0014991287607699633\n",
            "Epoch  18 Batch  453 / 488  Training Loss  0.0016269577899947762\n",
            "Epoch  18 Batch  454 / 488  Training Loss  0.0013212452176958323\n",
            "Epoch  18 Batch  455 / 488  Training Loss  0.0012402164284139872\n",
            "Epoch  18 Batch  456 / 488  Training Loss  0.0008593074744567275\n",
            "Epoch  18 Batch  457 / 488  Training Loss  0.0010478899348527193\n",
            "Epoch  18 Batch  458 / 488  Training Loss  0.0023977961391210556\n",
            "Epoch  18 Batch  459 / 488  Training Loss  0.0018503625178709626\n",
            "Epoch  18 Batch  460 / 488  Training Loss  0.0016721856081858277\n",
            "Epoch  18 Batch  461 / 488  Training Loss  0.008132023736834526\n",
            "Epoch  18 Batch  462 / 488  Training Loss  0.002289815340191126\n",
            "Epoch  18 Batch  463 / 488  Training Loss  0.006002488546073437\n",
            "Epoch  18 Batch  464 / 488  Training Loss  0.0018967915093526244\n",
            "Epoch  18 Batch  465 / 488  Training Loss  0.0015311130555346608\n",
            "Epoch  18 Batch  466 / 488  Training Loss  0.00253913807682693\n",
            "Epoch  18 Batch  467 / 488  Training Loss  0.0026593711227178574\n",
            "Epoch  18 Batch  468 / 488  Training Loss  0.0011117488611489534\n",
            "Epoch  18 Batch  469 / 488  Training Loss  0.0024579623714089394\n",
            "Epoch  18 Batch  470 / 488  Training Loss  0.001352934748865664\n",
            "Epoch  18 Batch  471 / 488  Training Loss  0.0017492485931143165\n",
            "Epoch  18 Batch  472 / 488  Training Loss  0.0018616912420839071\n",
            "Epoch  18 Batch  473 / 488  Training Loss  0.0021752058528363705\n",
            "Epoch  18 Batch  474 / 488  Training Loss  0.0024159280583262444\n",
            "Epoch  18 Batch  475 / 488  Training Loss  0.0024909761268645525\n",
            "Epoch  18 Batch  476 / 488  Training Loss  0.0015677826013416052\n",
            "Epoch  18 Batch  477 / 488  Training Loss  0.0014841181691735983\n",
            "Epoch  18 Batch  478 / 488  Training Loss  0.0011188557837158442\n",
            "Epoch  18 Batch  479 / 488  Training Loss  0.001143180881626904\n",
            "Epoch  18 Batch  480 / 488  Training Loss  0.0019324555760249496\n",
            "Epoch  18 Batch  481 / 488  Training Loss  0.0016005784273147583\n",
            "Epoch  18 Batch  482 / 488  Training Loss  0.0011816088808700442\n",
            "Epoch  18 Batch  483 / 488  Training Loss  0.0014962614513933659\n",
            "Epoch  18 Batch  484 / 488  Training Loss  0.00639957282692194\n",
            "Epoch  18 Batch  485 / 488  Training Loss  0.000698551710229367\n",
            "Epoch  18 Batch  486 / 488  Training Loss  0.005682141054421663\n",
            "Epoch  18 Batch  487 / 488  Training Loss  0.0012116297148168087\n",
            "  19    |    -    |   0.002057   | 44.503546\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 19\n",
            "Epoch  19 Batch  0 / 488  Training Loss  0.0009561969200149179\n",
            "Epoch  19 Batch  1 / 488  Training Loss  0.008520415052771568\n",
            "Epoch  19 Batch  2 / 488  Training Loss  0.001149636460468173\n",
            "Epoch  19 Batch  3 / 488  Training Loss  0.0017408548155799508\n",
            "Epoch  19 Batch  4 / 488  Training Loss  0.0012993687996640801\n",
            "Epoch  19 Batch  5 / 488  Training Loss  0.0006246581906452775\n",
            "Epoch  19 Batch  6 / 488  Training Loss  0.0007979386718943715\n",
            "Epoch  19 Batch  7 / 488  Training Loss  0.004676683805882931\n",
            "Epoch  19 Batch  8 / 488  Training Loss  0.0006723324186168611\n",
            "Epoch  19 Batch  9 / 488  Training Loss  0.0022709290497004986\n",
            "Epoch  19 Batch  10 / 488  Training Loss  0.0007035065209493041\n",
            "Epoch  19 Batch  11 / 488  Training Loss  0.0006104611093178391\n",
            "Epoch  19 Batch  12 / 488  Training Loss  0.0017290260875597596\n",
            "Epoch  19 Batch  13 / 488  Training Loss  0.0005623043398372829\n",
            "Epoch  19 Batch  14 / 488  Training Loss  0.0006104890489950776\n",
            "Epoch  19 Batch  15 / 488  Training Loss  0.0030021057464182377\n",
            "Epoch  19 Batch  16 / 488  Training Loss  0.0008887512376531959\n",
            "Epoch  19 Batch  17 / 488  Training Loss  0.0005761822685599327\n",
            "Epoch  19 Batch  18 / 488  Training Loss  0.0004752335080411285\n",
            "Epoch  19 Batch  19 / 488  Training Loss  0.0010354620171710849\n",
            "Epoch  19 Batch  20 / 488  Training Loss  0.0019315818790346384\n",
            "Epoch  19 Batch  21 / 488  Training Loss  0.0004397820739541203\n",
            "Epoch  19 Batch  22 / 488  Training Loss  0.0008912423509173095\n",
            "Epoch  19 Batch  23 / 488  Training Loss  0.0006133577553555369\n",
            "Epoch  19 Batch  24 / 488  Training Loss  0.0005198008730076253\n",
            "Epoch  19 Batch  25 / 488  Training Loss  0.0006460202275775373\n",
            "Epoch  19 Batch  26 / 488  Training Loss  0.0006834937958046794\n",
            "Epoch  19 Batch  27 / 488  Training Loss  0.001645231619477272\n",
            "Epoch  19 Batch  28 / 488  Training Loss  0.0011530698975548148\n",
            "Epoch  19 Batch  29 / 488  Training Loss  0.0007231564377434552\n",
            "Epoch  19 Batch  30 / 488  Training Loss  0.004711268004029989\n",
            "Epoch  19 Batch  31 / 488  Training Loss  0.001956275198608637\n",
            "Epoch  19 Batch  32 / 488  Training Loss  0.00802464596927166\n",
            "Epoch  19 Batch  33 / 488  Training Loss  0.0010138374054804444\n",
            "Epoch  19 Batch  34 / 488  Training Loss  0.0012896578991785645\n",
            "Epoch  19 Batch  35 / 488  Training Loss  0.0007216506637632847\n",
            "Epoch  19 Batch  36 / 488  Training Loss  0.001908538630232215\n",
            "Epoch  19 Batch  37 / 488  Training Loss  0.0008434383198618889\n",
            "Epoch  19 Batch  38 / 488  Training Loss  0.0011566041503101587\n",
            "Epoch  19 Batch  39 / 488  Training Loss  0.01793835312128067\n",
            "Epoch  19 Batch  40 / 488  Training Loss  0.0012660733191296458\n",
            "Epoch  19 Batch  41 / 488  Training Loss  0.0010788006475195289\n",
            "Epoch  19 Batch  42 / 488  Training Loss  0.0005521351122297347\n",
            "Epoch  19 Batch  43 / 488  Training Loss  0.0013139971997588873\n",
            "Epoch  19 Batch  44 / 488  Training Loss  0.0009632715955376625\n",
            "Epoch  19 Batch  45 / 488  Training Loss  0.0020302128978073597\n",
            "Epoch  19 Batch  46 / 488  Training Loss  0.0008503437275066972\n",
            "Epoch  19 Batch  47 / 488  Training Loss  0.0005616085836663842\n",
            "Epoch  19 Batch  48 / 488  Training Loss  0.0004748731735162437\n",
            "Epoch  19 Batch  49 / 488  Training Loss  0.0004207523597870022\n",
            "Epoch  19 Batch  50 / 488  Training Loss  0.000523318478371948\n",
            "Epoch  19 Batch  51 / 488  Training Loss  0.0009019235149025917\n",
            "Epoch  19 Batch  52 / 488  Training Loss  0.0005127392942085862\n",
            "Epoch  19 Batch  53 / 488  Training Loss  0.0005648935330100358\n",
            "Epoch  19 Batch  54 / 488  Training Loss  0.009531890042126179\n",
            "Epoch  19 Batch  55 / 488  Training Loss  0.005629589315503836\n",
            "Epoch  19 Batch  56 / 488  Training Loss  0.009114437736570835\n",
            "Epoch  19 Batch  57 / 488  Training Loss  0.007482926361262798\n",
            "Epoch  19 Batch  58 / 488  Training Loss  0.0028160098008811474\n",
            "Epoch  19 Batch  59 / 488  Training Loss  0.0026054622139781713\n",
            "Epoch  19 Batch  60 / 488  Training Loss  0.00810436811298132\n",
            "Epoch  19 Batch  61 / 488  Training Loss  0.0013004078064113855\n",
            "Epoch  19 Batch  62 / 488  Training Loss  0.002390702487900853\n",
            "Epoch  19 Batch  63 / 488  Training Loss  0.002696698997169733\n",
            "Epoch  19 Batch  64 / 488  Training Loss  0.0022590842563658953\n",
            "Epoch  19 Batch  65 / 488  Training Loss  0.0016558878123760223\n",
            "Epoch  19 Batch  66 / 488  Training Loss  0.010255569592118263\n",
            "Epoch  19 Batch  67 / 488  Training Loss  0.002734722336754203\n",
            "Epoch  19 Batch  68 / 488  Training Loss  0.01179218478500843\n",
            "Epoch  19 Batch  69 / 488  Training Loss  0.0008920915424823761\n",
            "Epoch  19 Batch  70 / 488  Training Loss  0.00159528199583292\n",
            "Epoch  19 Batch  71 / 488  Training Loss  0.0010510820429772139\n",
            "Epoch  19 Batch  72 / 488  Training Loss  0.0011434458428993821\n",
            "Epoch  19 Batch  73 / 488  Training Loss  0.0013790560187771916\n",
            "Epoch  19 Batch  74 / 488  Training Loss  0.0049843210726976395\n",
            "Epoch  19 Batch  75 / 488  Training Loss  0.006017522420734167\n",
            "Epoch  19 Batch  76 / 488  Training Loss  0.001149512710981071\n",
            "Epoch  19 Batch  77 / 488  Training Loss  0.001967158867046237\n",
            "Epoch  19 Batch  78 / 488  Training Loss  0.004459713585674763\n",
            "Epoch  19 Batch  79 / 488  Training Loss  0.003106083022430539\n",
            "Epoch  19 Batch  80 / 488  Training Loss  0.0021125334315001965\n",
            "Epoch  19 Batch  81 / 488  Training Loss  0.0019692687783390284\n",
            "Epoch  19 Batch  82 / 488  Training Loss  0.005511059425771236\n",
            "Epoch  19 Batch  83 / 488  Training Loss  0.0031236824579536915\n",
            "Epoch  19 Batch  84 / 488  Training Loss  0.002304371679201722\n",
            "Epoch  19 Batch  85 / 488  Training Loss  0.008163245394825935\n",
            "Epoch  19 Batch  86 / 488  Training Loss  0.002102765953168273\n",
            "Epoch  19 Batch  87 / 488  Training Loss  0.0011292715789750218\n",
            "Epoch  19 Batch  88 / 488  Training Loss  0.0013851298717781901\n",
            "Epoch  19 Batch  89 / 488  Training Loss  0.002214691834524274\n",
            "Epoch  19 Batch  90 / 488  Training Loss  0.0016273197252303362\n",
            "Epoch  19 Batch  91 / 488  Training Loss  0.0008491678163409233\n",
            "Epoch  19 Batch  92 / 488  Training Loss  0.0013127659913152456\n",
            "Epoch  19 Batch  93 / 488  Training Loss  0.0018928681965917349\n",
            "Epoch  19 Batch  94 / 488  Training Loss  0.0033921196591109037\n",
            "Epoch  19 Batch  95 / 488  Training Loss  0.002354210941120982\n",
            "Epoch  19 Batch  96 / 488  Training Loss  0.0010656127706170082\n",
            "Epoch  19 Batch  97 / 488  Training Loss  0.0018933427054435015\n",
            "Epoch  19 Batch  98 / 488  Training Loss  0.0010110557777807117\n",
            "Epoch  19 Batch  99 / 488  Training Loss  0.0011544808512553573\n",
            "Epoch  19 Batch  100 / 488  Training Loss  0.0010795241687446833\n",
            "Epoch  19 Batch  101 / 488  Training Loss  0.00718761095777154\n",
            "Epoch  19 Batch  102 / 488  Training Loss  0.0043908050283789635\n",
            "Epoch  19 Batch  103 / 488  Training Loss  0.0011552366195246577\n",
            "Epoch  19 Batch  104 / 488  Training Loss  0.0005159990978427231\n",
            "Epoch  19 Batch  105 / 488  Training Loss  0.0010065720416605473\n",
            "Epoch  19 Batch  106 / 488  Training Loss  0.0050385138019919395\n",
            "Epoch  19 Batch  107 / 488  Training Loss  0.0020081819966435432\n",
            "Epoch  19 Batch  108 / 488  Training Loss  0.0014360429486259818\n",
            "Epoch  19 Batch  109 / 488  Training Loss  0.007788342889398336\n",
            "Epoch  19 Batch  110 / 488  Training Loss  0.0007519454811699688\n",
            "Epoch  19 Batch  111 / 488  Training Loss  0.0026494546327739954\n",
            "Epoch  19 Batch  112 / 488  Training Loss  0.003945684991776943\n",
            "Epoch  19 Batch  113 / 488  Training Loss  0.017681721597909927\n",
            "Epoch  19 Batch  114 / 488  Training Loss  0.002444976707920432\n",
            "Epoch  19 Batch  115 / 488  Training Loss  0.0039217788726091385\n",
            "Epoch  19 Batch  116 / 488  Training Loss  0.008059801533818245\n",
            "Epoch  19 Batch  117 / 488  Training Loss  0.001389514422044158\n",
            "Epoch  19 Batch  118 / 488  Training Loss  0.001074084546416998\n",
            "Epoch  19 Batch  119 / 488  Training Loss  0.001480616396293044\n",
            "Epoch  19 Batch  120 / 488  Training Loss  0.0014770292909815907\n",
            "Epoch  19 Batch  121 / 488  Training Loss  0.0015908371424302459\n",
            "Epoch  19 Batch  122 / 488  Training Loss  0.002435260219499469\n",
            "Epoch  19 Batch  123 / 488  Training Loss  0.0024248710833489895\n",
            "Epoch  19 Batch  124 / 488  Training Loss  0.0018903551390394568\n",
            "Epoch  19 Batch  125 / 488  Training Loss  0.0010870612459257245\n",
            "Epoch  19 Batch  126 / 488  Training Loss  0.002921193838119507\n",
            "Epoch  19 Batch  127 / 488  Training Loss  0.0008273838320747018\n",
            "Epoch  19 Batch  128 / 488  Training Loss  0.0017603563610464334\n",
            "Epoch  19 Batch  129 / 488  Training Loss  0.0017107262974604964\n",
            "Epoch  19 Batch  130 / 488  Training Loss  0.0018051539082080126\n",
            "Epoch  19 Batch  131 / 488  Training Loss  0.0009726250427775085\n",
            "Epoch  19 Batch  132 / 488  Training Loss  0.00373400142416358\n",
            "Epoch  19 Batch  133 / 488  Training Loss  0.0017542788991704583\n",
            "Epoch  19 Batch  134 / 488  Training Loss  0.0013603769475594163\n",
            "Epoch  19 Batch  135 / 488  Training Loss  0.0004757778369821608\n",
            "Epoch  19 Batch  136 / 488  Training Loss  0.0009153458522632718\n",
            "Epoch  19 Batch  137 / 488  Training Loss  0.0012120814062654972\n",
            "Epoch  19 Batch  138 / 488  Training Loss  0.0010542621603235602\n",
            "Epoch  19 Batch  139 / 488  Training Loss  0.002079781610518694\n",
            "Epoch  19 Batch  140 / 488  Training Loss  0.0008278944296762347\n",
            "Epoch  19 Batch  141 / 488  Training Loss  0.0008705632062628865\n",
            "Epoch  19 Batch  142 / 488  Training Loss  0.0037369851488620043\n",
            "Epoch  19 Batch  143 / 488  Training Loss  0.0005315302405506372\n",
            "Epoch  19 Batch  144 / 488  Training Loss  0.0016766854096204042\n",
            "Epoch  19 Batch  145 / 488  Training Loss  0.0011001781094819307\n",
            "Epoch  19 Batch  146 / 488  Training Loss  0.0011831574374809861\n",
            "Epoch  19 Batch  147 / 488  Training Loss  0.0011034474009647965\n",
            "Epoch  19 Batch  148 / 488  Training Loss  0.0007391670951619744\n",
            "Epoch  19 Batch  149 / 488  Training Loss  0.0009567428496666253\n",
            "Epoch  19 Batch  150 / 488  Training Loss  0.0017714134883135557\n",
            "Epoch  19 Batch  151 / 488  Training Loss  0.0009343557176180184\n",
            "Epoch  19 Batch  152 / 488  Training Loss  0.000587640271987766\n",
            "Epoch  19 Batch  153 / 488  Training Loss  0.0011419455986469984\n",
            "Epoch  19 Batch  154 / 488  Training Loss  0.0012715717311948538\n",
            "Epoch  19 Batch  155 / 488  Training Loss  0.0007448025280609727\n",
            "Epoch  19 Batch  156 / 488  Training Loss  0.007444897200912237\n",
            "Epoch  19 Batch  157 / 488  Training Loss  0.0009150208788923919\n",
            "Epoch  19 Batch  158 / 488  Training Loss  0.001423554727807641\n",
            "Epoch  19 Batch  159 / 488  Training Loss  0.0056190332397818565\n",
            "Epoch  19 Batch  160 / 488  Training Loss  0.00293238228186965\n",
            "Epoch  19 Batch  161 / 488  Training Loss  0.0010708993067964911\n",
            "Epoch  19 Batch  162 / 488  Training Loss  0.002316610189154744\n",
            "Epoch  19 Batch  163 / 488  Training Loss  0.0013633595081046224\n",
            "Epoch  19 Batch  164 / 488  Training Loss  0.0015575153520330787\n",
            "Epoch  19 Batch  165 / 488  Training Loss  0.0004363376647233963\n",
            "Epoch  19 Batch  166 / 488  Training Loss  0.0010388304945081472\n",
            "Epoch  19 Batch  167 / 488  Training Loss  0.0013048080727458\n",
            "Epoch  19 Batch  168 / 488  Training Loss  0.0018335143104195595\n",
            "Epoch  19 Batch  169 / 488  Training Loss  0.0037135067395865917\n",
            "Epoch  19 Batch  170 / 488  Training Loss  0.002303973538801074\n",
            "Epoch  19 Batch  171 / 488  Training Loss  0.0016679152613505721\n",
            "Epoch  19 Batch  172 / 488  Training Loss  0.0012368602911010385\n",
            "Epoch  19 Batch  173 / 488  Training Loss  0.0007485727546736598\n",
            "Epoch  19 Batch  174 / 488  Training Loss  0.0008055140497162938\n",
            "Epoch  19 Batch  175 / 488  Training Loss  0.0011525274021551013\n",
            "Epoch  19 Batch  176 / 488  Training Loss  0.0008739581098780036\n",
            "Epoch  19 Batch  177 / 488  Training Loss  0.0013240908738225698\n",
            "Epoch  19 Batch  178 / 488  Training Loss  0.001397268264554441\n",
            "Epoch  19 Batch  179 / 488  Training Loss  0.0011425463017076254\n",
            "Epoch  19 Batch  180 / 488  Training Loss  0.0021795514039695263\n",
            "Epoch  19 Batch  181 / 488  Training Loss  0.0011632140958681703\n",
            "Epoch  19 Batch  182 / 488  Training Loss  0.0007912953151389956\n",
            "Epoch  19 Batch  183 / 488  Training Loss  0.001187954330816865\n",
            "Epoch  19 Batch  184 / 488  Training Loss  0.002250006888061762\n",
            "Epoch  19 Batch  185 / 488  Training Loss  0.005872433073818684\n",
            "Epoch  19 Batch  186 / 488  Training Loss  0.0017585937166586518\n",
            "Epoch  19 Batch  187 / 488  Training Loss  0.002841088455170393\n",
            "Epoch  19 Batch  188 / 488  Training Loss  0.0016578964423388243\n",
            "Epoch  19 Batch  189 / 488  Training Loss  0.001653126673772931\n",
            "Epoch  19 Batch  190 / 488  Training Loss  0.002456397982314229\n",
            "Epoch  19 Batch  191 / 488  Training Loss  0.0018394229700788856\n",
            "Epoch  19 Batch  192 / 488  Training Loss  0.001467578113079071\n",
            "Epoch  19 Batch  193 / 488  Training Loss  0.0017865203553810716\n",
            "Epoch  19 Batch  194 / 488  Training Loss  0.001401624409481883\n",
            "Epoch  19 Batch  195 / 488  Training Loss  0.0019114355091005564\n",
            "Epoch  19 Batch  196 / 488  Training Loss  0.002202663803473115\n",
            "Epoch  19 Batch  197 / 488  Training Loss  0.001471681403927505\n",
            "Epoch  19 Batch  198 / 488  Training Loss  0.0008314261212944984\n",
            "Epoch  19 Batch  199 / 488  Training Loss  0.0006528589874505997\n",
            "Epoch  19 Batch  200 / 488  Training Loss  0.001367442891933024\n",
            "Epoch  19 Batch  201 / 488  Training Loss  0.0016247592866420746\n",
            "Epoch  19 Batch  202 / 488  Training Loss  0.0009671882726252079\n",
            "Epoch  19 Batch  203 / 488  Training Loss  0.0011936078080907464\n",
            "Epoch  19 Batch  204 / 488  Training Loss  0.0014695192221552134\n",
            "Epoch  19 Batch  205 / 488  Training Loss  0.0011643243487924337\n",
            "Epoch  19 Batch  206 / 488  Training Loss  0.0013923963997513056\n",
            "Epoch  19 Batch  207 / 488  Training Loss  0.00138655467890203\n",
            "Epoch  19 Batch  208 / 488  Training Loss  0.001297802897170186\n",
            "Epoch  19 Batch  209 / 488  Training Loss  0.0015870558563619852\n",
            "Epoch  19 Batch  210 / 488  Training Loss  0.0006855329265818\n",
            "Epoch  19 Batch  211 / 488  Training Loss  0.0011421561939641833\n",
            "Epoch  19 Batch  212 / 488  Training Loss  0.0009366211597807705\n",
            "Epoch  19 Batch  213 / 488  Training Loss  0.0027146642096340656\n",
            "Epoch  19 Batch  214 / 488  Training Loss  0.001047925790771842\n",
            "Epoch  19 Batch  215 / 488  Training Loss  0.001175013603642583\n",
            "Epoch  19 Batch  216 / 488  Training Loss  0.002242429181933403\n",
            "Epoch  19 Batch  217 / 488  Training Loss  0.0012346006697043777\n",
            "Epoch  19 Batch  218 / 488  Training Loss  0.000947442080359906\n",
            "Epoch  19 Batch  219 / 488  Training Loss  0.000674154085572809\n",
            "Epoch  19 Batch  220 / 488  Training Loss  0.0019802451133728027\n",
            "Epoch  19 Batch  221 / 488  Training Loss  0.0015206631505861878\n",
            "Epoch  19 Batch  222 / 488  Training Loss  0.0009536485886201262\n",
            "Epoch  19 Batch  223 / 488  Training Loss  0.0010529099963605404\n",
            "Epoch  19 Batch  224 / 488  Training Loss  0.0007494523306377232\n",
            "Epoch  19 Batch  225 / 488  Training Loss  0.0011381624499335885\n",
            "Epoch  19 Batch  226 / 488  Training Loss  0.0003818861150648445\n",
            "Epoch  19 Batch  227 / 488  Training Loss  0.0007597085786983371\n",
            "Epoch  19 Batch  228 / 488  Training Loss  0.0006070752860978246\n",
            "Epoch  19 Batch  229 / 488  Training Loss  0.0020159445703029633\n",
            "Epoch  19 Batch  230 / 488  Training Loss  0.0011954514775425196\n",
            "Epoch  19 Batch  231 / 488  Training Loss  0.0006243601674214005\n",
            "Epoch  19 Batch  232 / 488  Training Loss  0.0006004871102049947\n",
            "Epoch  19 Batch  233 / 488  Training Loss  0.00075962720438838\n",
            "Epoch  19 Batch  234 / 488  Training Loss  0.0004554883053060621\n",
            "Epoch  19 Batch  235 / 488  Training Loss  0.0011371125001460314\n",
            "Epoch  19 Batch  236 / 488  Training Loss  0.0009229955030605197\n",
            "Epoch  19 Batch  237 / 488  Training Loss  0.0010910282144322991\n",
            "Epoch  19 Batch  238 / 488  Training Loss  0.0026354435831308365\n",
            "Epoch  19 Batch  239 / 488  Training Loss  0.0008632083190605044\n",
            "Epoch  19 Batch  240 / 488  Training Loss  0.000991825363598764\n",
            "Epoch  19 Batch  241 / 488  Training Loss  0.0005796034820377827\n",
            "Epoch  19 Batch  242 / 488  Training Loss  0.011634214781224728\n",
            "Epoch  19 Batch  243 / 488  Training Loss  0.002261634450405836\n",
            "Epoch  19 Batch  244 / 488  Training Loss  0.007113281637430191\n",
            "Epoch  19 Batch  245 / 488  Training Loss  0.0016925025265663862\n",
            "Epoch  19 Batch  246 / 488  Training Loss  0.007102414034307003\n",
            "Epoch  19 Batch  247 / 488  Training Loss  0.000915953132789582\n",
            "Epoch  19 Batch  248 / 488  Training Loss  0.0012269581202417612\n",
            "Epoch  19 Batch  249 / 488  Training Loss  0.006470287684351206\n",
            "Epoch  19 Batch  250 / 488  Training Loss  0.0022839843295514584\n",
            "Epoch  19 Batch  251 / 488  Training Loss  0.005468236282467842\n",
            "Epoch  19 Batch  252 / 488  Training Loss  0.000992900924757123\n",
            "Epoch  19 Batch  253 / 488  Training Loss  0.0026439621578902006\n",
            "Epoch  19 Batch  254 / 488  Training Loss  0.0031968578696250916\n",
            "Epoch  19 Batch  255 / 488  Training Loss  0.0009565731743350625\n",
            "Epoch  19 Batch  256 / 488  Training Loss  0.0013653531204909086\n",
            "Epoch  19 Batch  257 / 488  Training Loss  0.0026623522862792015\n",
            "Epoch  19 Batch  258 / 488  Training Loss  0.0007862105267122388\n",
            "Epoch  19 Batch  259 / 488  Training Loss  0.001306009478867054\n",
            "Epoch  19 Batch  260 / 488  Training Loss  0.0008004420669749379\n",
            "Epoch  19 Batch  261 / 488  Training Loss  0.0004890374257229269\n",
            "Epoch  19 Batch  262 / 488  Training Loss  0.0015059347497299314\n",
            "Epoch  19 Batch  263 / 488  Training Loss  0.0017677253345027566\n",
            "Epoch  19 Batch  264 / 488  Training Loss  0.000725179968867451\n",
            "Epoch  19 Batch  265 / 488  Training Loss  0.0025639659725129604\n",
            "Epoch  19 Batch  266 / 488  Training Loss  0.0009457581909373403\n",
            "Epoch  19 Batch  267 / 488  Training Loss  0.000754467851947993\n",
            "Epoch  19 Batch  268 / 488  Training Loss  0.0031038057059049606\n",
            "Epoch  19 Batch  269 / 488  Training Loss  0.0012185366358608007\n",
            "Epoch  19 Batch  270 / 488  Training Loss  0.0010686919558793306\n",
            "Epoch  19 Batch  271 / 488  Training Loss  0.006247352343052626\n",
            "Epoch  19 Batch  272 / 488  Training Loss  0.0020109503529965878\n",
            "Epoch  19 Batch  273 / 488  Training Loss  0.0011563372099772096\n",
            "Epoch  19 Batch  274 / 488  Training Loss  0.0008113663643598557\n",
            "Epoch  19 Batch  275 / 488  Training Loss  0.0015354009810835123\n",
            "Epoch  19 Batch  276 / 488  Training Loss  0.0010278947884216905\n",
            "Epoch  19 Batch  277 / 488  Training Loss  0.000772574101574719\n",
            "Epoch  19 Batch  278 / 488  Training Loss  0.004788226447999477\n",
            "Epoch  19 Batch  279 / 488  Training Loss  0.0026646701153367758\n",
            "Epoch  19 Batch  280 / 488  Training Loss  0.000994467525742948\n",
            "Epoch  19 Batch  281 / 488  Training Loss  0.001160158310085535\n",
            "Epoch  19 Batch  282 / 488  Training Loss  0.002994618844240904\n",
            "Epoch  19 Batch  283 / 488  Training Loss  0.0025092570576816797\n",
            "Epoch  19 Batch  284 / 488  Training Loss  0.0006788796745240688\n",
            "Epoch  19 Batch  285 / 488  Training Loss  0.0018202472710981965\n",
            "Epoch  19 Batch  286 / 488  Training Loss  0.0009923293255269527\n",
            "Epoch  19 Batch  287 / 488  Training Loss  0.003736967220902443\n",
            "Epoch  19 Batch  288 / 488  Training Loss  0.001413640333339572\n",
            "Epoch  19 Batch  289 / 488  Training Loss  0.002683461643755436\n",
            "Epoch  19 Batch  290 / 488  Training Loss  0.0011819649953395128\n",
            "Epoch  19 Batch  291 / 488  Training Loss  0.0009412635117769241\n",
            "Epoch  19 Batch  292 / 488  Training Loss  0.0039311861619353294\n",
            "Epoch  19 Batch  293 / 488  Training Loss  0.0017727408558130264\n",
            "Epoch  19 Batch  294 / 488  Training Loss  0.0007247980101965368\n",
            "Epoch  19 Batch  295 / 488  Training Loss  0.0006167374667711556\n",
            "Epoch  19 Batch  296 / 488  Training Loss  0.0007764738402329385\n",
            "Epoch  19 Batch  297 / 488  Training Loss  0.0014703084016218781\n",
            "Epoch  19 Batch  298 / 488  Training Loss  0.0012769390596076846\n",
            "Epoch  19 Batch  299 / 488  Training Loss  0.0016333665698766708\n",
            "Epoch  19 Batch  300 / 488  Training Loss  0.0009584760991856456\n",
            "Epoch  19 Batch  301 / 488  Training Loss  0.000607017835136503\n",
            "Epoch  19 Batch  302 / 488  Training Loss  0.0008924513822421432\n",
            "Epoch  19 Batch  303 / 488  Training Loss  0.0008952434291131794\n",
            "Epoch  19 Batch  304 / 488  Training Loss  0.005792416166514158\n",
            "Epoch  19 Batch  305 / 488  Training Loss  0.001679124659858644\n",
            "Epoch  19 Batch  306 / 488  Training Loss  0.002195588545873761\n",
            "Epoch  19 Batch  307 / 488  Training Loss  0.0011730839032679796\n",
            "Epoch  19 Batch  308 / 488  Training Loss  0.0020998467225581408\n",
            "Epoch  19 Batch  309 / 488  Training Loss  0.0005460225511342287\n",
            "Epoch  19 Batch  310 / 488  Training Loss  0.001075330306775868\n",
            "Epoch  19 Batch  311 / 488  Training Loss  0.0009255891782231629\n",
            "Epoch  19 Batch  312 / 488  Training Loss  0.0005813011666759849\n",
            "Epoch  19 Batch  313 / 488  Training Loss  0.0004968163557350636\n",
            "Epoch  19 Batch  314 / 488  Training Loss  0.002361869905143976\n",
            "Epoch  19 Batch  315 / 488  Training Loss  0.001179077080450952\n",
            "Epoch  19 Batch  316 / 488  Training Loss  0.0011989525519311428\n",
            "Epoch  19 Batch  317 / 488  Training Loss  0.0010973403695970774\n",
            "Epoch  19 Batch  318 / 488  Training Loss  0.0008269812678918242\n",
            "Epoch  19 Batch  319 / 488  Training Loss  0.0005657754372805357\n",
            "Epoch  19 Batch  320 / 488  Training Loss  0.0035241071600466967\n",
            "Epoch  19 Batch  321 / 488  Training Loss  0.0009131230181083083\n",
            "Epoch  19 Batch  322 / 488  Training Loss  0.0007203973946161568\n",
            "Epoch  19 Batch  323 / 488  Training Loss  0.0005056454101577401\n",
            "Epoch  19 Batch  324 / 488  Training Loss  0.000663366517983377\n",
            "Epoch  19 Batch  325 / 488  Training Loss  0.0014352251309901476\n",
            "Epoch  19 Batch  326 / 488  Training Loss  0.0006010491633787751\n",
            "Epoch  19 Batch  327 / 488  Training Loss  0.000636790064163506\n",
            "Epoch  19 Batch  328 / 488  Training Loss  0.00946300383657217\n",
            "Epoch  19 Batch  329 / 488  Training Loss  0.0010136350756511092\n",
            "Epoch  19 Batch  330 / 488  Training Loss  0.0010014071594923735\n",
            "Epoch  19 Batch  331 / 488  Training Loss  0.007655886001884937\n",
            "Epoch  19 Batch  332 / 488  Training Loss  0.0012185617815703154\n",
            "Epoch  19 Batch  333 / 488  Training Loss  0.0007861214689910412\n",
            "Epoch  19 Batch  334 / 488  Training Loss  0.004453455097973347\n",
            "Epoch  19 Batch  335 / 488  Training Loss  0.007363912649452686\n",
            "Epoch  19 Batch  336 / 488  Training Loss  0.0012431531213223934\n",
            "Epoch  19 Batch  337 / 488  Training Loss  0.0019980105571448803\n",
            "Epoch  19 Batch  338 / 488  Training Loss  0.0006326829898171127\n",
            "Epoch  19 Batch  339 / 488  Training Loss  0.0031113510485738516\n",
            "Epoch  19 Batch  340 / 488  Training Loss  0.0011885326821357012\n",
            "Epoch  19 Batch  341 / 488  Training Loss  0.003439622465521097\n",
            "Epoch  19 Batch  342 / 488  Training Loss  0.003288151230663061\n",
            "Epoch  19 Batch  343 / 488  Training Loss  0.0010573185281828046\n",
            "Epoch  19 Batch  344 / 488  Training Loss  0.0015559801831841469\n",
            "Epoch  19 Batch  345 / 488  Training Loss  0.0009323587873950601\n",
            "Epoch  19 Batch  346 / 488  Training Loss  0.0007201144471764565\n",
            "Epoch  19 Batch  347 / 488  Training Loss  0.0008332600700668991\n",
            "Epoch  19 Batch  348 / 488  Training Loss  0.0019639129750430584\n",
            "Epoch  19 Batch  349 / 488  Training Loss  0.0012337063672021031\n",
            "Epoch  19 Batch  350 / 488  Training Loss  0.004844547715038061\n",
            "Epoch  19 Batch  351 / 488  Training Loss  0.0007671635830774903\n",
            "Epoch  19 Batch  352 / 488  Training Loss  0.011729947291314602\n",
            "Epoch  19 Batch  353 / 488  Training Loss  0.0033722470980137587\n",
            "Epoch  19 Batch  354 / 488  Training Loss  0.0015289206057786942\n",
            "Epoch  19 Batch  355 / 488  Training Loss  0.0017276344588026404\n",
            "Epoch  19 Batch  356 / 488  Training Loss  0.0009061911841854453\n",
            "Epoch  19 Batch  357 / 488  Training Loss  0.001323868054896593\n",
            "Epoch  19 Batch  358 / 488  Training Loss  0.0034932680428028107\n",
            "Epoch  19 Batch  359 / 488  Training Loss  0.0025391867384314537\n",
            "Epoch  19 Batch  360 / 488  Training Loss  0.0013257551472634077\n",
            "Epoch  19 Batch  361 / 488  Training Loss  0.0009817546233534813\n",
            "Epoch  19 Batch  362 / 488  Training Loss  0.0009306665742769837\n",
            "Epoch  19 Batch  363 / 488  Training Loss  0.0019143655663356185\n",
            "Epoch  19 Batch  364 / 488  Training Loss  0.0019285572925582528\n",
            "Epoch  19 Batch  365 / 488  Training Loss  0.00200084550306201\n",
            "Epoch  19 Batch  366 / 488  Training Loss  0.0017197660636156797\n",
            "Epoch  19 Batch  367 / 488  Training Loss  0.0022001969628036022\n",
            "Epoch  19 Batch  368 / 488  Training Loss  0.0023890563752502203\n",
            "Epoch  19 Batch  369 / 488  Training Loss  0.0016439363826066256\n",
            "Epoch  19 Batch  370 / 488  Training Loss  0.0016610801685601473\n",
            "Epoch  19 Batch  371 / 488  Training Loss  0.0010879563633352518\n",
            "Epoch  19 Batch  372 / 488  Training Loss  0.0013687405735254288\n",
            "Epoch  19 Batch  373 / 488  Training Loss  0.0008069734903983772\n",
            "Epoch  19 Batch  374 / 488  Training Loss  0.004239700268954039\n",
            "Epoch  19 Batch  375 / 488  Training Loss  0.001019880874082446\n",
            "Epoch  19 Batch  376 / 488  Training Loss  0.0017546931048855186\n",
            "Epoch  19 Batch  377 / 488  Training Loss  0.0014385465765371919\n",
            "Epoch  19 Batch  378 / 488  Training Loss  0.0017464689444750547\n",
            "Epoch  19 Batch  379 / 488  Training Loss  0.0008156616240739822\n",
            "Epoch  19 Batch  380 / 488  Training Loss  0.0013477737084031105\n",
            "Epoch  19 Batch  381 / 488  Training Loss  0.0009253306197933853\n",
            "Epoch  19 Batch  382 / 488  Training Loss  0.0015128303784877062\n",
            "Epoch  19 Batch  383 / 488  Training Loss  0.0011734429281204939\n",
            "Epoch  19 Batch  384 / 488  Training Loss  0.0007898332551121712\n",
            "Epoch  19 Batch  385 / 488  Training Loss  0.00672356691211462\n",
            "Epoch  19 Batch  386 / 488  Training Loss  0.002801086287945509\n",
            "Epoch  19 Batch  387 / 488  Training Loss  0.008466011844575405\n",
            "Epoch  19 Batch  388 / 488  Training Loss  0.0006396346725523472\n",
            "Epoch  19 Batch  389 / 488  Training Loss  0.001901647076010704\n",
            "Epoch  19 Batch  390 / 488  Training Loss  0.0019305292516946793\n",
            "Epoch  19 Batch  391 / 488  Training Loss  0.0027703267987817526\n",
            "Epoch  19 Batch  392 / 488  Training Loss  0.018421227112412453\n",
            "Epoch  19 Batch  393 / 488  Training Loss  0.0015796335646882653\n",
            "Epoch  19 Batch  394 / 488  Training Loss  0.0019711176864802837\n",
            "Epoch  19 Batch  395 / 488  Training Loss  0.0012344048591330647\n",
            "Epoch  19 Batch  396 / 488  Training Loss  0.0012295322958379984\n",
            "Epoch  19 Batch  397 / 488  Training Loss  0.00227144593372941\n",
            "Epoch  19 Batch  398 / 488  Training Loss  0.0017715876456350088\n",
            "Epoch  19 Batch  399 / 488  Training Loss  0.004305046051740646\n",
            "Epoch  19 Batch  400 / 488  Training Loss  0.007067353930324316\n",
            "Epoch  19 Batch  401 / 488  Training Loss  0.003126493887975812\n",
            "Epoch  19 Batch  402 / 488  Training Loss  0.0006255618645809591\n",
            "Epoch  19 Batch  403 / 488  Training Loss  0.004605266731232405\n",
            "Epoch  19 Batch  404 / 488  Training Loss  0.009477317333221436\n",
            "Epoch  19 Batch  405 / 488  Training Loss  0.0012116121361032128\n",
            "Epoch  19 Batch  406 / 488  Training Loss  0.002493618056178093\n",
            "Epoch  19 Batch  407 / 488  Training Loss  0.0007376164430752397\n",
            "Epoch  19 Batch  408 / 488  Training Loss  0.002353873336687684\n",
            "Epoch  19 Batch  409 / 488  Training Loss  0.01478433609008789\n",
            "Epoch  19 Batch  410 / 488  Training Loss  0.0018565170466899872\n",
            "Epoch  19 Batch  411 / 488  Training Loss  0.0004390489775687456\n",
            "Epoch  19 Batch  412 / 488  Training Loss  0.0014138845726847649\n",
            "Epoch  19 Batch  413 / 488  Training Loss  0.0008615840342827141\n",
            "Epoch  19 Batch  414 / 488  Training Loss  0.002246567513793707\n",
            "Epoch  19 Batch  415 / 488  Training Loss  0.0011351141147315502\n",
            "Epoch  19 Batch  416 / 488  Training Loss  0.0008002331596799195\n",
            "Epoch  19 Batch  417 / 488  Training Loss  0.0033746100962162018\n",
            "Epoch  19 Batch  418 / 488  Training Loss  0.0009022933663800359\n",
            "Epoch  19 Batch  419 / 488  Training Loss  0.0013305029133334756\n",
            "Epoch  19 Batch  420 / 488  Training Loss  0.0029229545034468174\n",
            "Epoch  19 Batch  421 / 488  Training Loss  0.0006514102569781244\n",
            "Epoch  19 Batch  422 / 488  Training Loss  0.0018480580765753984\n",
            "Epoch  19 Batch  423 / 488  Training Loss  0.0008974002557806671\n",
            "Epoch  19 Batch  424 / 488  Training Loss  0.0009151058038696647\n",
            "Epoch  19 Batch  425 / 488  Training Loss  0.001235077972523868\n",
            "Epoch  19 Batch  426 / 488  Training Loss  0.00101120350882411\n",
            "Epoch  19 Batch  427 / 488  Training Loss  0.0018919134745374322\n",
            "Epoch  19 Batch  428 / 488  Training Loss  0.005158442538231611\n",
            "Epoch  19 Batch  429 / 488  Training Loss  0.0017446313286200166\n",
            "Epoch  19 Batch  430 / 488  Training Loss  0.0010514911264181137\n",
            "Epoch  19 Batch  431 / 488  Training Loss  0.0006872364319860935\n",
            "Epoch  19 Batch  432 / 488  Training Loss  0.0008161248406395316\n",
            "Epoch  19 Batch  433 / 488  Training Loss  0.0010949179995805025\n",
            "Epoch  19 Batch  434 / 488  Training Loss  0.002184839453548193\n",
            "Epoch  19 Batch  435 / 488  Training Loss  0.0023282566107809544\n",
            "Epoch  19 Batch  436 / 488  Training Loss  0.0004634539072867483\n",
            "Epoch  19 Batch  437 / 488  Training Loss  0.0014062040718272328\n",
            "Epoch  19 Batch  438 / 488  Training Loss  0.0016331125516444445\n",
            "Epoch  19 Batch  439 / 488  Training Loss  0.003180931555107236\n",
            "Epoch  19 Batch  440 / 488  Training Loss  0.002287305658683181\n",
            "Epoch  19 Batch  441 / 488  Training Loss  0.0014327558455988765\n",
            "Epoch  19 Batch  442 / 488  Training Loss  0.0007048514089547098\n",
            "Epoch  19 Batch  443 / 488  Training Loss  0.0010267433244735003\n",
            "Epoch  19 Batch  444 / 488  Training Loss  0.0073606097139418125\n",
            "Epoch  19 Batch  445 / 488  Training Loss  0.0011183141032233834\n",
            "Epoch  19 Batch  446 / 488  Training Loss  0.0023279855959117413\n",
            "Epoch  19 Batch  447 / 488  Training Loss  0.002026547212153673\n",
            "Epoch  19 Batch  448 / 488  Training Loss  0.006126456428319216\n",
            "Epoch  19 Batch  449 / 488  Training Loss  0.003402848495170474\n",
            "Epoch  19 Batch  450 / 488  Training Loss  0.002055761869996786\n",
            "Epoch  19 Batch  451 / 488  Training Loss  0.0010825998615473509\n",
            "Epoch  19 Batch  452 / 488  Training Loss  0.0020709247328341007\n",
            "Epoch  19 Batch  453 / 488  Training Loss  0.0009062307071872056\n",
            "Epoch  19 Batch  454 / 488  Training Loss  0.0016443025087937713\n",
            "Epoch  19 Batch  455 / 488  Training Loss  0.0019284153822809458\n",
            "Epoch  19 Batch  456 / 488  Training Loss  0.002697532530874014\n",
            "Epoch  19 Batch  457 / 488  Training Loss  0.0038880896754562855\n",
            "Epoch  19 Batch  458 / 488  Training Loss  0.0036520007997751236\n",
            "Epoch  19 Batch  459 / 488  Training Loss  0.0018352397019043565\n",
            "Epoch  19 Batch  460 / 488  Training Loss  0.0013461426133289933\n",
            "Epoch  19 Batch  461 / 488  Training Loss  0.0011051686014980078\n",
            "Epoch  19 Batch  462 / 488  Training Loss  0.0008758084150031209\n",
            "Epoch  19 Batch  463 / 488  Training Loss  0.0019505979726091027\n",
            "Epoch  19 Batch  464 / 488  Training Loss  0.002940611680969596\n",
            "Epoch  19 Batch  465 / 488  Training Loss  0.0010271971113979816\n",
            "Epoch  19 Batch  466 / 488  Training Loss  0.0008340500062331557\n",
            "Epoch  19 Batch  467 / 488  Training Loss  0.007660713046789169\n",
            "Epoch  19 Batch  468 / 488  Training Loss  0.0007288343040272593\n",
            "Epoch  19 Batch  469 / 488  Training Loss  0.0006964518106542528\n",
            "Epoch  19 Batch  470 / 488  Training Loss  0.0016831548418849707\n",
            "Epoch  19 Batch  471 / 488  Training Loss  0.0009525174973532557\n",
            "Epoch  19 Batch  472 / 488  Training Loss  0.0016344956820830703\n",
            "Epoch  19 Batch  473 / 488  Training Loss  0.0008735467563383281\n",
            "Epoch  19 Batch  474 / 488  Training Loss  0.0014520820695906878\n",
            "Epoch  19 Batch  475 / 488  Training Loss  0.0010172721231356263\n",
            "Epoch  19 Batch  476 / 488  Training Loss  0.0006801524432376027\n",
            "Epoch  19 Batch  477 / 488  Training Loss  0.0013728751800954342\n",
            "Epoch  19 Batch  478 / 488  Training Loss  0.001396397827193141\n",
            "Epoch  19 Batch  479 / 488  Training Loss  0.0014308181125670671\n",
            "Epoch  19 Batch  480 / 488  Training Loss  0.0016614336054772139\n",
            "Epoch  19 Batch  481 / 488  Training Loss  0.002140745520591736\n",
            "Epoch  19 Batch  482 / 488  Training Loss  0.0010549492435529828\n",
            "Epoch  19 Batch  483 / 488  Training Loss  0.0008012354373931885\n",
            "Epoch  19 Batch  484 / 488  Training Loss  0.0006585600785911083\n",
            "Epoch  19 Batch  485 / 488  Training Loss  0.0022955199237912893\n",
            "Epoch  19 Batch  486 / 488  Training Loss  0.0011158224660903215\n",
            "Epoch  19 Batch  487 / 488  Training Loss  0.0007171716424636543\n",
            "  20    |    -    |   0.002141   | 45.622784\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 20\n",
            "Epoch  20 Batch  0 / 488  Training Loss  0.00041440362110733986\n",
            "Epoch  20 Batch  1 / 488  Training Loss  0.0005534029332920909\n",
            "Epoch  20 Batch  2 / 488  Training Loss  0.0005026886938139796\n",
            "Epoch  20 Batch  3 / 488  Training Loss  0.0004557932261377573\n",
            "Epoch  20 Batch  4 / 488  Training Loss  0.006307968404144049\n",
            "Epoch  20 Batch  5 / 488  Training Loss  0.0007815271383151412\n",
            "Epoch  20 Batch  6 / 488  Training Loss  0.0006820887210778892\n",
            "Epoch  20 Batch  7 / 488  Training Loss  0.0004548975848592818\n",
            "Epoch  20 Batch  8 / 488  Training Loss  0.0014453816693276167\n",
            "Epoch  20 Batch  9 / 488  Training Loss  0.002347997622564435\n",
            "Epoch  20 Batch  10 / 488  Training Loss  0.00140570686198771\n",
            "Epoch  20 Batch  11 / 488  Training Loss  0.001204219413921237\n",
            "Epoch  20 Batch  12 / 488  Training Loss  0.0039673722349107265\n",
            "Epoch  20 Batch  13 / 488  Training Loss  0.0007799147861078382\n",
            "Epoch  20 Batch  14 / 488  Training Loss  0.0014324632938951254\n",
            "Epoch  20 Batch  15 / 488  Training Loss  0.0015920972218737006\n",
            "Epoch  20 Batch  16 / 488  Training Loss  0.0012630679411813617\n",
            "Epoch  20 Batch  17 / 488  Training Loss  0.001082067028619349\n",
            "Epoch  20 Batch  18 / 488  Training Loss  0.003844284685328603\n",
            "Epoch  20 Batch  19 / 488  Training Loss  0.0013542098458856344\n",
            "Epoch  20 Batch  20 / 488  Training Loss  0.0007352339453063905\n",
            "Epoch  20 Batch  21 / 488  Training Loss  0.0014686902286484838\n",
            "Epoch  20 Batch  22 / 488  Training Loss  0.0010809714440256357\n",
            "Epoch  20 Batch  23 / 488  Training Loss  0.0003828979970421642\n",
            "Epoch  20 Batch  24 / 488  Training Loss  0.0005531164351850748\n",
            "Epoch  20 Batch  25 / 488  Training Loss  0.0011694190325215459\n",
            "Epoch  20 Batch  26 / 488  Training Loss  0.0002346370165469125\n",
            "Epoch  20 Batch  27 / 488  Training Loss  0.0005286027444526553\n",
            "Epoch  20 Batch  28 / 488  Training Loss  0.0006004375172778964\n",
            "Epoch  20 Batch  29 / 488  Training Loss  0.00030696357134729624\n",
            "Epoch  20 Batch  30 / 488  Training Loss  0.0037752788048237562\n",
            "Epoch  20 Batch  31 / 488  Training Loss  0.0013648709282279015\n",
            "Epoch  20 Batch  32 / 488  Training Loss  0.014976806938648224\n",
            "Epoch  20 Batch  33 / 488  Training Loss  0.0005994918756186962\n",
            "Epoch  20 Batch  34 / 488  Training Loss  0.0009412666549906135\n",
            "Epoch  20 Batch  35 / 488  Training Loss  0.0021983268670737743\n",
            "Epoch  20 Batch  36 / 488  Training Loss  0.0008239528397098184\n",
            "Epoch  20 Batch  37 / 488  Training Loss  0.0007956128683872521\n",
            "Epoch  20 Batch  38 / 488  Training Loss  0.005139322020113468\n",
            "Epoch  20 Batch  39 / 488  Training Loss  0.0022726254537701607\n",
            "Epoch  20 Batch  40 / 488  Training Loss  0.0030843503773212433\n",
            "Epoch  20 Batch  41 / 488  Training Loss  0.0007606857689097524\n",
            "Epoch  20 Batch  42 / 488  Training Loss  0.005396828521043062\n",
            "Epoch  20 Batch  43 / 488  Training Loss  0.0009950448293238878\n",
            "Epoch  20 Batch  44 / 488  Training Loss  0.0006619574851356447\n",
            "Epoch  20 Batch  45 / 488  Training Loss  0.00046043176553212106\n",
            "Epoch  20 Batch  46 / 488  Training Loss  0.001691166078671813\n",
            "Epoch  20 Batch  47 / 488  Training Loss  0.0009756747749634087\n",
            "Epoch  20 Batch  48 / 488  Training Loss  0.0007191300974227488\n",
            "Epoch  20 Batch  49 / 488  Training Loss  0.007595354225486517\n",
            "Epoch  20 Batch  50 / 488  Training Loss  0.002140629570931196\n",
            "Epoch  20 Batch  51 / 488  Training Loss  0.001132852048613131\n",
            "Epoch  20 Batch  52 / 488  Training Loss  0.0014060374815016985\n",
            "Epoch  20 Batch  53 / 488  Training Loss  0.0028297631070017815\n",
            "Epoch  20 Batch  54 / 488  Training Loss  0.002327829133719206\n",
            "Epoch  20 Batch  55 / 488  Training Loss  0.002335259458050132\n",
            "Epoch  20 Batch  56 / 488  Training Loss  0.0013209532480686903\n",
            "Epoch  20 Batch  57 / 488  Training Loss  0.0012803783174604177\n",
            "Epoch  20 Batch  58 / 488  Training Loss  0.001144148176535964\n",
            "Epoch  20 Batch  59 / 488  Training Loss  0.0012814225628972054\n",
            "Epoch  20 Batch  60 / 488  Training Loss  0.0013321966398507357\n",
            "Epoch  20 Batch  61 / 488  Training Loss  0.000897773657925427\n",
            "Epoch  20 Batch  62 / 488  Training Loss  0.001061583636328578\n",
            "Epoch  20 Batch  63 / 488  Training Loss  0.00043261554674245417\n",
            "Epoch  20 Batch  64 / 488  Training Loss  0.00046862143790349364\n",
            "Epoch  20 Batch  65 / 488  Training Loss  0.001586990780197084\n",
            "Epoch  20 Batch  66 / 488  Training Loss  0.001752725220285356\n",
            "Epoch  20 Batch  67 / 488  Training Loss  0.0006745329010300338\n",
            "Epoch  20 Batch  68 / 488  Training Loss  0.0005158035783097148\n",
            "Epoch  20 Batch  69 / 488  Training Loss  0.0012465526815503836\n",
            "Epoch  20 Batch  70 / 488  Training Loss  0.0005663834745064378\n",
            "Epoch  20 Batch  71 / 488  Training Loss  0.00040528373210690916\n",
            "Epoch  20 Batch  72 / 488  Training Loss  0.0024957682471722364\n",
            "Epoch  20 Batch  73 / 488  Training Loss  0.0012109410017728806\n",
            "Epoch  20 Batch  74 / 488  Training Loss  0.000980018638074398\n",
            "Epoch  20 Batch  75 / 488  Training Loss  0.0008102100109681487\n",
            "Epoch  20 Batch  76 / 488  Training Loss  0.0006102998740971088\n",
            "Epoch  20 Batch  77 / 488  Training Loss  0.0037794667296111584\n",
            "Epoch  20 Batch  78 / 488  Training Loss  0.0004972314345650375\n",
            "Epoch  20 Batch  79 / 488  Training Loss  0.0012361083645373583\n",
            "Epoch  20 Batch  80 / 488  Training Loss  0.0007646981393918395\n",
            "Epoch  20 Batch  81 / 488  Training Loss  0.00036909975460730493\n",
            "Epoch  20 Batch  82 / 488  Training Loss  0.0014127023750916123\n",
            "Epoch  20 Batch  83 / 488  Training Loss  0.0006035149563103914\n",
            "Epoch  20 Batch  84 / 488  Training Loss  0.0009046895429491997\n",
            "Epoch  20 Batch  85 / 488  Training Loss  0.0009321790421381593\n",
            "Epoch  20 Batch  86 / 488  Training Loss  0.0005401680828072131\n",
            "Epoch  20 Batch  87 / 488  Training Loss  0.002610504627227783\n",
            "Epoch  20 Batch  88 / 488  Training Loss  0.0015923589235171676\n",
            "Epoch  20 Batch  89 / 488  Training Loss  0.0009958908194676042\n",
            "Epoch  20 Batch  90 / 488  Training Loss  0.0008654646808281541\n",
            "Epoch  20 Batch  91 / 488  Training Loss  0.0006509548402391374\n",
            "Epoch  20 Batch  92 / 488  Training Loss  0.00045954520464874804\n",
            "Epoch  20 Batch  93 / 488  Training Loss  0.0013306033797562122\n",
            "Epoch  20 Batch  94 / 488  Training Loss  0.0009501773165538907\n",
            "Epoch  20 Batch  95 / 488  Training Loss  0.000629618123639375\n",
            "Epoch  20 Batch  96 / 488  Training Loss  0.0005017783842049539\n",
            "Epoch  20 Batch  97 / 488  Training Loss  0.0016815245617181063\n",
            "Epoch  20 Batch  98 / 488  Training Loss  0.0009592914720997214\n",
            "Epoch  20 Batch  99 / 488  Training Loss  0.0007915166206657887\n",
            "Epoch  20 Batch  100 / 488  Training Loss  0.0006039724103175104\n",
            "Epoch  20 Batch  101 / 488  Training Loss  0.0008907782030291855\n",
            "Epoch  20 Batch  102 / 488  Training Loss  0.0012075998820364475\n",
            "Epoch  20 Batch  103 / 488  Training Loss  0.0014784358208999038\n",
            "Epoch  20 Batch  104 / 488  Training Loss  0.0005444861599244177\n",
            "Epoch  20 Batch  105 / 488  Training Loss  0.0007478733314201236\n",
            "Epoch  20 Batch  106 / 488  Training Loss  0.00037847599014639854\n",
            "Epoch  20 Batch  107 / 488  Training Loss  0.0007300857687368989\n",
            "Epoch  20 Batch  108 / 488  Training Loss  0.000918970734346658\n",
            "Epoch  20 Batch  109 / 488  Training Loss  0.0007555399788543582\n",
            "Epoch  20 Batch  110 / 488  Training Loss  0.0005386484554037452\n",
            "Epoch  20 Batch  111 / 488  Training Loss  0.001297353534027934\n",
            "Epoch  20 Batch  112 / 488  Training Loss  0.0007959307404235005\n",
            "Epoch  20 Batch  113 / 488  Training Loss  0.0034992683213204145\n",
            "Epoch  20 Batch  114 / 488  Training Loss  0.0009987499797716737\n",
            "Epoch  20 Batch  115 / 488  Training Loss  0.0004884710069745779\n",
            "Epoch  20 Batch  116 / 488  Training Loss  0.0004593586199916899\n",
            "Epoch  20 Batch  117 / 488  Training Loss  0.0004006451927125454\n",
            "Epoch  20 Batch  118 / 488  Training Loss  0.0019735959358513355\n",
            "Epoch  20 Batch  119 / 488  Training Loss  0.0009745730203576386\n",
            "Epoch  20 Batch  120 / 488  Training Loss  0.00030954493558965623\n",
            "Epoch  20 Batch  121 / 488  Training Loss  0.0007965816184878349\n",
            "Epoch  20 Batch  122 / 488  Training Loss  0.0006351632764562964\n",
            "Epoch  20 Batch  123 / 488  Training Loss  0.0030432986095547676\n",
            "Epoch  20 Batch  124 / 488  Training Loss  0.0003650083381216973\n",
            "Epoch  20 Batch  125 / 488  Training Loss  0.00120432092808187\n",
            "Epoch  20 Batch  126 / 488  Training Loss  0.0007144275587052107\n",
            "Epoch  20 Batch  127 / 488  Training Loss  0.0006993985734879971\n",
            "Epoch  20 Batch  128 / 488  Training Loss  0.0005710438126698136\n",
            "Epoch  20 Batch  129 / 488  Training Loss  0.0007126448908820748\n",
            "Epoch  20 Batch  130 / 488  Training Loss  0.0008416920900344849\n",
            "Epoch  20 Batch  131 / 488  Training Loss  0.0008798005292192101\n",
            "Epoch  20 Batch  132 / 488  Training Loss  0.0005462311091832817\n",
            "Epoch  20 Batch  133 / 488  Training Loss  0.0004937194171361625\n",
            "Epoch  20 Batch  134 / 488  Training Loss  0.0007152865873649716\n",
            "Epoch  20 Batch  135 / 488  Training Loss  0.00033605375210754573\n",
            "Epoch  20 Batch  136 / 488  Training Loss  0.0009865930769592524\n",
            "Epoch  20 Batch  137 / 488  Training Loss  0.0005296440212987363\n",
            "Epoch  20 Batch  138 / 488  Training Loss  0.0002163215249311179\n",
            "Epoch  20 Batch  139 / 488  Training Loss  0.0007746204501017928\n",
            "Epoch  20 Batch  140 / 488  Training Loss  0.0007173721678555012\n",
            "Epoch  20 Batch  141 / 488  Training Loss  0.0006502940086647868\n",
            "Epoch  20 Batch  142 / 488  Training Loss  0.0010180746903643012\n",
            "Epoch  20 Batch  143 / 488  Training Loss  0.0003836235555354506\n",
            "Epoch  20 Batch  144 / 488  Training Loss  0.0005457879742607474\n",
            "Epoch  20 Batch  145 / 488  Training Loss  0.0009043844183906913\n",
            "Epoch  20 Batch  146 / 488  Training Loss  0.0006605126545764506\n",
            "Epoch  20 Batch  147 / 488  Training Loss  0.0006352273048833013\n",
            "Epoch  20 Batch  148 / 488  Training Loss  0.0007244923617690802\n",
            "Epoch  20 Batch  149 / 488  Training Loss  0.00046668219147250056\n",
            "Epoch  20 Batch  150 / 488  Training Loss  0.0004635357763618231\n",
            "Epoch  20 Batch  151 / 488  Training Loss  0.00046770242624916136\n",
            "Epoch  20 Batch  152 / 488  Training Loss  0.0013269387418404222\n",
            "Epoch  20 Batch  153 / 488  Training Loss  0.0007152323378250003\n",
            "Epoch  20 Batch  154 / 488  Training Loss  0.0006901145097799599\n",
            "Epoch  20 Batch  155 / 488  Training Loss  0.0021795916836708784\n",
            "Epoch  20 Batch  156 / 488  Training Loss  0.0006024354370310903\n",
            "Epoch  20 Batch  157 / 488  Training Loss  0.0003132046840619296\n",
            "Epoch  20 Batch  158 / 488  Training Loss  0.00037559433258138597\n",
            "Epoch  20 Batch  159 / 488  Training Loss  0.0007312375819310546\n",
            "Epoch  20 Batch  160 / 488  Training Loss  0.0012657236075028777\n",
            "Epoch  20 Batch  161 / 488  Training Loss  0.0005019598756916821\n",
            "Epoch  20 Batch  162 / 488  Training Loss  0.00027266511460766196\n",
            "Epoch  20 Batch  163 / 488  Training Loss  0.0035062835086137056\n",
            "Epoch  20 Batch  164 / 488  Training Loss  0.0007460388005711138\n",
            "Epoch  20 Batch  165 / 488  Training Loss  0.0007458561449311674\n",
            "Epoch  20 Batch  166 / 488  Training Loss  0.0005477415979839861\n",
            "Epoch  20 Batch  167 / 488  Training Loss  0.00042745345854200423\n",
            "Epoch  20 Batch  168 / 488  Training Loss  0.0018871680367738008\n",
            "Epoch  20 Batch  169 / 488  Training Loss  0.0012141258921474218\n",
            "Epoch  20 Batch  170 / 488  Training Loss  0.0002601228188723326\n",
            "Epoch  20 Batch  171 / 488  Training Loss  0.001489767455495894\n",
            "Epoch  20 Batch  172 / 488  Training Loss  0.001560521312057972\n",
            "Epoch  20 Batch  173 / 488  Training Loss  0.0017822661902755499\n",
            "Epoch  20 Batch  174 / 488  Training Loss  0.0011972388019785285\n",
            "Epoch  20 Batch  175 / 488  Training Loss  0.000914494157768786\n",
            "Epoch  20 Batch  176 / 488  Training Loss  0.0006688713910989463\n",
            "Epoch  20 Batch  177 / 488  Training Loss  0.0006661072256974876\n",
            "Epoch  20 Batch  178 / 488  Training Loss  0.0003834398230537772\n",
            "Epoch  20 Batch  179 / 488  Training Loss  0.0026542413979768753\n",
            "Epoch  20 Batch  180 / 488  Training Loss  0.0005012148758396506\n",
            "Epoch  20 Batch  181 / 488  Training Loss  0.0011016152566298842\n",
            "Epoch  20 Batch  182 / 488  Training Loss  0.00033628687378950417\n",
            "Epoch  20 Batch  183 / 488  Training Loss  0.0004492283333092928\n",
            "Epoch  20 Batch  184 / 488  Training Loss  0.0007706700125709176\n",
            "Epoch  20 Batch  185 / 488  Training Loss  0.0007374411798082292\n",
            "Epoch  20 Batch  186 / 488  Training Loss  0.00044242158764973283\n",
            "Epoch  20 Batch  187 / 488  Training Loss  0.000350800808519125\n",
            "Epoch  20 Batch  188 / 488  Training Loss  0.0011999579146504402\n",
            "Epoch  20 Batch  189 / 488  Training Loss  0.0006967152003198862\n",
            "Epoch  20 Batch  190 / 488  Training Loss  0.00037718767998740077\n",
            "Epoch  20 Batch  191 / 488  Training Loss  0.0005378425121307373\n",
            "Epoch  20 Batch  192 / 488  Training Loss  0.0019174363696947694\n",
            "Epoch  20 Batch  193 / 488  Training Loss  0.0009027454070746899\n",
            "Epoch  20 Batch  194 / 488  Training Loss  0.0012527740327641368\n",
            "Epoch  20 Batch  195 / 488  Training Loss  0.0005265395157039165\n",
            "Epoch  20 Batch  196 / 488  Training Loss  0.0023256931453943253\n",
            "Epoch  20 Batch  197 / 488  Training Loss  0.0010579776717349887\n",
            "Epoch  20 Batch  198 / 488  Training Loss  0.0006220535142347217\n",
            "Epoch  20 Batch  199 / 488  Training Loss  0.002175653586164117\n",
            "Epoch  20 Batch  200 / 488  Training Loss  0.00039208069210872054\n",
            "Epoch  20 Batch  201 / 488  Training Loss  0.000307940092170611\n",
            "Epoch  20 Batch  202 / 488  Training Loss  0.0014398515922948718\n",
            "Epoch  20 Batch  203 / 488  Training Loss  0.0007308904314413667\n",
            "Epoch  20 Batch  204 / 488  Training Loss  0.0005279224133118987\n",
            "Epoch  20 Batch  205 / 488  Training Loss  0.00040209750295616686\n",
            "Epoch  20 Batch  206 / 488  Training Loss  0.00042350759031251073\n",
            "Epoch  20 Batch  207 / 488  Training Loss  0.00041343196062371135\n",
            "Epoch  20 Batch  208 / 488  Training Loss  0.0010503444354981184\n",
            "Epoch  20 Batch  209 / 488  Training Loss  0.0005227577057667077\n",
            "Epoch  20 Batch  210 / 488  Training Loss  0.0006692552124150097\n",
            "Epoch  20 Batch  211 / 488  Training Loss  0.0003580053453333676\n",
            "Epoch  20 Batch  212 / 488  Training Loss  0.0006627741386182606\n",
            "Epoch  20 Batch  213 / 488  Training Loss  0.001059947069734335\n",
            "Epoch  20 Batch  214 / 488  Training Loss  0.0014172338414937258\n",
            "Epoch  20 Batch  215 / 488  Training Loss  0.0007498020422644913\n",
            "Epoch  20 Batch  216 / 488  Training Loss  0.0008954334771260619\n",
            "Epoch  20 Batch  217 / 488  Training Loss  0.00042239553295075893\n",
            "Epoch  20 Batch  218 / 488  Training Loss  0.0003491497482173145\n",
            "Epoch  20 Batch  219 / 488  Training Loss  0.0004500751383602619\n",
            "Epoch  20 Batch  220 / 488  Training Loss  0.0013767012860625982\n",
            "Epoch  20 Batch  221 / 488  Training Loss  0.0007230574847199023\n",
            "Epoch  20 Batch  222 / 488  Training Loss  0.0012556483270600438\n",
            "Epoch  20 Batch  223 / 488  Training Loss  0.0007354494300670922\n",
            "Epoch  20 Batch  224 / 488  Training Loss  0.0009876856347545981\n",
            "Epoch  20 Batch  225 / 488  Training Loss  0.0005354419117793441\n",
            "Epoch  20 Batch  226 / 488  Training Loss  0.0004062243679072708\n",
            "Epoch  20 Batch  227 / 488  Training Loss  0.00030708740814588964\n",
            "Epoch  20 Batch  228 / 488  Training Loss  0.0006616783211939037\n",
            "Epoch  20 Batch  229 / 488  Training Loss  0.00044711423106491566\n",
            "Epoch  20 Batch  230 / 488  Training Loss  0.0007692303624935448\n",
            "Epoch  20 Batch  231 / 488  Training Loss  0.0008418356301262975\n",
            "Epoch  20 Batch  232 / 488  Training Loss  0.00047959820949472487\n",
            "Epoch  20 Batch  233 / 488  Training Loss  0.0015252225566655397\n",
            "Epoch  20 Batch  234 / 488  Training Loss  0.0007025684462860227\n",
            "Epoch  20 Batch  235 / 488  Training Loss  0.0006700943922623992\n",
            "Epoch  20 Batch  236 / 488  Training Loss  0.0005941548151895404\n",
            "Epoch  20 Batch  237 / 488  Training Loss  0.0007799757295288146\n",
            "Epoch  20 Batch  238 / 488  Training Loss  0.00064051867229864\n",
            "Epoch  20 Batch  239 / 488  Training Loss  0.006661663763225079\n",
            "Epoch  20 Batch  240 / 488  Training Loss  0.0004260956193320453\n",
            "Epoch  20 Batch  241 / 488  Training Loss  0.0003817062242887914\n",
            "Epoch  20 Batch  242 / 488  Training Loss  0.0022235296200960875\n",
            "Epoch  20 Batch  243 / 488  Training Loss  0.0005893524503335357\n",
            "Epoch  20 Batch  244 / 488  Training Loss  0.00031653643236495554\n",
            "Epoch  20 Batch  245 / 488  Training Loss  0.0013246249873191118\n",
            "Epoch  20 Batch  246 / 488  Training Loss  0.001717445789836347\n",
            "Epoch  20 Batch  247 / 488  Training Loss  0.00034785587922669947\n",
            "Epoch  20 Batch  248 / 488  Training Loss  0.0004352568066678941\n",
            "Epoch  20 Batch  249 / 488  Training Loss  0.0006137879681773484\n",
            "Epoch  20 Batch  250 / 488  Training Loss  0.000878920778632164\n",
            "Epoch  20 Batch  251 / 488  Training Loss  0.0004903030931018293\n",
            "Epoch  20 Batch  252 / 488  Training Loss  0.000517498585395515\n",
            "Epoch  20 Batch  253 / 488  Training Loss  0.000509033037815243\n",
            "Epoch  20 Batch  254 / 488  Training Loss  0.00045734253944829106\n",
            "Epoch  20 Batch  255 / 488  Training Loss  0.0004002165514975786\n",
            "Epoch  20 Batch  256 / 488  Training Loss  0.0008005855488590896\n",
            "Epoch  20 Batch  257 / 488  Training Loss  0.0003263255348429084\n",
            "Epoch  20 Batch  258 / 488  Training Loss  0.0008155014365911484\n",
            "Epoch  20 Batch  259 / 488  Training Loss  0.00035436905454844236\n",
            "Epoch  20 Batch  260 / 488  Training Loss  0.0009909749496728182\n",
            "Epoch  20 Batch  261 / 488  Training Loss  0.0011692766565829515\n",
            "Epoch  20 Batch  262 / 488  Training Loss  0.003970061428844929\n",
            "Epoch  20 Batch  263 / 488  Training Loss  0.0005654726992361248\n",
            "Epoch  20 Batch  264 / 488  Training Loss  0.000458654627436772\n",
            "Epoch  20 Batch  265 / 488  Training Loss  0.0009687139536254108\n",
            "Epoch  20 Batch  266 / 488  Training Loss  0.0005223930929787457\n",
            "Epoch  20 Batch  267 / 488  Training Loss  0.0006880863220430911\n",
            "Epoch  20 Batch  268 / 488  Training Loss  0.0005099082482047379\n",
            "Epoch  20 Batch  269 / 488  Training Loss  0.000669735309202224\n",
            "Epoch  20 Batch  270 / 488  Training Loss  0.0008355475729331374\n",
            "Epoch  20 Batch  271 / 488  Training Loss  0.00045460392720997334\n",
            "Epoch  20 Batch  272 / 488  Training Loss  0.0005530783091671765\n",
            "Epoch  20 Batch  273 / 488  Training Loss  0.001325557241216302\n",
            "Epoch  20 Batch  274 / 488  Training Loss  0.0006045104237273335\n",
            "Epoch  20 Batch  275 / 488  Training Loss  0.0006334007484838367\n",
            "Epoch  20 Batch  276 / 488  Training Loss  0.006708197295665741\n",
            "Epoch  20 Batch  277 / 488  Training Loss  0.0003600965137593448\n",
            "Epoch  20 Batch  278 / 488  Training Loss  0.000858887389767915\n",
            "Epoch  20 Batch  279 / 488  Training Loss  0.0007545422413386405\n",
            "Epoch  20 Batch  280 / 488  Training Loss  0.0004421869234647602\n",
            "Epoch  20 Batch  281 / 488  Training Loss  0.0007864769431762397\n",
            "Epoch  20 Batch  282 / 488  Training Loss  0.0005696261068806052\n",
            "Epoch  20 Batch  283 / 488  Training Loss  0.000433681852882728\n",
            "Epoch  20 Batch  284 / 488  Training Loss  0.0014557454269379377\n",
            "Epoch  20 Batch  285 / 488  Training Loss  0.0007171835750341415\n",
            "Epoch  20 Batch  286 / 488  Training Loss  0.0003327984013594687\n",
            "Epoch  20 Batch  287 / 488  Training Loss  0.0005265139043331146\n",
            "Epoch  20 Batch  288 / 488  Training Loss  0.00045959107228554785\n",
            "Epoch  20 Batch  289 / 488  Training Loss  0.000409652799135074\n",
            "Epoch  20 Batch  290 / 488  Training Loss  0.0004860623739659786\n",
            "Epoch  20 Batch  291 / 488  Training Loss  0.0005608248175121844\n",
            "Epoch  20 Batch  292 / 488  Training Loss  0.000332213327055797\n",
            "Epoch  20 Batch  293 / 488  Training Loss  0.0002507770259398967\n",
            "Epoch  20 Batch  294 / 488  Training Loss  0.0006632769946008921\n",
            "Epoch  20 Batch  295 / 488  Training Loss  0.0022528062108904123\n",
            "Epoch  20 Batch  296 / 488  Training Loss  0.0007895574672147632\n",
            "Epoch  20 Batch  297 / 488  Training Loss  0.0008268844103440642\n",
            "Epoch  20 Batch  298 / 488  Training Loss  0.0007885469240136445\n",
            "Epoch  20 Batch  299 / 488  Training Loss  0.00033884303411468863\n",
            "Epoch  20 Batch  300 / 488  Training Loss  0.0003156476595904678\n",
            "Epoch  20 Batch  301 / 488  Training Loss  0.0013983866665512323\n",
            "Epoch  20 Batch  302 / 488  Training Loss  0.0008161303703673184\n",
            "Epoch  20 Batch  303 / 488  Training Loss  0.0007064229575917125\n",
            "Epoch  20 Batch  304 / 488  Training Loss  0.0006401872960850596\n",
            "Epoch  20 Batch  305 / 488  Training Loss  0.0005286255618557334\n",
            "Epoch  20 Batch  306 / 488  Training Loss  0.0005624790792353451\n",
            "Epoch  20 Batch  307 / 488  Training Loss  0.0006024003378115594\n",
            "Epoch  20 Batch  308 / 488  Training Loss  0.0003690811572596431\n",
            "Epoch  20 Batch  309 / 488  Training Loss  0.0004017565806861967\n",
            "Epoch  20 Batch  310 / 488  Training Loss  0.0008684922941029072\n",
            "Epoch  20 Batch  311 / 488  Training Loss  0.0005213586846366525\n",
            "Epoch  20 Batch  312 / 488  Training Loss  0.00040968586108647287\n",
            "Epoch  20 Batch  313 / 488  Training Loss  0.00032271360396407545\n",
            "Epoch  20 Batch  314 / 488  Training Loss  0.000500498921610415\n",
            "Epoch  20 Batch  315 / 488  Training Loss  0.0008026747964322567\n",
            "Epoch  20 Batch  316 / 488  Training Loss  0.0002888814196921885\n",
            "Epoch  20 Batch  317 / 488  Training Loss  0.0007241092389449477\n",
            "Epoch  20 Batch  318 / 488  Training Loss  0.0006644792156293988\n",
            "Epoch  20 Batch  319 / 488  Training Loss  0.001249516848474741\n",
            "Epoch  20 Batch  320 / 488  Training Loss  0.0003404305025469512\n",
            "Epoch  20 Batch  321 / 488  Training Loss  0.00044996049837209284\n",
            "Epoch  20 Batch  322 / 488  Training Loss  0.0002851245808415115\n",
            "Epoch  20 Batch  323 / 488  Training Loss  0.004608985967934132\n",
            "Epoch  20 Batch  324 / 488  Training Loss  0.0009719712543301284\n",
            "Epoch  20 Batch  325 / 488  Training Loss  0.0010573860490694642\n",
            "Epoch  20 Batch  326 / 488  Training Loss  0.0004967708373442292\n",
            "Epoch  20 Batch  327 / 488  Training Loss  0.0005530439084395766\n",
            "Epoch  20 Batch  328 / 488  Training Loss  0.0006026671035215259\n",
            "Epoch  20 Batch  329 / 488  Training Loss  0.0002694604336284101\n",
            "Epoch  20 Batch  330 / 488  Training Loss  0.00027165943174622953\n",
            "Epoch  20 Batch  331 / 488  Training Loss  0.0007389175007119775\n",
            "Epoch  20 Batch  332 / 488  Training Loss  0.0004364271298982203\n",
            "Epoch  20 Batch  333 / 488  Training Loss  0.00042182806646451354\n",
            "Epoch  20 Batch  334 / 488  Training Loss  0.0009221284417435527\n",
            "Epoch  20 Batch  335 / 488  Training Loss  0.0004068921261932701\n",
            "Epoch  20 Batch  336 / 488  Training Loss  0.000740127987228334\n",
            "Epoch  20 Batch  337 / 488  Training Loss  0.0003635131870396435\n",
            "Epoch  20 Batch  338 / 488  Training Loss  0.0006437862757593393\n",
            "Epoch  20 Batch  339 / 488  Training Loss  0.00039246660890057683\n",
            "Epoch  20 Batch  340 / 488  Training Loss  0.0011034510098397732\n",
            "Epoch  20 Batch  341 / 488  Training Loss  0.0003277670475654304\n",
            "Epoch  20 Batch  342 / 488  Training Loss  0.0004752454406116158\n",
            "Epoch  20 Batch  343 / 488  Training Loss  0.0004936830955557525\n",
            "Epoch  20 Batch  344 / 488  Training Loss  0.0006694906624034047\n",
            "Epoch  20 Batch  345 / 488  Training Loss  0.000338590529281646\n",
            "Epoch  20 Batch  346 / 488  Training Loss  0.0005226583452895284\n",
            "Epoch  20 Batch  347 / 488  Training Loss  0.00025177968200296164\n",
            "Epoch  20 Batch  348 / 488  Training Loss  0.00041415472514927387\n",
            "Epoch  20 Batch  349 / 488  Training Loss  0.00041566131403669715\n",
            "Epoch  20 Batch  350 / 488  Training Loss  0.00048159173456951976\n",
            "Epoch  20 Batch  351 / 488  Training Loss  0.0008905690046958625\n",
            "Epoch  20 Batch  352 / 488  Training Loss  0.0004974539624527097\n",
            "Epoch  20 Batch  353 / 488  Training Loss  0.00045495014637708664\n",
            "Epoch  20 Batch  354 / 488  Training Loss  0.0005646379431709647\n",
            "Epoch  20 Batch  355 / 488  Training Loss  0.0005199692095629871\n",
            "Epoch  20 Batch  356 / 488  Training Loss  0.00042809327715076506\n",
            "Epoch  20 Batch  357 / 488  Training Loss  0.0006353940698318183\n",
            "Epoch  20 Batch  358 / 488  Training Loss  0.001926099299453199\n",
            "Epoch  20 Batch  359 / 488  Training Loss  0.00042408666922710836\n",
            "Epoch  20 Batch  360 / 488  Training Loss  0.008279062807559967\n",
            "Epoch  20 Batch  361 / 488  Training Loss  0.0009726605494506657\n",
            "Epoch  20 Batch  362 / 488  Training Loss  0.00042974119423888624\n",
            "Epoch  20 Batch  363 / 488  Training Loss  0.0007121832459233701\n",
            "Epoch  20 Batch  364 / 488  Training Loss  0.0012825897429138422\n",
            "Epoch  20 Batch  365 / 488  Training Loss  0.0003747436567209661\n",
            "Epoch  20 Batch  366 / 488  Training Loss  0.0015025788452476263\n",
            "Epoch  20 Batch  367 / 488  Training Loss  0.00048235460417345166\n",
            "Epoch  20 Batch  368 / 488  Training Loss  0.0008198516443371773\n",
            "Epoch  20 Batch  369 / 488  Training Loss  0.0035691808443516493\n",
            "Epoch  20 Batch  370 / 488  Training Loss  0.0013278486439958215\n",
            "Epoch  20 Batch  371 / 488  Training Loss  0.000749830505810678\n",
            "Epoch  20 Batch  372 / 488  Training Loss  0.0008327236282639205\n",
            "Epoch  20 Batch  373 / 488  Training Loss  0.00024571502581238747\n",
            "Epoch  20 Batch  374 / 488  Training Loss  0.0006694145267829299\n",
            "Epoch  20 Batch  375 / 488  Training Loss  0.0005545787280425429\n",
            "Epoch  20 Batch  376 / 488  Training Loss  0.0003373615618329495\n",
            "Epoch  20 Batch  377 / 488  Training Loss  0.0003309482126496732\n",
            "Epoch  20 Batch  378 / 488  Training Loss  0.0015527887735515833\n",
            "Epoch  20 Batch  379 / 488  Training Loss  0.0005965122836641967\n",
            "Epoch  20 Batch  380 / 488  Training Loss  0.0008017889922484756\n",
            "Epoch  20 Batch  381 / 488  Training Loss  0.0005069900071248412\n",
            "Epoch  20 Batch  382 / 488  Training Loss  0.0004434307920746505\n",
            "Epoch  20 Batch  383 / 488  Training Loss  0.0007893203874118626\n",
            "Epoch  20 Batch  384 / 488  Training Loss  0.0008640959858894348\n",
            "Epoch  20 Batch  385 / 488  Training Loss  0.00025057882885448635\n",
            "Epoch  20 Batch  386 / 488  Training Loss  0.00024246482644230127\n",
            "Epoch  20 Batch  387 / 488  Training Loss  0.0019125931430608034\n",
            "Epoch  20 Batch  388 / 488  Training Loss  0.0005698676104657352\n",
            "Epoch  20 Batch  389 / 488  Training Loss  0.0036179828457534313\n",
            "Epoch  20 Batch  390 / 488  Training Loss  0.0005912509514018893\n",
            "Epoch  20 Batch  391 / 488  Training Loss  0.0006702046375721693\n",
            "Epoch  20 Batch  392 / 488  Training Loss  0.0012216048780828714\n",
            "Epoch  20 Batch  393 / 488  Training Loss  0.0010093679884448647\n",
            "Epoch  20 Batch  394 / 488  Training Loss  0.0006068253424018621\n",
            "Epoch  20 Batch  395 / 488  Training Loss  0.0004419180622790009\n",
            "Epoch  20 Batch  396 / 488  Training Loss  0.000454973429441452\n",
            "Epoch  20 Batch  397 / 488  Training Loss  0.0004922814550809562\n",
            "Epoch  20 Batch  398 / 488  Training Loss  0.0005522244609892368\n",
            "Epoch  20 Batch  399 / 488  Training Loss  0.00039265170926228166\n",
            "Epoch  20 Batch  400 / 488  Training Loss  0.0005088512552902102\n",
            "Epoch  20 Batch  401 / 488  Training Loss  0.00044367299415171146\n",
            "Epoch  20 Batch  402 / 488  Training Loss  0.0006883508758619428\n",
            "Epoch  20 Batch  403 / 488  Training Loss  0.0006278554210439324\n",
            "Epoch  20 Batch  404 / 488  Training Loss  0.0004635443037841469\n",
            "Epoch  20 Batch  405 / 488  Training Loss  0.0005223833722993731\n",
            "Epoch  20 Batch  406 / 488  Training Loss  0.00021280732471495867\n",
            "Epoch  20 Batch  407 / 488  Training Loss  0.00038872507866472006\n",
            "Epoch  20 Batch  408 / 488  Training Loss  0.0014079924440011382\n",
            "Epoch  20 Batch  409 / 488  Training Loss  0.0005122915608808398\n",
            "Epoch  20 Batch  410 / 488  Training Loss  0.0006909090443514287\n",
            "Epoch  20 Batch  411 / 488  Training Loss  0.0003708881849888712\n",
            "Epoch  20 Batch  412 / 488  Training Loss  0.002144699450582266\n",
            "Epoch  20 Batch  413 / 488  Training Loss  0.0002667693770490587\n",
            "Epoch  20 Batch  414 / 488  Training Loss  0.0011576628312468529\n",
            "Epoch  20 Batch  415 / 488  Training Loss  0.001931641949340701\n",
            "Epoch  20 Batch  416 / 488  Training Loss  0.0005637761205434799\n",
            "Epoch  20 Batch  417 / 488  Training Loss  0.0007076652254909277\n",
            "Epoch  20 Batch  418 / 488  Training Loss  0.0006982894847169518\n",
            "Epoch  20 Batch  419 / 488  Training Loss  0.0005270271794870496\n",
            "Epoch  20 Batch  420 / 488  Training Loss  0.000408828811487183\n",
            "Epoch  20 Batch  421 / 488  Training Loss  0.0007681536953896284\n",
            "Epoch  20 Batch  422 / 488  Training Loss  0.0004971168818883598\n",
            "Epoch  20 Batch  423 / 488  Training Loss  0.0004413888673298061\n",
            "Epoch  20 Batch  424 / 488  Training Loss  0.0006226954865269363\n",
            "Epoch  20 Batch  425 / 488  Training Loss  0.00041638268157839775\n",
            "Epoch  20 Batch  426 / 488  Training Loss  0.00043665882549248636\n",
            "Epoch  20 Batch  427 / 488  Training Loss  0.0006231205188669264\n",
            "Epoch  20 Batch  428 / 488  Training Loss  0.0002450016909278929\n",
            "Epoch  20 Batch  429 / 488  Training Loss  0.00024212116841226816\n",
            "Epoch  20 Batch  430 / 488  Training Loss  0.00033560232259333134\n",
            "Epoch  20 Batch  431 / 488  Training Loss  0.0003458819119259715\n",
            "Epoch  20 Batch  432 / 488  Training Loss  0.0008515099762007594\n",
            "Epoch  20 Batch  433 / 488  Training Loss  0.0004033725708723068\n",
            "Epoch  20 Batch  434 / 488  Training Loss  0.0006491082021966577\n",
            "Epoch  20 Batch  435 / 488  Training Loss  0.0003560129553079605\n",
            "Epoch  20 Batch  436 / 488  Training Loss  0.00040597180486656725\n",
            "Epoch  20 Batch  437 / 488  Training Loss  0.0013976293848827481\n",
            "Epoch  20 Batch  438 / 488  Training Loss  0.000916142133064568\n",
            "Epoch  20 Batch  439 / 488  Training Loss  0.0005048097809776664\n",
            "Epoch  20 Batch  440 / 488  Training Loss  0.0003799145924858749\n",
            "Epoch  20 Batch  441 / 488  Training Loss  0.00033292226726189256\n",
            "Epoch  20 Batch  442 / 488  Training Loss  0.0010008852696046233\n",
            "Epoch  20 Batch  443 / 488  Training Loss  0.001212962088175118\n",
            "Epoch  20 Batch  444 / 488  Training Loss  0.0006829501362517476\n",
            "Epoch  20 Batch  445 / 488  Training Loss  0.0006495967390947044\n",
            "Epoch  20 Batch  446 / 488  Training Loss  0.00034134971792809665\n",
            "Epoch  20 Batch  447 / 488  Training Loss  0.0012778269592672586\n",
            "Epoch  20 Batch  448 / 488  Training Loss  0.0004969933070242405\n",
            "Epoch  20 Batch  449 / 488  Training Loss  0.0005549193010665476\n",
            "Epoch  20 Batch  450 / 488  Training Loss  0.0002871575707104057\n",
            "Epoch  20 Batch  451 / 488  Training Loss  0.005764409434050322\n",
            "Epoch  20 Batch  452 / 488  Training Loss  0.0018830305198207498\n",
            "Epoch  20 Batch  453 / 488  Training Loss  0.0012231196742504835\n",
            "Epoch  20 Batch  454 / 488  Training Loss  0.009066776372492313\n",
            "Epoch  20 Batch  455 / 488  Training Loss  0.0006653913878835738\n",
            "Epoch  20 Batch  456 / 488  Training Loss  0.0007850812980905175\n",
            "Epoch  20 Batch  457 / 488  Training Loss  0.000539258704520762\n",
            "Epoch  20 Batch  458 / 488  Training Loss  0.0010557578643783927\n",
            "Epoch  20 Batch  459 / 488  Training Loss  0.00039045498124323785\n",
            "Epoch  20 Batch  460 / 488  Training Loss  0.0010128766298294067\n",
            "Epoch  20 Batch  461 / 488  Training Loss  0.00047275485121645033\n",
            "Epoch  20 Batch  462 / 488  Training Loss  0.0006256431806832552\n",
            "Epoch  20 Batch  463 / 488  Training Loss  0.0024416865780949593\n",
            "Epoch  20 Batch  464 / 488  Training Loss  0.0008325421949848533\n",
            "Epoch  20 Batch  465 / 488  Training Loss  0.0003169445844832808\n",
            "Epoch  20 Batch  466 / 488  Training Loss  0.0004950995789840817\n",
            "Epoch  20 Batch  467 / 488  Training Loss  0.00048725632950663567\n",
            "Epoch  20 Batch  468 / 488  Training Loss  0.0006017135456204414\n",
            "Epoch  20 Batch  469 / 488  Training Loss  0.0012901285663247108\n",
            "Epoch  20 Batch  470 / 488  Training Loss  0.0011944274883717299\n",
            "Epoch  20 Batch  471 / 488  Training Loss  0.0006806884193792939\n",
            "Epoch  20 Batch  472 / 488  Training Loss  0.0007333942339755595\n",
            "Epoch  20 Batch  473 / 488  Training Loss  0.0012151200789958239\n",
            "Epoch  20 Batch  474 / 488  Training Loss  0.00017330741684418172\n",
            "Epoch  20 Batch  475 / 488  Training Loss  0.0004383002524264157\n",
            "Epoch  20 Batch  476 / 488  Training Loss  0.0006143558421172202\n",
            "Epoch  20 Batch  477 / 488  Training Loss  0.00032099970849230886\n",
            "Epoch  20 Batch  478 / 488  Training Loss  0.0004342941683717072\n",
            "Epoch  20 Batch  479 / 488  Training Loss  0.0006257906788960099\n",
            "Epoch  20 Batch  480 / 488  Training Loss  0.0004242044815327972\n",
            "Epoch  20 Batch  481 / 488  Training Loss  0.000608414295129478\n",
            "Epoch  20 Batch  482 / 488  Training Loss  0.009877326898276806\n",
            "Epoch  20 Batch  483 / 488  Training Loss  0.0003311139880679548\n",
            "Epoch  20 Batch  484 / 488  Training Loss  0.004076700657606125\n",
            "Epoch  20 Batch  485 / 488  Training Loss  0.0008885847637429833\n",
            "Epoch  20 Batch  486 / 488  Training Loss  0.0011916857911273837\n",
            "Epoch  20 Batch  487 / 488  Training Loss  0.0004593045450747013\n",
            "  21    |    -    |   0.001021   | 46.476064\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 21\n",
            "Epoch  21 Batch  0 / 488  Training Loss  0.0003168455441482365\n",
            "Epoch  21 Batch  1 / 488  Training Loss  0.0003950814716517925\n",
            "Epoch  21 Batch  2 / 488  Training Loss  0.0006008862401358783\n",
            "Epoch  21 Batch  3 / 488  Training Loss  0.00024530867813155055\n",
            "Epoch  21 Batch  4 / 488  Training Loss  0.00032040971564128995\n",
            "Epoch  21 Batch  5 / 488  Training Loss  0.000277042476227507\n",
            "Epoch  21 Batch  6 / 488  Training Loss  0.0004207806778140366\n",
            "Epoch  21 Batch  7 / 488  Training Loss  0.0003571864799596369\n",
            "Epoch  21 Batch  8 / 488  Training Loss  0.0002465140132699162\n",
            "Epoch  21 Batch  9 / 488  Training Loss  0.00045661060721613467\n",
            "Epoch  21 Batch  10 / 488  Training Loss  0.00040059295133687556\n",
            "Epoch  21 Batch  11 / 488  Training Loss  0.000468608399387449\n",
            "Epoch  21 Batch  12 / 488  Training Loss  0.0005446897121146321\n",
            "Epoch  21 Batch  13 / 488  Training Loss  0.0006189772975631058\n",
            "Epoch  21 Batch  14 / 488  Training Loss  0.00035901664523407817\n",
            "Epoch  21 Batch  15 / 488  Training Loss  0.00023830868303775787\n",
            "Epoch  21 Batch  16 / 488  Training Loss  0.00042192847467958927\n",
            "Epoch  21 Batch  17 / 488  Training Loss  0.0014746231026947498\n",
            "Epoch  21 Batch  18 / 488  Training Loss  0.0024074651300907135\n",
            "Epoch  21 Batch  19 / 488  Training Loss  0.0002916505909524858\n",
            "Epoch  21 Batch  20 / 488  Training Loss  0.0012079088483005762\n",
            "Epoch  21 Batch  21 / 488  Training Loss  0.0003362326533533633\n",
            "Epoch  21 Batch  22 / 488  Training Loss  0.000270050106337294\n",
            "Epoch  21 Batch  23 / 488  Training Loss  0.0004156620125286281\n",
            "Epoch  21 Batch  24 / 488  Training Loss  0.0003663253446575254\n",
            "Epoch  21 Batch  25 / 488  Training Loss  0.00027154068811796606\n",
            "Epoch  21 Batch  26 / 488  Training Loss  0.0003642885712906718\n",
            "Epoch  21 Batch  27 / 488  Training Loss  0.0005655362037941813\n",
            "Epoch  21 Batch  28 / 488  Training Loss  0.00025439815362915397\n",
            "Epoch  21 Batch  29 / 488  Training Loss  0.00038936661439947784\n",
            "Epoch  21 Batch  30 / 488  Training Loss  0.00029376213205978274\n",
            "Epoch  21 Batch  31 / 488  Training Loss  0.0021606632508337498\n",
            "Epoch  21 Batch  32 / 488  Training Loss  0.0003912098181899637\n",
            "Epoch  21 Batch  33 / 488  Training Loss  0.00019797112327069044\n",
            "Epoch  21 Batch  34 / 488  Training Loss  0.000792726525105536\n",
            "Epoch  21 Batch  35 / 488  Training Loss  0.0015766400611028075\n",
            "Epoch  21 Batch  36 / 488  Training Loss  0.0004663293366320431\n",
            "Epoch  21 Batch  37 / 488  Training Loss  0.0020619125571101904\n",
            "Epoch  21 Batch  38 / 488  Training Loss  0.0007953247986733913\n",
            "Epoch  21 Batch  39 / 488  Training Loss  0.00041835667798295617\n",
            "Epoch  21 Batch  40 / 488  Training Loss  0.00043471489334478974\n",
            "Epoch  21 Batch  41 / 488  Training Loss  0.00056328100617975\n",
            "Epoch  21 Batch  42 / 488  Training Loss  0.0008992655202746391\n",
            "Epoch  21 Batch  43 / 488  Training Loss  0.00017207543714903295\n",
            "Epoch  21 Batch  44 / 488  Training Loss  0.0003833359805867076\n",
            "Epoch  21 Batch  45 / 488  Training Loss  0.0003109669778496027\n",
            "Epoch  21 Batch  46 / 488  Training Loss  0.00019197093206457794\n",
            "Epoch  21 Batch  47 / 488  Training Loss  0.00047625103616155684\n",
            "Epoch  21 Batch  48 / 488  Training Loss  0.0006119923782534897\n",
            "Epoch  21 Batch  49 / 488  Training Loss  0.0015629592817276716\n",
            "Epoch  21 Batch  50 / 488  Training Loss  0.0004293998354114592\n",
            "Epoch  21 Batch  51 / 488  Training Loss  0.0003368557954672724\n",
            "Epoch  21 Batch  52 / 488  Training Loss  0.005380193702876568\n",
            "Epoch  21 Batch  53 / 488  Training Loss  0.0005310171982273459\n",
            "Epoch  21 Batch  54 / 488  Training Loss  0.0003976364678237587\n",
            "Epoch  21 Batch  55 / 488  Training Loss  0.00040316005470231175\n",
            "Epoch  21 Batch  56 / 488  Training Loss  0.0008976209792308509\n",
            "Epoch  21 Batch  57 / 488  Training Loss  0.0003174774465151131\n",
            "Epoch  21 Batch  58 / 488  Training Loss  0.00023391337890643626\n",
            "Epoch  21 Batch  59 / 488  Training Loss  0.005007545463740826\n",
            "Epoch  21 Batch  60 / 488  Training Loss  0.0011030805762857199\n",
            "Epoch  21 Batch  61 / 488  Training Loss  0.00037747781607322395\n",
            "Epoch  21 Batch  62 / 488  Training Loss  0.00026139241526834667\n",
            "Epoch  21 Batch  63 / 488  Training Loss  0.00030926079489290714\n",
            "Epoch  21 Batch  64 / 488  Training Loss  0.00030494460952468216\n",
            "Epoch  21 Batch  65 / 488  Training Loss  0.0005678845336660743\n",
            "Epoch  21 Batch  66 / 488  Training Loss  0.0004274906241334975\n",
            "Epoch  21 Batch  67 / 488  Training Loss  0.0003722792607732117\n",
            "Epoch  21 Batch  68 / 488  Training Loss  0.004506385885179043\n",
            "Epoch  21 Batch  69 / 488  Training Loss  0.00039242292405106127\n",
            "Epoch  21 Batch  70 / 488  Training Loss  0.0002708831161726266\n",
            "Epoch  21 Batch  71 / 488  Training Loss  0.0005341102951206267\n",
            "Epoch  21 Batch  72 / 488  Training Loss  0.00016139206127263606\n",
            "Epoch  21 Batch  73 / 488  Training Loss  0.0005018275114707649\n",
            "Epoch  21 Batch  74 / 488  Training Loss  0.0002828235155902803\n",
            "Epoch  21 Batch  75 / 488  Training Loss  0.0003720071108546108\n",
            "Epoch  21 Batch  76 / 488  Training Loss  0.00030094460817053914\n",
            "Epoch  21 Batch  77 / 488  Training Loss  0.00044399569742381573\n",
            "Epoch  21 Batch  78 / 488  Training Loss  0.0004712473601102829\n",
            "Epoch  21 Batch  79 / 488  Training Loss  0.0015705093974247575\n",
            "Epoch  21 Batch  80 / 488  Training Loss  0.007305268198251724\n",
            "Epoch  21 Batch  81 / 488  Training Loss  0.00044948849244974554\n",
            "Epoch  21 Batch  82 / 488  Training Loss  0.0005684721400029957\n",
            "Epoch  21 Batch  83 / 488  Training Loss  0.0004695378593169153\n",
            "Epoch  21 Batch  84 / 488  Training Loss  0.00031513141584582627\n",
            "Epoch  21 Batch  85 / 488  Training Loss  0.0035261274315416813\n",
            "Epoch  21 Batch  86 / 488  Training Loss  0.0031938408501446247\n",
            "Epoch  21 Batch  87 / 488  Training Loss  0.0023715330753475428\n",
            "Epoch  21 Batch  88 / 488  Training Loss  0.0008780102361924946\n",
            "Epoch  21 Batch  89 / 488  Training Loss  0.0005802428931929171\n",
            "Epoch  21 Batch  90 / 488  Training Loss  0.0005595317343249917\n",
            "Epoch  21 Batch  91 / 488  Training Loss  0.0006283604307100177\n",
            "Epoch  21 Batch  92 / 488  Training Loss  0.0003620378265623003\n",
            "Epoch  21 Batch  93 / 488  Training Loss  0.00035186432069167495\n",
            "Epoch  21 Batch  94 / 488  Training Loss  0.0004487187252379954\n",
            "Epoch  21 Batch  95 / 488  Training Loss  0.0006928440416231751\n",
            "Epoch  21 Batch  96 / 488  Training Loss  0.0008125545573420823\n",
            "Epoch  21 Batch  97 / 488  Training Loss  0.00014257901057135314\n",
            "Epoch  21 Batch  98 / 488  Training Loss  0.00027388575836084783\n",
            "Epoch  21 Batch  99 / 488  Training Loss  0.000571014650631696\n",
            "Epoch  21 Batch  100 / 488  Training Loss  0.0007389233214780688\n",
            "Epoch  21 Batch  101 / 488  Training Loss  0.0005800561048090458\n",
            "Epoch  21 Batch  102 / 488  Training Loss  0.0004723012971226126\n",
            "Epoch  21 Batch  103 / 488  Training Loss  0.0005566364852711558\n",
            "Epoch  21 Batch  104 / 488  Training Loss  0.0004900592612102628\n",
            "Epoch  21 Batch  105 / 488  Training Loss  0.0002106107131112367\n",
            "Epoch  21 Batch  106 / 488  Training Loss  0.0003110524849034846\n",
            "Epoch  21 Batch  107 / 488  Training Loss  0.00041876337490975857\n",
            "Epoch  21 Batch  108 / 488  Training Loss  0.004540253430604935\n",
            "Epoch  21 Batch  109 / 488  Training Loss  0.00221078097820282\n",
            "Epoch  21 Batch  110 / 488  Training Loss  0.0005488133756443858\n",
            "Epoch  21 Batch  111 / 488  Training Loss  0.000350080372299999\n",
            "Epoch  21 Batch  112 / 488  Training Loss  0.0007181546534411609\n",
            "Epoch  21 Batch  113 / 488  Training Loss  0.00041270171641372144\n",
            "Epoch  21 Batch  114 / 488  Training Loss  0.00040166956023313105\n",
            "Epoch  21 Batch  115 / 488  Training Loss  0.00019363887258805335\n",
            "Epoch  21 Batch  116 / 488  Training Loss  0.0008769502164795995\n",
            "Epoch  21 Batch  117 / 488  Training Loss  0.0002486041630618274\n",
            "Epoch  21 Batch  118 / 488  Training Loss  0.00032082779216580093\n",
            "Epoch  21 Batch  119 / 488  Training Loss  0.0006560897454619408\n",
            "Epoch  21 Batch  120 / 488  Training Loss  0.0016344243194907904\n",
            "Epoch  21 Batch  121 / 488  Training Loss  0.001031412510201335\n",
            "Epoch  21 Batch  122 / 488  Training Loss  0.00034904078347608447\n",
            "Epoch  21 Batch  123 / 488  Training Loss  0.00035552718327380717\n",
            "Epoch  21 Batch  124 / 488  Training Loss  0.00040685004205442965\n",
            "Epoch  21 Batch  125 / 488  Training Loss  0.00029997731326147914\n",
            "Epoch  21 Batch  126 / 488  Training Loss  0.0002653912524692714\n",
            "Epoch  21 Batch  127 / 488  Training Loss  0.00046066538197919726\n",
            "Epoch  21 Batch  128 / 488  Training Loss  0.0007589273154735565\n",
            "Epoch  21 Batch  129 / 488  Training Loss  0.00037877290742471814\n",
            "Epoch  21 Batch  130 / 488  Training Loss  0.00021471362560987473\n",
            "Epoch  21 Batch  131 / 488  Training Loss  0.00023446658451575786\n",
            "Epoch  21 Batch  132 / 488  Training Loss  0.0002723313227761537\n",
            "Epoch  21 Batch  133 / 488  Training Loss  0.00017858066712506115\n",
            "Epoch  21 Batch  134 / 488  Training Loss  0.00040837665437720716\n",
            "Epoch  21 Batch  135 / 488  Training Loss  0.006857165601104498\n",
            "Epoch  21 Batch  136 / 488  Training Loss  0.00033387792063876987\n",
            "Epoch  21 Batch  137 / 488  Training Loss  0.000160517796757631\n",
            "Epoch  21 Batch  138 / 488  Training Loss  0.0018160829786211252\n",
            "Epoch  21 Batch  139 / 488  Training Loss  0.0006555411964654922\n",
            "Epoch  21 Batch  140 / 488  Training Loss  0.0005898234667256474\n",
            "Epoch  21 Batch  141 / 488  Training Loss  0.00021616248704958707\n",
            "Epoch  21 Batch  142 / 488  Training Loss  0.00019272734061814845\n",
            "Epoch  21 Batch  143 / 488  Training Loss  0.0007150762248784304\n",
            "Epoch  21 Batch  144 / 488  Training Loss  0.00017319308244623244\n",
            "Epoch  21 Batch  145 / 488  Training Loss  0.0006208776030689478\n",
            "Epoch  21 Batch  146 / 488  Training Loss  0.0003834224189631641\n",
            "Epoch  21 Batch  147 / 488  Training Loss  0.0003083258052356541\n",
            "Epoch  21 Batch  148 / 488  Training Loss  0.0005657257279381156\n",
            "Epoch  21 Batch  149 / 488  Training Loss  0.000365363375749439\n",
            "Epoch  21 Batch  150 / 488  Training Loss  0.000320454768370837\n",
            "Epoch  21 Batch  151 / 488  Training Loss  0.000402958074118942\n",
            "Epoch  21 Batch  152 / 488  Training Loss  0.0012212328147143126\n",
            "Epoch  21 Batch  153 / 488  Training Loss  0.000576476682908833\n",
            "Epoch  21 Batch  154 / 488  Training Loss  0.0003304670681245625\n",
            "Epoch  21 Batch  155 / 488  Training Loss  0.00027268979465588927\n",
            "Epoch  21 Batch  156 / 488  Training Loss  0.00191522017121315\n",
            "Epoch  21 Batch  157 / 488  Training Loss  0.0006030207732692361\n",
            "Epoch  21 Batch  158 / 488  Training Loss  0.009230789728462696\n",
            "Epoch  21 Batch  159 / 488  Training Loss  0.0008848216384649277\n",
            "Epoch  21 Batch  160 / 488  Training Loss  0.0005636560963466763\n",
            "Epoch  21 Batch  161 / 488  Training Loss  0.0003155761514790356\n",
            "Epoch  21 Batch  162 / 488  Training Loss  0.002076459815725684\n",
            "Epoch  21 Batch  163 / 488  Training Loss  0.00032972550252452493\n",
            "Epoch  21 Batch  164 / 488  Training Loss  0.0008857825887389481\n",
            "Epoch  21 Batch  165 / 488  Training Loss  0.00034727700403891504\n",
            "Epoch  21 Batch  166 / 488  Training Loss  0.0003781338164117187\n",
            "Epoch  21 Batch  167 / 488  Training Loss  0.0012291418388485909\n",
            "Epoch  21 Batch  168 / 488  Training Loss  0.0009425670141354203\n",
            "Epoch  21 Batch  169 / 488  Training Loss  0.00035232683876529336\n",
            "Epoch  21 Batch  170 / 488  Training Loss  0.00032331497641280293\n",
            "Epoch  21 Batch  171 / 488  Training Loss  0.0011674814159050584\n",
            "Epoch  21 Batch  172 / 488  Training Loss  0.00041668611811473966\n",
            "Epoch  21 Batch  173 / 488  Training Loss  0.0002526832395233214\n",
            "Epoch  21 Batch  174 / 488  Training Loss  0.00025313725927844644\n",
            "Epoch  21 Batch  175 / 488  Training Loss  0.00045760924695059657\n",
            "Epoch  21 Batch  176 / 488  Training Loss  0.00033931195503100753\n",
            "Epoch  21 Batch  177 / 488  Training Loss  0.0002678160963114351\n",
            "Epoch  21 Batch  178 / 488  Training Loss  0.0012571545084938407\n",
            "Epoch  21 Batch  179 / 488  Training Loss  0.0008285517687909305\n",
            "Epoch  21 Batch  180 / 488  Training Loss  0.0011940643889829516\n",
            "Epoch  21 Batch  181 / 488  Training Loss  0.00032315735006704926\n",
            "Epoch  21 Batch  182 / 488  Training Loss  0.0002066227316390723\n",
            "Epoch  21 Batch  183 / 488  Training Loss  0.0003113783022854477\n",
            "Epoch  21 Batch  184 / 488  Training Loss  0.0003264482947997749\n",
            "Epoch  21 Batch  185 / 488  Training Loss  0.0009650051360949874\n",
            "Epoch  21 Batch  186 / 488  Training Loss  0.00033337375498376787\n",
            "Epoch  21 Batch  187 / 488  Training Loss  0.0007161914836615324\n",
            "Epoch  21 Batch  188 / 488  Training Loss  0.001115277293138206\n",
            "Epoch  21 Batch  189 / 488  Training Loss  0.000474139756988734\n",
            "Epoch  21 Batch  190 / 488  Training Loss  0.0006726601277478039\n",
            "Epoch  21 Batch  191 / 488  Training Loss  0.0005029937019571662\n",
            "Epoch  21 Batch  192 / 488  Training Loss  0.00022951567370910197\n",
            "Epoch  21 Batch  193 / 488  Training Loss  0.0003533756243996322\n",
            "Epoch  21 Batch  194 / 488  Training Loss  0.0011087561724707484\n",
            "Epoch  21 Batch  195 / 488  Training Loss  0.0003065274213440716\n",
            "Epoch  21 Batch  196 / 488  Training Loss  0.0001852885470725596\n",
            "Epoch  21 Batch  197 / 488  Training Loss  0.0005921146948821843\n",
            "Epoch  21 Batch  198 / 488  Training Loss  0.0005080938572064042\n",
            "Epoch  21 Batch  199 / 488  Training Loss  0.00033071404322981834\n",
            "Epoch  21 Batch  200 / 488  Training Loss  0.00032816577004268765\n",
            "Epoch  21 Batch  201 / 488  Training Loss  0.0008276410517282784\n",
            "Epoch  21 Batch  202 / 488  Training Loss  0.0002063744468614459\n",
            "Epoch  21 Batch  203 / 488  Training Loss  0.0003633162996266037\n",
            "Epoch  21 Batch  204 / 488  Training Loss  0.0004752947425004095\n",
            "Epoch  21 Batch  205 / 488  Training Loss  0.00022447746596299112\n",
            "Epoch  21 Batch  206 / 488  Training Loss  0.00027526760823093355\n",
            "Epoch  21 Batch  207 / 488  Training Loss  0.0002967347390949726\n",
            "Epoch  21 Batch  208 / 488  Training Loss  0.0008867952856235206\n",
            "Epoch  21 Batch  209 / 488  Training Loss  0.0024158807937055826\n",
            "Epoch  21 Batch  210 / 488  Training Loss  0.0006528018275275826\n",
            "Epoch  21 Batch  211 / 488  Training Loss  0.00019542837981134653\n",
            "Epoch  21 Batch  212 / 488  Training Loss  0.00027256886824034154\n",
            "Epoch  21 Batch  213 / 488  Training Loss  0.0007721331785432994\n",
            "Epoch  21 Batch  214 / 488  Training Loss  0.0006989173125475645\n",
            "Epoch  21 Batch  215 / 488  Training Loss  0.0009227307746186852\n",
            "Epoch  21 Batch  216 / 488  Training Loss  0.0003437465347815305\n",
            "Epoch  21 Batch  217 / 488  Training Loss  0.00046548558748327196\n",
            "Epoch  21 Batch  218 / 488  Training Loss  0.0005713285645470023\n",
            "Epoch  21 Batch  219 / 488  Training Loss  0.0003482018946669996\n",
            "Epoch  21 Batch  220 / 488  Training Loss  0.0002973710943479091\n",
            "Epoch  21 Batch  221 / 488  Training Loss  0.002083847764879465\n",
            "Epoch  21 Batch  222 / 488  Training Loss  0.0003140309127047658\n",
            "Epoch  21 Batch  223 / 488  Training Loss  0.0006416307878680527\n",
            "Epoch  21 Batch  224 / 488  Training Loss  0.0008675141143612564\n",
            "Epoch  21 Batch  225 / 488  Training Loss  0.00048573408275842667\n",
            "Epoch  21 Batch  226 / 488  Training Loss  0.00045280769700184464\n",
            "Epoch  21 Batch  227 / 488  Training Loss  0.0006872800295241177\n",
            "Epoch  21 Batch  228 / 488  Training Loss  0.0004166116414126009\n",
            "Epoch  21 Batch  229 / 488  Training Loss  0.0003544439096003771\n",
            "Epoch  21 Batch  230 / 488  Training Loss  0.00048664785572327673\n",
            "Epoch  21 Batch  231 / 488  Training Loss  0.0003572567948140204\n",
            "Epoch  21 Batch  232 / 488  Training Loss  0.000648366054520011\n",
            "Epoch  21 Batch  233 / 488  Training Loss  0.00031920630135573447\n",
            "Epoch  21 Batch  234 / 488  Training Loss  0.000469899270683527\n",
            "Epoch  21 Batch  235 / 488  Training Loss  0.00023141110432334244\n",
            "Epoch  21 Batch  236 / 488  Training Loss  0.00026742700720205903\n",
            "Epoch  21 Batch  237 / 488  Training Loss  0.0012188720284029841\n",
            "Epoch  21 Batch  238 / 488  Training Loss  0.00036512096994556487\n",
            "Epoch  21 Batch  239 / 488  Training Loss  0.00027223690995015204\n",
            "Epoch  21 Batch  240 / 488  Training Loss  0.0003260798694100231\n",
            "Epoch  21 Batch  241 / 488  Training Loss  0.0002760779461823404\n",
            "Epoch  21 Batch  242 / 488  Training Loss  0.0003946653159800917\n",
            "Epoch  21 Batch  243 / 488  Training Loss  0.0002444908022880554\n",
            "Epoch  21 Batch  244 / 488  Training Loss  0.0002930109912995249\n",
            "Epoch  21 Batch  245 / 488  Training Loss  0.0002696773153729737\n",
            "Epoch  21 Batch  246 / 488  Training Loss  0.00034559908090159297\n",
            "Epoch  21 Batch  247 / 488  Training Loss  0.0002486616722308099\n",
            "Epoch  21 Batch  248 / 488  Training Loss  0.00026739455643109977\n",
            "Epoch  21 Batch  249 / 488  Training Loss  0.00025161178200505674\n",
            "Epoch  21 Batch  250 / 488  Training Loss  0.00036171317333355546\n",
            "Epoch  21 Batch  251 / 488  Training Loss  0.004461704287678003\n",
            "Epoch  21 Batch  252 / 488  Training Loss  0.0004785476194228977\n",
            "Epoch  21 Batch  253 / 488  Training Loss  0.0005265967920422554\n",
            "Epoch  21 Batch  254 / 488  Training Loss  0.00093761773314327\n",
            "Epoch  21 Batch  255 / 488  Training Loss  0.0003213065501768142\n",
            "Epoch  21 Batch  256 / 488  Training Loss  0.0005189820076338947\n",
            "Epoch  21 Batch  257 / 488  Training Loss  0.0005078724352642894\n",
            "Epoch  21 Batch  258 / 488  Training Loss  0.00034898280864581466\n",
            "Epoch  21 Batch  259 / 488  Training Loss  0.0003116031875833869\n",
            "Epoch  21 Batch  260 / 488  Training Loss  0.00032098550582304597\n",
            "Epoch  21 Batch  261 / 488  Training Loss  0.001107582007534802\n",
            "Epoch  21 Batch  262 / 488  Training Loss  0.0005409918376244605\n",
            "Epoch  21 Batch  263 / 488  Training Loss  0.000454336783150211\n",
            "Epoch  21 Batch  264 / 488  Training Loss  0.00036753309541381896\n",
            "Epoch  21 Batch  265 / 488  Training Loss  0.0010080214124172926\n",
            "Epoch  21 Batch  266 / 488  Training Loss  0.00029717039433307946\n",
            "Epoch  21 Batch  267 / 488  Training Loss  0.00039938767440617085\n",
            "Epoch  21 Batch  268 / 488  Training Loss  0.00036956489202566445\n",
            "Epoch  21 Batch  269 / 488  Training Loss  0.0007329405634663999\n",
            "Epoch  21 Batch  270 / 488  Training Loss  0.0002927806635852903\n",
            "Epoch  21 Batch  271 / 488  Training Loss  0.00024332951579708606\n",
            "Epoch  21 Batch  272 / 488  Training Loss  0.00039706481038592756\n",
            "Epoch  21 Batch  273 / 488  Training Loss  0.0002663169871084392\n",
            "Epoch  21 Batch  274 / 488  Training Loss  0.0009218457853421569\n",
            "Epoch  21 Batch  275 / 488  Training Loss  0.0003541805490385741\n",
            "Epoch  21 Batch  276 / 488  Training Loss  0.0014851202722638845\n",
            "Epoch  21 Batch  277 / 488  Training Loss  0.0007268404588103294\n",
            "Epoch  21 Batch  278 / 488  Training Loss  0.00046676816418766975\n",
            "Epoch  21 Batch  279 / 488  Training Loss  0.00043487208313308656\n",
            "Epoch  21 Batch  280 / 488  Training Loss  0.0003080219612456858\n",
            "Epoch  21 Batch  281 / 488  Training Loss  0.0005260225152596831\n",
            "Epoch  21 Batch  282 / 488  Training Loss  0.0005740771885029972\n",
            "Epoch  21 Batch  283 / 488  Training Loss  0.0004125589912291616\n",
            "Epoch  21 Batch  284 / 488  Training Loss  0.0006430507637560368\n",
            "Epoch  21 Batch  285 / 488  Training Loss  0.001464255852624774\n",
            "Epoch  21 Batch  286 / 488  Training Loss  0.00037123795482330024\n",
            "Epoch  21 Batch  287 / 488  Training Loss  0.00042624835623428226\n",
            "Epoch  21 Batch  288 / 488  Training Loss  0.000368826265912503\n",
            "Epoch  21 Batch  289 / 488  Training Loss  0.0003217918274458498\n",
            "Epoch  21 Batch  290 / 488  Training Loss  0.00048079402768053114\n",
            "Epoch  21 Batch  291 / 488  Training Loss  0.00026175595121458173\n",
            "Epoch  21 Batch  292 / 488  Training Loss  0.0004619669634848833\n",
            "Epoch  21 Batch  293 / 488  Training Loss  0.00046672075404785573\n",
            "Epoch  21 Batch  294 / 488  Training Loss  0.0006834286032244563\n",
            "Epoch  21 Batch  295 / 488  Training Loss  0.0004638918617274612\n",
            "Epoch  21 Batch  296 / 488  Training Loss  0.0003823134466074407\n",
            "Epoch  21 Batch  297 / 488  Training Loss  0.0058557214215397835\n",
            "Epoch  21 Batch  298 / 488  Training Loss  0.0006362648564390838\n",
            "Epoch  21 Batch  299 / 488  Training Loss  0.0004926156834699214\n",
            "Epoch  21 Batch  300 / 488  Training Loss  0.0012260026996955276\n",
            "Epoch  21 Batch  301 / 488  Training Loss  0.00030681496718898416\n",
            "Epoch  21 Batch  302 / 488  Training Loss  0.0001627116434974596\n",
            "Epoch  21 Batch  303 / 488  Training Loss  0.0005721730412915349\n",
            "Epoch  21 Batch  304 / 488  Training Loss  0.0006778040551580489\n",
            "Epoch  21 Batch  305 / 488  Training Loss  0.0005848624277859926\n",
            "Epoch  21 Batch  306 / 488  Training Loss  0.00027299608336761594\n",
            "Epoch  21 Batch  307 / 488  Training Loss  0.0004316320118959993\n",
            "Epoch  21 Batch  308 / 488  Training Loss  0.0009793841745704412\n",
            "Epoch  21 Batch  309 / 488  Training Loss  0.0002389677392784506\n",
            "Epoch  21 Batch  310 / 488  Training Loss  0.00019507725664880127\n",
            "Epoch  21 Batch  311 / 488  Training Loss  0.0005711855483241379\n",
            "Epoch  21 Batch  312 / 488  Training Loss  0.0003729752206709236\n",
            "Epoch  21 Batch  313 / 488  Training Loss  0.000266374961938709\n",
            "Epoch  21 Batch  314 / 488  Training Loss  0.0004062770458403975\n",
            "Epoch  21 Batch  315 / 488  Training Loss  0.0003570963745005429\n",
            "Epoch  21 Batch  316 / 488  Training Loss  0.00035042400122620165\n",
            "Epoch  21 Batch  317 / 488  Training Loss  0.00023366397363133729\n",
            "Epoch  21 Batch  318 / 488  Training Loss  0.0004234797670505941\n",
            "Epoch  21 Batch  319 / 488  Training Loss  0.0007617563824169338\n",
            "Epoch  21 Batch  320 / 488  Training Loss  0.0006036091363057494\n",
            "Epoch  21 Batch  321 / 488  Training Loss  0.00023259346198756248\n",
            "Epoch  21 Batch  322 / 488  Training Loss  0.0005371749866753817\n",
            "Epoch  21 Batch  323 / 488  Training Loss  0.0018410456832498312\n",
            "Epoch  21 Batch  324 / 488  Training Loss  0.00032484886469319463\n",
            "Epoch  21 Batch  325 / 488  Training Loss  0.00034681675606407225\n",
            "Epoch  21 Batch  326 / 488  Training Loss  0.00025023616035468876\n",
            "Epoch  21 Batch  327 / 488  Training Loss  0.0004262326692696661\n",
            "Epoch  21 Batch  328 / 488  Training Loss  0.0005811768351122737\n",
            "Epoch  21 Batch  329 / 488  Training Loss  0.0005487902089953423\n",
            "Epoch  21 Batch  330 / 488  Training Loss  0.0009456855477765203\n",
            "Epoch  21 Batch  331 / 488  Training Loss  0.00017850806761998683\n",
            "Epoch  21 Batch  332 / 488  Training Loss  0.0004974292241968215\n",
            "Epoch  21 Batch  333 / 488  Training Loss  0.00043598198681138456\n",
            "Epoch  21 Batch  334 / 488  Training Loss  0.00023337927996180952\n",
            "Epoch  21 Batch  335 / 488  Training Loss  0.0003185998066328466\n",
            "Epoch  21 Batch  336 / 488  Training Loss  0.00027117831632494926\n",
            "Epoch  21 Batch  337 / 488  Training Loss  0.0003066168283112347\n",
            "Epoch  21 Batch  338 / 488  Training Loss  0.0003288990701548755\n",
            "Epoch  21 Batch  339 / 488  Training Loss  0.00044150586472824216\n",
            "Epoch  21 Batch  340 / 488  Training Loss  0.0009554916759952903\n",
            "Epoch  21 Batch  341 / 488  Training Loss  0.0008520272676832974\n",
            "Epoch  21 Batch  342 / 488  Training Loss  0.00032114016357809305\n",
            "Epoch  21 Batch  343 / 488  Training Loss  0.0003071498067583889\n",
            "Epoch  21 Batch  344 / 488  Training Loss  0.0003328587918076664\n",
            "Epoch  21 Batch  345 / 488  Training Loss  0.0005071539781056345\n",
            "Epoch  21 Batch  346 / 488  Training Loss  0.000483344221720472\n",
            "Epoch  21 Batch  347 / 488  Training Loss  0.0002887271693907678\n",
            "Epoch  21 Batch  348 / 488  Training Loss  0.00033802672987803817\n",
            "Epoch  21 Batch  349 / 488  Training Loss  0.00031351676443591714\n",
            "Epoch  21 Batch  350 / 488  Training Loss  0.0003228912246413529\n",
            "Epoch  21 Batch  351 / 488  Training Loss  0.0005467640003189445\n",
            "Epoch  21 Batch  352 / 488  Training Loss  0.0005287769017741084\n",
            "Epoch  21 Batch  353 / 488  Training Loss  0.0003121255722362548\n",
            "Epoch  21 Batch  354 / 488  Training Loss  0.00027698330814018846\n",
            "Epoch  21 Batch  355 / 488  Training Loss  0.00021631165873259306\n",
            "Epoch  21 Batch  356 / 488  Training Loss  0.0005989978672005236\n",
            "Epoch  21 Batch  357 / 488  Training Loss  0.0006474588881246746\n",
            "Epoch  21 Batch  358 / 488  Training Loss  0.00038726365892216563\n",
            "Epoch  21 Batch  359 / 488  Training Loss  0.00021549523808062077\n",
            "Epoch  21 Batch  360 / 488  Training Loss  0.0003378575202077627\n",
            "Epoch  21 Batch  361 / 488  Training Loss  0.00021400113473646343\n",
            "Epoch  21 Batch  362 / 488  Training Loss  0.0003634870226960629\n",
            "Epoch  21 Batch  363 / 488  Training Loss  0.0003112846170552075\n",
            "Epoch  21 Batch  364 / 488  Training Loss  0.004377431236207485\n",
            "Epoch  21 Batch  365 / 488  Training Loss  0.0003390272904653102\n",
            "Epoch  21 Batch  366 / 488  Training Loss  0.0005107064498588443\n",
            "Epoch  21 Batch  367 / 488  Training Loss  0.0004896129248663783\n",
            "Epoch  21 Batch  368 / 488  Training Loss  0.0009858714183792472\n",
            "Epoch  21 Batch  369 / 488  Training Loss  0.00035100762033835053\n",
            "Epoch  21 Batch  370 / 488  Training Loss  0.0001881740608951077\n",
            "Epoch  21 Batch  371 / 488  Training Loss  0.00028702738927677274\n",
            "Epoch  21 Batch  372 / 488  Training Loss  0.0007454058504663408\n",
            "Epoch  21 Batch  373 / 488  Training Loss  0.0007774102850817144\n",
            "Epoch  21 Batch  374 / 488  Training Loss  0.0004776839050464332\n",
            "Epoch  21 Batch  375 / 488  Training Loss  0.00030310990405268967\n",
            "Epoch  21 Batch  376 / 488  Training Loss  0.0005331943975761533\n",
            "Epoch  21 Batch  377 / 488  Training Loss  0.000939018907956779\n",
            "Epoch  21 Batch  378 / 488  Training Loss  0.0003687043208628893\n",
            "Epoch  21 Batch  379 / 488  Training Loss  0.0006103997002355754\n",
            "Epoch  21 Batch  380 / 488  Training Loss  0.0003251823363825679\n",
            "Epoch  21 Batch  381 / 488  Training Loss  0.0004952571471221745\n",
            "Epoch  21 Batch  382 / 488  Training Loss  0.00027178687741979957\n",
            "Epoch  21 Batch  383 / 488  Training Loss  0.00248510017991066\n",
            "Epoch  21 Batch  384 / 488  Training Loss  0.00046442775055766106\n",
            "Epoch  21 Batch  385 / 488  Training Loss  0.0007385273347608745\n",
            "Epoch  21 Batch  386 / 488  Training Loss  0.0005452324403449893\n",
            "Epoch  21 Batch  387 / 488  Training Loss  0.00031750131165608764\n",
            "Epoch  21 Batch  388 / 488  Training Loss  0.0005931998020969331\n",
            "Epoch  21 Batch  389 / 488  Training Loss  0.0003196463221684098\n",
            "Epoch  21 Batch  390 / 488  Training Loss  0.0031719908583909273\n",
            "Epoch  21 Batch  391 / 488  Training Loss  0.0006069266819395125\n",
            "Epoch  21 Batch  392 / 488  Training Loss  0.0007606965955346823\n",
            "Epoch  21 Batch  393 / 488  Training Loss  0.0006001401925459504\n",
            "Epoch  21 Batch  394 / 488  Training Loss  0.00041514067561365664\n",
            "Epoch  21 Batch  395 / 488  Training Loss  0.00021957303397357464\n",
            "Epoch  21 Batch  396 / 488  Training Loss  0.0005380123038776219\n",
            "Epoch  21 Batch  397 / 488  Training Loss  0.00021932064555585384\n",
            "Epoch  21 Batch  398 / 488  Training Loss  0.0004967311397194862\n",
            "Epoch  21 Batch  399 / 488  Training Loss  0.0005142633453942835\n",
            "Epoch  21 Batch  400 / 488  Training Loss  0.00039515490061603487\n",
            "Epoch  21 Batch  401 / 488  Training Loss  0.00025315250968560576\n",
            "Epoch  21 Batch  402 / 488  Training Loss  0.0012154404539614916\n",
            "Epoch  21 Batch  403 / 488  Training Loss  0.00037397921551018953\n",
            "Epoch  21 Batch  404 / 488  Training Loss  0.002044419990852475\n",
            "Epoch  21 Batch  405 / 488  Training Loss  0.000453802349511534\n",
            "Epoch  21 Batch  406 / 488  Training Loss  0.00042746905819512904\n",
            "Epoch  21 Batch  407 / 488  Training Loss  0.00034358137054368854\n",
            "Epoch  21 Batch  408 / 488  Training Loss  0.0003646861878223717\n",
            "Epoch  21 Batch  409 / 488  Training Loss  0.0002601913292892277\n",
            "Epoch  21 Batch  410 / 488  Training Loss  0.0005401076632551849\n",
            "Epoch  21 Batch  411 / 488  Training Loss  0.00028759572887793183\n",
            "Epoch  21 Batch  412 / 488  Training Loss  0.0004975729389116168\n",
            "Epoch  21 Batch  413 / 488  Training Loss  0.0004302184097468853\n",
            "Epoch  21 Batch  414 / 488  Training Loss  0.00019007237278856337\n",
            "Epoch  21 Batch  415 / 488  Training Loss  0.0003651414590422064\n",
            "Epoch  21 Batch  416 / 488  Training Loss  0.001009022118523717\n",
            "Epoch  21 Batch  417 / 488  Training Loss  0.0006901824381202459\n",
            "Epoch  21 Batch  418 / 488  Training Loss  0.00027885730378329754\n",
            "Epoch  21 Batch  419 / 488  Training Loss  0.00029241404263302684\n",
            "Epoch  21 Batch  420 / 488  Training Loss  0.00038609583862125874\n",
            "Epoch  21 Batch  421 / 488  Training Loss  0.00029724877094849944\n",
            "Epoch  21 Batch  422 / 488  Training Loss  0.0004483716038521379\n",
            "Epoch  21 Batch  423 / 488  Training Loss  0.0006207365659065545\n",
            "Epoch  21 Batch  424 / 488  Training Loss  0.00018998685118276626\n",
            "Epoch  21 Batch  425 / 488  Training Loss  0.00026645074831321836\n",
            "Epoch  21 Batch  426 / 488  Training Loss  0.0006934143602848053\n",
            "Epoch  21 Batch  427 / 488  Training Loss  0.00022504855587612838\n",
            "Epoch  21 Batch  428 / 488  Training Loss  0.00033220485784113407\n",
            "Epoch  21 Batch  429 / 488  Training Loss  0.00033165630884468555\n",
            "Epoch  21 Batch  430 / 488  Training Loss  0.0005278819007799029\n",
            "Epoch  21 Batch  431 / 488  Training Loss  0.0007001172052696347\n",
            "Epoch  21 Batch  432 / 488  Training Loss  0.0002639245940372348\n",
            "Epoch  21 Batch  433 / 488  Training Loss  0.00040531312697567046\n",
            "Epoch  21 Batch  434 / 488  Training Loss  0.0003061816387344152\n",
            "Epoch  21 Batch  435 / 488  Training Loss  0.0006685956614091992\n",
            "Epoch  21 Batch  436 / 488  Training Loss  0.0005054730572737753\n",
            "Epoch  21 Batch  437 / 488  Training Loss  0.0013923423830419779\n",
            "Epoch  21 Batch  438 / 488  Training Loss  0.000497698609251529\n",
            "Epoch  21 Batch  439 / 488  Training Loss  0.00069344014627859\n",
            "Epoch  21 Batch  440 / 488  Training Loss  0.0002129860658897087\n",
            "Epoch  21 Batch  441 / 488  Training Loss  0.00038357250741682947\n",
            "Epoch  21 Batch  442 / 488  Training Loss  0.00045335773029364645\n",
            "Epoch  21 Batch  443 / 488  Training Loss  0.000262154673691839\n",
            "Epoch  21 Batch  444 / 488  Training Loss  0.0005489917821250856\n",
            "Epoch  21 Batch  445 / 488  Training Loss  0.0003939898742828518\n",
            "Epoch  21 Batch  446 / 488  Training Loss  0.00040168603300116956\n",
            "Epoch  21 Batch  447 / 488  Training Loss  0.0002790109138004482\n",
            "Epoch  21 Batch  448 / 488  Training Loss  0.0003367224708199501\n",
            "Epoch  21 Batch  449 / 488  Training Loss  0.0004678214027080685\n",
            "Epoch  21 Batch  450 / 488  Training Loss  0.00033489865018054843\n",
            "Epoch  21 Batch  451 / 488  Training Loss  0.0003956838045269251\n",
            "Epoch  21 Batch  452 / 488  Training Loss  0.00022119264758657664\n",
            "Epoch  21 Batch  453 / 488  Training Loss  0.0003574838920030743\n",
            "Epoch  21 Batch  454 / 488  Training Loss  0.0003306875587441027\n",
            "Epoch  21 Batch  455 / 488  Training Loss  0.00035309838131070137\n",
            "Epoch  21 Batch  456 / 488  Training Loss  0.0003194834280293435\n",
            "Epoch  21 Batch  457 / 488  Training Loss  0.0005216915160417557\n",
            "Epoch  21 Batch  458 / 488  Training Loss  0.00029799429466947913\n",
            "Epoch  21 Batch  459 / 488  Training Loss  0.0003624043893069029\n",
            "Epoch  21 Batch  460 / 488  Training Loss  0.0003790976479649544\n",
            "Epoch  21 Batch  461 / 488  Training Loss  0.0017400536453351378\n",
            "Epoch  21 Batch  462 / 488  Training Loss  0.002205067314207554\n",
            "Epoch  21 Batch  463 / 488  Training Loss  0.0005180801963433623\n",
            "Epoch  21 Batch  464 / 488  Training Loss  0.0003830460482276976\n",
            "Epoch  21 Batch  465 / 488  Training Loss  0.00039439782267436385\n",
            "Epoch  21 Batch  466 / 488  Training Loss  0.00024677100009284914\n",
            "Epoch  21 Batch  467 / 488  Training Loss  0.00024206758826039732\n",
            "Epoch  21 Batch  468 / 488  Training Loss  0.0005668256198987365\n",
            "Epoch  21 Batch  469 / 488  Training Loss  0.0006735564675182104\n",
            "Epoch  21 Batch  470 / 488  Training Loss  0.0002368125133216381\n",
            "Epoch  21 Batch  471 / 488  Training Loss  0.0010249618208035827\n",
            "Epoch  21 Batch  472 / 488  Training Loss  0.00022391653328668326\n",
            "Epoch  21 Batch  473 / 488  Training Loss  0.00034594425233080983\n",
            "Epoch  21 Batch  474 / 488  Training Loss  0.00035990908509120345\n",
            "Epoch  21 Batch  475 / 488  Training Loss  0.000466071447590366\n",
            "Epoch  21 Batch  476 / 488  Training Loss  0.00038244991446845233\n",
            "Epoch  21 Batch  477 / 488  Training Loss  0.0003101782058365643\n",
            "Epoch  21 Batch  478 / 488  Training Loss  0.00023177421826403588\n",
            "Epoch  21 Batch  479 / 488  Training Loss  0.0006100781029090285\n",
            "Epoch  21 Batch  480 / 488  Training Loss  0.0002682822523638606\n",
            "Epoch  21 Batch  481 / 488  Training Loss  0.0005006183637306094\n",
            "Epoch  21 Batch  482 / 488  Training Loss  0.0004037621838506311\n",
            "Epoch  21 Batch  483 / 488  Training Loss  0.00022723320580553263\n",
            "Epoch  21 Batch  484 / 488  Training Loss  0.0002544235612731427\n",
            "Epoch  21 Batch  485 / 488  Training Loss  0.00045759137719869614\n",
            "Epoch  21 Batch  486 / 488  Training Loss  0.00044156069634482265\n",
            "Epoch  21 Batch  487 / 488  Training Loss  0.0004143793194089085\n",
            "  22    |    -    |   0.000649   | 46.620124\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 22\n",
            "Epoch  22 Batch  0 / 488  Training Loss  0.00038487627170979977\n",
            "Epoch  22 Batch  1 / 488  Training Loss  0.00024110302911140025\n",
            "Epoch  22 Batch  2 / 488  Training Loss  0.0001837715390138328\n",
            "Epoch  22 Batch  3 / 488  Training Loss  0.00023623967717867345\n",
            "Epoch  22 Batch  4 / 488  Training Loss  0.0006555391591973603\n",
            "Epoch  22 Batch  5 / 488  Training Loss  0.00022598329815082252\n",
            "Epoch  22 Batch  6 / 488  Training Loss  0.0002652950934134424\n",
            "Epoch  22 Batch  7 / 488  Training Loss  0.00018814335635397583\n",
            "Epoch  22 Batch  8 / 488  Training Loss  0.0003769143659155816\n",
            "Epoch  22 Batch  9 / 488  Training Loss  0.00018182501662522554\n",
            "Epoch  22 Batch  10 / 488  Training Loss  0.00023604412854183465\n",
            "Epoch  22 Batch  11 / 488  Training Loss  0.0002212370454799384\n",
            "Epoch  22 Batch  12 / 488  Training Loss  0.00034533676807768643\n",
            "Epoch  22 Batch  13 / 488  Training Loss  0.0001345440250588581\n",
            "Epoch  22 Batch  14 / 488  Training Loss  0.0001200978658744134\n",
            "Epoch  22 Batch  15 / 488  Training Loss  0.00017530066543258727\n",
            "Epoch  22 Batch  16 / 488  Training Loss  0.00021755415946245193\n",
            "Epoch  22 Batch  17 / 488  Training Loss  0.00014751868729945272\n",
            "Epoch  22 Batch  18 / 488  Training Loss  0.0001902360818348825\n",
            "Epoch  22 Batch  19 / 488  Training Loss  0.00022030991385690868\n",
            "Epoch  22 Batch  20 / 488  Training Loss  0.000253742968197912\n",
            "Epoch  22 Batch  21 / 488  Training Loss  0.0003310838364996016\n",
            "Epoch  22 Batch  22 / 488  Training Loss  0.00017325076623819768\n",
            "Epoch  22 Batch  23 / 488  Training Loss  0.0001730462972773239\n",
            "Epoch  22 Batch  24 / 488  Training Loss  0.00030932071967981756\n",
            "Epoch  22 Batch  25 / 488  Training Loss  0.0003697586362250149\n",
            "Epoch  22 Batch  26 / 488  Training Loss  0.0001374003040837124\n",
            "Epoch  22 Batch  27 / 488  Training Loss  0.00013373667025007308\n",
            "Epoch  22 Batch  28 / 488  Training Loss  0.00020786517416127026\n",
            "Epoch  22 Batch  29 / 488  Training Loss  0.0001756850688252598\n",
            "Epoch  22 Batch  30 / 488  Training Loss  0.00020411920559126884\n",
            "Epoch  22 Batch  31 / 488  Training Loss  0.00019430203246884048\n",
            "Epoch  22 Batch  32 / 488  Training Loss  0.0003088655066676438\n",
            "Epoch  22 Batch  33 / 488  Training Loss  0.0002657481818459928\n",
            "Epoch  22 Batch  34 / 488  Training Loss  0.0002548222546465695\n",
            "Epoch  22 Batch  35 / 488  Training Loss  0.00016059671179391444\n",
            "Epoch  22 Batch  36 / 488  Training Loss  0.000264187779976055\n",
            "Epoch  22 Batch  37 / 488  Training Loss  0.00032591650960966945\n",
            "Epoch  22 Batch  38 / 488  Training Loss  0.00020254173432476819\n",
            "Epoch  22 Batch  39 / 488  Training Loss  0.00029416155302897096\n",
            "Epoch  22 Batch  40 / 488  Training Loss  0.00021683625527657568\n",
            "Epoch  22 Batch  41 / 488  Training Loss  0.0006947554647922516\n",
            "Epoch  22 Batch  42 / 488  Training Loss  0.00028193945763632655\n",
            "Epoch  22 Batch  43 / 488  Training Loss  0.00017622389714233577\n",
            "Epoch  22 Batch  44 / 488  Training Loss  0.00018814670329447836\n",
            "Epoch  22 Batch  45 / 488  Training Loss  0.0003828825138043612\n",
            "Epoch  22 Batch  46 / 488  Training Loss  0.0002498943358659744\n",
            "Epoch  22 Batch  47 / 488  Training Loss  0.00032891827868297696\n",
            "Epoch  22 Batch  48 / 488  Training Loss  0.00023807608522474766\n",
            "Epoch  22 Batch  49 / 488  Training Loss  0.00021898809063713998\n",
            "Epoch  22 Batch  50 / 488  Training Loss  0.00027235064771957695\n",
            "Epoch  22 Batch  51 / 488  Training Loss  0.00016318060806952417\n",
            "Epoch  22 Batch  52 / 488  Training Loss  0.0002881135733332485\n",
            "Epoch  22 Batch  53 / 488  Training Loss  0.00046574213774874806\n",
            "Epoch  22 Batch  54 / 488  Training Loss  0.00017507320444565266\n",
            "Epoch  22 Batch  55 / 488  Training Loss  0.00016402092296630144\n",
            "Epoch  22 Batch  56 / 488  Training Loss  0.00016592269821558148\n",
            "Epoch  22 Batch  57 / 488  Training Loss  0.00029129566974006593\n",
            "Epoch  22 Batch  58 / 488  Training Loss  0.00021674475283361971\n",
            "Epoch  22 Batch  59 / 488  Training Loss  0.00045785060501657426\n",
            "Epoch  22 Batch  60 / 488  Training Loss  0.00022246611479204148\n",
            "Epoch  22 Batch  61 / 488  Training Loss  0.00014174652460496873\n",
            "Epoch  22 Batch  62 / 488  Training Loss  0.0001854664005804807\n",
            "Epoch  22 Batch  63 / 488  Training Loss  0.00025200675008818507\n",
            "Epoch  22 Batch  64 / 488  Training Loss  0.00020555224909912795\n",
            "Epoch  22 Batch  65 / 488  Training Loss  0.00020316037989687175\n",
            "Epoch  22 Batch  66 / 488  Training Loss  0.00015511176025029272\n",
            "Epoch  22 Batch  67 / 488  Training Loss  0.00036600412568077445\n",
            "Epoch  22 Batch  68 / 488  Training Loss  0.00011904514394700527\n",
            "Epoch  22 Batch  69 / 488  Training Loss  0.0002136531547876075\n",
            "Epoch  22 Batch  70 / 488  Training Loss  0.00020120092085562646\n",
            "Epoch  22 Batch  71 / 488  Training Loss  0.00020236856653355062\n",
            "Epoch  22 Batch  72 / 488  Training Loss  0.00029365692171268165\n",
            "Epoch  22 Batch  73 / 488  Training Loss  0.00017967671738006175\n",
            "Epoch  22 Batch  74 / 488  Training Loss  0.0002071310591418296\n",
            "Epoch  22 Batch  75 / 488  Training Loss  0.000230464109336026\n",
            "Epoch  22 Batch  76 / 488  Training Loss  0.0002082469582092017\n",
            "Epoch  22 Batch  77 / 488  Training Loss  0.00018396413361188024\n",
            "Epoch  22 Batch  78 / 488  Training Loss  0.00017439045768696815\n",
            "Epoch  22 Batch  79 / 488  Training Loss  0.00018826135783456266\n",
            "Epoch  22 Batch  80 / 488  Training Loss  0.00018970758537761867\n",
            "Epoch  22 Batch  81 / 488  Training Loss  0.0001287653431063518\n",
            "Epoch  22 Batch  82 / 488  Training Loss  0.0004578275256790221\n",
            "Epoch  22 Batch  83 / 488  Training Loss  0.00013823916378896683\n",
            "Epoch  22 Batch  84 / 488  Training Loss  0.0003008977510035038\n",
            "Epoch  22 Batch  85 / 488  Training Loss  0.00013467666576616466\n",
            "Epoch  22 Batch  86 / 488  Training Loss  0.00037138577317819\n",
            "Epoch  22 Batch  87 / 488  Training Loss  0.0013299217680469155\n",
            "Epoch  22 Batch  88 / 488  Training Loss  0.003922698087990284\n",
            "Epoch  22 Batch  89 / 488  Training Loss  0.0014596731634810567\n",
            "Epoch  22 Batch  90 / 488  Training Loss  0.00025437559816055\n",
            "Epoch  22 Batch  91 / 488  Training Loss  0.0003150069387629628\n",
            "Epoch  22 Batch  92 / 488  Training Loss  0.0002966828178614378\n",
            "Epoch  22 Batch  93 / 488  Training Loss  0.0001966446143342182\n",
            "Epoch  22 Batch  94 / 488  Training Loss  0.00015513092512264848\n",
            "Epoch  22 Batch  95 / 488  Training Loss  0.0003101654874626547\n",
            "Epoch  22 Batch  96 / 488  Training Loss  0.00039798818761482835\n",
            "Epoch  22 Batch  97 / 488  Training Loss  0.00028315442614257336\n",
            "Epoch  22 Batch  98 / 488  Training Loss  0.00023115538351703435\n",
            "Epoch  22 Batch  99 / 488  Training Loss  0.0001736869162414223\n",
            "Epoch  22 Batch  100 / 488  Training Loss  0.00024612920242361724\n",
            "Epoch  22 Batch  101 / 488  Training Loss  0.00013141172530595213\n",
            "Epoch  22 Batch  102 / 488  Training Loss  0.0001838922908063978\n",
            "Epoch  22 Batch  103 / 488  Training Loss  0.00017120208940468729\n",
            "Epoch  22 Batch  104 / 488  Training Loss  0.00022827628708910197\n",
            "Epoch  22 Batch  105 / 488  Training Loss  0.0013944345992058516\n",
            "Epoch  22 Batch  106 / 488  Training Loss  0.001848088577389717\n",
            "Epoch  22 Batch  107 / 488  Training Loss  0.00032536746584810317\n",
            "Epoch  22 Batch  108 / 488  Training Loss  0.00012037665874231607\n",
            "Epoch  22 Batch  109 / 488  Training Loss  0.0002093805669574067\n",
            "Epoch  22 Batch  110 / 488  Training Loss  0.00020699195738416165\n",
            "Epoch  22 Batch  111 / 488  Training Loss  0.00019593215256463736\n",
            "Epoch  22 Batch  112 / 488  Training Loss  0.00016020073962863535\n",
            "Epoch  22 Batch  113 / 488  Training Loss  0.00018198697944171727\n",
            "Epoch  22 Batch  114 / 488  Training Loss  0.00021542783360928297\n",
            "Epoch  22 Batch  115 / 488  Training Loss  0.00019205603166483343\n",
            "Epoch  22 Batch  116 / 488  Training Loss  0.000247999036218971\n",
            "Epoch  22 Batch  117 / 488  Training Loss  0.000219840178033337\n",
            "Epoch  22 Batch  118 / 488  Training Loss  0.00011359914060449228\n",
            "Epoch  22 Batch  119 / 488  Training Loss  0.0002835025661624968\n",
            "Epoch  22 Batch  120 / 488  Training Loss  0.00012277986388653517\n",
            "Epoch  22 Batch  121 / 488  Training Loss  0.0004145376442465931\n",
            "Epoch  22 Batch  122 / 488  Training Loss  0.00021022814325988293\n",
            "Epoch  22 Batch  123 / 488  Training Loss  0.0002177178830606863\n",
            "Epoch  22 Batch  124 / 488  Training Loss  0.0001598624949110672\n",
            "Epoch  22 Batch  125 / 488  Training Loss  0.00031940481858327985\n",
            "Epoch  22 Batch  126 / 488  Training Loss  0.00021124814520590007\n",
            "Epoch  22 Batch  127 / 488  Training Loss  0.00042907375609502196\n",
            "Epoch  22 Batch  128 / 488  Training Loss  0.00027698473422788084\n",
            "Epoch  22 Batch  129 / 488  Training Loss  0.00022229687601793557\n",
            "Epoch  22 Batch  130 / 488  Training Loss  0.0001674476807238534\n",
            "Epoch  22 Batch  131 / 488  Training Loss  0.00021125837520230561\n",
            "Epoch  22 Batch  132 / 488  Training Loss  0.00018373967031948268\n",
            "Epoch  22 Batch  133 / 488  Training Loss  0.0002621564199216664\n",
            "Epoch  22 Batch  134 / 488  Training Loss  0.00018884160090237856\n",
            "Epoch  22 Batch  135 / 488  Training Loss  0.00022896142036188394\n",
            "Epoch  22 Batch  136 / 488  Training Loss  0.0002537929394748062\n",
            "Epoch  22 Batch  137 / 488  Training Loss  0.0002309575502295047\n",
            "Epoch  22 Batch  138 / 488  Training Loss  0.000219499968807213\n",
            "Epoch  22 Batch  139 / 488  Training Loss  0.0003618194896262139\n",
            "Epoch  22 Batch  140 / 488  Training Loss  0.0001615215151105076\n",
            "Epoch  22 Batch  141 / 488  Training Loss  0.00018517128773964942\n",
            "Epoch  22 Batch  142 / 488  Training Loss  0.00019882494234479964\n",
            "Epoch  22 Batch  143 / 488  Training Loss  0.0003635351895354688\n",
            "Epoch  22 Batch  144 / 488  Training Loss  0.00016764484462328255\n",
            "Epoch  22 Batch  145 / 488  Training Loss  0.0002215876738773659\n",
            "Epoch  22 Batch  146 / 488  Training Loss  0.00016539546777494252\n",
            "Epoch  22 Batch  147 / 488  Training Loss  0.00016764069732744247\n",
            "Epoch  22 Batch  148 / 488  Training Loss  0.00025307340547442436\n",
            "Epoch  22 Batch  149 / 488  Training Loss  0.0001375754945911467\n",
            "Epoch  22 Batch  150 / 488  Training Loss  0.00019367007189430296\n",
            "Epoch  22 Batch  151 / 488  Training Loss  0.00012651286670006812\n",
            "Epoch  22 Batch  152 / 488  Training Loss  0.0002167263301089406\n",
            "Epoch  22 Batch  153 / 488  Training Loss  0.00018025982717517763\n",
            "Epoch  22 Batch  154 / 488  Training Loss  0.00016728145419619977\n",
            "Epoch  22 Batch  155 / 488  Training Loss  0.0003044594486709684\n",
            "Epoch  22 Batch  156 / 488  Training Loss  0.0002850265591405332\n",
            "Epoch  22 Batch  157 / 488  Training Loss  0.00014997427933849394\n",
            "Epoch  22 Batch  158 / 488  Training Loss  0.00020855022012256086\n",
            "Epoch  22 Batch  159 / 488  Training Loss  0.00015559517487417907\n",
            "Epoch  22 Batch  160 / 488  Training Loss  0.00014953571371734142\n",
            "Epoch  22 Batch  161 / 488  Training Loss  0.0001862768258433789\n",
            "Epoch  22 Batch  162 / 488  Training Loss  0.00025476692826487124\n",
            "Epoch  22 Batch  163 / 488  Training Loss  0.00023174029774963856\n",
            "Epoch  22 Batch  164 / 488  Training Loss  0.0002963326987810433\n",
            "Epoch  22 Batch  165 / 488  Training Loss  0.00022045025252737105\n",
            "Epoch  22 Batch  166 / 488  Training Loss  0.00022172628086991608\n",
            "Epoch  22 Batch  167 / 488  Training Loss  0.0002513714134693146\n",
            "Epoch  22 Batch  168 / 488  Training Loss  0.00017005018889904022\n",
            "Epoch  22 Batch  169 / 488  Training Loss  0.00017951297922991216\n",
            "Epoch  22 Batch  170 / 488  Training Loss  0.0003792803909163922\n",
            "Epoch  22 Batch  171 / 488  Training Loss  0.00016861871699802577\n",
            "Epoch  22 Batch  172 / 488  Training Loss  0.00017603013839107007\n",
            "Epoch  22 Batch  173 / 488  Training Loss  0.00018945573538076133\n",
            "Epoch  22 Batch  174 / 488  Training Loss  0.00018889822240453213\n",
            "Epoch  22 Batch  175 / 488  Training Loss  0.00019758423150051385\n",
            "Epoch  22 Batch  176 / 488  Training Loss  0.00020980369299650192\n",
            "Epoch  22 Batch  177 / 488  Training Loss  0.0001919221831485629\n",
            "Epoch  22 Batch  178 / 488  Training Loss  0.000159975970746018\n",
            "Epoch  22 Batch  179 / 488  Training Loss  0.0001134261692641303\n",
            "Epoch  22 Batch  180 / 488  Training Loss  8.431595051661134e-05\n",
            "Epoch  22 Batch  181 / 488  Training Loss  0.0004895786987617612\n",
            "Epoch  22 Batch  182 / 488  Training Loss  0.0002128569467458874\n",
            "Epoch  22 Batch  183 / 488  Training Loss  0.000185371856787242\n",
            "Epoch  22 Batch  184 / 488  Training Loss  0.00014806160470470786\n",
            "Epoch  22 Batch  185 / 488  Training Loss  0.00010295677930116653\n",
            "Epoch  22 Batch  186 / 488  Training Loss  0.0002584137546364218\n",
            "Epoch  22 Batch  187 / 488  Training Loss  0.0001666126336203888\n",
            "Epoch  22 Batch  188 / 488  Training Loss  0.00021043869492132217\n",
            "Epoch  22 Batch  189 / 488  Training Loss  0.00018443797307554632\n",
            "Epoch  22 Batch  190 / 488  Training Loss  0.00020997859246563166\n",
            "Epoch  22 Batch  191 / 488  Training Loss  0.00013540161307901144\n",
            "Epoch  22 Batch  192 / 488  Training Loss  0.00018557027215138078\n",
            "Epoch  22 Batch  193 / 488  Training Loss  0.00020650107762776315\n",
            "Epoch  22 Batch  194 / 488  Training Loss  0.00021095247939229012\n",
            "Epoch  22 Batch  195 / 488  Training Loss  0.00017319119069725275\n",
            "Epoch  22 Batch  196 / 488  Training Loss  0.00021612732962239534\n",
            "Epoch  22 Batch  197 / 488  Training Loss  0.0002209768572356552\n",
            "Epoch  22 Batch  198 / 488  Training Loss  0.00013080301869194955\n",
            "Epoch  22 Batch  199 / 488  Training Loss  0.00025932356948032975\n",
            "Epoch  22 Batch  200 / 488  Training Loss  0.0004005574155598879\n",
            "Epoch  22 Batch  201 / 488  Training Loss  0.00017521643894724548\n",
            "Epoch  22 Batch  202 / 488  Training Loss  0.0001602476986590773\n",
            "Epoch  22 Batch  203 / 488  Training Loss  0.0001589185412740335\n",
            "Epoch  22 Batch  204 / 488  Training Loss  0.00012691557640209794\n",
            "Epoch  22 Batch  205 / 488  Training Loss  0.00021269533317536116\n",
            "Epoch  22 Batch  206 / 488  Training Loss  0.00014320896298158914\n",
            "Epoch  22 Batch  207 / 488  Training Loss  0.00018884487508330494\n",
            "Epoch  22 Batch  208 / 488  Training Loss  0.00028731152997352183\n",
            "Epoch  22 Batch  209 / 488  Training Loss  0.0001314324326813221\n",
            "Epoch  22 Batch  210 / 488  Training Loss  0.00011849732982227579\n",
            "Epoch  22 Batch  211 / 488  Training Loss  0.00015812661149539053\n",
            "Epoch  22 Batch  212 / 488  Training Loss  0.00015378643001895398\n",
            "Epoch  22 Batch  213 / 488  Training Loss  0.0003191744035575539\n",
            "Epoch  22 Batch  214 / 488  Training Loss  0.0001676167012192309\n",
            "Epoch  22 Batch  215 / 488  Training Loss  0.00017781031783670187\n",
            "Epoch  22 Batch  216 / 488  Training Loss  0.00014426694542635232\n",
            "Epoch  22 Batch  217 / 488  Training Loss  0.007959133014082909\n",
            "Epoch  22 Batch  218 / 488  Training Loss  0.0003308322047814727\n",
            "Epoch  22 Batch  219 / 488  Training Loss  0.00018022824951913208\n",
            "Epoch  22 Batch  220 / 488  Training Loss  0.0005034449277445674\n",
            "Epoch  22 Batch  221 / 488  Training Loss  0.0002204371412517503\n",
            "Epoch  22 Batch  222 / 488  Training Loss  0.00021249645214993507\n",
            "Epoch  22 Batch  223 / 488  Training Loss  0.00030673801666125655\n",
            "Epoch  22 Batch  224 / 488  Training Loss  0.0001841321209212765\n",
            "Epoch  22 Batch  225 / 488  Training Loss  0.00019667542073875666\n",
            "Epoch  22 Batch  226 / 488  Training Loss  0.0004719136341009289\n",
            "Epoch  22 Batch  227 / 488  Training Loss  0.00013952607696410269\n",
            "Epoch  22 Batch  228 / 488  Training Loss  0.00040324049768969417\n",
            "Epoch  22 Batch  229 / 488  Training Loss  0.00020358958863653243\n",
            "Epoch  22 Batch  230 / 488  Training Loss  0.00022745032038073987\n",
            "Epoch  22 Batch  231 / 488  Training Loss  0.0005329846171662211\n",
            "Epoch  22 Batch  232 / 488  Training Loss  0.00029134060605429113\n",
            "Epoch  22 Batch  233 / 488  Training Loss  0.00021919971914030612\n",
            "Epoch  22 Batch  234 / 488  Training Loss  0.0002842106041498482\n",
            "Epoch  22 Batch  235 / 488  Training Loss  0.000233330691116862\n",
            "Epoch  22 Batch  236 / 488  Training Loss  0.00015874960809014738\n",
            "Epoch  22 Batch  237 / 488  Training Loss  0.0001687036856310442\n",
            "Epoch  22 Batch  238 / 488  Training Loss  0.0002291733690071851\n",
            "Epoch  22 Batch  239 / 488  Training Loss  0.00024006135936360806\n",
            "Epoch  22 Batch  240 / 488  Training Loss  0.00017043109983205795\n",
            "Epoch  22 Batch  241 / 488  Training Loss  0.00017964231665246189\n",
            "Epoch  22 Batch  242 / 488  Training Loss  0.0002415352500975132\n",
            "Epoch  22 Batch  243 / 488  Training Loss  0.0003624516539275646\n",
            "Epoch  22 Batch  244 / 488  Training Loss  0.00017306605877820402\n",
            "Epoch  22 Batch  245 / 488  Training Loss  0.00022049670224078\n",
            "Epoch  22 Batch  246 / 488  Training Loss  0.00011516445374581963\n",
            "Epoch  22 Batch  247 / 488  Training Loss  0.0001885512174339965\n",
            "Epoch  22 Batch  248 / 488  Training Loss  0.00015037636330816895\n",
            "Epoch  22 Batch  249 / 488  Training Loss  0.00016373518155887723\n",
            "Epoch  22 Batch  250 / 488  Training Loss  0.00026212551165372133\n",
            "Epoch  22 Batch  251 / 488  Training Loss  0.00016769915237091482\n",
            "Epoch  22 Batch  252 / 488  Training Loss  0.00017797235341276973\n",
            "Epoch  22 Batch  253 / 488  Training Loss  0.00018128863302990794\n",
            "Epoch  22 Batch  254 / 488  Training Loss  0.00013919593766331673\n",
            "Epoch  22 Batch  255 / 488  Training Loss  0.00028215369093231857\n",
            "Epoch  22 Batch  256 / 488  Training Loss  0.0002192981482949108\n",
            "Epoch  22 Batch  257 / 488  Training Loss  0.00025180360535159707\n",
            "Epoch  22 Batch  258 / 488  Training Loss  0.0001523006649222225\n",
            "Epoch  22 Batch  259 / 488  Training Loss  0.00011135123349959031\n",
            "Epoch  22 Batch  260 / 488  Training Loss  0.00020282862533349544\n",
            "Epoch  22 Batch  261 / 488  Training Loss  0.00015634004375897348\n",
            "Epoch  22 Batch  262 / 488  Training Loss  0.00020425510592758656\n",
            "Epoch  22 Batch  263 / 488  Training Loss  0.00020820526697207242\n",
            "Epoch  22 Batch  264 / 488  Training Loss  0.00021994106646161526\n",
            "Epoch  22 Batch  265 / 488  Training Loss  0.00030506099574267864\n",
            "Epoch  22 Batch  266 / 488  Training Loss  0.00015492606326006353\n",
            "Epoch  22 Batch  267 / 488  Training Loss  0.0001733804092509672\n",
            "Epoch  22 Batch  268 / 488  Training Loss  0.0001941670197993517\n",
            "Epoch  22 Batch  269 / 488  Training Loss  0.0006428536144085228\n",
            "Epoch  22 Batch  270 / 488  Training Loss  0.00041216076351702213\n",
            "Epoch  22 Batch  271 / 488  Training Loss  0.0002740945783443749\n",
            "Epoch  22 Batch  272 / 488  Training Loss  0.000323474348988384\n",
            "Epoch  22 Batch  273 / 488  Training Loss  0.00018621780327521265\n",
            "Epoch  22 Batch  274 / 488  Training Loss  0.003930854611098766\n",
            "Epoch  22 Batch  275 / 488  Training Loss  0.002164671430364251\n",
            "Epoch  22 Batch  276 / 488  Training Loss  0.0003502658219076693\n",
            "Epoch  22 Batch  277 / 488  Training Loss  0.0019780390430241823\n",
            "Epoch  22 Batch  278 / 488  Training Loss  0.00028344671591185033\n",
            "Epoch  22 Batch  279 / 488  Training Loss  0.001278562005609274\n",
            "Epoch  22 Batch  280 / 488  Training Loss  0.0007759771542623639\n",
            "Epoch  22 Batch  281 / 488  Training Loss  0.0005346802063286304\n",
            "Epoch  22 Batch  282 / 488  Training Loss  0.0002459287643432617\n",
            "Epoch  22 Batch  283 / 488  Training Loss  0.00022894947323948145\n",
            "Epoch  22 Batch  284 / 488  Training Loss  0.00026519392849877477\n",
            "Epoch  22 Batch  285 / 488  Training Loss  0.00019456582958810031\n",
            "Epoch  22 Batch  286 / 488  Training Loss  0.0002766696270555258\n",
            "Epoch  22 Batch  287 / 488  Training Loss  0.00016340431466232985\n",
            "Epoch  22 Batch  288 / 488  Training Loss  0.0002785789838526398\n",
            "Epoch  22 Batch  289 / 488  Training Loss  0.0002569291682448238\n",
            "Epoch  22 Batch  290 / 488  Training Loss  0.00023093013442121446\n",
            "Epoch  22 Batch  291 / 488  Training Loss  0.0017919872188940644\n",
            "Epoch  22 Batch  292 / 488  Training Loss  0.00025557426852174103\n",
            "Epoch  22 Batch  293 / 488  Training Loss  0.00021512317471206188\n",
            "Epoch  22 Batch  294 / 488  Training Loss  0.00022594581241719425\n",
            "Epoch  22 Batch  295 / 488  Training Loss  0.000379942444851622\n",
            "Epoch  22 Batch  296 / 488  Training Loss  0.0004466006939765066\n",
            "Epoch  22 Batch  297 / 488  Training Loss  0.00028728460893034935\n",
            "Epoch  22 Batch  298 / 488  Training Loss  0.00039358591311611235\n",
            "Epoch  22 Batch  299 / 488  Training Loss  0.00048009809688664973\n",
            "Epoch  22 Batch  300 / 488  Training Loss  0.00023093083291314542\n",
            "Epoch  22 Batch  301 / 488  Training Loss  0.00044552632607519627\n",
            "Epoch  22 Batch  302 / 488  Training Loss  0.000517364009283483\n",
            "Epoch  22 Batch  303 / 488  Training Loss  0.0001595392677700147\n",
            "Epoch  22 Batch  304 / 488  Training Loss  0.0004368404042907059\n",
            "Epoch  22 Batch  305 / 488  Training Loss  0.00035653883242048323\n",
            "Epoch  22 Batch  306 / 488  Training Loss  0.00016825700004119426\n",
            "Epoch  22 Batch  307 / 488  Training Loss  0.008378075435757637\n",
            "Epoch  22 Batch  308 / 488  Training Loss  0.00036711769644171\n",
            "Epoch  22 Batch  309 / 488  Training Loss  0.0011849772417917848\n",
            "Epoch  22 Batch  310 / 488  Training Loss  0.00032738540903665125\n",
            "Epoch  22 Batch  311 / 488  Training Loss  0.00019333811360411346\n",
            "Epoch  22 Batch  312 / 488  Training Loss  0.00020691353711299598\n",
            "Epoch  22 Batch  313 / 488  Training Loss  0.000146515725646168\n",
            "Epoch  22 Batch  314 / 488  Training Loss  0.0001579920353833586\n",
            "Epoch  22 Batch  315 / 488  Training Loss  0.00032250070944428444\n",
            "Epoch  22 Batch  316 / 488  Training Loss  0.0002287647221237421\n",
            "Epoch  22 Batch  317 / 488  Training Loss  0.017089059576392174\n",
            "Epoch  22 Batch  318 / 488  Training Loss  0.00024026757455430925\n",
            "Epoch  22 Batch  319 / 488  Training Loss  0.00036615849239751697\n",
            "Epoch  22 Batch  320 / 488  Training Loss  0.0002757656911853701\n",
            "Epoch  22 Batch  321 / 488  Training Loss  0.00040913023985922337\n",
            "Epoch  22 Batch  322 / 488  Training Loss  0.00024086139455903322\n",
            "Epoch  22 Batch  323 / 488  Training Loss  0.00019082389189861715\n",
            "Epoch  22 Batch  324 / 488  Training Loss  0.0002942146093118936\n",
            "Epoch  22 Batch  325 / 488  Training Loss  0.0005363094387575984\n",
            "Epoch  22 Batch  326 / 488  Training Loss  0.00013275313540361822\n",
            "Epoch  22 Batch  327 / 488  Training Loss  0.000449963758001104\n",
            "Epoch  22 Batch  328 / 488  Training Loss  0.00028413819381967187\n",
            "Epoch  22 Batch  329 / 488  Training Loss  0.0004884389927610755\n",
            "Epoch  22 Batch  330 / 488  Training Loss  0.0003254564362578094\n",
            "Epoch  22 Batch  331 / 488  Training Loss  0.00032298092264682055\n",
            "Epoch  22 Batch  332 / 488  Training Loss  0.001878592767752707\n",
            "Epoch  22 Batch  333 / 488  Training Loss  0.0003556479641702026\n",
            "Epoch  22 Batch  334 / 488  Training Loss  0.00014827458653599024\n",
            "Epoch  22 Batch  335 / 488  Training Loss  0.00026089552557095885\n",
            "Epoch  22 Batch  336 / 488  Training Loss  0.00011434122279752046\n",
            "Epoch  22 Batch  337 / 488  Training Loss  0.0011182621819898486\n",
            "Epoch  22 Batch  338 / 488  Training Loss  0.0002512386126909405\n",
            "Epoch  22 Batch  339 / 488  Training Loss  0.0002870212192647159\n",
            "Epoch  22 Batch  340 / 488  Training Loss  0.00038179598050192\n",
            "Epoch  22 Batch  341 / 488  Training Loss  0.00025804206961765885\n",
            "Epoch  22 Batch  342 / 488  Training Loss  0.00018400090630166233\n",
            "Epoch  22 Batch  343 / 488  Training Loss  0.0015456766122952104\n",
            "Epoch  22 Batch  344 / 488  Training Loss  0.00025719753466546535\n",
            "Epoch  22 Batch  345 / 488  Training Loss  0.0001815261784940958\n",
            "Epoch  22 Batch  346 / 488  Training Loss  0.0002960358979180455\n",
            "Epoch  22 Batch  347 / 488  Training Loss  0.0002728177350945771\n",
            "Epoch  22 Batch  348 / 488  Training Loss  0.00029370278934948146\n",
            "Epoch  22 Batch  349 / 488  Training Loss  0.00023513915948569775\n",
            "Epoch  22 Batch  350 / 488  Training Loss  0.000324490072671324\n",
            "Epoch  22 Batch  351 / 488  Training Loss  0.0012046846095472574\n",
            "Epoch  22 Batch  352 / 488  Training Loss  0.00038241359288804233\n",
            "Epoch  22 Batch  353 / 488  Training Loss  0.00027510052314028144\n",
            "Epoch  22 Batch  354 / 488  Training Loss  0.00046308129094541073\n",
            "Epoch  22 Batch  355 / 488  Training Loss  0.0002540649438742548\n",
            "Epoch  22 Batch  356 / 488  Training Loss  0.00047427290701307356\n",
            "Epoch  22 Batch  357 / 488  Training Loss  0.0002769773709587753\n",
            "Epoch  22 Batch  358 / 488  Training Loss  0.00023909038281999528\n",
            "Epoch  22 Batch  359 / 488  Training Loss  0.00034539663465693593\n",
            "Epoch  22 Batch  360 / 488  Training Loss  0.0004469822160899639\n",
            "Epoch  22 Batch  361 / 488  Training Loss  0.0006489288061857224\n",
            "Epoch  22 Batch  362 / 488  Training Loss  0.0002038881357293576\n",
            "Epoch  22 Batch  363 / 488  Training Loss  0.00040346887544728816\n",
            "Epoch  22 Batch  364 / 488  Training Loss  0.0005353757878765464\n",
            "Epoch  22 Batch  365 / 488  Training Loss  0.0002456590882502496\n",
            "Epoch  22 Batch  366 / 488  Training Loss  0.0006375451921485364\n",
            "Epoch  22 Batch  367 / 488  Training Loss  0.000264018279267475\n",
            "Epoch  22 Batch  368 / 488  Training Loss  0.00027656240854412317\n",
            "Epoch  22 Batch  369 / 488  Training Loss  0.00027574971318244934\n",
            "Epoch  22 Batch  370 / 488  Training Loss  0.003915940877050161\n",
            "Epoch  22 Batch  371 / 488  Training Loss  0.00024212461721617728\n",
            "Epoch  22 Batch  372 / 488  Training Loss  0.0002594971447251737\n",
            "Epoch  22 Batch  373 / 488  Training Loss  0.0002483920252416283\n",
            "Epoch  22 Batch  374 / 488  Training Loss  0.00019906829402316362\n",
            "Epoch  22 Batch  375 / 488  Training Loss  0.0006363543798215687\n",
            "Epoch  22 Batch  376 / 488  Training Loss  0.00022921638446860015\n",
            "Epoch  22 Batch  377 / 488  Training Loss  0.00018701149383559823\n",
            "Epoch  22 Batch  378 / 488  Training Loss  0.0002318694459972903\n",
            "Epoch  22 Batch  379 / 488  Training Loss  0.00022086648095864803\n",
            "Epoch  22 Batch  380 / 488  Training Loss  0.0004373390693217516\n",
            "Epoch  22 Batch  381 / 488  Training Loss  0.0002279330074088648\n",
            "Epoch  22 Batch  382 / 488  Training Loss  0.00036296126199886203\n",
            "Epoch  22 Batch  383 / 488  Training Loss  0.00019859996973536909\n",
            "Epoch  22 Batch  384 / 488  Training Loss  0.0002924042346421629\n",
            "Epoch  22 Batch  385 / 488  Training Loss  0.00015658032498322427\n",
            "Epoch  22 Batch  386 / 488  Training Loss  0.00020930342725478113\n",
            "Epoch  22 Batch  387 / 488  Training Loss  0.00022427047952078283\n",
            "Epoch  22 Batch  388 / 488  Training Loss  0.0001956392516149208\n",
            "Epoch  22 Batch  389 / 488  Training Loss  0.00023345807858277112\n",
            "Epoch  22 Batch  390 / 488  Training Loss  0.00015090356464497745\n",
            "Epoch  22 Batch  391 / 488  Training Loss  0.0002946448512375355\n",
            "Epoch  22 Batch  392 / 488  Training Loss  0.0002045793953584507\n",
            "Epoch  22 Batch  393 / 488  Training Loss  0.00042649562237784266\n",
            "Epoch  22 Batch  394 / 488  Training Loss  0.0001447068789275363\n",
            "Epoch  22 Batch  395 / 488  Training Loss  0.00020777627651114017\n",
            "Epoch  22 Batch  396 / 488  Training Loss  0.0003688112774398178\n",
            "Epoch  22 Batch  397 / 488  Training Loss  0.0001486562832724303\n",
            "Epoch  22 Batch  398 / 488  Training Loss  0.0003323973505757749\n",
            "Epoch  22 Batch  399 / 488  Training Loss  0.00028137199115008116\n",
            "Epoch  22 Batch  400 / 488  Training Loss  0.00016985464026220143\n",
            "Epoch  22 Batch  401 / 488  Training Loss  0.0002769330167211592\n",
            "Epoch  22 Batch  402 / 488  Training Loss  0.00028621425735764205\n",
            "Epoch  22 Batch  403 / 488  Training Loss  0.0002619318547658622\n",
            "Epoch  22 Batch  404 / 488  Training Loss  0.0005473548080772161\n",
            "Epoch  22 Batch  405 / 488  Training Loss  0.0003314345085527748\n",
            "Epoch  22 Batch  406 / 488  Training Loss  0.0010044988011941314\n",
            "Epoch  22 Batch  407 / 488  Training Loss  0.0006586347008123994\n",
            "Epoch  22 Batch  408 / 488  Training Loss  0.00017317815218120813\n",
            "Epoch  22 Batch  409 / 488  Training Loss  0.0002588474308140576\n",
            "Epoch  22 Batch  410 / 488  Training Loss  0.00039880420081317425\n",
            "Epoch  22 Batch  411 / 488  Training Loss  0.00029666037880815566\n",
            "Epoch  22 Batch  412 / 488  Training Loss  0.00016289723862428218\n",
            "Epoch  22 Batch  413 / 488  Training Loss  0.00018042890587821603\n",
            "Epoch  22 Batch  414 / 488  Training Loss  0.0002781755174510181\n",
            "Epoch  22 Batch  415 / 488  Training Loss  0.0003119312459602952\n",
            "Epoch  22 Batch  416 / 488  Training Loss  0.00018834331422112882\n",
            "Epoch  22 Batch  417 / 488  Training Loss  0.00022061266645323485\n",
            "Epoch  22 Batch  418 / 488  Training Loss  0.0002499779511708766\n",
            "Epoch  22 Batch  419 / 488  Training Loss  0.00012052452802890912\n",
            "Epoch  22 Batch  420 / 488  Training Loss  0.0002426610590191558\n",
            "Epoch  22 Batch  421 / 488  Training Loss  0.00014267947699408978\n",
            "Epoch  22 Batch  422 / 488  Training Loss  0.00017432832100894302\n",
            "Epoch  22 Batch  423 / 488  Training Loss  0.0001917452027555555\n",
            "Epoch  22 Batch  424 / 488  Training Loss  0.00023821054492145777\n",
            "Epoch  22 Batch  425 / 488  Training Loss  0.00021811322949361056\n",
            "Epoch  22 Batch  426 / 488  Training Loss  0.0001908492122311145\n",
            "Epoch  22 Batch  427 / 488  Training Loss  0.0001847854582592845\n",
            "Epoch  22 Batch  428 / 488  Training Loss  0.0001496527111157775\n",
            "Epoch  22 Batch  429 / 488  Training Loss  0.00026450518635101616\n",
            "Epoch  22 Batch  430 / 488  Training Loss  0.0002105742896674201\n",
            "Epoch  22 Batch  431 / 488  Training Loss  0.00021747515711467713\n",
            "Epoch  22 Batch  432 / 488  Training Loss  0.00017023537657223642\n",
            "Epoch  22 Batch  433 / 488  Training Loss  0.0002050387265626341\n",
            "Epoch  22 Batch  434 / 488  Training Loss  0.0002631582028698176\n",
            "Epoch  22 Batch  435 / 488  Training Loss  0.00026307327789254487\n",
            "Epoch  22 Batch  436 / 488  Training Loss  0.00021794527128804475\n",
            "Epoch  22 Batch  437 / 488  Training Loss  0.0005095719243399799\n",
            "Epoch  22 Batch  438 / 488  Training Loss  0.000324241875205189\n",
            "Epoch  22 Batch  439 / 488  Training Loss  0.0001892058935482055\n",
            "Epoch  22 Batch  440 / 488  Training Loss  0.0002412925532553345\n",
            "Epoch  22 Batch  441 / 488  Training Loss  0.00012456653348635882\n",
            "Epoch  22 Batch  442 / 488  Training Loss  0.0001730652729747817\n",
            "Epoch  22 Batch  443 / 488  Training Loss  0.00019027295638807118\n",
            "Epoch  22 Batch  444 / 488  Training Loss  0.0002584594185464084\n",
            "Epoch  22 Batch  445 / 488  Training Loss  0.000170426195836626\n",
            "Epoch  22 Batch  446 / 488  Training Loss  0.0012260861694812775\n",
            "Epoch  22 Batch  447 / 488  Training Loss  0.00027222197968512774\n",
            "Epoch  22 Batch  448 / 488  Training Loss  0.00024956121342256665\n",
            "Epoch  22 Batch  449 / 488  Training Loss  0.000225088995648548\n",
            "Epoch  22 Batch  450 / 488  Training Loss  0.0002533497754484415\n",
            "Epoch  22 Batch  451 / 488  Training Loss  0.00036060501588508487\n",
            "Epoch  22 Batch  452 / 488  Training Loss  0.00024056233814917505\n",
            "Epoch  22 Batch  453 / 488  Training Loss  0.00025468747480772436\n",
            "Epoch  22 Batch  454 / 488  Training Loss  0.0002024984423769638\n",
            "Epoch  22 Batch  455 / 488  Training Loss  0.0001888848200906068\n",
            "Epoch  22 Batch  456 / 488  Training Loss  0.00016901649360079318\n",
            "Epoch  22 Batch  457 / 488  Training Loss  0.0003555361763574183\n",
            "Epoch  22 Batch  458 / 488  Training Loss  0.0003467272617854178\n",
            "Epoch  22 Batch  459 / 488  Training Loss  0.00025942042702808976\n",
            "Epoch  22 Batch  460 / 488  Training Loss  0.00012185866944491863\n",
            "Epoch  22 Batch  461 / 488  Training Loss  0.0003593617002479732\n",
            "Epoch  22 Batch  462 / 488  Training Loss  0.00017645853222347796\n",
            "Epoch  22 Batch  463 / 488  Training Loss  0.0002771978615783155\n",
            "Epoch  22 Batch  464 / 488  Training Loss  0.00025816954439505935\n",
            "Epoch  22 Batch  465 / 488  Training Loss  0.0003176735481247306\n",
            "Epoch  22 Batch  466 / 488  Training Loss  0.0004658580292016268\n",
            "Epoch  22 Batch  467 / 488  Training Loss  0.0001546939747640863\n",
            "Epoch  22 Batch  468 / 488  Training Loss  0.00016536997281946242\n",
            "Epoch  22 Batch  469 / 488  Training Loss  0.00026946444995701313\n",
            "Epoch  22 Batch  470 / 488  Training Loss  0.0002474038046784699\n",
            "Epoch  22 Batch  471 / 488  Training Loss  0.0001891682477435097\n",
            "Epoch  22 Batch  472 / 488  Training Loss  0.00011090973566751927\n",
            "Epoch  22 Batch  473 / 488  Training Loss  0.00027576053980737925\n",
            "Epoch  22 Batch  474 / 488  Training Loss  0.0002171831438317895\n",
            "Epoch  22 Batch  475 / 488  Training Loss  0.00025387760251760483\n",
            "Epoch  22 Batch  476 / 488  Training Loss  0.00024040248536039144\n",
            "Epoch  22 Batch  477 / 488  Training Loss  0.0005094140651635826\n",
            "Epoch  22 Batch  478 / 488  Training Loss  0.00015390219050459564\n",
            "Epoch  22 Batch  479 / 488  Training Loss  0.0003243938845116645\n",
            "Epoch  22 Batch  480 / 488  Training Loss  0.00026609114138409495\n",
            "Epoch  22 Batch  481 / 488  Training Loss  0.00012966325448360294\n",
            "Epoch  22 Batch  482 / 488  Training Loss  0.00020585251331795007\n",
            "Epoch  22 Batch  483 / 488  Training Loss  0.00020801162463612854\n",
            "Epoch  22 Batch  484 / 488  Training Loss  0.00019828703079838306\n",
            "Epoch  22 Batch  485 / 488  Training Loss  0.00011911134060937911\n",
            "Epoch  22 Batch  486 / 488  Training Loss  0.00017788444529287517\n",
            "Epoch  22 Batch  487 / 488  Training Loss  0.00017697695875540376\n",
            "  23    |    -    |   0.000377   | 47.052305\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 23\n",
            "Epoch  23 Batch  0 / 488  Training Loss  0.000115044298581779\n",
            "Epoch  23 Batch  1 / 488  Training Loss  0.00019759967108257115\n",
            "Epoch  23 Batch  2 / 488  Training Loss  0.00010696172103052959\n",
            "Epoch  23 Batch  3 / 488  Training Loss  0.00012461816368158907\n",
            "Epoch  23 Batch  4 / 488  Training Loss  0.00012710204464383423\n",
            "Epoch  23 Batch  5 / 488  Training Loss  0.00016424500790890306\n",
            "Epoch  23 Batch  6 / 488  Training Loss  0.00012115902791265398\n",
            "Epoch  23 Batch  7 / 488  Training Loss  0.00015074934344738722\n",
            "Epoch  23 Batch  8 / 488  Training Loss  0.0001600339455762878\n",
            "Epoch  23 Batch  9 / 488  Training Loss  0.0001927496341522783\n",
            "Epoch  23 Batch  10 / 488  Training Loss  0.00013488446711562574\n",
            "Epoch  23 Batch  11 / 488  Training Loss  0.00013972120359539986\n",
            "Epoch  23 Batch  12 / 488  Training Loss  9.242634405381978e-05\n",
            "Epoch  23 Batch  13 / 488  Training Loss  0.00010795976413646713\n",
            "Epoch  23 Batch  14 / 488  Training Loss  0.00011141491995658726\n",
            "Epoch  23 Batch  15 / 488  Training Loss  0.00012524604971986264\n",
            "Epoch  23 Batch  16 / 488  Training Loss  0.0001276240946026519\n",
            "Epoch  23 Batch  17 / 488  Training Loss  0.0001504726824350655\n",
            "Epoch  23 Batch  18 / 488  Training Loss  0.00011964103032369167\n",
            "Epoch  23 Batch  19 / 488  Training Loss  0.00015301842358894646\n",
            "Epoch  23 Batch  20 / 488  Training Loss  0.0001618304377188906\n",
            "Epoch  23 Batch  21 / 488  Training Loss  0.00011897259537363425\n",
            "Epoch  23 Batch  22 / 488  Training Loss  0.00012094451813027263\n",
            "Epoch  23 Batch  23 / 488  Training Loss  0.00011667583748931065\n",
            "Epoch  23 Batch  24 / 488  Training Loss  9.851260256255046e-05\n",
            "Epoch  23 Batch  25 / 488  Training Loss  0.00012526288628578186\n",
            "Epoch  23 Batch  26 / 488  Training Loss  0.00013511259749066085\n",
            "Epoch  23 Batch  27 / 488  Training Loss  0.0001766608766047284\n",
            "Epoch  23 Batch  28 / 488  Training Loss  0.00011932697088923305\n",
            "Epoch  23 Batch  29 / 488  Training Loss  0.0001325082703260705\n",
            "Epoch  23 Batch  30 / 488  Training Loss  0.0001338859583484009\n",
            "Epoch  23 Batch  31 / 488  Training Loss  9.882941230898723e-05\n",
            "Epoch  23 Batch  32 / 488  Training Loss  0.00015934828843455762\n",
            "Epoch  23 Batch  33 / 488  Training Loss  0.0013914874289184809\n",
            "Epoch  23 Batch  34 / 488  Training Loss  0.00011773592268582433\n",
            "Epoch  23 Batch  35 / 488  Training Loss  8.58846033224836e-05\n",
            "Epoch  23 Batch  36 / 488  Training Loss  0.00014272055705077946\n",
            "Epoch  23 Batch  37 / 488  Training Loss  0.00013142809621058404\n",
            "Epoch  23 Batch  38 / 488  Training Loss  0.00011624912440311164\n",
            "Epoch  23 Batch  39 / 488  Training Loss  0.00021225032105576247\n",
            "Epoch  23 Batch  40 / 488  Training Loss  0.0001428414398105815\n",
            "Epoch  23 Batch  41 / 488  Training Loss  0.00013986458361614496\n",
            "Epoch  23 Batch  42 / 488  Training Loss  0.00011827229900518432\n",
            "Epoch  23 Batch  43 / 488  Training Loss  0.0002976939140353352\n",
            "Epoch  23 Batch  44 / 488  Training Loss  0.00013804904301650822\n",
            "Epoch  23 Batch  45 / 488  Training Loss  9.630060230847448e-05\n",
            "Epoch  23 Batch  46 / 488  Training Loss  9.159593901131302e-05\n",
            "Epoch  23 Batch  47 / 488  Training Loss  0.00015867131878621876\n",
            "Epoch  23 Batch  48 / 488  Training Loss  0.00015741608513053507\n",
            "Epoch  23 Batch  49 / 488  Training Loss  0.0001596405782038346\n",
            "Epoch  23 Batch  50 / 488  Training Loss  0.0001472457661293447\n",
            "Epoch  23 Batch  51 / 488  Training Loss  0.0001481891085859388\n",
            "Epoch  23 Batch  52 / 488  Training Loss  0.00012715987395495176\n",
            "Epoch  23 Batch  53 / 488  Training Loss  0.00011819527571788058\n",
            "Epoch  23 Batch  54 / 488  Training Loss  0.0001554746850160882\n",
            "Epoch  23 Batch  55 / 488  Training Loss  0.00015049199282657355\n",
            "Epoch  23 Batch  56 / 488  Training Loss  0.00022620086383540183\n",
            "Epoch  23 Batch  57 / 488  Training Loss  0.00011582644947338849\n",
            "Epoch  23 Batch  58 / 488  Training Loss  7.246840687002987e-05\n",
            "Epoch  23 Batch  59 / 488  Training Loss  0.0001712140947347507\n",
            "Epoch  23 Batch  60 / 488  Training Loss  0.00010879209730774164\n",
            "Epoch  23 Batch  61 / 488  Training Loss  0.00021724597900174558\n",
            "Epoch  23 Batch  62 / 488  Training Loss  0.00015295484627131373\n",
            "Epoch  23 Batch  63 / 488  Training Loss  0.00012023704766761512\n",
            "Epoch  23 Batch  64 / 488  Training Loss  0.00011892545444425195\n",
            "Epoch  23 Batch  65 / 488  Training Loss  0.0001401650079060346\n",
            "Epoch  23 Batch  66 / 488  Training Loss  0.00013595774362329394\n",
            "Epoch  23 Batch  67 / 488  Training Loss  0.00014898207155056298\n",
            "Epoch  23 Batch  68 / 488  Training Loss  0.0001298249262617901\n",
            "Epoch  23 Batch  69 / 488  Training Loss  9.446001058677211e-05\n",
            "Epoch  23 Batch  70 / 488  Training Loss  9.906086779665202e-05\n",
            "Epoch  23 Batch  71 / 488  Training Loss  0.00022413504484575242\n",
            "Epoch  23 Batch  72 / 488  Training Loss  0.0001502117229392752\n",
            "Epoch  23 Batch  73 / 488  Training Loss  0.00012278807116672397\n",
            "Epoch  23 Batch  74 / 488  Training Loss  8.891009201761335e-05\n",
            "Epoch  23 Batch  75 / 488  Training Loss  0.0001664903829805553\n",
            "Epoch  23 Batch  76 / 488  Training Loss  0.00010250613559037447\n",
            "Epoch  23 Batch  77 / 488  Training Loss  0.00017577757535036653\n",
            "Epoch  23 Batch  78 / 488  Training Loss  0.000176843095687218\n",
            "Epoch  23 Batch  79 / 488  Training Loss  0.0001338884758297354\n",
            "Epoch  23 Batch  80 / 488  Training Loss  0.00011359985364833847\n",
            "Epoch  23 Batch  81 / 488  Training Loss  0.00019273572252131999\n",
            "Epoch  23 Batch  82 / 488  Training Loss  0.00014310060942079872\n",
            "Epoch  23 Batch  83 / 488  Training Loss  0.00019306334434077144\n",
            "Epoch  23 Batch  84 / 488  Training Loss  0.00018577156879473478\n",
            "Epoch  23 Batch  85 / 488  Training Loss  0.00016279703413601965\n",
            "Epoch  23 Batch  86 / 488  Training Loss  0.000186660181498155\n",
            "Epoch  23 Batch  87 / 488  Training Loss  0.0001610876788618043\n",
            "Epoch  23 Batch  88 / 488  Training Loss  0.00011849535076180473\n",
            "Epoch  23 Batch  89 / 488  Training Loss  0.00013423309428617358\n",
            "Epoch  23 Batch  90 / 488  Training Loss  0.00010516783368075266\n",
            "Epoch  23 Batch  91 / 488  Training Loss  0.00012017602421110496\n",
            "Epoch  23 Batch  92 / 488  Training Loss  0.0001741239393595606\n",
            "Epoch  23 Batch  93 / 488  Training Loss  0.0001368481171084568\n",
            "Epoch  23 Batch  94 / 488  Training Loss  0.0001142337205237709\n",
            "Epoch  23 Batch  95 / 488  Training Loss  0.00013965681137051433\n",
            "Epoch  23 Batch  96 / 488  Training Loss  0.000121936624054797\n",
            "Epoch  23 Batch  97 / 488  Training Loss  0.00013623712584376335\n",
            "Epoch  23 Batch  98 / 488  Training Loss  0.00039856688817963004\n",
            "Epoch  23 Batch  99 / 488  Training Loss  0.00014559146075043827\n",
            "Epoch  23 Batch  100 / 488  Training Loss  0.00018141190230380744\n",
            "Epoch  23 Batch  101 / 488  Training Loss  0.00011907971929758787\n",
            "Epoch  23 Batch  102 / 488  Training Loss  0.00016762930317781866\n",
            "Epoch  23 Batch  103 / 488  Training Loss  0.00015696309856139123\n",
            "Epoch  23 Batch  104 / 488  Training Loss  0.0002844682603608817\n",
            "Epoch  23 Batch  105 / 488  Training Loss  0.00017545392620377243\n",
            "Epoch  23 Batch  106 / 488  Training Loss  0.0001490578579250723\n",
            "Epoch  23 Batch  107 / 488  Training Loss  0.00019877517479471862\n",
            "Epoch  23 Batch  108 / 488  Training Loss  0.0001379953173454851\n",
            "Epoch  23 Batch  109 / 488  Training Loss  0.00013337921700440347\n",
            "Epoch  23 Batch  110 / 488  Training Loss  0.0001545651030028239\n",
            "Epoch  23 Batch  111 / 488  Training Loss  0.00015224954404402524\n",
            "Epoch  23 Batch  112 / 488  Training Loss  0.00012075470294803381\n",
            "Epoch  23 Batch  113 / 488  Training Loss  0.00017178451525978744\n",
            "Epoch  23 Batch  114 / 488  Training Loss  9.939877782016993e-05\n",
            "Epoch  23 Batch  115 / 488  Training Loss  0.00012450647773221135\n",
            "Epoch  23 Batch  116 / 488  Training Loss  0.00014913189806975424\n",
            "Epoch  23 Batch  117 / 488  Training Loss  0.00012988827074877918\n",
            "Epoch  23 Batch  118 / 488  Training Loss  0.00010042637586593628\n",
            "Epoch  23 Batch  119 / 488  Training Loss  0.0014611472142860293\n",
            "Epoch  23 Batch  120 / 488  Training Loss  0.0001373518753098324\n",
            "Epoch  23 Batch  121 / 488  Training Loss  9.817567479331046e-05\n",
            "Epoch  23 Batch  122 / 488  Training Loss  0.00020894384942948818\n",
            "Epoch  23 Batch  123 / 488  Training Loss  0.00011634823022177443\n",
            "Epoch  23 Batch  124 / 488  Training Loss  0.000303710054140538\n",
            "Epoch  23 Batch  125 / 488  Training Loss  0.00020615386893041432\n",
            "Epoch  23 Batch  126 / 488  Training Loss  0.00016704868176020682\n",
            "Epoch  23 Batch  127 / 488  Training Loss  0.000152764972881414\n",
            "Epoch  23 Batch  128 / 488  Training Loss  0.00018188156536780298\n",
            "Epoch  23 Batch  129 / 488  Training Loss  0.0001468176778871566\n",
            "Epoch  23 Batch  130 / 488  Training Loss  9.669970313552767e-05\n",
            "Epoch  23 Batch  131 / 488  Training Loss  0.00015917603741399944\n",
            "Epoch  23 Batch  132 / 488  Training Loss  0.0002383753890171647\n",
            "Epoch  23 Batch  133 / 488  Training Loss  0.0004520481452345848\n",
            "Epoch  23 Batch  134 / 488  Training Loss  0.000159263945533894\n",
            "Epoch  23 Batch  135 / 488  Training Loss  0.00020392515580169857\n",
            "Epoch  23 Batch  136 / 488  Training Loss  0.00010614989878376946\n",
            "Epoch  23 Batch  137 / 488  Training Loss  9.150493133347481e-05\n",
            "Epoch  23 Batch  138 / 488  Training Loss  0.00015387464372906834\n",
            "Epoch  23 Batch  139 / 488  Training Loss  0.00011814590834546834\n",
            "Epoch  23 Batch  140 / 488  Training Loss  0.00013000855688005686\n",
            "Epoch  23 Batch  141 / 488  Training Loss  0.00018178534810431302\n",
            "Epoch  23 Batch  142 / 488  Training Loss  0.0001320074952673167\n",
            "Epoch  23 Batch  143 / 488  Training Loss  0.00010280111018801108\n",
            "Epoch  23 Batch  144 / 488  Training Loss  0.00019616300414782017\n",
            "Epoch  23 Batch  145 / 488  Training Loss  0.00010454782750457525\n",
            "Epoch  23 Batch  146 / 488  Training Loss  0.00013046636013314128\n",
            "Epoch  23 Batch  147 / 488  Training Loss  0.00019793453975580633\n",
            "Epoch  23 Batch  148 / 488  Training Loss  0.00016408381634391844\n",
            "Epoch  23 Batch  149 / 488  Training Loss  0.00031454366398975253\n",
            "Epoch  23 Batch  150 / 488  Training Loss  8.82481544977054e-05\n",
            "Epoch  23 Batch  151 / 488  Training Loss  0.0001340834132861346\n",
            "Epoch  23 Batch  152 / 488  Training Loss  0.00011724072828656062\n",
            "Epoch  23 Batch  153 / 488  Training Loss  0.0001348157529719174\n",
            "Epoch  23 Batch  154 / 488  Training Loss  9.44450221140869e-05\n",
            "Epoch  23 Batch  155 / 488  Training Loss  0.00020726097864098847\n",
            "Epoch  23 Batch  156 / 488  Training Loss  0.00014863356773275882\n",
            "Epoch  23 Batch  157 / 488  Training Loss  0.00011289888061583042\n",
            "Epoch  23 Batch  158 / 488  Training Loss  0.0001404403301421553\n",
            "Epoch  23 Batch  159 / 488  Training Loss  0.00014147264300845563\n",
            "Epoch  23 Batch  160 / 488  Training Loss  0.0001302444579778239\n",
            "Epoch  23 Batch  161 / 488  Training Loss  0.00016807165229693055\n",
            "Epoch  23 Batch  162 / 488  Training Loss  0.00017730699619278312\n",
            "Epoch  23 Batch  163 / 488  Training Loss  0.0010315305553376675\n",
            "Epoch  23 Batch  164 / 488  Training Loss  9.727002907311544e-05\n",
            "Epoch  23 Batch  165 / 488  Training Loss  0.0002040173130808398\n",
            "Epoch  23 Batch  166 / 488  Training Loss  0.0003086408250965178\n",
            "Epoch  23 Batch  167 / 488  Training Loss  0.00012216382310725749\n",
            "Epoch  23 Batch  168 / 488  Training Loss  0.0002159183204639703\n",
            "Epoch  23 Batch  169 / 488  Training Loss  0.00010152476897928864\n",
            "Epoch  23 Batch  170 / 488  Training Loss  0.0001062904266291298\n",
            "Epoch  23 Batch  171 / 488  Training Loss  0.00013888067041989416\n",
            "Epoch  23 Batch  172 / 488  Training Loss  0.00018057922716252506\n",
            "Epoch  23 Batch  173 / 488  Training Loss  0.0001601026306161657\n",
            "Epoch  23 Batch  174 / 488  Training Loss  0.00022933571017347276\n",
            "Epoch  23 Batch  175 / 488  Training Loss  0.00017533097707200795\n",
            "Epoch  23 Batch  176 / 488  Training Loss  0.0001648509205551818\n",
            "Epoch  23 Batch  177 / 488  Training Loss  0.00018264571554027498\n",
            "Epoch  23 Batch  178 / 488  Training Loss  0.00017370688146911561\n",
            "Epoch  23 Batch  179 / 488  Training Loss  0.0001265316823264584\n",
            "Epoch  23 Batch  180 / 488  Training Loss  9.083493205253035e-05\n",
            "Epoch  23 Batch  181 / 488  Training Loss  8.154100214596838e-05\n",
            "Epoch  23 Batch  182 / 488  Training Loss  0.00015204686496872455\n",
            "Epoch  23 Batch  183 / 488  Training Loss  0.00012752966722473502\n",
            "Epoch  23 Batch  184 / 488  Training Loss  0.00023313661222346127\n",
            "Epoch  23 Batch  185 / 488  Training Loss  0.00011909648310393095\n",
            "Epoch  23 Batch  186 / 488  Training Loss  9.851482172962278e-05\n",
            "Epoch  23 Batch  187 / 488  Training Loss  0.00016700713604222983\n",
            "Epoch  23 Batch  188 / 488  Training Loss  0.0001318699651164934\n",
            "Epoch  23 Batch  189 / 488  Training Loss  0.00015340551908593625\n",
            "Epoch  23 Batch  190 / 488  Training Loss  0.00014327852113638073\n",
            "Epoch  23 Batch  191 / 488  Training Loss  0.0001889276027213782\n",
            "Epoch  23 Batch  192 / 488  Training Loss  0.0001415766018908471\n",
            "Epoch  23 Batch  193 / 488  Training Loss  9.024253813549876e-05\n",
            "Epoch  23 Batch  194 / 488  Training Loss  0.00016249400505330414\n",
            "Epoch  23 Batch  195 / 488  Training Loss  0.0002152957022190094\n",
            "Epoch  23 Batch  196 / 488  Training Loss  0.00018381790141575038\n",
            "Epoch  23 Batch  197 / 488  Training Loss  0.0001374187704641372\n",
            "Epoch  23 Batch  198 / 488  Training Loss  0.0001281034928979352\n",
            "Epoch  23 Batch  199 / 488  Training Loss  0.00017738863243721426\n",
            "Epoch  23 Batch  200 / 488  Training Loss  0.00015242969675455242\n",
            "Epoch  23 Batch  201 / 488  Training Loss  0.00015949641237966716\n",
            "Epoch  23 Batch  202 / 488  Training Loss  0.00013223863788880408\n",
            "Epoch  23 Batch  203 / 488  Training Loss  0.0005507503519766033\n",
            "Epoch  23 Batch  204 / 488  Training Loss  0.00011521494889166206\n",
            "Epoch  23 Batch  205 / 488  Training Loss  0.00013535577454604208\n",
            "Epoch  23 Batch  206 / 488  Training Loss  7.763555913697928e-05\n",
            "Epoch  23 Batch  207 / 488  Training Loss  0.0001278042036574334\n",
            "Epoch  23 Batch  208 / 488  Training Loss  0.0001342760951956734\n",
            "Epoch  23 Batch  209 / 488  Training Loss  0.00017955995281226933\n",
            "Epoch  23 Batch  210 / 488  Training Loss  0.00013121202937327325\n",
            "Epoch  23 Batch  211 / 488  Training Loss  0.00016654355567879975\n",
            "Epoch  23 Batch  212 / 488  Training Loss  0.00011469960736576468\n",
            "Epoch  23 Batch  213 / 488  Training Loss  0.0001365191419608891\n",
            "Epoch  23 Batch  214 / 488  Training Loss  0.00015651786816306412\n",
            "Epoch  23 Batch  215 / 488  Training Loss  0.00017431683954782784\n",
            "Epoch  23 Batch  216 / 488  Training Loss  0.0002513407962396741\n",
            "Epoch  23 Batch  217 / 488  Training Loss  0.00020474960911087692\n",
            "Epoch  23 Batch  218 / 488  Training Loss  0.0001432473218301311\n",
            "Epoch  23 Batch  219 / 488  Training Loss  0.0001436309248674661\n",
            "Epoch  23 Batch  220 / 488  Training Loss  0.00010069394193124026\n",
            "Epoch  23 Batch  221 / 488  Training Loss  0.00015184699441306293\n",
            "Epoch  23 Batch  222 / 488  Training Loss  0.00011137776164105162\n",
            "Epoch  23 Batch  223 / 488  Training Loss  8.20099885459058e-05\n",
            "Epoch  23 Batch  224 / 488  Training Loss  8.68952993187122e-05\n",
            "Epoch  23 Batch  225 / 488  Training Loss  0.00011063065903726965\n",
            "Epoch  23 Batch  226 / 488  Training Loss  0.00013057547039352357\n",
            "Epoch  23 Batch  227 / 488  Training Loss  0.00017920316895470023\n",
            "Epoch  23 Batch  228 / 488  Training Loss  0.0001674475206527859\n",
            "Epoch  23 Batch  229 / 488  Training Loss  0.00011657031427603215\n",
            "Epoch  23 Batch  230 / 488  Training Loss  9.339320240542293e-05\n",
            "Epoch  23 Batch  231 / 488  Training Loss  0.00010105781257152557\n",
            "Epoch  23 Batch  232 / 488  Training Loss  0.00012022627925034612\n",
            "Epoch  23 Batch  233 / 488  Training Loss  0.00011220532178413123\n",
            "Epoch  23 Batch  234 / 488  Training Loss  8.98043072083965e-05\n",
            "Epoch  23 Batch  235 / 488  Training Loss  0.00011047571751987562\n",
            "Epoch  23 Batch  236 / 488  Training Loss  0.00018359077512286603\n",
            "Epoch  23 Batch  237 / 488  Training Loss  0.0001933526073116809\n",
            "Epoch  23 Batch  238 / 488  Training Loss  0.00014153248048387468\n",
            "Epoch  23 Batch  239 / 488  Training Loss  0.00021635388839058578\n",
            "Epoch  23 Batch  240 / 488  Training Loss  0.00013605582353193313\n",
            "Epoch  23 Batch  241 / 488  Training Loss  0.0002071007911581546\n",
            "Epoch  23 Batch  242 / 488  Training Loss  9.428145858692005e-05\n",
            "Epoch  23 Batch  243 / 488  Training Loss  0.00011809429997811094\n",
            "Epoch  23 Batch  244 / 488  Training Loss  8.721481572138146e-05\n",
            "Epoch  23 Batch  245 / 488  Training Loss  0.0001326401688857004\n",
            "Epoch  23 Batch  246 / 488  Training Loss  0.00014232660760171711\n",
            "Epoch  23 Batch  247 / 488  Training Loss  0.00013317345292307436\n",
            "Epoch  23 Batch  248 / 488  Training Loss  0.0001296894915867597\n",
            "Epoch  23 Batch  249 / 488  Training Loss  0.00011062913108617067\n",
            "Epoch  23 Batch  250 / 488  Training Loss  0.0001122710746130906\n",
            "Epoch  23 Batch  251 / 488  Training Loss  0.00013176797074265778\n",
            "Epoch  23 Batch  252 / 488  Training Loss  0.00014478783123195171\n",
            "Epoch  23 Batch  253 / 488  Training Loss  0.0001532902242615819\n",
            "Epoch  23 Batch  254 / 488  Training Loss  0.00010978572390740737\n",
            "Epoch  23 Batch  255 / 488  Training Loss  0.00013586833665613085\n",
            "Epoch  23 Batch  256 / 488  Training Loss  0.00010199165262747556\n",
            "Epoch  23 Batch  257 / 488  Training Loss  0.00010873457358684391\n",
            "Epoch  23 Batch  258 / 488  Training Loss  8.866584539646283e-05\n",
            "Epoch  23 Batch  259 / 488  Training Loss  0.00012936860730405897\n",
            "Epoch  23 Batch  260 / 488  Training Loss  0.00022508646361529827\n",
            "Epoch  23 Batch  261 / 488  Training Loss  0.00022731395438313484\n",
            "Epoch  23 Batch  262 / 488  Training Loss  0.00014550962077919394\n",
            "Epoch  23 Batch  263 / 488  Training Loss  0.00014533483772538602\n",
            "Epoch  23 Batch  264 / 488  Training Loss  0.00013077682524453849\n",
            "Epoch  23 Batch  265 / 488  Training Loss  0.000246552808675915\n",
            "Epoch  23 Batch  266 / 488  Training Loss  0.0001693501544650644\n",
            "Epoch  23 Batch  267 / 488  Training Loss  0.00010572500468697399\n",
            "Epoch  23 Batch  268 / 488  Training Loss  0.00011171503865625709\n",
            "Epoch  23 Batch  269 / 488  Training Loss  9.650263382354751e-05\n",
            "Epoch  23 Batch  270 / 488  Training Loss  0.00011899488890776411\n",
            "Epoch  23 Batch  271 / 488  Training Loss  0.00010953913442790508\n",
            "Epoch  23 Batch  272 / 488  Training Loss  7.890428969403729e-05\n",
            "Epoch  23 Batch  273 / 488  Training Loss  0.00012980365136172622\n",
            "Epoch  23 Batch  274 / 488  Training Loss  9.49634486460127e-05\n",
            "Epoch  23 Batch  275 / 488  Training Loss  0.0002097562828566879\n",
            "Epoch  23 Batch  276 / 488  Training Loss  0.000148430306580849\n",
            "Epoch  23 Batch  277 / 488  Training Loss  8.426294516539201e-05\n",
            "Epoch  23 Batch  278 / 488  Training Loss  8.949344919528812e-05\n",
            "Epoch  23 Batch  279 / 488  Training Loss  7.916570029919967e-05\n",
            "Epoch  23 Batch  280 / 488  Training Loss  0.001853004447184503\n",
            "Epoch  23 Batch  281 / 488  Training Loss  0.00011030827590730041\n",
            "Epoch  23 Batch  282 / 488  Training Loss  0.00017114603542722762\n",
            "Epoch  23 Batch  283 / 488  Training Loss  0.0002360375365242362\n",
            "Epoch  23 Batch  284 / 488  Training Loss  0.00014411141455639154\n",
            "Epoch  23 Batch  285 / 488  Training Loss  0.00016069832781795412\n",
            "Epoch  23 Batch  286 / 488  Training Loss  0.0001549988373881206\n",
            "Epoch  23 Batch  287 / 488  Training Loss  0.00015301939856726676\n",
            "Epoch  23 Batch  288 / 488  Training Loss  0.0001377334410790354\n",
            "Epoch  23 Batch  289 / 488  Training Loss  0.00013033008144702762\n",
            "Epoch  23 Batch  290 / 488  Training Loss  0.00014462821127381176\n",
            "Epoch  23 Batch  291 / 488  Training Loss  0.00011888805602211505\n",
            "Epoch  23 Batch  292 / 488  Training Loss  7.625261059729382e-05\n",
            "Epoch  23 Batch  293 / 488  Training Loss  9.257272176910192e-05\n",
            "Epoch  23 Batch  294 / 488  Training Loss  0.0001178296297439374\n",
            "Epoch  23 Batch  295 / 488  Training Loss  0.00010335069237044081\n",
            "Epoch  23 Batch  296 / 488  Training Loss  0.0001988278527278453\n",
            "Epoch  23 Batch  297 / 488  Training Loss  0.00013247059541754425\n",
            "Epoch  23 Batch  298 / 488  Training Loss  0.0001328263315372169\n",
            "Epoch  23 Batch  299 / 488  Training Loss  0.00015027585322968662\n",
            "Epoch  23 Batch  300 / 488  Training Loss  0.00013431720435619354\n",
            "Epoch  23 Batch  301 / 488  Training Loss  0.00012123248598072678\n",
            "Epoch  23 Batch  302 / 488  Training Loss  0.00012274367327336222\n",
            "Epoch  23 Batch  303 / 488  Training Loss  0.00018306713900528848\n",
            "Epoch  23 Batch  304 / 488  Training Loss  0.00016271103231702\n",
            "Epoch  23 Batch  305 / 488  Training Loss  0.00019271191558800638\n",
            "Epoch  23 Batch  306 / 488  Training Loss  9.374658111482859e-05\n",
            "Epoch  23 Batch  307 / 488  Training Loss  0.00019588689610827714\n",
            "Epoch  23 Batch  308 / 488  Training Loss  0.0001369763194816187\n",
            "Epoch  23 Batch  309 / 488  Training Loss  0.00015743293624836951\n",
            "Epoch  23 Batch  310 / 488  Training Loss  9.397086250828579e-05\n",
            "Epoch  23 Batch  311 / 488  Training Loss  0.00011336331954225898\n",
            "Epoch  23 Batch  312 / 488  Training Loss  0.0001758539874572307\n",
            "Epoch  23 Batch  313 / 488  Training Loss  0.00011787202674895525\n",
            "Epoch  23 Batch  314 / 488  Training Loss  0.00012573585263453424\n",
            "Epoch  23 Batch  315 / 488  Training Loss  0.00013705679157283157\n",
            "Epoch  23 Batch  316 / 488  Training Loss  0.00018225880921818316\n",
            "Epoch  23 Batch  317 / 488  Training Loss  8.16226311144419e-05\n",
            "Epoch  23 Batch  318 / 488  Training Loss  0.00020002065866719931\n",
            "Epoch  23 Batch  319 / 488  Training Loss  0.00021386177104432136\n",
            "Epoch  23 Batch  320 / 488  Training Loss  0.00012998988677281886\n",
            "Epoch  23 Batch  321 / 488  Training Loss  0.00011183255264768377\n",
            "Epoch  23 Batch  322 / 488  Training Loss  0.0001326290366705507\n",
            "Epoch  23 Batch  323 / 488  Training Loss  9.767798474058509e-05\n",
            "Epoch  23 Batch  324 / 488  Training Loss  0.0001509350404376164\n",
            "Epoch  23 Batch  325 / 488  Training Loss  0.00010684684093575925\n",
            "Epoch  23 Batch  326 / 488  Training Loss  0.00020134716760367155\n",
            "Epoch  23 Batch  327 / 488  Training Loss  0.0001530813897261396\n",
            "Epoch  23 Batch  328 / 488  Training Loss  0.00015570104005746543\n",
            "Epoch  23 Batch  329 / 488  Training Loss  0.00011048993474105373\n",
            "Epoch  23 Batch  330 / 488  Training Loss  0.00012072180106770247\n",
            "Epoch  23 Batch  331 / 488  Training Loss  0.00015728235302958637\n",
            "Epoch  23 Batch  332 / 488  Training Loss  9.079866140382364e-05\n",
            "Epoch  23 Batch  333 / 488  Training Loss  0.00010297675908077508\n",
            "Epoch  23 Batch  334 / 488  Training Loss  0.00012190137204015628\n",
            "Epoch  23 Batch  335 / 488  Training Loss  0.00014010722225066274\n",
            "Epoch  23 Batch  336 / 488  Training Loss  0.0001279512362089008\n",
            "Epoch  23 Batch  337 / 488  Training Loss  0.00014030907186679542\n",
            "Epoch  23 Batch  338 / 488  Training Loss  0.0001407438248861581\n",
            "Epoch  23 Batch  339 / 488  Training Loss  0.00014661019667983055\n",
            "Epoch  23 Batch  340 / 488  Training Loss  0.00010283159645041451\n",
            "Epoch  23 Batch  341 / 488  Training Loss  0.000129819949506782\n",
            "Epoch  23 Batch  342 / 488  Training Loss  0.00010926763934548944\n",
            "Epoch  23 Batch  343 / 488  Training Loss  0.00016769094509072602\n",
            "Epoch  23 Batch  344 / 488  Training Loss  0.00012421762221492827\n",
            "Epoch  23 Batch  345 / 488  Training Loss  0.0001462648215238005\n",
            "Epoch  23 Batch  346 / 488  Training Loss  0.0001724861649563536\n",
            "Epoch  23 Batch  347 / 488  Training Loss  0.00015448222984559834\n",
            "Epoch  23 Batch  348 / 488  Training Loss  0.00020997277169954032\n",
            "Epoch  23 Batch  349 / 488  Training Loss  0.00014562024443875998\n",
            "Epoch  23 Batch  350 / 488  Training Loss  0.00013301691797096282\n",
            "Epoch  23 Batch  351 / 488  Training Loss  0.0001254134112969041\n",
            "Epoch  23 Batch  352 / 488  Training Loss  0.00010613018093863502\n",
            "Epoch  23 Batch  353 / 488  Training Loss  0.00012534127745311707\n",
            "Epoch  23 Batch  354 / 488  Training Loss  9.119932656176388e-05\n",
            "Epoch  23 Batch  355 / 488  Training Loss  0.00012409068585839123\n",
            "Epoch  23 Batch  356 / 488  Training Loss  7.436042506014928e-05\n",
            "Epoch  23 Batch  357 / 488  Training Loss  0.00012430713104549795\n",
            "Epoch  23 Batch  358 / 488  Training Loss  0.00010490887507330626\n",
            "Epoch  23 Batch  359 / 488  Training Loss  0.00013553531607612967\n",
            "Epoch  23 Batch  360 / 488  Training Loss  9.452948870602995e-05\n",
            "Epoch  23 Batch  361 / 488  Training Loss  0.0001500084181316197\n",
            "Epoch  23 Batch  362 / 488  Training Loss  0.00016482750652357936\n",
            "Epoch  23 Batch  363 / 488  Training Loss  0.0001976680214283988\n",
            "Epoch  23 Batch  364 / 488  Training Loss  0.00010960871441056952\n",
            "Epoch  23 Batch  365 / 488  Training Loss  0.00011825167166534811\n",
            "Epoch  23 Batch  366 / 488  Training Loss  0.00010797069262480363\n",
            "Epoch  23 Batch  367 / 488  Training Loss  0.0002409000153420493\n",
            "Epoch  23 Batch  368 / 488  Training Loss  0.00012981664622202516\n",
            "Epoch  23 Batch  369 / 488  Training Loss  0.00011015142081305385\n",
            "Epoch  23 Batch  370 / 488  Training Loss  7.77796667534858e-05\n",
            "Epoch  23 Batch  371 / 488  Training Loss  0.0001501557562733069\n",
            "Epoch  23 Batch  372 / 488  Training Loss  0.0001279466086998582\n",
            "Epoch  23 Batch  373 / 488  Training Loss  0.00011580359569052234\n",
            "Epoch  23 Batch  374 / 488  Training Loss  0.0001410715631209314\n",
            "Epoch  23 Batch  375 / 488  Training Loss  0.0001828558451961726\n",
            "Epoch  23 Batch  376 / 488  Training Loss  0.00010994346666848287\n",
            "Epoch  23 Batch  377 / 488  Training Loss  0.00012864657037425786\n",
            "Epoch  23 Batch  378 / 488  Training Loss  0.00011494950013002381\n",
            "Epoch  23 Batch  379 / 488  Training Loss  0.00012494165275711566\n",
            "Epoch  23 Batch  380 / 488  Training Loss  0.00015481543960049748\n",
            "Epoch  23 Batch  381 / 488  Training Loss  0.00014168878260534257\n",
            "Epoch  23 Batch  382 / 488  Training Loss  0.0001280001742998138\n",
            "Epoch  23 Batch  383 / 488  Training Loss  0.00012022894952679053\n",
            "Epoch  23 Batch  384 / 488  Training Loss  6.934553675819188e-05\n",
            "Epoch  23 Batch  385 / 488  Training Loss  9.145579679170623e-05\n",
            "Epoch  23 Batch  386 / 488  Training Loss  0.00012878990673925728\n",
            "Epoch  23 Batch  387 / 488  Training Loss  0.00015241446089930832\n",
            "Epoch  23 Batch  388 / 488  Training Loss  0.00013701578427571803\n",
            "Epoch  23 Batch  389 / 488  Training Loss  0.00021751034364569932\n",
            "Epoch  23 Batch  390 / 488  Training Loss  0.00010920073691522703\n",
            "Epoch  23 Batch  391 / 488  Training Loss  0.00014489702880382538\n",
            "Epoch  23 Batch  392 / 488  Training Loss  0.00012502324534580112\n",
            "Epoch  23 Batch  393 / 488  Training Loss  9.56835865508765e-05\n",
            "Epoch  23 Batch  394 / 488  Training Loss  0.0001100471563404426\n",
            "Epoch  23 Batch  395 / 488  Training Loss  0.00013927197142038494\n",
            "Epoch  23 Batch  396 / 488  Training Loss  0.00014452615869231522\n",
            "Epoch  23 Batch  397 / 488  Training Loss  0.00015426117170136422\n",
            "Epoch  23 Batch  398 / 488  Training Loss  7.779119187034667e-05\n",
            "Epoch  23 Batch  399 / 488  Training Loss  8.850364247336984e-05\n",
            "Epoch  23 Batch  400 / 488  Training Loss  0.00011588435154408216\n",
            "Epoch  23 Batch  401 / 488  Training Loss  0.00015410370542667806\n",
            "Epoch  23 Batch  402 / 488  Training Loss  9.1223293566145e-05\n",
            "Epoch  23 Batch  403 / 488  Training Loss  0.00010635227954480797\n",
            "Epoch  23 Batch  404 / 488  Training Loss  8.710329711902887e-05\n",
            "Epoch  23 Batch  405 / 488  Training Loss  0.00013927652616985142\n",
            "Epoch  23 Batch  406 / 488  Training Loss  0.00012250427971594036\n",
            "Epoch  23 Batch  407 / 488  Training Loss  0.0021771143656224012\n",
            "Epoch  23 Batch  408 / 488  Training Loss  0.00011438232468208298\n",
            "Epoch  23 Batch  409 / 488  Training Loss  0.00013410877727437764\n",
            "Epoch  23 Batch  410 / 488  Training Loss  0.00014200339501257986\n",
            "Epoch  23 Batch  411 / 488  Training Loss  0.00018732012540567666\n",
            "Epoch  23 Batch  412 / 488  Training Loss  0.00012312244507484138\n",
            "Epoch  23 Batch  413 / 488  Training Loss  0.00016637366206850857\n",
            "Epoch  23 Batch  414 / 488  Training Loss  0.00010900841152761132\n",
            "Epoch  23 Batch  415 / 488  Training Loss  9.982056508306414e-05\n",
            "Epoch  23 Batch  416 / 488  Training Loss  0.00014543010911438614\n",
            "Epoch  23 Batch  417 / 488  Training Loss  0.00016885509830899537\n",
            "Epoch  23 Batch  418 / 488  Training Loss  0.0002010909083765\n",
            "Epoch  23 Batch  419 / 488  Training Loss  0.00014521638513542712\n",
            "Epoch  23 Batch  420 / 488  Training Loss  0.00010586879216134548\n",
            "Epoch  23 Batch  421 / 488  Training Loss  0.00015600852202624083\n",
            "Epoch  23 Batch  422 / 488  Training Loss  0.00016527884872630239\n",
            "Epoch  23 Batch  423 / 488  Training Loss  0.0001312924432568252\n",
            "Epoch  23 Batch  424 / 488  Training Loss  0.00019569919095374644\n",
            "Epoch  23 Batch  425 / 488  Training Loss  0.0001350600505247712\n",
            "Epoch  23 Batch  426 / 488  Training Loss  0.00014055721112526953\n",
            "Epoch  23 Batch  427 / 488  Training Loss  9.870919166132808e-05\n",
            "Epoch  23 Batch  428 / 488  Training Loss  0.00012466487532947212\n",
            "Epoch  23 Batch  429 / 488  Training Loss  0.00010695417586248368\n",
            "Epoch  23 Batch  430 / 488  Training Loss  0.0025273438077419996\n",
            "Epoch  23 Batch  431 / 488  Training Loss  0.00010414576536277309\n",
            "Epoch  23 Batch  432 / 488  Training Loss  0.00012807095481548458\n",
            "Epoch  23 Batch  433 / 488  Training Loss  0.00014368943811859936\n",
            "Epoch  23 Batch  434 / 488  Training Loss  0.0001318994036410004\n",
            "Epoch  23 Batch  435 / 488  Training Loss  0.00013426408986561\n",
            "Epoch  23 Batch  436 / 488  Training Loss  9.921664604917169e-05\n",
            "Epoch  23 Batch  437 / 488  Training Loss  0.00017269476666115224\n",
            "Epoch  23 Batch  438 / 488  Training Loss  0.00012759903620462865\n",
            "Epoch  23 Batch  439 / 488  Training Loss  0.0001264119055122137\n",
            "Epoch  23 Batch  440 / 488  Training Loss  0.000102599362435285\n",
            "Epoch  23 Batch  441 / 488  Training Loss  0.00013739622954744846\n",
            "Epoch  23 Batch  442 / 488  Training Loss  0.00012351009354460984\n",
            "Epoch  23 Batch  443 / 488  Training Loss  0.00020015616610180587\n",
            "Epoch  23 Batch  444 / 488  Training Loss  0.00011842111416626722\n",
            "Epoch  23 Batch  445 / 488  Training Loss  0.00014313129941001534\n",
            "Epoch  23 Batch  446 / 488  Training Loss  0.00013186639989726245\n",
            "Epoch  23 Batch  447 / 488  Training Loss  0.00015190322301350534\n",
            "Epoch  23 Batch  448 / 488  Training Loss  0.00016010861145332456\n",
            "Epoch  23 Batch  449 / 488  Training Loss  0.0001135927886934951\n",
            "Epoch  23 Batch  450 / 488  Training Loss  9.214464807882905e-05\n",
            "Epoch  23 Batch  451 / 488  Training Loss  0.00012203721416881308\n",
            "Epoch  23 Batch  452 / 488  Training Loss  0.00014628887583967298\n",
            "Epoch  23 Batch  453 / 488  Training Loss  0.0001575068017700687\n",
            "Epoch  23 Batch  454 / 488  Training Loss  0.00012206799874547869\n",
            "Epoch  23 Batch  455 / 488  Training Loss  8.533609798178077e-05\n",
            "Epoch  23 Batch  456 / 488  Training Loss  0.00010232866043224931\n",
            "Epoch  23 Batch  457 / 488  Training Loss  8.981217251857743e-05\n",
            "Epoch  23 Batch  458 / 488  Training Loss  0.00020899754599668086\n",
            "Epoch  23 Batch  459 / 488  Training Loss  8.256510045612231e-05\n",
            "Epoch  23 Batch  460 / 488  Training Loss  0.00011943172285100445\n",
            "Epoch  23 Batch  461 / 488  Training Loss  0.00011832142627099529\n",
            "Epoch  23 Batch  462 / 488  Training Loss  0.00016723531007301062\n",
            "Epoch  23 Batch  463 / 488  Training Loss  9.384604345541447e-05\n",
            "Epoch  23 Batch  464 / 488  Training Loss  0.00012454111129045486\n",
            "Epoch  23 Batch  465 / 488  Training Loss  0.00011684931814670563\n",
            "Epoch  23 Batch  466 / 488  Training Loss  0.00011739278852473944\n",
            "Epoch  23 Batch  467 / 488  Training Loss  8.872289618011564e-05\n",
            "Epoch  23 Batch  468 / 488  Training Loss  0.00011203611211385578\n",
            "Epoch  23 Batch  469 / 488  Training Loss  0.00010839216702152044\n",
            "Epoch  23 Batch  470 / 488  Training Loss  0.0001109302174882032\n",
            "Epoch  23 Batch  471 / 488  Training Loss  0.00013571098679676652\n",
            "Epoch  23 Batch  472 / 488  Training Loss  0.00013980778749100864\n",
            "Epoch  23 Batch  473 / 488  Training Loss  0.00014718172315042466\n",
            "Epoch  23 Batch  474 / 488  Training Loss  0.00010771608504001051\n",
            "Epoch  23 Batch  475 / 488  Training Loss  0.00013905514788348228\n",
            "Epoch  23 Batch  476 / 488  Training Loss  9.680043149273843e-05\n",
            "Epoch  23 Batch  477 / 488  Training Loss  0.0001699266431387514\n",
            "Epoch  23 Batch  478 / 488  Training Loss  0.00010335948172723874\n",
            "Epoch  23 Batch  479 / 488  Training Loss  0.00015174229338299483\n",
            "Epoch  23 Batch  480 / 488  Training Loss  9.164059156319126e-05\n",
            "Epoch  23 Batch  481 / 488  Training Loss  0.00014140897837933153\n",
            "Epoch  23 Batch  482 / 488  Training Loss  9.781275730347261e-05\n",
            "Epoch  23 Batch  483 / 488  Training Loss  0.00010343452595407143\n",
            "Epoch  23 Batch  484 / 488  Training Loss  0.00013115158071741462\n",
            "Epoch  23 Batch  485 / 488  Training Loss  9.993118146667257e-05\n",
            "Epoch  23 Batch  486 / 488  Training Loss  7.303524034796283e-05\n",
            "Epoch  23 Batch  487 / 488  Training Loss  4.865192750003189e-05\n",
            "  24    |    -    |   0.000160   | 47.152039\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 24\n",
            "Epoch  24 Batch  0 / 488  Training Loss  6.826227763667703e-05\n",
            "Epoch  24 Batch  1 / 488  Training Loss  8.271091064671054e-05\n",
            "Epoch  24 Batch  2 / 488  Training Loss  8.635157428216189e-05\n",
            "Epoch  24 Batch  3 / 488  Training Loss  6.94328555255197e-05\n",
            "Epoch  24 Batch  4 / 488  Training Loss  0.00010210723849013448\n",
            "Epoch  24 Batch  5 / 488  Training Loss  8.709418762009591e-05\n",
            "Epoch  24 Batch  6 / 488  Training Loss  5.6773995311232284e-05\n",
            "Epoch  24 Batch  7 / 488  Training Loss  9.129193495027721e-05\n",
            "Epoch  24 Batch  8 / 488  Training Loss  8.117832476273179e-05\n",
            "Epoch  24 Batch  9 / 488  Training Loss  9.855763346422464e-05\n",
            "Epoch  24 Batch  10 / 488  Training Loss  9.437333210371435e-05\n",
            "Epoch  24 Batch  11 / 488  Training Loss  9.755957580637187e-05\n",
            "Epoch  24 Batch  12 / 488  Training Loss  7.262495637405664e-05\n",
            "Epoch  24 Batch  13 / 488  Training Loss  9.556634904583916e-05\n",
            "Epoch  24 Batch  14 / 488  Training Loss  8.411942690145224e-05\n",
            "Epoch  24 Batch  15 / 488  Training Loss  0.00010709431080613285\n",
            "Epoch  24 Batch  16 / 488  Training Loss  0.00013171040336601436\n",
            "Epoch  24 Batch  17 / 488  Training Loss  0.00011043732229154557\n",
            "Epoch  24 Batch  18 / 488  Training Loss  0.00012052241072524339\n",
            "Epoch  24 Batch  19 / 488  Training Loss  0.00011542016000021249\n",
            "Epoch  24 Batch  20 / 488  Training Loss  7.85197044024244e-05\n",
            "Epoch  24 Batch  21 / 488  Training Loss  7.932014705147594e-05\n",
            "Epoch  24 Batch  22 / 488  Training Loss  0.00011452314356574789\n",
            "Epoch  24 Batch  23 / 488  Training Loss  8.693731797393411e-05\n",
            "Epoch  24 Batch  24 / 488  Training Loss  0.00010105896217282861\n",
            "Epoch  24 Batch  25 / 488  Training Loss  6.198943447088823e-05\n",
            "Epoch  24 Batch  26 / 488  Training Loss  7.341239688685164e-05\n",
            "Epoch  24 Batch  27 / 488  Training Loss  9.048126958077773e-05\n",
            "Epoch  24 Batch  28 / 488  Training Loss  9.165698429569602e-05\n",
            "Epoch  24 Batch  29 / 488  Training Loss  6.212814332684502e-05\n",
            "Epoch  24 Batch  30 / 488  Training Loss  9.591568232281134e-05\n",
            "Epoch  24 Batch  31 / 488  Training Loss  8.27348412713036e-05\n",
            "Epoch  24 Batch  32 / 488  Training Loss  7.73611682234332e-05\n",
            "Epoch  24 Batch  33 / 488  Training Loss  6.553526327479631e-05\n",
            "Epoch  24 Batch  34 / 488  Training Loss  0.0001021317220875062\n",
            "Epoch  24 Batch  35 / 488  Training Loss  0.0001573805493535474\n",
            "Epoch  24 Batch  36 / 488  Training Loss  8.057647937675938e-05\n",
            "Epoch  24 Batch  37 / 488  Training Loss  5.785269968328066e-05\n",
            "Epoch  24 Batch  38 / 488  Training Loss  9.493678226135671e-05\n",
            "Epoch  24 Batch  39 / 488  Training Loss  9.720078378450125e-05\n",
            "Epoch  24 Batch  40 / 488  Training Loss  9.182561188936234e-05\n",
            "Epoch  24 Batch  41 / 488  Training Loss  9.577137825544924e-05\n",
            "Epoch  24 Batch  42 / 488  Training Loss  0.00012104788038413972\n",
            "Epoch  24 Batch  43 / 488  Training Loss  8.917458035284653e-05\n",
            "Epoch  24 Batch  44 / 488  Training Loss  7.504734821850434e-05\n",
            "Epoch  24 Batch  45 / 488  Training Loss  8.185416663764045e-05\n",
            "Epoch  24 Batch  46 / 488  Training Loss  8.877982327248901e-05\n",
            "Epoch  24 Batch  47 / 488  Training Loss  7.930290303193033e-05\n",
            "Epoch  24 Batch  48 / 488  Training Loss  7.575470954179764e-05\n",
            "Epoch  24 Batch  49 / 488  Training Loss  6.360273255268112e-05\n",
            "Epoch  24 Batch  50 / 488  Training Loss  8.299802721012384e-05\n",
            "Epoch  24 Batch  51 / 488  Training Loss  8.591182995587587e-05\n",
            "Epoch  24 Batch  52 / 488  Training Loss  9.558810415910557e-05\n",
            "Epoch  24 Batch  53 / 488  Training Loss  7.638981333002448e-05\n",
            "Epoch  24 Batch  54 / 488  Training Loss  6.930682866368443e-05\n",
            "Epoch  24 Batch  55 / 488  Training Loss  9.599769691703841e-05\n",
            "Epoch  24 Batch  56 / 488  Training Loss  0.00013245863374322653\n",
            "Epoch  24 Batch  57 / 488  Training Loss  7.435803127009422e-05\n",
            "Epoch  24 Batch  58 / 488  Training Loss  0.00010439760808367282\n",
            "Epoch  24 Batch  59 / 488  Training Loss  9.0548608568497e-05\n",
            "Epoch  24 Batch  60 / 488  Training Loss  0.00013939020573161542\n",
            "Epoch  24 Batch  61 / 488  Training Loss  9.555403084959835e-05\n",
            "Epoch  24 Batch  62 / 488  Training Loss  6.023094829288311e-05\n",
            "Epoch  24 Batch  63 / 488  Training Loss  0.00010643996938597411\n",
            "Epoch  24 Batch  64 / 488  Training Loss  9.248814603779465e-05\n",
            "Epoch  24 Batch  65 / 488  Training Loss  6.288368604145944e-05\n",
            "Epoch  24 Batch  66 / 488  Training Loss  9.071765816770494e-05\n",
            "Epoch  24 Batch  67 / 488  Training Loss  0.0001041673676809296\n",
            "Epoch  24 Batch  68 / 488  Training Loss  8.544848242308944e-05\n",
            "Epoch  24 Batch  69 / 488  Training Loss  6.272095197346061e-05\n",
            "Epoch  24 Batch  70 / 488  Training Loss  6.465714977821335e-05\n",
            "Epoch  24 Batch  71 / 488  Training Loss  9.70582987065427e-05\n",
            "Epoch  24 Batch  72 / 488  Training Loss  8.770425483817235e-05\n",
            "Epoch  24 Batch  73 / 488  Training Loss  9.01344683370553e-05\n",
            "Epoch  24 Batch  74 / 488  Training Loss  8.191788947442546e-05\n",
            "Epoch  24 Batch  75 / 488  Training Loss  8.786304533714429e-05\n",
            "Epoch  24 Batch  76 / 488  Training Loss  0.00014489142631646246\n",
            "Epoch  24 Batch  77 / 488  Training Loss  8.466786675853655e-05\n",
            "Epoch  24 Batch  78 / 488  Training Loss  9.191812569042668e-05\n",
            "Epoch  24 Batch  79 / 488  Training Loss  8.69519790285267e-05\n",
            "Epoch  24 Batch  80 / 488  Training Loss  0.00010720394493546337\n",
            "Epoch  24 Batch  81 / 488  Training Loss  6.620489148190245e-05\n",
            "Epoch  24 Batch  82 / 488  Training Loss  0.0001341944298474118\n",
            "Epoch  24 Batch  83 / 488  Training Loss  6.456815026467666e-05\n",
            "Epoch  24 Batch  84 / 488  Training Loss  7.172298501245677e-05\n",
            "Epoch  24 Batch  85 / 488  Training Loss  0.00011066260049119592\n",
            "Epoch  24 Batch  86 / 488  Training Loss  0.00011807792907347903\n",
            "Epoch  24 Batch  87 / 488  Training Loss  6.697812932543457e-05\n",
            "Epoch  24 Batch  88 / 488  Training Loss  0.00012923941540066153\n",
            "Epoch  24 Batch  89 / 488  Training Loss  8.828509453451261e-05\n",
            "Epoch  24 Batch  90 / 488  Training Loss  9.026177576743066e-05\n",
            "Epoch  24 Batch  91 / 488  Training Loss  0.00014052084588911384\n",
            "Epoch  24 Batch  92 / 488  Training Loss  7.390135579043999e-05\n",
            "Epoch  24 Batch  93 / 488  Training Loss  6.742816185578704e-05\n",
            "Epoch  24 Batch  94 / 488  Training Loss  0.00010549298895057291\n",
            "Epoch  24 Batch  95 / 488  Training Loss  0.00012663891538977623\n",
            "Epoch  24 Batch  96 / 488  Training Loss  8.313840953633189e-05\n",
            "Epoch  24 Batch  97 / 488  Training Loss  0.00010224889410892501\n",
            "Epoch  24 Batch  98 / 488  Training Loss  6.889305223012343e-05\n",
            "Epoch  24 Batch  99 / 488  Training Loss  7.799017475917935e-05\n",
            "Epoch  24 Batch  100 / 488  Training Loss  0.00010851059050764889\n",
            "Epoch  24 Batch  101 / 488  Training Loss  9.998885070672259e-05\n",
            "Epoch  24 Batch  102 / 488  Training Loss  7.295539398910478e-05\n",
            "Epoch  24 Batch  103 / 488  Training Loss  5.9795180277433246e-05\n",
            "Epoch  24 Batch  104 / 488  Training Loss  6.767844024579972e-05\n",
            "Epoch  24 Batch  105 / 488  Training Loss  0.00010239813855150715\n",
            "Epoch  24 Batch  106 / 488  Training Loss  9.530788520351052e-05\n",
            "Epoch  24 Batch  107 / 488  Training Loss  0.00010708670743042603\n",
            "Epoch  24 Batch  108 / 488  Training Loss  0.00010971466690534726\n",
            "Epoch  24 Batch  109 / 488  Training Loss  7.052195724099874e-05\n",
            "Epoch  24 Batch  110 / 488  Training Loss  9.768664313014597e-05\n",
            "Epoch  24 Batch  111 / 488  Training Loss  9.727649739943445e-05\n",
            "Epoch  24 Batch  112 / 488  Training Loss  6.68560533085838e-05\n",
            "Epoch  24 Batch  113 / 488  Training Loss  8.463188714813441e-05\n",
            "Epoch  24 Batch  114 / 488  Training Loss  7.702309812884778e-05\n",
            "Epoch  24 Batch  115 / 488  Training Loss  9.225118992617354e-05\n",
            "Epoch  24 Batch  116 / 488  Training Loss  9.851660433923826e-05\n",
            "Epoch  24 Batch  117 / 488  Training Loss  7.94165680417791e-05\n",
            "Epoch  24 Batch  118 / 488  Training Loss  9.972663974622265e-05\n",
            "Epoch  24 Batch  119 / 488  Training Loss  0.00010701116116251796\n",
            "Epoch  24 Batch  120 / 488  Training Loss  9.858474368229508e-05\n",
            "Epoch  24 Batch  121 / 488  Training Loss  7.67700548749417e-05\n",
            "Epoch  24 Batch  122 / 488  Training Loss  7.338773139053956e-05\n",
            "Epoch  24 Batch  123 / 488  Training Loss  8.8706765382085e-05\n",
            "Epoch  24 Batch  124 / 488  Training Loss  0.00011096242815256119\n",
            "Epoch  24 Batch  125 / 488  Training Loss  6.87963402015157e-05\n",
            "Epoch  24 Batch  126 / 488  Training Loss  6.425671017495915e-05\n",
            "Epoch  24 Batch  127 / 488  Training Loss  6.620564818149433e-05\n",
            "Epoch  24 Batch  128 / 488  Training Loss  8.380386861972511e-05\n",
            "Epoch  24 Batch  129 / 488  Training Loss  9.453994425712153e-05\n",
            "Epoch  24 Batch  130 / 488  Training Loss  7.406953227473423e-05\n",
            "Epoch  24 Batch  131 / 488  Training Loss  8.091104973573238e-05\n",
            "Epoch  24 Batch  132 / 488  Training Loss  6.788300379412249e-05\n",
            "Epoch  24 Batch  133 / 488  Training Loss  0.00010643783025443554\n",
            "Epoch  24 Batch  134 / 488  Training Loss  0.00010834010026883334\n",
            "Epoch  24 Batch  135 / 488  Training Loss  8.295715815620497e-05\n",
            "Epoch  24 Batch  136 / 488  Training Loss  6.834744999650866e-05\n",
            "Epoch  24 Batch  137 / 488  Training Loss  0.0001438222243450582\n",
            "Epoch  24 Batch  138 / 488  Training Loss  6.287569703999907e-05\n",
            "Epoch  24 Batch  139 / 488  Training Loss  9.181802306557074e-05\n",
            "Epoch  24 Batch  140 / 488  Training Loss  6.757888331776485e-05\n",
            "Epoch  24 Batch  141 / 488  Training Loss  0.00011107968020951375\n",
            "Epoch  24 Batch  142 / 488  Training Loss  8.241146133514121e-05\n",
            "Epoch  24 Batch  143 / 488  Training Loss  7.432796701323241e-05\n",
            "Epoch  24 Batch  144 / 488  Training Loss  0.0001362788025289774\n",
            "Epoch  24 Batch  145 / 488  Training Loss  9.039512951858342e-05\n",
            "Epoch  24 Batch  146 / 488  Training Loss  0.00010311133519280702\n",
            "Epoch  24 Batch  147 / 488  Training Loss  0.00011115315282950178\n",
            "Epoch  24 Batch  148 / 488  Training Loss  9.023322490975261e-05\n",
            "Epoch  24 Batch  149 / 488  Training Loss  8.910009637475014e-05\n",
            "Epoch  24 Batch  150 / 488  Training Loss  0.001731273951008916\n",
            "Epoch  24 Batch  151 / 488  Training Loss  9.472638339502737e-05\n",
            "Epoch  24 Batch  152 / 488  Training Loss  0.00011247610382270068\n",
            "Epoch  24 Batch  153 / 488  Training Loss  7.531308801844716e-05\n",
            "Epoch  24 Batch  154 / 488  Training Loss  6.629170093219727e-05\n",
            "Epoch  24 Batch  155 / 488  Training Loss  7.647612073924392e-05\n",
            "Epoch  24 Batch  156 / 488  Training Loss  0.00013525168469641358\n",
            "Epoch  24 Batch  157 / 488  Training Loss  7.724673196207732e-05\n",
            "Epoch  24 Batch  158 / 488  Training Loss  9.836460230872035e-05\n",
            "Epoch  24 Batch  159 / 488  Training Loss  8.959483238868415e-05\n",
            "Epoch  24 Batch  160 / 488  Training Loss  0.00012096973659936339\n",
            "Epoch  24 Batch  161 / 488  Training Loss  0.00010067776020150632\n",
            "Epoch  24 Batch  162 / 488  Training Loss  0.00010339914297219366\n",
            "Epoch  24 Batch  163 / 488  Training Loss  0.00011713297863025218\n",
            "Epoch  24 Batch  164 / 488  Training Loss  7.699635170865804e-05\n",
            "Epoch  24 Batch  165 / 488  Training Loss  0.00010235357331112027\n",
            "Epoch  24 Batch  166 / 488  Training Loss  7.910733256721869e-05\n",
            "Epoch  24 Batch  167 / 488  Training Loss  8.18678381619975e-05\n",
            "Epoch  24 Batch  168 / 488  Training Loss  8.903192792786285e-05\n",
            "Epoch  24 Batch  169 / 488  Training Loss  0.00013562399544753134\n",
            "Epoch  24 Batch  170 / 488  Training Loss  0.00011411322338972241\n",
            "Epoch  24 Batch  171 / 488  Training Loss  8.80713778315112e-05\n",
            "Epoch  24 Batch  172 / 488  Training Loss  9.041485463967547e-05\n",
            "Epoch  24 Batch  173 / 488  Training Loss  0.00012576013978105038\n",
            "Epoch  24 Batch  174 / 488  Training Loss  6.922505417605862e-05\n",
            "Epoch  24 Batch  175 / 488  Training Loss  6.399222183972597e-05\n",
            "Epoch  24 Batch  176 / 488  Training Loss  7.417249435093254e-05\n",
            "Epoch  24 Batch  177 / 488  Training Loss  8.856444037519395e-05\n",
            "Epoch  24 Batch  178 / 488  Training Loss  0.0001032071522786282\n",
            "Epoch  24 Batch  179 / 488  Training Loss  0.00010609624587232247\n",
            "Epoch  24 Batch  180 / 488  Training Loss  6.215656321728602e-05\n",
            "Epoch  24 Batch  181 / 488  Training Loss  9.49678651522845e-05\n",
            "Epoch  24 Batch  182 / 488  Training Loss  9.993834100896493e-05\n",
            "Epoch  24 Batch  183 / 488  Training Loss  6.434073293348774e-05\n",
            "Epoch  24 Batch  184 / 488  Training Loss  0.0001816745789255947\n",
            "Epoch  24 Batch  185 / 488  Training Loss  0.00010302044393029064\n",
            "Epoch  24 Batch  186 / 488  Training Loss  0.0001223627186845988\n",
            "Epoch  24 Batch  187 / 488  Training Loss  9.144424984697253e-05\n",
            "Epoch  24 Batch  188 / 488  Training Loss  0.00011497763625811785\n",
            "Epoch  24 Batch  189 / 488  Training Loss  8.117768447846174e-05\n",
            "Epoch  24 Batch  190 / 488  Training Loss  9.760069224284962e-05\n",
            "Epoch  24 Batch  191 / 488  Training Loss  8.787684782873839e-05\n",
            "Epoch  24 Batch  192 / 488  Training Loss  6.834310624981299e-05\n",
            "Epoch  24 Batch  193 / 488  Training Loss  8.440586680080742e-05\n",
            "Epoch  24 Batch  194 / 488  Training Loss  7.338661089306697e-05\n",
            "Epoch  24 Batch  195 / 488  Training Loss  5.969926860416308e-05\n",
            "Epoch  24 Batch  196 / 488  Training Loss  6.252295861486346e-05\n",
            "Epoch  24 Batch  197 / 488  Training Loss  0.00012047154450556263\n",
            "Epoch  24 Batch  198 / 488  Training Loss  7.927707338239998e-05\n",
            "Epoch  24 Batch  199 / 488  Training Loss  7.939391798572615e-05\n",
            "Epoch  24 Batch  200 / 488  Training Loss  7.925416866783053e-05\n",
            "Epoch  24 Batch  201 / 488  Training Loss  7.745187031105161e-05\n",
            "Epoch  24 Batch  202 / 488  Training Loss  0.00010941986693069339\n",
            "Epoch  24 Batch  203 / 488  Training Loss  0.000319277576636523\n",
            "Epoch  24 Batch  204 / 488  Training Loss  9.235939069185406e-05\n",
            "Epoch  24 Batch  205 / 488  Training Loss  0.00011175370309501886\n",
            "Epoch  24 Batch  206 / 488  Training Loss  0.00011711629485944286\n",
            "Epoch  24 Batch  207 / 488  Training Loss  0.00012008998601231724\n",
            "Epoch  24 Batch  208 / 488  Training Loss  8.305830124299973e-05\n",
            "Epoch  24 Batch  209 / 488  Training Loss  9.637154289521277e-05\n",
            "Epoch  24 Batch  210 / 488  Training Loss  0.00011466967407613993\n",
            "Epoch  24 Batch  211 / 488  Training Loss  0.00010524821846047416\n",
            "Epoch  24 Batch  212 / 488  Training Loss  9.04275439097546e-05\n",
            "Epoch  24 Batch  213 / 488  Training Loss  8.209153020288795e-05\n",
            "Epoch  24 Batch  214 / 488  Training Loss  7.988058496266603e-05\n",
            "Epoch  24 Batch  215 / 488  Training Loss  0.00011711004481185228\n",
            "Epoch  24 Batch  216 / 488  Training Loss  5.888139639864676e-05\n",
            "Epoch  24 Batch  217 / 488  Training Loss  0.0002173214452341199\n",
            "Epoch  24 Batch  218 / 488  Training Loss  5.968112236587331e-05\n",
            "Epoch  24 Batch  219 / 488  Training Loss  0.00012392422650009394\n",
            "Epoch  24 Batch  220 / 488  Training Loss  0.00011502850247779861\n",
            "Epoch  24 Batch  221 / 488  Training Loss  8.313953730976209e-05\n",
            "Epoch  24 Batch  222 / 488  Training Loss  0.00012507036444731057\n",
            "Epoch  24 Batch  223 / 488  Training Loss  9.651198342908174e-05\n",
            "Epoch  24 Batch  224 / 488  Training Loss  0.0001287439081352204\n",
            "Epoch  24 Batch  225 / 488  Training Loss  8.572115621063858e-05\n",
            "Epoch  24 Batch  226 / 488  Training Loss  0.000130813306896016\n",
            "Epoch  24 Batch  227 / 488  Training Loss  9.376400703331456e-05\n",
            "Epoch  24 Batch  228 / 488  Training Loss  9.32127222768031e-05\n",
            "Epoch  24 Batch  229 / 488  Training Loss  0.00012076985876774415\n",
            "Epoch  24 Batch  230 / 488  Training Loss  8.962347055785358e-05\n",
            "Epoch  24 Batch  231 / 488  Training Loss  9.042541205417365e-05\n",
            "Epoch  24 Batch  232 / 488  Training Loss  7.683023432036862e-05\n",
            "Epoch  24 Batch  233 / 488  Training Loss  7.980952796060592e-05\n",
            "Epoch  24 Batch  234 / 488  Training Loss  0.00010063770605484024\n",
            "Epoch  24 Batch  235 / 488  Training Loss  0.00011823482054751366\n",
            "Epoch  24 Batch  236 / 488  Training Loss  7.310087676160038e-05\n",
            "Epoch  24 Batch  237 / 488  Training Loss  0.00013980382937006652\n",
            "Epoch  24 Batch  238 / 488  Training Loss  0.00010406946239527315\n",
            "Epoch  24 Batch  239 / 488  Training Loss  8.10940473456867e-05\n",
            "Epoch  24 Batch  240 / 488  Training Loss  0.00014564982848241925\n",
            "Epoch  24 Batch  241 / 488  Training Loss  9.575219883117825e-05\n",
            "Epoch  24 Batch  242 / 488  Training Loss  9.926773782353848e-05\n",
            "Epoch  24 Batch  243 / 488  Training Loss  8.326399984071031e-05\n",
            "Epoch  24 Batch  244 / 488  Training Loss  9.584029612597078e-05\n",
            "Epoch  24 Batch  245 / 488  Training Loss  0.00010958446364384145\n",
            "Epoch  24 Batch  246 / 488  Training Loss  0.0001183835556730628\n",
            "Epoch  24 Batch  247 / 488  Training Loss  0.00010910366108873859\n",
            "Epoch  24 Batch  248 / 488  Training Loss  9.237133053829893e-05\n",
            "Epoch  24 Batch  249 / 488  Training Loss  0.00010943129746010527\n",
            "Epoch  24 Batch  250 / 488  Training Loss  6.721102545270696e-05\n",
            "Epoch  24 Batch  251 / 488  Training Loss  8.614883699920028e-05\n",
            "Epoch  24 Batch  252 / 488  Training Loss  7.0369424065575e-05\n",
            "Epoch  24 Batch  253 / 488  Training Loss  8.429949230048805e-05\n",
            "Epoch  24 Batch  254 / 488  Training Loss  0.00010425350774312392\n",
            "Epoch  24 Batch  255 / 488  Training Loss  6.286613643169403e-05\n",
            "Epoch  24 Batch  256 / 488  Training Loss  5.169182986719534e-05\n",
            "Epoch  24 Batch  257 / 488  Training Loss  9.880973811959848e-05\n",
            "Epoch  24 Batch  258 / 488  Training Loss  0.00011488409654702991\n",
            "Epoch  24 Batch  259 / 488  Training Loss  7.611718319822103e-05\n",
            "Epoch  24 Batch  260 / 488  Training Loss  0.00011544192966539413\n",
            "Epoch  24 Batch  261 / 488  Training Loss  0.00012170909758424386\n",
            "Epoch  24 Batch  262 / 488  Training Loss  0.0001222875580424443\n",
            "Epoch  24 Batch  263 / 488  Training Loss  7.940566865727305e-05\n",
            "Epoch  24 Batch  264 / 488  Training Loss  5.9891812270507216e-05\n",
            "Epoch  24 Batch  265 / 488  Training Loss  8.890689059626311e-05\n",
            "Epoch  24 Batch  266 / 488  Training Loss  8.190777589334175e-05\n",
            "Epoch  24 Batch  267 / 488  Training Loss  7.569226727355272e-05\n",
            "Epoch  24 Batch  268 / 488  Training Loss  9.698969370219857e-05\n",
            "Epoch  24 Batch  269 / 488  Training Loss  6.502550968434662e-05\n",
            "Epoch  24 Batch  270 / 488  Training Loss  9.836438402999192e-05\n",
            "Epoch  24 Batch  271 / 488  Training Loss  9.075031994143501e-05\n",
            "Epoch  24 Batch  272 / 488  Training Loss  0.00011910778994206339\n",
            "Epoch  24 Batch  273 / 488  Training Loss  9.336900257039815e-05\n",
            "Epoch  24 Batch  274 / 488  Training Loss  7.433279824908823e-05\n",
            "Epoch  24 Batch  275 / 488  Training Loss  0.00011147089389851317\n",
            "Epoch  24 Batch  276 / 488  Training Loss  7.237043610075489e-05\n",
            "Epoch  24 Batch  277 / 488  Training Loss  0.00011158084816997871\n",
            "Epoch  24 Batch  278 / 488  Training Loss  7.172835466917604e-05\n",
            "Epoch  24 Batch  279 / 488  Training Loss  9.983265772461891e-05\n",
            "Epoch  24 Batch  280 / 488  Training Loss  8.571762009523809e-05\n",
            "Epoch  24 Batch  281 / 488  Training Loss  0.00011347789404680952\n",
            "Epoch  24 Batch  282 / 488  Training Loss  6.263124669203535e-05\n",
            "Epoch  24 Batch  283 / 488  Training Loss  9.33685660129413e-05\n",
            "Epoch  24 Batch  284 / 488  Training Loss  7.204544090200216e-05\n",
            "Epoch  24 Batch  285 / 488  Training Loss  9.909681102726609e-05\n",
            "Epoch  24 Batch  286 / 488  Training Loss  0.00010334102262277156\n",
            "Epoch  24 Batch  287 / 488  Training Loss  9.652695007389411e-05\n",
            "Epoch  24 Batch  288 / 488  Training Loss  0.0001069802456186153\n",
            "Epoch  24 Batch  289 / 488  Training Loss  7.932753942441195e-05\n",
            "Epoch  24 Batch  290 / 488  Training Loss  9.174831211566925e-05\n",
            "Epoch  24 Batch  291 / 488  Training Loss  0.00010702879808377475\n",
            "Epoch  24 Batch  292 / 488  Training Loss  8.865610288921744e-05\n",
            "Epoch  24 Batch  293 / 488  Training Loss  6.58801436657086e-05\n",
            "Epoch  24 Batch  294 / 488  Training Loss  7.370392268057913e-05\n",
            "Epoch  24 Batch  295 / 488  Training Loss  9.496099664829671e-05\n",
            "Epoch  24 Batch  296 / 488  Training Loss  8.399504440603778e-05\n",
            "Epoch  24 Batch  297 / 488  Training Loss  8.013724436750636e-05\n",
            "Epoch  24 Batch  298 / 488  Training Loss  0.00011191301746293902\n",
            "Epoch  24 Batch  299 / 488  Training Loss  0.00010061731154564768\n",
            "Epoch  24 Batch  300 / 488  Training Loss  8.544341835658997e-05\n",
            "Epoch  24 Batch  301 / 488  Training Loss  0.0001103418180719018\n",
            "Epoch  24 Batch  302 / 488  Training Loss  0.00010102111991727725\n",
            "Epoch  24 Batch  303 / 488  Training Loss  9.307372965849936e-05\n",
            "Epoch  24 Batch  304 / 488  Training Loss  8.73997196322307e-05\n",
            "Epoch  24 Batch  305 / 488  Training Loss  0.00010905273666139692\n",
            "Epoch  24 Batch  306 / 488  Training Loss  0.00010270408529322594\n",
            "Epoch  24 Batch  307 / 488  Training Loss  5.0579295930219814e-05\n",
            "Epoch  24 Batch  308 / 488  Training Loss  7.220056431833655e-05\n",
            "Epoch  24 Batch  309 / 488  Training Loss  0.00011605038162088022\n",
            "Epoch  24 Batch  310 / 488  Training Loss  0.0001228817563969642\n",
            "Epoch  24 Batch  311 / 488  Training Loss  5.840554877067916e-05\n",
            "Epoch  24 Batch  312 / 488  Training Loss  8.573076775064692e-05\n",
            "Epoch  24 Batch  313 / 488  Training Loss  9.144386422121897e-05\n",
            "Epoch  24 Batch  314 / 488  Training Loss  8.969976624939591e-05\n",
            "Epoch  24 Batch  315 / 488  Training Loss  0.00012785792932845652\n",
            "Epoch  24 Batch  316 / 488  Training Loss  0.00010725615720730275\n",
            "Epoch  24 Batch  317 / 488  Training Loss  7.979664951562881e-05\n",
            "Epoch  24 Batch  318 / 488  Training Loss  9.757076622918248e-05\n",
            "Epoch  24 Batch  319 / 488  Training Loss  8.917813829611987e-05\n",
            "Epoch  24 Batch  320 / 488  Training Loss  5.0206763262394816e-05\n",
            "Epoch  24 Batch  321 / 488  Training Loss  8.532438369002193e-05\n",
            "Epoch  24 Batch  322 / 488  Training Loss  9.040639270097017e-05\n",
            "Epoch  24 Batch  323 / 488  Training Loss  8.91497838892974e-05\n",
            "Epoch  24 Batch  324 / 488  Training Loss  9.683395182946697e-05\n",
            "Epoch  24 Batch  325 / 488  Training Loss  6.415982352336869e-05\n",
            "Epoch  24 Batch  326 / 488  Training Loss  9.513572149444371e-05\n",
            "Epoch  24 Batch  327 / 488  Training Loss  0.00012264908582437783\n",
            "Epoch  24 Batch  328 / 488  Training Loss  7.44494391256012e-05\n",
            "Epoch  24 Batch  329 / 488  Training Loss  9.680912626208737e-05\n",
            "Epoch  24 Batch  330 / 488  Training Loss  8.578757115174085e-05\n",
            "Epoch  24 Batch  331 / 488  Training Loss  0.00010687766189221293\n",
            "Epoch  24 Batch  332 / 488  Training Loss  7.513521268265322e-05\n",
            "Epoch  24 Batch  333 / 488  Training Loss  7.886845560278744e-05\n",
            "Epoch  24 Batch  334 / 488  Training Loss  8.836145570967346e-05\n",
            "Epoch  24 Batch  335 / 488  Training Loss  7.372211257461458e-05\n",
            "Epoch  24 Batch  336 / 488  Training Loss  0.00010158678924199194\n",
            "Epoch  24 Batch  337 / 488  Training Loss  0.00012435992539394647\n",
            "Epoch  24 Batch  338 / 488  Training Loss  8.018089283723384e-05\n",
            "Epoch  24 Batch  339 / 488  Training Loss  6.875784310977906e-05\n",
            "Epoch  24 Batch  340 / 488  Training Loss  9.277721255784854e-05\n",
            "Epoch  24 Batch  341 / 488  Training Loss  0.00010375728743383661\n",
            "Epoch  24 Batch  342 / 488  Training Loss  9.364356083096936e-05\n",
            "Epoch  24 Batch  343 / 488  Training Loss  8.404268010053784e-05\n",
            "Epoch  24 Batch  344 / 488  Training Loss  7.951539009809494e-05\n",
            "Epoch  24 Batch  345 / 488  Training Loss  9.476613195147365e-05\n",
            "Epoch  24 Batch  346 / 488  Training Loss  7.229081529658288e-05\n",
            "Epoch  24 Batch  347 / 488  Training Loss  9.09410446183756e-05\n",
            "Epoch  24 Batch  348 / 488  Training Loss  9.816619422053918e-05\n",
            "Epoch  24 Batch  349 / 488  Training Loss  7.949252903927118e-05\n",
            "Epoch  24 Batch  350 / 488  Training Loss  7.341986201936379e-05\n",
            "Epoch  24 Batch  351 / 488  Training Loss  0.0001525494735687971\n",
            "Epoch  24 Batch  352 / 488  Training Loss  0.00012637386680580676\n",
            "Epoch  24 Batch  353 / 488  Training Loss  0.00010120410297531635\n",
            "Epoch  24 Batch  354 / 488  Training Loss  8.807642007013783e-05\n",
            "Epoch  24 Batch  355 / 488  Training Loss  8.503992285113782e-05\n",
            "Epoch  24 Batch  356 / 488  Training Loss  9.589972614776343e-05\n",
            "Epoch  24 Batch  357 / 488  Training Loss  6.380324339261279e-05\n",
            "Epoch  24 Batch  358 / 488  Training Loss  9.782563574844971e-05\n",
            "Epoch  24 Batch  359 / 488  Training Loss  9.030856017488986e-05\n",
            "Epoch  24 Batch  360 / 488  Training Loss  8.655201236251742e-05\n",
            "Epoch  24 Batch  361 / 488  Training Loss  8.506336598657072e-05\n",
            "Epoch  24 Batch  362 / 488  Training Loss  9.230444265995175e-05\n",
            "Epoch  24 Batch  363 / 488  Training Loss  5.868497464689426e-05\n",
            "Epoch  24 Batch  364 / 488  Training Loss  7.615963113494217e-05\n",
            "Epoch  24 Batch  365 / 488  Training Loss  8.667374640936032e-05\n",
            "Epoch  24 Batch  366 / 488  Training Loss  7.337901479331776e-05\n",
            "Epoch  24 Batch  367 / 488  Training Loss  7.587978325318545e-05\n",
            "Epoch  24 Batch  368 / 488  Training Loss  8.180031727533787e-05\n",
            "Epoch  24 Batch  369 / 488  Training Loss  6.937558646313846e-05\n",
            "Epoch  24 Batch  370 / 488  Training Loss  7.073808228597045e-05\n",
            "Epoch  24 Batch  371 / 488  Training Loss  8.71155207278207e-05\n",
            "Epoch  24 Batch  372 / 488  Training Loss  8.469918975606561e-05\n",
            "Epoch  24 Batch  373 / 488  Training Loss  0.00010513655433896929\n",
            "Epoch  24 Batch  374 / 488  Training Loss  9.155397128779441e-05\n",
            "Epoch  24 Batch  375 / 488  Training Loss  9.912706445902586e-05\n",
            "Epoch  24 Batch  376 / 488  Training Loss  9.516237332718447e-05\n",
            "Epoch  24 Batch  377 / 488  Training Loss  0.00012896386033389717\n",
            "Epoch  24 Batch  378 / 488  Training Loss  7.74568397901021e-05\n",
            "Epoch  24 Batch  379 / 488  Training Loss  9.216954640578479e-05\n",
            "Epoch  24 Batch  380 / 488  Training Loss  0.00010287111217621714\n",
            "Epoch  24 Batch  381 / 488  Training Loss  7.313821697607636e-05\n",
            "Epoch  24 Batch  382 / 488  Training Loss  0.00010056819155579433\n",
            "Epoch  24 Batch  383 / 488  Training Loss  6.511309038614854e-05\n",
            "Epoch  24 Batch  384 / 488  Training Loss  0.00013430521357804537\n",
            "Epoch  24 Batch  385 / 488  Training Loss  0.00011469883611425757\n",
            "Epoch  24 Batch  386 / 488  Training Loss  0.00011367723345756531\n",
            "Epoch  24 Batch  387 / 488  Training Loss  7.354279659921303e-05\n",
            "Epoch  24 Batch  388 / 488  Training Loss  6.242514791665599e-05\n",
            "Epoch  24 Batch  389 / 488  Training Loss  9.06405330169946e-05\n",
            "Epoch  24 Batch  390 / 488  Training Loss  5.553053779294714e-05\n",
            "Epoch  24 Batch  391 / 488  Training Loss  0.00010906744137173519\n",
            "Epoch  24 Batch  392 / 488  Training Loss  9.455302642891183e-05\n",
            "Epoch  24 Batch  393 / 488  Training Loss  7.316043775063008e-05\n",
            "Epoch  24 Batch  394 / 488  Training Loss  8.931761112762615e-05\n",
            "Epoch  24 Batch  395 / 488  Training Loss  9.310063614975661e-05\n",
            "Epoch  24 Batch  396 / 488  Training Loss  0.00010118789214175195\n",
            "Epoch  24 Batch  397 / 488  Training Loss  6.143328209873289e-05\n",
            "Epoch  24 Batch  398 / 488  Training Loss  9.386902820551768e-05\n",
            "Epoch  24 Batch  399 / 488  Training Loss  9.759964450495318e-05\n",
            "Epoch  24 Batch  400 / 488  Training Loss  0.0001143932095146738\n",
            "Epoch  24 Batch  401 / 488  Training Loss  7.509033457608894e-05\n",
            "Epoch  24 Batch  402 / 488  Training Loss  0.00010903069050982594\n",
            "Epoch  24 Batch  403 / 488  Training Loss  0.00010021469643106684\n",
            "Epoch  24 Batch  404 / 488  Training Loss  8.645025809528306e-05\n",
            "Epoch  24 Batch  405 / 488  Training Loss  8.923826680984348e-05\n",
            "Epoch  24 Batch  406 / 488  Training Loss  8.125101157929748e-05\n",
            "Epoch  24 Batch  407 / 488  Training Loss  7.569281297037378e-05\n",
            "Epoch  24 Batch  408 / 488  Training Loss  0.00011656216520350426\n",
            "Epoch  24 Batch  409 / 488  Training Loss  6.37590856058523e-05\n",
            "Epoch  24 Batch  410 / 488  Training Loss  0.00010295080573996529\n",
            "Epoch  24 Batch  411 / 488  Training Loss  6.866787589387968e-05\n",
            "Epoch  24 Batch  412 / 488  Training Loss  8.834776963340119e-05\n",
            "Epoch  24 Batch  413 / 488  Training Loss  8.993304072646424e-05\n",
            "Epoch  24 Batch  414 / 488  Training Loss  6.770616892026737e-05\n",
            "Epoch  24 Batch  415 / 488  Training Loss  8.583760063629597e-05\n",
            "Epoch  24 Batch  416 / 488  Training Loss  8.427165448665619e-05\n",
            "Epoch  24 Batch  417 / 488  Training Loss  7.669300975976512e-05\n",
            "Epoch  24 Batch  418 / 488  Training Loss  8.221027383115143e-05\n",
            "Epoch  24 Batch  419 / 488  Training Loss  9.32873590500094e-05\n",
            "Epoch  24 Batch  420 / 488  Training Loss  0.00010164121340494603\n",
            "Epoch  24 Batch  421 / 488  Training Loss  6.601328641409054e-05\n",
            "Epoch  24 Batch  422 / 488  Training Loss  8.163569873431697e-05\n",
            "Epoch  24 Batch  423 / 488  Training Loss  7.674684457015246e-05\n",
            "Epoch  24 Batch  424 / 488  Training Loss  5.512290954357013e-05\n",
            "Epoch  24 Batch  425 / 488  Training Loss  6.196663889568299e-05\n",
            "Epoch  24 Batch  426 / 488  Training Loss  9.45058636716567e-05\n",
            "Epoch  24 Batch  427 / 488  Training Loss  9.125915676122531e-05\n",
            "Epoch  24 Batch  428 / 488  Training Loss  8.600320143159479e-05\n",
            "Epoch  24 Batch  429 / 488  Training Loss  6.178059265948832e-05\n",
            "Epoch  24 Batch  430 / 488  Training Loss  7.727099728072062e-05\n",
            "Epoch  24 Batch  431 / 488  Training Loss  0.00011735173029592261\n",
            "Epoch  24 Batch  432 / 488  Training Loss  0.000115382150397636\n",
            "Epoch  24 Batch  433 / 488  Training Loss  9.21434912015684e-05\n",
            "Epoch  24 Batch  434 / 488  Training Loss  8.291091944556683e-05\n",
            "Epoch  24 Batch  435 / 488  Training Loss  0.00010714749805629253\n",
            "Epoch  24 Batch  436 / 488  Training Loss  0.00010808995284605771\n",
            "Epoch  24 Batch  437 / 488  Training Loss  6.029335781931877e-05\n",
            "Epoch  24 Batch  438 / 488  Training Loss  8.090735354926437e-05\n",
            "Epoch  24 Batch  439 / 488  Training Loss  0.00011797074694186449\n",
            "Epoch  24 Batch  440 / 488  Training Loss  7.382134936051443e-05\n",
            "Epoch  24 Batch  441 / 488  Training Loss  6.208539707586169e-05\n",
            "Epoch  24 Batch  442 / 488  Training Loss  8.2270591519773e-05\n",
            "Epoch  24 Batch  443 / 488  Training Loss  8.419985533691943e-05\n",
            "Epoch  24 Batch  444 / 488  Training Loss  0.00012854405213147402\n",
            "Epoch  24 Batch  445 / 488  Training Loss  7.983107934705913e-05\n",
            "Epoch  24 Batch  446 / 488  Training Loss  8.374237222597003e-05\n",
            "Epoch  24 Batch  447 / 488  Training Loss  8.732637797947973e-05\n",
            "Epoch  24 Batch  448 / 488  Training Loss  6.784270226489753e-05\n",
            "Epoch  24 Batch  449 / 488  Training Loss  9.189346747007221e-05\n",
            "Epoch  24 Batch  450 / 488  Training Loss  0.00010461606871103868\n",
            "Epoch  24 Batch  451 / 488  Training Loss  8.933067147154361e-05\n",
            "Epoch  24 Batch  452 / 488  Training Loss  0.00010290461068507284\n",
            "Epoch  24 Batch  453 / 488  Training Loss  9.63057900662534e-05\n",
            "Epoch  24 Batch  454 / 488  Training Loss  0.00010223418939858675\n",
            "Epoch  24 Batch  455 / 488  Training Loss  8.43643065309152e-05\n",
            "Epoch  24 Batch  456 / 488  Training Loss  8.359717321582139e-05\n",
            "Epoch  24 Batch  457 / 488  Training Loss  9.37218137551099e-05\n",
            "Epoch  24 Batch  458 / 488  Training Loss  8.52560915518552e-05\n",
            "Epoch  24 Batch  459 / 488  Training Loss  7.213787466753274e-05\n",
            "Epoch  24 Batch  460 / 488  Training Loss  0.00010436608135933056\n",
            "Epoch  24 Batch  461 / 488  Training Loss  5.393200626713224e-05\n",
            "Epoch  24 Batch  462 / 488  Training Loss  9.368135215481743e-05\n",
            "Epoch  24 Batch  463 / 488  Training Loss  0.00010558995563769713\n",
            "Epoch  24 Batch  464 / 488  Training Loss  0.00010885801020776853\n",
            "Epoch  24 Batch  465 / 488  Training Loss  8.238186273956671e-05\n",
            "Epoch  24 Batch  466 / 488  Training Loss  6.208512058947235e-05\n",
            "Epoch  24 Batch  467 / 488  Training Loss  8.475464710500091e-05\n",
            "Epoch  24 Batch  468 / 488  Training Loss  7.99972694949247e-05\n",
            "Epoch  24 Batch  469 / 488  Training Loss  7.900896162027493e-05\n",
            "Epoch  24 Batch  470 / 488  Training Loss  7.30912433937192e-05\n",
            "Epoch  24 Batch  471 / 488  Training Loss  9.339942334918305e-05\n",
            "Epoch  24 Batch  472 / 488  Training Loss  0.00010878520697588101\n",
            "Epoch  24 Batch  473 / 488  Training Loss  7.976126653375104e-05\n",
            "Epoch  24 Batch  474 / 488  Training Loss  6.474068504758179e-05\n",
            "Epoch  24 Batch  475 / 488  Training Loss  0.00010040060442406684\n",
            "Epoch  24 Batch  476 / 488  Training Loss  6.692831811960787e-05\n",
            "Epoch  24 Batch  477 / 488  Training Loss  7.039052434265614e-05\n",
            "Epoch  24 Batch  478 / 488  Training Loss  7.029376865830272e-05\n",
            "Epoch  24 Batch  479 / 488  Training Loss  9.570717520546168e-05\n",
            "Epoch  24 Batch  480 / 488  Training Loss  0.00011594514216994867\n",
            "Epoch  24 Batch  481 / 488  Training Loss  7.371548417722806e-05\n",
            "Epoch  24 Batch  482 / 488  Training Loss  0.00010912180732702836\n",
            "Epoch  24 Batch  483 / 488  Training Loss  0.00010041345376521349\n",
            "Epoch  24 Batch  484 / 488  Training Loss  6.204142118804157e-05\n",
            "Epoch  24 Batch  485 / 488  Training Loss  0.00011724208161467686\n",
            "Epoch  24 Batch  486 / 488  Training Loss  8.04105366114527e-05\n",
            "Epoch  24 Batch  487 / 488  Training Loss  6.583709910046309e-05\n",
            "  25    |    -    |   0.000095   | 47.584220\n",
            "----------------------------------------------------------------------\n",
            "Running epoch: 25\n",
            "Epoch  25 Batch  0 / 488  Training Loss  5.8137386076850817e-05\n",
            "Epoch  25 Batch  1 / 488  Training Loss  6.42007653368637e-05\n",
            "Epoch  25 Batch  2 / 488  Training Loss  7.987133722053841e-05\n",
            "Epoch  25 Batch  3 / 488  Training Loss  4.3477419239934534e-05\n",
            "Epoch  25 Batch  4 / 488  Training Loss  7.144033588701859e-05\n",
            "Epoch  25 Batch  5 / 488  Training Loss  7.372720574494451e-05\n",
            "Epoch  25 Batch  6 / 488  Training Loss  6.976297299843282e-05\n",
            "Epoch  25 Batch  7 / 488  Training Loss  8.803528180578724e-05\n",
            "Epoch  25 Batch  8 / 488  Training Loss  6.658380152657628e-05\n",
            "Epoch  25 Batch  9 / 488  Training Loss  5.8027671911986545e-05\n",
            "Epoch  25 Batch  10 / 488  Training Loss  6.473521352745593e-05\n",
            "Epoch  25 Batch  11 / 488  Training Loss  5.136824256624095e-05\n",
            "Epoch  25 Batch  12 / 488  Training Loss  5.547514956560917e-05\n",
            "Epoch  25 Batch  13 / 488  Training Loss  6.87399078742601e-05\n",
            "Epoch  25 Batch  14 / 488  Training Loss  7.483750960091129e-05\n",
            "Epoch  25 Batch  15 / 488  Training Loss  6.286957795964554e-05\n",
            "Epoch  25 Batch  16 / 488  Training Loss  9.984326607082039e-05\n",
            "Epoch  25 Batch  17 / 488  Training Loss  7.373574771918356e-05\n",
            "Epoch  25 Batch  18 / 488  Training Loss  8.338965562870726e-05\n",
            "Epoch  25 Batch  19 / 488  Training Loss  7.716591062489897e-05\n",
            "Epoch  25 Batch  20 / 488  Training Loss  7.155405910452828e-05\n",
            "Epoch  25 Batch  21 / 488  Training Loss  6.743358972016722e-05\n",
            "Epoch  25 Batch  22 / 488  Training Loss  6.251916056498885e-05\n",
            "Epoch  25 Batch  23 / 488  Training Loss  7.58802198106423e-05\n",
            "Epoch  25 Batch  24 / 488  Training Loss  5.4453197662951425e-05\n",
            "Epoch  25 Batch  25 / 488  Training Loss  6.42136947135441e-05\n",
            "Epoch  25 Batch  26 / 488  Training Loss  6.288407166721299e-05\n",
            "Epoch  25 Batch  27 / 488  Training Loss  8.117694960674271e-05\n",
            "Epoch  25 Batch  28 / 488  Training Loss  5.5344375141430646e-05\n",
            "Epoch  25 Batch  29 / 488  Training Loss  5.817396959173493e-05\n",
            "Epoch  25 Batch  30 / 488  Training Loss  8.400343358516693e-05\n",
            "Epoch  25 Batch  31 / 488  Training Loss  5.1311297283973545e-05\n",
            "Epoch  25 Batch  32 / 488  Training Loss  7.789905794197693e-05\n",
            "Epoch  25 Batch  33 / 488  Training Loss  7.106872362783179e-05\n",
            "Epoch  25 Batch  34 / 488  Training Loss  6.184544326970354e-05\n",
            "Epoch  25 Batch  35 / 488  Training Loss  6.765363650629297e-05\n",
            "Epoch  25 Batch  36 / 488  Training Loss  8.129661728162318e-05\n",
            "Epoch  25 Batch  37 / 488  Training Loss  6.407545879483223e-05\n",
            "Epoch  25 Batch  38 / 488  Training Loss  8.855096530169249e-05\n",
            "Epoch  25 Batch  39 / 488  Training Loss  6.122830382082611e-05\n",
            "Epoch  25 Batch  40 / 488  Training Loss  8.162487210938707e-05\n",
            "Epoch  25 Batch  41 / 488  Training Loss  7.471717981388792e-05\n",
            "Epoch  25 Batch  42 / 488  Training Loss  5.299773329170421e-05\n",
            "Epoch  25 Batch  43 / 488  Training Loss  8.382833038922399e-05\n",
            "Epoch  25 Batch  44 / 488  Training Loss  5.637195863528177e-05\n",
            "Epoch  25 Batch  45 / 488  Training Loss  7.449167605955154e-05\n",
            "Epoch  25 Batch  46 / 488  Training Loss  8.64817266119644e-05\n",
            "Epoch  25 Batch  47 / 488  Training Loss  7.348119106609374e-05\n",
            "Epoch  25 Batch  48 / 488  Training Loss  0.00010415642464067787\n",
            "Epoch  25 Batch  49 / 488  Training Loss  0.00010107904381584376\n",
            "Epoch  25 Batch  50 / 488  Training Loss  7.852154521970078e-05\n",
            "Epoch  25 Batch  51 / 488  Training Loss  4.463131699594669e-05\n",
            "Epoch  25 Batch  52 / 488  Training Loss  6.458729330915958e-05\n",
            "Epoch  25 Batch  53 / 488  Training Loss  5.885055725229904e-05\n",
            "Epoch  25 Batch  54 / 488  Training Loss  8.75652112881653e-05\n",
            "Epoch  25 Batch  55 / 488  Training Loss  6.986025255173445e-05\n",
            "Epoch  25 Batch  56 / 488  Training Loss  8.266809163615108e-05\n",
            "Epoch  25 Batch  57 / 488  Training Loss  8.956663077697158e-05\n",
            "Epoch  25 Batch  58 / 488  Training Loss  6.74360417178832e-05\n",
            "Epoch  25 Batch  59 / 488  Training Loss  5.319136107573286e-05\n",
            "Epoch  25 Batch  60 / 488  Training Loss  0.00010038460459327325\n",
            "Epoch  25 Batch  61 / 488  Training Loss  5.3201882110442966e-05\n",
            "Epoch  25 Batch  62 / 488  Training Loss  7.591446046717465e-05\n",
            "Epoch  25 Batch  63 / 488  Training Loss  9.186538954963908e-05\n",
            "Epoch  25 Batch  64 / 488  Training Loss  6.936858699191362e-05\n",
            "Epoch  25 Batch  65 / 488  Training Loss  6.31783259450458e-05\n",
            "Epoch  25 Batch  66 / 488  Training Loss  8.069462637649849e-05\n",
            "Epoch  25 Batch  67 / 488  Training Loss  6.251486774999648e-05\n",
            "Epoch  25 Batch  68 / 488  Training Loss  6.990823749219999e-05\n",
            "Epoch  25 Batch  69 / 488  Training Loss  6.293346086749807e-05\n",
            "Epoch  25 Batch  70 / 488  Training Loss  8.84245237102732e-05\n",
            "Epoch  25 Batch  71 / 488  Training Loss  6.82690879330039e-05\n",
            "Epoch  25 Batch  72 / 488  Training Loss  6.607229443034157e-05\n",
            "Epoch  25 Batch  73 / 488  Training Loss  6.332337216008455e-05\n",
            "Epoch  25 Batch  74 / 488  Training Loss  6.957040022825822e-05\n",
            "Epoch  25 Batch  75 / 488  Training Loss  5.759300620411523e-05\n",
            "Epoch  25 Batch  76 / 488  Training Loss  7.798518345225602e-05\n",
            "Epoch  25 Batch  77 / 488  Training Loss  6.147762906039134e-05\n",
            "Epoch  25 Batch  78 / 488  Training Loss  4.5447512093232945e-05\n",
            "Epoch  25 Batch  79 / 488  Training Loss  6.455405673477799e-05\n",
            "Epoch  25 Batch  80 / 488  Training Loss  6.670034053968266e-05\n",
            "Epoch  25 Batch  81 / 488  Training Loss  6.174550799187273e-05\n",
            "Epoch  25 Batch  82 / 488  Training Loss  8.885488205123693e-05\n",
            "Epoch  25 Batch  83 / 488  Training Loss  6.649221904808655e-05\n",
            "Epoch  25 Batch  84 / 488  Training Loss  7.898567127995193e-05\n",
            "Epoch  25 Batch  85 / 488  Training Loss  5.1075337978545576e-05\n",
            "Epoch  25 Batch  86 / 488  Training Loss  7.112786988727748e-05\n",
            "Epoch  25 Batch  87 / 488  Training Loss  5.834076364408247e-05\n",
            "Epoch  25 Batch  88 / 488  Training Loss  9.197321924148127e-05\n",
            "Epoch  25 Batch  89 / 488  Training Loss  6.55749681754969e-05\n",
            "Epoch  25 Batch  90 / 488  Training Loss  7.602543337270617e-05\n",
            "Epoch  25 Batch  91 / 488  Training Loss  5.448905358207412e-05\n",
            "Epoch  25 Batch  92 / 488  Training Loss  7.504421955673024e-05\n",
            "Epoch  25 Batch  93 / 488  Training Loss  5.195698031457141e-05\n",
            "Epoch  25 Batch  94 / 488  Training Loss  7.070021820254624e-05\n",
            "Epoch  25 Batch  95 / 488  Training Loss  8.700967737240717e-05\n",
            "Epoch  25 Batch  96 / 488  Training Loss  6.408890476450324e-05\n",
            "Epoch  25 Batch  97 / 488  Training Loss  9.019326535053551e-05\n",
            "Epoch  25 Batch  98 / 488  Training Loss  6.128227687440813e-05\n",
            "Epoch  25 Batch  99 / 488  Training Loss  6.62789898342453e-05\n",
            "Epoch  25 Batch  100 / 488  Training Loss  6.602364010177553e-05\n",
            "Epoch  25 Batch  101 / 488  Training Loss  8.422000973951072e-05\n",
            "Epoch  25 Batch  102 / 488  Training Loss  5.584330210695043e-05\n",
            "Epoch  25 Batch  103 / 488  Training Loss  5.513579162652604e-05\n",
            "Epoch  25 Batch  104 / 488  Training Loss  5.9573845646809787e-05\n",
            "Epoch  25 Batch  105 / 488  Training Loss  5.167625567992218e-05\n",
            "Epoch  25 Batch  106 / 488  Training Loss  7.389242819044739e-05\n",
            "Epoch  25 Batch  107 / 488  Training Loss  6.59641227684915e-05\n",
            "Epoch  25 Batch  108 / 488  Training Loss  6.1011320212855935e-05\n",
            "Epoch  25 Batch  109 / 488  Training Loss  6.275087071117014e-05\n",
            "Epoch  25 Batch  110 / 488  Training Loss  3.7231446185614914e-05\n",
            "Epoch  25 Batch  111 / 488  Training Loss  6.28233392490074e-05\n",
            "Epoch  25 Batch  112 / 488  Training Loss  6.284049595706165e-05\n",
            "Epoch  25 Batch  113 / 488  Training Loss  7.87193130236119e-05\n",
            "Epoch  25 Batch  114 / 488  Training Loss  6.86594721628353e-05\n",
            "Epoch  25 Batch  115 / 488  Training Loss  5.354607128538191e-05\n",
            "Epoch  25 Batch  116 / 488  Training Loss  9.425264579476789e-05\n",
            "Epoch  25 Batch  117 / 488  Training Loss  7.606678263982758e-05\n",
            "Epoch  25 Batch  118 / 488  Training Loss  7.498785271309316e-05\n",
            "Epoch  25 Batch  119 / 488  Training Loss  7.641909178346395e-05\n",
            "Epoch  25 Batch  120 / 488  Training Loss  8.65310212248005e-05\n",
            "Epoch  25 Batch  121 / 488  Training Loss  8.738603355595842e-05\n",
            "Epoch  25 Batch  122 / 488  Training Loss  5.788435373688117e-05\n",
            "Epoch  25 Batch  123 / 488  Training Loss  5.3988962463336065e-05\n",
            "Epoch  25 Batch  124 / 488  Training Loss  7.459957851096988e-05\n",
            "Epoch  25 Batch  125 / 488  Training Loss  8.002831600606441e-05\n",
            "Epoch  25 Batch  126 / 488  Training Loss  5.8387631725054234e-05\n",
            "Epoch  25 Batch  127 / 488  Training Loss  6.144261715235189e-05\n",
            "Epoch  25 Batch  128 / 488  Training Loss  6.431525252992287e-05\n",
            "Epoch  25 Batch  129 / 488  Training Loss  7.910368731245399e-05\n",
            "Epoch  25 Batch  130 / 488  Training Loss  7.498542254325002e-05\n",
            "Epoch  25 Batch  131 / 488  Training Loss  6.444692553486675e-05\n",
            "Epoch  25 Batch  132 / 488  Training Loss  6.457425479311496e-05\n",
            "Epoch  25 Batch  133 / 488  Training Loss  7.465174712706357e-05\n",
            "Epoch  25 Batch  134 / 488  Training Loss  5.2661507652373984e-05\n",
            "Epoch  25 Batch  135 / 488  Training Loss  8.032139157876372e-05\n",
            "Epoch  25 Batch  136 / 488  Training Loss  6.417806434910744e-05\n",
            "Epoch  25 Batch  137 / 488  Training Loss  6.574627332156524e-05\n",
            "Epoch  25 Batch  138 / 488  Training Loss  6.825746095273644e-05\n",
            "Epoch  25 Batch  139 / 488  Training Loss  5.500718907569535e-05\n",
            "Epoch  25 Batch  140 / 488  Training Loss  5.241572580416687e-05\n",
            "Epoch  25 Batch  141 / 488  Training Loss  7.224279397632927e-05\n",
            "Epoch  25 Batch  142 / 488  Training Loss  5.1977200200781226e-05\n",
            "Epoch  25 Batch  143 / 488  Training Loss  7.300036668311805e-05\n",
            "Epoch  25 Batch  144 / 488  Training Loss  6.742494588252157e-05\n",
            "Epoch  25 Batch  145 / 488  Training Loss  5.680333561031148e-05\n",
            "Epoch  25 Batch  146 / 488  Training Loss  7.395825377898291e-05\n",
            "Epoch  25 Batch  147 / 488  Training Loss  7.071242725942284e-05\n",
            "Epoch  25 Batch  148 / 488  Training Loss  6.016347469994798e-05\n",
            "Epoch  25 Batch  149 / 488  Training Loss  6.225342804100364e-05\n",
            "Epoch  25 Batch  150 / 488  Training Loss  5.2901566959917545e-05\n",
            "Epoch  25 Batch  151 / 488  Training Loss  0.00010795002890517935\n",
            "Epoch  25 Batch  152 / 488  Training Loss  7.254756928887218e-05\n",
            "Epoch  25 Batch  153 / 488  Training Loss  4.6031484089326113e-05\n",
            "Epoch  25 Batch  154 / 488  Training Loss  7.728455238975585e-05\n",
            "Epoch  25 Batch  155 / 488  Training Loss  4.748784704133868e-05\n",
            "Epoch  25 Batch  156 / 488  Training Loss  8.170120418071747e-05\n",
            "Epoch  25 Batch  157 / 488  Training Loss  7.074525638017803e-05\n",
            "Epoch  25 Batch  158 / 488  Training Loss  7.61363044148311e-05\n",
            "Epoch  25 Batch  159 / 488  Training Loss  4.9926613428397104e-05\n",
            "Epoch  25 Batch  160 / 488  Training Loss  6.356269295793027e-05\n",
            "Epoch  25 Batch  161 / 488  Training Loss  5.9302739828126505e-05\n",
            "Epoch  25 Batch  162 / 488  Training Loss  7.990236190380529e-05\n",
            "Epoch  25 Batch  163 / 488  Training Loss  6.860902067273855e-05\n",
            "Epoch  25 Batch  164 / 488  Training Loss  6.446435872931033e-05\n",
            "Epoch  25 Batch  165 / 488  Training Loss  7.974815525813028e-05\n",
            "Epoch  25 Batch  166 / 488  Training Loss  9.883635357255116e-05\n",
            "Epoch  25 Batch  167 / 488  Training Loss  6.75012415740639e-05\n",
            "Epoch  25 Batch  168 / 488  Training Loss  6.260160444071516e-05\n",
            "Epoch  25 Batch  169 / 488  Training Loss  8.222755423048511e-05\n",
            "Epoch  25 Batch  170 / 488  Training Loss  7.299175194930285e-05\n",
            "Epoch  25 Batch  171 / 488  Training Loss  6.338133243843913e-05\n",
            "Epoch  25 Batch  172 / 488  Training Loss  6.59896177239716e-05\n",
            "Epoch  25 Batch  173 / 488  Training Loss  6.976174336159602e-05\n",
            "Epoch  25 Batch  174 / 488  Training Loss  7.704662129981443e-05\n",
            "Epoch  25 Batch  175 / 488  Training Loss  7.260784332174808e-05\n",
            "Epoch  25 Batch  176 / 488  Training Loss  6.587929965462536e-05\n",
            "Epoch  25 Batch  177 / 488  Training Loss  6.26028049737215e-05\n",
            "Epoch  25 Batch  178 / 488  Training Loss  7.575927156722173e-05\n",
            "Epoch  25 Batch  179 / 488  Training Loss  8.743836224311963e-05\n",
            "Epoch  25 Batch  180 / 488  Training Loss  6.759136158507317e-05\n",
            "Epoch  25 Batch  181 / 488  Training Loss  7.423743954859674e-05\n",
            "Epoch  25 Batch  182 / 488  Training Loss  7.593473128508776e-05\n",
            "Epoch  25 Batch  183 / 488  Training Loss  5.2859111747238785e-05\n",
            "Epoch  25 Batch  184 / 488  Training Loss  4.960421210853383e-05\n",
            "Epoch  25 Batch  185 / 488  Training Loss  6.658711936324835e-05\n",
            "Epoch  25 Batch  186 / 488  Training Loss  7.821525650797412e-05\n",
            "Epoch  25 Batch  187 / 488  Training Loss  9.187497926177457e-05\n",
            "Epoch  25 Batch  188 / 488  Training Loss  9.277373465010896e-05\n",
            "Epoch  25 Batch  189 / 488  Training Loss  5.99990707996767e-05\n",
            "Epoch  25 Batch  190 / 488  Training Loss  7.958008791320026e-05\n",
            "Epoch  25 Batch  191 / 488  Training Loss  9.577704622643068e-05\n",
            "Epoch  25 Batch  192 / 488  Training Loss  7.041231583571061e-05\n",
            "Epoch  25 Batch  193 / 488  Training Loss  8.436979987891391e-05\n",
            "Epoch  25 Batch  194 / 488  Training Loss  4.9541024054633453e-05\n",
            "Epoch  25 Batch  195 / 488  Training Loss  5.7137280236929655e-05\n",
            "Epoch  25 Batch  196 / 488  Training Loss  6.458739517256618e-05\n",
            "Epoch  25 Batch  197 / 488  Training Loss  5.74337100260891e-05\n",
            "Epoch  25 Batch  198 / 488  Training Loss  8.888147567631677e-05\n",
            "Epoch  25 Batch  199 / 488  Training Loss  7.26040598237887e-05\n",
            "Epoch  25 Batch  200 / 488  Training Loss  4.600694592227228e-05\n",
            "Epoch  25 Batch  201 / 488  Training Loss  8.375375909963623e-05\n",
            "Epoch  25 Batch  202 / 488  Training Loss  7.661218842258677e-05\n",
            "Epoch  25 Batch  203 / 488  Training Loss  4.9904811021406204e-05\n",
            "Epoch  25 Batch  204 / 488  Training Loss  6.275501073105261e-05\n",
            "Epoch  25 Batch  205 / 488  Training Loss  6.511901301564649e-05\n",
            "Epoch  25 Batch  206 / 488  Training Loss  7.95987798483111e-05\n",
            "Epoch  25 Batch  207 / 488  Training Loss  7.239061960717663e-05\n",
            "Epoch  25 Batch  208 / 488  Training Loss  9.211756696458906e-05\n",
            "Epoch  25 Batch  209 / 488  Training Loss  7.668974285479635e-05\n",
            "Epoch  25 Batch  210 / 488  Training Loss  4.494841778068803e-05\n",
            "Epoch  25 Batch  211 / 488  Training Loss  7.576285861432552e-05\n",
            "Epoch  25 Batch  212 / 488  Training Loss  4.260934656485915e-05\n",
            "Epoch  25 Batch  213 / 488  Training Loss  6.131766713224351e-05\n",
            "Epoch  25 Batch  214 / 488  Training Loss  5.3077594202477485e-05\n",
            "Epoch  25 Batch  215 / 488  Training Loss  6.441307778004557e-05\n",
            "Epoch  25 Batch  216 / 488  Training Loss  7.578324584756047e-05\n",
            "Epoch  25 Batch  217 / 488  Training Loss  6.865294562885538e-05\n",
            "Epoch  25 Batch  218 / 488  Training Loss  6.518643203889951e-05\n",
            "Epoch  25 Batch  219 / 488  Training Loss  6.598430627491325e-05\n",
            "Epoch  25 Batch  220 / 488  Training Loss  6.853308150311932e-05\n",
            "Epoch  25 Batch  221 / 488  Training Loss  6.409797060769051e-05\n",
            "Epoch  25 Batch  222 / 488  Training Loss  6.44532119622454e-05\n",
            "Epoch  25 Batch  223 / 488  Training Loss  8.038208761718124e-05\n",
            "Epoch  25 Batch  224 / 488  Training Loss  5.743798101320863e-05\n",
            "Epoch  25 Batch  225 / 488  Training Loss  8.400674414588138e-05\n",
            "Epoch  25 Batch  226 / 488  Training Loss  7.198649836936966e-05\n",
            "Epoch  25 Batch  227 / 488  Training Loss  5.46283190487884e-05\n",
            "Epoch  25 Batch  228 / 488  Training Loss  6.60465084365569e-05\n",
            "Epoch  25 Batch  229 / 488  Training Loss  6.917357677593827e-05\n",
            "Epoch  25 Batch  230 / 488  Training Loss  5.3226725867716596e-05\n",
            "Epoch  25 Batch  231 / 488  Training Loss  6.654147000517696e-05\n",
            "Epoch  25 Batch  232 / 488  Training Loss  8.361575601156801e-05\n",
            "Epoch  25 Batch  233 / 488  Training Loss  6.279903755057603e-05\n",
            "Epoch  25 Batch  234 / 488  Training Loss  5.0376336730550975e-05\n",
            "Epoch  25 Batch  235 / 488  Training Loss  9.336135553894565e-05\n",
            "Epoch  25 Batch  236 / 488  Training Loss  6.320593092823401e-05\n",
            "Epoch  25 Batch  237 / 488  Training Loss  8.002546383067966e-05\n",
            "Epoch  25 Batch  238 / 488  Training Loss  4.3557789467740804e-05\n",
            "Epoch  25 Batch  239 / 488  Training Loss  8.216725836973637e-05\n",
            "Epoch  25 Batch  240 / 488  Training Loss  7.55100481910631e-05\n",
            "Epoch  25 Batch  241 / 488  Training Loss  7.394876593025401e-05\n",
            "Epoch  25 Batch  242 / 488  Training Loss  8.549772610422224e-05\n",
            "Epoch  25 Batch  243 / 488  Training Loss  8.14403683762066e-05\n",
            "Epoch  25 Batch  244 / 488  Training Loss  7.85214506322518e-05\n",
            "Epoch  25 Batch  245 / 488  Training Loss  8.504006109433249e-05\n",
            "Epoch  25 Batch  246 / 488  Training Loss  8.490297477692366e-05\n",
            "Epoch  25 Batch  247 / 488  Training Loss  6.987646338529885e-05\n",
            "Epoch  25 Batch  248 / 488  Training Loss  6.196210597408935e-05\n",
            "Epoch  25 Batch  249 / 488  Training Loss  5.066718949819915e-05\n",
            "Epoch  25 Batch  250 / 488  Training Loss  5.867574509466067e-05\n",
            "Epoch  25 Batch  251 / 488  Training Loss  7.826661749277264e-05\n",
            "Epoch  25 Batch  252 / 488  Training Loss  7.986933633219451e-05\n",
            "Epoch  25 Batch  253 / 488  Training Loss  6.336306250886992e-05\n",
            "Epoch  25 Batch  254 / 488  Training Loss  8.752689609536901e-05\n",
            "Epoch  25 Batch  255 / 488  Training Loss  6.402197323041037e-05\n",
            "Epoch  25 Batch  256 / 488  Training Loss  7.23473058314994e-05\n",
            "Epoch  25 Batch  257 / 488  Training Loss  7.332380482694134e-05\n",
            "Epoch  25 Batch  258 / 488  Training Loss  6.530141399707645e-05\n",
            "Epoch  25 Batch  259 / 488  Training Loss  5.5802818678785115e-05\n",
            "Epoch  25 Batch  260 / 488  Training Loss  5.271011468721554e-05\n",
            "Epoch  25 Batch  261 / 488  Training Loss  6.62129168631509e-05\n",
            "Epoch  25 Batch  262 / 488  Training Loss  6.471199594670907e-05\n",
            "Epoch  25 Batch  263 / 488  Training Loss  5.758978659287095e-05\n",
            "Epoch  25 Batch  264 / 488  Training Loss  5.56100276298821e-05\n",
            "Epoch  25 Batch  265 / 488  Training Loss  6.276638305280358e-05\n",
            "Epoch  25 Batch  266 / 488  Training Loss  8.564793097320944e-05\n",
            "Epoch  25 Batch  267 / 488  Training Loss  7.179892418207601e-05\n",
            "Epoch  25 Batch  268 / 488  Training Loss  5.1171773520763963e-05\n",
            "Epoch  25 Batch  269 / 488  Training Loss  5.472331031342037e-05\n",
            "Epoch  25 Batch  270 / 488  Training Loss  6.1015649407636374e-05\n",
            "Epoch  25 Batch  271 / 488  Training Loss  6.399089761544019e-05\n",
            "Epoch  25 Batch  272 / 488  Training Loss  6.583584763575345e-05\n",
            "Epoch  25 Batch  273 / 488  Training Loss  5.3607713198289275e-05\n",
            "Epoch  25 Batch  274 / 488  Training Loss  4.335687117418274e-05\n",
            "Epoch  25 Batch  275 / 488  Training Loss  8.425685518886894e-05\n",
            "Epoch  25 Batch  276 / 488  Training Loss  7.190996257122606e-05\n",
            "Epoch  25 Batch  277 / 488  Training Loss  7.465347152901813e-05\n",
            "Epoch  25 Batch  278 / 488  Training Loss  6.07425972702913e-05\n",
            "Epoch  25 Batch  279 / 488  Training Loss  5.9099103964399546e-05\n",
            "Epoch  25 Batch  280 / 488  Training Loss  5.519304613699205e-05\n",
            "Epoch  25 Batch  281 / 488  Training Loss  5.6613796914462e-05\n",
            "Epoch  25 Batch  282 / 488  Training Loss  8.536533277947456e-05\n",
            "Epoch  25 Batch  283 / 488  Training Loss  7.668523903703317e-05\n",
            "Epoch  25 Batch  284 / 488  Training Loss  7.179478416219354e-05\n",
            "Epoch  25 Batch  285 / 488  Training Loss  5.0232920330017805e-05\n",
            "Epoch  25 Batch  286 / 488  Training Loss  7.097212073858827e-05\n",
            "Epoch  25 Batch  287 / 488  Training Loss  4.835892468690872e-05\n",
            "Epoch  25 Batch  288 / 488  Training Loss  6.303880218183622e-05\n",
            "Epoch  25 Batch  289 / 488  Training Loss  6.946486246306449e-05\n",
            "Epoch  25 Batch  290 / 488  Training Loss  6.641885556746274e-05\n",
            "Epoch  25 Batch  291 / 488  Training Loss  6.821105489507318e-05\n",
            "Epoch  25 Batch  292 / 488  Training Loss  6.844849849585444e-05\n",
            "Epoch  25 Batch  293 / 488  Training Loss  6.258600478759035e-05\n",
            "Epoch  25 Batch  294 / 488  Training Loss  7.469723641406745e-05\n",
            "Epoch  25 Batch  295 / 488  Training Loss  5.793053423985839e-05\n",
            "Epoch  25 Batch  296 / 488  Training Loss  6.194849265739322e-05\n",
            "Epoch  25 Batch  297 / 488  Training Loss  8.72322270879522e-05\n",
            "Epoch  25 Batch  298 / 488  Training Loss  6.0077923990320414e-05\n",
            "Epoch  25 Batch  299 / 488  Training Loss  7.720054418314248e-05\n",
            "Epoch  25 Batch  300 / 488  Training Loss  7.92218852438964e-05\n",
            "Epoch  25 Batch  301 / 488  Training Loss  6.520369788631797e-05\n",
            "Epoch  25 Batch  302 / 488  Training Loss  6.579920591320843e-05\n",
            "Epoch  25 Batch  303 / 488  Training Loss  5.4788175475550815e-05\n",
            "Epoch  25 Batch  304 / 488  Training Loss  7.399285095743835e-05\n",
            "Epoch  25 Batch  305 / 488  Training Loss  6.2429768149741e-05\n",
            "Epoch  25 Batch  306 / 488  Training Loss  7.720090070506558e-05\n",
            "Epoch  25 Batch  307 / 488  Training Loss  5.377645720727742e-05\n",
            "Epoch  25 Batch  308 / 488  Training Loss  5.997002517688088e-05\n",
            "Epoch  25 Batch  309 / 488  Training Loss  5.5057305871741846e-05\n",
            "Epoch  25 Batch  310 / 488  Training Loss  5.0177106459159404e-05\n",
            "Epoch  25 Batch  311 / 488  Training Loss  7.712868682574481e-05\n",
            "Epoch  25 Batch  312 / 488  Training Loss  7.011337584117427e-05\n",
            "Epoch  25 Batch  313 / 488  Training Loss  6.097017467254773e-05\n",
            "Epoch  25 Batch  314 / 488  Training Loss  7.717004336882383e-05\n",
            "Epoch  25 Batch  315 / 488  Training Loss  4.9821399443317205e-05\n",
            "Epoch  25 Batch  316 / 488  Training Loss  6.462406599894166e-05\n",
            "Epoch  25 Batch  317 / 488  Training Loss  7.4639348895289e-05\n",
            "Epoch  25 Batch  318 / 488  Training Loss  8.050094766076654e-05\n",
            "Epoch  25 Batch  319 / 488  Training Loss  7.314929098356515e-05\n",
            "Epoch  25 Batch  320 / 488  Training Loss  5.306243110680953e-05\n",
            "Epoch  25 Batch  321 / 488  Training Loss  7.594963244628161e-05\n",
            "Epoch  25 Batch  322 / 488  Training Loss  6.670922448392957e-05\n",
            "Epoch  25 Batch  323 / 488  Training Loss  5.751302160206251e-05\n",
            "Epoch  25 Batch  324 / 488  Training Loss  8.433054608758539e-05\n",
            "Epoch  25 Batch  325 / 488  Training Loss  6.556455628015101e-05\n",
            "Epoch  25 Batch  326 / 488  Training Loss  6.70094887027517e-05\n",
            "Epoch  25 Batch  327 / 488  Training Loss  6.0160098655615e-05\n",
            "Epoch  25 Batch  328 / 488  Training Loss  8.245948993135244e-05\n",
            "Epoch  25 Batch  329 / 488  Training Loss  5.156292536412366e-05\n",
            "Epoch  25 Batch  330 / 488  Training Loss  7.427217497024685e-05\n",
            "Epoch  25 Batch  331 / 488  Training Loss  6.044816836947575e-05\n",
            "Epoch  25 Batch  332 / 488  Training Loss  5.359383067116141e-05\n",
            "Epoch  25 Batch  333 / 488  Training Loss  4.7450692363781855e-05\n",
            "Epoch  25 Batch  334 / 488  Training Loss  7.58509268052876e-05\n",
            "Epoch  25 Batch  335 / 488  Training Loss  6.363880675053224e-05\n",
            "Epoch  25 Batch  336 / 488  Training Loss  9.545082866679877e-05\n",
            "Epoch  25 Batch  337 / 488  Training Loss  5.994364619255066e-05\n",
            "Epoch  25 Batch  338 / 488  Training Loss  6.474221299868077e-05\n",
            "Epoch  25 Batch  339 / 488  Training Loss  6.936139106983319e-05\n",
            "Epoch  25 Batch  340 / 488  Training Loss  8.062900451477617e-05\n",
            "Epoch  25 Batch  341 / 488  Training Loss  8.104638254735619e-05\n",
            "Epoch  25 Batch  342 / 488  Training Loss  6.90968445269391e-05\n",
            "Epoch  25 Batch  343 / 488  Training Loss  8.492868073517457e-05\n",
            "Epoch  25 Batch  344 / 488  Training Loss  7.95410160208121e-05\n",
            "Epoch  25 Batch  345 / 488  Training Loss  5.916485679335892e-05\n",
            "Epoch  25 Batch  346 / 488  Training Loss  6.877379928482696e-05\n",
            "Epoch  25 Batch  347 / 488  Training Loss  5.976802276563831e-05\n",
            "Epoch  25 Batch  348 / 488  Training Loss  8.405247353948653e-05\n",
            "Epoch  25 Batch  349 / 488  Training Loss  6.357392703648657e-05\n",
            "Epoch  25 Batch  350 / 488  Training Loss  7.807424844941124e-05\n",
            "Epoch  25 Batch  351 / 488  Training Loss  8.33389422041364e-05\n",
            "Epoch  25 Batch  352 / 488  Training Loss  5.4900658142287284e-05\n",
            "Epoch  25 Batch  353 / 488  Training Loss  6.046757334843278e-05\n",
            "Epoch  25 Batch  354 / 488  Training Loss  5.49312389921397e-05\n",
            "Epoch  25 Batch  355 / 488  Training Loss  5.068464815849438e-05\n",
            "Epoch  25 Batch  356 / 488  Training Loss  8.03166622063145e-05\n",
            "Epoch  25 Batch  357 / 488  Training Loss  5.300647535477765e-05\n",
            "Epoch  25 Batch  358 / 488  Training Loss  5.9887097449973226e-05\n",
            "Epoch  25 Batch  359 / 488  Training Loss  7.069665298331529e-05\n",
            "Epoch  25 Batch  360 / 488  Training Loss  7.006667146924883e-05\n",
            "Epoch  25 Batch  361 / 488  Training Loss  7.971326704137027e-05\n",
            "Epoch  25 Batch  362 / 488  Training Loss  7.08544539520517e-05\n",
            "Epoch  25 Batch  363 / 488  Training Loss  5.683307972503826e-05\n",
            "Epoch  25 Batch  364 / 488  Training Loss  6.55961484881118e-05\n",
            "Epoch  25 Batch  365 / 488  Training Loss  5.9971596783725545e-05\n",
            "Epoch  25 Batch  366 / 488  Training Loss  8.116938988678157e-05\n",
            "Epoch  25 Batch  367 / 488  Training Loss  5.2539755415637046e-05\n",
            "Epoch  25 Batch  368 / 488  Training Loss  8.450817404082045e-05\n",
            "Epoch  25 Batch  369 / 488  Training Loss  6.683378160232678e-05\n",
            "Epoch  25 Batch  370 / 488  Training Loss  5.777212936664e-05\n",
            "Epoch  25 Batch  371 / 488  Training Loss  5.736048842663877e-05\n",
            "Epoch  25 Batch  372 / 488  Training Loss  7.58887836127542e-05\n",
            "Epoch  25 Batch  373 / 488  Training Loss  6.812256469856948e-05\n",
            "Epoch  25 Batch  374 / 488  Training Loss  6.450079672504216e-05\n",
            "Epoch  25 Batch  375 / 488  Training Loss  6.169881817186251e-05\n",
            "Epoch  25 Batch  376 / 488  Training Loss  6.578023749170825e-05\n",
            "Epoch  25 Batch  377 / 488  Training Loss  7.146907591959462e-05\n",
            "Epoch  25 Batch  378 / 488  Training Loss  6.053165270714089e-05\n",
            "Epoch  25 Batch  379 / 488  Training Loss  7.648953032912686e-05\n",
            "Epoch  25 Batch  380 / 488  Training Loss  7.272828224813566e-05\n",
            "Epoch  25 Batch  381 / 488  Training Loss  7.252018986037001e-05\n",
            "Epoch  25 Batch  382 / 488  Training Loss  6.609386036871001e-05\n",
            "Epoch  25 Batch  383 / 488  Training Loss  6.475499685620889e-05\n",
            "Epoch  25 Batch  384 / 488  Training Loss  6.816801760578528e-05\n",
            "Epoch  25 Batch  385 / 488  Training Loss  6.081120955059305e-05\n",
            "Epoch  25 Batch  386 / 488  Training Loss  5.727207098971121e-05\n",
            "Epoch  25 Batch  387 / 488  Training Loss  8.215623529395089e-05\n",
            "Epoch  25 Batch  388 / 488  Training Loss  5.985521784168668e-05\n",
            "Epoch  25 Batch  389 / 488  Training Loss  4.250591155141592e-05\n",
            "Epoch  25 Batch  390 / 488  Training Loss  5.474286081152968e-05\n",
            "Epoch  25 Batch  391 / 488  Training Loss  7.214565994217992e-05\n",
            "Epoch  25 Batch  392 / 488  Training Loss  6.29640489933081e-05\n",
            "Epoch  25 Batch  393 / 488  Training Loss  6.575180304935202e-05\n",
            "Epoch  25 Batch  394 / 488  Training Loss  5.9731060900958255e-05\n",
            "Epoch  25 Batch  395 / 488  Training Loss  7.718772394582629e-05\n",
            "Epoch  25 Batch  396 / 488  Training Loss  6.9980371335987e-05\n",
            "Epoch  25 Batch  397 / 488  Training Loss  4.410337714944035e-05\n",
            "Epoch  25 Batch  398 / 488  Training Loss  6.972518167458475e-05\n",
            "Epoch  25 Batch  399 / 488  Training Loss  5.793707532575354e-05\n",
            "Epoch  25 Batch  400 / 488  Training Loss  5.893932029721327e-05\n",
            "Epoch  25 Batch  401 / 488  Training Loss  7.748891948722303e-05\n",
            "Epoch  25 Batch  402 / 488  Training Loss  6.862088048364967e-05\n",
            "Epoch  25 Batch  403 / 488  Training Loss  5.4725467634852976e-05\n",
            "Epoch  25 Batch  404 / 488  Training Loss  4.701235593529418e-05\n",
            "Epoch  25 Batch  405 / 488  Training Loss  5.8372468629386276e-05\n",
            "Epoch  25 Batch  406 / 488  Training Loss  5.7597637351136655e-05\n",
            "Epoch  25 Batch  407 / 488  Training Loss  7.476680912077427e-05\n",
            "Epoch  25 Batch  408 / 488  Training Loss  5.601291195489466e-05\n",
            "Epoch  25 Batch  409 / 488  Training Loss  9.669932478573173e-05\n",
            "Epoch  25 Batch  410 / 488  Training Loss  5.2920106099918485e-05\n",
            "Epoch  25 Batch  411 / 488  Training Loss  5.430115925264545e-05\n",
            "Epoch  25 Batch  412 / 488  Training Loss  7.4086754466407e-05\n",
            "Epoch  25 Batch  413 / 488  Training Loss  6.38486017123796e-05\n",
            "Epoch  25 Batch  414 / 488  Training Loss  8.153112867148593e-05\n",
            "Epoch  25 Batch  415 / 488  Training Loss  7.127384014893323e-05\n",
            "Epoch  25 Batch  416 / 488  Training Loss  6.609973206650466e-05\n",
            "Epoch  25 Batch  417 / 488  Training Loss  4.248152981745079e-05\n",
            "Epoch  25 Batch  418 / 488  Training Loss  8.123551378957927e-05\n",
            "Epoch  25 Batch  419 / 488  Training Loss  7.297199044842273e-05\n",
            "Epoch  25 Batch  420 / 488  Training Loss  7.974178151926026e-05\n",
            "Epoch  25 Batch  421 / 488  Training Loss  4.388833622215316e-05\n",
            "Epoch  25 Batch  422 / 488  Training Loss  8.172366506187245e-05\n",
            "Epoch  25 Batch  423 / 488  Training Loss  5.318248804542236e-05\n",
            "Epoch  25 Batch  424 / 488  Training Loss  0.00010969825234496966\n",
            "Epoch  25 Batch  425 / 488  Training Loss  6.702545942971483e-05\n",
            "Epoch  25 Batch  426 / 488  Training Loss  6.432646478060633e-05\n",
            "Epoch  25 Batch  427 / 488  Training Loss  5.0997034122701734e-05\n",
            "Epoch  25 Batch  428 / 488  Training Loss  9.165176015812904e-05\n",
            "Epoch  25 Batch  429 / 488  Training Loss  5.811959272250533e-05\n",
            "Epoch  25 Batch  430 / 488  Training Loss  7.935324538266286e-05\n",
            "Epoch  25 Batch  431 / 488  Training Loss  6.990636029513553e-05\n",
            "Epoch  25 Batch  432 / 488  Training Loss  7.132495375117287e-05\n",
            "Epoch  25 Batch  433 / 488  Training Loss  7.20927637303248e-05\n",
            "Epoch  25 Batch  434 / 488  Training Loss  0.00011941971024498343\n",
            "Epoch  25 Batch  435 / 488  Training Loss  7.632932829437777e-05\n",
            "Epoch  25 Batch  436 / 488  Training Loss  7.343324250541627e-05\n",
            "Epoch  25 Batch  437 / 488  Training Loss  5.693635466741398e-05\n",
            "Epoch  25 Batch  438 / 488  Training Loss  5.24814058735501e-05\n",
            "Epoch  25 Batch  439 / 488  Training Loss  7.102531526470557e-05\n",
            "Epoch  25 Batch  440 / 488  Training Loss  5.5220374633790925e-05\n",
            "Epoch  25 Batch  441 / 488  Training Loss  6.602055509574711e-05\n",
            "Epoch  25 Batch  442 / 488  Training Loss  5.698935274267569e-05\n",
            "Epoch  25 Batch  443 / 488  Training Loss  6.244975520530716e-05\n",
            "Epoch  25 Batch  444 / 488  Training Loss  6.540576578117907e-05\n",
            "Epoch  25 Batch  445 / 488  Training Loss  4.2314466554671526e-05\n",
            "Epoch  25 Batch  446 / 488  Training Loss  4.79293339594733e-05\n",
            "Epoch  25 Batch  447 / 488  Training Loss  4.248058394296095e-05\n",
            "Epoch  25 Batch  448 / 488  Training Loss  6.134821887826547e-05\n",
            "Epoch  25 Batch  449 / 488  Training Loss  7.370429375441745e-05\n",
            "Epoch  25 Batch  450 / 488  Training Loss  7.178574742283672e-05\n",
            "Epoch  25 Batch  451 / 488  Training Loss  6.232337182154879e-05\n",
            "Epoch  25 Batch  452 / 488  Training Loss  3.969680255977437e-05\n",
            "Epoch  25 Batch  453 / 488  Training Loss  5.7917168305721134e-05\n",
            "Epoch  25 Batch  454 / 488  Training Loss  8.381852967431769e-05\n",
            "Epoch  25 Batch  455 / 488  Training Loss  4.663651998271234e-05\n",
            "Epoch  25 Batch  456 / 488  Training Loss  5.935387889621779e-05\n",
            "Epoch  25 Batch  457 / 488  Training Loss  6.864832539577037e-05\n",
            "Epoch  25 Batch  458 / 488  Training Loss  5.875512579223141e-05\n",
            "Epoch  25 Batch  459 / 488  Training Loss  6.948290683794767e-05\n",
            "Epoch  25 Batch  460 / 488  Training Loss  9.572025010129437e-05\n",
            "Epoch  25 Batch  461 / 488  Training Loss  6.840575952082872e-05\n",
            "Epoch  25 Batch  462 / 488  Training Loss  7.691469363635406e-05\n",
            "Epoch  25 Batch  463 / 488  Training Loss  5.949863771093078e-05\n",
            "Epoch  25 Batch  464 / 488  Training Loss  5.7774352171691135e-05\n",
            "Epoch  25 Batch  465 / 488  Training Loss  7.409891986753792e-05\n",
            "Epoch  25 Batch  466 / 488  Training Loss  4.593384073814377e-05\n",
            "Epoch  25 Batch  467 / 488  Training Loss  4.075738979736343e-05\n",
            "Epoch  25 Batch  468 / 488  Training Loss  6.782886339351535e-05\n",
            "Epoch  25 Batch  469 / 488  Training Loss  7.326808554353192e-05\n",
            "Epoch  25 Batch  470 / 488  Training Loss  6.146682426333427e-05\n",
            "Epoch  25 Batch  471 / 488  Training Loss  6.364280125126243e-05\n",
            "Epoch  25 Batch  472 / 488  Training Loss  5.4564839956583455e-05\n",
            "Epoch  25 Batch  473 / 488  Training Loss  5.5928248912096024e-05\n",
            "Epoch  25 Batch  474 / 488  Training Loss  6.330858741421252e-05\n",
            "Epoch  25 Batch  475 / 488  Training Loss  6.663717795163393e-05\n",
            "Epoch  25 Batch  476 / 488  Training Loss  4.973193790647201e-05\n",
            "Epoch  25 Batch  477 / 488  Training Loss  5.581963341683149e-05\n",
            "Epoch  25 Batch  478 / 488  Training Loss  6.786106678191572e-05\n",
            "Epoch  25 Batch  479 / 488  Training Loss  7.101260416675359e-05\n",
            "Epoch  25 Batch  480 / 488  Training Loss  7.370397361228243e-05\n",
            "Epoch  25 Batch  481 / 488  Training Loss  8.720603364054114e-05\n",
            "Epoch  25 Batch  482 / 488  Training Loss  3.9881553675513715e-05\n",
            "Epoch  25 Batch  483 / 488  Training Loss  4.446446837391704e-05\n",
            "Epoch  25 Batch  484 / 488  Training Loss  4.606177026289515e-05\n",
            "Epoch  25 Batch  485 / 488  Training Loss  7.955484034027904e-05\n",
            "Epoch  25 Batch  486 / 488  Training Loss  7.624849968124181e-05\n",
            "Epoch  25 Batch  487 / 488  Training Loss  7.814462151145563e-05\n",
            "  26    |    -    |   0.000068   | 47.417996\n",
            "----------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yj-oEf1Usscm"
      },
      "source": [
        "final_list = []\n",
        "for index, row in data_test_inter.iterrows():\n",
        "    tmp = predictSum(row['inputs'], row['target'])\n",
        "    final_list.append(tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UqswvR4su3b"
      },
      "source": [
        "df_final = pd.DataFrame(final_list, columns=[\"text\", \"min_len\", \"max_len\", \"predict_len\", \"target\", \"predictions\"])\n",
        "df_final.to_csv('results_3num_bias.csv', index = False, header=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ezLwOycs-ZF"
      },
      "source": [
        "## Testing - Extrapolation (5-digit)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NqV6fEgts9le"
      },
      "source": [
        "data = pd.read_csv('finetune_extra_dataset.csv', header=None, names=['inputs', 'target'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rfT5VptuRZ7"
      },
      "source": [
        "data_extra = data.sample(n = 2000, random_state = 42).reset_index(drop=True)\n",
        "\n",
        "data_extra = data_extra.reset_index(drop=True)\n",
        "\n",
        "# Run function `preprocessing_for_bert` on the train set and the validation set\n",
        "inputs_extra, masks_extra = get_word_embeddings(data_extra['inputs'])\n",
        "\n",
        "data_extra['target_str'] = data_extra['target'].astype(str)\n",
        "\n",
        "#convert lists to tensors\n",
        "labels_extra = get_word_embeddings(data_extra['target_str'])[0]\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "data_extra = TensorDataset(inputs_extra, masks_extra, labels_extra)\n",
        "dataloader_extra = DataLoader(data_extra, shuffle = True, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vU-CEHIuFJG",
        "outputId": "8a4c3f9e-0dae-488f-b489-6afbeda9ed13"
      },
      "source": [
        "evaluate(model, dataloader_extra)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "11.21031746031746"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    }
  ]
}